{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import re\n",
    "from utils import helpers as hpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading OpenStack changes...\n",
      "OpenStack changes loaded successfully...\n"
     ]
    }
   ],
   "source": [
    "df = hpr.combine_openstack_data()\n",
    "df = df[df['status']!='NEW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['messages'] = df.loc[df['messages'].notna(), 'messages'].map(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = df['created'].map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The median of changes per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'creation_date' to string format 'year-month-day'\n",
    "df['created_str'] = df['created'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Group by 'creation_date' and count the number of rows per day\n",
    "daily_counts = df.groupby(df['created'].dt.date).size()\n",
    "\n",
    "# Display the result\n",
    "daily_counts_df = daily_counts.reset_index(name='count')\n",
    "daily_counts_df.columns = ['created_str', 'count']\n",
    "\n",
    "daily_counts_df['count'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_build_fialure = pd.read_csv(osp.join(\".\", \"Files\", \"Metrics\", \"num_build_failures.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"./Results/first_model_perf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The growth of dependent changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dep_ned_cha = pd.read_csv(osp.join('Files', 'source_target_evolution.csv'))\n",
    "deps_need_changes = df_dep_ned_cha['Source'].tolist() + df_dep_ned_cha['Target'].tolist()\n",
    "df['Count'] = 0\n",
    "df_reduced = df.loc[df['number'].isin(deps_need_changes), ['status', 'created', 'Count']]\n",
    "\n",
    "df_reduced['Year'] = df_reduced['created'].map(lambda x: x.year)\n",
    "\n",
    "# df_reduced = df_reduced.groupby('Year').count().reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_reduced[df_reduced['status']=='MERGED']\n",
    "df_merged = df_merged.groupby('Year').count().reset_index(level=0)\n",
    "df_merged['Status'] = 'MERGED'\n",
    "\n",
    "df_abandoned = df_reduced[df_reduced['status']=='ABANDONED']\n",
    "df_abandoned = df_abandoned.groupby('Year').count().reset_index(level=0)\n",
    "df_abandoned['Status'] = 'ABANDONED'\n",
    "\n",
    "df_new = df_reduced[df_reduced['status']=='NEW']\n",
    "df_new = df_new.groupby('Year').count().reset_index(level=0)\n",
    "df_new['Status'] = 'NEW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = pd.concat((df_merged[['Year', 'Status', 'Count']], df_abandoned[['Year', 'Status', 'Count']], df_new[['Year', 'Status', 'Count']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_perc_dependencies(row):\n",
    "    all_changes = df.loc[(df['status']==row['Status'])&(df['Year'] == row['Year']), 'number'].nunique()\n",
    "    return round(100 * (row['Count'] / all_changes), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['percentage'] = df_reduced.apply(calc_perc_dependencies, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.to_csv(\"./Files/Preliminary/deps_evolution.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When dependent changes are identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff(start, end):\n",
    "    if start > end:\n",
    "        start, end = end, start\n",
    "    current_date =  datetime.strptime(end, \"%Y-%m-%d %H:%M:%S\") \n",
    "    previous_date = datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\") \n",
    "    diff = current_date - previous_date\n",
    "    diff = float(\"{:.2f}\".format(diff.total_seconds() / 3600))\n",
    "    return diff\n",
    "\n",
    "\n",
    "def extract_attr(x, attr):\n",
    "    '''Extracts the passed-on parameter values out of the commit message \n",
    "    '''\n",
    "    rs = re.findall(\"%s:\\s[a-zA-Z0-9/\\.\\:\\+\\-\\#]{6,}\" % (attr), x)\n",
    "    result = []\n",
    "    for row in rs:\n",
    "        row = row[len(attr) + 2:]\n",
    "        change_id_pattern = re.search(r\"[a-zA-Z0-9]{41}\", row)\n",
    "        if change_id_pattern:\n",
    "            result.append(change_id_pattern[0])\n",
    "            continue\n",
    "        number_pattern = re.search(\"#?https?[\\:][/]{2}review[\\.](opendev|openstack)[\\.]org([a-z0-9A-Z\\-\\+/\\.#]*)\\d+\", row)\n",
    "        if number_pattern:\n",
    "            result.append(int(re.search(\"\\d+$\", number_pattern[0][0:])[0]))\n",
    "    return result if len(result) != 0 else None\n",
    "\n",
    "\n",
    "def retrieve_revision_date(row, attr, return_revision_date=True):\n",
    "    number = None\n",
    "    second_number = None\n",
    "\n",
    "    if attr == \"Depends-On\":\n",
    "        number = row[\"Target\"]\n",
    "        second_number = row[\"Source\"]\n",
    "        change_id = row[\"Source_change_id\"]\n",
    "    else:\n",
    "        number = row[\"Source\"]\n",
    "        second_number = row[\"Target\"]\n",
    "        change_id = row[\"Target_change_id\"]\n",
    "\n",
    "    df_row = df.loc[df[\"number\"] == number]\n",
    "    revisions = ast.literal_eval(df_row[\"revisions\"].values[0])\n",
    "    revisions = sorted(revisions, key=lambda x: x[\"created\"])\n",
    "    if  len(revisions) == 1:\n",
    "        if return_revision_date:\n",
    "            return revisions[0][\"created\"][:-11]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    first_revision = revisions[0]\n",
    "    first_message = first_revision[\"message\"]\n",
    "\n",
    "    results = extract_attr(first_message, attr)\n",
    "\n",
    "    if results and ((change_id in results) or (second_number in results)):\n",
    "        if return_revision_date:\n",
    "            return first_revision[\"created\"][:-11]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    for i in range(1,len(revisions)):\n",
    "        current_message = revisions[i][\"message\"]\n",
    "        created = revisions[i][\"created\"]\n",
    "        results = extract_attr(current_message, attr)\n",
    "        \n",
    "        if results and ((change_id in results) or (second_number in results)):\n",
    "\n",
    "            if return_revision_date:\n",
    "                return created[:-11]\n",
    "            else:\n",
    "                return i + 1\n",
    "\n",
    "def is_same_developer(row):\n",
    "    return \"Same\" if row[\"Source_dev\"] == row[\"Target_dev\"] else \"Different\"\n",
    "\n",
    "def identify_dependency(row):\n",
    "    source_date = row[\"Source_date\"] \n",
    "    target_date = row[\"Target_date\"]\n",
    "    link_date = datetime.strptime(row[\"link_date\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "    delta1 = (target_date - link_date).total_seconds() / (60 * 60)\n",
    "    delta2 = (source_date - link_date).total_seconds() / (60 * 60)\n",
    "\n",
    "    return abs(delta2)\n",
    "    # return min(abs(delta1), abs(delta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depends-On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_changes = df['number'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depends_on = pd.read_csv(\"./Files/source_target_depends.csv\")\n",
    "df_depends_on = df_depends_on[df_depends_on['Source'].isin(all_changes)&df_depends_on['Target'].isin(all_changes)]\n",
    "df_depends_on[\"Source_status\"] = df_depends_on[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"status\"].values[0])\n",
    "df_depends_on[\"Target_status\"] = df_depends_on[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"status\"].values[0])\n",
    "\n",
    "df_depends_on[\"revisions\"] = df_depends_on[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"revisions\"].values[0])\n",
    "df_depends_on[\"Source_change_id\"] = df_depends_on[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"change_id\"].values[0])\n",
    "df_depends_on[\"Target_change_id\"] = df_depends_on[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"change_id\"].values[0])\n",
    "\n",
    "df_depends_on[\"link_date\"] = df_depends_on.apply(retrieve_revision_date, args=(\"Depends-On\",), axis=1)\n",
    "\n",
    "df_depends_on[\"worked_revisions\"] = df_depends_on.apply(retrieve_revision_date, args=(\"Depends-On\",False,), axis=1)\n",
    "\n",
    "df_depends_on[\"is_cross\"] = df_depends_on.apply(lambda row: \"Cross\" if row[\"Source_repo\"]!=row[\"Target_repo\"] else \"Same\", axis=1)\n",
    "\n",
    "df_depends_on[\"is_source_bot\"] = df_depends_on[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x,\"is_owner_bot\"].values[0])\n",
    "df_depends_on[\"is_target_bot\"] = df_depends_on[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x,\"is_owner_bot\"].values[0])\n",
    "\n",
    "df_depends_on[\"Source_dev\"] = df_depends_on[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x,\"owner_account_id\"].values[0])\n",
    "df_depends_on[\"Target_dev\"] = df_depends_on[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x,\"owner_account_id\"].values[0])\n",
    "\n",
    "df_depends_on[\"same_dev\"] = df_depends_on.apply(is_same_developer, axis=1)\n",
    "\n",
    "df_depends_on[\"Source_date\"] = df_depends_on[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"created\"].values[0])\n",
    "df_depends_on[\"Target_date\"] = df_depends_on[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"created\"].values[0])\n",
    "\n",
    "df_depends_on[\"when_identified\"] = df_depends_on[[\"Source_date\", \"Target_date\", \"link_date\"]].apply(identify_dependency, axis=1)\n",
    "\n",
    "df_depends_on = df_depends_on[(df_depends_on['Source_status']=='MERGED')|(df_depends_on['Target_status']=='MERGED')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needed-By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needed_by = pd.read_csv(\"./Files/source_target_needed.csv\")\n",
    "df_needed_by = df_needed_by[df_needed_by['Source'].isin(all_changes)&df_needed_by['Target'].isin(all_changes)]\n",
    "df_needed_by[\"Source_status\"] = df_needed_by[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"status\"].values[0])\n",
    "df_needed_by[\"Target_status\"] = df_needed_by[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"status\"].values[0])\n",
    "\n",
    "df_needed_by[\"revisions\"] = df_needed_by[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"revisions\"].values[0])\n",
    "df_needed_by[\"Source_change_id\"] = df_needed_by[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"change_id\"].values[0])\n",
    "df_needed_by[\"Target_change_id\"] = df_needed_by[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"change_id\"].values[0])\n",
    "\n",
    "df_needed_by[\"link_date\"] = df_needed_by.apply(retrieve_revision_date, args=(\"Needed-By\",), axis=1)\n",
    "df_needed_by[\"worked_revisions\"] = df_needed_by.apply(retrieve_revision_date, args=(\"Needed-By\",False,), axis=1)\n",
    "\n",
    "df_needed_by[\"is_cross\"] = df_needed_by.apply(lambda row: \"Cross\" if row[\"Source_repo\"]!=row[\"Target_repo\"] else \"Same\", axis=1)\n",
    "\n",
    "df_needed_by[\"is_source_bot\"] = df_needed_by[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x,\"is_owner_bot\"].values[0])\n",
    "df_needed_by[\"is_target_bot\"] = df_needed_by[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x,\"is_owner_bot\"].values[0])\n",
    "\n",
    "df_needed_by[\"Source_dev\"] = df_needed_by[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x,\"owner_account_id\"].values[0])\n",
    "df_needed_by[\"Target_dev\"] = df_needed_by[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x,\"owner_account_id\"].values[0])\n",
    "\n",
    "df_needed_by[\"same_dev\"] = df_needed_by.apply(is_same_developer, axis=1)\n",
    "\n",
    "df_needed_by[\"Source_date\"] = df_needed_by[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"created\"].values[0])\n",
    "df_needed_by[\"Target_date\"] = df_needed_by[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"created\"].values[0])\n",
    "\n",
    "df_needed_by[\"when_identified\"] = df_needed_by[[\"Source_date\", \"Target_date\", \"link_date\"]].apply(identify_dependency, axis=1)\n",
    "df_needed_by = df_needed_by[(df_needed_by['Source_status']=='MERGED')|(df_needed_by['Target_status']=='MERGED')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_identification = pd.concat((df_depends_on, df_needed_by)).sort_values(\"when_identified\")\n",
    "dependency_identification = dependency_identification.drop_duplicates(subset=[\"Source\", \"Target\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff(row):\n",
    "    start, end = row['Source_date'], row['Target_date']\n",
    "    if start > end:\n",
    "        start, end = end, start\n",
    "    # current_date =  datetime.strptime(end, \"%Y-%m-%d %H:%M:%S\") \n",
    "    # previous_date = datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\") \n",
    "    diff = end - start\n",
    "    diff = float(\"{:.2f}\".format(diff.total_seconds() / 3600))\n",
    "    return diff\n",
    "\n",
    "def lag_nbr_changes(row):\n",
    "    start, end = row['Source_date'], row['Target_date']\n",
    "    if start > end:\n",
    "        start, end = end, start\n",
    "    end =  datetime.strptime(end, \"%Y-%m-%d %H:%M:%S\") \n",
    "    start = datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\") \n",
    "    res = df.loc[(df['created']>=start)&(df['created']<=end), \"number\"].nunique()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_identification['lag'] = dependency_identification.apply(time_diff, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_identification = dependency_identification[['lag']]\n",
    "\n",
    "# Calculate Z-scores\n",
    "z_scores = np.abs((dependency_identification - dependency_identification.mean()) / dependency_identification.std())\n",
    "\n",
    "# Set a threshold for identifying outliers\n",
    "threshold = 3\n",
    "\n",
    "# Filter out the outliers\n",
    "df_clean = dependency_identification[(z_scores < threshold).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572188404940606"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_clean.loc[df_clean['lag']<=38*24, \"lag\"])/len(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6315.89"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['lag'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61999.54"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(dependency_identification.loc[dependency_identification['lag']<=38*24, \"lag\"])/len(dependency_identification)\n",
    "dependency_identification['lag'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency_identification['lag_changes'] = dependency_identification.apply(lag_nbr_changes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_identification.loc[dependency_identification['lag']==57, 'lag_changes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When miss a dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_revi_deps = df_depends_on.loc[df_depends_on['when_identified']>1, 'Source'].unique()\n",
    "after_revi_need = df_needed_by.loc[(~df_needed_by['Target'].isin(after_revi_deps))&(df_needed_by['when_identified']>1), 'Target'].unique()\n",
    "after_review = set(after_revi_deps.tolist()+after_revi_deps.tolist())\n",
    "\n",
    "all_deps_deps_on = df_depends_on['Source'].unique()\n",
    "all_deps_need = df_needed_by.loc[(~df_needed_by['Target'].isin(all_deps_deps_on)), 'Target'].unique()\n",
    "all_dependent_changes = set(all_deps_deps_on.tolist()+all_deps_need.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depends_ident = df_depends_on.loc[(df_depends_on['Source'].isin(after_revi_deps)), \"when_identified\"].tolist()\n",
    "df_needed_ident = df_needed_by.loc[(df_needed_by['Target'].isin(after_revi_need)), \"when_identified\"].tolist()\n",
    "ident_time = df_depends_ident + df_needed_ident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(ident_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depends_ident = df_depends_on.loc[(df_depends_on['worked_revisions']>1)&(df_depends_on['when_identified']>0)&(df_depends_on['Source'].isin(after_revi_deps)), \"when_identified\"].tolist()\n",
    "df_needed_ident = df_needed_by.loc[(df_needed_by['worked_revisions']>1)&(df_needed_by['when_identified']>0)&(df_needed_by['Target'].isin(after_revi_need)), \"when_identified\"].tolist()\n",
    "after_rev_ident_time = df_depends_ident + df_needed_ident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"time\": after_rev_ident_time}).to_csv(osp.join(\".\", \"Files\",\"Preliminary\", \"deps_ident.csv\"), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dependent_changes = pd.concat((df_depends_on, df_needed_by))\n",
    "all_dependent_changes.drop_duplicates(subset=['Source', 'Target'], inplace=True)\n",
    "all_dependent_changes = set(hpr.flatten_list(all_dependent_changes[['Source', 'Target']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2732972874733064"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df['status']=='MERGED')&df['number'].isin(after_review)])/len(df[df['number'].isin(all_dependent_changes)&(df['status']=='MERGED')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17336.070277777777"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_identification.loc[(dependency_identification['worked_revisions']>1)&(dependency_identification['worked_revisions']>1), \"when_identified\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat((df_depends_on, df_needed_by))\n",
    "test.drop_duplicates(subset=['Source', 'Target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49000"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(hpr.flatten_list(test[['Source', 'Target']].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_identification.to_csv(osp.join(\".\", \"Files\", \"Preliminary\", \"deps_ident.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_identification = pd.read_csv(osp.join(\".\", \"Files\", \"Preliminary\", \"deps_ident.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.7265162037037"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_identification.loc[dependency_identification['iden_type']=='slow', 'when_identified'].median()/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency_identification['when_identified'].median()\n",
    "dependency_identification.loc[(dependency_identification['Source_status']=='MERGED')&(dependency_identification['Target_status']=='MERGED')].to_csv(osp.join(\".\", \"Files\", \"Preliminary\", \"deps_ident.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many dependent changes contain a build failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dependent_cha = pd.DataFrame({'number': list(all_dependent_changes)})\n",
    "df_dependent_cha = pd.merge(\n",
    "    df_dependent_cha,\n",
    "    right=df_build_fialure,\n",
    "    on='number'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5158020274299344"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dependent_cha[(df_dependent_cha['num_build_failures']>1)&df_dependent_cha['number'].isin(after_review)])/len(df_dependent_cha[(df_dependent_cha['num_build_failures']>1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_build_failure(nbr):\n",
    "    res = df.loc[df['number']==nbr, 'messages'].tolist()[0]\n",
    "    # print(res)\n",
    "    return type(res) == list\n",
    "\n",
    "def when_deps_linked(row):\n",
    "    link_date_deps = df_depends_on.loc[df_depends_on['Target']==row['number'], 'link_date'].tolist()\n",
    "    link_date_need = df_needed_by.loc[df_needed_by['Source']==row['number'], 'link_date'].tolist()\n",
    "    res = link_date_deps + link_date_need\n",
    "    res.sort()\n",
    "        \n",
    "    return res if len(res) > 0 else None\n",
    "\n",
    "def when_build_failure_occur(row):\n",
    "    linked_date = row['when_linked'][0]\n",
    "    messages = row['messages']\n",
    "    messages = sorted(messages, key=lambda d: d['date'])\n",
    "    build_fail_date = messages[0]['date'][:18]\n",
    "    print(linked_date, build_fail_date)\n",
    "    if linked_date == build_fail_date:\n",
    "        return 'Instant'\n",
    "    elif linked_date > build_fail_date:\n",
    "        return 'Before'\n",
    "    else:\n",
    "        return 'After'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dependent_cha['has_build_failure'] = df_dependent_cha['number'].map(has_build_failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dependent_cha = pd.merge(\n",
    "    left=df_dependent_cha, \n",
    "    right=df[['number', 'revisions', 'messages']], \n",
    "    left_on='number', \n",
    "    right_on='number', \n",
    "    how='inner',\n",
    "    suffixes=('_target', '_source')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dependent_cha[\"revisions\"] = df_dependent_cha.loc[df_dependent_cha['when_linked'].notna(), \"revisions\"].map(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6457274105788957"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dependent_cha[(df_dependent_cha['has_build_failure']==True)])/len(df_dependent_cha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dependent_cha['when_linked'] = df_dependent_cha[df_dependent_cha['has_build_failure']==True].apply(when_deps_linked, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dependent_cha['when_failure_occur'] = df_dependent_cha[df_dependent_cha['when_linked'].notna()].apply(when_build_failure_occur,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4076660988074957"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7179/len(df_dependent_cha.loc[df_dependent_cha['when_linked'].notna(), \"when_failure_occur\"])#.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the size of our testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(10):\n",
    "    test = pd.read_csv(f\"./Files/Data/Test/{i}.csv\")\n",
    "    result.append(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_analysis = pd.DataFrame({'number': df_depends_on['Target'].unique().tolist() + df_needed_by['Source'].unique().tolist()})\n",
    "df_manual_analysis.drop_duplicates(subset='number', inplace=True)\n",
    "df_manual_analysis = df_manual_analysis.sample(n=379)\n",
    "df_manual_analysis.sort_values('number', inplace=True)\n",
    "df_manual_analysis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_manual_analysis = pd.merge(\n",
    "    left=df_manual_analysis, \n",
    "    right=df_depends_on.drop_duplicates(subset='Target')[['Target', 'worked_revisions']], \n",
    "    left_on='number',\n",
    "    right_on='Target',\n",
    "    how='left',\n",
    "    suffixes=('_target', '_source')\n",
    ")\n",
    "df_manual_analysis.loc[df_manual_analysis['worked_revisions'].isnull(), 'worked_revisions'] = df_manual_analysis.loc[df_manual_analysis['worked_revisions'].isnull(), 'number'].map(lambda x: df_needed_by.loc[df_needed_by['Source']==x, 'worked_revisions'].values[0])\n",
    "labels = {\n",
    "    1: \"Configuration\",\n",
    "    2: \"Dependency\",\n",
    "    3: \"Code enhancement\",\n",
    "    4: \"New features\",\n",
    "    5: \"Docs\",\n",
    "    6: \"Renaming\",\n",
    "    7: \"Tests\",\n",
    "    8: \"Refactoring\",\n",
    "    9: \"Moving resources\",\n",
    "    10: \"Others\",\n",
    "}\n",
    "nbr = 920233\n",
    "df_manual_analysis.loc[\n",
    "    df_manual_analysis['number']==nbr,\n",
    "    [\"reviewer_suggest\", \"reason\", 'comment']\n",
    "] = [0, labels[2], \"\"]\n",
    "df_manual_analysis.to_csv(osp.join(\".\", \"Files\", \"manual_analysis.csv\"), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_analysis = pd.read_csv(osp.join(\".\", \"Files\", \"manual_analysis.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>worked_revisions</th>\n",
       "      <th>reason</th>\n",
       "      <th>comment</th>\n",
       "      <th>reviewer_suggest</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156691</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Refactoring</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Code enhancement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Code enhancement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181260</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tests</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>182072</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Configuration</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>918920</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tests</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>919265</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tests</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>919339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tests</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>919494</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tests</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>920233</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dependency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     number  worked_revisions            reason comment  reviewer_suggest  tag\n",
       "0    156691               1.0       Refactoring     NaN               0.0  NaN\n",
       "1    161355               1.0  Code enhancement     NaN               0.0  NaN\n",
       "2    175394               1.0  Code enhancement     NaN               0.0  NaN\n",
       "3    181260               1.0             Tests     NaN               0.0  NaN\n",
       "4    182072               1.0     Configuration     NaN               0.0  NaN\n",
       "..      ...               ...               ...     ...               ...  ...\n",
       "374  918920               1.0             Tests     NaN               0.0  NaN\n",
       "375  919265               1.0             Tests     NaN               0.0  NaN\n",
       "376  919339               1.0             Tests     NaN               0.0  NaN\n",
       "377  919494               1.0             Tests     NaN               0.0  NaN\n",
       "378  920233               1.0        Dependency     NaN               0.0  NaN\n",
       "\n",
       "[387 rows x 6 columns]"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_analysis = pd.read_csv(osp.join(\".\", \"Files\", \"manual_analysis.csv\"))\n",
    "df_manual_analysis.loc[df_manual_analysis['reason']=='Test', 'reason'] = 'Tests'\n",
    "df_manual_analysis['reason'] = df_manual_analysis['reason'].map(lambda x: x.split(','))\n",
    "df_manual_analysis = df_manual_analysis.explode(column='reason')\n",
    "# Calculate the percentage of missing dependencies\n",
    "# df_manual_analysis = df_manual_analysis.groupby('reason').apply(lambda x: pd.Series({\n",
    "#     '\\# missing deps': (x['worked_revisions'] > 1).sum(),\n",
    "#     'Total': x['worked_revisions'].count(),\n",
    "#     '\\\\% of missing': f\"{round((x['worked_revisions'] > 1).sum() / len(x) * 100, 2)}\\%\"\n",
    "#     # 'Description': 'No description yet'\n",
    "# })).reset_index()\n",
    "\n",
    "# # # Rename columns to match the desired output\n",
    "# df_manual_analysis.rename(columns={'reason': 'Category'}, inplace=True)\n",
    "# df_manual_analysis.sort_values(by='\\# missing deps', ascending=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {\n",
    "    1: 'Complex change',\n",
    "    2: 'Project unfamiliarity',\n",
    "    3: 'Inconsistent deps',\n",
    "    4: 'Build failure'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{A detailed overview of various reasons for which developers miss adding a dependency.}\n",
      "\\label{tab:manual-analysis}\n",
      "\\begin{tabular}{lrrl}\n",
      "\\toprule\n",
      "Category & \\# missing deps & Total & \\% of missing \\\\\n",
      "\\midrule\n",
      "Configuration & 43 & 110 & 39.09\\% \\\\\n",
      "Dependency & 28 & 55 & 50.91\\% \\\\\n",
      "Refactoring & 26 & 60 & 43.33\\% \\\\\n",
      "New features & 25 & 48 & 52.08\\% \\\\\n",
      "Tests & 23 & 52 & 44.23\\% \\\\\n",
      "Code enhancement & 14 & 26 & 53.85\\% \\\\\n",
      "Docs & 8 & 13 & 61.54\\% \\\\\n",
      "Moving resources & 3 & 11 & 27.27\\% \\\\\n",
      "Renaming & 2 & 11 & 18.18\\% \\\\\n",
      "Duplicate & 0 & 1 & 0.0\\% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_manual_analysis.to_latex(index=False, caption='A detailed overview of various reasons for which developers miss adding a dependency.', label='tab:manual-analysis', float_format=\"%.2f\", ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
