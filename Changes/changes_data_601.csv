id,project,branch,change_id,subject,status,created,updated,submitted,reviewers,revisions,total_comment_count,number,current_revision,discussion_messages_count,reviewers_count,revisions_count,owner_account_id,owner_name,owner_username,is_owner_bot,commit_message,git_command,changed_files,files_count,commit_id,topic,added_lines,deleted_lines,insertions,deletions
openstack%2Fdevstack-gate~master~I8fd1965d61fd83269b6f919d4d0b967acb9975d2,openstack/devstack-gate,master,I8fd1965d61fd83269b6f919d4d0b967acb9975d2,Test the devstack-gate,NEW,2016-10-08 02:27:24.000000000,2017-12-18 04:47:03.000000000,,[{'_account_id': 20671}],"[{'number': 1, 'created': '2016-10-08 02:27:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/0df117312b9c3591683b3cb1ca5deea8820543e1', 'message': 'Test the devstack-gate\n\nThis is a test patch, please ignore it.\n\nChange-Id: I8fd1965d61fd83269b6f919d4d0b967acb9975d2\n'}, {'number': 2, 'created': '2016-10-08 07:11:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/d963420bf6b275b630c53c5401896aea67e3f054', 'message': 'Test the devstack-gate\n\nThis is a test patch, please ignore it.\n\nChange-Id: I8fd1965d61fd83269b6f919d4d0b967acb9975d2\n'}, {'number': 3, 'created': '2016-10-09 16:30:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/ac1d6f40c564951b3835c01ad79c9771cae21f43', 'message': 'Test the devstack-gate\n\nThis is a test patch, please ignore it.\n\nChange-Id: I8fd1965d61fd83269b6f919d4d0b967acb9975d2\n'}, {'number': 4, 'created': '2016-10-10 01:31:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/9c3072f5311d0b320a3253f5995b60554b217272', 'message': 'Test the devstack-gate\n\nThis is a test patch, please ignore it.\n\nChange-Id: I8fd1965d61fd83269b6f919d4d0b967acb9975d2\n'}, {'number': 5, 'created': '2016-10-10 03:56:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/bc79ba7e3e82c52435b1ef386e47eb73b8ad7138', 'message': 'Test the devstack-gate\n\nThis is a test patch, please ignore it.\n\nChange-Id: I8fd1965d61fd83269b6f919d4d0b967acb9975d2\n'}, {'number': 6, 'created': '2016-10-10 08:11:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/454cb5cbbafb31a9adf86466dfd08f063c643ea4', 'message': 'Test the devstack-gate\n\nThis is a test patch, please ignore it.\n\nChange-Id: I8fd1965d61fd83269b6f919d4d0b967acb9975d2\n'}, {'number': 7, 'created': '2016-10-10 09:45:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/41fcd96cf79c4b04eb66c60aef1acb1e353dd41a', 'message': 'Test the devstack-gate\n\nThis is a test patch, please ignore it.\n\nChange-Id: I8fd1965d61fd83269b6f919d4d0b967acb9975d2\n'}, {'number': 8, 'created': '2016-10-10 11:38:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/dd76afabfcab9d716c8d6c0766793cd9ef2183a7', 'message': 'Test the devstack-gate\n\nThis is a test patch, please ignore it.\n\nChange-Id: I8fd1965d61fd83269b6f919d4d0b967acb9975d2\n'}, {'number': 9, 'created': '2016-10-10 13:21:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/a04e8e3adeac13d6b1e977bb56b27d6fa4326cdf', 'message': 'Test the devstack-gate\n\nThis is a test patch, please ignore it.\n\nChange-Id: I8fd1965d61fd83269b6f919d4d0b967acb9975d2\n'}, {'number': 10, 'created': '2016-10-10 16:16:25.000000000', 'files': ['devstack-vm-gate.sh'], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/3623c5ac1e74a17890d09c0c4ca68eb500c83918', 'message': 'Test the devstack-gate\n\nThis is a test patch, please ignore it.\n\nChange-Id: I8fd1965d61fd83269b6f919d4d0b967acb9975d2\n'}]",0,383996,3623c5ac1e74a17890d09c0c4ca68eb500c83918,52,1,10,20671,,,0,"Test the devstack-gate

This is a test patch, please ignore it.

Change-Id: I8fd1965d61fd83269b6f919d4d0b967acb9975d2
",git fetch https://review.opendev.org/openstack/devstack-gate refs/changes/96/383996/7 && git format-patch -1 --stdout FETCH_HEAD,['devstack-vm-gate.sh'],1,0df117312b9c3591683b3cb1ca5deea8820543e1,," remote_command ""$NODE"" ""ifconfig -a >/tmp/see.txt"" remote_command ""$NODE"" ""cat /tmp/see.txt""",,2,0
openstack%2Ftacker~master~I6822e3f7bc00e47ee2c75b0944bcb4637310fa02,openstack/tacker,master,I6822e3f7bc00e47ee2c75b0944bcb4637310fa02,Validate the Tosca VNFD templates,NEW,2016-06-24 11:07:28.000000000,2017-12-18 04:46:48.000000000,,"[{'_account_id': 2874}, {'_account_id': 10487}, {'_account_id': 12455}, {'_account_id': 13380}, {'_account_id': 13485}, {'_account_id': 16803}, {'_account_id': 18955}, {'_account_id': 20866}, {'_account_id': 22061}]","[{'number': 1, 'created': '2016-06-24 11:07:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/87f29df5c6aad6cdf13a1c7937162fb4c2f449e2', 'message': 'Validate the Tosca VNFD templates\n\nFunctionality has been added for validating VNFD templates and raising exceptions if invalid.\nPartial-Bug: #1483953\n\nChange-Id: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n'}, {'number': 2, 'created': '2016-06-28 10:31:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/1c2533f0e4658404d5fb92a2f32a766845694bb3', 'message': 'Validate the Tosca VNFD templates\n\nFunctionality has been added for validating VNFD templates\nand raising exceptions if invalid.\nPartial-Bug: #1483953\n\nChange-Id: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n'}, {'number': 3, 'created': '2016-07-05 07:20:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/938d54b56382e90905f9320bcd93f14bee622c0a', 'message': 'Validate the Tosca VNFD templates\n\nFunctionality has been added for validating VNFD templates\nand raising exceptions if invalid.\nPartial-Bug: #1483953\n\nChange-Id: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n'}, {'number': 4, 'created': '2016-07-19 06:49:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/aafd93d9aeac01c933240db9fc68e60e8beaa789', 'message': 'Validate the Tosca VNFD templates\n\nFunctionality has been added for validating VNFD templates\nand raising exceptions if invalid.\nPartial-Bug: #1483953\n\nChange-Id: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n'}, {'number': 5, 'created': '2016-08-01 06:31:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/fb9a0cb021b8132dccf78d0288a19b67798cc025', 'message': 'Validate the Tosca VNFD templates\n\nFunctionality has been added for validating VNFD templates.\nPartial-Bug: #1483953\n\nChange-Id: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n'}, {'number': 6, 'created': '2016-08-09 06:49:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/8647451b87bcd8f8564bf7e2052b009318eaa296', 'message': 'Validate the Tosca VNFD templates\n\nFunctionality has been added for validating VNFD templates.\nPartial-Bug: #1483953\n\nChange-Id: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n'}, {'number': 7, 'created': '2016-08-18 12:06:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/d32f42a19898ea4f89db322bb4744d9010ae1b7b', 'message': 'Validate the Tosca VNFD templates\n\nFunctionality has been added for validating VNFD templates.\nPartial-Bug: #1483953\n\nChange-Id: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n'}, {'number': 8, 'created': '2016-08-30 06:16:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/5b7f41f37c9456399822bbc59dcb6080086809d4', 'message': 'Validate the Tosca VNFD templates\n\nFunctionality has been added for validating VNFD templates.\nPartial-Bug: #1483953\n\nChange-Id: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n'}, {'number': 9, 'created': '2016-09-15 06:51:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/8eeff9156055121afa42354cd9e9d5e9bb6a2d83', 'message': 'Validate the Tosca VNFD templates\n\nFunctionality has been added for validating VNFD templates.\nPartial-Bug: #1483953\n\nChange-Id: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n'}, {'number': 10, 'created': '2016-09-22 06:13:58.000000000', 'files': ['tacker/vnfm/plugin.py', 'tacker/extensions/vnfm.py', 'tacker/api/v1/base.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/98a805e25380dd1c882c0896a7555365a2bb07de', 'message': 'Validate the Tosca VNFD templates\n\nFunctionality has been added for validating VNFD templates.\nPartial-Bug: #1483953\n\nChange-Id: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n'}]",14,333852,98a805e25380dd1c882c0896a7555365a2bb07de,43,9,10,22061,,,0,"Validate the Tosca VNFD templates

Functionality has been added for validating VNFD templates.
Partial-Bug: #1483953

Change-Id: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02
",git fetch https://review.opendev.org/openstack/tacker refs/changes/52/333852/4 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/vm/plugin.py', 'tacker/api/v1/base.py']",2,87f29df5c6aad6cdf13a1c7937162fb4c2f449e2,template-validate," VALIDATE = 'validate' for action in [self.CREATE, self.UPDATE, self.DELETE, self.VALIDATE]: def _to_validate_vnfd(self, body_dict, key): return key in body_dict['vnfd']['attributes'] if self._resource == 'vnfd' and self._to_validate_vnfd( body, 'vnfd-validate-only'): action = self._plugin_handlers[self.VALIDATE] obj_validator = getattr(self._plugin, action) kwargs.update({self._resource: body}) return {self._resource: obj_validator(request.context, **kwargs)} else: action = self._plugin_handlers[self.CREATE]"," for action in [self.CREATE, self.UPDATE, self.DELETE]: action = self._plugin_handlers[self.CREATE]",57,2
openstack%2Fswift~master~Ia6f6760159dc0d537c5ce7dbe59b850a21007ea7,openstack/swift,master,Ia6f6760159dc0d537c5ce7dbe59b850a21007ea7,Refactored test_tempauth account acl tests,NEW,2016-09-14 21:12:50.000000000,2017-12-18 04:46:34.000000000,,"[{'_account_id': 1179}, {'_account_id': 9576}]","[{'number': 1, 'created': '2016-09-14 21:12:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/07ab2e3b1861a1857da0986f99c22189d04be30a', 'message': 'Refactored test_tempauth account acl tests\n\nListed out the different scenarios where success and failures\nare encountered. This makes the tests a bit easier to read.\n\nChange-Id: Ia6f6760159dc0d537c5ce7dbe59b850a21007ea7\n'}, {'number': 2, 'created': '2016-09-14 21:13:23.000000000', 'files': ['test/unit/common/middleware/test_tempauth.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/ad016d6cf9056851103731f97e8fcd4ffe117148', 'message': 'Refactored test_tempauth account acl tests\n\nListed out the different scenarios where success and failures\nare encountered. This makes the tests a bit easier to read.\n\nChange-Id: Ia6f6760159dc0d537c5ce7dbe59b850a21007ea7\n'}]",0,370436,ad016d6cf9056851103731f97e8fcd4ffe117148,6,2,2,9576,,,0,"Refactored test_tempauth account acl tests

Listed out the different scenarios where success and failures
are encountered. This makes the tests a bit easier to read.

Change-Id: Ia6f6760159dc0d537c5ce7dbe59b850a21007ea7
",git fetch https://review.opendev.org/openstack/swift refs/changes/36/370436/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/common/middleware/test_tempauth.py', 'test/unit/common/test_header_key_dict.py']",2,07ab2e3b1861a1857da0986f99c22189d04be30a,tempauthacl-refactor," self.assertIn('Content-Length', headers) self.assertNotIn('Content-Length', headers) def test_pop(self): headers = HeaderKeyDict() headers['content-length'] = 20 headers['cOntent-tYpe'] = 'text/plain' self.assertEqual(headers.pop('content-Length'), '20') self.assertEqual(headers.pop('Content-type'), 'text/plain') self.assertEqual(headers.pop('Something-Else', 'somevalue'), 'somevalue')", self.assertTrue('Content-Length' in headers) self.assertTrue('Content-Length' not in headers),107,95
openstack%2Fironic~master~Ic0a1f7bbdbef7dd74f8e462ccf0d7e774cbfa5c2,openstack/ironic,master,Ic0a1f7bbdbef7dd74f8e462ccf0d7e774cbfa5c2,Add a new DB API get_missing_conductors(),NEW,2016-06-03 03:29:10.000000000,2017-12-18 04:46:32.000000000,,"[{'_account_id': 10118}, {'_account_id': 10239}, {'_account_id': 13295}, {'_account_id': 13362}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 20311}]","[{'number': 1, 'created': '2016-06-03 03:29:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8c988980997a8043659ee8a810d708d784ebe4b5', 'message': 'Add a new db_api get_remains_conductors\n\nget_remains_conductors() will return a list conductors hostnames\nthat were unregistered but still reserved nodes\n\n_check_deploying_status() will call this method to clean lock from\nremains conductors.\n\nChange-Id: Ic0a1f7bbdbef7dd74f8e462ccf0d7e774cbfa5c2\nPartial-Bug: #1587694\n'}, {'number': 2, 'created': '2016-06-03 08:50:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e0db4dffc169ac73f879700da9d2f3f2b5f7ce6b', 'message': 'Add a new DB API get_remains_conductors()\n\nget_remains_conductors() will return a list conductors hostnames\nthat were unregistered but still reserved nodes\n\n_check_deploying_status() will call this method to clean lock from\nremains conductors.\n\nWill add unit tests later\nChange-Id: Ic0a1f7bbdbef7dd74f8e462ccf0d7e774cbfa5c2\nPartial-Bug: #1587694\n'}, {'number': 3, 'created': '2016-07-06 08:09:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3ca761e4bb987ff85780095b8b6d8f9445435dfb', 'message': 'Add a new DB API get_remains_conductors()\n\nget_remains_conductors() will return a list conductors hostnames\nthat were unregistered but still reserved nodes\n\n_check_deploying_status() will call this method to clean lock from\nremains conductors.\n\nChange-Id: Ic0a1f7bbdbef7dd74f8e462ccf0d7e774cbfa5c2\nPartial-Bug: #1587694\n'}, {'number': 4, 'created': '2016-07-08 02:11:15.000000000', 'files': ['ironic/conductor/manager.py', 'ironic/tests/unit/db/test_conductor.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/db/sqlalchemy/api.py', 'ironic/db/api.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/1dba9823ecbc54b2852d44eca92178979e682ec8', 'message': 'Add a new DB API get_missing_conductors()\n\nget_missing_conductors() will return a list conductors hostnames\nthat were unregistered but still reserved nodes\n\n_check_deploying_status() will call this method to clean lock from\nmissing conductors.\n\nChange-Id: Ic0a1f7bbdbef7dd74f8e462ccf0d7e774cbfa5c2\nPartial-Bug: #1587694\n'}]",6,325026,1dba9823ecbc54b2852d44eca92178979e682ec8,32,9,4,13362,,,0,"Add a new DB API get_missing_conductors()

get_missing_conductors() will return a list conductors hostnames
that were unregistered but still reserved nodes

_check_deploying_status() will call this method to clean lock from
missing conductors.

Change-Id: Ic0a1f7bbdbef7dd74f8e462ccf0d7e774cbfa5c2
Partial-Bug: #1587694
",git fetch https://review.opendev.org/openstack/ironic refs/changes/26/325026/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/conductor/manager.py', 'ironic/db/sqlalchemy/api.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/db/api.py']",4,8c988980997a8043659ee8a810d708d784ebe4b5,bug/1587694-3," def get_remains_conductors(self): """"""Get a list remain conductors Get a list conductors hostnames that were unregistered but still reserved nodes. :returns: A list of conductor hostnames """""" @abc.abstractmethod",,36,4
openstack%2Fironic-python-agent~master~Iaea8c8ebe95edd9388b872f7f90e2500adf0492a,openstack/ironic-python-agent,master,Iaea8c8ebe95edd9388b872f7f90e2500adf0492a,Provides convenience for logging metrics data.,NEW,2016-08-31 21:57:06.000000000,2017-12-18 04:46:05.000000000,,"[{'_account_id': 10239}, {'_account_id': 10342}, {'_account_id': 19686}]","[{'number': 1, 'created': '2016-08-31 21:57:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/83bc8f749e511df20e96fdf0996e15c25789b546', 'message': 'Implements: ironic_lib.metrics w/ old metrics sigs.\n\nChange-Id: Iaea8c8ebe95edd9388b872f7f90e2500adf0492a\n'}, {'number': 2, 'created': '2016-09-01 20:16:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/a013bc35c16f65c62f71fe1bb4b064e364db4a44', 'message': 'Partial-bug: #1611553\n\nProvides convenience for logging metrics data. This uses metrics_utils\nfor the metrics log gathering and provides a decorator method for timing\nmethod execution and shorthand versions of a context manager and key\nvalue data logging.\n\nChange-Id: Iaea8c8ebe95edd9388b872f7f90e2500adf0492a\n'}, {'number': 3, 'created': '2016-09-01 20:52:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/7e4253ad130dc929ba15ef0678c61ab1c25a933d', 'message': 'Provides convenience for logging metrics data.\n\nThis uses metrics_utils for the metrics log gathering and provides a\ndecorator method for timing method execution and shorthand versions of a\ncontext manager and key value data logging.\n\nPartial-bug: #1611553\n\nChange-Id: Iaea8c8ebe95edd9388b872f7f90e2500adf0492a\n'}, {'number': 4, 'created': '2016-09-06 19:45:02.000000000', 'files': ['doc/source/metrics.rst', 'ironic_python_agent/metrics.py', 'ironic_python_agent/api/controllers/v1/command.py', 'ironic_python_agent/api/controllers/v1/status.py', 'ironic_python_agent/api/controllers/root.py'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/152f64196bf6cb9773f9fb629f4f46d793bd246d', 'message': 'Provides convenience for logging metrics data.\n\nThis uses metrics_utils for the metrics log gathering and provides a\ndecorator method for timing method execution and shorthand versions of a\ncontext manager and key value data logging.\n\nFixed pep8 errors.\n\nPartial-bug: #1611553\n\nChange-Id: Iaea8c8ebe95edd9388b872f7f90e2500adf0492a\n'}]",7,363959,152f64196bf6cb9773f9fb629f4f46d793bd246d,13,3,4,16255,,,0,"Provides convenience for logging metrics data.

This uses metrics_utils for the metrics log gathering and provides a
decorator method for timing method execution and shorthand versions of a
context manager and key value data logging.

Fixed pep8 errors.

Partial-bug: #1611553

Change-Id: Iaea8c8ebe95edd9388b872f7f90e2500adf0492a
",git fetch https://review.opendev.org/openstack/ironic-python-agent refs/changes/59/363959/3 && git format-patch -1 --stdout FETCH_HEAD,['ironic_python_agent/metrics.py'],1,83bc8f749e511df20e96fdf0996e15c25789b546,bug/1611553,"# Copyright 2015 Rackspace, Inc. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # Metrics module tranlation layer for new metrics compatibality. from contextlib import contextmanager from ironic_lib import metrics_utils def instrument(module_name, method_name=None): """"""Translation layer decorator for new metrics. Post back timer based metrics using new ironic_lib metrics with decorators. """""" def dec(func): def f2(*args, **kwargs): with ( metrics_utils.get_metrics_logger(module_name).timer( method_name or func.__name__) ): return func(*args, **kwargs) return f2 return dec @contextmanager def instrument_context(module_name, method_name): """"""Translation layer for metrics instrument context using new metrics. Will allow use of new metrics without change from previous metrics method signature. """""" with metrics_utils.get_metrics_logger(module_name).timer(method_name): yield ",,47,0
openstack%2Fyaql~master~I344776bdeb9dc800d4a0a33d13273ad1f16fc55c,openstack/yaql,master,I344776bdeb9dc800d4a0a33d13273ad1f16fc55c,[WIP] Add logs to wrong methods usages,NEW,2016-09-28 16:33:26.000000000,2017-12-18 04:45:50.000000000,,"[{'_account_id': 7226}, {'_account_id': 15168}, {'_account_id': 20563}]","[{'number': 1, 'created': '2016-09-28 16:33:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/yaql/commit/bcda79ec45cf5db83b4a0e3483c42df657ab5901', 'message': 'Add logs to function usages\n\nChange-Id: I344776bdeb9dc800d4a0a33d13273ad1f16fc55c\n'}, {'number': 2, 'created': '2016-09-28 16:34:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/yaql/commit/8815ed68e758ab064065fae0274a4842ba9f3a26', 'message': 'Add logs to function usages\n\nChange-Id: I344776bdeb9dc800d4a0a33d13273ad1f16fc55c\n'}, {'number': 3, 'created': '2016-09-29 09:49:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/yaql/commit/27ace0facebc8502e5159f5728df2be6d904f036', 'message': 'Add logs to wrong methods usages\n\nIf there is no suitable function for yaql expression,\nlogs describing which overloads were skipped and why\nshould be presented.\n\nTBD:\nimpove loggs\nadd the way to represent functions\nadd logs to type checkers\n\nCloses-bug: #1443405\n\nChange-Id: I344776bdeb9dc800d4a0a33d13273ad1f16fc55c\n'}, {'number': 4, 'created': '2016-09-30 22:00:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/yaql/commit/c3726b7b62504b6b1811b381fc3ca75d91d8c374', 'message': 'Add logs to wrong methods usages\n\nIf there is no suitable function for yaql expression,\nlogs describing which overloads were skipped and why\nshould be presented.\n\nTBD:\nimpove logs\nimprove logs to type checkers\n\nCloses-bug: #1443405\n\nChange-Id: I344776bdeb9dc800d4a0a33d13273ad1f16fc55c\n'}, {'number': 5, 'created': '2016-10-02 11:51:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/yaql/commit/5bd2853054048cf7d767cb280470169a5f197600', 'message': 'Add logs to wrong methods usages\n\nIf there is no suitable function for yaql expression,\nlogs describing which overloads were skipped and why\nshould be presented.\n\nCloses-bug: #1443405\n\nChange-Id: I344776bdeb9dc800d4a0a33d13273ad1f16fc55c\n'}, {'number': 6, 'created': '2016-10-04 12:14:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/yaql/commit/c9ba601f53b9731a22db09a484870186215590b3', 'message': 'Add logs to wrong methods usages\n\nIf there is no suitable function for yaql expression,\nlogs describing which overloads were skipped and why\nshould be presented.\n\nCloses-bug: #1443405\n\nChange-Id: I344776bdeb9dc800d4a0a33d13273ad1f16fc55c\n'}, {'number': 7, 'created': '2016-10-11 09:33:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/yaql/commit/67c6154918978b4c07926d66d9c4f9f625c6330a', 'message': '[WIP] Add logs to wrong methods usages\n\nThe info why overloads for YAQL-functions are skipped\nwhen no overloads are found should be provided.\n\nTBD:\n1. Put logs exactly to error message or traceback.\n2. Add tests.\n\nCloses-bug: #1443405\n\nChange-Id: I344776bdeb9dc800d4a0a33d13273ad1f16fc55c\n'}, {'number': 8, 'created': '2016-10-18 08:42:25.000000000', 'files': ['yaql/language/specs.py', 'yaql/log.py', 'yaql/language/runner.py', 'yaql/tests/test_functions_logs.py'], 'web_link': 'https://opendev.org/openstack/yaql/commit/8cb0ae75d34a2cbab987bfe517d283f99d2e2794', 'message': '[WIP] Add logs to wrong methods usages\n\nThe info why overloads for YAQL-functions are skipped\nwhen no overloads are found should be provided.\n\nTBD:\n1. Put logs exactly to error message or traceback.\n2. Make printing logs as option.\n3. Add tests.\n\nCloses-bug: #1443405\n\nChange-Id: I344776bdeb9dc800d4a0a33d13273ad1f16fc55c\n'}]",0,378772,8cb0ae75d34a2cbab987bfe517d283f99d2e2794,27,3,8,20563,,,0,"[WIP] Add logs to wrong methods usages

The info why overloads for YAQL-functions are skipped
when no overloads are found should be provided.

TBD:
1. Put logs exactly to error message or traceback.
2. Make printing logs as option.
3. Add tests.

Closes-bug: #1443405

Change-Id: I344776bdeb9dc800d4a0a33d13273ad1f16fc55c
",git fetch https://review.opendev.org/openstack/yaql refs/changes/72/378772/8 && git format-patch -1 --stdout FETCH_HEAD,"['yaql/language/specs.py', 'yaql/language/runner.py']",2,bcda79ec45cf5db83b4a0e3483c42df657ab5901,bug/1443405,"from yaql import log as yaql_log logs = yaql_log.Logger() logs.append(""There are at least two overloads with different"" "" kwargs resolution"") mapping = c.map_args(args, kwargs, context, engine, logs) logs.append(""There are at least two overloads with different"" "" lazy parameters resolution"") logs.append(""No overloads are found after provided skippings"")"," mapping = c.map_args(args, kwargs, context, engine)",37,2
openstack%2Fdiskimage-builder~master~Ic49752de4ac8b2a3bd160d7ab5f68ad6c5c54bc6,openstack/diskimage-builder,master,Ic49752de4ac8b2a3bd160d7ab5f68ad6c5c54bc6,WIP nodepool element,NEW,2016-10-18 22:22:29.000000000,2017-12-18 04:45:45.000000000,,[{'_account_id': 10035}],"[{'number': 1, 'created': '2016-10-18 22:22:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/ce1c6cdc8311beae16cc877d0b434ad0c5530570', 'message': 'WIP nodepool element\n\nChange-Id: Ic49752de4ac8b2a3bd160d7ab5f68ad6c5c54bc6\n'}, {'number': 2, 'created': '2016-10-18 22:26:20.000000000', 'files': ['elements/nodepool/source-repositories', 'elements/nodepool/element-deps', 'elements/nodepool/install.d/50-install-nodepool'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/08c259f771a1db680fbbfeda26427c86247ed521', 'message': 'WIP nodepool element\n\nChange-Id: Ic49752de4ac8b2a3bd160d7ab5f68ad6c5c54bc6\n'}]",0,388241,08c259f771a1db680fbbfeda26427c86247ed521,6,1,2,10035,,,0,"WIP nodepool element

Change-Id: Ic49752de4ac8b2a3bd160d7ab5f68ad6c5c54bc6
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/41/388241/2 && git format-patch -1 --stdout FETCH_HEAD,"['elements/nodepool/source-repositories', 'elements/nodepool/element-deps', 'elements/nodepool/install.d/50-install-nodepool']",3,ce1c6cdc8311beae16cc877d0b434ad0c5530570,,#!/bin/bash pip install /opt/stack/nodepool ,,6,0
openstack%2Frally~master~Ibc5af4d3a3a5aaf95e480f17344888381b21c72e,openstack/rally,master,Ibc5af4d3a3a5aaf95e480f17344888381b21c72e,Move registration of config options to one module,NEW,2016-08-27 15:02:18.000000000,2017-12-18 04:45:35.000000000,,"[{'_account_id': 6172}, {'_account_id': 6536}, {'_account_id': 9545}, {'_account_id': 14817}]","[{'number': 1, 'created': '2016-08-27 15:02:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/187608b019a30199b345269739c9445be006c538', 'message': 'Move registration of config options to one module\n\nChange-Id: Ibc5af4d3a3a5aaf95e480f17344888381b21c72e\nCloses-Bug: 1615700\n'}, {'number': 2, 'created': '2016-09-27 10:26:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a7a7677d34f16b5f872986a64959436223dd18b9', 'message': 'Move registration of config options to one module\n\nChange-Id: Ibc5af4d3a3a5aaf95e480f17344888381b21c72e\nCloses-Bug: 1615700\n'}, {'number': 3, 'created': '2016-10-13 16:01:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/cea82c53da6d5358980cd18aaf1208d9f944e689', 'message': 'Move registration of config options to one module\n\nChange-Id: Ibc5af4d3a3a5aaf95e480f17344888381b21c72e\nCloses-Bug: 1615700\n'}, {'number': 4, 'created': '2016-10-18 11:02:14.000000000', 'files': ['rally/plugins/openstack/scenarios/ec2/utils.py', 'rally/plugins/openstack/scenarios/cinder/utils.py', 'rally/plugins/openstack/scenarios/monasca/utils.py', 'rally/plugins/openstack/scenarios/heat/utils.py', 'rally/plugins/openstack/scenarios/vm/utils.py', 'rally/plugins/openstack/scenarios/glance/utils.py', 'rally/plugins/openstack/scenarios/manila/utils.py', 'rally/common/opts.py', 'rally/verification/tempest/config.py', 'rally/plugins/openstack/scenarios/nova/utils.py', 'rally/plugins/openstack/context/keystone/users.py', 'rally/plugins/openstack/context/keystone/roles.py', 'rally/plugins/openstack/scenarios/murano/utils.py', 'rally/plugins/openstack/scenarios/ironic/utils.py', 'rally/plugins/openstack/scenarios/sahara/utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/01b729e3b46dfb662eb8f2b0eae75c78af6dc1c6', 'message': 'Move registration of config options to one module\n\nChange-Id: Ibc5af4d3a3a5aaf95e480f17344888381b21c72e\nCloses-Bug: 1615700\n'}]",2,361617,01b729e3b46dfb662eb8f2b0eae75c78af6dc1c6,23,4,4,6536,,,0,"Move registration of config options to one module

Change-Id: Ibc5af4d3a3a5aaf95e480f17344888381b21c72e
Closes-Bug: 1615700
",git fetch https://review.opendev.org/openstack/rally refs/changes/17/361617/4 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/openstack/scenarios/ec2/utils.py', 'rally/plugins/openstack/scenarios/cinder/utils.py', 'rally/plugins/openstack/scenarios/monasca/utils.py', 'rally/plugins/openstack/scenarios/heat/utils.py', 'rally/plugins/openstack/scenarios/vm/utils.py', 'rally/plugins/openstack/scenarios/glance/utils.py', 'rally/plugins/openstack/cleanup/base.py', 'rally/plugins/openstack/scenarios/manila/utils.py', 'rally/common/opts.py', 'rally/verification/tempest/config.py', 'rally/plugins/openstack/scenarios/nova/utils.py', 'rally/plugins/openstack/context/keystone/users.py', 'rally/plugins/openstack/context/keystone/roles.py', 'rally/plugins/openstack/scenarios/murano/utils.py', 'rally/plugins/openstack/scenarios/ironic/utils.py', 'rally/plugins/openstack/scenarios/sahara/utils.py']",16,187608b019a30199b345269739c9445be006c538,bug/1615700,"from rally.common import optsCONF.register_opts(opts.SAHARA_BENCHMARK_OPTS, group=benchmark_group)"," SAHARA_BENCHMARK_OPTS = [ cfg.IntOpt(""sahara_cluster_create_timeout"", default=1800, deprecated_name=""cluster_create_timeout"", help=""A timeout in seconds for a cluster create operation""), cfg.IntOpt(""sahara_cluster_delete_timeout"", default=900, deprecated_name=""cluster_delete_timeout"", help=""A timeout in seconds for a cluster delete operation""), cfg.IntOpt(""sahara_cluster_check_interval"", default=5, deprecated_name=""cluster_check_interval"", help=""Cluster status polling interval in seconds""), cfg.IntOpt(""sahara_job_execution_timeout"", default=600, deprecated_name=""job_execution_timeout"", help=""A timeout in seconds for a Job Execution to complete""), cfg.IntOpt(""sahara_job_check_interval"", default=5, deprecated_name=""job_check_interval"", help=""Job Execution status polling interval in seconds""), cfg.IntOpt(""sahara_workers_per_proxy"", default=20, help=""Amount of workers one proxy should serve to."") ] CONF.register_opts(SAHARA_BENCHMARK_OPTS, group=benchmark_group)",409,402
openstack%2Frally~master~I61601b6a1d70b26f320cdee0a543b4a9f4bd34b9,openstack/rally,master,I61601b6a1d70b26f320cdee0a543b4a9f4bd34b9,Add nova.CreateKeypairWithPublicKey,NEW,2016-09-26 08:23:08.000000000,2017-12-18 04:45:26.000000000,,"[{'_account_id': 9545}, {'_account_id': 11869}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 23435}]","[{'number': 1, 'created': '2016-09-26 08:23:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/7973104da9826cbd897a75ce55f3888dc7f32bdc', 'message': 'Add nova.CreateKeypairWithPublicKey\n\nThis scenario creates a keypair with public key\n\nChange-Id: I61601b6a1d70b26f320cdee0a543b4a9f4bd34b9\n'}, {'number': 2, 'created': '2016-09-28 06:18:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/75ca4eb6bdda1f705bd096d7259da8f0caa50ac3', 'message': 'Add nova.CreateKeypairWithPublicKey\n\nThis scenario creates a keypair with public key\n\nChange-Id: I61601b6a1d70b26f320cdee0a543b4a9f4bd34b9\n'}, {'number': 3, 'created': '2016-09-29 02:18:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5d6b06af62c9b433b6462083aff6c53fa75002d2', 'message': 'Add nova.CreateKeypairWithPublicKey\n\nThis scenario creates a keypair with public key\n\nChange-Id: I61601b6a1d70b26f320cdee0a543b4a9f4bd34b9\n'}, {'number': 4, 'created': '2016-10-10 05:29:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/acf7a807edc4b5fe98f58c4ae8a5bd6f51398fbb', 'message': 'Add nova.CreateKeypairWithPublicKey\n\nThis scenario creates a keypair with public key\n\nChange-Id: I61601b6a1d70b26f320cdee0a543b4a9f4bd34b9\n'}, {'number': 5, 'created': '2016-10-11 06:14:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5b0dcdf44bb3c76811fdbcdc1d648f05e97edd96', 'message': 'Add nova.CreateKeypairWithPublicKey\n\nThis scenario creates a keypair with public key\n\nChange-Id: I61601b6a1d70b26f320cdee0a543b4a9f4bd34b9\n'}, {'number': 6, 'created': '2016-10-21 08:33:03.000000000', 'files': ['samples/tasks/scenarios/nova/create-keypair-with-public-key.json', 'rally/plugins/openstack/scenarios/nova/keypairs.py', 'rally-jobs/extra/public_key', 'rally-jobs/nova.yaml', 'samples/tasks/scenarios/nova/create-keypair-with-public-key.yaml', 'tests/unit/plugins/openstack/scenarios/nova/test_keypairs.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/4ffed771661a17b40a833075a8894aef787e405d', 'message': 'Add nova.CreateKeypairWithPublicKey\n\nThis scenario creates a keypair with public key\n\nChange-Id: I61601b6a1d70b26f320cdee0a543b4a9f4bd34b9\n'}]",16,376288,4ffed771661a17b40a833075a8894aef787e405d,35,5,6,23435,,,0,"Add nova.CreateKeypairWithPublicKey

This scenario creates a keypair with public key

Change-Id: I61601b6a1d70b26f320cdee0a543b4a9f4bd34b9
",git fetch https://review.opendev.org/openstack/rally refs/changes/88/376288/4 && git format-patch -1 --stdout FETCH_HEAD,"['samples/tasks/scenarios/nova/create-keypair-with-public-key.json', 'rally/plugins/openstack/scenarios/nova/keypairs.py', 'rally-jobs/nova.yaml', 'samples/tasks/scenarios/nova/create-keypair-with-public-key.yaml', 'tests/unit/plugins/openstack/scenarios/nova/test_keypairs.py']",5,7973104da9826cbd897a75ce55f3888dc7f32bdc,nova.CreateKeypairWithPublicKey," def test_create_keypair_with_public_key(self): scenario = keypairs.CreateKeypairWithPublicKey(self.context) scenario._create_keypair = mock.MagicMock(return_value=""foo_keypair"") scenario.run(""path"", fakearg=""fakearg"") scenario._create_keypair.assert_called_once_with(public_key=""path"", fakearg=""fakearg"") ",,74,0
openstack%2Fsolum~master~Ic3d399c77823a899776e1235ee70991ef04d4d67,openstack/solum,master,Ic3d399c77823a899776e1235ee70991ef04d4d67,Fixed type of tags property,NEW,2016-10-06 10:20:30.000000000,2017-12-18 04:45:18.000000000,,[],"[{'number': 1, 'created': '2016-10-06 10:20:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/solum/commit/68134a4e5eec63d3517f44b5fe1f4bbbf10454fa', 'message': 'Fixed type of tags property\n\nThere was an array expected, for storing the values in JSON we however only\nneed a simple string.\n\nChange-Id: Ic3d399c77823a899776e1235ee70991ef04d4d67\n'}, {'number': 2, 'created': '2016-10-08 18:32:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/solum/commit/9b49cc639aee7e2202e053b3193a82a99c1a97ce', 'message': 'Fixed type of tags property\n\nThere was an array expected, for storing the values in JSON we however only\nneed a simple string.\n\nChange-Id: Ic3d399c77823a899776e1235ee70991ef04d4d67\n'}, {'number': 3, 'created': '2016-10-16 20:44:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/solum/commit/895bb592e688272ff5629d742e6c4b8d62281379', 'message': 'Fixed type of tags property\n\nThere was an array expected, for storing the values in JSON we however only\nneed a simple string.\n\nChange-Id: Ic3d399c77823a899776e1235ee70991ef04d4d67\n'}, {'number': 4, 'created': '2016-10-21 19:15:43.000000000', 'files': ['solum/api/controllers/v1/datamodel/types.py', 'solum/api/handlers/language_pack_handler.py'], 'web_link': 'https://opendev.org/openstack/solum/commit/740dc410d6feb28cee4d031dea17cb05c651dae9', 'message': 'Fixed type of tags property\n\nThere was an array expected, for storing the values in JSON we however only\nneed a simple string.\n\nChange-Id: Ic3d399c77823a899776e1235ee70991ef04d4d67\n'}]",0,382828,740dc410d6feb28cee4d031dea17cb05c651dae9,9,0,4,21886,,,0,"Fixed type of tags property

There was an array expected, for storing the values in JSON we however only
need a simple string.

Change-Id: Ic3d399c77823a899776e1235ee70991ef04d4d67
",git fetch https://review.opendev.org/openstack/solum refs/changes/28/382828/1 && git format-patch -1 --stdout FETCH_HEAD,"['examples/apps/wordpress.yaml', 'examples/language-packs/centos.json', 'solum/api/controllers/v1/datamodel/types.py']",3,68134a4e5eec63d3517f44b5fe1f4bbbf10454fa,build_vm, tags = wtypes.text, tags = [wtypes.text],5,1
openstack%2Fswift~master~I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead,openstack/swift,master,I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead,On demand data migration for Swift,NEW,2013-12-30 11:49:08.000000000,2017-12-18 04:45:04.000000000,,"[{'_account_id': 330}, {'_account_id': 866}, {'_account_id': 1179}, {'_account_id': 2622}, {'_account_id': 6889}, {'_account_id': 6968}, {'_account_id': 7233}, {'_account_id': 7847}, {'_account_id': 8871}, {'_account_id': 9205}, {'_account_id': 9625}, {'_account_id': 13052}]","[{'number': 1, 'created': '2013-12-30 11:49:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c6e3b8495e70c0a66247ac943325ee6bcab06f1c', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate their data\nfrom other storage clouds (or file system) into Swift. This is achieved without\nbeing dependent on any special support from the other clouds.\n(Please see related blueprint and documentation inside modules for more\ninformation and examples.)\n\nIt's obvious that on demand data migration uses network to transfer user's data\nfrom old storage into Swift, an operation that consumes certain time and\nresources. On the other hand, data migration provides various benefits for the\nend users. In particular, users may start easily work with Swift and access\ntheir data through Swift, even if their data is stored on some other storage\nprovider. Users will no longer need to be concerned how to migrate their data\ninto Swift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they don't need to\nwait till the whole data is migrated. Data migration layer will access remote\ncloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be highly\nversatile for different kind of storage clouds or file systems, that users\nmay wish to migrate their data from. This commit includes various access\ndrivers and in addition the suggested modular architecture allows to easily\nadd more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\n\ta. Validates data migration setup by expecting metadata of container\n\n\tb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\n\tContains implementation for access drivers that are used by Data\n\tMigration MW to access remote clouds.\n\n\ta. Migration from file system ( default implementation )\n\n\tb. Migration from storage clouds, using Apache LibCloud (Optional)\n\tTo avoid dependence between Swift and Apache LibCloud, this\n\timplementation is an optional and will only work if LibCloud is\n\tinstalled in Python. It should be noticed that the benefit of LibCloud\n\tis high, since LibCloud comes prepackaged with drivers for Amazon S3,\n\tAtmos, Microsoft Azure, Google storage,Rackspace Cloudfiles,Nimbus.\n\n\tc. Migration from another Swift by using swift-client. Similar to\n\tLibCloud this implementation is an optional.\n\n\td. One can easily implement new driver and add it to the configuration\n\tof Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n1. Data Migration MW let Swift to handle original request without any\nmodifications and activates migration logic only if Swift returned\n'404 Not Found' as a response for GET / HEAD object requests.\n2. Data Migration MW inspects then local container metadata to identify\nwhether local container is linked with another cloud. It then pick up a\ncorrect access driver and sends GET object request to the remote cloud to\nread an object. Upon receiving a response from the remote cloud, object is\nstored locally in Swift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user. Each object will be\nmigrated only once, and all subsequent GET / HEAD requests to the same object\nwill not trigger data migration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was tested to migrate\ndata from Amazon S3 only. For this reason, currently no other clouds supported,\nbut anyone welcome to check data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue print ) is not part\nof this commit. Will be committed separately.\n\n3. Migration with usage of Swift internal client. Due to current particular\nlimitation of Swift internal client – an object is read entirely into memory\nbefore being writing to Swift. A correct solution will be provided later with\nsuggested changes to swift-client.\n\n4. The proposed architecture, keeps read access credentials to the old cloud,\nas part of Swift container's metadata. It's an orthogonal question how to\nprotect this metadata from being easily retrieved. Seems there are various\nsolutions how to keep this metadata private. This middleware does not deal\nwith this.\n\n5. Data migration supports the maximal object size that Swift can handle in\na single request and doesn't yet uses multi part uploads to handle objects\nof more than 4GB size.\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 2, 'created': '2013-12-30 12:44:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/3d6b0eaf7d09738f13d44ba1384c2cdb31839027', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Migration from storage clouds, using Apache LibCloud (Optional)\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client. Similar to\nLibCloud this implementation is an optional.\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned  '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. Migration with usage of Swift internal client. Due to current\nparticular limitation of Swift internal client – an object is\nread entirely into memory before being writing to Swift.\nA correct solution will be provided later with suggested changes\nto swift-client.\n\n4. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n5. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 3, 'created': '2014-01-04 09:11:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/9f69d5d2487d50b491d26aab2096e66d925165f4', 'message': ""On demand data migration for Swift\n\nIn this patch:\n\n1. Data migration MW now uses WSGIContext() to proceed\n   original request.\n2. Data migration MW now registers itself in swift_info,\n   providing information about what drivers were loaded.\n3. Improved some of the documentation.\n4. Data migration MW now uses config_true_value to\n   identify if migration is active or not\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Migration from storage clouds, using Apache LibCloud (Optional)\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client. Similar to\nLibCloud this implementation is an optional.\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. Migration with usage of Swift internal client. Due to current\nparticular limitation of Swift internal client – an object is\nread entirely into memory before being writing to Swift.\nA correct solution will be provided later with suggested changes\nto swift-client.\n\n4. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n5. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 4, 'created': '2014-01-04 18:56:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/cfea55d0a70b74bb1636fe81cbd696d0d6681393', 'message': ""On demand data migration for Swift\n\nIn this patch:\n\n1. Data migration MW now uses WSGIContext() to proceed\n   original request.\n2. Data migration MW now registers itself in swift_info,\n   providing information about what drivers were loaded.\n3. Improved some of the documentation.\n4. Data migration MW now uses config_true_value to\n   identify if migration is active or not\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Migration from storage clouds, using Apache LibCloud (Optional)\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client. Similar to\nLibCloud this implementation is an optional.\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. Migration with usage of Swift internal client. Due to current\nparticular limitation of Swift internal client – an object is\nread entirely into memory before being writing to Swift.\nA correct solution will be provided later with suggested changes\nto swift-client.\n\n4. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n5. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 5, 'created': '2014-01-04 20:08:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/94be0c854c189f22f9f1debf0f4a022c09fc3f40', 'message': ""On demand data migration for Swift\n\nIn this patch:\n\n1. Data migration MW now uses WSGIContext() to proceed\n   original request.\n2. Data migration MW now registers itself in swift_info,\n   providing information about what drivers were loaded.\n3. Improved some of the documentation.\n4. Data migration MW now uses config_true_value to\n   identify if migration is active or not\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Migration from storage clouds, using Apache LibCloud (Optional)\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client. Similar to\nLibCloud this implementation is an optional.\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. Migration with usage of Swift internal client. Due to current\nparticular limitation of Swift internal client – an object is\nread entirely into memory before being writing to Swift.\nA correct solution will be provided later with suggested changes\nto swift-client.\n\n4. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n5. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 6, 'created': '2014-01-07 09:36:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/67e60e80374a389e8ccbcc754f26825ede168d3b', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Migration from storage clouds, using Apache LibCloud (Optional)\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client. Similar to\nLibCloud this implementation is an optional.\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. Migration with usage of Swift internal client. Due to current\nparticular limitation of Swift internal client – an object is\nread entirely into memory before being writing to Swift.\nA correct solution will be provided later with suggested changes\nto swift-client.\n\n4. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n5. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom an Amazon S3 bucket:\nhttps://gist.github.com/gilv/b69a151ff0266c18404b\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 7, 'created': '2014-02-04 07:28:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/3030daeb26acef68762917353ce1d699f28237d5', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Migration from storage clouds, using Apache LibCloud (Optional)\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client. Similar to\nLibCloud this implementation is an optional.\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. Migration with usage of Swift internal client. Due to current\nparticular limitation of Swift internal client – an object is\nread entirely into memory before being writing to Swift.\nA correct solution will be provided later with suggested changes\nto swift-client.\n\n4. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n5. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom an Amazon S3 bucket:\nhttps://gist.github.com/gilv/b69a151ff0266c18404b\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 8, 'created': '2014-02-12 18:57:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/31b4aaeaebcc8bcec9eed67a0bb65352f55f9467', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Migration from storage clouds, using Apache LibCloud (Optional)\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client. Similar to\nLibCloud this implementation is an optional.\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. Migration with usage of Swift internal client. Due to current\nparticular limitation of Swift internal client – an object is\nread entirely into memory before being writing to Swift.\nA correct solution will be provided later with suggested changes\nto swift-client.\n\n4. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n5. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom an Amazon S3 bucket:\nhttps://gist.github.com/gilv/b69a151ff0266c18404b\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 9, 'created': '2014-02-16 08:56:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c1d8d61b8b94ab114e201962725983ebce85ea9e', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Migration from storage clouds, using Apache LibCloud (Optional)\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client. Similar to\nLibCloud this implementation is an optional.\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. Migration with usage of Swift internal client. Due to current\nparticular limitation of Swift internal client – an object is\nread entirely into memory before being writing to Swift.\nA correct solution will be provided later with suggested changes\nto swift-client.\n\n4. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n5. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom an Amazon S3 bucket:\nhttps://gist.github.com/gilv/b69a151ff0266c18404b\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 10, 'created': '2014-02-16 10:47:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/b71e48c2fe960f88096abe2dc6c99a8e663d926d', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Migration from storage clouds, using Apache LibCloud (Optional)\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client. Similar to\nLibCloud this implementation is an optional.\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. Migration with usage of Swift internal client. Due to current\nparticular limitation of Swift internal client – an object is\nread entirely into memory before being writing to Swift.\nA correct solution will be provided later with suggested changes\nto swift-client.\n\n4. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n5. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom an Amazon S3 bucket:\nhttps://gist.github.com/gilv/b69a151ff0266c18404b\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 11, 'created': '2014-02-25 07:22:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/fdde2c7927691c6ee1a720e10df86e0da70bfc37', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Migration from storage clouds, using Apache LibCloud (Optional)\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client. Similar to\nLibCloud this implementation is an optional.\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n4. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom an Amazon S3 bucket:\nhttps://gist.github.com/gilv/b69a151ff0266c18404b\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 12, 'created': '2014-02-25 08:15:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/219e29e5c9dac4e12faf4c15d204b5da4a5d1b09', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Migration from storage clouds, using Apache LibCloud (Optional)\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client. Similar to\nLibCloud this implementation is an optional.\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n4. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom an Amazon S3 bucket:\nhttps://gist.github.com/gilv/b69a151ff0266c18404b\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 13, 'created': '2014-07-10 15:11:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/38bf06b8de46421318c3d119faa56b03689d0d70', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Migration from storage clouds, using Apache LibCloud (Optional)\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client. Similar to\nLibCloud this implementation is an optional.\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n4. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom an Amazon S3 bucket:\nhttps://gist.github.com/gilv/b69a151ff0266c18404b\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 14, 'created': '2014-07-31 07:00:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/cd8a0d5ed1e0685111396beaf3d693e136bc5997', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n4. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 15, 'created': '2014-08-07 04:51:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/09ebb48bd4682e5b5a7b69f99c9050bc7faa5a0f', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n4. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 16, 'created': '2014-08-07 05:50:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1e5ddde5fc3bc514d59e66a5c43592cbb502143c', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Migration with usage of Apache LibCloud. Current code was\ntested to migrate data from Amazon S3 only. For this reason,\ncurrently no other clouds supported, but anyone welcome to\ncheck data migration from other storage clouds.\n\n2. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n3. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's metadata. It's an\northogonal question how to protect this metadata from being\neasily retrieved. Seems there are various solutions how to keep\nthis metadata private. This middleware does not deal\nwith this.\n\n4. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 17, 'created': '2014-08-24 07:26:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/610f4ddaace7ac8c85eb6ac427b10ae7730c9fa9', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nOur model assumes that the new cloud (Swift) is the most updated one.\nIf object was migrated, it will not be migrated again. The same, if\nobject was migrated and deleted, it will not be migrated again.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n2. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's system metadata.\nAll keys related credentials are filtered and are not returned\nback to the user. Additional patches may improve the security\nof credentials by using encryption.\n\n3. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nDocImpact\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 18, 'created': '2014-08-27 07:29:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/97b2e67ebf70f20b8fd5f4883840872a3cc0d8f1', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nOur model assumes that the new cloud (Swift) is the most updated one.\nIf object was migrated, it will not be migrated again. The same, if\nobject was migrated and deleted, it will not be migrated again.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n2. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's system metadata.\nAll keys related credentials are filtered and are not returned\nback to the user. Additional patches may improve the security\nof credentials by using encryption.\n\n3. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nDocImpact\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 19, 'created': '2014-08-27 15:50:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c7c0f380c6ec8a5ceecdd6044fbcd39b24d7488a', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nOur model assumes that the new cloud (Swift) is the most updated one.\nIf object was migrated, it will not be migrated again. The same, if\nobject was migrated and deleted, it will not be migrated again.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n2. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's system metadata.\nAll keys related credentials are filtered and are not returned\nback to the user. Additional patches may improve the security\nof credentials by using encryption.\n\n3. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nDocImpact\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 20, 'created': '2014-08-31 04:07:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/d20cd4f09c40fd195da3e94aad079e0baa9fb2e4', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nOur model assumes that the new cloud (Swift) is the most updated one.\nIf object was migrated, it will not be migrated again. The same, if\nobject was migrated and deleted, it will not be migrated again.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n2. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's system metadata.\nAll keys related credentials are filtered and are not returned\nback to the user. Additional patches may improve the security\nof credentials by using encryption.\n\n3. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nDocImpact\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 21, 'created': '2014-09-11 06:12:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f54f7ff4b834e9e893d701a9f7a9217096bad8ae', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nOur model assumes that the new cloud (Swift) is the most updated one.\nIf object was migrated, it will not be migrated again. The same, if\nobject was migrated and deleted, it will not be migrated again.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n2. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's system metadata.\nAll keys related credentials are filtered and are not returned\nback to the user. Additional patches may improve the security\nof credentials by using encryption.\n\n3. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nDocImpact\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 22, 'created': '2014-09-13 18:30:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/63a19a6152b5eae20641d541aa9fa1c7b9977208', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nOur model assumes that the new cloud (Swift) is the most updated one.\nIf object was migrated, it will not be migrated again. The same, if\nobject was migrated and deleted, it will not be migrated again.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n2. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's system metadata.\nAll keys related credentials are filtered and are not returned\nback to the user. Additional patches may improve the security\nof credentials by using encryption.\n\n3. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nDocImpact\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 23, 'created': '2014-09-21 07:47:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4a4004287583df78997ec351b17896fc611d9fca', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nOur model assumes that the new cloud (Swift) is the most updated one.\nIf object was migrated, it will not be migrated again. The same, if\nobject was migrated and deleted, it will not be migrated again.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n2. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's system metadata.\nAll keys related credentials are filtered and are not returned\nback to the user. Additional patches may improve the security\nof credentials by using encryption.\n\n3. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nCo-Authored-By: Thiago da Silva <thiago@redhat.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hp.com>\n\nDocImpact\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 24, 'created': '2014-09-23 06:03:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f8af425febd8a059f0da2262e4446f3be0ece9b3', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nOur model assumes that the new cloud (Swift) is the most updated one.\nIf object was migrated, it will not be migrated again. The same, if\nobject was migrated and deleted, it will not be migrated again.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n2. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's system metadata.\nAll keys related credentials are filtered and are not returned\nback to the user. Additional patches may improve the security\nof credentials by using encryption.\n\n3. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nCo-Authored-By: Thiago da Silva <thiago@redhat.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hp.com>\n\nDocImpact\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 25, 'created': '2014-09-24 12:35:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/9bae8dc71b3a3b62935eda25ee218c8e90538eff', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nOur model assumes that the new cloud (Swift) is the most updated one.\nIf object was migrated, it will not be migrated again. The same, if\nobject was migrated and deleted, it will not be migrated again.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n2. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's system metadata.\nAll keys related credentials are filtered and are not returned\nback to the user. Additional patches may improve the security\nof credentials by using encryption.\n\n3. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nCo-Authored-By: Thiago da Silva <thiago@redhat.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hp.com>\n\nDocImpact\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 26, 'created': '2014-09-24 12:46:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/111223d0ac9553bccf1be6529870cb622894f545', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nOur model assumes that the new cloud (Swift) is the most updated one.\nIf object was migrated, it will not be migrated again. The same, if\nobject was migrated and deleted, it will not be migrated again.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n2. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's system metadata.\nAll keys related credentials are filtered and are not returned\nback to the user. Additional patches may improve the security\nof credentials by using encryption.\n\n3. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nCo-Authored-By: Thiago da Silva <thiago@redhat.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hp.com>\n\nDocImpact\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}, {'number': 27, 'created': '2014-09-24 13:27:18.000000000', 'files': ['swift/common/data_migration_common.py', 'swift/common/middleware/data_migration.py', 'etc/proxy-server.conf-sample', 'swift/common/data_migrator_drivers.py', 'test/unit/common/middleware/test_data_migration.py', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/swift/commit/52eeff33144174bbd51593163c5391ce48e08473', 'message': ""On demand data migration for Swift\n\nOn demand data migration allows customers a natural way to migrate\ntheir data from other storage clouds (or file system) into Swift.\nThis is achieved without being dependent on any special support\nfrom the other clouds.\n\nIt's obvious that on demand data migration uses network to transfer\nuser's data from old storage into Swift, an operation that consumes\ncertain time and resources. On the other hand, data migration\nprovides various benefits for the end users. In particular, users\nmay start easily work with Swift and access their data through Swift\neven if their data is stored on some other storage provider. Users\nwill no longer need to be concerned how to migrate their data into\nSwift. In addition on demand data migration provides end users a\ncontinuous access to their data during data migration, since they\ndon't need to wait till the whole data is migrated. Data migration\nlayer will access remote cloud only once per single object.\n\nAn important aspect of proposed functionality is the need to be\nhighly versatile for different kind of storage clouds or file\nsystems, that users may wish to migrate their data from. This commit\nincludes various access drivers and in addition the suggested\nmodular architecture allows to easily add more drivers.\n\nOur model assumes that the new cloud (Swift) is the most updated one.\nIf object was migrated, it will not be migrated again. The same, if\nobject was migrated and deleted, it will not be migrated again.\n\nIn this commit\n--------------\n1. Data Migration Middleware. (data_migration.py)\n\na. Validates data migration setup by expecting metadata of container\n\nb. Object migration on demand.\n\n2. Storage Access Drivers (data_migrator_drivers.py)\nContains implementation for access drivers that are used by Data\nMigration MW to access remote clouds.\n\na. Migration from file system ( default implementation )\n\nb. Additional patch will contain migration from storage clouds,\nusing Apache LibCloud (Optional).\nTo avoid dependence between Swift and Apache LibCloud, this\nimplementation is an optional and will only work if LibCloud is\ninstalled in Python. It should be noticed that the benefit of\nLibCloud is high, since LibCloud comes prepackaged with drivers\nfor Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace\nCloudfiles,Nimbus.\n\nc. Migration from another Swift by using swift-client.( optional)\n\nd. One can easily implement new driver and add it to the\nconfiguration of Data Migration MW.\n\nThe general flow of object migration on demand as follows:\n---------------------------------------------------------\n1. Data Migration MW let Swift to handle original request without\nany modifications and activates migration logic only if Swift\nreturned '404 Not Found' as a response for GET / HEAD object\nrequests.\n2. Data Migration MW inspects then local container metadata to\nidentify whether local container is linked with another cloud.\nIt then pick up a correct access driver and sends GET object\nrequest to the remote cloud to read an object. Upon receiving\na response from the remote cloud, object is stored locally in\nSwift ( without reading an entire object into memory ) and\nthen it returns a response with object's data to the user.\nEach object will be migrated only once, and all subsequent\nGET / HEAD requests to the same object will not trigger data\nmigration anymore.\n\nCurrent limitations\n-------------------\n1. Due to code complexity, unified view ( described in blue\nprint ) is not part of this commit. Will be committed separately.\n\n2. The proposed architecture, keeps read access credentials to\nthe old cloud, as part of Swift container's system metadata.\nAll keys related credentials are filtered and are not returned\nback to the user. Additional patches may improve the security\nof credentials by using encryption.\n\n3. Data migration supports the maximal object size that Swift\ncan handle in a single request and doesn't yet uses multi part\nuploads to handle objects of more than 4GB size.\n\nAn example code how to use this feature to migrate data\nfrom file system:\nhttps://gist.github.com/gilv/64d43be25d0b4cb8ef29\n\nCo-Authored-By: Thiago da Silva <thiago@redhat.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hp.com>\n\nDocImpact\nChange-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead\nImplements: blueprint on-demand-data-migration-for-swift\n""}]",173,64430,52eeff33144174bbd51593163c5391ce48e08473,201,12,27,9205,,,0,"On demand data migration for Swift

On demand data migration allows customers a natural way to migrate
their data from other storage clouds (or file system) into Swift.
This is achieved without being dependent on any special support
from the other clouds.

It's obvious that on demand data migration uses network to transfer
user's data from old storage into Swift, an operation that consumes
certain time and resources. On the other hand, data migration
provides various benefits for the end users. In particular, users
may start easily work with Swift and access their data through Swift
even if their data is stored on some other storage provider. Users
will no longer need to be concerned how to migrate their data into
Swift. In addition on demand data migration provides end users a
continuous access to their data during data migration, since they
don't need to wait till the whole data is migrated. Data migration
layer will access remote cloud only once per single object.

An important aspect of proposed functionality is the need to be
highly versatile for different kind of storage clouds or file
systems, that users may wish to migrate their data from. This commit
includes various access drivers and in addition the suggested
modular architecture allows to easily add more drivers.

Our model assumes that the new cloud (Swift) is the most updated one.
If object was migrated, it will not be migrated again. The same, if
object was migrated and deleted, it will not be migrated again.

In this commit
--------------
1. Data Migration Middleware. (data_migration.py)

a. Validates data migration setup by expecting metadata of container

b. Object migration on demand.

2. Storage Access Drivers (data_migrator_drivers.py)
Contains implementation for access drivers that are used by Data
Migration MW to access remote clouds.

a. Migration from file system ( default implementation )

b. Additional patch will contain migration from storage clouds,
using Apache LibCloud (Optional).
To avoid dependence between Swift and Apache LibCloud, this
implementation is an optional and will only work if LibCloud is
installed in Python. It should be noticed that the benefit of
LibCloud is high, since LibCloud comes prepackaged with drivers
for Amazon S3,Atmos, Microsoft Azure, Google storage,Rackspace
Cloudfiles,Nimbus.

c. Migration from another Swift by using swift-client.( optional)

d. One can easily implement new driver and add it to the
configuration of Data Migration MW.

The general flow of object migration on demand as follows:
---------------------------------------------------------
1. Data Migration MW let Swift to handle original request without
any modifications and activates migration logic only if Swift
returned '404 Not Found' as a response for GET / HEAD object
requests.
2. Data Migration MW inspects then local container metadata to
identify whether local container is linked with another cloud.
It then pick up a correct access driver and sends GET object
request to the remote cloud to read an object. Upon receiving
a response from the remote cloud, object is stored locally in
Swift ( without reading an entire object into memory ) and
then it returns a response with object's data to the user.
Each object will be migrated only once, and all subsequent
GET / HEAD requests to the same object will not trigger data
migration anymore.

Current limitations
-------------------
1. Due to code complexity, unified view ( described in blue
print ) is not part of this commit. Will be committed separately.

2. The proposed architecture, keeps read access credentials to
the old cloud, as part of Swift container's system metadata.
All keys related credentials are filtered and are not returned
back to the user. Additional patches may improve the security
of credentials by using encryption.

3. Data migration supports the maximal object size that Swift
can handle in a single request and doesn't yet uses multi part
uploads to handle objects of more than 4GB size.

An example code how to use this feature to migrate data
from file system:
https://gist.github.com/gilv/64d43be25d0b4cb8ef29

Co-Authored-By: Thiago da Silva <thiago@redhat.com>
Co-Authored-By: Alistair Coles <alistair.coles@hp.com>

DocImpact
Change-Id: I3c82f8c0e7eafa3fcfc4385c9a240b14bc766ead
Implements: blueprint on-demand-data-migration-for-swift
",git fetch https://review.opendev.org/openstack/swift refs/changes/30/64430/24 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/middleware/data_migration.py', 'etc/proxy-server.conf-sample', 'swift/common/data_migrator_drivers.py', 'test/unit/common/middleware/test_data_migration.py', 'setup.cfg']",5,c6e3b8495e70c0a66247ac943325ee6bcab06f1c,bp/on-demand-data-migration-for-swift, data_migration = swift.common.middleware.data_migration:filter_factory,,934,0
openstack%2Frally~master~I201ff25d3e0e43ae149111514561c593e59460c1,openstack/rally,master,I201ff25d3e0e43ae149111514561c593e59460c1,Remove deprecated arguments from code,NEW,2016-10-06 11:59:54.000000000,2017-12-18 04:44:54.000000000,,"[{'_account_id': 9545}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 23094}]","[{'number': 1, 'created': '2016-10-06 11:59:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1822d241fce9bd39a9642e3384570971aac9523e', 'message': 'Remove deprecated arguments from code\n\nFollowing files were affected:\nrally/plugins/openstack/scenarios/cinder/volumes.py\nrally/plugins/openstack/scenarios/nova/keypairs.py\n\nChange-Id: I201ff25d3e0e43ae149111514561c593e59460c1\n'}, {'number': 2, 'created': '2016-10-06 12:29:07.000000000', 'files': ['rally/plugins/openstack/scenarios/nova/keypairs.py', 'rally/plugins/openstack/scenarios/cinder/volumes.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/927b09e3335a4f87105da6f16e01a57dcda9140e', 'message': 'Remove deprecated arguments from code\n\nFollowing files were affected:\nrally/plugins/openstack/scenarios/cinder/volumes.py\nrally/plugins/openstack/scenarios/nova/keypairs.py\n\nChange-Id: I201ff25d3e0e43ae149111514561c593e59460c1\n'}]",7,382881,927b09e3335a4f87105da6f16e01a57dcda9140e,19,4,2,23094,,,0,"Remove deprecated arguments from code

Following files were affected:
rally/plugins/openstack/scenarios/cinder/volumes.py
rally/plugins/openstack/scenarios/nova/keypairs.py

Change-Id: I201ff25d3e0e43ae149111514561c593e59460c1
",git fetch https://review.opendev.org/openstack/rally refs/changes/81/382881/2 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/openstack/scenarios/nova/keypairs.py', 'rally/plugins/openstack/scenarios/cinder/volumes.py']",2,1822d241fce9bd39a9642e3384570971aac9523e,astarove-deprecated-things," def run(self, size, image, flavor, create_volume_params=None, **kwargs): create_vm_params = kwargs or {} ""0.5"", [""kwargs""], once=True)"," @logging.log_deprecated_args( ""Use 'create_vm_params' for additional instance parameters."", ""0.2.0"", [""kwargs""], once=True) def run(self, size, image, flavor, create_volume_params=None, create_vm_params=None, **kwargs): :param create_vm_params: optional arguments for VM creation if kwargs and create_vm_params: raise ValueError(""You can not set both 'kwargs'"" ""and 'create_vm_params' attributes."" ""Please use 'create_vm_params'."") create_vm_params = create_vm_params or kwargs or {} ""0.4.1"", [""kwargs""], once=True)",4,19
openstack%2Fswift~master~Iaa11aced47c1adc14bde0988e5b693d33d2bbeaf,openstack/swift,master,Iaa11aced47c1adc14bde0988e5b693d33d2bbeaf,Fix reclaimable PUT racing .durable/.data cleanup,NEW,2016-03-08 07:12:29.000000000,2017-12-18 04:44:37.000000000,,"[{'_account_id': 330}, {'_account_id': 4608}, {'_account_id': 7847}, {'_account_id': 13052}]","[{'number': 1, 'created': '2016-03-08 07:12:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a2f7fa17a4a723abeea36fb0818710c552c91b26', 'message': ""Fix reclaimable PUT racing .durable/.data cleanup\n\nDurable fragment set (which has both .data and .durable) can be\nreclaimed (deleted in the worst case) in racing condition.\n\nFor example, if all PUTs are in older than reclaime age (i.e. request\nX-Timestamp << (now - reclaime_age), e.g. reconstruction PUT)\n\nThe following sequence *may* happen in the older code:\n\n===============================================================================\n2 nodes have different timestamp object each other with same path\n(overwritten), and then,\n\n1. a reconstructor (t0) attempts a PUT and object-server make a\n   t0.data file\n2: another reconstructor (t1) attempts a PUT and object-server makes a\n   t1.data file\n\nlikely:\n[a reconstructor] -(PUT t0)-> [object-sever] <-(PUT t1)-[another reconstructor]\n\nAnd then,\n\n3. object-server makes a t0.durable and call cleanup_list_dir which triggers to\n   remove t1.data because it doesn't have .durable and older than reclaim age.\n\n4. After that, object-server makes a t1.durable and call cleanup_list_dir will\n   remove all .data .durable file older than the newest t1.durable.\n\n5. finally all .data files are gone, unexpectedly.\n\n===============================================================================\n\nThis patch prevents such an unexpected racing cleanup. With this code,\nobject-server at the state 4 (above) will search the newest .durable file\nwhich has at least 1 .data file before cleanup and then keep the durable\nfrag set as the most recent durable fragment set even if newer durable\n(but no .data files) files are found in the device.\n\nCloses-Bug: #1554378\n\nChange-Id: Iaa11aced47c1adc14bde0988e5b693d33d2bbeaf\n""}, {'number': 2, 'created': '2016-03-08 10:33:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f61df9550b3e084bec6d8ac9725b908a89da5606', 'message': ""Fix reclaimable PUT racing .durable/.data cleanup\n\nDurable fragment set (which has both .data and .durable) can be\nreclaimed (deleted in the worst case) in racing condition.\n\nFor example, if all PUTs are in older than reclaime age (i.e. request\nX-Timestamp << (now - reclaime_age), e.g. reconstruction PUT)\n\nThe following sequence *may* happen in the older code:\n\n===============================================================================\n2 nodes have different timestamp object each other with same path\n(overwritten), and then,\n\n1. a reconstructor (t0) attempts a PUT and object-server make a\n   t0.data file\n2: another reconstructor (t1) attempts a PUT and object-server makes a\n   t1.data file\n\nlikely:\n[a reconstructor] -(PUT t0)-> [object-sever] <-(PUT t1)-[another reconstructor]\n\nAnd then,\n\n3. object-server makes a t0.durable and call cleanup_list_dir which triggers to\n   remove t1.data because it doesn't have .durable and older than reclaim age.\n\n4. After that, object-server makes a t1.durable and call cleanup_list_dir will\n   remove all .data .durable file older than the newest t1.durable.\n\n5. finally all .data files are gone, unexpectedly.\n\n===============================================================================\n\nThis patch prevents such an unexpected racing cleanup. With this code,\nobject-server at the state 4 (above) will search the newest .durable file\nwhich has at least 1 .data file before cleanup and then keep the durable\nfrag set as the most recent durable fragment set even if newer durable\n(but no .data files) files are found in the device. The orphaned durable\nwill be reclaimed expecting to be reconstructed again.\n\nCloses-Bug: #1554378\n\nChange-Id: Iaa11aced47c1adc14bde0988e5b693d33d2bbeaf\n""}, {'number': 3, 'created': '2016-03-10 01:43:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2acd94d63e127364800a3e62a694fe731d2400a6', 'message': ""Fix reclaimable PUT racing .durable/.data cleanup\n\nDurable fragment set (which has both .data and .durable) can be\nreclaimed (deleted in the worst case) in racing condition.\n\nFor example, if all PUTs are in older than reclaime age (i.e. request\nX-Timestamp << (now - reclaime_age), e.g. reconstruction PUT)\n\nThe following sequence *may* happen in the older code:\n\n===============================================================================\n2 nodes have different timestamp object each other with same path\n(overwritten), and then,\n\n1. a reconstructor (t0) attempts a PUT and object-server make a\n   t0.data file\n2: another reconstructor (t1) attempts a PUT and object-server makes a\n   t1.data file\n\nlikely:\n[a reconstructor] -(PUT t0)-> [object-sever] <-(PUT t1)-[another reconstructor]\n\nAnd then,\n\n3. object-server makes a t0.durable and call cleanup_list_dir which triggers to\n   remove t1.data because it doesn't have .durable and older than reclaim age.\n\n4. After that, object-server makes a t1.durable and call cleanup_list_dir will\n   remove all .data .durable file older than the newest t1.durable.\n\n5. finally all .data files are gone, unexpectedly.\n\n===============================================================================\n\nThis patch prevents such an unexpected racing cleanup. With this code,\nobject-server at the state 4 (above) will search the newest .durable file\nwhich has at least 1 .data file before cleanup and then keep the durable\nfrag set as the most recent durable fragment set even if newer durable\n(but no .data files) files are found in the device. The orphaned durable\nwill be reclaimed expecting to be reconstructed again.\n\nCloses-Bug: #1554378\n\nChange-Id: Iaa11aced47c1adc14bde0988e5b693d33d2bbeaf\n""}, {'number': 4, 'created': '2016-03-10 02:55:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/68c08e3774513c23bc61a9f6ceda7e4d9029a942', 'message': ""Fix reclaimable PUT racing .durable/.data cleanup\n\nDurable fragment set (which has both .data and .durable) can be\nreclaimed (deleted in the worst case) in racing condition.\n\nFor example, if all PUTs are in older than reclaime age (i.e. request\nX-Timestamp << (now - reclaime_age), e.g. reconstruction PUT)\n\nThe following sequence *may* happen in the older code:\n\n===============================================================================\n2 nodes have different timestamp object each other with same path\n(overwritten), and then,\n\n1. a reconstructor (t0) attempts a PUT and object-server make a\n   t0.data file\n2: another reconstructor (t1) attempts a PUT and object-server makes a\n   t1.data file\n\nlikely:\n[a reconstructor] -(PUT t0)-> [object-sever] <-(PUT t1)-[another reconstructor]\n\nAnd then,\n\n3. object-server makes a t0.durable and call cleanup_list_dir which triggers to\n   remove t1.data because it doesn't have .durable and older than reclaim age.\n\n4. After that, object-server makes a t1.durable and call cleanup_list_dir will\n   remove all .data .durable file older than the newest t1.durable.\n\n5. finally all .data files are gone, unexpectedly.\n\n===============================================================================\n\nThis patch prevents such an unexpected racing cleanup. With this code,\nobject-server at the state 4 (above) will search the newest .durable file\nwhich has at least 1 .data file before cleanup and then keep the durable\nfrag set as the most recent durable fragment set even if newer durable\n(but no .data files) files are found in the device. The orphaned durable\nwill be reclaimed expecting to be reconstructed again.\n\nCloses-Bug: #1554378\n\nChange-Id: Iaa11aced47c1adc14bde0988e5b693d33d2bbeaf\n""}, {'number': 5, 'created': '2016-03-14 08:13:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7e84cfe5dc4c887c01780ae46d4bcce7d04500c7', 'message': ""Fix reclaimable PUT racing .durable/.data cleanup\n\nDurable fragment set (which has both .data and .durable) can be\nreclaimed (deleted in the worst case) in racing condition.\n\nFor example, if all PUTs are in older than reclaime age (i.e. request\nX-Timestamp << (now - reclaime_age), e.g. reconstruction PUT)\n\nThe following sequence *may* happen in the older code:\n\n===============================================================================\n2 nodes have different timestamp object each other with same path\n(overwritten), and then,\n\n1. a reconstructor (t0) attempts a PUT and object-server make a\n   t0.data file\n2: another reconstructor (t1) attempts a PUT and object-server makes a\n   t1.data file\n\nlikely:\n[a reconstructor] -(PUT t0)-> [object-sever] <-(PUT t1)-[another reconstructor]\n\nAnd then,\n\n3. object-server makes a t0.durable and call cleanup_list_dir which triggers to\n   remove t1.data because it doesn't have .durable and older than reclaim age.\n\n4. After that, object-server makes a t1.durable and call cleanup_list_dir will\n   remove all .data .durable file older than the newest t1.durable.\n\n5. finally all .data files are gone, unexpectedly.\n\n===============================================================================\n\nThis patch prevents such an unexpected racing cleanup. With this code,\nobject-server at the state 4 (above) will search the newest .durable file\nwhich has at least 1 .data file before cleanup and then keep the durable\nfrag set as the most recent durable fragment set even if newer durable\n(but no .data files) files are found in the device. The orphaned durable\nwill be reclaimed expecting to be reconstructed again.\n\nCloses-Bug: #1554378\n\nChange-Id: Iaa11aced47c1adc14bde0988e5b693d33d2bbeaf\n""}, {'number': 6, 'created': '2016-03-14 08:22:38.000000000', 'files': ['test/unit/obj/test_diskfile.py', 'swift/obj/diskfile.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/79df0291f45f8b1397e3ea5823566f425926407b', 'message': ""Fix reclaimable PUT racing .durable/.data cleanup\n\nDurable fragment set (which has both .data and .durable) can be\nreclaimed (deleted in the worst case) in racing condition.\n\nFor example, if all PUTs are in older than reclaime age (i.e. request\nX-Timestamp << (now - reclaime_age), e.g. reconstruction PUT)\n\nThe following sequence *may* happen in the older code:\n\n===============================================================================\n2 nodes have different timestamp object each other with same path\n(overwritten), and then,\n\n1. a reconstructor (t0) attempts a PUT and object-server make a\n   t0.data file\n2: another reconstructor (t1) attempts a PUT and object-server makes a\n   t1.data file\n\nlikely:\n[a reconstructor] -(PUT t0)-> [object-sever] <-(PUT t1)-[another reconstructor]\n\nAnd then,\n\n3. object-server makes a t0.durable and call cleanup_list_dir which triggers to\n   remove t1.data because it doesn't have .durable and older than reclaim age.\n\n4. After that, object-server makes a t1.durable and call cleanup_list_dir will\n   remove all .data .durable file older than the newest t1.durable.\n\n5. finally all .data files are gone, unexpectedly.\n\n===============================================================================\n\nThis patch prevents such an unexpected racing cleanup. With this code,\nobject-server at the state 4 (above) will search the newest .durable file\nwhich has at least 1 .data file before cleanup and then keep the durable\nfrag set as the most recent durable fragment set even if newer durable\n(but no .data files) files are found in the device. The orphaned durable\nwill be reclaimed expecting to be reconstructed again.\n\nCloses-Bug: #1554378\n\nChange-Id: Iaa11aced47c1adc14bde0988e5b693d33d2bbeaf\n""}]",4,289756,79df0291f45f8b1397e3ea5823566f425926407b,31,4,6,4608,,,0,"Fix reclaimable PUT racing .durable/.data cleanup

Durable fragment set (which has both .data and .durable) can be
reclaimed (deleted in the worst case) in racing condition.

For example, if all PUTs are in older than reclaime age (i.e. request
X-Timestamp << (now - reclaime_age), e.g. reconstruction PUT)

The following sequence *may* happen in the older code:

===============================================================================
2 nodes have different timestamp object each other with same path
(overwritten), and then,

1. a reconstructor (t0) attempts a PUT and object-server make a
   t0.data file
2: another reconstructor (t1) attempts a PUT and object-server makes a
   t1.data file

likely:
[a reconstructor] -(PUT t0)-> [object-sever] <-(PUT t1)-[another reconstructor]

And then,

3. object-server makes a t0.durable and call cleanup_list_dir which triggers to
   remove t1.data because it doesn't have .durable and older than reclaim age.

4. After that, object-server makes a t1.durable and call cleanup_list_dir will
   remove all .data .durable file older than the newest t1.durable.

5. finally all .data files are gone, unexpectedly.

===============================================================================

This patch prevents such an unexpected racing cleanup. With this code,
object-server at the state 4 (above) will search the newest .durable file
which has at least 1 .data file before cleanup and then keep the durable
frag set as the most recent durable fragment set even if newer durable
(but no .data files) files are found in the device. The orphaned durable
will be reclaimed expecting to be reconstructed again.

Closes-Bug: #1554378

Change-Id: Iaa11aced47c1adc14bde0988e5b693d33d2bbeaf
",git fetch https://review.opendev.org/openstack/swift refs/changes/56/289756/6 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/obj/test_diskfile.py', 'swift/obj/diskfile.py']",2,a2f7fa17a4a723abeea36fb0818710c552c91b26,bug/1554378," all_data_files = exts.get('.data', []) # Find out .durable which has .data for durable_info in exts['.durable']: new_or_eq, all_data_files = self._split_gte_timestamp( all_data_files, durable_info['timestamp']) if new_or_eq: break ", durable_info = exts['.durable'][0],15,1
openstack%2Fswift~master~I98c2e522830c7e156dbaa6dd21e2b6affe115951,openstack/swift,master,I98c2e522830c7e156dbaa6dd21e2b6affe115951,WIP: diskfile: Only reclaim static file,NEW,2016-10-21 09:58:29.000000000,2017-12-18 04:44:35.000000000,,"[{'_account_id': 4608}, {'_account_id': 13052}, {'_account_id': 13852}]","[{'number': 1, 'created': '2016-10-21 09:58:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/bdc443f532958aa09537b8f4f1ff0c667997dc15', 'message': 'WIP: diskfile: Only reclaim static file\n\nWhen 2 processes work on the same disk, one can be uploading an old object\n(replication) and the other one can be deleting the file as it has not yet\na .durable file. This patch ensure that a directory is cleaned only if the\nfile has not been modified in the last $reclaim_age seconds\n\nNote: From prod XP, does not fix every race conditions, but a majority of them\n\nChange-Id: I98c2e522830c7e156dbaa6dd21e2b6affe115951\n'}, {'number': 2, 'created': '2016-10-27 09:59:59.000000000', 'files': ['test/unit/obj/test_diskfile.py', 'swift/obj/diskfile.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/ffe87943ce273b624ab2d5bdd222f2b4ebf126b6', 'message': 'WIP: diskfile: Only reclaim static file\n\nWhen 2 processes work on the same disk, one can be uploading an old object\n(replication) and the other one can be deleting the file as it has not yet\na .durable file. This patch ensure that a directory is cleaned only if the\nfile has not been modified in the last $reclaim_age seconds\n\nNote: From prod XP, does not fix every race conditions, but a majority of them\n\nCloses-Bug: #1554378\n\nChange-Id: I98c2e522830c7e156dbaa6dd21e2b6affe115951\n'}]",0,389627,ffe87943ce273b624ab2d5bdd222f2b4ebf126b6,9,3,2,13852,,,0,"WIP: diskfile: Only reclaim static file

When 2 processes work on the same disk, one can be uploading an old object
(replication) and the other one can be deleting the file as it has not yet
a .durable file. This patch ensure that a directory is cleaned only if the
file has not been modified in the last $reclaim_age seconds

Note: From prod XP, does not fix every race conditions, but a majority of them

Closes-Bug: #1554378

Change-Id: I98c2e522830c7e156dbaa6dd21e2b6affe115951
",git fetch https://review.opendev.org/openstack/swift refs/changes/27/389627/2 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/obj/test_diskfile.py', 'swift/obj/diskfile.py']",2,bdc443f532958aa09537b8f4f1ff0c667997dc15,bug/1597429," def is_reclaimable(timestamp, check_mtime=False, fname=None): if (time.time() - float(timestamp)) > reclaim_age: if check_mtime: try: timestamp = os.path.getmtime(fname) return (time.time() - float(timestamp)) > reclaim_age except: self.logger.exception('Cannot check mtime of possibly ' 'reclaimable file: %s' % fname) return False else: return True else: return False if is_reclaimable(file_info['timestamp'], check_mtime=True, fname=join(hsh_path, file_info['filename'])):", def is_reclaimable(timestamp): return (time.time() - float(timestamp)) > reclaim_age if is_reclaimable(file_info['timestamp']):,29,7
openstack%2Fdiskimage-builder~master~Ic74fbe340e45d46f2545a2c7401493f183199030,openstack/diskimage-builder,master,Ic74fbe340e45d46f2545a2c7401493f183199030,Introduce 'zuul' as a new source-repositories type,NEW,2016-08-30 16:27:50.000000000,2017-12-18 04:44:25.000000000,,"[{'_account_id': 7080}, {'_account_id': 7118}, {'_account_id': 10035}]","[{'number': 1, 'created': '2016-08-30 16:27:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/c5f06fa0df2a4be2bfbdecd961aadc1dd5cc6b13', 'message': '(WIP) Zuul-cloner first attempt\n\nDepends-On: I61fcc41d590432ae8bcdd019fb0ba72365c4fd3d\nChange-Id: Ic74fbe340e45d46f2545a2c7401493f183199030\n'}, {'number': 2, 'created': '2016-08-30 19:37:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/5ca48e990c69ddc1b59e17f6c7b374515b77ce6e', 'message': '(WIP) Zuul-cloner second attempt\n\nDepends-On: I61fcc41d590432ae8bcdd019fb0ba72365c4fd3d\nChange-Id: Ic74fbe340e45d46f2545a2c7401493f183199030\n'}, {'number': 3, 'created': '2016-08-30 19:39:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/66b81e1adf13156b4569b3de54bfe7137f0fd429', 'message': '(WIP) Zuul-cloner third attempt\n\nDepends-On: I61fcc41d590432ae8bcdd019fb0ba72365c4fd3d\nChange-Id: Ic74fbe340e45d46f2545a2c7401493f183199030\n'}, {'number': 4, 'created': '2016-08-30 20:24:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/06ea091b8d45f595a11ae0f2988486aacade9c00', 'message': 'Introduce \'zuul\' as a new source-repositories type\n\nA new source type, \'zuul\', has been added to the source-repositories\nelement. This type takes the same arguments as the git type but will use\nzuul-cloner instead of a regular git clone.\n\nAdditionally, a new \'-z\' option has been added to run_functests.sh.\nThis option sets a special environment variable,\n""SOURCE_REPOSITORIES_USE_ZUUL_FOR_GIT"" prior to running the tests. The\nsource repositories element will read and honor this variable by\nchanging any \'git\' type to \'zuul\'.\n\nThis feature closes a bug where elements could not use the ""Depends-On:""\nnotation to test in-flight changes. The aim is to pass the ""-z""\nparameter in project-config.\n\nPartial-bug: #1616999\nChange-Id: Ic74fbe340e45d46f2545a2c7401493f183199030\n'}, {'number': 5, 'created': '2016-08-30 20:33:07.000000000', 'files': ['tests/run_functests.sh', 'elements/source-repositories/README.rst', 'elements/source-repositories/extra-data.d/98-source-repositories'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/6c0b2e0de63415dab2fac2b944479d4368b10f7b', 'message': 'Introduce \'zuul\' as a new source-repositories type\n\nA new source type, \'zuul\', has been added to the source-repositories\nelement. This type takes the same arguments as the git type but will use\nzuul-cloner instead of a regular git clone.\n\nAdditionally, a new \'-z\' option has been added to run_functests.sh.\nThis option sets a special environment variable,\n""SOURCE_REPOSITORIES_USE_ZUUL_FOR_GIT"" prior to running the tests. The\nsource repositories element will read and honor this variable by\nchanging any \'git\' type to \'zuul\'.\n\nThis feature closes a bug where elements could not use the ""Depends-On:""\nnotation to test in-flight changes. The aim is to pass the ""-z""\nparameter in project-config.\n\nPartial-bug: #1616999\nChange-Id: Ic74fbe340e45d46f2545a2c7401493f183199030\n'}]",10,363013,6c0b2e0de63415dab2fac2b944479d4368b10f7b,23,3,5,7080,,,0,"Introduce 'zuul' as a new source-repositories type

A new source type, 'zuul', has been added to the source-repositories
element. This type takes the same arguments as the git type but will use
zuul-cloner instead of a regular git clone.

Additionally, a new '-z' option has been added to run_functests.sh.
This option sets a special environment variable,
""SOURCE_REPOSITORIES_USE_ZUUL_FOR_GIT"" prior to running the tests. The
source repositories element will read and honor this variable by
changing any 'git' type to 'zuul'.

This feature closes a bug where elements could not use the ""Depends-On:""
notation to test in-flight changes. The aim is to pass the ""-z""
parameter in project-config.

Partial-bug: #1616999
Change-Id: Ic74fbe340e45d46f2545a2c7401493f183199030
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/13/363013/4 && git format-patch -1 --stdout FETCH_HEAD,"['tests/run_functests.sh', 'elements/source-repositories/extra-data.d/98-source-repositories']",2,c5f06fa0df2a4be2bfbdecd961aadc1dd5cc6b13,bug/1616999," local REGEX=""^([^ ]+) (zuul|git|tar|file|package) ?(/[^ ]+)? ?([^ ]+)? ?([^ ]*)$"" if [ ""${SOURCE_REPOSITORIES_USE_ZUUL_FOR_GIT:-""0""}"" == ""1"" ] && [ ""${REPOTYPE}"" == ""git"" ]; then echo ""Using zuul provider instead of git, as requested."" REPOTYPE=""zuul"" fi zuul) echo ""USING ZUUL"" ZUUL_CLONER=""${ZUUL_CLONER:-""/usr/zuul-env/bin/zuul-cloner""}"" if [ ! -e ${ZUUL_CLONER} ]; then echo ""Error: zuul-cloner does not exist at ${ZUUL_CLONER}"" exit 1 fi # NOTE(mmitchell): Zuul handles caching on it's own, so simply invoke it. echo ""REPONAME: ${REPONAME}"" echo ""REPOTYPE: ${REPOTYPE}"" echo ""REPOPATH: ${REPOPATH}"" echo ""REPOLOCATION: ${REPOLOCATION}"" ${ZUUL_CLONER} --workspace /tmp --cache-dir ${ZUUL_CACHE_DIR:-""/opt/git""} \ git://git.openstack.org \ openstack/ironic-python-agent sudo mv /tmp/openstack/ironic-python-agent ${REPO_DEST} ls -la ${REPO_DEST} ls -la ${REPO_DEST}/TEST_FILE ;;"," local REGEX=""^([^ ]+) (git|tar|file|package) ?(/[^ ]+)? ?([^ ]+)? ?([^ ]*)$""",39,3
openstack%2Frally~master~I2c25ad60db0b396f69cb164646743032fc686648,openstack/rally,master,I2c25ad60db0b396f69cb164646743032fc686648,CleanUp Error log traces in network context,NEW,2016-05-26 11:13:58.000000000,2017-12-18 04:44:13.000000000,,"[{'_account_id': 6172}, {'_account_id': 7369}, {'_account_id': 11748}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 21004}]","[{'number': 1, 'created': '2016-05-26 11:13:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0347b2e1a4f6359f06d4ee425121e5ce122a7111', 'message': 'CleanUp Error log traces in network context\n\nDuplicated cleanup resources action causes fake log errors\nin network context log. While rally is used for test and validate\nother projects, it is important to generate correct and clear log\ninformation.\n\nThis patch fix the bug.\n\nChange-Id: I2c25ad60db0b396f69cb164646743032fc686648\nCloses-Bug: #1570920\n'}, {'number': 2, 'created': '2016-05-26 12:55:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/07cc156ca859f1c6488b653e614647ec2c0436c6', 'message': 'CleanUp Error log traces in network context\n\nDuplicated cleanup resources action causes fake log errors\nin network context log. While rally is used for test and validate\nother projects, it is important to generate correct and clear log\ninformation.\n\nThis patch fix the bug.\n\nChange-Id: I2c25ad60db0b396f69cb164646743032fc686648\nCloses-Bug: #1570920\n'}, {'number': 3, 'created': '2016-05-27 09:11:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/92412ccb03fbe11b146ce48c23f881a30b1a7b3f', 'message': 'CleanUp Error log traces in network context\n\nDuplicated cleanup resources action causes fake log errors\nin network context log. While rally is used for test and validate\nother projects, it is important to generate correct and clear log\ninformation.\n\nThis patch fix the bug.\n\nChange-Id: I2c25ad60db0b396f69cb164646743032fc686648\nCloses-Bug: #1570920\n'}, {'number': 4, 'created': '2016-05-27 11:46:37.000000000', 'files': ['tests/unit/plugins/openstack/context/network/test_network.py', 'rally/plugins/openstack/context/network/networks.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/6f9c20bf6ae4ad78c3399734d59d31cc2c2db67b', 'message': 'CleanUp Error log traces in network context\n\nDuplicated cleanup resources action causes fake log errors\nin network context log. While rally is used for test and validate\nother projects, it is important to generate correct and clear log\ninformation.\n\nThis patch fix the bug.\n\nChange-Id: I2c25ad60db0b396f69cb164646743032fc686648\nCloses-Bug: #1570920\n'}]",8,321476,6f9c20bf6ae4ad78c3399734d59d31cc2c2db67b,28,6,4,21004,,,0,"CleanUp Error log traces in network context

Duplicated cleanup resources action causes fake log errors
in network context log. While rally is used for test and validate
other projects, it is important to generate correct and clear log
information.

This patch fix the bug.

Change-Id: I2c25ad60db0b396f69cb164646743032fc686648
Closes-Bug: #1570920
",git fetch https://review.opendev.org/openstack/rally refs/changes/76/321476/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/plugins/openstack/context/network/test_network.py', 'rally/plugins/openstack/context/network/networks.py']",2,0347b2e1a4f6359f06d4ee425121e5ce122a7111,bug/1570920," network_id_list = [n[""id""] for n in net_wrapper.list_networks()] LOG.debug(""Network to be deleted is %s"" % network[""id""]) if network[""id""] and network[""id""] in network_id_list: with logging.ExceptionLogger( LOG, _(""Failed to delete network for tenant %s"") % tenant_id): net_wrapper.delete_network(network) else: LOG.warning(""Network %s has already "" ""been deleted"" % network[""id""])"," with logging.ExceptionLogger( LOG, _(""Failed to delete network for tenant %s"") % tenant_id): net_wrapper.delete_network(network)",25,8
openstack%2Fswift~master~I45e21f651c3b23326c23a718202b579377abbd75,openstack/swift,master,I45e21f651c3b23326c23a718202b579377abbd75,Fix policy and ring usage from --swift-dir option,NEW,2016-10-18 21:48:24.000000000,2017-12-18 04:44:08.000000000,,"[{'_account_id': 13052}, {'_account_id': 18334}, {'_account_id': 18838}]","[{'number': 1, 'created': '2016-10-18 21:48:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/5baa54888196dd347380bc942cac157ad03303f2', 'message': 'Fix policy and ring usage from --swift-dir option\n\nPrior to this fix, swift-account-info and swift-container-info were\nnot using swift-dir passed as an option, instead would go to look\nfor policy and ring information in the default swift-dir\nlocation - /et/swift/swift.conf\n\nThis patch ensures both the ring data and policy information is looked\nup in the swift-dir that is passed as an option\n\nChange-Id: I45e21f651c3b23326c23a718202b579377abbd75\nCloses-Bug: #1631389\nCloses-Bug: #1631386\n'}, {'number': 2, 'created': '2016-10-18 21:52:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/5e95c4450f56e44c038f33baf8957ffbbc2f8740', 'message': 'Fix policy and ring usage from --swift-dir option\n\nPrior to this fix, swift-account-info and swift-container-info were\nnot using swift-dir passed as an option, instead would go to look\nfor policy and ring information in the default swift-dir\nlocation - /etc/swift/swift.conf\n\nThis patch ensures both the ring data and policy information is looked\nup in the swift-dir that is passed as an option\n\nChange-Id: I45e21f651c3b23326c23a718202b579377abbd75\nCloses-Bug: #1631389\nCloses-Bug: #1631386\n'}, {'number': 3, 'created': '2016-10-18 22:57:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7df9feb19dc73ce8e071d0fc69e24647718f5179', 'message': 'Fix policy and ring usage from --swift-dir option\n\nPrior to this fix, swift-account-info and swift-container-info were\nnot using swift-dir passed as an option, instead would go to look\nfor policy and ring information in the default swift-dir\nlocation - /etc/swift/swift.conf\n\nThis patch ensures both the ring data and policy information is looked\nup in the swift-dir that is passed as an option\n\nChange-Id: I45e21f651c3b23326c23a718202b579377abbd75\nCloses-Bug: #1631389\nCloses-Bug: #1631386\n'}, {'number': 4, 'created': '2016-10-21 13:38:36.000000000', 'files': ['bin/swift-container-info', 'bin/swift-account-info'], 'web_link': 'https://opendev.org/openstack/swift/commit/de0fe2f842b0d7c40bd668752f2b6438ee55122c', 'message': 'Fix policy and ring usage from --swift-dir option\n\nPrior to this fix, swift-account-info and swift-container-info were\nnot using swift-dir passed as an option, instead would go to look\nfor policy and ring information in the default swift-dir\nlocation - /etc/swift/swift.conf\n\nThis patch ensures both the ring data and policy information is looked\nup in the swift-dir that is passed as an option\n\nChange-Id: I45e21f651c3b23326c23a718202b579377abbd75\nCloses-Bug: #1631389\nCloses-Bug: #1631386\n'}]",4,388231,de0fe2f842b0d7c40bd668752f2b6438ee55122c,16,3,4,18334,,,0,"Fix policy and ring usage from --swift-dir option

Prior to this fix, swift-account-info and swift-container-info were
not using swift-dir passed as an option, instead would go to look
for policy and ring information in the default swift-dir
location - /etc/swift/swift.conf

This patch ensures both the ring data and policy information is looked
up in the swift-dir that is passed as an option

Change-Id: I45e21f651c3b23326c23a718202b579377abbd75
Closes-Bug: #1631389
Closes-Bug: #1631386
",git fetch https://review.opendev.org/openstack/swift refs/changes/31/388231/4 && git format-patch -1 --stdout FETCH_HEAD,"['bin/swift-container-info', 'bin/swift-account-info']",2,5baa54888196dd347380bc942cac157ad03303f2,bug/swift-dir_option,"from swift.common.storage_policy import reload_storage_policies '-d', '--swift-dir', default='/etc/swift', dest='swift_dir', if options.swift_dir is not '/etc/swift': reload_storage_policies(options.swift_dir) "," '-d', '--swift-dir', default='/etc/swift',",10,4
openstack%2Fswift~master~I8864f4bf7eb489d4c6ef97e427e5472c306d3d57,openstack/swift,master,I8864f4bf7eb489d4c6ef97e427e5472c306d3d57,Storage Account Object Count Quota Support,NEW,2016-05-04 15:39:44.000000000,2017-12-18 04:44:06.000000000,,"[{'_account_id': 5600}, {'_account_id': 7233}, {'_account_id': 7847}, {'_account_id': 9576}, {'_account_id': 9591}, {'_account_id': 12279}, {'_account_id': 13052}, {'_account_id': 20553}]","[{'number': 1, 'created': '2016-05-04 15:39:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4156554bb72071cb69d13dc9c0daf1883d6f68fe', 'message': 'Storage Account Object Count Quota Support\n\nAdds support for object count quotas at a storage account level to\ncompliment the current support for byte level quotas. This brings the\nquota functionality of storage accounts in line with what is supported\non containers.\n\nIn addition, added functional test to cover the existing object count\nquota functionality on containers.\n\nChange-Id: I8864f4bf7eb489d4c6ef97e427e5472c306d3d57\n'}, {'number': 2, 'created': '2016-05-05 12:09:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/02a443914d37ccf6987de6fd7b3e5892e80b07df', 'message': 'Storage Account Object Count Quota Support\n\nAdds support for object count quotas at a storage account level to\ncompliment the current support for byte level quotas. This brings the\nquota functionality of storage accounts in line with what is supported\non containers.\n\nIn addition, added functional test to cover the existing object count\nquota functionality on containers.\n\nChange-Id: I8864f4bf7eb489d4c6ef97e427e5472c306d3d57\n'}, {'number': 3, 'created': '2016-05-05 21:24:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7717ef83de4414bb3930f8dbf2b93b141b6074e5', 'message': 'Storage Account Object Count Quota Support\n\nAdds support for object count quotas at a storage account level to\ncompliment the current support for byte level quotas. This brings the\nquota functionality of storage accounts in line with what is supported\non containers.\n\nIn addition, added functional test to cover the existing object count\nquota functionality on containers.\n\nChange-Id: I8864f4bf7eb489d4c6ef97e427e5472c306d3d57\n'}, {'number': 4, 'created': '2016-05-06 03:26:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1c90f6d900096563f17f5c6f2921f572eb94e8f3', 'message': 'Storage Account Object Count Quota Support\n\nAdds support for object count quotas at a storage account level to\ncompliment the current support for byte level quotas. This brings the\nquota functionality of storage accounts in line with what is supported\non containers.\n\nIn addition, added functional test to cover the existing object count\nquota functionality on containers.\n\nChange-Id: I8864f4bf7eb489d4c6ef97e427e5472c306d3d57\n'}, {'number': 5, 'created': '2016-05-09 15:17:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/696f7ad6ff50aac31b097219f5df19b16de7fcf9', 'message': 'Storage Account Object Count Quota Support\n\nAdds support for object count quotas at a storage account level to\ncompliment the current support for byte level quotas. This brings the\nquota functionality of storage accounts in line with what is supported\non containers.\n\nChange-Id: I8864f4bf7eb489d4c6ef97e427e5472c306d3d57\n'}, {'number': 6, 'created': '2016-05-16 22:36:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/bec24de90bb027690a824bd511424dc7d9c3caf3', 'message': 'Storage Account Object Count Quota Support\n\nAdds support for object count quotas at a storage account level to\ncompliment the current support for byte level quotas. This brings the\nquota functionality of storage accounts in line with what is supported\non containers.\n\nChange-Id: I8864f4bf7eb489d4c6ef97e427e5472c306d3d57\n'}, {'number': 7, 'created': '2016-06-21 20:26:30.000000000', 'files': ['test/sample.conf', 'swift/common/middleware/account_quotas.py', 'test/functional/test_account.py', 'test/functional/__init__.py', 'test/unit/common/test_utils.py', 'test/unit/common/middleware/test_account_quotas.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/e061e4c1d4638dce292b3daf0952640c1d126c9d', 'message': 'Storage Account Object Count Quota Support\n\nAdds support for object count quotas at a storage account level to\ncompliment the current support for byte level quotas. This brings the\nquota functionality of storage accounts in line with what is supported\non containers.\n\nChange-Id: I8864f4bf7eb489d4c6ef97e427e5472c306d3d57\n'}]",12,312603,e061e4c1d4638dce292b3daf0952640c1d126c9d,38,8,7,20553,,,0,"Storage Account Object Count Quota Support

Adds support for object count quotas at a storage account level to
compliment the current support for byte level quotas. This brings the
quota functionality of storage accounts in line with what is supported
on containers.

Change-Id: I8864f4bf7eb489d4c6ef97e427e5472c306d3d57
",git fetch https://review.opendev.org/openstack/swift refs/changes/03/312603/4 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/middleware/account_quotas.py', 'test/functional/test_account.py', 'test/functional/test_container.py', 'test/unit/common/middleware/test_account_quotas.py']",4,4156554bb72071cb69d13dc9c0daf1883d6f68fe,account-object-quota-support," def test_unauthorized_object_count(self): headers = [('x-account-object-count', '10'), ] def test_no_quotas_bytes_used(self): def test_no_quotas_object_count(self): headers = [('x-account-object-count', '10'), ] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_obj_request_ignores_attempt_to_set_quota_bytes_used(self): def test_obj_request_ignores_attempt_to_set_quota_object_count(self): # If you try to set X-Account-Meta-* on an object, it's ignored, so # the quota middleware shouldn't complain about it even if we're not a # reseller admin. headers = [('x-account-object-count', '10')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', headers={'X-Account-Meta-Quota-Count': '99999'}, environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_container_request_ignores_attempt_to_set_quota_bytes_used(self): def test_container_request_ignores_attempt_to_set_quota_object_count(self): # As with an object, if you try to set X-Account-Meta-* on a # container, it's ignored. headers = [('x-account-object-count', '1000')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c', headers={'X-Account-Meta-Quota-Count': '99999'}, environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_bogus_quota_is_ignored_bytes_used(self): def test_bogus_quota_is_ignored_object_count(self): # This can happen if the metadata was set by a user prior to the # activation of the account-quota middleware headers = [('x-account-object-count', '1000'), ('x-account-meta-quota-count', 'pasty-plastogene')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_exceed_object_quota(self): headers = [('x-account-object-count', '10'), ('x-account-meta-quota-count', '1')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 413) self.assertEqual(res.body, 'Upload exceeds quota.') def test_exceed_bytes_quota_not_authorized(self): def test_exceed_object_quota_not_authorized(self): headers = [('x-account-object-count', '1000'), ('x-account-meta-quota-count', '0')] app = FakeAuthFilter( account_quotas.AccountQuotaMiddleware(FakeApp(headers))) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', method='PUT', headers={'x-auth-token': 'bad-secret'}, environ={'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 403) def test_exceed_bytes_quota_authorized(self): self.assertEqual(res.body, 'Upload exceeds quota.') def test_exceed_object_quota_authorized(self): headers = [('x-account-object-count', '1000'), ('x-account-meta-quota-count', '0')] app = FakeAuthFilter( account_quotas.AccountQuotaMiddleware(FakeApp(headers))) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', method='PUT', headers={'x-auth-token': 'secret'}, environ={'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 413) self.assertEqual(res.body, 'Upload exceeds quota.') def test_exceed_bytes_quota_both_set_authorized(self): # Ensure that if the byte quota exceeds and the count quota # passes that a 413 is still sent headers = [('x-account-bytes-used', '1000'), ('x-account-meta-quota-bytes', '999'), ('x-account-object-count', '0'), ('x-account-meta-quota-count', '1000')] app = FakeAuthFilter( account_quotas.AccountQuotaMiddleware(FakeApp(headers))) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', method='PUT', headers={'x-auth-token': 'secret'}, environ={'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 413) self.assertEqual(res.body, 'Upload exceeds quota.') def test_exceed_count_quota_both_set_authorized(self): # Ensure that if the count quota exceeds and the byte quota # passes that a 413 ist still sent headers = [('x-account-bytes-used', '0'), ('x-account-meta-quota-bytes', '1000'), ('x-account-object-count', '1000'), ('x-account-meta-quota-count', '1000')] app = FakeAuthFilter( account_quotas.AccountQuotaMiddleware(FakeApp(headers))) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', method='PUT', headers={'x-auth-token': 'secret'}, environ={'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 413) self.assertEqual(res.body, 'Upload exceeds quota.') def test_exceed_both_quota_both_set_authorized(self): # Ensure that if the count quota exceeds and the byte quota # passes that a 413 ist still sent headers = [('x-account-bytes-used', '1001'), ('x-account-meta-quota-bytes', '1000'), ('x-account-object-count', '1000'), ('x-account-meta-quota-count', '1000')] app = FakeAuthFilter( account_quotas.AccountQuotaMiddleware(FakeApp(headers))) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', method='PUT', headers={'x-auth-token': 'secret'}, environ={'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 413) self.assertEqual(res.body, 'Upload exceeds quota.') def test_under_bytes_quota_not_authorized(self): def test_under_object_quota_not_authorized(self): headers = [('x-account-object-count', '0'), ('x-account-meta-quota-count', '1000')] app = FakeAuthFilter( account_quotas.AccountQuotaMiddleware(FakeApp(headers))) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', method='PUT', headers={'x-auth-token': 'bad-secret'}, environ={'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 403) def test_under_bytes_quota_authorized(self): def test_under_object_quota_authorized(self): headers = [('x-account-object-count', '0'), ('x-account-meta-quota-count', '1000')] app = FakeAuthFilter( account_quotas.AccountQuotaMiddleware(FakeApp(headers))) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', method='PUT', headers={'x-auth-token': 'secret'}, environ={'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_over_bytes_quota_container_create_still_works(self): def test_over_object_quota_container_create_still_works(self): headers = [('x-account-object-count', '1001'), ('x-account-meta-quota-count', '1000')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/new_container', environ={'REQUEST_METHOD': 'PUT', 'HTTP_X_CONTAINER_META_BERT': 'ernie', 'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_over_bytes_quota_container_post_still_works(self): def test_over_object_quota_container_post_still_works(self): headers = [('x-account-object-count', '1001'), ('x-account-meta-quota-count', '1000')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/new_container', environ={'REQUEST_METHOD': 'POST', 'HTTP_X_CONTAINER_META_BERT': 'ernie', 'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_over_bytes_quota_obj_post_still_works(self): def test_over_object_quota_obj_post_still_works(self): headers = [('x-account-object-count', '1001'), ('x-account-meta-quota-count', '1000')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', environ={'REQUEST_METHOD': 'POST', 'HTTP_X_OBJECT_META_BERT': 'ernie', 'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_exceed_count_quota_copy_from(self): headers = [('x-account-object-count', '1'), ('x-account-meta-quota-count', '1')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}, headers={'x-copy-from': '/c2/o2'}) res = req.get_response(app) self.assertEqual(res.status_int, 413) self.assertEqual(res.body, 'Upload exceeds quota.') def test_exceed_count_quota_copy_verb(self): headers = [('x-account-object-count', '1'), ('x-account-meta-quota-count', '1')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c2/o2', environ={'REQUEST_METHOD': 'COPY', 'swift.cache': cache}, headers={'Destination': '/c/o'}) res = req.get_response(app) self.assertEqual(res.status_int, 413) self.assertEqual(res.body, 'Upload exceeds quota.') def test_not_exceed_count_quota_copy_from(self): headers = [('x-account-object-count', '1'), ('x-account-meta-quota-count', '2')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}, headers={'x-copy-from': '/c2/o2'}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_not_exceed_count_quota_copy_verb(self): headers = [('x-account-object-count', '1'), ('x-account-meta-quota-count', '2')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c2/o2', environ={'REQUEST_METHOD': 'COPY', 'swift.cache': cache}, headers={'Destination': '/c/o'}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_bytes_quota_copy_from_no_src(self): def test_count_quota_copy_from_no_src(self): headers = [('x-account-object-count', '1'), ('x-account-meta-quota-count', '2')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}, headers={'x-copy-from': '/c2/o3'}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_bytes_quota_copy_from_bad_src(self): def test_count_quota_copy_from_bad_src(self): # Unlike the byte quota this will result in a 200, object count # quota does not require content length and does not need to # verify the copied content length for quota validation headers = [('x-account-object-count', '1'), ('x-account-meta-quota-count', '2')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}, headers={'x-copy-from': 'bad_path'}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_exceed_object_quota_reseller(self): headers = [('x-account-object-count', '1000'), ('x-account-meta-quota-count', '0')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a', environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache, 'reseller_request': True}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_not_exceed_object_quota(self): headers = [('x-account-object-count', '1000'), ('x-account-meta-quota-count', 2000)] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_not_exceed_both_set(self): # Ensure that if the byte quota exceeds and the count quota # passes that a 413 is still sent headers = [('x-account-bytes-used', '1000'), ('x-account-meta-quota-bytes', '1001'), ('x-account-object-count', '1000'), ('x-account-meta-quota-count', '1001')] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_invalid_bytes_quotas(self): def test_invalid_object_quotas(self): headers = [('x-account-object-count', '0'), ] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a', environ={'REQUEST_METHOD': 'POST', 'swift.cache': cache, 'HTTP_X_ACCOUNT_META_QUOTA_COUNT': 'abc', 'reseller_request': True}) res = req.get_response(app) self.assertEqual(res.status_int, 400) def test_valid_bytes_quotas_admin(self): def test_valid_object_quotas_admin(self): headers = [('x-account-object-count', '0'), ] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a', environ={'REQUEST_METHOD': 'POST', 'swift.cache': cache, 'HTTP_X_ACCOUNT_META_QUOTA_COUNT': '100'}) res = req.get_response(app) self.assertEqual(res.status_int, 403) def test_valid_bytes_quotas_reseller(self): def test_valid_object_quotas_reseller(self): headers = [('x-account-object-count', '0'), ] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a', environ={'REQUEST_METHOD': 'POST', 'swift.cache': cache, 'HTTP_X_ACCOUNT_META_QUOTA_COUNT': '100', 'reseller_request': True}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_delete_bytes_quota(self): def test_delete_object_quota(self): headers = [('x-account-object-count', '0'), ] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a', environ={'REQUEST_METHOD': 'POST', 'swift.cache': cache, 'HTTP_X_ACCOUNT_META_QUOTA_COUNT': ''}) res = req.get_response(app) self.assertEqual(res.status_int, 403) def test_delete_bytes_quota_with_remove_header(self): def test_delete_object_quota_with_remove_header(self): headers = [('x-account-object-count', '0'), ] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a', environ={ 'REQUEST_METHOD': 'POST', 'swift.cache': cache, 'HTTP_X_REMOVE_ACCOUNT_META_QUOTA_COUNT': 'True'}) res = req.get_response(app) self.assertEqual(res.status_int, 403) def test_delete_bytes_quota_reseller(self): def test_delete_object_quota_reseller(self): headers = [('x-account-object-count', '0'), ] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) req = Request.blank('/v1/a', environ={'REQUEST_METHOD': 'POST', 'HTTP_X_ACCOUNT_META_QUOTA_COUNT': '', 'reseller_request': True}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_delete_bytes_quota_with_remove_header_reseller(self): def test_delete_object_quota_with_remove_header_reseller(self): headers = [('x-account-object-count', '0'), ] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a', environ={ 'REQUEST_METHOD': 'POST', 'swift.cache': cache, 'HTTP_X_REMOVE_ACCOUNT_META_QUOTA_COUNT': 'True', 'reseller_request': True}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_invalid_request_exception_bytes_used(self): def test_invalid_request_exception_object_count(self): headers = [('x-account-object-count', '1000'), ] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1', environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}) res = req.get_response(app) # Response code of 200 because authentication itself is not done here self.assertEqual(res.status_int, 200) def test_bypass_quotas(self): # Bypassing quota all together. Test case is intended for coverage headers = [] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a', environ={'REQUEST_METHOD': 'GET', 'swift.cache': cache}) res = req.get_response(app) self.assertEqual(res.status_int, 200) def test_unauthorized_bytes_used(self): headers = [('x-account-bytes-used', '1000'), ] app = account_quotas.AccountQuotaMiddleware(FakeApp(headers)) cache = FakeCache(None) req = Request.blank('/v1/a/c/o', environ={'REQUEST_METHOD': 'PUT', 'swift.cache': cache}) res = req.get_response(app) # Response code of 200 because authentication itself is not done here self.assertEqual(res.status_int, 200)"," def test_unauthorized(self): headers = [('x-account-bytes-used', '1000'), ] def test_no_quotas(self): def test_obj_request_ignores_attempt_to_set_quotas(self): def test_container_request_ignores_attempt_to_set_quotas(self): def test_bogus_quota_is_ignored(self): def test_exceed_quota_not_authorized(self): def test_exceed_quota_authorized(self): def test_under_quota_not_authorized(self): def test_under_quota_authorized(self): def test_over_quota_container_create_still_works(self): def test_over_quota_container_post_still_works(self): def test_over_quota_obj_post_still_works(self): def test_quota_copy_from_no_src(self): def test_quota_copy_from_bad_src(self): def test_invalid_quotas(self): def test_valid_quotas_admin(self): def test_valid_quotas_reseller(self): def test_delete_quotas(self): def test_delete_quotas_with_remove_header(self): def test_delete_quotas_reseller(self): def test_delete_quotas_with_remove_header_reseller(self): def test_invalid_request_exception(self):",752,63
openstack%2Fswift~master~I4b67af8c10c712fbce0acf73c649d0cff06eead5,openstack/swift,master,I4b67af8c10c712fbce0acf73c649d0cff06eead5,Optimization of the reconstructor for handling of displaced fragments,NEW,2015-06-29 20:32:55.000000000,2017-12-18 04:44:04.000000000,,"[{'_account_id': 1179}, {'_account_id': 4608}, {'_account_id': 6968}, {'_account_id': 7233}, {'_account_id': 13052}, {'_account_id': 13104}]","[{'number': 1, 'created': '2015-06-29 20:32:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/15c13cf690b148a678b41a4dc90738abf0bbff27', 'message': 'EC: Handoff node to push existing fragment to the correct location.\n\nIf the fragment needing to be reconstructed already exists, let the\nhandoff node push it to the primary.\n\nChange-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5\nCloses-Bug: #1469815\n'}, {'number': 2, 'created': '2015-06-30 18:19:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7b591359069696cce91bdbfd8002f2db6a7ef858', 'message': 'EC: Handoff node to push existing fragment to the correct location.\n\nIf the fragment needing to be reconstructed already exists, let the\nhandoff node push it to the primary.\n\nChange-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5\nCloses-Bug: #1469815\n'}, {'number': 3, 'created': '2015-07-14 20:45:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/dd488ff1bd6f684fcacc6efab057c56f4a1f3bdf', 'message': 'EC: Handoff node to push existing fragment to the correct location.\n\nIf the fragment needing to be reconstructed already exists, let the\nhandoff node push it to the primary.\n\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\n\nChange-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5\nCloses-Bug: #1469815\n'}, {'number': 4, 'created': '2015-07-28 15:00:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/03f7899e5e76cfc60eaa1728db128b3159ba72a8', 'message': 'EC: Handoff node to push existing fragment to the correct location.\n\nIf the fragment needing to be reconstructed already exists, let the\nhandoff node push it to the primary.\n\nChange-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5\nCloses-Bug: #1469815\n'}, {'number': 5, 'created': '2015-07-30 17:56:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6e618850d1ed00bd7bd0da5df62a31164ce5ef4e', 'message': 'EC: Handoff node to push existing fragment to the correct location.\n\nIf the fragment needing to be reconstructed already exists, let the\nhandoff node push it to the primary.\n\nChange-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5\nCloses-Bug: #1469815\n'}, {'number': 6, 'created': '2015-07-30 20:21:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/dc250d9d6e34fa844c785d7ebbd3ccb5f428efc4', 'message': 'EC: Handoff node to push existing fragment to the correct location.\n\nIf the fragment needing to be reconstructed already exists, let the\nhandoff node push it to the primary.\n\nChange-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5\nCloses-Bug: #1469815\n'}, {'number': 7, 'created': '2015-08-25 21:37:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f94b6b52b238d925413f086dbbd073f6443a0341', 'message': 'EC: Handoff node to push existing fragment to the correct location.\n\nIf the fragment needing to be reconstructed already exists, let the\nhandoff node push it to the primary.\n\nChange-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5\nCloses-Bug: #1469815\n'}, {'number': 8, 'created': '2015-09-01 16:00:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1567a6e39969d29c38976cbbdfa76973d7d73200', 'message': ""EC: Handoff node to push existing fragment to the correct location.\n\nIf the fragment needing to be reconstructed already exists, let the\nhandoff node push it to the primary.\n\nAdded a probe test that will test reconstructor behavior with\nrespect to fragment transition when primary and handoff nodes\nswitch roles (e.g., in the case of a ring rebalance).\n\nAdded a probe test that will test partner primary reconstructor\nbehavior to make sure that it does not rebuild its partner's\nfragment in a scenario where it already exists within the\ncluster.\n\nChange-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5\nCloses-Bug: #1469815\n""}, {'number': 9, 'created': '2015-09-01 22:30:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ea299996bd2b3be5715f39ff561f79b3866b6652', 'message': ""EC: Handoff node to push existing fragment to the correct location.\n\nIf the fragment needing to be reconstructed already exists, let the\nhandoff (or other primary) node push it to the original location.\n\nAdded a probe test that will test reconstructor behavior with\nrespect to fragment transition when primary and handoff nodes\nswitch roles (e.g., in the case of a ring rebalance).\n\nAdded a probe test that will test partner primary reconstructor\nbehavior to make sure that it does not rebuild its partner's\nfragment in a scenario where it already exists within the\ncluster.\n\nChange-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5\nCloses-Bug: #1469815\n""}, {'number': 10, 'created': '2015-09-30 19:58:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/735b2fdc2011d4c96ffcc300d90f03348b250505', 'message': ""EC: Handoff node to push existing fragment to the correct location.\n\nIf the fragment needing to be reconstructed already exists, let the\nhandoff (or other primary) node push it to the original location.\n\nAdded a probe test that will test reconstructor behavior with\nrespect to fragment transition when primary and handoff nodes\nswitch roles (e.g., in the case of a ring rebalance).\n\nAdded a probe test that will test partner primary reconstructor\nbehavior to make sure that it does not rebuild its partner's\nfragment in a scenario where it already exists within the\ncluster.\n\nChange-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5\nCloses-Bug: #1469815\n""}, {'number': 11, 'created': '2015-09-30 20:46:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c8d5ed1d6e0198a81877b360c6c00bbaf2ba6255', 'message': ""Optimization of the reconstructor for handling of displaced fragments\n\nIf the fragment needing to be reconstructed already exists, let the\nhandoff (or other primary) node push it to the original location.\n\nAdded a probe test that will test reconstructor behavior with\nrespect to fragment transition when primary and handoff nodes\nswitch roles (e.g., in the case of a ring rebalance).\n\nAdded a probe test that will test partner primary reconstructor\nbehavior to make sure that it does not rebuild its partner's\nfragment in a scenario where it already exists within the\ncluster.\n\nChange-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5\nCloses-Bug: #1469815\n""}, {'number': 12, 'created': '2016-01-25 21:05:00.000000000', 'files': ['test/probe/test_reconstructor_revert.py', 'swift/obj/reconstructor.py', 'test/unit/obj/test_reconstructor.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/4e6903d83d28286d1aba33da6bef4a017cfae0fa', 'message': ""Optimization of the reconstructor for handling of displaced fragments\n\nIf the fragment needing to be reconstructed already exists, let the\nhandoff (or other primary) node push it to the original location.\n\nAdded a probe test that will test reconstructor behavior with\nrespect to fragment transition when primary and handoff nodes\nswitch roles (e.g., in the case of a ring rebalance).\n\nAdded a probe test that will test partner primary reconstructor\nbehavior to make sure that it does not rebuild its partner's\nfragment in a scenario where it already exists within the\ncluster.\n\nChange-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5\nCloses-Bug: #1469815\n""}]",7,196848,4e6903d83d28286d1aba33da6bef4a017cfae0fa,46,6,12,13104,,,0,"Optimization of the reconstructor for handling of displaced fragments

If the fragment needing to be reconstructed already exists, let the
handoff (or other primary) node push it to the original location.

Added a probe test that will test reconstructor behavior with
respect to fragment transition when primary and handoff nodes
switch roles (e.g., in the case of a ring rebalance).

Added a probe test that will test partner primary reconstructor
behavior to make sure that it does not rebuild its partner's
fragment in a scenario where it already exists within the
cluster.

Change-Id: I4b67af8c10c712fbce0acf73c649d0cff06eead5
Closes-Bug: #1469815
",git fetch https://review.opendev.org/openstack/swift refs/changes/48/196848/7 && git format-patch -1 --stdout FETCH_HEAD,"['swift/obj/reconstructor.py', 'test/unit/obj/test_reconstructor.py']",2,15c13cf690b148a678b41a4dc90738abf0bbff27,193279," def test_reconstruct_fa_finds_itself_does_not_reconstruct(self): self.assertRaises(DiskFileError, self.reconstructor.reconstruct_fa, job, node, metadata)"," def test_reconstruct_fa_finds_itself_does_not_fail(self): df = self.reconstructor.reconstruct_fa( job, node, metadata) fixed_body = ''.join(df.reader()) self.assertEqual(len(fixed_body), len(broken_body)) self.assertEqual(md5(fixed_body).hexdigest(), md5(broken_body).hexdigest())",4,8
openstack%2Fswift~master~I16e2d14a2e930c0a9b75dddc8ba1f31f53743038,openstack/swift,master,I16e2d14a2e930c0a9b75dddc8ba1f31f53743038,Add missing tests for x-if-delete-at header,NEW,2016-03-02 15:27:55.000000000,2017-12-18 04:44:02.000000000,,"[{'_account_id': 4608}, {'_account_id': 7847}, {'_account_id': 13052}]","[{'number': 1, 'created': '2016-03-02 15:27:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a604b74d6ebed63415946b2e8417bb2104d1ede9', 'message': 'Add missing test for x-if-delete-at header\n\nWhile reviewing https://review.openstack.org/#/c/283351\nI found a test was missing. That is to assert ""DELETE request\nwith x-if-delete-at against to an object which doesn\'t keep\nDelete-At header metadata should always fail as 412"".\n(This situation will be happen when expired object overwritten\nwith an object w/o expir headers)\n\nThis patch adds that one. No test changes, just added one more\nassertion.\n\nChange-Id: I16e2d14a2e930c0a9b75dddc8ba1f31f53743038\n'}, {'number': 2, 'created': '2016-03-02 15:59:00.000000000', 'files': ['test/unit/obj/test_server.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/ce5de26b254f5fd0450df469b1c81a88c9bcd6ee', 'message': 'Add missing tests for x-if-delete-at header\n\nWhile reviewing https://review.openstack.org/#/c/283351\nI found a couple of tests was missing. That is to assert for a\nkind of ""DELETE request with x-if-delete-at against to an object\nwhich doesn\'t keep Delete-At header metadata should always fail"".\n(This situation will happen when expired object overwritten\nwith an object w/o expire headers)\n\nThis patch adds them. No tests changed, just added 2 assertions.\n\nChange-Id: I16e2d14a2e930c0a9b75dddc8ba1f31f53743038\n'}]",1,287262,ce5de26b254f5fd0450df469b1c81a88c9bcd6ee,9,3,2,4608,,,0,"Add missing tests for x-if-delete-at header

While reviewing https://review.openstack.org/#/c/283351
I found a couple of tests was missing. That is to assert for a
kind of ""DELETE request with x-if-delete-at against to an object
which doesn't keep Delete-At header metadata should always fail"".
(This situation will happen when expired object overwritten
with an object w/o expire headers)

This patch adds them. No tests changed, just added 2 assertions.

Change-Id: I16e2d14a2e930c0a9b75dddc8ba1f31f53743038
",git fetch https://review.opendev.org/openstack/swift refs/changes/62/287262/1 && git format-patch -1 --stdout FETCH_HEAD,['test/unit/obj/test_server.py'],1,a604b74d6ebed63415946b2e8417bb2104d1ede9,add-missing-test," headers={'X-Timestamp': normalize_timestamp(test_time - 98), 'X-If-Delete-At': str(int(test_time))}) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 412) req = Request.blank( '/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'DELETE'},",,8,0
openstack%2Fswift~master~Ic73186e319793d976aacb8b4d4dc2d9bf7e0668a,openstack/swift,master,Ic73186e319793d976aacb8b4d4dc2d9bf7e0668a,Add test that authorize callback cannot violate constraints,NEW,2016-08-01 13:24:48.000000000,2017-12-18 04:43:50.000000000,,"[{'_account_id': 7847}, {'_account_id': 8542}, {'_account_id': 12261}]","[{'number': 1, 'created': '2016-08-01 13:24:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7758b1f03ba839401fd0ee11f9556029f1313250', 'message': 'Add test that authorize callback cannot violate constraints\n\nTest that the swift.authorize callback cannot add metadata\nto a PUT request that violates the metadata constraints without\ntriggering a 400 response.\n\nThis is a follow-up to the Related-Change and also expands the\ncoverage of the similar test for POST requests added in that\nchange.\n\nRemove redundant lines setting the now obsoelete object_post_as_copy\nvariable in the proxy app - this is now implemented in copy middleware.\n\nChange-Id: Ic73186e319793d976aacb8b4d4dc2d9bf7e0668a\nRelated-Change: I5f05039498c406473952e78c6a40ec11e8b53f8e\n'}, {'number': 2, 'created': '2016-08-02 11:27:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8f4f46f1ae7ffa2ef7ad3458678c10cc1f30127f', 'message': 'Add test that authorize callback cannot violate constraints\n\nTest that the swift.authorize callback cannot add metadata\nto a PUT request that violates the metadata constraints without\ntriggering a 400 response.\n\nAdds a helper method to test/unit/__init__.py which can be used\nto construct a set of metadata headers of an exact size.\n\nThis is a follow-up to the Related-Change and also expands the\ncoverage of the similar test for POST requests added in that\nchange.\n\nRemove redundant lines setting the now obsolete object_post_as_copy\nvariable in the proxy app - this is now implemented in copy middleware.\n\nChange-Id: Ic73186e319793d976aacb8b4d4dc2d9bf7e0668a\nRelated-Change: I5f05039498c406473952e78c6a40ec11e8b53f8e\n'}, {'number': 3, 'created': '2016-09-20 11:34:06.000000000', 'files': ['test/unit/proxy/test_server.py', 'test/unit/__init__.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/efd393a16ce207920afa7442138ea8be36429aa2', 'message': 'Add test that authorize callback cannot violate constraints\n\nTest that the swift.authorize callback cannot add metadata\nto a PUT request that violates the metadata constraints without\ntriggering a 400 response.\n\nAdds a helper method to test/unit/__init__.py which can be used\nto construct a set of metadata headers of an exact size.\n\nThis is a follow-up to the Related-Change and also expands the\ncoverage of the similar test for POST requests added in that\nchange.\n\nRemove redundant lines setting the now obsolete object_post_as_copy\nvariable in the proxy app - this is now implemented in copy middleware.\n\nChange-Id: Ic73186e319793d976aacb8b4d4dc2d9bf7e0668a\nRelated-Change: I5f05039498c406473952e78c6a40ec11e8b53f8e\n'}]",7,349501,efd393a16ce207920afa7442138ea8be36429aa2,18,3,3,7847,,,0,"Add test that authorize callback cannot violate constraints

Test that the swift.authorize callback cannot add metadata
to a PUT request that violates the metadata constraints without
triggering a 400 response.

Adds a helper method to test/unit/__init__.py which can be used
to construct a set of metadata headers of an exact size.

This is a follow-up to the Related-Change and also expands the
coverage of the similar test for POST requests added in that
change.

Remove redundant lines setting the now obsolete object_post_as_copy
variable in the proxy app - this is now implemented in copy middleware.

Change-Id: Ic73186e319793d976aacb8b4d4dc2d9bf7e0668a
Related-Change: I5f05039498c406473952e78c6a40ec11e8b53f8e
",git fetch https://review.opendev.org/openstack/swift refs/changes/01/349501/1 && git format-patch -1 --stdout FETCH_HEAD,['test/unit/proxy/test_server.py'],1,7758b1f03ba839401fd0ee11f9556029f1313250,(detached," # verify that authorize callback cannot add metadata that violates # constraints def do_test(headers, expected_status): called = [0] def authorize(req): called[0] += 1 req.headers.update(headers) return with save_globals(): controller = ReplicatedObjectController( self.app, 'account', 'container', 'object') set_http_connect(200, 200, 202, 202, 202) # acct cont obj obj obj req = Request.blank('/v1/a/c/o', {'REQUEST_METHOD': 'POST'}, headers={'Content-Type': 'foo/bar'}) req.environ['swift.authorize'] = authorize self.app.update_request(req) res = controller.POST(req) self.assertEqual(1, called[0]) self.assertEqual(expected_status, res.status_int) limit = constraints.MAX_META_VALUE_LENGTH do_test({'X-Object-Meta-Foo': 'x' * limit}, 202) do_test({'X-Object-Meta-Foo': 'x' * (limit + 1)}, 400) limit = constraints.MAX_META_NAME_LENGTH do_test({'X-Object-Meta-' + 'x' * limit: 'foo'}, 202) do_test({'X-Object-Meta-' + 'x' * (limit + 1): 'foo'}, 400) limit = constraints.MAX_META_COUNT do_test(dict(('X-Object-Meta-%s' % i, i) for i in range(limit)), 202) do_test( dict(('X-Object-Meta-%s' % i, i) for i in range(limit + 1)), 400) limit = constraints.MAX_META_OVERALL_SIZE # construct a set of headers with overall size of at most # MAX_META_OVERALL_SIZE - 2, leaving space to add one final header overall = 0 headers = {} val = 'x' * constraints.MAX_META_VALUE_LENGTH while overall + constraints.MAX_META_VALUE_LENGTH + 8 <= limit - 2: headers['X-Object-Meta-%8s' % overall] = val overall += constraints.MAX_META_VALUE_LENGTH + 8 final_val = 'x' * min( limit - overall - 1, constraints.MAX_META_VALUE_LENGTH) test_headers = dict(headers) test_headers_overall = overall + len(final_val) key = 'k' * (limit - test_headers_overall) test_headers['X-Object-Meta-%s' % key] = final_val test_headers_overall += len(key) self.assertEqual(limit, test_headers_overall) # sanity check do_test(test_headers, 202) test_headers = dict(headers) test_headers_overall = overall + len(final_val) key = 'k' * (limit - test_headers_overall + 1) test_headers['X-Object-Meta-%s' % key] = final_val test_headers_overall += len(key) self.assertEqual(limit + 1, test_headers_overall) # sanity check do_test(test_headers, 400) def test_PUT_checks_constraints_after_authorize(self): # verify that authorize callback cannot add metadata that violates # constraints def do_test(headers, expected_status): called = [0] def authorize(req): called[0] += 1 req.headers.update(headers) return with save_globals(): set_http_connect(200, 200, 201, 201, 201) controller = ReplicatedObjectController( self.app, 'account', 'container', 'object') req = Request.blank('/v1/a/c/o', method='PUT', body='12345') req.environ['swift.authorize'] = authorize self.app.update_request(req) resp = controller.PUT(req) self.assertEqual(1, called[0]) self.assertEqual(expected_status, resp.status_int) limit = constraints.MAX_META_VALUE_LENGTH do_test({'X-Object-Meta-Foo': 'x' * limit}, 201) do_test({'X-Object-Meta-Foo': 'x' * (limit + 1)}, 400) limit = constraints.MAX_META_NAME_LENGTH do_test({'X-Object-Meta-' + 'x' * limit: 'foo'}, 201) do_test({'X-Object-Meta-' + 'x' * (limit + 1): 'foo'}, 400) limit = constraints.MAX_META_COUNT do_test(dict(('X-Object-Meta-%s' % i, i) for i in range(limit)), 201) do_test( dict(('X-Object-Meta-%s' % i, i) for i in range(limit + 1)), 400) limit = constraints.MAX_META_OVERALL_SIZE # construct a set of headers with overall size of at most # MAX_META_OVERALL_SIZE - 2, leaving space to add one final header overall = 0 headers = {} val = 'x' * constraints.MAX_META_VALUE_LENGTH while overall + constraints.MAX_META_VALUE_LENGTH + 8 <= limit - 2: headers['X-Object-Meta-%8s' % overall] = val overall += constraints.MAX_META_VALUE_LENGTH + 8 final_val = 'x' * min( limit - overall - 1, constraints.MAX_META_VALUE_LENGTH) test_headers = dict(headers) test_headers_overall = overall + len(final_val) key = 'k' * (limit - test_headers_overall) test_headers['X-Object-Meta-%s' % key] = final_val test_headers_overall += len(key) self.assertEqual(limit, test_headers_overall) # sanity check do_test(test_headers, 201) test_headers = dict(headers) test_headers_overall = overall + len(final_val) key = 'k' * (limit - test_headers_overall + 1) test_headers['X-Object-Meta-%s' % key] = final_val test_headers_overall += len(key) self.assertEqual(limit + 1, test_headers_overall) # sanity check do_test(test_headers, 400) "," self.app.object_post_as_copy = False self.app.object_post_as_copy = False self.app.object_post_as_copy = False def authorize(req): req.headers['X-Object-Meta-Foo'] = 'x' * (limit + 1) return with save_globals(): limit = constraints.MAX_META_VALUE_LENGTH self.app.object_post_as_copy = False controller = ReplicatedObjectController( self.app, 'account', 'container', 'object') set_http_connect(200, 200, 202, 202, 202) # acct cont obj obj obj req = Request.blank('/v1/a/c/o', {'REQUEST_METHOD': 'POST'}, headers={'Content-Type': 'foo/bar', 'X-Object-Meta-Foo': 'x'}) req.environ['swift.authorize'] = authorize self.app.update_request(req) res = controller.POST(req) self.assertEqual(res.status_int, 400) self.app.object_post_as_copy = False self.app.object_post_as_copy = False self.app.object_post_as_copy = False self.app.object_post_as_copy = False self.app.object_post_as_copy = False",128,25
openstack%2Fdiskimage-builder~feature%2Fv2~I1dd150c17eadecacd4b4c9ec2d86db9e912e6d6a,openstack/diskimage-builder,feature/v2,I1dd150c17eadecacd4b4c9ec2d86db9e912e6d6a,Move dib-run-parts in to python,NEW,2016-11-02 21:35:35.000000000,2017-12-18 04:43:35.000000000,,[],"[{'number': 1, 'created': '2016-11-02 21:35:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/65454255d8058caaa4c79aca578660ed26d653f2', 'message': 'Move dib-run-parts in to python\n\nChange-Id: I1dd150c17eadecacd4b4c9ec2d86db9e912e6d6a\n'}, {'number': 2, 'created': '2016-11-02 21:49:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/fa1ad211dfb54e88842ba412660a36c9056401d9', 'message': 'Move dib-run-parts in to python\n\nChange-Id: I1dd150c17eadecacd4b4c9ec2d86db9e912e6d6a\n'}, {'number': 3, 'created': '2016-11-02 23:09:54.000000000', 'files': ['requirements.txt', 'diskimage_builder/tests/test_run_parts.py', 'diskimage_builder/lib/common-functions', 'diskimage_builder/run_parts.py', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/ead7663aadbb731f694e8701ee294863276bd8e7', 'message': 'Move dib-run-parts in to python\n\nChange-Id: I1dd150c17eadecacd4b4c9ec2d86db9e912e6d6a\n'}]",0,392973,ead7663aadbb731f694e8701ee294863276bd8e7,8,0,3,10035,,,0,"Move dib-run-parts in to python

Change-Id: I1dd150c17eadecacd4b4c9ec2d86db9e912e6d6a
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/73/392973/1 && git format-patch -1 --stdout FETCH_HEAD,"['diskimage_builder/tests/test_run_parts.py', 'diskimage_builder/run_parts.py']",2,65454255d8058caaa4c79aca578660ed26d653f2,,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import argparse import contextlib import os import stat import subprocess import tempfile def get_parts_list(parts_dir, require_exec=True): parts = [] for dir_item in os.listdir(parts_dir): item_path = os.path.join(parts_dir, dir_item) if os.path.isfile(item_path): if require_exec: if os.access(item_path, os.X_OK): parts.append(dir_item) else: parts.append(dir_item) return parts @contextlib.contextmanager def parts_runner_script(run_parts, env_parts): fh, script_file = tempfile.mkstemp() try: os.close(fh) os.chmod(script_file, stat.S_IXUSR | stat.S_IWUSR | stat.S_IRUSR) with open(script_file, 'w') as fh: fh.write('#!/bin/bash\n\n') for part in env_parts: fh.write('source %s\n' % part) for part in run_parts: fh.write('%s\n' % part) yield script_file finally: os.unlink(script_file) def run_parts(parts_dir, environment_dir): env_parts = get_parts_list(environment_dir, False) run_parts = get_parts_list(parts_dir) abs_env_parts = [] for part in env_parts: abs_part = os.path.abspath(os.path.join(environment_dir, part)) abs_env_parts.append(abs_part) abs_run_parts = [] for part in run_parts: abs_part = os.path.abspath(os.path.join(parts_dir, part)) abs_run_parts.append(abs_part) with parts_runner_script(abs_run_parts, abs_env_parts) as script: proc = subprocess.Popen(['/bin/bash', script]) proc.wait() def main(): parser = argparse.ArgumentParser() parser.add_argument('--list', '-l', action='store_true', default=False, help='Print names of all valid parts scripts.') parser.add_argument('scripts_directory', help='Path to directory of run parts scripts,') parser.add_argument('environment_dir', default=None, help='Path to directory of environment source parts.') args = parser.parse_args() ",,130,0
openstack%2Frally~master~Ia412dbcc675166029942f38bfa500d7a349c9d46,openstack/rally,master,Ia412dbcc675166029942f38bfa500d7a349c9d46,[spec]Proposal for new rally.osclients base class,NEW,2016-10-10 12:04:42.000000000,2017-12-18 04:42:40.000000000,,"[{'_account_id': 9545}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 23094}]","[{'number': 1, 'created': '2016-10-10 12:04:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ebd0c819f4beafc0d07b2ac68c5e1a151f1f6ee5', 'message': 'Proposal for new rally.osclients base class\n\nChange-Id: Ia412dbcc675166029942f38bfa500d7a349c9d46\n'}, {'number': 2, 'created': '2016-10-11 07:21:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e43d58a44e283d50c0aa36aea49126b415191685', 'message': '[spec]Proposal for new rally.osclients base class\n\nChange-Id: Ia412dbcc675166029942f38bfa500d7a349c9d46\n'}, {'number': 3, 'created': '2016-10-11 09:41:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/38970065b5cb2356b3f059b722eea3e03efd6ac4', 'message': '[spec]Proposal for new rally.osclients base class\n\nChange-Id: Ia412dbcc675166029942f38bfa500d7a349c9d46\n'}, {'number': 4, 'created': '2016-10-25 08:47:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5544d45d56124c8a62cff008895fb25f6f1b8d90', 'message': '[spec]Proposal for new rally.osclients base class\n\nChange-Id: Ia412dbcc675166029942f38bfa500d7a349c9d46\n'}, {'number': 5, 'created': '2016-10-25 10:15:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0314efff472e3547ed824c2507e621124d525499', 'message': '[spec]Proposal for new rally.osclients base class\n\nChange-Id: Ia412dbcc675166029942f38bfa500d7a349c9d46\n'}, {'number': 6, 'created': '2016-10-26 14:03:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/9c10c5ea52a8a6b7e35022bd8cd9cd4099bf61b0', 'message': '[spec]Proposal for new rally.osclients base class\n\nChange-Id: Ia412dbcc675166029942f38bfa500d7a349c9d46\n'}, {'number': 7, 'created': '2016-10-27 07:44:44.000000000', 'files': ['doc/specs/in-progress/rally_osclient_base_class.rst'], 'web_link': 'https://opendev.org/openstack/rally/commit/d5bfb28c9d11b07e342503a5e29a1c2c32db7631', 'message': '[spec]Proposal for new rally.osclients base class\n\nChange-Id: Ia412dbcc675166029942f38bfa500d7a349c9d46\n'}]",15,384455,d5bfb28c9d11b07e342503a5e29a1c2c32db7631,26,4,7,23094,,,0,"[spec]Proposal for new rally.osclients base class

Change-Id: Ia412dbcc675166029942f38bfa500d7a349c9d46
",git fetch https://review.opendev.org/openstack/rally refs/changes/55/384455/2 && git format-patch -1 --stdout FETCH_HEAD,['doc/specs/in-progress/rally_osclient_base_class.rst'],1,ebd0c819f4beafc0d07b2ac68c5e1a151f1f6ee5,astarove-spec,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode .. This template should be in ReSTructured text. The filename in the git repository should match the launchpad URL, for example a URL of https://blueprints.launchpad.net/heat/+spec/awesome-thing should be named awesome-thing.rst . Please do not delete any of the sections in this template. If you have nothing to say for a whole section, just write: None For help with syntax, see http://sphinx-doc.org/rest.html To test out your formatting, see http://www.tele3.cz/jbar/rest/rest.html ==================================================== Base class for 'rally.osclients' plugin ==================================================== Implement parrent class for class-aggregator 'rally.osclients.Clients' Problem description =================== Current implementation of openstack clients realizes support of multi-plugin architecture, but other systems have one entry-point per client. In that case, usage of multi-plugin architecture is useless. Also current realization of ‘rally.osclients’ is one of the few hardcoded places to OpenStack, which prevents to use Rally with wider infrastructure. Alternatives ============ None Proposed change =============== 1. All current clients can be moved into rally.plugins.openstack directory 2. Create new base class for all clients: ..code-block:: python @plugin.base() class Client(plugin.Plugin): def __init__(self, env, system_config, **kwargs): self.env = env self.system_config = system_config # kwargs can be used for client initialization self._kwargs = kwargs 3. Class rally.oscients.Clients should be renamed as OpenstackClients and based on new class Class without any changes of methods of class: ..code-block:: python @plugin.base() class OpenStackClients(Clients): … 4. All new plugins should have the same name, ‘base_client’, for example, and contains following code implementation: ..code-block:: python # data about required systems should be taken from validators system_type = scenario_obj.system_type print(system_type) # for OpenStack scenarios it would be ""openstack"" # common code for all plugins clients = Client.get(name=""base_client"",namespace=system_type) Alternatives ------------ No Implementation ============== Assignee(s) ----------- Primary assignee: astaroverov <astaroverov@mirantis.com> Work Items ---------- rally.osclients Dependencies ============ None ",,101,0
openstack%2Ftacker~master~I2e4533d45efa6b3395fbe757855458c167d6f1a0,openstack/tacker,master,I2e4533d45efa6b3395fbe757855458c167d6f1a0,Implements: Details of reason of ERROR for VNF shown in tacker vnf-event-list Closes-Bug: #1613445,NEW,2016-11-14 07:26:01.000000000,2017-12-18 04:42:36.000000000,,[],"[{'number': 1, 'created': '2016-11-14 07:26:01.000000000', 'files': ['tox.ini', 'tacker/db/vnfm/vnfm_db.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/0627442f4c1d38601d3971c012d291071b9856af', 'message': 'Implements: Details of reason of ERROR for VNF shown in tacker vnf-event-list\nCloses-Bug: #1613445\n\nChange-Id: I2e4533d45efa6b3395fbe757855458c167d6f1a0\n'}]",0,397029,0627442f4c1d38601d3971c012d291071b9856af,3,0,1,23480,,,0,"Implements: Details of reason of ERROR for VNF shown in tacker vnf-event-list
Closes-Bug: #1613445

Change-Id: I2e4533d45efa6b3395fbe757855458c167d6f1a0
",git fetch https://review.opendev.org/openstack/tacker refs/changes/29/397029/1 && git format-patch -1 --stdout FETCH_HEAD,"['tox.ini', 'tacker/db/vnfm/vnfm_db.py']",2,0627442f4c1d38601d3971c012d291071b9856af,,"from tacker.vnfm.infra_drivers.openstack import openstack ""Mgmt URL set: %s. The error reason is %s"") % (instance_id, mgmt_url,openstack.LOG.warning)"," ""Mgmt URL set: %s"") % (instance_id, mgmt_url)",4,2
openstack%2Fpython-gnocchiclient~master~I2b55a30363e2938586f9debd08d9cc190e7b0438,openstack/python-gnocchiclient,master,I2b55a30363e2938586f9debd08d9cc190e7b0438,"Add ""gnocchi metric search resource_id query"" command",NEW,2016-11-07 03:22:04.000000000,2017-12-18 04:42:33.000000000,,"[{'_account_id': 1669}, {'_account_id': 6537}, {'_account_id': 22514}]","[{'number': 1, 'created': '2016-11-07 03:22:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-gnocchiclient/commit/3b94bc57e0429ceeb15f8c35d153b2950de5b247', 'message': 'Add ""gnocchi metric search "" command\n\nNow only support command ""gnocchi metric search metric_id \'> num1 and < num2\'""\nTODO:\n   support command ""gnocchi metric search metric_id \'>= {+ 23} 50\'""\n\nChange-Id: I2b55a30363e2938586f9debd08d9cc190e7b0438\nCloses-Bug: #1635936\n'}, {'number': 2, 'created': '2016-11-07 11:16:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-gnocchiclient/commit/aac3c3ab1d63802e5469aded1a2618b75b4c1372', 'message': 'Add ""gnocchi metric search "" command\n\nNow only support command ""gnocchi metric search metric_id \'> num1 and < num2\'""\nTODO:\n   1. support command ""gnocchi metric search metric_id \'>= {+ 23} 50\'""\n   2. add tests\n\nChange-Id: I2b55a30363e2938586f9debd08d9cc190e7b0438\nCloses-Bug: #1635936\n'}, {'number': 3, 'created': '2016-11-07 12:26:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-gnocchiclient/commit/dc68f72888504bcd631220cb1a9ae8f2ad782bfa', 'message': 'Add ""gnocchi metric search "" command\n\nNow only support command ""gnocchi metric search metric_id \'> num1 and < num2\'""\nTODO:\n   1. support command ""gnocchi metric search metric_id \'>= {+ 23} 50\'""\n   2. add tests\n\nChange-Id: I2b55a30363e2938586f9debd08d9cc190e7b0438\nCloses-Bug: #1635936\n'}, {'number': 4, 'created': '2016-11-08 04:11:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-gnocchiclient/commit/34146bb1c38f61046616695a61b7d2c55b2deb00', 'message': 'Add ""gnocchi metric search "" command\n\nNow only support simple binary operators.\nTODO:\n   1. support arithmetic operators\n   2. add unit tests for unit.py\n\nChange-Id: I2b55a30363e2938586f9debd08d9cc190e7b0438\nCloses-Bug: #1635936\n'}, {'number': 5, 'created': '2016-11-08 06:20:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-gnocchiclient/commit/88042c4e87f9253dbb02eacfcb9ab0788ed0de89', 'message': 'Add ""gnocchi metric search resource_id query"" command\n\nNow only support simple binary operators.\nTODO:\n   support arithmetic operators\n\nChange-Id: I2b55a30363e2938586f9debd08d9cc190e7b0438\nCloses-Bug: #1635936\n'}, {'number': 6, 'created': '2016-11-08 06:46:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-gnocchiclient/commit/843202533c0c1e89bdca084e49523e0f9a43542c', 'message': 'Add ""gnocchi metric search resource_id query"" command\n\nNow only support simple binary operators.\nTODO:\n   support arithmetic operators\n\nChange-Id: I2b55a30363e2938586f9debd08d9cc190e7b0438\nCloses-Bug: #1635936\n'}, {'number': 7, 'created': '2016-11-08 07:10:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-gnocchiclient/commit/06b9294934dbf644f1ea16ac19234a953a664854', 'message': 'Add ""gnocchi metric search resource_id query"" command\n\nNow only support simple binary operators.\nTODO:\n   support arithmetic operators\n\nChange-Id: I2b55a30363e2938586f9debd08d9cc190e7b0438\nCloses-Bug: #1635936\n'}, {'number': 8, 'created': '2016-11-08 07:21:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-gnocchiclient/commit/4e8d86986dee68588f17471f6b948bbe0c3d0a24', 'message': 'Add ""gnocchi metric search resource_id query"" command\n\nNow only support simple binary operators.\nTODO:\n   support arithmetic operators\n\nChange-Id: I2b55a30363e2938586f9debd08d9cc190e7b0438\nCloses-Bug: #1635936\n'}, {'number': 9, 'created': '2016-11-08 07:43:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-gnocchiclient/commit/e9f3b99d05604fb582ade7cc7ea0819381f3683b', 'message': 'Add ""gnocchi metric search resource_id query"" command\n\nNow only support simple binary operators.\nTODO:\n   support arithmetic operators\n\nChange-Id: I2b55a30363e2938586f9debd08d9cc190e7b0438\nCloses-Bug: #1635936\n'}, {'number': 10, 'created': '2016-11-08 08:00:32.000000000', 'files': ['gnocchiclient/v1/resource_cli.py', 'gnocchiclient/utils.py', 'gnocchiclient/tests/unit/test_utils.py', 'gnocchiclient/v1/metric.py', 'gnocchiclient/v1/metric_cli.py', 'gnocchiclient/tests/functional/test_metric.py', 'gnocchiclient/shell.py'], 'web_link': 'https://opendev.org/openstack/python-gnocchiclient/commit/0f2fda4ec1e5f7620bc84f9ec23459f227219a28', 'message': 'Add ""gnocchi metric search resource_id query"" command\n\nNow only support simple binary operators.\nTODO:\n   support arithmetic operators\n\nChange-Id: I2b55a30363e2938586f9debd08d9cc190e7b0438\nCloses-Bug: #1635936\n'}]",1,394245,0f2fda4ec1e5f7620bc84f9ec23459f227219a28,29,3,10,22514,,,0,"Add ""gnocchi metric search resource_id query"" command

Now only support simple binary operators.
TODO:
   support arithmetic operators

Change-Id: I2b55a30363e2938586f9debd08d9cc190e7b0438
Closes-Bug: #1635936
",git fetch https://review.opendev.org/openstack/python-gnocchiclient refs/changes/45/394245/8 && git format-patch -1 --stdout FETCH_HEAD,"['gnocchiclient/v1/resource_cli.py', 'gnocchiclient/utils.py', 'gnocchiclient/v1/metric.py', 'gnocchiclient/v1/metric_cli.py', 'gnocchiclient/shell.py']",5,3b94bc57e0429ceeb15f8c35d153b2950de5b247,bug/1635936," ""metric search"": metric_cli.CliMetricSearch,",,117,23
openstack%2Ftrove~master~If545704967c4b18dd131e63e744eb1df99a957d0,openstack/trove,master,If545704967c4b18dd131e63e744eb1df99a957d0,[WIP] Cluster Upgrade for PXC,NEW,2016-08-29 20:28:30.000000000,2017-12-18 04:42:30.000000000,,"[{'_account_id': 5293}, {'_account_id': 9664}, {'_account_id': 9782}]","[{'number': 1, 'created': '2016-08-29 20:28:30.000000000', 'files': ['trove/common/strategies/cluster/experimental/galera_common/api.py', 'trove/common/strategies/cluster/experimental/galera_common/taskmanager.py', 'trove/instance/tasks.py', 'trove/cluster/tasks.py'], 'web_link': 'https://opendev.org/openstack/trove/commit/34c43c574b5642e18f0f91a14d79e9ed901895eb', 'message': '[WIP] Cluster Upgrade for PXC\n\nImplement cluster upgrade for the Percona XTRADB Cluster database.\n\nChange-Id: If545704967c4b18dd131e63e744eb1df99a957d0\n'}]",1,362416,34c43c574b5642e18f0f91a14d79e9ed901895eb,8,3,1,9782,,,0,"[WIP] Cluster Upgrade for PXC

Implement cluster upgrade for the Percona XTRADB Cluster database.

Change-Id: If545704967c4b18dd131e63e744eb1df99a957d0
",git fetch https://review.opendev.org/openstack/trove refs/changes/16/362416/1 && git format-patch -1 --stdout FETCH_HEAD,"['trove/common/strategies/cluster/experimental/galera_common/api.py', 'trove/common/strategies/cluster/experimental/galera_common/taskmanager.py', 'trove/instance/tasks.py', 'trove/cluster/tasks.py']",4,34c43c574b5642e18f0f91a14d79e9ed901895eb,bp/cluster-upgrade," UPGRADING_CLUSTER = ClusterTask( 0x07, 'UPGRADING_CLUSTER', 'Upgrading the cluster to new version.')",,50,0
openstack%2Frally~master~I7669e78c75fedc11495113a1643005139b7ca624,openstack/rally,master,I7669e78c75fedc11495113a1643005139b7ca624,Port several neutron scenarios to use network ctx,NEW,2016-10-19 12:09:54.000000000,2017-12-18 04:42:28.000000000,,"[{'_account_id': 9545}, {'_account_id': 10475}, {'_account_id': 14817}]","[{'number': 1, 'created': '2016-10-19 12:09:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/66befc782ca532d12ad41e580ddd297ce4626568', 'message': '[WIP] Port several neutron scenario to use network context\n\nChange-Id: I7669e78c75fedc11495113a1643005139b7ca624\n'}, {'number': 2, 'created': '2016-10-19 13:23:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ea6c9789a4c2a3e57f159add73b58b24a0597c9a', 'message': 'Port several neutron scenario to use network context\n\nSeveral Neutron scenarios should not take care about creation of network\nresources. All this logic should be done by Network context.\n\nChanges in this patch:\n- Add new validator ""required_subnet_in_network""  to check number of\n  subnets_per_network configured in network context\n- Add ability to setup Network context to create only networks without\n  subnets. It is needed for next scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n- Deprecate scenario arguments for creation non-related resources(for example:\n  networks, subnets). Add Network context to default contexts. Affected\n  scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n   * NeutronNetworks.create_and_list_ports\n   * NeutronNetworks.create_and_update_ports\n   * NeutronNetworks.create_and_delete_ports\n\nChange-Id: I7669e78c75fedc11495113a1643005139b7ca624\n'}, {'number': 3, 'created': '2016-10-19 14:21:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b805706a40d0a700dc43d03e7fbfbde3d18be702', 'message': 'Port several neutron scenario to use network context\n\nSeveral Neutron scenarios should not take care about creation of network\nresources. All this logic should be done by Network context.\n\nChanges in this patch:\n- Add new validator ""required_subnet_in_network""  to check number of\n  subnets_per_network configured in network context\n- Add ability to setup Network context to create only networks without\n  subnets. It is needed for next scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n- Deprecate scenario arguments for creation non-related resources(for example:\n  networks, subnets). Add Network context to default contexts. Affected\n  scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n   * NeutronNetworks.create_and_list_ports\n   * NeutronNetworks.create_and_update_ports\n   * NeutronNetworks.create_and_delete_ports\n\nChange-Id: I7669e78c75fedc11495113a1643005139b7ca624\n'}, {'number': 4, 'created': '2016-10-19 15:45:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/81f2110a424bb305befd1e18bf1af0cbfdfca0ca', 'message': 'Port several neutron scenario to use network context\n\nSeveral Neutron scenarios should not take care about creation of network\nresources. All this logic should be done by Network context.\n\nChanges in this patch:\n- Add new validator ""required_subnet_in_network""  to check number of\n  subnets_per_network configured in network context\n- Add ability to setup Network context to create only networks without\n  subnets. It is needed for next scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n- Deprecate scenario arguments for creation non-related resources(for example:\n  networks, subnets). Add Network context to default contexts. Affected\n  scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n   * NeutronNetworks.create_and_list_ports\n   * NeutronNetworks.create_and_update_ports\n   * NeutronNetworks.create_and_delete_ports\n\nChange-Id: I7669e78c75fedc11495113a1643005139b7ca624\n'}, {'number': 5, 'created': '2016-10-19 15:48:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/bf0993a885abfe9e87f4b4c6a752a6812184b38a', 'message': 'Port several neutron scenario to use network context\n\nSeveral Neutron scenarios should not take care about creation of network\nresources. All this logic should be done by Network context.\n\nChanges in this patch:\n- Add new validator ""required_subnet_in_network""  to check number of\n  subnets_per_network configured in network context\n- Add ability to setup Network context to create only networks without\n  subnets. It is needed for next scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n- Deprecate scenario arguments for creation non-related resources(for example:\n  networks, subnets). Add Network context to default contexts. Affected\n  scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n   * NeutronNetworks.create_and_list_ports\n   * NeutronNetworks.create_and_update_ports\n   * NeutronNetworks.create_and_delete_ports\n\nChange-Id: I7669e78c75fedc11495113a1643005139b7ca624\n'}, {'number': 6, 'created': '2016-10-19 17:19:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a17cbe4a83ad02d7788d477c840155ada82cac01', 'message': 'Port several neutron scenario to use network context\n\nSeveral Neutron scenarios should not take care about creation of network\nresources. All this logic should be done by Network context.\n\nChanges in this patch:\n- Add new validator ""required_subnet_in_network""  to check number of\n  subnets_per_network configured in network context\n- Add ability to setup Network context to create only networks without\n  subnets. It is needed for next scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n- Deprecate scenario arguments for creation non-related resources(for example:\n  networks, subnets). Add Network context to default contexts. Affected\n  scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n   * NeutronNetworks.create_and_list_ports\n   * NeutronNetworks.create_and_update_ports\n   * NeutronNetworks.create_and_delete_ports\n\nChange-Id: I7669e78c75fedc11495113a1643005139b7ca624\n'}, {'number': 7, 'created': '2016-10-21 16:09:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5b3b00292bbab82e4a8b91c22f90b5c44747948d', 'message': 'Port several neutron scenario to use network context\n\nSeveral Neutron scenarios should not take care about creation of network\nresources. All this logic should be done by Network context.\n\nChanges in this patch:\n- Add new validator ""required_subnet_in_network""  to check number of\n  subnets_per_network configured in network context\n- Add ability to setup Network context to create only networks without\n  subnets. It is needed for next scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n- Deprecate scenario arguments for creation non-related resources(for example:\n  networks, subnets). Add Network context to default contexts. Affected\n  scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n   * NeutronNetworks.create_and_list_ports\n   * NeutronNetworks.create_and_update_ports\n   * NeutronNetworks.create_and_delete_ports\n\nChange-Id: I7669e78c75fedc11495113a1643005139b7ca624\n'}, {'number': 8, 'created': '2016-10-21 17:30:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/68f2d3b55f7e2ed33961dd896c7b97c937f39332', 'message': 'Port several neutron scenario to use network context\n\nSeveral Neutron scenarios should not take care about creation of network\nresources. All this logic should be done by Network context.\n\nChanges in this patch:\n- Add new validator ""required_subnet_in_network""  to check number of\n  subnets_per_network configured in network context\n- Add ability to setup Network context to create only networks without\n  subnets. It is needed for next scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n- Deprecate scenario arguments for creation non-related resources(for example:\n  networks, subnets). Add Network context to default contexts. Affected\n  scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n   * NeutronNetworks.create_and_list_ports\n   * NeutronNetworks.create_and_update_ports\n   * NeutronNetworks.create_and_delete_ports\n\nChange-Id: I7669e78c75fedc11495113a1643005139b7ca624\n'}, {'number': 9, 'created': '2016-10-28 10:21:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/01c423355136c18fa6f95405e3d163a78fee1cbb', 'message': 'Port several neutron scenarios to use network ctx\n\nSeveral Neutron scenarios should not take care about creation of network\nresources. All this logic should be done by Network context.\n\nChanges in this patch:\n- Add ability to setup Network context to create only networks without\n  subnets. It is needed for next scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n- Deprecate scenario arguments for creation non-related resources(for example:\n  networks, subnets). Add Network context to default contexts. Affected\n  scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n   * NeutronNetworks.create_and_list_ports\n   * NeutronNetworks.create_and_update_ports\n   * NeutronNetworks.create_and_delete_ports\n\nChange-Id: I7669e78c75fedc11495113a1643005139b7ca624\n'}, {'number': 10, 'created': '2016-11-08 17:51:06.000000000', 'files': ['tests/unit/plugins/openstack/scenarios/neutron/test_network.py', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'rally/plugins/openstack/context/network/networks.py', 'rally/plugins/openstack/scenarios/neutron/network.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/cb02bf08ee41b9fe3520d116d20940f4de97fcd2', 'message': 'Port several neutron scenarios to use network ctx\n\nSeveral Neutron scenarios should not take care about creation of network\nresources. All this logic should be done by Network context.\n\nChanges in this patch:\n- Add ability to setup Network context to create only networks without\n  subnets. It is needed for next scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n- Deprecate scenario arguments for creation non-related resources(for example:\n  networks, subnets). Add Network context to default contexts. Affected\n  scenarios:\n   * NeutronNetworks.create_and_list_subnets\n   * NeutronNetworks.create_and_update_subnets\n   * NeutronNetworks.create_and_delete_subnets\n   * NeutronNetworks.create_and_list_routers\n   * NeutronNetworks.create_and_update_routers\n   * NeutronNetworks.create_and_delete_routers\n   * NeutronNetworks.create_and_list_ports\n   * NeutronNetworks.create_and_update_ports\n   * NeutronNetworks.create_and_delete_ports\n\nChange-Id: I7669e78c75fedc11495113a1643005139b7ca624\n'}]",1,388652,cb02bf08ee41b9fe3520d116d20940f4de97fcd2,35,3,10,9545,,,0,"Port several neutron scenarios to use network ctx

Several Neutron scenarios should not take care about creation of network
resources. All this logic should be done by Network context.

Changes in this patch:
- Add ability to setup Network context to create only networks without
  subnets. It is needed for next scenarios:
   * NeutronNetworks.create_and_list_subnets
   * NeutronNetworks.create_and_update_subnets
   * NeutronNetworks.create_and_delete_subnets
   * NeutronNetworks.create_and_list_routers
   * NeutronNetworks.create_and_update_routers
   * NeutronNetworks.create_and_delete_routers
- Deprecate scenario arguments for creation non-related resources(for example:
  networks, subnets). Add Network context to default contexts. Affected
  scenarios:
   * NeutronNetworks.create_and_list_subnets
   * NeutronNetworks.create_and_update_subnets
   * NeutronNetworks.create_and_delete_subnets
   * NeutronNetworks.create_and_list_routers
   * NeutronNetworks.create_and_update_routers
   * NeutronNetworks.create_and_delete_routers
   * NeutronNetworks.create_and_list_ports
   * NeutronNetworks.create_and_update_ports
   * NeutronNetworks.create_and_delete_ports

Change-Id: I7669e78c75fedc11495113a1643005139b7ca624
",git fetch https://review.opendev.org/openstack/rally refs/changes/52/388652/8 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/plugins/openstack/scenarios/neutron/test_network.py', 'rally/task/validation.py', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'rally/plugins/openstack/context/network/networks.py', 'rally/plugins/openstack/scenarios/neutron/network.py']",5,66befc782ca532d12ad41e580ddd297ce4626568,neutron,"from rally.common import logging@logging.log_deprecated_args(""Network context should pre-create all required "" ""resources before workload execution"", ""0.8.0"", [""network_create_args""], once=True)@scenario.configure(context={""cleanup"": [""neutron""], ""network"": {""subnets_per_network"": 0}}, def run(self, subnet_create_args=None, subnet_cidr_start=None, subnets_per_network=None, network_create_args=None, ): The scenario creates a given number of subnets and then lists subnets. :param network_create_args: Deprecated and will be ignored network = self._choose_random_network()@logging.log_deprecated_args(""Network context should pre-create all required "" ""resources before workload execution"", ""0.8.0"", [""network_create_args""], once=True)@scenario.configure(context={""cleanup"": [""neutron""], ""network"": {""subnets_per_network"": 0}}, def run(self, subnet_update_args, subnet_create_args=None, subnet_cidr_start=None, subnets_per_network=None, network_create_args=None): The scenario creates a given number of subnets and then updates the subnet. This scenario measures the ""neutron subnet-update"" command performance. :param network_create_args: Deprecated and will be ignored network = self._choose_random_network()@logging.log_deprecated_args(""Network context should pre-create all required "" ""resources before workload execution"", ""0.8.0"", [""network_create_args""], once=True)@scenario.configure(context={""cleanup"": [""neutron""], ""network"": {""subnets_per_network"": 0}}, def run(self, subnet_create_args=None, subnet_cidr_start=None, subnets_per_network=None, network_create_args=None): The scenario creates a given number of subnets and then deletes subnets. :param network_create_args: Deprecated and will be ignored network = self._choose_random_network()@logging.log_deprecated_args(""Network context should pre-create all required "" ""resources before workload execution"", ""0.8.0"", [""network_create_args"", ""subnet_create_args"", ""subnet_cidr_start"", ""subnets_per_network""], once=True) @validation.required_subnet_in_network(min=0)@scenario.configure(context={""cleanup"": [""neutron""], ""network"": {}}, def run(self, router_create_args=None, network_create_args=None, subnet_create_args=None, subnet_cidr_start=None, subnets_per_network=None): Create a router per subnetwork and then list all routers. :param network_create_args: Deprecated and will be ignored :param subnet_create_args: Deprecated and will be ignored :param subnet_cidr_start: Deprecated and will be ignored :param subnets_per_network: Deprecated and will be ignored net = self._choose_random_network()[""network""] for subnet in net[""subnets""]: router = self._create_router(router_create_args or {}) self._add_interface_router(subnet[""subnet""], router[""router""])@logging.log_deprecated_args(""Network context should pre-create all required "" ""resources before workload execution"", ""0.8.0"", [""network_create_args"", ""subnet_create_args"", ""subnet_cidr_start"", ""subnets_per_network""], once=True) @validation.required_subnet_in_network(min=1)@scenario.configure(context={""cleanup"": [""neutron""], ""network"": {}}, def run(self, router_update_args, router_create_args=None, network_create_args=None, subnet_create_args=None, subnet_cidr_start=None, subnets_per_network=None): Create a router per subnetwork and then update all of them. :param network_create_args: Deprecated and will be ignored :param subnet_create_args: Deprecated and will be ignored :param subnet_cidr_start: Deprecated and will be ignored :param subnets_per_network: Deprecated and will be ignored routers = [] net = self._choose_random_network()[""network""] for subnet in net[""subnets""]: router = self._create_router(router_create_args or {}) self._add_interface_router(subnet[""subnet""], router[""router""]) routers.append(router)@logging.log_deprecated_args(""Network context should pre-create all required "" ""resources before workload execution"", ""0.8.0"", [""network_create_args"", ""subnet_create_args"", ""subnet_cidr_start"", ""subnets_per_network""], once=True) @validation.required_subnet_in_network(min=1) def run(self, router_create_args=None, network_create_args=None, subnet_create_args=None, subnet_cidr_start=None, subnets_per_network=None): Create a router per subnetwork and then delete all routers. :param network_create_args: Deprecated and will be ignored :param subnet_create_args: Deprecated and will be ignored :param subnet_cidr_start: Deprecated and will be ignored :param subnets_per_network: Deprecated and will be ignored """""" routers = [] net = self._choose_random_network()[""network""] for subnet in net[""subnets""]: router = self._create_router(router_create_args or {}) self._add_interface_router(subnet[""subnet""], router[""router""]) routers.append(router) for e in range(len(routers)): subnet = net[""subnets""][e]@logging.log_deprecated_args(""Network context should pre-create all required "" ""resources before workload execution"", ""0.8.0"", [""network_create_args""], once=True)@scenario.configure(context={""cleanup"": [""neutron""], ""network"": {}}, def run(self, port_create_args=None, ports_per_network=None, network_create_args=None): :param network_create_args: Deprecated and will be ignored network = self._choose_random_network()@logging.log_deprecated_args(""Network context should pre-create all required "" ""resources before workload execution"", ""0.8.0"", [""network_create_args""], once=True)@scenario.configure(context={""cleanup"": [""neutron""], ""network"": {}}, def run(self, port_update_args, port_create_args=None, ports_per_network=None, network_create_args=None): :param network_create_args: Deprecated and will be ignored network = self._choose_random_network()@logging.log_deprecated_args(""Network context should pre-create all required "" ""resources before workload execution"", ""0.8.0"", [""network_create_args""], once=True) def run(self, network_create_args=None, port_create_args=None, ports_per_network=None): """"""Create and delete a port. Measure the ""neutron port-create"" and ""neutron port-delete"" commands performance. :param port_create_args: dict, POST /v2.0/ports request options :param ports_per_network: int, number of ports for one network :param network_create_args: Deprecated and will be ignored """""" network = self._choose_random_network() for i in range(ports_per_network): port = self._create_port(network, port_create_args) self._delete_port(port)","@scenario.configure(context={""cleanup"": [""neutron""]}, def run(self, network_create_args=None, subnet_create_args=None, subnet_cidr_start=None, subnets_per_network=None): The scenario creates a network, a given number of subnets and then lists subnets. :param network_create_args: dict, POST /v2.0/networks request options. Deprecated network = self._get_or_create_network(network_create_args)@scenario.configure(context={""cleanup"": [""neutron""]}, def run(self, subnet_update_args, network_create_args=None, subnet_create_args=None, subnet_cidr_start=None, subnets_per_network=None): The scenario creates a network, a given number of subnets and then updates the subnet. This scenario measures the ""neutron subnet-update"" command performance. :param network_create_args: dict, POST /v2.0/networks request options. Deprecated. network = self._get_or_create_network(network_create_args)@scenario.configure(context={""cleanup"": [""neutron""]}, def run(self, network_create_args=None, subnet_create_args=None, subnet_cidr_start=None, subnets_per_network=None): The scenario creates a network, a given number of subnets and then deletes subnets. :param network_create_args: dict, POST /v2.0/networks request options. Deprecated. network = self._get_or_create_network(network_create_args)@validation.number(""subnets_per_network"", minval=1, integer_only=True)@scenario.configure(context={""cleanup"": [""neutron""]}, def run(self, network_create_args=None, subnet_create_args=None, subnet_cidr_start=None, subnets_per_network=None, router_create_args=None): Create a network, a given number of subnets and routers and then list all routers. :param network_create_args: dict, POST /v2.0/networks request options. Deprecated. :param subnet_create_args: dict, POST /v2.0/subnets request options :param subnet_cidr_start: str, start value for subnets CIDR :param subnets_per_network: int, number of subnets for one network self._create_network_structure(network_create_args, subnet_create_args, subnet_cidr_start, subnets_per_network, router_create_args)@validation.number(""subnets_per_network"", minval=1, integer_only=True)@scenario.configure(context={""cleanup"": [""neutron""]}, def run(self, router_update_args, network_create_args=None, subnet_create_args=None, subnet_cidr_start=None, subnets_per_network=None, router_create_args=None): Create a network, a given number of subnets and routers and then updating all routers. :param network_create_args: dict, POST /v2.0/networks request options. Deprecated. :param subnet_create_args: dict, POST /v2.0/subnets request options :param subnet_cidr_start: str, start value for subnets CIDR :param subnets_per_network: int, number of subnets for one network network, subnets, routers = self._create_network_structure( network_create_args, subnet_create_args, subnet_cidr_start, subnets_per_network, router_create_args)@validation.required_parameters(""subnets_per_network"") def run(self, network_create_args=None, subnet_create_args=None, subnet_cidr_start=None, subnets_per_network=None, router_create_args=None): Create a network, a given number of subnets and routers and then delete all routers. :param network_create_args: dict, POST /v2.0/networks request options. Deprecated. :param subnet_create_args: dict, POST /v2.0/subnets request options :param subnet_cidr_start: str, start value for subnets CIDR :param subnets_per_network: int, number of subnets for one network """""" network, subnets, routers = self._create_network_structure( network_create_args, subnet_create_args, subnet_cidr_start, subnets_per_network, router_create_args) for e in range(subnets_per_network): subnet = subnets[e]@scenario.configure(context={""cleanup"": [""neutron""]}, def run(self, network_create_args=None, port_create_args=None, ports_per_network=None): :param network_create_args: dict, POST /v2.0/networks request options. Deprecated. network = self._get_or_create_network(network_create_args)@scenario.configure(context={""cleanup"": [""neutron""]}, def run(self, port_update_args, network_create_args=None, port_create_args=None, ports_per_network=None): :param network_create_args: dict, POST /v2.0/networks request options. Deprecated. network = self._get_or_create_network(network_create_args) def run(self, network_create_args=None, port_create_args=None, ports_per_network=None): """"""Create and delete a port. Measure the ""neutron port-create"" and ""neutron port-delete"" commands performance. :param network_create_args: dict, POST /v2.0/networks request options. Deprecated. :param port_create_args: dict, POST /v2.0/ports request options :param ports_per_network: int, number of ports for one network """""" network = self._get_or_create_network(network_create_args) for i in range(ports_per_network): port = self._create_port(network, port_create_args) self._delete_port(port)",173,113
openstack%2Ftacker~master~I8b0e67e5bf78b252434f30f085f65ff54589e819,openstack/tacker,master,I8b0e67e5bf78b252434f30f085f65ff54589e819,small description change,NEW,2016-11-12 00:45:40.000000000,2017-12-18 04:42:26.000000000,,"[{'_account_id': 8253}, {'_account_id': 10487}, {'_account_id': 16237}]","[{'number': 1, 'created': '2016-11-12 00:45:40.000000000', 'files': ['tacker/vnfm/infra_drivers/openstack/openstack.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/14342d71d8187e1ce2a7a35d2620bfd560a4fb3c', 'message': 'small description change\n\nChange-Id: I8b0e67e5bf78b252434f30f085f65ff54589e819\n'}]",4,396806,14342d71d8187e1ce2a7a35d2620bfd560a4fb3c,7,3,1,8253,,,0,"small description change

Change-Id: I8b0e67e5bf78b252434f30f085f65ff54589e819
",git fetch https://review.opendev.org/openstack/tacker refs/changes/06/396806/1 && git format-patch -1 --stdout FETCH_HEAD,['tacker/vnfm/infra_drivers/openstack/openstack.py'],1,14342d71d8187e1ce2a7a35d2620bfd560a4fb3c,nova-id-new," 'description': 'ID of the instance',"," 'description': 'nova instance id',",1,1
openstack%2Ftacker~master~I04f5fbd09b01b5d5dc424dba1b8e414dd4ad243a,openstack/tacker,master,I04f5fbd09b01b5d5dc424dba1b8e414dd4ad243a,adding the instance id as part of heat output list,NEW,2016-11-12 00:34:38.000000000,2017-12-18 04:41:51.000000000,,"[{'_account_id': 8253}, {'_account_id': 10487}, {'_account_id': 16237}]","[{'number': 1, 'created': '2016-11-12 00:34:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/7eb9d8436ea74f2b45fbb4cdde0082e585ad66a7', 'message': 'adding the instance id as part of heat output list\n\nChange-Id: I04f5fbd09b01b5d5dc424dba1b8e414dd4ad243a\n'}, {'number': 2, 'created': '2016-11-14 20:41:55.000000000', 'files': ['tacker/vnfm/infra_drivers/openstack/openstack.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/70300286b44abc7baf9280c4fd092761c441e706', 'message': 'adding the instance id as part of heat output list\n\nChange-Id: I04f5fbd09b01b5d5dc424dba1b8e414dd4ad243a\n'}]",3,396801,70300286b44abc7baf9280c4fd092761c441e706,7,3,2,8253,,,0,"adding the instance id as part of heat output list

Change-Id: I04f5fbd09b01b5d5dc424dba1b8e414dd4ad243a
",git fetch https://review.opendev.org/openstack/tacker refs/changes/01/396801/2 && git format-patch -1 --stdout FETCH_HEAD,['tacker/vnfm/infra_drivers/openstack/openstack.py'],1,7eb9d8436ea74f2b45fbb4cdde0082e585ad66a7,(detached," nova_id = 'nova_id-%s' %vdu_id outputs_dict[nova_id] = { 'description': 'nova instance id', 'value': { 'get_resource': vdu_id } } ",,9,0
openstack%2Fpython-ironicclient~master~Ib9eb7f70f0cca71f443ea7e4c2770a66146b6779,openstack/python-ironicclient,master,Ib9eb7f70f0cca71f443ea7e4c2770a66146b6779,Negative tests for openstack baremetal node commands.,NEW,2016-10-06 15:41:37.000000000,2017-12-18 04:41:02.000000000,,[{'_account_id': 14614}],"[{'number': 1, 'created': '2016-10-06 15:41:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/4aec128e04647194570634182e8e8b1ea49d7f38', 'message': 'Negative tests for openstack baremetal node commands.\n\nNegative test for the openstack baremetal node commands which checks\nactions with node delete, abort, adopt, inspect, maintenance unset,\nmanage, provide, reboot, rebuild, undeploy, commands like use commands\nwithout arguments or with incorrect arguments\nand check that correct error message raised.\n\nPartial-Bug: #1631040\nChange-Id: Ib9eb7f70f0cca71f443ea7e4c2770a66146b6779\n'}, {'number': 2, 'created': '2016-10-07 09:01:31.000000000', 'files': ['ironicclient/tests/functional/osc/v1/test_baremetal_node_delete_negative.py'], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/e0218e17bcdadb0883d050418e71971da200b388', 'message': 'Negative tests for openstack baremetal node commands.\n\nNegative test for the openstack baremetal node commands which checks\nactions with node delete, abort, adopt, inspect, maintenance unset,\nmanage, provide, reboot, rebuild, undeploy, commands like use commands\nwithout arguments or with incorrect arguments\nand check that correct error message raised.\n\nPartial-Bug: #1631040\nChange-Id: Ib9eb7f70f0cca71f443ea7e4c2770a66146b6779\n'}]",4,383044,e0218e17bcdadb0883d050418e71971da200b388,7,1,2,17270,,,0,"Negative tests for openstack baremetal node commands.

Negative test for the openstack baremetal node commands which checks
actions with node delete, abort, adopt, inspect, maintenance unset,
manage, provide, reboot, rebuild, undeploy, commands like use commands
without arguments or with incorrect arguments
and check that correct error message raised.

Partial-Bug: #1631040
Change-Id: Ib9eb7f70f0cca71f443ea7e4c2770a66146b6779
",git fetch https://review.opendev.org/openstack/python-ironicclient refs/changes/44/383044/1 && git format-patch -1 --stdout FETCH_HEAD,['ironicclient/tests/functional/osc/v1/test_baremetal_node_delete_negative.py'],1,4aec128e04647194570634182e8e8b1ea49d7f38,bug/1631040,"# Copyright (c) 2016 Mirantis, Inc. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import ddt import six from tempest.lib import exceptions from ironicclient.tests.functional.osc.v1 import base @ddt.ddt class BaremetalNodeNegativeTests(base.TestCase): """"""Negative tests for node commands."""""" def setUp(self): super(BaremetalNodeNegativeTests, self).setUp() @ddt.data( ('delete', '', 'too few arguments'), ('delete', '!@#$^*&%^', 'Invalid input for field'), ('delete', '0000 0000', 'could not be found'), ('abort', '', 'too few arguments'), ('abort', '!@#$^*&%^', 'Method Not Allowed'), ('abort', '0000 0000', 'unrecognized arguments'), ('adopt', '', 'too few arguments'), ('adopt', '!@#$^*&%^', 'Method Not Allowed'), ('adopt', '0000 0000', 'unrecognized arguments'), ('adopt', '', 'too few arguments'), ('adopt', '!@#$^*&%^', 'Method Not Allowed'), ('adopt', '0000 0000', 'unrecognized arguments'), ('inspect', '', 'too few arguments'), ('inspect', '!@#$^*&%^', 'Method Not Allowed'), ('inspect', '0000 0000', 'unrecognized arguments'), ('maintenance unset', '', 'too few arguments'), ('maintenance unset', '!@#$^*&%^', 'unable to convert to uuid_or_name'), ('maintenance unset', '0000 0000', 'unrecognized arguments'), ('manage', '', 'too few arguments'), ('manage', '!@#$^*&%^', 'Method Not Allowed'), ('manage', '0000 0000', 'unrecognized arguments'), ('provide', '', 'too few arguments'), ('provide', '!@#$^*&%^', 'Method Not Allowed'), ('provide', '0000 0000', 'unrecognized arguments'), ('reboot', '', 'too few arguments'), ('reboot', '!@#$^*&%^', 'Method Not Allowed'), ('reboot', '0000 0000', 'unrecognized arguments'), ('rebuild', '', 'too few arguments'), ('rebuild', '!@#$^*&%^', 'Method Not Allowed'), ('rebuild', '0000 0000', 'unrecognized arguments'), ('undeploy', '', 'too few arguments'), ('undeploy', '!@#$^*&%^', 'Method Not Allowed'), ('undeploy', '0000 0000', 'unrecognized arguments'), ) @ddt.unpack def test_baremetal_node(self, argument, value, ex_text): base_cmd = 'baremetal node' command = '{0} {1} {2}'.format(base_cmd, argument, value) six.assertRaisesRegex(self, exceptions.CommandFailed, ex_text, self.openstack, command) ",,70,0
openstack%2Fvitrage-specs~master~I687c3700be44d17944796ae9637938bcb5901023,openstack/vitrage-specs,master,I687c3700be44d17944796ae9637938bcb5901023,Replace deprecated library function os.popen() with subprocess,NEW,2016-01-14 14:30:38.000000000,2017-12-18 04:40:27.000000000,,[],"[{'number': 1, 'created': '2016-01-14 14:30:38.000000000', 'files': ['doc/source/conf.py'], 'web_link': 'https://opendev.org/openstack/vitrage-specs/commit/48daf996134cdd39b97a387d8dd345ee06696e34', 'message': 'Replace deprecated library function os.popen() with subprocess\n\nos.popen() is deprecated since version 2.6. Resolved with use of\nsubprocess module.\n\nChange-Id: I687c3700be44d17944796ae9637938bcb5901023\nCloses-Bug: #1529836\n'}]",0,267597,48daf996134cdd39b97a387d8dd345ee06696e34,4,0,1,10984,,,0,"Replace deprecated library function os.popen() with subprocess

os.popen() is deprecated since version 2.6. Resolved with use of
subprocess module.

Change-Id: I687c3700be44d17944796ae9637938bcb5901023
Closes-Bug: #1529836
",git fetch https://review.opendev.org/openstack/vitrage-specs refs/changes/97/267597/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/conf.py'],1,48daf996134cdd39b97a387d8dd345ee06696e34,1529836,"import subprocessgit_cmd = [""git"", ""log"", ""--pretty=format:'%ad, commit %h'"", ""--date=local"", ""-n1""] html_last_updated_fmt = subprocess.Popen( git_cmd, stdout=subprocess.PIPE).communicate()[0]#epub_tocdup = True ","git_cmd = ""git log --pretty=format:'%ad, commit %h' --date=local -n1"" html_last_updated_fmt = os.popen(git_cmd).read()#epub_tocdup = True",6,3
openstack%2Frally~master~I0ddd3c8ce616c62fcc50c5254b3f4f6762483091,openstack/rally,master,I0ddd3c8ce616c62fcc50c5254b3f4f6762483091,New base class for rally.osclients.Clients,NEW,2016-10-13 14:53:49.000000000,2017-12-18 04:40:17.000000000,,"[{'_account_id': 7369}, {'_account_id': 9545}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 21528}, {'_account_id': 23094}]","[{'number': 1, 'created': '2016-10-13 14:53:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/9b298a9bffd93494e665f07f3de9663fdf6b5cb8', 'message': '[WIP][DoNotReview]New base class for rally.osclients.Clients\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n'}, {'number': 2, 'created': '2016-10-14 08:19:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e3200caf9ad5f3ec30616a3e1fb0bd9c69006af3', 'message': '[WIP][DoNotReview]New base class for rally.osclients.Clients\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n'}, {'number': 3, 'created': '2016-10-14 09:55:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4a338dac6ead946239f3c27d8f60fb1bae409f50', 'message': '[WIP][DoNotReview]New base class for rally.osclients.Clients\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n'}, {'number': 4, 'created': '2016-10-14 11:13:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d65bb06a03bcd4d159726a1d1e865bcc46cf2213', 'message': '[WIP][DoNotReview]New base class for rally.osclients.Clients\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n'}, {'number': 5, 'created': '2016-10-17 13:16:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/854f783ad48b3bab96935c8e97d316653962b195', 'message': '[WIP][DoNotReview]New base class for rally.osclients.Clients\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n'}, {'number': 6, 'created': '2016-10-17 13:46:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d2ac2c054908700c027c5609560c0a73b56e5583', 'message': '[WIP][DoNotReview]New base class for rally.osclients.Clients\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n'}, {'number': 7, 'created': '2016-10-19 08:53:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8ca92c1862ee0d9f67a647e8555043ab06b2f9f9', 'message': '[WIP][DoNotReview]New base class for rally.osclients.Clients\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n'}, {'number': 8, 'created': '2016-10-19 11:53:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/34fe20f12764bdf3ad5b19ce18dda09052215cb7', 'message': '[WIP][DoNotReview]New base class for rally.osclients.Clients\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n'}, {'number': 9, 'created': '2016-10-19 16:04:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e99b5aa5673ddcbb96186243c769c13158997377', 'message': '[WIP][DoNotReview]New base class for rally.osclients.Clients\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n'}, {'number': 10, 'created': '2016-10-20 08:11:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/3eb7a5d922b86d863b479b7d1506198bad05ae3e', 'message': '[WIP][DoNotReview]New base class for rally.osclients.Clients\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n'}, {'number': 11, 'created': '2016-10-20 09:42:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/594f01c82292ce9ac63e57d90816287b12413833', 'message': '[WIP][DoNotReview]New base class for rally.osclients.Clients\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n'}, {'number': 12, 'created': '2016-10-20 12:16:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/2b7ef06e7e520d4e98c91d7e79dc8894bcddb8f1', 'message': '[WIP][DoNotReview]New base class for rally.osclients.Clients\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n'}, {'number': 13, 'created': '2016-10-20 13:37:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/3e192c2821dcdf6d27920842aeac6af9e4e4bc3e', 'message': ""New base class for rally.osclients.Clients\n\nMoving of OpenStackClients class on plugin base.\nBase class was added into osclients.py, name of class 'Clients' was\nchanged on 'OpenStackClients'\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n""}, {'number': 14, 'created': '2016-10-21 08:13:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1023d070f3da5987c79f58bf3b3384ceb48c964d', 'message': ""New base class for rally.osclients.Clients\n\nMoving of OpenStackClients class on plugin base.\nBase class was added into osclients.py, name of class 'Clients' was\nchanged on 'OpenStackClients'\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n""}, {'number': 15, 'created': '2016-10-24 11:59:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/525034053ae292f42824230eff7c798ec3dd8f17', 'message': ""[WIP]New base class for rally.osclients.Clients\n\nMoving of OpenStackClients class on plugin base.\nBase class was added into osclients.py, name of class 'Clients' was\nchanged on 'OpenStackClients'\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n""}, {'number': 16, 'created': '2016-11-01 12:08:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/fb840d78650b7f7edbefb84a62fa7d278764a4a5', 'message': ""New base class for rally.osclients.Clients\n\nMoving of OpenStackClients class on plugin base.\nBase class was added into osclients.py, name of class 'Clients' was\nchanged on 'OpenStackClients'\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n""}, {'number': 17, 'created': '2016-11-03 13:38:40.000000000', 'files': ['tests/unit/plugins/openstack/context/swift/test_objects.py', 'tests/unit/plugins/openstack/context/keystone/test_existing_users.py', 'tests/unit/plugins/openstack/context/swift/test_utils.py', 'tests/unit/plugins/openstack/context/sahara/test_sahara_image.py', 'tests/unit/plugins/openstack/context/neutron/test_lbaas.py', 'tests/unit/plugins/openstack/context/test_api_versions.py', 'rally/task/engine.py', 'tests/unit/plugins/openstack/context/keystone/test_roles.py', 'rally/api.py', 'tests/unit/task/test_types.py', 'tests/unit/cli/commands/test_show.py', 'rally/task/validation.py', 'tests/unit/plugins/openstack/context/vm/test_custom_image.py', 'tests/unit/plugins/openstack/context/network/test_allow_ssh.py', 'rally/plugins/openstack/context/keystone/existing_users.py', 'rally/plugins/openstack/context/keystone/roles.py', 'rally/plugins/openstack/context/network/existing_network.py', 'rally/plugins/openstack/context/network/allow_ssh.py', 'tests/unit/plugins/openstack/context/manila/test_manila_share_networks.py', 'tests/unit/plugins/openstack/context/sahara/test_sahara_input_data_sources.py', 'tests/ci/osresources.py', 'rally/plugins/openstack/context/neutron/lbaas.py', 'rally/plugins/openstack/context/murano/murano_packages.py', 'rally/plugins/openstack/context/nova/servers.py', 'rally/plugins/openstack/osclients.py', 'rally/plugins/openstack/context/vm/custom_image.py', 'rally/plugins/openstack/context/sahara/sahara_image.py', 'tests/unit/plugins/openstack/context/nova/test_keypairs.py', 'rally/task/types.py', 'tests/ci/rally_verify.py', 'tests/unit/plugins/openstack/context/glance/test_images.py', 'rally/deployment/serverprovider/providers/openstack.py', 'tests/unit/plugins/openstack/scenarios/cinder/test_utils.py', 'rally/plugins/openstack/context/watcher/audit_templates.py', 'tests/unit/plugins/openstack/cleanup/test_manager.py', 'tests/unit/plugins/openstack/context/nova/test_flavors.py', 'samples/plugins/context/context_plugin.py', 'tests/unit/plugins/openstack/context/neutron/test_existing_network.py', 'tests/unit/task/test_engine.py', 'tests/unit/deployment/serverprovider/providers/test_openstack.py', 'rally/plugins/openstack/context/nova/flavors.py', 'rally/osclients.py', 'rally/plugins/openstack/context/sahara/sahara_output_data_sources.py', 'tests/unit/verification/test_config.py', 'rally/plugins/openstack/context/network/networks.py', 'tests/unit/plugins/openstack/context/dataplane/test_heat.py', 'rally/plugins/openstack/context/quotas/quotas.py', 'rally/cli/commands/show.py', 'tests/unit/test_api.py', 'rally/plugins/openstack/context/ec2/servers.py', 'tests/unit/plugins/openstack/context/sahara/test_sahara_output_data_sources.py', 'tests/unit/plugins/openstack/context/network/test_network.py', 'tests/unit/plugins/openstack/context/quotas/test_quotas.py', 'rally/plugins/openstack/wrappers/network.py', 'tests/unit/plugins/openstack/test_scenario.py', 'tests/unit/test.py', 'tests/unit/cli/commands/test_verify.py', 'tests/unit/plugins/openstack/context/sahara/test_sahara_job_binaries.py', 'rally/plugins/openstack/scenario.py', 'rally/plugins/openstack/context/dataplane/heat.py', 'rally/verification/tempest/config.py', 'rally/plugins/openstack/context/glance/images.py', 'rally/plugins/openstack/context/api_versions.py', 'rally/plugins/openstack/context/sahara/sahara_input_data_sources.py', 'rally/plugins/openstack/context/keystone/users.py', 'tests/unit/test_osclients.py', 'rally/plugins/openstack/cleanup/manager.py', 'tests/unit/plugins/openstack/context/keystone/test_users.py', 'rally/plugins/openstack/scenarios/fuel/utils.py', 'tests/unit/task/test_validation.py', 'rally/plugins/openstack/context/nova/keypairs.py', 'tests/unit/plugins/openstack/context/murano/test_murano_packages.py', 'rally/plugins/openstack/context/sahara/sahara_job_binaries.py', 'rally/plugins/openstack/context/cinder/volume_types.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/5f5c3b4d5392d32274d31b1d20c978d9264934c5', 'message': ""New base class for rally.osclients.Clients\n\nMoving of OpenStackClients class on plugin base.\nBase class was added into osclients.py, name of class 'Clients' was\nchanged on 'OpenStackClients'\n\nChange-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091\n""}]",9,386026,5f5c3b4d5392d32274d31b1d20c978d9264934c5,84,6,17,23094,,,0,"New base class for rally.osclients.Clients

Moving of OpenStackClients class on plugin base.
Base class was added into osclients.py, name of class 'Clients' was
changed on 'OpenStackClients'

Change-Id: I0ddd3c8ce616c62fcc50c5254b3f4f6762483091
",git fetch https://review.opendev.org/openstack/rally refs/changes/26/386026/16 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/plugins/openstack/context/swift/test_objects.py', 'tests/unit/plugins/openstack/context/keystone/test_existing_users.py', 'tests/unit/plugins/openstack/context/swift/test_utils.py', 'tests/unit/plugins/openstack/context/sahara/test_sahara_image.py', 'tests/unit/plugins/openstack/context/neutron/test_lbaas.py', 'tests/unit/plugins/openstack/context/test_api_versions.py', 'rally/task/engine.py', 'rally/api.py', 'tests/unit/task/test_types.py', 'rally/task/validation.py', 'tests/unit/plugins/openstack/context/vm/test_custom_image.py', 'tests/unit/plugins/openstack/context/network/test_allow_ssh.py', 'rally/plugins/openstack/context/keystone/existing_users.py', 'rally/plugins/openstack/context/network/existing_network.py', 'rally/plugins/openstack/context/network/allow_ssh.py', 'tests/unit/plugins/openstack/context/manila/test_manila_share_networks.py', 'rally/plugins/openstack/context/neutron/lbaas.py', 'rally/plugins/openstack/context/vm/custom_image.py', 'rally/plugins/openstack/context/sahara/sahara_image.py', 'tests/unit/plugins/openstack/context/nova/test_keypairs.py', 'rally/task/types.py', 'tests/unit/plugins/openstack/context/glance/test_images.py', 'rally/deployment/serverprovider/providers/openstack.py', 'tests/unit/plugins/openstack/scenarios/cinder/test_utils.py', 'tests/unit/plugins/openstack/cleanup/test_manager.py', 'tests/unit/plugins/openstack/context/nova/test_flavors.py', 'tests/unit/plugins/openstack/context/neutron/test_existing_network.py', 'tests/unit/task/test_engine.py', 'tests/unit/deployment/serverprovider/providers/test_openstack.py', 'rally/plugins/openstack/context/nova/flavors.py', 'rally/osclients.py', 'tests/unit/verification/test_config.py', 'rally/plugins/openstack/context/network/networks.py', 'tests/unit/plugins/openstack/context/dataplane/test_heat.py', 'rally/plugins/openstack/context/quotas/quotas.py', 'rally/cli/commands/show.py', 'tests/unit/test_api.py', 'tests/unit/plugins/openstack/context/network/test_network.py', 'tests/unit/plugins/openstack/context/quotas/test_quotas.py', 'tests/unit/plugins/openstack/test_scenario.py', 'tests/unit/test.py', 'tests/unit/cli/commands/test_verify.py', 'rally/plugins/openstack/scenario.py', 'rally/plugins/openstack/context/dataplane/heat.py', 'rally/verification/tempest/config.py', 'rally/plugins/openstack/context/glance/images.py', 'rally/plugins/openstack/context/api_versions.py', 'tests/unit/test_osclients.py', 'rally/plugins/openstack/cleanup/manager.py', 'tests/unit/task/test_validation.py', 'rally/plugins/openstack/context/nova/keypairs.py', 'rally/plugins/openstack/context/cinder/volume_types.py']",52,9b298a9bffd93494e665f07f3de9663fdf6b5cb8,astarove-osclients, admin_clients = osclients.OpenStackClients( admin_clients = osclients.OpenStackClients(, admin_clients = osclients.Clients( admin_clients = osclients.Clients(,301,253
openstack%2Frally~master~I67df752d7b42ce9b9ad6b07b144a8f5a104bd036,openstack/rally,master,I67df752d7b42ce9b9ad6b07b144a8f5a104bd036,Add NeutronNetworks.create_floating_ip_with_given_ip scenario,NEW,2016-09-30 09:45:32.000000000,2017-12-18 04:40:08.000000000,,"[{'_account_id': 10475}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 23668}]","[{'number': 1, 'created': '2016-09-30 09:45:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/85385ca7a41f6e9eff91780176c66fd7fb515a4b', 'message': 'Add NeutronNetworks.create_and_list_static_floating_ips\nThis scenario first create a static floatingip and then list static floating IPs\n\nChange-Id: I67df752d7b42ce9b9ad6b07b144a8f5a104bd036\n'}, {'number': 2, 'created': '2016-10-12 06:51:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/953d5ebc4d37563c41cd32621ccf9049565d6947', 'message': 'Add NeutronNetworks.create_and_list_static_floating_ips\n\nThe scenario first create a  floatingip with a specified IP address\nand then list static floating IPs, finally delete the floatingip.\n\nChange-Id: I67df752d7b42ce9b9ad6b07b144a8f5a104bd036\n'}, {'number': 3, 'created': '2016-10-14 08:10:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/f1ef8cc77f27c26cb941dcb47d618e9cfef4a326', 'message': 'Add NeutronNetworks.create_list_and_delete_floating_ip\n\nThe scenario first create a  floatingip with a specified IP address\nand then list static floating IPs, finally delete the floatingip.\n\nChange-Id: I67df752d7b42ce9b9ad6b07b144a8f5a104bd036\n'}, {'number': 4, 'created': '2016-11-09 09:33:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6767fcafd6a4200e97759b447c7532e96ab20f63', 'message': 'Add NeutronNetworks.create_list_and_delete_floating_ip\n\nThe scenario first create a  floatingip with a specified IP address\nand then list static floating IPs, finally delete the floatingip.\n\nChange-Id: I67df752d7b42ce9b9ad6b07b144a8f5a104bd036\n'}, {'number': 5, 'created': '2016-11-14 09:34:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d683d70f237a0d866bc9f3b625172f298a5819e1', 'message': 'Add NeutronNetworks.create_list_and_delete_floating_ip scenario\n\nThe scenario first create a  floatingip with a specified IP address\nand then list static floating IPs, finally delete the floatingip.\n\nChange-Id: I67df752d7b42ce9b9ad6b07b144a8f5a104bd036\n'}, {'number': 6, 'created': '2016-11-17 09:29:40.000000000', 'files': ['tests/unit/plugins/openstack/scenarios/neutron/test_network.py', 'samples/tasks/scenarios/neutron/create-floating-ip-with-given-ip.json', 'rally-jobs/rally-neutron.yaml', 'samples/tasks/scenarios/neutron/create-floating-ip-with-given-ip.yaml', 'rally/plugins/openstack/scenarios/neutron/network.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/d6e1ab88f81da38ebdf881d779ffe455f4dbe376', 'message': 'Add NeutronNetworks.create_floating_ip_with_given_ip scenario\n\nThe scenario create a floatingip with a given ip address.\n\nChange-Id: I67df752d7b42ce9b9ad6b07b144a8f5a104bd036\n'}]",20,380060,d6e1ab88f81da38ebdf881d779ffe455f4dbe376,28,4,6,23668,,,0,"Add NeutronNetworks.create_floating_ip_with_given_ip scenario

The scenario create a floatingip with a given ip address.

Change-Id: I67df752d7b42ce9b9ad6b07b144a8f5a104bd036
",git fetch https://review.opendev.org/openstack/rally refs/changes/60/380060/4 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/plugins/openstack/scenarios/neutron/test_network.py', 'samples/tasks/scenarios/neutron/create-and-list-static-floating-ips.json', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'rally-jobs/rally-neutron.yaml', 'samples/tasks/scenarios/neutron/create-and-list-static-floating-ips.yaml', 'rally/plugins/openstack/scenarios/neutron/network.py', 'tests/unit/plugins/openstack/scenarios/neutron/test_utils.py']",7,85385ca7a41f6e9eff91780176c66fd7fb515a4b,neutron.test_create_floating_ip_with_given_ip," {""floating_ip_args"": {""floating_ip_address"": ""193.168.73.88""}, ""floating_network"": ""public"", ""floating_ip_address"": ""193.168.73.88""}, ) @ddt.unpack def test__create_static_floating_ip(self, floating_network, floating_ip_address, floating_ip_args=None): floating_network = ""public"" network_id = ""net-id"" fip = {""floatingip"": {""id"": ""fip-id""}} floating_ip_args = floating_ip_args or {} self.clients(""neutron"").create_floatingip.return_value = fip mock_get_network_id = self.scenario._get_network_id = mock.Mock() mock_get_network_id.return_value = network_id args = {""floating_network_id"": network_id, ""floating_ip_address"": floating_ip_address} args.update(floating_ip_args) expected_fip_data = {""floatingip"": args} resultant_fip = self.scenario._create_floatingip( floating_network, **floating_ip_args) self.assertEqual(resultant_fip, fip) self.clients(""neutron"").create_floatingip.assert_called_once_with( expected_fip_data) mock_get_network_id.assert_called_once_with(floating_network) self._test_atomic_action_timer(self.scenario.atomic_actions(), ""neutron.create_floating_ip"") @ddt.data(",,155,0
openstack%2Fpython-ironicclient~master~Ie9877961aa49f30408ff0060bce4701e23335ac9,openstack/python-ironicclient,master,Ie9877961aa49f30408ff0060bce4701e23335ac9,Make console commands support multiple node operation,NEW,2016-09-28 09:41:13.000000000,2017-12-18 04:39:58.000000000,,[],"[{'number': 1, 'created': '2016-09-28 09:41:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/a22ac26e51804f25ef9bcb83ae87c26bb0fba838', 'message': 'Make console commands support multiple node operation\n\nThis patch improve ""baremetal node console enable/disable""\ncommands to support multiple node operation.\n\nChange-Id: Ie9877961aa49f30408ff0060bce4701e23335ac9\nPartial-bug: #1526490\n'}, {'number': 2, 'created': '2016-09-29 01:38:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/86782a9fb123bf31eb763e4680c15e2e59301868', 'message': 'Make console commands support multiple node operation\n\nThis patch improve ""baremetal node console enable/disable""\ncommands to support multiple node operation.\n\nChange-Id: Ie9877961aa49f30408ff0060bce4701e23335ac9\nPartial-bug: #1526490\n'}, {'number': 3, 'created': '2016-10-11 10:25:11.000000000', 'files': ['ironicclient/tests/unit/osc/v1/test_baremetal_node.py', 'ironicclient/osc/v1/baremetal_node.py'], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/9fa8d4095718741b9f494592be960eef33abfbad', 'message': 'Make console commands support multiple node operation\n\nThis patch improve ""baremetal node console enable/disable""\ncommands to support multiple node operation.\n\nChange-Id: Ie9877961aa49f30408ff0060bce4701e23335ac9\nPartial-bug: #1526490\n'}]",0,378477,9fa8d4095718741b9f494592be960eef33abfbad,8,0,3,14937,,,0,"Make console commands support multiple node operation

This patch improve ""baremetal node console enable/disable""
commands to support multiple node operation.

Change-Id: Ie9877961aa49f30408ff0060bce4701e23335ac9
Partial-bug: #1526490
",git fetch https://review.opendev.org/openstack/python-ironicclient refs/changes/77/378477/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironicclient/osc/v1/baremetal_node.py', 'ironicclient/tests/unit/osc/v1/test_baremetal_node.py']",2,a22ac26e51804f25ef9bcb83ae87c26bb0fba838,bug/1526490," verifylist = [('node', ['node_uuid'])] def test_console_disable_multiple(self): arglist = ['node_uuid', 'node_name'] verifylist = [('node', ['node_uuid', 'node_name'])] parsed_args = self.check_parser(self.cmd, arglist, verifylist) self.cmd.take_action(parsed_args) # Set expected values args = [('node_uuid', False), ('node_name', False)] self.baremetal_mock.node.set_console_mode.has_calls( [mock.call(x) for x in args] ) self.assertEqual(2, self.baremetal_mock.node.set_console_mode.call_count) def test_console_disable_multiple_with_failure(self): arglist = ['node_uuid', 'badname'] verifylist = [('node', ['node_uuid', 'badname'])] self.baremetal_mock.node.set_console_mode.side_effect = \ ['', exc.ClientException] parsed_args = self.check_parser(self.cmd, arglist, verifylist) self.assertRaises(exc.ClientException, self.cmd.take_action, parsed_args) # Set expected values args = [('node_uuid', False), ('badname', False)] self.baremetal_mock.node.set_console_mode.has_calls( [mock.call(x) for x in args] ) self.assertEqual(2, self.baremetal_mock.node.set_console_mode.call_count) verifylist = [('node', ['node_uuid'])] def test_console_enable_multiple(self): arglist = ['node_uuid', 'node_name'] verifylist = [('node', ['node_uuid', 'node_name'])] parsed_args = self.check_parser(self.cmd, arglist, verifylist) self.cmd.take_action(parsed_args) # Set expected values args = [('node_uuid', True), ('node_name', True)] self.baremetal_mock.node.set_console_mode.has_calls( [mock.call(x) for x in args] ) self.assertEqual(2, self.baremetal_mock.node.set_console_mode.call_count) def test_console_enable_multiple_with_failure(self): arglist = ['node_uuid', 'badname'] verifylist = [('node', ['node_uuid', 'badname'])] self.baremetal_mock.node.set_console_mode.side_effect = \ ['', exc.ClientException] parsed_args = self.check_parser(self.cmd, arglist, verifylist) self.assertRaises(exc.ClientException, self.cmd.take_action, parsed_args) # Set expected values args = [('node_uuid', True), ('badname', True)] self.baremetal_mock.node.set_console_mode.has_calls( [mock.call(x) for x in args] ) self.assertEqual(2, self.baremetal_mock.node.set_console_mode.call_count) "," verifylist = [('node', 'node_uuid')] verifylist = [('node', 'node_uuid')]",123,6
openstack%2Fswift~master~If5bd096efa08c48bdecb2e1d749f886347e14e99,openstack/swift,master,If5bd096efa08c48bdecb2e1d749f886347e14e99,Use direct_get_suffix_hashes in the reconstructor,NEW,2016-11-07 17:52:23.000000000,2017-12-18 04:37:39.000000000,,"[{'_account_id': 1179}, {'_account_id': 7847}, {'_account_id': 12261}, {'_account_id': 13052}, {'_account_id': 15343}, {'_account_id': 16896}, {'_account_id': 18334}, {'_account_id': 23739}]","[{'number': 1, 'created': '2016-11-07 17:52:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/fb420dc5987f3e840c1a8567561c95e86ec13629', 'message': 'Use direct_get_suffix_hashes in the reconstructor\n\nThe reconstructor uses the direct_get_suffix_hashes function\ninstead of generating http verbs by hand.\nError handling will be done with exception handling.\nIf rehash-remote fails an error message\nwith backtrace will be written.\n\nThis commit depends on the commit of Ondřej Nový\nat\nOct 29 6:16 PM\nCloses-Bug: 1550565\n\nChange-Id: If5bd096efa08c48bdecb2e1d749f886347e14e99\n'}, {'number': 2, 'created': '2016-11-07 18:46:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/42fad00b8a31a26a0eef76c12c477eb0fc30179a', 'message': 'Use direct_get_suffix_hashes in the reconstructor\n\nThe reconstructor uses the direct_get_suffix_hashes function\ninstead of generating http verbs by hand.\nError handling will be done with exception handling.\nIf rehash-remote fails an error message\nwith backtrace will be written.\n\nThis commit depends on the commit of Ondřej Nový\nat\nOct 29 6:16 PM\nCloses-Bug: 1550565\n\nChange-Id: If5bd096efa08c48bdecb2e1d749f886347e14e99\n'}, {'number': 3, 'created': '2016-11-07 19:23:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ed8d22757c39cdc21a66fcf007a99f421c429731', 'message': 'Use direct_get_suffix_hashes in the reconstructor\n\nThe reconstructor uses the direct_get_suffix_hashes function\ninstead of generating http verbs by hand.\nError handling will be done with exception handling.\nIf rehash-remote fails an error message\nwith backtrace will be written.\n\nThis commit depends on the commit of Ondřej Nový\nat\nOct 29 6:16 PM\nCloses-Bug: 1550565\n\nChange-Id: If5bd096efa08c48bdecb2e1d749f886347e14e99\n'}, {'number': 4, 'created': '2016-11-08 11:38:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2867d1e10be5e36dbe2a203786d11fe7456f2233', 'message': 'Use direct_get_suffix_hashes in the reconstructor\n\nThe reconstructor uses the direct_get_suffix_hashes function\ninstead of generating http verbs by hand.\nError handling will be done with exception handling.\nIf rehash-remote fails an error message\nwith backtrace will be written.\n\nThis commit depends on the commit of Ondřej Nový\nat\nOct 29 6:16 PM\nCloses-Bug: 1550565\n\nChange-Id: If5bd096efa08c48bdecb2e1d749f886347e14e99\n'}, {'number': 5, 'created': '2016-11-15 12:23:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e12d1b1ff5fdb64eb6769d64ef29c1ff69e7ad24', 'message': 'Use direct_get_suffix_hashes in the reconstructor\n\nThe reconstructor uses the direct_get_suffix_hashes function\ninstead of generating http verbs by hand.\nError handling will be done with exception handling.\nIf rehash-remote fails an error message\nwith backtrace will be written.\n\nThis commit depends on the commit of Ondřej Nový\nat\nOct 29 6:16 PM\nCloses-Bug: 1550565\n\nChange-Id: If5bd096efa08c48bdecb2e1d749f886347e14e99\n'}, {'number': 6, 'created': '2016-11-15 21:27:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/baba4bf425cf4aaf308127e7221feafdc27d5506', 'message': 'Use direct_get_suffix_hashes in the reconstructor\n\nThe reconstructor uses the direct_get_suffix_hashes function\ninstead of generating http verbs by hand.\nError handling will be done with exception handling.\nIf rehash-remote fails an error message\nwill be written as well.\n\nCloses-Bug: 1550565\nPartial-Bug: 1550565\n\nChange-Id: If5bd096efa08c48bdecb2e1d749f886347e14e99\nRelated-Change: Icb723ee9eada8728f14eca919b982492c98a24bb\n'}, {'number': 7, 'created': '2016-11-19 20:57:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2cb845fc07993af8c8cfdf17a265c578ccf5f1d9', 'message': 'Use direct_get_suffix_hashes in the reconstructor\n\nThe reconstructor uses the direct_get_suffix_hashes function\ninstead of generating http verbs by hand.\nError handling will be done with exception handling.\nIf rehash-remote fails an error message\nwill be written as well.\n\nPartial-Bug: 1550565\n\nsquashed with: I157df1397ff35a3e05e05a353b2f11df698e4acd\nMake direct_client not overrider User-Agent\n\nChange-Id: If5bd096efa08c48bdecb2e1d749f886347e14e99\nRelated-Change: Icb723ee9eada8728f14eca919b982492c98a24bb\n'}, {'number': 8, 'created': '2016-11-19 21:41:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ab7ab0111d66ac6ae3a2a75e1ac77839de34f0d8', 'message': 'Use direct_get_suffix_hashes in the reconstructor\n\nThe reconstructor uses the direct_get_suffix_hashes function\ninstead of generating http verbs by hand.\nError handling will be done with exception handling.\nIf rehash-remote fails an error message\nwill be written as well.\n\nPartial-Bug: 1550565\n\nChange-Id: If5bd096efa08c48bdecb2e1d749f886347e14e99\nRelated-Change: Icb723ee9eada8728f14eca919b982492c98a24bb\n'}, {'number': 9, 'created': '2016-11-22 08:03:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/867e4abacf6d29a8335e428ab0a4881d18d33b1c', 'message': 'Use direct_get_suffix_hashes in the reconstructor\n\nThe reconstructor uses the direct_get_suffix_hashes function\ninstead of generating http verbs by hand.\nError handling will be done with exception handling.\nIf rehash-remote fails an error message\nwill be written as well.\n\nPartial-Bug: 1550565\n\nChange-Id: If5bd096efa08c48bdecb2e1d749f886347e14e99\nRelated-Change: Icb723ee9eada8728f14eca919b982492c98a24bb\n'}, {'number': 10, 'created': '2016-11-23 13:07:18.000000000', 'files': ['swift/obj/reconstructor.py', 'test/unit/obj/test_reconstructor.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/51a04d587f9a77e6ac4b9bcde31ef5b3174b6362', 'message': 'Use direct_get_suffix_hashes in the reconstructor\n\nThe reconstructor uses the direct_get_suffix_hashes function\ninstead of generating http verbs by hand.\nError handling will be done with exception handling.\nIf rehash-remote fails an error message\nwill be written as well.\n\nPartial-Bug: 1550565\nCo-Authored-By: Alistair Coles <alistair.coles@hpe.com>\nChange-Id: If5bd096efa08c48bdecb2e1d749f886347e14e99\nRelated-Change: Icb723ee9eada8728f14eca919b982492c98a24bb\n'}]",8,394551,51a04d587f9a77e6ac4b9bcde31ef5b3174b6362,45,8,10,23739,,,0,"Use direct_get_suffix_hashes in the reconstructor

The reconstructor uses the direct_get_suffix_hashes function
instead of generating http verbs by hand.
Error handling will be done with exception handling.
If rehash-remote fails an error message
will be written as well.

Partial-Bug: 1550565
Co-Authored-By: Alistair Coles <alistair.coles@hpe.com>
Change-Id: If5bd096efa08c48bdecb2e1d749f886347e14e99
Related-Change: Icb723ee9eada8728f14eca919b982492c98a24bb
",git fetch https://review.opendev.org/openstack/swift refs/changes/51/394551/1 && git format-patch -1 --stdout FETCH_HEAD,"['swift/obj/reconstructor.py', 'test/unit/obj/test_reconstructor.py']",2,fb420dc5987f3e840c1a8567561c95e86ec13629,bug/1550565," errors = self.logger.get_lines_for_level('error') self.assertTrue(errors) for line in self.logger.get_lines_for_level('error'): if job['job_type'] == REVERT: # from ""rehash remote"" self.assertTrue( 'Error syncing with node' in line) else: # from ""_get_suffixes_to_sync"" self.assertTrue( 'responded as unmounted' in line) errors = self.logger.get_lines_for_level('error') self.assertTrue(errors) for line in self.logger.get_lines_for_level('error'): if job['job_type'] == REVERT: # from ""rehash remote"" self.assertTrue( 'Error syncing with node' in line) else: # from ""_get_suffixes_to_sync"" self.assertTrue( 'Invalid response 400' in line)", for line in self.logger.get_lines_for_level('error'): self.assertTrue('responded as unmounted' in line) for line in self.logger.get_lines_for_level('error'): self.assertTrue('Invalid response 400' in line),39,22
openstack%2Fswift~master~Ie54ed620bc61d396c44e98a3ed45453662304974,openstack/swift,master,Ie54ed620bc61d396c44e98a3ed45453662304974,update object-info to print md checksum error msg,NEW,2016-11-30 21:00:53.000000000,2017-12-18 04:37:34.000000000,,[{'_account_id': 13052}],"[{'number': 1, 'created': '2016-11-30 21:00:53.000000000', 'files': ['swift/cli/info.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/cabaa8de839e73ee3fc4decc04dc891476fbc619', 'message': 'update object-info to print md checksum error msg\n\nWhen using swift-object-info, if the metadata has been\ncorrupted, it currently displays a Traceback to the user.\nException is now caught and a more useful message is displayed\nto the user.\n\nChange-Id: Ie54ed620bc61d396c44e98a3ed45453662304974\nSigned-off-by: Thiago da Silva <thiago@redhat.com>\n'}]",0,404966,cabaa8de839e73ee3fc4decc04dc891476fbc619,5,1,1,9625,,,0,"update object-info to print md checksum error msg

When using swift-object-info, if the metadata has been
corrupted, it currently displays a Traceback to the user.
Exception is now caught and a more useful message is displayed
to the user.

Change-Id: Ie54ed620bc61d396c44e98a3ed45453662304974
Signed-off-by: Thiago da Silva <thiago@redhat.com>
",git fetch https://review.opendev.org/openstack/swift refs/changes/66/404966/1 && git format-patch -1 --stdout FETCH_HEAD,['swift/cli/info.py'],1,cabaa8de839e73ee3fc4decc04dc891476fbc619,fix_object-info,from swift.common.exceptions import DiskFileBadMetadataChecksum except DiskFileBadMetadataChecksum as err: print(err.message) raise InfoSystemExit(),,4,0
openstack%2Fironic~master~I3ea68664ad73dfae2b0fab856e93c6f868191026,openstack/ironic,master,I3ea68664ad73dfae2b0fab856e93c6f868191026,Block some port updates,NEW,2016-07-26 13:01:55.000000000,2017-12-18 04:37:29.000000000,,"[{'_account_id': 6637}, {'_account_id': 6773}, {'_account_id': 7711}, {'_account_id': 10068}, {'_account_id': 10118}, {'_account_id': 10239}, {'_account_id': 10342}, {'_account_id': 10379}, {'_account_id': 11655}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 20311}, {'_account_id': 22749}]","[{'number': 1, 'created': '2016-07-26 13:01:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/69e9615967c91a85551ace4e4d6bd2464180f31e', 'message': 'Block some port updates\n\nUpdate method ""update port"" so that the conductor\nis not able to change port\'s MAC-address unless\nthe node\'s state is Enrolled or Manageable\n(or unless the node is maintenance mode)\n\nChange-Id: I3ea68664ad73dfae2b0fab856e93c6f868191026\nCloses-Bug: #1606492\n'}, {'number': 2, 'created': '2016-11-10 12:06:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/48c69d602d87427d23547e5883c9b1a37ce1d8d1', 'message': 'Block some port updates\n\nUpdate method ""update port"" so that the conductor\nis not able to change port\'s MAC-address unless\nthe node\'s state is Enrolled or Manageable\n(or unless the node is maintenance mode)\n\nChange-Id: I3ea68664ad73dfae2b0fab856e93c6f868191026\nCloses-Bug: #1606492\n'}, {'number': 3, 'created': '2016-11-10 13:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cdd6fe95b8e9d25b07e8a705d3c06fa28773dfff', 'message': 'Block some port updates\n\nUpdate method ""update port"" so that the conductor\nis not able to change port\'s MAC-address unless\nthe node\'s state is Enrolled or Manageable\n(or unless the node is maintenance mode)\n\nChange-Id: I3ea68664ad73dfae2b0fab856e93c6f868191026\nCloses-Bug: #1606492\n'}, {'number': 4, 'created': '2016-11-23 15:09:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2c75a78359399e353f841c546a69cae96ab5380d', 'message': 'Block some port updates\n\nUpdate method ""update port"" so that the conductor\nis not able to change port\'s MAC-address unless\nthe node\'s state is Enrolled or Manageable\n(or unless the node is maintenance mode)\n\nChange-Id: I3ea68664ad73dfae2b0fab856e93c6f868191026\nCloses-Bug: #1606492\n'}, {'number': 5, 'created': '2016-11-30 13:53:54.000000000', 'files': ['releasenotes/notes/limit-node-states-for-port-updates-e3c053158ff07239.yaml', 'ironic/conductor/manager.py', 'ironic/tests/unit/conductor/test_manager.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/74e865677aeb3f62dbcaff3ce85034229d82e19f', 'message': 'Block some port updates\n\nUpdate method ""update port"" so that the conductor\nis not able to change port\'s MAC-address unless\nthe node\'s state is Enrolled or Manageable\n(or unless the node is maintenance mode)\n\nChange-Id: I3ea68664ad73dfae2b0fab856e93c6f868191026\nCloses-Bug: #1606492\n'}]",4,347360,74e865677aeb3f62dbcaff3ce85034229d82e19f,55,15,5,22749,,,0,"Block some port updates

Update method ""update port"" so that the conductor
is not able to change port's MAC-address unless
the node's state is Enrolled or Manageable
(or unless the node is maintenance mode)

Change-Id: I3ea68664ad73dfae2b0fab856e93c6f868191026
Closes-Bug: #1606492
",git fetch https://review.opendev.org/openstack/ironic refs/changes/60/347360/4 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/conductor/manager.py', 'ironic/tests/unit/conductor/test_manager.py']",2,69e9615967c91a85551ace4e4d6bd2464180f31e,bug/1606492," node = obj_utils.create_test_node(self.context, driver='fake', provision_state=states.MANAGEABLE) node = obj_utils.create_test_node(self.context, driver='fake', provision_state=states.MANAGEABLE) node = obj_utils.create_test_node(self.context, driver='fake', provision_state=states.MANAGEABLE) @mock.patch('ironic.dhcp.neutron.NeutronDHCPApi.update_port_address') def test_update_port_address_node_invalid_state(self, mac_update_mock): node = obj_utils.create_test_node(self.context, driver='fake', provision_state=states.ACTIVE) port = obj_utils.create_test_port(self.context, node_id=node.id) port.pxe_enabled = False old_address = port.address port.address = '11:22:33:44:55:bb' mac_update_mock.side_effect = ( exception.FailedToUpdateMacOnPort(port_id=port.uuid)) exc = self.assertRaises(messaging.rpc.ExpectedException, self.service.update_port, self.context, port) # Compare true exception hidden by @messaging.expected_exceptions self.assertEqual(exception.InvalidState, exc.exc_info[0]) port.refresh() self.assertEqual(old_address, port.address) "," node = obj_utils.create_test_node(self.context, driver='fake') node = obj_utils.create_test_node(self.context, driver='fake') node = obj_utils.create_test_node(self.context, driver='fake')",56,16
openstack%2Ftacker~master~I77944e533cde33abe586447301c26c2048c2f1ff,openstack/tacker,master,I77944e533cde33abe586447301c26c2048c2f1ff,Fixing tox documentation errors.,NEW,2016-11-29 13:40:05.000000000,2017-12-18 04:37:05.000000000,,"[{'_account_id': 2874}, {'_account_id': 22061}]","[{'number': 1, 'created': '2016-11-29 13:40:05.000000000', 'files': ['doc/source/devref/alarm_monitoring_usage_guide.rst', 'doc/source/install/manual_installation.rst', 'doc/source/devref/vnffgd_template_description.rst', 'doc/source/install/devstack.rst'], 'web_link': 'https://opendev.org/openstack/tacker/commit/3f234a539a1a8316f43d8b91d4b92804f7406c85', 'message': 'Fixing tox documentation errors.\n\nChange-Id: I77944e533cde33abe586447301c26c2048c2f1ff\n'}]",0,404218,3f234a539a1a8316f43d8b91d4b92804f7406c85,6,2,1,22061,,,0,"Fixing tox documentation errors.

Change-Id: I77944e533cde33abe586447301c26c2048c2f1ff
",git fetch https://review.opendev.org/openstack/tacker refs/changes/18/404218/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/devref/alarm_monitoring_usage_guide.rst', 'doc/source/install/manual_installation.rst', 'doc/source/devref/vnffgd_template_description.rst', 'doc/source/install/devstack.rst']",4,3f234a539a1a8316f43d8b91d4b92804f7406c85,docs-fix,"~~~~~~~~~~~~~~~~~~~~~~~~~~~~ By default, the tacker devstack plugin will install the tacker and other OpenStack services together. By setting TACKER_MODE=standalone in local.conf, we will install a standalone tacker environment with some mandatory OpenStack services, such as KeyStone. After this installation, a default VIM must be registered manually.","~~~~~~~~~~~~~~~~~~~~~~~~~~ By default, the tacker devstack plugin will install the tacker and other OpenStack services together. By setting TACKER_MODE=standalone in local.conf, we will install a standalone tacker environment with some mandatory OpenStack services, such as KeyStone. After this installation, a default VIM must be registered manually.",20,16
openstack%2Fpyeclib~master~Ib4fe8132d564ca27929357bcd513aae39d9450b9,openstack/pyeclib,master,Ib4fe8132d564ca27929357bcd513aae39d9450b9,Make hard-coded dependency for liberasurecode>=1.3.1,NEW,2016-11-10 08:06:18.000000000,2017-12-18 04:36:51.000000000,,"[{'_account_id': 1179}, {'_account_id': 4608}]","[{'number': 1, 'created': '2016-11-10 08:06:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/pyeclib/commit/2571622562aa7f8d20148ea5fe88c6da5b742b09', 'message': 'Make hard-coded dependency for liberasurecode>=1.3.1\n\nTo apply the fix for a liberasurecode issue [1], we need hard depencency\nof liberasurecode requires >=1.3.1. However current binary dependency\nmaintainance tool ""bindep"" works only for packagers\' repository. (i.e. it\nreffers the version of apt/yum/etc...) And nothing cared on the binary\nbuilt from source.\n\nThis patch provides a way to detect incompatible liberasurecode and\nreject to use the version when the ec driver instantiated.\n\nNOTE:\n- This dependency managemnet depends on erasurecode_version.h header\n  file in liberasurecode. i.e. it cannot care of overwritten .so library\n  after PyECLib built once.\n- This patch may be unnecessary if packagers\' repo updated to 1.3.1\n  soon. And, IMO, updating packagers\' repo immediately is the best wa\n  because this patch will cause gate failures which is using\n  liberasurecode<1.3.1 installation via bindep in gerrit.\n\n1: Icee788a0931fe692fe0de31fabc4ba450e338a87\n\nChange-Id: Ib4fe8132d564ca27929357bcd513aae39d9450b9\n'}, {'number': 2, 'created': '2016-11-10 13:56:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/pyeclib/commit/f1171d634919e3ad8f1b9ecd986f186f7f3e0237', 'message': 'Make hard-coded dependency for liberasurecode>=1.3.1\n\nTo apply the fix for a liberasurecode issue [1], we need hard depencency\nof liberasurecode requires >=1.3.1. However current binary dependency\nmaintainance tool ""bindep"" works only for packagers\' repository. (i.e. it\nreffers the version of apt/yum/etc...) And nothing cared on the binary\nbuilt from source.\n\nThis patch provides a way to detect incompatible liberasurecode and\nreject to use the version when the ec driver instantiated.\n\nNOTE:\n- This dependency managemnet depends on erasurecode_version.h header\n  file in liberasurecode. i.e. it cannot care of overwritten .so library\n  after PyECLib built once.\n- This patch may be unnecessary if packagers\' repo updated to 1.3.1\n  soon. And, IMO, updating packagers\' repo immediately is the best wa\n  because this patch will cause gate failures which is using\n  liberasurecode<1.3.1 installation via bindep in gerrit.\n\n1: Icee788a0931fe692fe0de31fabc4ba450e338a87\n\nChange-Id: Ib4fe8132d564ca27929357bcd513aae39d9450b9\n'}, {'number': 3, 'created': '2016-11-10 14:11:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/pyeclib/commit/6013802d128c8ff910b04112da57c3dd9b0fb03d', 'message': 'Make hard-coded dependency for liberasurecode>=1.3.1\n\nTo apply the fix for a liberasurecode issue [1], we need hard depencency\nof liberasurecode requires >=1.3.1. However current binary dependency\nmaintainance tool ""bindep"" works only for packagers\' repository. (i.e. it\nreffers the version of apt/yum/etc...) And nothing cared on the binary\nbuilt from source.\n\nThis patch provides a way to detect incompatible liberasurecode and\nreject to use the version when the ec driver instantiated.\n\nNOTE:\n- This dependency managemnet depends on erasurecode_version.h header\n  file in liberasurecode. i.e. it cannot care of overwritten .so library\n  after PyECLib built once.\n- This patch may be unnecessary if packagers\' repo updated to 1.3.1\n  soon. And, IMO, updating packagers\' repo immediately is the best wa\n  because this patch will cause gate failures which is using\n  liberasurecode<1.3.1 installation via bindep in gerrit.\n\n1: Icee788a0931fe692fe0de31fabc4ba450e338a87\n\nChange-Id: Ib4fe8132d564ca27929357bcd513aae39d9450b9\n'}, {'number': 4, 'created': '2016-11-11 12:26:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/pyeclib/commit/ff13127b4ae583a96d93d79b0ba56009f76fb0bf', 'message': 'Make hard-coded dependency for liberasurecode>=1.3.1\n\nTo apply the fix for a liberasurecode issue [1], we need hard depencency\nof liberasurecode requires >=1.3.1. However current binary dependency\nmaintainance tool ""bindep"" works only for packagers\' repository. (i.e. it\nreffers the version of apt/yum/etc...) And nothing cared on the binary\nbuilt from source.\n\nThis patch provides a way to detect incompatible liberasurecode and\nreject to use the version when the ec driver instantiated.\n\nNOTE:\n- This dependency managemnet depends on erasurecode_version.h header\n  file in liberasurecode. i.e. it cannot care of overwritten .so library\n  after PyECLib built once.\n- This patch may be unnecessary if packagers\' repo updated to 1.3.1\n  soon. And, IMO, updating packagers\' repo immediately is the best wa\n  because this patch will cause gate failures which is using\n  liberasurecode<1.3.1 installation via bindep in gerrit.\n\n1: Icee788a0931fe692fe0de31fabc4ba450e338a87\n\nChange-Id: Ib4fe8132d564ca27929357bcd513aae39d9450b9\n'}, {'number': 5, 'created': '2016-12-07 03:32:12.000000000', 'files': ['bindep.txt', 'test/test_pyeclib_api.py', 'pyeclib/ec_iface.py'], 'web_link': 'https://opendev.org/openstack/pyeclib/commit/c38a86cf89e1a64b25b01eb3f908dbbcebbec4c1', 'message': 'Make hard-coded dependency for liberasurecode>=1.3.1\n\nTo apply the fix for a liberasurecode issue [1], we need hard depencency\nof liberasurecode requires >=1.3.1. However current binary dependency\nmaintainance tool ""bindep"" works only for packagers\' repository. (i.e. it\nreffers the version of apt/yum/etc...) And nothing cared on the binary\nbuilt from source.\n\nThis patch provides a way to detect incompatible liberasurecode and\nreject to use the version when the ec driver instantiated.\n\nNOTE:\n- This dependency managemnet depends on erasurecode_version.h header\n  file in liberasurecode. i.e. it cannot care of overwritten .so library\n  after PyECLib built once.\n- This patch may be unnecessary if packagers\' repo updated to 1.3.1\n  soon. And, IMO, updating packagers\' repo immediately is the best wa\n  because this patch will cause gate failures which is using\n  liberasurecode<1.3.1 installation via bindep in gerrit.\n\n1: Icee788a0931fe692fe0de31fabc4ba450e338a87\n\nChange-Id: Ib4fe8132d564ca27929357bcd513aae39d9450b9\n'}]",0,395998,c38a86cf89e1a64b25b01eb3f908dbbcebbec4c1,16,2,5,4608,,,0,"Make hard-coded dependency for liberasurecode>=1.3.1

To apply the fix for a liberasurecode issue [1], we need hard depencency
of liberasurecode requires >=1.3.1. However current binary dependency
maintainance tool ""bindep"" works only for packagers' repository. (i.e. it
reffers the version of apt/yum/etc...) And nothing cared on the binary
built from source.

This patch provides a way to detect incompatible liberasurecode and
reject to use the version when the ec driver instantiated.

NOTE:
- This dependency managemnet depends on erasurecode_version.h header
  file in liberasurecode. i.e. it cannot care of overwritten .so library
  after PyECLib built once.
- This patch may be unnecessary if packagers' repo updated to 1.3.1
  soon. And, IMO, updating packagers' repo immediately is the best wa
  because this patch will cause gate failures which is using
  liberasurecode<1.3.1 installation via bindep in gerrit.

1: Icee788a0931fe692fe0de31fabc4ba450e338a87

Change-Id: Ib4fe8132d564ca27929357bcd513aae39d9450b9
",git fetch https://review.opendev.org/openstack/pyeclib refs/changes/98/395998/4 && git format-patch -1 --stdout FETCH_HEAD,"['bindep.txt', 'src/c/pyeclib_c/pyeclib_c.h', 'pyeclib/ec_iface.py', 'src/c/pyeclib_c/pyeclib_c.c']",4,2571622562aa7f8d20148ea5fe88c6da5b742b09,dependency,"#define COMPATIBLE_LIBERASURECODE_VERSION _VERSION(1,3,1) case -ELIBERASURECODE_INCOMPATIBLE: eo = import_class(""pyeclib.ec_iface"", ""ECIncompatibleLiberasurecode""); strcat(err, ""Older incompatible backend found. ""); // extract the version number to be readable int major = COMPATIBLE_LIBERASURECODE_VERSION >> 16; int minor = (COMPATIBLE_LIBERASURECODE_VERSION - (major << 16)) >> 8; int rev = (COMPATIBLE_LIBERASURECODE_VERSION - (major << 16) - (minor << 8)); char version_str[10]; sprintf(version_str, "">=%d.%d.%d"", major, minor, rev); // append version info and ""required"" word strcat(err, version_str); strcat(err, "" required""); break; #if ( LIBERASURECODE_VERSION < COMPATIBLE_LIBERASURECODE_VERSION ) // FIXME: Right now we don't have a way to know the on-load // libeasurecode version so check the version from // erasurecode_version.h via macro defined at building // python - c binding. In the future, an API like // get_liberasurecode_version available, it would be // great to use for comparing the actual so object // version with COMPATIBLE_LIBERASURECODE_VERSION. // Or perhaps liberasurecode-dev >= 1.3.1 in bindep.txt // could be a solution for this after packagers support // the version. pyeclib_c_seterr(-ELIBERASURECODE_INCOMPATIBLE, ""pyeclib_c_init ERROR: ""); return NULL; #endif",,42,2
openstack%2Fironic~master~I03b1d535d87be34aa3e4dcc2f1ee5df4d34c62f0,openstack/ironic,master,I03b1d535d87be34aa3e4dcc2f1ee5df4d34c62f0,Move root controller tests to functional dir,NEW,2016-08-17 02:40:53.000000000,2017-12-18 04:35:53.000000000,,"[{'_account_id': 14937}, {'_account_id': 17998}, {'_account_id': 19339}, {'_account_id': 24268}]","[{'number': 1, 'created': '2016-08-17 02:40:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5a379523d408131891830094b8ec2fc601155f35', 'message': 'Move root contriller tests to functional dir\n\nRoot contriller tests are all functional tests using a real\npecan server. So move them to functional dir.\n\nChange-Id: I03b1d535d87be34aa3e4dcc2f1ee5df4d34c62f0\nPartial-bug: #1607679\nPartial-bug: #1491670\n'}, {'number': 2, 'created': '2016-08-17 04:01:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6b6d9aa24e6f8ffd800ac2fc6a41dbacbc89dbf7', 'message': 'Move root controller tests to functional dir\n\nRoot controller tests are all functional tests using a real\npecan server. So move them to functional dir.\n\nChange-Id: I03b1d535d87be34aa3e4dcc2f1ee5df4d34c62f0\nPartial-bug: #1607679\nPartial-bug: #1491670\n'}, {'number': 3, 'created': '2016-08-19 01:35:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2a9510544b8396f7d2fdb5648a266f9542796fe3', 'message': 'Move root controller tests to functional dir\n\nRoot controller tests are all functional tests using a real\npecan server. So move them to functional dir.\n\nChange-Id: I03b1d535d87be34aa3e4dcc2f1ee5df4d34c62f0\nPartial-bug: #1607679\nPartial-bug: #1491670\n'}, {'number': 4, 'created': '2016-09-21 06:18:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b2b3c830bc71b7239684632e8ebf4a822ebd6175', 'message': 'Move root controller tests to functional dir\n\nRoot controller tests are all functional tests using a real\npecan server. So move them to functional dir.\n\nChange-Id: I03b1d535d87be34aa3e4dcc2f1ee5df4d34c62f0\nPartial-bug: #1607679\nPartial-bug: #1491670\n'}, {'number': 5, 'created': '2016-11-29 08:43:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2244138dc412659b842befc9baa5f3090e5235f4', 'message': 'Move root controller tests to functional dir\n\nRoot controller tests are all functional tests using a real\npecan server. So move them to functional dir.\n\nCo-Authored-By: jiangwei <wei.jiang@easystack.cn>\nChange-Id: I03b1d535d87be34aa3e4dcc2f1ee5df4d34c62f0\nPartial-bug: #1607679\nPartial-bug: #1491670\n'}, {'number': 6, 'created': '2016-11-29 08:53:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ef2c62e38973a0c43f81163949ae823e3ef2166e', 'message': 'Move root controller tests to functional dir\n\nRoot controller tests are all functional tests using a real\npecan server. So move them to functional dir.\n\nCo-Authored-By: jiangwei <wei.jiang@easystack.cn>\nChange-Id: I03b1d535d87be34aa3e4dcc2f1ee5df4d34c62f0\nPartial-bug: #1607679\nPartial-bug: #1491670\n'}, {'number': 7, 'created': '2016-12-01 02:28:09.000000000', 'files': ['ironic/tests/unit/api/test_root.py', 'ironic/tests/functional/api/v1/test_root.py', 'ironic/tests/functional/api/test_root.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/edeea310dad254ff818e9f5a1717d58e1d44cb9c', 'message': 'Move root controller tests to functional dir\n\nRoot controller tests are all functional tests using a real\npecan server. So move them to functional dir.\n\nCo-Authored-By: jiangwei <wei.jiang@easystack.cn>\nChange-Id: I03b1d535d87be34aa3e4dcc2f1ee5df4d34c62f0\nPartial-bug: #1607679\nPartial-bug: #1491670\n'}]",0,356188,edeea310dad254ff818e9f5a1717d58e1d44cb9c,37,4,7,14937,,,0,"Move root controller tests to functional dir

Root controller tests are all functional tests using a real
pecan server. So move them to functional dir.

Co-Authored-By: jiangwei <wei.jiang@easystack.cn>
Change-Id: I03b1d535d87be34aa3e4dcc2f1ee5df4d34c62f0
Partial-bug: #1607679
Partial-bug: #1491670
",git fetch https://review.opendev.org/openstack/ironic refs/changes/88/356188/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/tests/unit/api/test_root.py', 'ironic/tests/functional/api/v1/test_root.py', 'ironic/tests/functional/api/test_root.py']",3,5a379523d408131891830094b8ec2fc601155f35,bug/1607679,"# Copyright 2013 Red Hat, Inc. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from ironic.api.controllers.v1 import versions from ironic.tests.functional.api import base class TestRoot(base.BaseApiTest): def test_get_root(self): response = self.get_json('/', path_prefix='') # Check fields are not empty [self.assertNotIn(f, ['', []]) for f in response] self.assertEqual('OpenStack Ironic API', response['name']) self.assertTrue(response['description']) self.assertEqual([response['default_version']], response['versions']) version1 = response['default_version'] self.assertEqual('v1', version1['id']) self.assertEqual('CURRENT', version1['status']) self.assertEqual(versions.MIN_VERSION_STRING, version1['min_version']) self.assertEqual(versions.MAX_VERSION_STRING, version1['version']) ",,71,71
openstack%2Ftrove~master~I6dfe26f446149426d00ae6e53877384d99bd3bfc,openstack/trove,master,I6dfe26f446149426d00ae6e53877384d99bd3bfc,[WIP] Extended Filesystem Support,NEW,2016-11-30 20:29:08.000000000,2017-12-18 04:35:50.000000000,,"[{'_account_id': 9664}, {'_account_id': 9746}, {'_account_id': 9782}, {'_account_id': 10215}]","[{'number': 1, 'created': '2016-11-30 20:29:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/addb96f24b0f591ffac1fcc0b5fc1febec16dbc2', 'message': '[WIP] Extended Filesystem Support\n\nPreliminary prototype/implementation for Extended Filesystem\nSupport.\n\nChange-Id: I6dfe26f446149426d00ae6e53877384d99bd3bfc\nImplements: blueprint extended-filesytem\n'}, {'number': 2, 'created': '2016-12-12 17:43:39.000000000', 'files': ['trove/tests/unittests/utils/test_utils.py', 'trove/tests/unittests/trove_testtools.py', 'trove/tests/unittests/taskmanager/test_api.py', 'trove/instance/models.py', 'trove/taskmanager/manager.py', 'trove/common/apischema.py', 'trove/common/utils.py', 'trove/tests/unittests/instance/test_instance_models.py', 'trove/db/sqlalchemy/migrate_repo/versions/040_add_ext_fs_tables.py', 'trove/tests/unittests/taskmanager/test_models.py', 'trove/common/cfg.py', 'trove/taskmanager/api.py', 'trove/tests/unittests/mgmt/test_models.py', 'trove/common/notification.py', 'trove/taskmanager/models.py', 'trove/tests/unittests/taskmanager/test_manager.py'], 'web_link': 'https://opendev.org/openstack/trove/commit/4b3de77e21b2a9f53a9ba4ae8bfb36c7f22fa2c7', 'message': '[WIP] Extended Filesystem Support\n\nPreliminary prototype/implementation for Extended Filesystem\nSupport.\n\nChange-Id: I6dfe26f446149426d00ae6e53877384d99bd3bfc\nImplements: blueprint extended-filesytem\n'}]",5,404956,4b3de77e21b2a9f53a9ba4ae8bfb36c7f22fa2c7,16,4,2,9782,,,0,"[WIP] Extended Filesystem Support

Preliminary prototype/implementation for Extended Filesystem
Support.

Change-Id: I6dfe26f446149426d00ae6e53877384d99bd3bfc
Implements: blueprint extended-filesytem
",git fetch https://review.opendev.org/openstack/trove refs/changes/56/404956/1 && git format-patch -1 --stdout FETCH_HEAD,"['trove/tests/unittests/taskmanager/test_api.py', 'trove/instance/models.py', 'trove/taskmanager/manager.py', 'trove/common/apischema.py', 'trove/tests/unittests/instance/test_instance_models.py', 'trove/db/sqlalchemy/migrate_repo/versions/040_add_ext_fs_tables.py', 'trove/tests/unittests/taskmanager/test_models.py', 'trove/common/cfg.py', 'trove/taskmanager/api.py', 'trove/common/notification.py', 'trove/taskmanager/models.py', 'trove/tests/unittests/taskmanager/test_manager.py']",12,addb96f24b0f591ffac1fcc0b5fc1febec16dbc2,bp/extended-filesytem," None, None, None) Mock(), 'some-master-id', None, None, None, None, None) None, None, None, None) None, None, None, None, 'affinity', None) {'group': 'sg-id'}, None)"," None, None) Mock(), 'some-master-id', None, None, None, None) None, None, None) None, None, None, None, 'affinity') {'group': 'sg-id'})",325,74
openstack%2Fironic~master~I6b06b2cd6b036e3bbc8acbc14bff42e1b2ac90a7,openstack/ironic,master,I6b06b2cd6b036e3bbc8acbc14bff42e1b2ac90a7,DNM: Test ironic-inspector tempest,NEW,2016-12-12 12:21:21.000000000,2017-12-18 04:35:48.000000000,,"[{'_account_id': 10118}, {'_account_id': 13636}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}]","[{'number': 1, 'created': '2016-12-12 12:21:21.000000000', 'files': ['devstack/lib/ironic'], 'web_link': 'https://opendev.org/openstack/ironic/commit/762b17a56376fb27605238cd40de4257dd8101a7', 'message': 'DNM: Test ironic-inspector tempest\n\nChange-Id: I6b06b2cd6b036e3bbc8acbc14bff42e1b2ac90a7\n'}]",0,409761,762b17a56376fb27605238cd40de4257dd8101a7,8,5,1,13636,,,0,"DNM: Test ironic-inspector tempest

Change-Id: I6b06b2cd6b036e3bbc8acbc14bff42e1b2ac90a7
",git fetch https://review.opendev.org/openstack/ironic refs/changes/61/409761/1 && git format-patch -1 --stdout FETCH_HEAD,['devstack/lib/ironic'],1,762b17a56376fb27605238cd40de4257dd8101a7,inspector-agent_ipmitool,IRONIC_DEPLOY_DRIVER=agent_ipmitool,IRONIC_DEPLOY_DRIVER=${IRONIC_DEPLOY_DRIVER:-pxe_ssh},1,1
openstack%2Ftacker~master~Ia726f459f5d73dacd28b2153c36397afb25893d6,openstack/tacker,master,Ia726f459f5d73dacd28b2153c36397afb25893d6,Changes in post_test_hook.sh to run dsvm-functional tests successfully,NEW,2016-09-19 10:57:46.000000000,2017-12-18 04:35:46.000000000,,"[{'_account_id': 2874}, {'_account_id': 13380}]","[{'number': 1, 'created': '2016-09-19 10:57:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/10951804cc748f7b734a327c2d4bee6f0b44c9a0', 'message': 'Changes in post_test_hook.sh to run dsvm-functional tests successfully\n\nChange-Id: Ia726f459f5d73dacd28b2153c36397afb25893d6\n'}, {'number': 2, 'created': '2016-09-19 11:02:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/ba599d735bd72c7ab4642165fc513690c4a4ae9a', 'message': 'Changes in post_test_hook.sh to run dsvm-functional tests successfully\n\nChange-Id: Ia726f459f5d73dacd28b2153c36397afb25893d6\nCloses-bug: #1597915\n'}, {'number': 3, 'created': '2016-09-23 06:21:03.000000000', 'files': ['tacker/tests/contrib/post_test_hook.sh'], 'web_link': 'https://opendev.org/openstack/tacker/commit/347a71d237c104ebdc4a8020c376a12e638594f3', 'message': 'Changes in post_test_hook.sh to run dsvm-functional tests successfully\n\nChange-Id: Ia726f459f5d73dacd28b2153c36397afb25893d6\nCloses-bug: #1597915\n'}]",0,372399,347a71d237c104ebdc4a8020c376a12e638594f3,10,2,3,23511,,,0,"Changes in post_test_hook.sh to run dsvm-functional tests successfully

Change-Id: Ia726f459f5d73dacd28b2153c36397afb25893d6
Closes-bug: #1597915
",git fetch https://review.opendev.org/openstack/tacker refs/changes/99/372399/3 && git format-patch -1 --stdout FETCH_HEAD,['tacker/tests/contrib/post_test_hook.sh'],1,10951804cc748f7b734a327c2d4bee6f0b44c9a0,bug/1597915, disable nova quota limit add nova key,,2,1
openstack%2Ftacker~master~Ifb18a8389b93dfcc5b4f5aab6e12f27904c1a23f,openstack/tacker,master,Ifb18a8389b93dfcc5b4f5aab6e12f27904c1a23f,Refactor Tacker unit tests for services/vm,NEW,2016-09-08 10:08:20.000000000,2017-12-18 04:35:43.000000000,,"[{'_account_id': 18955}, {'_account_id': 23304}]","[{'number': 1, 'created': '2016-09-08 10:08:20.000000000', 'files': ['tacker/tests/unit/services/vm/test_servicevm_extension.py', 'tacker/tests/unit/test_api_v2_extension.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/a6e713fb231a9dd999f9c2d9957f6f3346de13a1', 'message': 'Refactor Tacker unit tests for services/vm\n\nRefactor Tacker unit tests for services/vm\n\nChange-Id: Ifb18a8389b93dfcc5b4f5aab6e12f27904c1a23f\n'}]",0,367258,a6e713fb231a9dd999f9c2d9957f6f3346de13a1,8,2,1,18955,,,0,"Refactor Tacker unit tests for services/vm

Refactor Tacker unit tests for services/vm

Change-Id: Ifb18a8389b93dfcc5b4f5aab6e12f27904c1a23f
",git fetch https://review.opendev.org/openstack/tacker refs/changes/58/367258/1 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/tests/unit/services/vm/test_servicevm_extension.py', 'tacker/tests/unit/test_api_v2_extension.py']",2,a6e713fb231a9dd999f9c2d9957f6f3346de13a1,refactoring," resource_attribute_map, plural_mappings=None, path = ''","import webtest from tacker.api import extensionsfrom tacker.tests.unit import test_extensions resource_attribute_map, extension_class, resource_prefix, plural_mappings=None, self._resource_prefix = resource_prefix # Ensure existing ExtensionManager is not used extensions.PluginAwareExtensionManager._instance = None class ExtensionTestExtensionManager(object): def get_resources(self): # Add the resources to the global attribute map # This is done here as the setup process won't # initialize the main API router which extends # the global attribute map attributes.RESOURCE_ATTRIBUTE_MAP.update( resource_attribute_map) return extension_class.get_resources() def get_actions(self): return [] def get_request_extensions(self): return [] ext_mgr = ExtensionTestExtensionManager() self.ext_mdw = test_extensions.setup_extensions_middleware(ext_mgr) self.api = webtest.TestApp(self.ext_mdw) path = self._resource_prefix + '/' if self._resource_prefix else ''",54,84
openstack%2Fceilometer~master~Ieb16b3967e6147537c81e7fb5676945674943cf4,openstack/ceilometer,master,Ieb16b3967e6147537c81e7fb5676945674943cf4,add net-packets-rate counter for vmware hypervisior,NEW,2016-10-29 08:54:01.000000000,2017-12-18 04:35:41.000000000,,"[{'_account_id': 15843}, {'_account_id': 17289}, {'_account_id': 20100}]","[{'number': 1, 'created': '2016-10-29 08:54:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/4258500232c8ccdba5eb725a7bd0d1f413cbf8d3', 'message': 'add net-packets-rate counter for vmware hypervisior\n\nChange-Id: Ieb16b3967e6147537c81e7fb5676945674943cf4\n'}, {'number': 2, 'created': '2016-12-12 01:14:44.000000000', 'files': ['ceilometer/compute/virt/inspector.py', 'ceilometer/compute/virt/vmware/inspector.py', 'setup.cfg', 'ceilometer/compute/pollsters/net.py'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/572dc20a8cbee860032a8fb2fcaf7f0908bff4c2', 'message': 'add net-packets-rate counter for vmware hypervisior\n\nChange-Id: Ieb16b3967e6147537c81e7fb5676945674943cf4\n'}]",1,391586,572dc20a8cbee860032a8fb2fcaf7f0908bff4c2,12,3,2,17289,,,0,"add net-packets-rate counter for vmware hypervisior

Change-Id: Ieb16b3967e6147537c81e7fb5676945674943cf4
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/86/391586/1 && git format-patch -1 --stdout FETCH_HEAD,"['ceilometer/compute/virt/inspector.py', 'ceilometer/compute/virt/vmware/inspector.py', 'setup.cfg', 'ceilometer/compute/pollsters/net.py']",4,4258500232c8ccdba5eb725a7bd0d1f413cbf8d3,,"class _BytesRateBase(_Base): NET_USAGE_MESSAGE = ' '.join([""NETWORK BYTES RATE:"", ""%s %s:"",class _PacketsRateBase(_Base): NET_USAGE_MESSAGE = ' '.join([""NETWORK PACKETS RATE:"", ""%s %s:"", ""read-packets-rate=%d"", ""write-packets-rate=%d""]) CACHE_KEY_VNIC = 'vnic-rates' def _get_vnic_info(self, inspector, instance): return inspector.inspect_vnic_rates(instance, self._inspection_duration) @staticmethod def _get_rx_info(info): return info.rx_packets_rate @staticmethod def _get_tx_info(info): return info.tx_packets_rate class IncomingBytesRatePollster(_BytesRateBase):class OutgoingBytesRatePollster(_BytesRateBase):class IncomingPacketsRatePollster(_PacketsRateBase): def _get_sample(self, instance, vnic, info): return self.make_vnic_sample( instance, name='network.incoming.packets.rate', type=sample.TYPE_GAUGE, unit='packets/s', volume=info.rx_packets_rate, vnic_data=vnic, ) class OutgoingPacketsRatePollster(_PacketsRateBase): def _get_sample(self, instance, vnic, info): return self.make_vnic_sample( instance, name='network.outgoing.packets.rate', type=sample.TYPE_GAUGE, unit='packets/s', volume=info.tx_packets_rate, vnic_data=vnic, ) ","class _RateBase(_Base): NET_USAGE_MESSAGE = ' '.join([""NETWORK RATE:"", ""%s %s:"",class IncomingBytesRatePollster(_RateBase):class OutgoingBytesRatePollster(_RateBase):",78,12
openstack%2Ftacker~master~I490a8a26f6bbfa59fd0f45d28aa2b35e776d063e,openstack/tacker,master,I490a8a26f6bbfa59fd0f45d28aa2b35e776d063e,Add dependency on networking-sfc for VNFFG to work,NEW,2016-11-14 05:49:45.000000000,2017-12-18 04:35:00.000000000,,"[{'_account_id': 2874}, {'_account_id': 13380}, {'_account_id': 20560}]","[{'number': 1, 'created': '2016-11-14 05:49:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/89dfaa98ace029a740e112810b89bcd3b8dd36da', 'message': 'Add dependency on networking-sfc for VNFFG to work\n\nChange-Id: I490a8a26f6bbfa59fd0f45d28aa2b35e776d063e\n'}, {'number': 2, 'created': '2016-12-09 12:13:40.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/tacker/commit/28f908c879dcff8f78dcdf3274d31e15d8edb11f', 'message': 'Add dependency on networking-sfc for VNFFG to work\n\nChange-Id: I490a8a26f6bbfa59fd0f45d28aa2b35e776d063e\n'}]",0,397004,28f908c879dcff8f78dcdf3274d31e15d8edb11f,10,3,2,2874,,,0,"Add dependency on networking-sfc for VNFFG to work

Change-Id: I490a8a26f6bbfa59fd0f45d28aa2b35e776d063e
",git fetch https://review.opendev.org/openstack/tacker refs/changes/04/397004/2 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,89dfaa98ace029a740e112810b89bcd3b8dd36da,networking-sfc-standalone,networking-sfc>=2.0.0 # Apache-2.0,,1,0
openstack%2Fironic-python-agent~master~Id2550b6eab167c9aba030f7c946c8c5034b91b21,openstack/ironic-python-agent,master,Id2550b6eab167c9aba030f7c946c8c5034b91b21,[POC] BitTorrent image provisioning,NEW,2016-11-29 09:58:47.000000000,2017-12-18 04:34:57.000000000,,"[{'_account_id': 13636}, {'_account_id': 14250}, {'_account_id': 14826}, {'_account_id': 18653}]","[{'number': 1, 'created': '2016-11-29 09:58:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/c28e3b1e83ff9435e5c1ae3d1ec91fe903a6911e', 'message': '[POC] BitTorrent image provisioning\n\nChange-Id: Id2550b6eab167c9aba030f7c946c8c5034b91b21\n'}, {'number': 2, 'created': '2016-12-15 08:13:35.000000000', 'files': ['requirements.txt', 'imagebuild/tinyipa/build_files/finalreqs.lst', 'ironic_python_agent/extensions/standby.py'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/920338aac05440b86c0de0bff511f90a6eecf87d', 'message': '[POC] BitTorrent image provisioning\n\nChange-Id: Id2550b6eab167c9aba030f7c946c8c5034b91b21\n'}]",0,404120,920338aac05440b86c0de0bff511f90a6eecf87d,7,4,2,13636,,,0,"[POC] BitTorrent image provisioning

Change-Id: Id2550b6eab167c9aba030f7c946c8c5034b91b21
",git fetch https://review.opendev.org/openstack/ironic-python-agent refs/changes/20/404120/1 && git format-patch -1 --stdout FETCH_HEAD,['ironic_python_agent/extensions/standby.py'],1,c28e3b1e83ff9435e5c1ae3d1ec91fe903a6911e,torrent-provision,"class HTTPImageDownload(object): def __init__(self, image_info, image_location, time_obj=None): """"""Initialize an instance of the HTTPImageDownload class. :param image_location: # TODO(aarefiev): refactor me with open(image_location, 'wb+') as f: try: for chunk in self._request.iter_content(IMAGE_CHUNK_SIZE): self._md5checksum.update(chunk) f.write(chunk) except Exception as e: msg = 'Unable to write image to {}. Error: {}'.format( image_location, str(e)) raise errors.ImageDownloadError(image_info['id'], msg) totaltime = time.time() - time_obj LOG.info(""Image downloaded from {} in {} seconds"".format(image_location, totaltime)) self._verify_image(image_info, image_location, self.md5sum()) def _verify_image(self, image_info, image_location, checksum): """"""Verifies the checksum of the local images matches expectations. If this function does not raise ImageChecksumError then it is very likely that the local copy of the image was transmitted and stored correctly. :param image_info: Image information dictionary. :param image_location: The location of the local image. :param checksum: The computed checksum of the local image. :raises: ImageChecksumError if the checksum of the local image does not match the checksum as reported by glance in image_info. """""" LOG.debug('Verifying image at {} against MD5 checksum ' '{}'.format(image_location, checksum)) if checksum != image_info['checksum']: LOG.error(errors.ImageChecksumError.details_str.format( image_location, image_info['id'], image_info['checksum'], checksum)) raise errors.ImageChecksumError(image_location, image_info['id'], image_info['checksum'], checksum) class BitTorrentImageDownload(object): """"""Helper class that opens a HTTP connection to download an image. """""" def __init__(self, image_info, image_location, time_obj=None): """"""Initialize an instance of the BitTorrentImageDownload class. """""" LOG.info(""Attempting to download image with torrent %s"" % image_info) command = ['/usr/bin/aria2c', '--console-log-level=warn', '--check-certificate=false', '--file-allocation=none', '--allow-overwrite=true', '--seed-ratio=0.0', '--bt-tracker-interval=2', '--enable-dht=false', '--enable-dht6=false', '--disable-ipv6=true', '--enable-peer-exchange=true', '--bt-enable-lpd=true', '--follow-torrent=mem', '--dir=%s' % image_location, '--bt-tracker-connect-timeout=2', '--bt-tracker-timeout=5', '--seed-time=0', image_info['urls'][0]] LOG.debug('Downloading image with command: {0}'.format(' '.join(command))) try: _, _ = utils.execute(*command, check_exit_code=[0]) except processutils.ProcessExecutionError as e: raise errors.ImageDownloadError(image_info['id'], e.stdout + e.stderr) totaltime = time.time() - time_obj LOG.info(""Image downloaded in {0} seconds"".format(totaltime)) def _get_image_downloader(image_info): """""" :return: torrent_file = image_info['properties'].get('torrent_file') return BitTorrentImageDownload if torrent_file else HTTPImageDownload downloader = _get_image_downloader(image_info) downloader(image_info, image_location, time_obj=starttime) downloader = _get_image_downloader(image_info) downloader(image_info, device, time_obj=starttime) ","class ImageDownload(object): def __init__(self, image_info, time_obj=None): """"""Initialize an instance of the ImageDownload class.def _verify_image(image_info, image_location, checksum): """"""Verifies the checksum of the local images matches expectations. If this function does not raise ImageChecksumError then it is very likely that the local copy of the image was transmitted and stored correctly. :param image_location: The location of the local image. :param checksum: The computed checksum of the local image. :raises: ImageChecksumError if the checksum of the local image does not match the checksum as reported by glance in image_info. LOG.debug('Verifying image at {} against MD5 checksum ' '{}'.format(image_location, checksum)) if checksum != image_info['checksum']: LOG.error(errors.ImageChecksumError.details_str.format( image_location, image_info['id'], image_info['checksum'], checksum)) raise errors.ImageChecksumError(image_location, image_info['id'], image_info['checksum'], checksum) image_download = ImageDownload(image_info, time_obj=starttime) with open(image_location, 'wb') as f: try: for chunk in image_download: f.write(chunk) except Exception as e: msg = 'Unable to write image to {}. Error: {}'.format( image_location, str(e)) raise errors.ImageDownloadError(image_info['id'], msg) totaltime = time.time() - starttime LOG.info(""Image downloaded from {} in {} seconds"".format(image_location, totaltime)) _verify_image(image_info, image_location, image_download.md5sum()) image_download = ImageDownload(image_info, time_obj=starttime) with open(device, 'wb+') as f: try: for chunk in image_download: f.write(chunk) except Exception as e: msg = 'Unable to write image to device {}. Error: {}'.format( device, str(e)) raise errors.ImageDownloadError(image_info['id'], msg) totaltime = time.time() - starttime LOG.info(""Image streamed onto device {} in {} "" ""seconds"".format(device, totaltime)) # Verify if the checksum of the streamed image is correct _verify_image(image_info, device, image_download.md5sum())",80,49
openstack%2Fironic~master~I2adb9e3fe7e5196a2fb9f6d26ed839b690c7eb97,openstack/ironic,master,I2adb9e3fe7e5196a2fb9f6d26ed839b690c7eb97,[POC]: Torrent provisioning,NEW,2016-12-12 10:13:08.000000000,2017-12-18 04:34:38.000000000,,"[{'_account_id': 10118}, {'_account_id': 13636}, {'_account_id': 14250}, {'_account_id': 14629}, {'_account_id': 14826}, {'_account_id': 17998}, {'_account_id': 18653}, {'_account_id': 19003}, {'_account_id': 19339}]","[{'number': 1, 'created': '2016-12-12 10:13:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8cfe6d1b942884d777cd647d52e99c28b5bd1798', 'message': '[POC]: Torrent provisioning\n\nDevstack setup:\n   IRONIC_TORRENT_PROVISIONING_ENABLED=True\n\nFor deploying image with torrent provision, we need to\nconfigure ironic and image:\n\n1. Devstack should be deployed with:\nIRONIC_TORRENT_PROVISIONING_ENABLED=True\n\nAfter Devstack is deployed:\n\n2.Generate torrent file for required image\nmktorrent -a none <image>\n\n3.Upload the torrent file into glance:\nglance image-create --name <image>.torrent --disk-format raw\n                    --container-format bare --file <image>.torrent\n                    --visibility public\n4. Upload or update image in glance with torrent file property:\nglance image-update <image_id> --property torrent_file=<torrent_file_image_id>\n\nChange-Id: I2adb9e3fe7e5196a2fb9f6d26ed839b690c7eb97\n'}, {'number': 2, 'created': '2016-12-15 08:14:03.000000000', 'files': ['ironic/drivers/modules/agent.py', 'devstack/lib/ironic', 'ironic/drivers/modules/deploy_utils.py', 'ironic/conf/deploy.py', 'devstack/plugin.sh', 'devstack/files/rpms/ironic', 'devstack/files/debs/ironic'], 'web_link': 'https://opendev.org/openstack/ironic/commit/c0d15617118a2759378b9d33cb5a61723b541efd', 'message': '[POC]: Torrent provisioning\n\nDevstack setup:\n   IRONIC_TORRENT_PROVISIONING_ENABLED=True\n\nFor deploying image with torrent provision, we need to\nconfigure ironic and image:\n\n1. Devstack should be deployed with:\nIRONIC_TORRENT_PROVISIONING_ENABLED=True\n\nAfter Devstack is deployed:\n\n2.Generate torrent file for required image\nmktorrent -a none <image>\n\n3.Upload the torrent file into glance:\nglance image-create --name <image>.torrent --disk-format raw\n                    --container-format bare --file <image>.torrent\n                    --visibility public\n4. Upload or update image in glance with torrent file property:\nglance image-update <image_id> --property torrent_file=<torrent_file_image_id>\n\nChange-Id: I2adb9e3fe7e5196a2fb9f6d26ed839b690c7eb97\n'}]",0,409711,c0d15617118a2759378b9d33cb5a61723b541efd,16,9,2,13636,,,0,"[POC]: Torrent provisioning

Devstack setup:
   IRONIC_TORRENT_PROVISIONING_ENABLED=True

For deploying image with torrent provision, we need to
configure ironic and image:

1. Devstack should be deployed with:
IRONIC_TORRENT_PROVISIONING_ENABLED=True

After Devstack is deployed:

2.Generate torrent file for required image
mktorrent -a none <image>

3.Upload the torrent file into glance:
glance image-create --name <image>.torrent --disk-format raw
                    --container-format bare --file <image>.torrent
                    --visibility public
4. Upload or update image in glance with torrent file property:
glance image-update <image_id> --property torrent_file=<torrent_file_image_id>

Change-Id: I2adb9e3fe7e5196a2fb9f6d26ed839b690c7eb97
",git fetch https://review.opendev.org/openstack/ironic refs/changes/11/409711/1 && git format-patch -1 --stdout FETCH_HEAD,"['devstack/lib/ironic', 'ironic/drivers/modules/agent.py', 'ironic/drivers/modules/deploy_utils.py', 'ironic/conf/deploy.py', 'devstack/plugin.sh', 'devstack/files/rpms/ironic', 'devstack/files/debs/ironic']",7,8cfe6d1b942884d777cd647d52e99c28b5bd1798,torrent-provisioning,qbittorrent-nox,,62,2
openstack%2Fironic~master~Ie1090af4047dae317333e65645da607c8e1e335b,openstack/ironic,master,Ie1090af4047dae317333e65645da607c8e1e335b,made changes and edited the ironic install guide,NEW,2015-06-17 19:59:25.000000000,2017-12-18 04:34:33.000000000,,"[{'_account_id': 2889}, {'_account_id': 6616}, {'_account_id': 6618}, {'_account_id': 9382}, {'_account_id': 9383}, {'_account_id': 12459}, {'_account_id': 13589}, {'_account_id': 14920}, {'_account_id': 14943}]","[{'number': 1, 'created': '2015-06-17 19:59:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/983e8adf56a231dd8d48ae8f2b8ffcf0f29db0d7', 'message': 'anges and edited the ironic intall guied\n\nper doc conventions ironic should be lowercase or Bare metal service\n\nChange-Id: Ie1090af4047dae317333e65645da607c8e1e335b\n'}, {'number': 2, 'created': '2015-06-17 20:00:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7e47be84e3a03f33617d7eb1f1f02492c4bafff0', 'message': 'made changes and edited the ironic intall guied\n\nper doc conventions ironic should be lowercase or Bare metal service\n\nChange-Id: Ie1090af4047dae317333e65645da607c8e1e335b\n'}, {'number': 3, 'created': '2015-06-17 20:04:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0e3474296156dc6be1d16ad2dccc4b11684131c9', 'message': 'made changes and edited the ironic intall guide \n\nper doc conventions ironic should be lowercase or Bare metal service\n\nChange-Id: Ie1090af4047dae317333e65645da607c8e1e335b\n'}, {'number': 4, 'created': '2015-06-17 20:15:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/55152c7127e9094f2d4867dd0096270306332ab6', 'message': 'made changes and edited the ironic install guide \n\nper doc conventions ironic should be lowercase or Bare metal service\n\nChange-Id: Ie1090af4047dae317333e65645da607c8e1e335b\n'}, {'number': 5, 'created': '2015-06-17 20:16:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/dc96394f7ae01aedc7fa1ad539295e5ac7082b09', 'message': 'made changes and edited the ironic intall guide\n\nper doc conventions ironic should be lowercase or Bare metal service\n\nChange-Id: Ie1090af4047dae317333e65645da607c8e1e335b\n'}, {'number': 6, 'created': '2015-06-17 20:16:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7eb27fee659f252f198dc6c8c6dbc56449a9c578', 'message': 'made changes and edited the ironic install guide\n\nper doc conventions ironic should be lowercase or Bare metal service\n\nChange-Id: Ie1090af4047dae317333e65645da607c8e1e335b\n'}, {'number': 7, 'created': '2015-06-17 20:44:36.000000000', 'files': ['doc/source/drivers/amt.rst'], 'web_link': 'https://opendev.org/openstack/ironic/commit/62a3aca4206b31dc0ef659b176329dcbd56b1b7d', 'message': 'made changes and edited the ironic install guide\n\nper doc conventions ironic should be lowercase or Bare metal service\n\nChange-Id: Ie1090af4047dae317333e65645da607c8e1e335b\n'}]",8,192865,62a3aca4206b31dc0ef659b176329dcbd56b1b7d,25,9,7,15804,,,0,"made changes and edited the ironic install guide

per doc conventions ironic should be lowercase or Bare metal service

Change-Id: Ie1090af4047dae317333e65645da607c8e1e335b
",git fetch https://review.opendev.org/openstack/ironic refs/changes/65/192865/4 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/drivers/amt.rst'],1,983e8adf56a231dd8d48ae8f2b8ffcf0f29db0d7,ironic-doc,AMT (Active Management Technology) drivers extend Bare metal service range to the * Go to intel AMT configuration: * Select user consent and choose none (no password is needed) * Select network setup section and set IP * Activate network access * MEBx exit Bare metal service control (I.E. rebooted by a local user) because the node will,AMT (Active Management Technology) drivers extend Ironic's range to the * Go to Intel AMT Configuration: * Select User Consent and choose None (No password is needed) * Select Network Setup section and set IP * Activate Network Access * MEBx Exit Ironic's control (I.E. rebooted by a local user) because the node will,7,7
openstack%2Fpython-heatclient~master~I5306aae65e6127190ffa5f0ef60879f90becef7c,openstack/python-heatclient,master,I5306aae65e6127190ffa5f0ef60879f90becef7c,Fix openstack orchestration resource type show output format.,NEW,2016-12-16 09:00:30.000000000,2017-12-18 04:34:21.000000000,,"[{'_account_id': 6577}, {'_account_id': 10068}]","[{'number': 1, 'created': '2016-12-16 09:00:30.000000000', 'files': ['heatclient/osc/v1/resource_type.py'], 'web_link': 'https://opendev.org/openstack/python-heatclient/commit/47c32c342d8d21a438dd5d77bc4872a5b3ca2499', 'message': 'Fix openstack orchestration resource type show output format.\n\nopenstack orchestration resource type show list output in json that is\nnot in the default table format as most of the other commands list.\nchanged the defaultformat from YamlFormat to RawFormat\n\nChange-Id: I5306aae65e6127190ffa5f0ef60879f90becef7c\nCloses-Bug: #1643550\n'}]",0,411710,47c32c342d8d21a438dd5d77bc4872a5b3ca2499,5,2,1,24421,,,0,"Fix openstack orchestration resource type show output format.

openstack orchestration resource type show list output in json that is
not in the default table format as most of the other commands list.
changed the defaultformat from YamlFormat to RawFormat

Change-Id: I5306aae65e6127190ffa5f0ef60879f90becef7c
Closes-Bug: #1643550
",git fetch https://review.opendev.org/openstack/python-heatclient refs/changes/10/411710/1 && git format-patch -1 --stdout FETCH_HEAD,['heatclient/osc/v1/resource_type.py'],1,47c32c342d8d21a438dd5d77bc4872a5b3ca2499,lp/1643550,class ResourceTypeShow(format_utils.RawFormat):,class ResourceTypeShow(format_utils.YamlFormat):,1,1
openstack%2Fkuryr-libnetwork~master~Id944057b55f0476c078f40fb8355b272dca2b499,openstack/kuryr-libnetwork,master,Id944057b55f0476c078f40fb8355b272dca2b499,Delay neutron extension checks to first neutron interaction,NEW,2016-11-26 07:24:07.000000000,2017-12-18 04:34:04.000000000,,"[{'_account_id': 6598}, {'_account_id': 9820}, {'_account_id': 14352}, {'_account_id': 14867}, {'_account_id': 15967}, {'_account_id': 16516}, {'_account_id': 21273}, {'_account_id': 23567}]","[{'number': 1, 'created': '2016-11-26 07:24:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr-libnetwork/commit/ce1025499e5b4d1c5b18ff1fa5999186922577ef', 'message': 'Delay neutron extension checks to first neutron interaction\n\n1. Kuryr initialization should not be dependent on neutron availability.\n2. Currently neutron client creation attributes can not be used from\nconfig file. Reason is neutron client gets created even before config\nfile is used while starting kuryr server.\n\nIn this fix kuryr neutron client creation is being moved from Kuryr\nserver initialization to the the following libnetwork api handlers.\nThese api handlers can be first point of contact where neutron will be\nqueried. For example, /NetworkDriver.Leave and\n/NetworkDriver.DeleteNetwork could be first point of interactions with\nneutron in case Kuryr has got restarted and user tries to delete a\ncontainer or existing network(created before Kuryr restart)\nrespectively.\n\n/IpamDriver.RequestPool\n/IpamDriver.RequestAddress\n/NetworkDriver.DeleteNetwork\n/NetworkDriver.Leave\n\nChange-Id: Id944057b55f0476c078f40fb8355b272dca2b499\n'}, {'number': 2, 'created': '2016-11-26 07:54:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr-libnetwork/commit/4c76d2898ba99a5a0bb0a995f10d428a41998eac', 'message': 'Delay neutron extension checks to first neutron interaction\n\n1. Kuryr initialization should not be dependent on neutron availability.\n2. Currently neutron client creation attributes can not be used from\nconfig file. Reason is neutron client gets created even before config\nfile is used while starting kuryr server.\n\nIn this fix kuryr neutron client creation is being moved from Kuryr\nserver initialization to the the following libnetwork api handlers.\nThese api handlers can be first point of contact where neutron will be\nqueried. For example, /NetworkDriver.Leave and\n/NetworkDriver.DeleteNetwork could be first point of interactions with\nneutron in case Kuryr has got restarted and user tries to delete a\ncontainer or existing network(created before Kuryr restart)\nrespectively.\n\n/IpamDriver.RequestPool\n/IpamDriver.RequestAddress\n/NetworkDriver.DeleteNetwork\n/NetworkDriver.Leave\n\nChange-Id: Id944057b55f0476c078f40fb8355b272dca2b499\nCloses-bug: #1644970\n'}, {'number': 3, 'created': '2016-12-05 11:36:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr-libnetwork/commit/0e13c513e7126a482816e8de708de79a91311b43', 'message': 'Delay neutron extension checks to first neutron interaction\n\n1. Kuryr initialization should not be dependent on neutron availability.\n2. Currently neutron client creation attributes can not be used from\nconfig file. Reason is neutron client gets created even before config\nfile is used while starting kuryr server.\n\nIn this fix kuryr neutron client creation is being moved from Kuryr\nserver initialization to the the following libnetwork api handlers.\nThese api handlers can be first point of contact where neutron will be\nqueried. For example, /NetworkDriver.Leave and\n/NetworkDriver.DeleteNetwork could be first point of interactions with\nneutron in case Kuryr has got restarted and user tries to delete a\ncontainer or existing network(created before Kuryr restart)\nrespectively.\n\n/IpamDriver.RequestPool\n/IpamDriver.RequestAddress\n/NetworkDriver.DeleteNetwork\n/NetworkDriver.DeleteEndpoint\n\nChange-Id: Id944057b55f0476c078f40fb8355b272dca2b499\nCloses-bug: #1644970\n'}, {'number': 4, 'created': '2016-12-12 05:42:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr-libnetwork/commit/70fe6ac9941608e83f27e245f595b51d0afeb755', 'message': 'Delay neutron extension checks to first neutron interaction\n\n1. Kuryr initialization should not be dependent on neutron availability.\n2. Currently neutron client creation attributes can not be used from\nconfig file. Reason is neutron client gets created even before config\nfile is used while starting kuryr server.\n\nIn this fix kuryr neutron client creation is being moved from Kuryr\nserver initialization to the the following libnetwork api handlers.\nThese api handlers can be first point of contact where neutron will be\nqueried. For example, /NetworkDriver.Leave and\n/NetworkDriver.DeleteNetwork could be first point of interactions with\nneutron in case Kuryr has got restarted and user tries to delete a\ncontainer or existing network(created before Kuryr restart)\nrespectively.\n\n/IpamDriver.RequestPool\n/IpamDriver.RequestAddress\n/NetworkDriver.DeleteNetwork\n/NetworkDriver.DeleteEndpoint\n\nChange-Id: Id944057b55f0476c078f40fb8355b272dca2b499\nCloses-bug: #1644970\n'}, {'number': 5, 'created': '2016-12-12 08:53:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr-libnetwork/commit/b6c302098cde9c4ddf39c0c29e05ef5129ed98f0', 'message': 'Delay neutron extension checks to first neutron interaction\n\n1. Kuryr initialization should not be dependent on neutron availability.\n2. Currently neutron client creation attributes can not be used from\nconfig file. Reason is neutron client gets created even before config\nfile is used while starting kuryr server.\n\nIn this fix kuryr neutron client creation is being moved from Kuryr\nserver initialization to the the following libnetwork api handlers.\nThese api handlers can be first point of contact where neutron will be\nqueried. For example, /NetworkDriver.Leave and\n/NetworkDriver.DeleteNetwork could be first point of interactions with\nneutron in case Kuryr has got restarted and user tries to delete a\ncontainer or existing network(created before Kuryr restart)\nrespectively.\n\n/IpamDriver.RequestPool\n/IpamDriver.RequestAddress\n/NetworkDriver.DeleteNetwork\n/NetworkDriver.DeleteEndpoint\n\nChange-Id: Id944057b55f0476c078f40fb8355b272dca2b499\nCloses-bug: #1644970\n'}, {'number': 6, 'created': '2016-12-14 08:38:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr-libnetwork/commit/2b333a82b6ea07c7330ca2f4caa8023b9ece4c6b', 'message': 'Delay neutron extension checks to first neutron interaction\n\n1. Kuryr initialization should not be dependent on neutron availability.\n2. Currently neutron client creation attributes can not be used from\nconfig file. Reason is neutron client gets created even before config\nfile is used while starting kuryr server.\n\nIn this fix kuryr neutron client creation is being moved from Kuryr\nserver initialization to the the following libnetwork api handlers.\nThese api handlers can be first point of contact where neutron will be\nqueried. For example, /NetworkDriver.Leave and\n/NetworkDriver.DeleteNetwork could be first point of interactions with\nneutron in case Kuryr has got restarted and user tries to delete a\ncontainer or existing network(created before Kuryr restart)\nrespectively.\n\n/IpamDriver.RequestPool\n/IpamDriver.RequestAddress\n/NetworkDriver.DeleteNetwork\n/NetworkDriver.DeleteEndpoint\n\nChange-Id: Id944057b55f0476c078f40fb8355b272dca2b499\nCloses-bug: #1644970\n'}, {'number': 7, 'created': '2016-12-14 10:07:07.000000000', 'files': ['kuryr_libnetwork/tests/unit/test_kuryr.py', 'kuryr_libnetwork/tests/unit/test_config.py', 'kuryr_libnetwork/tests/unit/base.py', 'kuryr_libnetwork/server.py', 'kuryr_libnetwork/tests/unit/test_kuryr_ipam.py', 'kuryr_libnetwork/controllers.py', 'kuryr_libnetwork/utils.py'], 'web_link': 'https://opendev.org/openstack/kuryr-libnetwork/commit/6858d4ba3249d9804fbb8c87bde562f3f3b3f6ad', 'message': 'Delay neutron extension checks to first neutron interaction\n\n1. Kuryr initialization should not be dependent on neutron availability.\n2. Currently neutron client creation attributes can not be used from\nconfig file. Reason is neutron client gets created even before config\nfile is used while starting kuryr server.\n\nIn this fix kuryr neutron client creation is being moved from Kuryr\nserver initialization to the the following libnetwork api handlers.\nThese api handlers can be first point of contact where neutron will be\nqueried. For example, /NetworkDriver.Leave and\n/NetworkDriver.DeleteNetwork could be first point of interactions with\nneutron in case Kuryr has got restarted and user tries to delete a\ncontainer or existing network(created before Kuryr restart)\nrespectively.\n\n/IpamDriver.RequestPool\n/IpamDriver.RequestAddress\n/NetworkDriver.DeleteNetwork\n/NetworkDriver.DeleteEndpoint\n\nChange-Id: Id944057b55f0476c078f40fb8355b272dca2b499\nCloses-bug: #1644970\n'}]",11,403325,6858d4ba3249d9804fbb8c87bde562f3f3b3f6ad,27,8,7,15967,,,0,"Delay neutron extension checks to first neutron interaction

1. Kuryr initialization should not be dependent on neutron availability.
2. Currently neutron client creation attributes can not be used from
config file. Reason is neutron client gets created even before config
file is used while starting kuryr server.

In this fix kuryr neutron client creation is being moved from Kuryr
server initialization to the the following libnetwork api handlers.
These api handlers can be first point of contact where neutron will be
queried. For example, /NetworkDriver.Leave and
/NetworkDriver.DeleteNetwork could be first point of interactions with
neutron in case Kuryr has got restarted and user tries to delete a
container or existing network(created before Kuryr restart)
respectively.

/IpamDriver.RequestPool
/IpamDriver.RequestAddress
/NetworkDriver.DeleteNetwork
/NetworkDriver.DeleteEndpoint

Change-Id: Id944057b55f0476c078f40fb8355b272dca2b499
Closes-bug: #1644970
",git fetch https://review.opendev.org/openstack/kuryr-libnetwork refs/changes/25/403325/2 && git format-patch -1 --stdout FETCH_HEAD,"['kuryr_libnetwork/tests/unit/test_join.py', 'kuryr_libnetwork/tests/unit/test_kuryr.py', 'kuryr_libnetwork/tests/unit/test_config.py', 'kuryr_libnetwork/tests/unit/base.py', 'kuryr_libnetwork/server.py', 'kuryr_libnetwork/tests/unit/test_kuryr_ipam.py', 'kuryr_libnetwork/controllers.py']",7,ce1025499e5b4d1c5b18ff1fa5999186922577ef,bug/1644970, # TODO(vikasc): Handle scenario where neutron api goes down and comes # up again. Supported extensions list may get changed # after restart. Kuryr should confirm for extensions # support again. check_for_neutron_ext_support() check_for_neutron_ext_tag() neutron_client() neutron_client() neutron_client() neutron_client(),,86,17
openstack%2Fironic~master~I8f3e5f4e5c4279c4143ad6202f22c6f1dd94cc52,openstack/ironic,master,I8f3e5f4e5c4279c4143ad6202f22c6f1dd94cc52,Code suggestion for implementing https://blueprints.launchpad.net/ironic/+spec/generic-image-partition-aproach,NEW,2016-12-20 07:04:01.000000000,2017-12-18 04:33:38.000000000,,"[{'_account_id': 6773}, {'_account_id': 10118}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}]","[{'number': 1, 'created': '2016-12-20 07:04:01.000000000', 'files': ['ironic/drivers/modules/deploy_utils.py', 'ironic/drivers/modules/iscsi_deploy.py', 'ironic/tests/unit/db/utils.py', 'ironic/tests/unit/drivers/modules/test_iscsi_deploy.py', 'ironic/tests/unit/drivers/modules/test_deploy_utils.py', 'ironic/tests/unit/objects/utils.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/356d4086ac57100e899eea81f3c7040a6f78e0cd', 'message': 'Code suggestion for implementing\nhttps://blueprints.launchpad.net/ironic/+spec/generic-image-partition-aproach\n\nChange-Id: I8f3e5f4e5c4279c4143ad6202f22c6f1dd94cc52\n'}]",3,412810,356d4086ac57100e899eea81f3c7040a6f78e0cd,9,6,1,22094,,,0,"Code suggestion for implementing
https://blueprints.launchpad.net/ironic/+spec/generic-image-partition-aproach

Change-Id: I8f3e5f4e5c4279c4143ad6202f22c6f1dd94cc52
",git fetch https://review.opendev.org/openstack/ironic refs/changes/10/412810/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/drivers/modules/deploy_utils.py', 'ironic/drivers/modules/iscsi_deploy.py', 'ironic/tests/unit/db/utils.py', 'ironic/tests/unit/drivers/modules/test_iscsi_deploy.py', 'ironic/tests/unit/drivers/modules/test_deploy_utils.py', 'ironic/tests/unit/objects/utils.py']",6,356d4086ac57100e899eea81f3c7040a6f78e0cd,bp/s,"import yaml def create_partition_layout_file(partition_layout_path,storage_part_mb,storage_journal_part_mb): partition_yaml={""partitions"": [{""partition_name"": ""storage"",""size_mb"": storage_part_mb,""fs_type"": ""ext4"" ,""volume_label"": ""storage""}, {""partition_name"": ""storage_journal"",""size_mb"": storage_journal_part_mb,""fs_type"": ""ext4"", ""volume_label"": ""storage_journal""}]} with open(partition_layout_path, 'w') as f: yaml.dump(partition_yaml, f, default_flow_style=False) ",,239,14
openstack%2Fpython-tackerclient~master~I407b856b2b19907b6ec54d975e44efcba7b7363c,openstack/python-tackerclient,master,I407b856b2b19907b6ec54d975e44efcba7b7363c,Reducing number of columns in vim/vnfd/vnf-list,NEW,2016-07-01 09:57:57.000000000,2017-12-18 04:33:23.000000000,,"[{'_account_id': 2874}, {'_account_id': 10487}, {'_account_id': 13380}, {'_account_id': 16237}, {'_account_id': 18955}, {'_account_id': 20986}, {'_account_id': 22061}, {'_account_id': 22588}, {'_account_id': 23317}]","[{'number': 1, 'created': '2016-07-01 09:57:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/6d0a4fa1cb359be930206fd4fad36be8a1b1d0e0', 'message': ""Reducing number of columns in vim/vnfd/vnf-list\n\nAlso, introducing a new subarg to list '--all' which will list all the columns.\ne.g tacker vim-list --all\n\nCloses-Bug: 1593163\n\nChange-Id: I407b856b2b19907b6ec54d975e44efcba7b7363c\n""}, {'number': 2, 'created': '2016-07-01 10:03:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/9bde774a6ba358e6b5fccf1cceb90df46d735c11', 'message': ""Reducing number of columns in vim/vnfd/vnf-list\n\nIntroducing a new subarg to list '--all' which will list all the columns.\ne.g tacker vim-list --all\n\nCloses-Bug: 1593163\n\nChange-Id: I407b856b2b19907b6ec54d975e44efcba7b7363c\n""}, {'number': 3, 'created': '2016-07-05 10:20:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/d0acd28bf429c3e09fbb00d270f8e779e8524fbd', 'message': ""Reducing number of columns in vim/vnfd/vnf-list\n\nAdding a new subarg to list '--long' which shall list all columns.\n\nCloses-Bug: 1593163\n\nChange-Id: I407b856b2b19907b6ec54d975e44efcba7b7363c\n""}, {'number': 4, 'created': '2016-07-13 09:00:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/b65bcef11ed6d730c5c099939465db3f87fc21fc', 'message': ""Reducing number of columns in vim/vnfd/vnf-list\n\nAdding a new subarg to list '--long' which shall list all columns.\n\nCloses-Bug: 1593163\n\nChange-Id: I407b856b2b19907b6ec54d975e44efcba7b7363c\n""}, {'number': 5, 'created': '2016-07-13 09:07:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/d576a7cf2520883db790054f74081eac150c8545', 'message': ""Reducing number of columns in vim/vnfd/vnf-list\n\nAdding a new subarg to list '--long' which shall list all columns.\n\nCloses-Bug: 1593163\n\nChange-Id: I407b856b2b19907b6ec54d975e44efcba7b7363c\n""}, {'number': 6, 'created': '2016-10-18 05:36:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/8630497e1441c3e5f19ac255fcb261a59be62806', 'message': ""Reducing number of columns in vim/vnfd/vnf-list\n\nAdding a new subarg to list '--long' which shall list all columns.\n\nCloses-Bug: 1593163\n\nChange-Id: I407b856b2b19907b6ec54d975e44efcba7b7363c\n""}, {'number': 7, 'created': '2016-10-26 10:12:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/60e774ecc66fba22013d711c7594522a0f99e066', 'message': ""Reducing number of columns in vim/vnfd/vnf-list\n\nAdding a new subarg to list '--long' which shall list all columns.\nalso, adds Add service types in tacker vnfd-list.\n\nCloses-Bug: 1593163\nCloses-Bug: 1592278\nChange-Id: I407b856b2b19907b6ec54d975e44efcba7b7363c\nCo-Authored-By: Manikantha Srinivas Tadi <manikantha.tadi@gmail.com>\n""}, {'number': 8, 'created': '2016-11-23 08:30:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/9f41ac45e68029b7dce593dafa6933c73eae0145', 'message': ""Reducing number of columns in vim/vnfd/vnf-list\n\nAdding a new subarg to list '--long' which shall list all columns.\nalso, adds Add service types in tacker vnfd-list.\n\nCloses-Bug: 1593163\nCloses-Bug: 1592278\nChange-Id: I407b856b2b19907b6ec54d975e44efcba7b7363c\nCo-Authored-By: Manikantha Srinivas Tadi <manikantha.tadi@gmail.com>\n""}, {'number': 9, 'created': '2016-12-02 07:24:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/7b3d5175cca8739d24c4f03456618646aaa7b4d2', 'message': ""Reducing number of columns in vim/vnfd/vnf-list\n\nAdding a new subarg to list '--long' which shall list all columns.\nalso, adds Add service types in tacker vnfd-list.\n\nCloses-Bug: 1593163\nCloses-Bug: 1592278\nChange-Id: I407b856b2b19907b6ec54d975e44efcba7b7363c\nCo-Authored-By: Manikantha Srinivas Tadi <manikantha.tadi@gmail.com>\n""}, {'number': 10, 'created': '2016-12-14 09:06:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/a70341bfdd0d6ba12ae2a0dd6f4ee5bca468cdea', 'message': ""Reducing number of columns in vim/vnfd/vnf-list\n\nAdding a new subarg to list '--long' which shall list all columns.\nalso, adds Add service types in tacker vnfd-list.\n\nCloses-Bug: 1593163\nCloses-Bug: 1592278\nChange-Id: I407b856b2b19907b6ec54d975e44efcba7b7363c\nCo-Authored-By: Manikantha Srinivas Tadi <manikantha.tadi@gmail.com>\n""}, {'number': 11, 'created': '2016-12-14 09:21:37.000000000', 'files': ['tackerclient/tacker/v1_0/__init__.py', 'tackerclient/tacker/v1_0/nfvo/vim.py', 'tackerclient/tacker/v1_0/vnfm/vnfd.py'], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/9b4524d84f5d5368effd8192fa9bc5e95180b03c', 'message': ""Reducing number of columns in vim/vnfd/vnf-list\n\nAdding a new subarg to list '--long' which shall list all columns.\nalso, adds Add service types in tacker vnfd-list.\n\nCloses-Bug: 1593163\nCloses-Bug: 1592278\nChange-Id: I407b856b2b19907b6ec54d975e44efcba7b7363c\nCo-Authored-By: Manikantha Srinivas Tadi <manikantha.tadi@gmail.com>\n""}]",21,336459,9b4524d84f5d5368effd8192fa9bc5e95180b03c,49,9,11,22061,,,0,"Reducing number of columns in vim/vnfd/vnf-list

Adding a new subarg to list '--long' which shall list all columns.
also, adds Add service types in tacker vnfd-list.

Closes-Bug: 1593163
Closes-Bug: 1592278
Change-Id: I407b856b2b19907b6ec54d975e44efcba7b7363c
Co-Authored-By: Manikantha Srinivas Tadi <manikantha.tadi@gmail.com>
",git fetch https://review.opendev.org/openstack/python-tackerclient refs/changes/59/336459/5 && git format-patch -1 --stdout FETCH_HEAD,"['tackerclient/tacker/v1_0/__init__.py', 'tackerclient/tacker/v1_0/vm/vnf.py', 'tackerclient/tacker/v1_0/vm/vnfd.py', 'tackerclient/tacker/v1_0/nfvo/vim.py']",4,6d0a4fa1cb359be930206fd4fad36be8a1b1d0e0,bug/1593163," list_columns_all = ['id', 'type', 'tenant_id', 'name', 'description', 'placement_attr', 'auth_url', 'vim_project', 'auth_cred'] list_columns = ['id', 'name', 'type', 'description', 'auth_url', 'auth_cred']"," list_columns = ['id', 'tenant_id', 'name', 'type', 'description', 'auth_url', 'placement_attr', 'auth_cred']",14,3
openstack%2Fironic~master~If0b871085c08ab9617d97d91cfcb742d4faf796a,openstack/ironic,master,If0b871085c08ab9617d97d91cfcb742d4faf796a,[WIP / POC] Add Intel Node Manager sensor data collection,NEW,2016-12-14 00:04:49.000000000,2017-12-18 04:33:14.000000000,,"[{'_account_id': 5805}, {'_account_id': 10118}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}]","[{'number': 1, 'created': '2016-12-14 00:04:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bc361403a43056774def33d60915bdf6b69c60a3', 'message': '[WIP / POC] Add Intel Node Manager sensor data collection\n\nAdding support for collecting Intel Node Manager sensor data.\n\nThis is currently a Proof of concept and should not be reviewed (yet).\n\nChange-Id: If0b871085c08ab9617d97d91cfcb742d4faf796a\n'}, {'number': 2, 'created': '2016-12-15 00:07:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/02250e8922babf5194a30ee3b48932bd8026cc25', 'message': '[WIP / POC] Add Intel Node Manager sensor data collection\n\nAdding support for collecting Intel Node Manager sensor data.\n\nThis is currently a Proof of concept and should not be reviewed (yet).\n\nChange-Id: If0b871085c08ab9617d97d91cfcb742d4faf796a\n'}, {'number': 3, 'created': '2016-12-16 00:07:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/093ce26ec70b9f6cb68d8ed88cd5c35276686dec', 'message': '[WIP / POC] Add Intel Node Manager sensor data collection\n\nAdding support for collecting Intel Node Manager sensor data.\n\nThis is currently a Proof of concept and should not be reviewed (yet).\n\nChange-Id: If0b871085c08ab9617d97d91cfcb742d4faf796a\n'}, {'number': 4, 'created': '2016-12-16 23:17:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7bee7b1029137b52e6e536b57b92cd8d975ecce0', 'message': '[WIP / POC] Add Intel Node Manager sensor data collection\n\nAdding support for collecting Intel Node Manager sensor data.\n\nThis is currently a Proof of concept and should not be reviewed (yet).\n\nChange-Id: If0b871085c08ab9617d97d91cfcb742d4faf796a\n'}, {'number': 5, 'created': '2016-12-20 19:13:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e0a6456d2f4185492c526b6c982c90ea306541dc', 'message': '[WIP / POC] Add Intel Node Manager sensor data collection\n\nAdding support for collecting Intel Node Manager sensor data.\n\nThis is currently a Proof of concept and should not be reviewed (yet).\n\nChange-Id: If0b871085c08ab9617d97d91cfcb742d4faf796a\n'}, {'number': 6, 'created': '2016-12-20 23:45:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5edd2a3284003fa9451f11b670ad55cfbc8768fd', 'message': '[WIP / POC] Add Intel Node Manager sensor data collection\n\nAdding support for collecting Intel Node Manager sensor data.\n\nThis is currently a Proof of concept and should not be reviewed (yet).\n\nRelated-bug: #1645759\nChange-Id: If0b871085c08ab9617d97d91cfcb742d4faf796a\n'}, {'number': 7, 'created': '2016-12-21 15:32:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/509d53797749cbd7150af721f252bb347660d886', 'message': '[WIP / POC] Add Intel Node Manager sensor data collection\n\nAdding support for collecting Intel Node Manager sensor data.\n\nThis is currently a Proof of concept and should not be reviewed (yet).\n\nRelated-bug: #1645759\nChange-Id: If0b871085c08ab9617d97d91cfcb742d4faf796a\n'}, {'number': 8, 'created': '2016-12-21 19:46:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b640cafa875bd429152d316c7b9998b6eb3cfdbb', 'message': '[WIP / POC] Add Intel Node Manager sensor data collection\n\nAdding support for collecting Intel Node Manager sensor data.\n\nThis is currently a Proof of concept and should not be reviewed (yet).\n\nRelated-bug: #1645759\nChange-Id: If0b871085c08ab9617d97d91cfcb742d4faf796a\n'}, {'number': 9, 'created': '2016-12-23 00:25:23.000000000', 'files': ['ironic/common/exception.py', 'install-guide/source/setup-drivers.rst', 'ironic/drivers/modules/ipmitool.py', 'ironic/tests/unit/drivers/modules/test_ipmitool.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/b145cae8f275cd83846e5978276535276182e2e9', 'message': '[WIP / POC] Add Intel Node Manager sensor data collection\n\nAdding support for collecting Intel Node Manager sensor data.\n\nThis is currently a Proof of concept and should not be reviewed (yet).\n\nRelated-bug: #1645759\nChange-Id: If0b871085c08ab9617d97d91cfcb742d4faf796a\n'}]",0,410465,b145cae8f275cd83846e5978276535276182e2e9,53,5,9,5805,,,0,"[WIP / POC] Add Intel Node Manager sensor data collection

Adding support for collecting Intel Node Manager sensor data.

This is currently a Proof of concept and should not be reviewed (yet).

Related-bug: #1645759
Change-Id: If0b871085c08ab9617d97d91cfcb742d4faf796a
",git fetch https://review.opendev.org/openstack/ironic refs/changes/65/410465/2 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/common/exception.py', 'ironic/drivers/modules/ipmitool.py']",2,bc361403a43056774def33d60915bdf6b69c60a3,intel,"import binascii import collections# The INM template dicts are made according to the spec. They contain the # expected length of each item, And can be used to parse the output of IPMI # INM commands. ONE_RETURN_TEMPLATE = {""ret"": 1} BMC_INFO_TEMPLATE = collections.OrderedDict() BMC_INFO_TEMPLATE['Device_ID'] = 1 BMC_INFO_TEMPLATE['Device_Revision'] = 1 BMC_INFO_TEMPLATE['Firmware_Revision_1'] = 1 BMC_INFO_TEMPLATE['Firmware_Revision_2'] = 1 BMC_INFO_TEMPLATE['IPMI_Version'] = 1 BMC_INFO_TEMPLATE['Additional_Device_support'] = 1 BMC_INFO_TEMPLATE['Manufacturer_ID'] = 3 BMC_INFO_TEMPLATE['Product_ID'] = 2 BMC_INFO_TEMPLATE['Auxiliary_Firmware_Revision'] = 4 NM_STATISTICS_TEMPLATE = collections.OrderedDict() NM_STATISTICS_TEMPLATE['Manufacturer_ID'] = 3 NM_STATISTICS_TEMPLATE['Current_value'] = 2 NM_STATISTICS_TEMPLATE['Minimum_value'] = 2 NM_STATISTICS_TEMPLATE['Maximum_value'] = 2 NM_STATISTICS_TEMPLATE['Average_value'] = 2 NM_STATISTICS_TEMPLATE['Time_stamp'] = 4 NM_STATISTICS_TEMPLATE['Report_period'] = 4 NM_STATISTICS_TEMPLATE[""DomainID_PolicyState""] = 1 NM_GET_DEVICE_ID_TEMPLATE = collections.OrderedDict() NM_GET_DEVICE_ID_TEMPLATE['Device_ID'] = 1 NM_GET_DEVICE_ID_TEMPLATE['Device_revision'] = 1 NM_GET_DEVICE_ID_TEMPLATE['Firmware_revision_1'] = 1 NM_GET_DEVICE_ID_TEMPLATE['Firmware_Revision_2'] = 1 NM_GET_DEVICE_ID_TEMPLATE['IPMI_Version'] = 1 NM_GET_DEVICE_ID_TEMPLATE['Additinal_Device_support'] = 1 NM_GET_DEVICE_ID_TEMPLATE['Manufacturer_ID'] = 3 NM_GET_DEVICE_ID_TEMPLATE['Product_ID_min_version'] = 1 NM_GET_DEVICE_ID_TEMPLATE['Product_ID_major_version'] = 1 NM_GET_DEVICE_ID_TEMPLATE['Implemented_firmware'] = 1 NM_GET_DEVICE_ID_TEMPLATE['Firmware_build_number'] = 1 NM_GET_DEVICE_ID_TEMPLATE['Last_digit_firmware_build_number'] = 1 NM_GET_DEVICE_ID_TEMPLATE['Image_flags'] = 1 NM_GET_VERSION_TEMPLATE = collections.OrderedDict() NM_GET_VERSION_TEMPLATE['Manufacturer_ID'] = 3 NM_GET_VERSION_TEMPLATE['NM_Version'] = 1 NM_GET_VERSION_TEMPLATE['IPMI_Version'] = 1 NM_GET_VERSION_TEMPLATE['Patch_Version'] = 1 NM_GET_VERSION_TEMPLATE['Firmware_Revision_Major'] = 1 NM_GET_VERSION_TEMPLATE['Firmware_Revision_Minor'] = 1 NM_CUPS_UTILIZATION_TEMPLATE = collections.OrderedDict() NM_CUPS_UTILIZATION_TEMPLATE['Manufacturer_ID'] = 3 NM_CUPS_UTILIZATION_TEMPLATE['CPU_Utilization'] = 8 NM_CUPS_UTILIZATION_TEMPLATE['Mem_Utilization'] = 8 NM_CUPS_UTILIZATION_TEMPLATE['IO_Utilization'] = 8 NM_CUPS_INDEX_TEMPLATE = collections.OrderedDict() NM_CUPS_INDEX_TEMPLATE['Manufacturer_ID'] = 3 NM_CUPS_INDEX_TEMPLATE['CUPS_Index'] = 2 # INM functions def check_node_manager(task): """"""Intel Node Manager (INM) check and init Used to initialize INM and check the capability without throwing exception if the node does not support INM. It's safe to call it on non-NodeManager platform. """""" try: _init_node_manager(task) nm_version = _get_node_manager_version(task) except (exception.FailedToInitializeNodeManager, exception.IPMIFailure): return 0 return nm_version def _init_sensor_agent(task): """"""Send the INM init command as Raw :param task: Task object :returns: ONE_RETURN_TEMPLATE """""" inm_init_raw = ""0x0a 0x2c 0x01"" raw_ret = send_raw(task, inm_init_raw) ret = _inm_parse(raw_ret, ONE_RETURN_TEMPLATE) return ret def _init_sensor_agent_process(task): """"""Send the INM init command as Raw :param task: Task object :returns: ONE_RETURN_TEMPLATE """""" inm_init_raw = ""0x0a 0x2c 0x00"" raw_ret = send_raw(task, inm_init_raw) ret = _inm_parse(raw_ret, ONE_RETURN_TEMPLATE) return ret def init_node_manager(task): if _init_sensor_agent_process(task)['ret'] == ['01']: # INM active no need to reactivate it return # Run sensor initialization agent for i in max((CONF.ipmi.retry_timeout // CONF.ipmi.min_command_interval), 1): _init_sensor_agent(task) time.sleep(1) if _init_sensor_agent_process(task)['ret'] == ['01']: return raise exception.FailedToInitializeNodeManager(node=task.node.uuid, error='INM failed To Initialize') def _get_device_id(task): """"""Get the INM device ID :param task: task object :returns: BMC_INFO_TEMPLATE """""" inm_get_device_id_raw = ""0x06 0x01"" raw_ret = send_raw(task, inm_get_device_id_raw) ret = _inm_parse(raw_ret, BMC_INFO_TEMPLATE) return ret def _node_manager_get_device_id(task, chan_info): """"""GET_DEVICE_ID command in Intel Node Manager :param task: a task object :param channel_info: dict with channel and slave info :returns: NM_GET_DEVICE_ID_TEMPLATE """""" inm_get_device_id_raw = ""-b %s -t %s 0x06 0x01"" % (chan_info['channel'], chan_info['slave']) raw_ret = send_raw(task, inm_get_device_id_raw) ret = _inm_parse(raw_ret, NM_GET_DEVICE_ID_TEMPLATE) return ret def _get_node_manager_version(task): """"""Intel Node Manager capability checking This function is used to detect support for Intel Node Manager. Return version number or 0 (Zero if not supported). :param task: Task object :returns: int version number or 0 """""" ret = 0 valid_mfg_id = ['57', '01', '00'] found_mfg_id = _get_device_id(task)['Manufacturer_ID'] if valid_mfg_id != found_mfg_id: # If the manufacturer is not Intel, just set False and return. return ret chan_info_dict = _discover_slave_channel(task) support = _node_manager_get_device_id(task, chan_info_dict)['Implemented_firmware'] # According to Intel Node Manager spec, return value of GET_DEVICE_ID, # bits 3 to 0 shows if Intel NM implemented or not. if int(support[0], 16) & 0xf == 0: return ret nm_version = _node_manager_get_version(task, chan_info_dict)['NM_Version'] if nm_version: nm_version.reverse() ret = int(''.join(nm_version), 16) return ret def _node_manager_get_version(task, chan_info): """"""GET_NODE_MANAGER_VERSION command in Intel Node Manager Byte 4 of the response: 01h - Intel NM 1.0 02h - Intel NM 1.5 03h - Intel NM 2.0 04h - Intel NM 2.5 05h - Intel NM 3.0 :param task: a task object :param channel_info: dict with channel and slave info :returns: NM_GET_VERSION_TEMPLATE """""" inm_get_device_id_raw = ""-b %s -t %s 0x2e 0xca 0x57 0x01 0x00"" % (chan_info['channel'], chan_info['slave']) raw_ret = send_raw(task, inm_get_device_id_raw) ret = _inm_parse(raw_ret, NM_GET_VERSION_TEMPLATE) return ret def _inm_parse(output, template): """"""Parse the return value of INM ipmi command into dict :param output: output of the execution of IPMI command :param template: a dict that contains the expected items of IPMI command and its length. """""" ret = {} index = 0 if not (output and template): return ret output_list = output.strip().replace('\n', '').split(' ') if sum(template.values()) != len(output_list): raise exception.IPMIUnexpectedOutput(msg=""ipmitool output "" ""length mismatch"")) for item in template.items(): index_end = index + item[1] update_value = output_list[index: index_end] ret[item[0]] = update_value index = index_end return ret def _discover_slave_channel(task): """"""Discover target slave address and channel number for INM node :params task: task object :returns: dict {'channel': hex,'slave': hex} """""" file_path = tempfile.mkstemp(dir=CONF.tempdir)[1] dump_sdr(task, file_path) ret = _parse_slave_and_channel(file_path) slave_address = ''.join(['0x', ret[0]]) channel = ''.join(['0x', ret[1]]) # String of channel and slave_address ret_dict = {'channel': channel, 'slave': slave_address} # remove the temp file os.remove(file_path) return ret_dict def _parse_slave_and_channel(file_path): """"""Parse the dumped file to get slave address and channel number. :param file_path: file path of dumped SDR file. :return: slave address and channel number of target device or None if not found. """""" prefix = '5701000d01' # According to Intel Node Manager spec, section 4.5, for Intel NM # discovery OEM SDR records are type C0h. It contains manufacture ID # and OEM data in the record body. # 0-2 bytes are OEM ID, byte 3 is 0Dh and byte 4 is 01h. Byte 5, 6 # is Intel NM device slave address and channel number/sensor owner LUN. with open(file_path, 'rb') as bin_fp: data_str = binascii.hexlify(bin_fp.read()) if six.PY3: data_str = data_str.decode('ascii') oem_id_index = data_str.find(prefix) if oem_id_index != -1: ret = data_str[oem_id_index + len(prefix): oem_id_index + len(prefix) + 4] # Byte 5 is slave address. [7:4] from byte 6 is channel # number, so just pick ret[2] here. return (ret[0:2], ret[2]) ",,272,1
openstack%2Fpython-swiftclient~master~I90812828c5d9db0bc72c2f660e238ec1cd430a9e,openstack/python-swiftclient,master,I90812828c5d9db0bc72c2f660e238ec1cd430a9e,"Replace assertEqual(*, None) with assertIsNone in tests.",NEW,2016-01-13 13:19:54.000000000,2017-12-18 04:32:52.000000000,,[{'_account_id': 8556}],"[{'number': 1, 'created': '2016-01-13 13:19:54.000000000', 'files': ['tests/unit/test_service.py'], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/3bb3275fe2e712b5e89ac50366039804e37583e0', 'message': 'Replace assertEqual(*, None) with assertIsNone in tests.\n\nFor more clear messages in case of failure.\nCloses-bug: #1280522\n\nChange-Id: I90812828c5d9db0bc72c2f660e238ec1cd430a9e\n'}]",0,266906,3bb3275fe2e712b5e89ac50366039804e37583e0,8,1,1,11689,,,0,"Replace assertEqual(*, None) with assertIsNone in tests.

For more clear messages in case of failure.
Closes-bug: #1280522

Change-Id: I90812828c5d9db0bc72c2f660e238ec1cd430a9e
",git fetch https://review.opendev.org/openstack/python-swiftclient refs/changes/06/266906/1 && git format-patch -1 --stdout FETCH_HEAD,['tests/unit/test_service.py'],1,3bb3275fe2e712b5e89ac50366039804e37583e0,, self.assertIsNone(spo.options) self.assertIsNone(sr._content_length) self.assertIsNone(sr._expected_etag) self.assertIsNone(sr._content_length) self.assertIsNone(sr._expected_etag) self.assertIsNone(sr._actual_md5) self.assertIsNone(sr._content_length) self.assertIsNone(sr._expected_etag) self.assertIsNone(sr._actual_md5) self.assertIsNone(sr._expected_etag) self.assertIsNone(se.container) self.assertIsNone(se.obj) self.assertIsNone(se.segment) self.assertIsNone(se.exception) self.assertIsNone(suo.options) self.assertIsNone(suo.options) self.assertIsNone(suo.options) self.assertIsNone(suo.source) self.assertIsNone(suo.options)," self.assertEqual(spo.options, None) self.assertEqual(sr._content_length, None) self.assertEqual(sr._expected_etag, None) self.assertEqual(sr._content_length, None) self.assertEqual(sr._expected_etag, None) self.assertEqual(sr._actual_md5, None) self.assertEqual(sr._content_length, None) self.assertEqual(sr._expected_etag, None) self.assertEqual(sr._actual_md5, None) self.assertEqual(sr._expected_etag, None) self.assertEqual(se.container, None) self.assertEqual(se.obj, None) self.assertEqual(se.segment, None) self.assertEqual(se.exception, None) self.assertEqual(suo.options, None) self.assertEqual(suo.options, None) self.assertEqual(suo.options, None) self.assertEqual(suo.source, None) self.assertEqual(suo.options, None)",19,19
openstack%2Fironic~master~Ie2f281bc032e3cf2679b292cd1b079b0fd46ff35,openstack/ironic,master,Ie2f281bc032e3cf2679b292cd1b079b0fd46ff35,Move port api tests to functional dir,NEW,2016-08-17 07:41:24.000000000,2017-12-18 04:32:47.000000000,,"[{'_account_id': 10342}, {'_account_id': 14937}, {'_account_id': 17998}, {'_account_id': 19339}, {'_account_id': 24268}]","[{'number': 1, 'created': '2016-08-17 07:41:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ca2714ddab62ebef09cafc5664c415bea0b91f1d', 'message': 'Move port api tests to functional dir\n\nPort api tests are all functional tests using a real\npecan server. So move them to functional dir.\n\nChange-Id: Ie2f281bc032e3cf2679b292cd1b079b0fd46ff35\nPartial-bug: #1607679\nPartial-bug: #1491670\n'}, {'number': 2, 'created': '2016-08-19 01:35:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/27050446f928e66648560620cbcd347bdd049ba7', 'message': 'Move port api tests to functional dir\n\nPort api tests are all functional tests using a real\npecan server. So move them to functional dir.\n\nChange-Id: Ie2f281bc032e3cf2679b292cd1b079b0fd46ff35\nPartial-bug: #1607679\nPartial-bug: #1491670\n'}, {'number': 3, 'created': '2016-09-21 06:18:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f11c7a7dd9a747dafa29730ee2436e668c43117e', 'message': 'Move port api tests to functional dir\n\nPort api tests are all functional tests using a real\npecan server. So move them to functional dir.\n\nChange-Id: Ie2f281bc032e3cf2679b292cd1b079b0fd46ff35\nPartial-bug: #1607679\nPartial-bug: #1491670\n'}, {'number': 4, 'created': '2016-11-29 08:43:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fd51e884446b96d9d498bde92a63b6c2b5fc0533', 'message': 'Move port api tests to functional dir\n\nPort api tests are all functional tests using a real\npecan server. So move them to functional dir.\n\nChange-Id: Ie2f281bc032e3cf2679b292cd1b079b0fd46ff35\nPartial-bug: #1607679\nPartial-bug: #1491670\n'}, {'number': 5, 'created': '2016-12-01 02:28:09.000000000', 'files': ['ironic/tests/functional/api/v1/test_ports.py', 'ironic/tests/functional/api/base.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/6051ec1f1a69852c8e7b2fbfa835bd5d9622d21e', 'message': 'Move port api tests to functional dir\n\nPort api tests are all functional tests using a real\npecan server. So move them to functional dir.\n\nChange-Id: Ie2f281bc032e3cf2679b292cd1b079b0fd46ff35\nPartial-bug: #1607679\nPartial-bug: #1491670\n'}]",0,356287,6051ec1f1a69852c8e7b2fbfa835bd5d9622d21e,38,5,5,14937,,,0,"Move port api tests to functional dir

Port api tests are all functional tests using a real
pecan server. So move them to functional dir.

Change-Id: Ie2f281bc032e3cf2679b292cd1b079b0fd46ff35
Partial-bug: #1607679
Partial-bug: #1491670
",git fetch https://review.opendev.org/openstack/ironic refs/changes/87/356287/1 && git format-patch -1 --stdout FETCH_HEAD,['ironic/tests/functional/api/v1/test_ports.py'],1,ca2714ddab62ebef09cafc5664c415bea0b91f1d,bug/1607679,,,0,0
openstack%2Fironic~master~I1d9a591821fe8eaaf2cb574288f89e8387361936,openstack/ironic,master,I1d9a591821fe8eaaf2cb574288f89e8387361936,Cleanup internal_info if auto cleaning is disabled,NEW,2016-07-29 17:51:53.000000000,2017-12-18 04:32:44.000000000,,"[{'_account_id': 10118}, {'_account_id': 10239}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}]","[{'number': 1, 'created': '2016-07-29 17:51:53.000000000', 'files': ['ironic/conductor/manager.py', 'ironic/tests/unit/conductor/test_utils.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/tests/unit/drivers/modules/network/test_flat.py', 'ironic/conductor/utils.py', 'ironic/drivers/modules/network/flat.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/4d068d017aa681f395ad456a769766ad69d1ccfb', 'message': ""Cleanup internal_info if auto cleaning is disabled\n\nRight now it is possible for cleaning to fail. It will cause an\nattempt to tear down cleaning ports, which, if it does not succeed,\nwill leave cleaning_vif_port_id in port(group)'s internal_info, as\nthe port might still exist in neutron. If this happens, and the user\ndecides to disable auto cleaning, he may be stuck with\ncleaning_vif_port_id in internal_info forever, unless he modifies the\nDB manually. This may cause troubles, as DHCP options will be updated\nfor the leftover cleaning port instead of tenant port, or, if the\nport was in fact deleted, to fail, complaining that the port is not\nfound.\n\nChange-Id: I1d9a591821fe8eaaf2cb574288f89e8387361936\n""}]",2,348983,4d068d017aa681f395ad456a769766ad69d1ccfb,10,6,1,12356,,,0,"Cleanup internal_info if auto cleaning is disabled

Right now it is possible for cleaning to fail. It will cause an
attempt to tear down cleaning ports, which, if it does not succeed,
will leave cleaning_vif_port_id in port(group)'s internal_info, as
the port might still exist in neutron. If this happens, and the user
decides to disable auto cleaning, he may be stuck with
cleaning_vif_port_id in internal_info forever, unless he modifies the
DB manually. This may cause troubles, as DHCP options will be updated
for the leftover cleaning port instead of tenant port, or, if the
port was in fact deleted, to fail, complaining that the port is not
found.

Change-Id: I1d9a591821fe8eaaf2cb574288f89e8387361936
",git fetch https://review.opendev.org/openstack/ironic refs/changes/83/348983/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/conductor/manager.py', 'ironic/tests/unit/conductor/test_utils.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/conductor/utils.py', 'ironic/tests/unit/drivers/modules/network/test_flat.py', 'ironic/drivers/modules/network/flat.py']",6,4d068d017aa681f395ad456a769766ad69d1ccfb,disable-auto-clean-handling," :raises: NetworkError, InvalidParameterValue if not uuidutils.is_uuid_like(CONF.neutron.cleaning_network_uuid): raise exception.InvalidParameterValue(_( 'You must provide a valid cleaning network UUID in ' '[neutron]cleaning_network_uuid configuration option.'))", :raises: NetworkError,82,2
openstack%2Fdiskimage-builder~master~Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6,openstack/diskimage-builder,master,Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6,Add support for using infra's mirrors in tests,NEW,2016-12-08 23:34:15.000000000,2017-12-18 04:32:42.000000000,,[{'_account_id': 21741}],"[{'number': 1, 'created': '2016-12-08 23:34:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/a04629edf67c636bafd1bb9708acfd153e6cc280', 'message': ""Add support for using infra's mirrors in tests\n\nLets rely on the mirrors which are created for us in CI.\n\nChange-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6\n""}, {'number': 2, 'created': '2016-12-08 23:50:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/e7c755e37f9846c20bc06086f8a53c741f51b3c1', 'message': ""Add support for using infra's mirrors in tests\n\nLets rely on the mirrors which are created for us in CI.\n\nChange-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6\n""}, {'number': 3, 'created': '2016-12-09 01:51:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/7609f8597c5824c2cb71fd7bdef25037320ea51f', 'message': ""Add support for using infra's mirrors in tests\n\nLets rely on the mirrors which are created for us in CI.\n\nChange-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6\n""}, {'number': 4, 'created': '2016-12-09 02:43:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/f4a777007a2a86ac7483dd0005d1edbe83727b23', 'message': ""Add support for using infra's mirrors in tests\n\nLets rely on the mirrors which are created for us in CI.\n\nChange-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6\n""}, {'number': 5, 'created': '2016-12-09 03:01:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/621b4d20c6a781e1fd3cae08a9200428149352e9', 'message': ""Add support for using infra's mirrors in tests\n\nLets rely on the mirrors which are created for us in CI.\n\nChange-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6\n""}, {'number': 6, 'created': '2016-12-09 03:05:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/5e091ecd18c7cb2bec95f4837245a14786a03d4f', 'message': ""Add support for using infra's mirrors in tests\n\nLets rely on the mirrors which are created for us in CI.\n\nChange-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6\n""}, {'number': 7, 'created': '2016-12-09 03:12:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/5d6e71152614e8ec1b1b5381665f12da225f3077', 'message': ""Add support for using infra's mirrors in tests\n\nLets rely on the mirrors which are created for us in CI.\n\nChange-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6\n""}, {'number': 8, 'created': '2016-12-09 03:41:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/51fb8d6283e18937eae5e59e9b7c14ba99878582', 'message': ""Add support for using infra's mirrors in tests\n\nLets rely on the mirrors which are created for us in CI.\n\nChange-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6\n""}, {'number': 9, 'created': '2016-12-09 03:55:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/fe076afc8f05e601edfe4ecf4b2e24073f7bbac4', 'message': ""Add support for using infra's mirrors in tests\n\nLets rely on the mirrors which are created for us in CI.\n\nChange-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6\n""}, {'number': 10, 'created': '2016-12-10 01:15:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/1e8f729c30fb7a7d13cbc06acd2a042e56548fde', 'message': ""Add support for using infra's mirrors in tests\n\nLets rely on the mirrors which are created for us in CI.\n\nChange-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6\n""}, {'number': 11, 'created': '2016-12-10 01:42:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/b2dce3f607c10a934644cd1daa3b74e14a976324', 'message': ""Add support for using infra's mirrors in tests\n\nLets rely on the mirrors which are created for us in CI.\n\nChange-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6\n""}, {'number': 12, 'created': '2016-12-21 18:28:44.000000000', 'files': ['tests/run_functests.sh', 'tests/elements/openstack-infra-mirrors/environment.d/05-set-mirrors'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/2b5d10ddb0e87dd7cdcd7096a6744231f289dfbb', 'message': ""Add support for using infra's mirrors in tests\n\nLets rely on the mirrors which are created for us in CI.\n\nChange-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6\n""}]",0,408851,2b5d10ddb0e87dd7cdcd7096a6744231f289dfbb,26,1,12,10035,,,0,"Add support for using infra's mirrors in tests

Lets rely on the mirrors which are created for us in CI.

Change-Id: Ic78c3962d4f7d39a28406e50dfaae7344ab7f1f6
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/51/408851/4 && git format-patch -1 --stdout FETCH_HEAD,"['tests/run_functests.sh', 'tests/elements/openstack-infra-mirrors/environment.d/05-set-mirrors']",2,a04629edf67c636bafd1bb9708acfd153e6cc280,408851,NODEPOOL_MIRROR_HOST=${NODEPOOL_MIRROR_HOST:-mirror.$NODEPOOL_REGION.$NODEPOOL_CLOUD.openstack.org} NODEPOOL_MIRROR_HOST=$(echo $NODEPOOL_MIRROR_HOST|tr '[:upper:]' '[:lower:]') export DIB_DEBIAN_DISTRIBUTION_MIRROR=${NODEPOOL_DEBIAN_MIRROR:-http://$NODEPOOL_MIRROR_HOST/debian} export DIB_UBUNTU_DISTRIBUTION_MIRROR=${NODEPOOL_UBUNTU_MIRROR:-http://$NODEPOOL_MIRROR_HOST/ubuntu} export DIB_CENTOS_DISTRIBUTION_MIRROR=${NODEPOOL_CENTOS_MIRROR:-http://$NODEPOOL_MIRROR_HOST/centos} ,,22,0
openstack%2Fgnocchi~master~Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496,openstack/gnocchi,master,Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496,Resurrect influxdb backend driver,NEW,2016-10-24 10:32:22.000000000,2017-12-18 04:32:19.000000000,,"[{'_account_id': 1669}, {'_account_id': 3031}, {'_account_id': 8871}]","[{'number': 1, 'created': '2016-10-24 10:32:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/1abe6063452b378d52456283049bfd3bfc13f850', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 2, 'created': '2016-10-24 10:59:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/9f281209025d4e886fa97d84b38f3146492f8cbc', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 3, 'created': '2016-10-26 14:16:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/6f9ddac89a68cc3201657989c18c44f1996670a1', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 4, 'created': '2016-10-28 10:14:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/63bc5bddd740c8bb4540278d3d082dfc6ece45e2', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 5, 'created': '2016-10-28 10:24:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/4b77738e5269f190f132193643f2ce0e78d517e5', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 6, 'created': '2016-10-29 15:53:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/ff8881467218a853cb19c9adc11f950258f3e176', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 7, 'created': '2016-11-30 05:55:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/3983cc41c6a0e91904140c2f60228faea713fe5e', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 8, 'created': '2016-11-30 06:39:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/3631549141eb6152f32e8c1284f193be8360861d', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 9, 'created': '2016-11-30 07:25:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/eb0ce8d203dc84bcba047f7bb5d7525b21ef49bc', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 10, 'created': '2016-11-30 08:28:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/e72e9fcc3cc1ba805aefe212dbb9769a3ec20b76', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 11, 'created': '2016-11-30 08:57:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/c245cb100efed26c7b1e55eb52b9bcbb3df8639e', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 12, 'created': '2016-11-30 09:52:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/c85f2a9c4014d785468d34d948cf3a8c3255e06b', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 13, 'created': '2016-12-01 00:10:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/b8fa4f26e927a9b4c1574a178819be7b9f18dcde', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 14, 'created': '2016-12-01 02:09:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/6ef243ecc6476a9020c5e544025c423153f8a07a', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 15, 'created': '2016-12-04 23:09:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/dbe95b377d8135be7a176a32c31e99812bd55bf4', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 16, 'created': '2016-12-04 23:29:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/5d94c0808171a57dd6fbe120043bd116b5151009', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 17, 'created': '2016-12-05 21:00:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/e4f4fb1cd713fd7326aef2aebbd53913f14fe758', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 18, 'created': '2016-12-05 21:04:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/0ea145443786b23db3a2f72606b64f1c9bafe56b', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 19, 'created': '2016-12-05 23:02:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/c48ed96769213bc83649c275e11af77ea90f5b34', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 20, 'created': '2016-12-05 23:31:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/c635ed6ab996c44ed689042d824b08bbca2f38e3', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 21, 'created': '2016-12-06 22:47:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/68eb4b0de085d7a5d750c2db7d4b16ed845521c3', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 22, 'created': '2016-12-06 23:22:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/f54240e2eadc908108fb9da1044f11634234112f', 'message': 'WIP Resurrect influxdb backend driver\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 23, 'created': '2016-12-07 22:31:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/f24e9909005df22dc0461949cd007918d831c580', 'message': 'Resurrect influxdb backend driver\n\nThis driver is loosly based on the original driver in gnocchi.\nIt now utilises continuous queries and retention polices to downsample\ndata. It is recommended to use InfluxDB v1.1.0 or greater.\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 24, 'created': '2016-12-14 00:43:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/4ac375f9919c5157387fc94d9d6c5d94378fdca7', 'message': 'Resurrect influxdb backend driver\n\nThis driver is loosly based on the original driver in gnocchi.\nIt now utilises continuous queries and retention polices to downsample\ndata. It is recommended to use InfluxDB v1.1.0 or greater.\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 25, 'created': '2016-12-14 00:50:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/b94d57ca231078ec50176c4807948ac2a3345819', 'message': 'Resurrect influxdb backend driver\n\nThis driver is loosly based on the original driver in gnocchi.\nIt now utilises continuous queries and retention polices to downsample\ndata. It is recommended to use InfluxDB v1.1.0 or greater.\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 26, 'created': '2016-12-14 02:18:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/fcf63cd9d281e8ca3787eed6255c99a316f04d54', 'message': 'Resurrect influxdb backend driver\n\nThis driver is loosly based on the original driver in gnocchi.\nIt now utilises continuous queries and retention polices to downsample\ndata. It is recommended to use InfluxDB v1.1.0 or greater.\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 27, 'created': '2016-12-15 23:51:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/a5562d469cf64f4701a3544c0a1b377d582fc776', 'message': 'Resurrect influxdb backend driver\n\nThis driver is loosly based on the original driver in gnocchi.\nIt now utilises continuous queries and retention polices to downsample\ndata. It is recommended to use InfluxDB v1.1.0 or greater.\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}, {'number': 28, 'created': '2016-12-15 23:55:44.000000000', 'files': ['doc/source/install.rst', 'gnocchi/opts.py', 'gnocchi/storage/__init__.py', 'gnocchi/tests/test_statsd.py', 'doc/source/configuration.rst', 'tools/setup-influxdb-env.sh', 'gnocchi/storage/influxdb.py', 'run-tests.sh', 'gnocchi/tests/test_rest.py', 'gnocchi/rest/__init__.py', 'gnocchi/tests/base.py', 'releasenotes/notes/influxdb-driver-dd1ab3f090f7f31d.yaml', 'gnocchi/tests/test_storage.py', 'setup.cfg', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/d157d60982e3631fa2abf2c035156a603847f5c7', 'message': 'Resurrect influxdb backend driver\n\nThis driver is loosly based on the original driver in gnocchi.\nIt now utilises continuous queries and retention polices to downsample\ndata. It is recommended to use InfluxDB v1.1.0 or greater.\n\nChange-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496\n'}]",102,390260,d157d60982e3631fa2abf2c035156a603847f5c7,72,3,28,3031,,,0,"Resurrect influxdb backend driver

This driver is loosly based on the original driver in gnocchi.
It now utilises continuous queries and retention polices to downsample
data. It is recommended to use InfluxDB v1.1.0 or greater.

Change-Id: Iabb9322c1a1c4f0d8255f366dc9b19e3060e7496
",git fetch https://review.opendev.org/openstack/gnocchi refs/changes/60/390260/28 && git format-patch -1 --stdout FETCH_HEAD,"['gnocchi/opts.py', 'gnocchi/rest/__init__.py', 'gnocchi/tests/base.py', 'gnocchi/storage/__init__.py', 'gnocchi/storage/influxdb.py', 'gnocchi/tests/test_storage.py', 'setup.cfg', 'tox.ini', 'run-tests.sh', 'gnocchi/tests/test_rest.py']",10,1abe6063452b378d52456283049bfd3bfc13f850,influxdb-driver," if self.conf.storage.driver == 'influxdb': self.skipTest(""Influxdb driver handles retention differently"")",,590,7
openstack%2Fopenstack-health~master~Iffca6b2d2025900a0de4c00169eedd685ae810d5,openstack/openstack-health,master,Iffca6b2d2025900a0de4c00169eedd685ae810d5,WIP: Add runtime graph to grouped runs page,NEW,2016-09-15 15:00:03.000000000,2017-12-18 04:32:09.000000000,,"[{'_account_id': 5689}, {'_account_id': 11564}]","[{'number': 1, 'created': '2016-09-15 15:00:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-health/commit/6fcd1c25681271fb2223ca79ea0a2e7387ec897c', 'message': ""WIP: Add runtime graph to grouped runs page\n\nThis patch adds a runtime graph to the grouped runs page to visualize\nthe run times of the different jobs.\n\nThings that need to be sorted still:\n * Index formatting\n * Handle more than 20 job names\n * Appropriate strategy for missing data on stats functions (like avg)\n * A colormap that doesn't suck (and isn't limited to 20 items)\n * Figure out why averages sometimes have the same data points as normal\n   data when graphed together\n * Add a warning about the data currently being sum of all tests\n   executed in run (the infra subunit2sql collection will switch over to\n   wall time in the future)\n * And many more issues I probably can't remember\n\nChange-Id: Iffca6b2d2025900a0de4c00169eedd685ae810d5\n""}, {'number': 2, 'created': '2016-09-15 17:54:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-health/commit/ff1a68a6124a0631534111694d88c9311e6ae4f9', 'message': ""WIP: Add runtime graph to grouped runs page\n\nThis patch adds a runtime graph to the grouped runs page to visualize\nthe run times of the different jobs.\n\nThings that need to be sorted still:\n * Index formatting\n * Handle more than 20 job names\n * Appropriate strategy for missing data on stats functions (like avg)\n * A colormap that doesn't suck (and isn't limited to 20 items)\n * Figure out why averages sometimes have the same data points as normal\n   data when graphed together\n * Add a warning about the data currently being sum of all tests\n   executed in run (the infra subunit2sql collection will switch over to\n   wall time in the future)\n * And many more issues I probably can't remember\n\nDepends-On: I565aef69af1969bc0fb58a7c9459c9937658a19b\nChange-Id: Iffca6b2d2025900a0de4c00169eedd685ae810d5\n""}, {'number': 3, 'created': '2016-09-16 07:13:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-health/commit/af2ac5568db74c2ce7076ee5dc3d5755b02c3e62', 'message': ""WIP: Add runtime graph to grouped runs page\n\nThis patch adds a runtime graph to the grouped runs page to visualize\nthe run times of the different jobs.\n\nThings that need to be sorted still:\n * Index formatting\n * Handle more than 20 job names\n * Appropriate strategy for missing data on stats functions (like avg)\n * A colormap that doesn't suck (and isn't limited to 20 items)\n * Figure out why averages sometimes have the same data points as normal\n   data when graphed together\n * Add a warning about the data currently being sum of all tests\n   executed in run (the infra subunit2sql collection will switch over to\n   wall time in the future)\n * And many more issues I probably can't remember\n\nDepends-On: I565aef69af1969bc0fb58a7c9459c9937658a19b\nChange-Id: Iffca6b2d2025900a0de4c00169eedd685ae810d5\n""}, {'number': 4, 'created': '2017-01-03 19:56:52.000000000', 'files': ['openstack_health/tests/test_api.py', 'openstack_health/run_aggregator.py', 'openstack_health/api.py', 'app/js/controllers/grouped-runs.js', 'app/views/grouped-runs.html'], 'web_link': 'https://opendev.org/openstack/openstack-health/commit/2225710064ee256f9a767eeff0ac7356be09a24e', 'message': ""WIP: Add runtime graph to grouped runs page\n\nThis patch adds a runtime graph to the grouped runs page to visualize\nthe run times of the different jobs.\n\nThings that need to be sorted still:\n * Index formatting\n * Handle more than 20 job names\n * Appropriate strategy for missing data on stats functions (like avg)\n * A colormap that doesn't suck (and isn't limited to 20 items)\n * Figure out why averages sometimes have the same data points as normal\n   data when graphed together\n * And many more issues I probably can't remember\n\nDepends-On: I565aef69af1969bc0fb58a7c9459c9937658a19b\nChange-Id: Iffca6b2d2025900a0de4c00169eedd685ae810d5\n""}]",0,370913,2225710064ee256f9a767eeff0ac7356be09a24e,10,2,4,5196,,,0,"WIP: Add runtime graph to grouped runs page

This patch adds a runtime graph to the grouped runs page to visualize
the run times of the different jobs.

Things that need to be sorted still:
 * Index formatting
 * Handle more than 20 job names
 * Appropriate strategy for missing data on stats functions (like avg)
 * A colormap that doesn't suck (and isn't limited to 20 items)
 * Figure out why averages sometimes have the same data points as normal
   data when graphed together
 * And many more issues I probably can't remember

Depends-On: I565aef69af1969bc0fb58a7c9459c9937658a19b
Change-Id: Iffca6b2d2025900a0de4c00169eedd685ae810d5
",git fetch https://review.opendev.org/openstack/openstack-health refs/changes/13/370913/4 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_health/run_aggregator.py', 'openstack_health/api.py', 'app/js/controllers/grouped-runs.js', 'app/views/grouped-runs.html']",4,6fcd1c25681271fb2223ca79ea0a2e7387ec897c,(HEAD," <div class=""panel panel-default""> <div class=""panel-heading""> <h3 class=""panel-title"">Total Run Time</h3> </div> <div class=""panel-body""> <chart-multi data=""groupedRuns.numericChartData"" width=""100%"" height=""1050"" ></chart-scatter> </div> </div>",,103,9
openstack%2Fmonasca-api~master~Iad1ba3e9498ee2263e497c079d1524fa7ad12439,openstack/monasca-api,master,Iad1ba3e9498ee2263e497c079d1524fa7ad12439,Use tempest-plugin service client registration,NEW,2016-07-06 18:46:32.000000000,2017-12-18 04:32:07.000000000,,"[{'_account_id': 1921}, {'_account_id': 2419}, {'_account_id': 14517}, {'_account_id': 21264}]","[{'number': 1, 'created': '2016-07-06 18:46:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/4683a4568f177998ead102de2c0d28f875cea4c6', 'message': 'DNM Use tempest-plugin service client registration\n\nTempest is going to expose a new optional interface in the plugin\nclass, to automatically register service clients implemented in\na plugin. Along with this the former client manager, renamed to\nServiceClients, is going to move to a stable interface in\ntempest.lib.\n\nThis is a WIP change to test the interface on Tempest side along\nwith a real life plugin.\n\nChange-Id: Iad1ba3e9498ee2263e497c079d1524fa7ad12439\nDepends-on: I2cefbeec66dfdd25a2ae90554904b3c315f0b457\n'}, {'number': 2, 'created': '2016-07-06 20:01:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/7059a9979b83cf4b9044e2bdaba8064950baf1a3', 'message': 'DNM Use tempest-plugin service client registration\n\nTempest is going to expose a new optional interface in the plugin\nclass, to automatically register service clients implemented in\na plugin. Along with this the former client manager, renamed to\nServiceClients, is going to move to a stable interface in\ntempest.lib.\n\nThis is a WIP change to test the interface on Tempest side along\nwith a real life plugin.\n\nChange-Id: Iad1ba3e9498ee2263e497c079d1524fa7ad12439\nDepends-on: I2cefbeec66dfdd25a2ae90554904b3c315f0b457\n'}, {'number': 3, 'created': '2016-07-06 21:04:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/725278c0c480fbc0129dfa16554c199685058cd4', 'message': 'DNM Use tempest-plugin service client registration\n\nTempest is going to expose a new optional interface in the plugin\nclass, to automatically register service clients implemented in\na plugin. Along with this the former client manager, renamed to\nServiceClients, is going to move to a stable interface in\ntempest.lib.\n\nThis is a WIP change to test the interface on Tempest side along\nwith a real life plugin.\n\nChange-Id: Iad1ba3e9498ee2263e497c079d1524fa7ad12439\nDepends-on: I2cefbeec66dfdd25a2ae90554904b3c315f0b457\n'}, {'number': 4, 'created': '2016-07-27 13:13:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/00720326f59680250cce12c6c24cfe1cdd8c3217', 'message': 'DNM Use tempest-plugin service client registration\n\nTempest is going to expose a new optional interface in the plugin\nclass, to automatically register service clients implemented in\na plugin. Along with this the former client manager, renamed to\nServiceClients, is going to move to a stable interface in\ntempest.lib.\n\nThis is a WIP change to test the interface on Tempest side along\nwith a real life plugin.\n\nChange-Id: Iad1ba3e9498ee2263e497c079d1524fa7ad12439\nDepends-on: I6a4845edb95031243bca12a8d03c60cf18528212\n'}, {'number': 5, 'created': '2016-07-27 14:43:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/525bfc7b5cc7425e78b66742fa161fc7ad8abf23', 'message': 'DNM Use tempest-plugin service client registration\n\nTempest is going to expose a new optional interface in the plugin\nclass, to automatically register service clients implemented in\na plugin. Along with this the former client manager, renamed to\nServiceClients, is going to move to a stable interface in\ntempest.lib.\n\nThis is a WIP change to test the interface on Tempest side along\nwith a real life plugin.\n\nChange-Id: Iad1ba3e9498ee2263e497c079d1524fa7ad12439\nDepends-on: I6a4845edb95031243bca12a8d03c60cf18528212\n'}, {'number': 6, 'created': '2016-08-07 18:15:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/e9f7997ff3a666e50c1e8584d474e9c24c485c9d', 'message': 'DNM Use tempest-plugin service client registration\n\nTempest is going to expose a new optional interface in the plugin\nclass, to automatically register service clients implemented in\na plugin. Along with this the former client manager, renamed to\nServiceClients, is going to move to a stable interface in\ntempest.lib.\n\nThis is a WIP change to test the interface on Tempest side along\nwith a real life plugin.\n\nChange-Id: Iad1ba3e9498ee2263e497c079d1524fa7ad12439\nDepends-on: I6a4845edb95031243bca12a8d03c60cf18528212\n'}, {'number': 7, 'created': '2016-08-12 19:13:39.000000000', 'files': ['monasca_tempest_tests/services/monasca_client.py', 'monasca_tempest_tests/tests/api/base.py', 'monasca_tempest_tests/plugin.py', 'monasca_tempest_tests/tests/api/test_read_only_role.py', 'monasca_tempest_tests/services/__init__.py', 'monasca_tempest_tests/clients.py'], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/1b1206cf8024f8dedae0769b40a3e1b8b7b79908', 'message': 'Use tempest-plugin service client registration\n\nTempest exposes a new optional interface in the plugin\nclass, to automatically register service clients implemented in\na plugin. Along with this the former client manager, renamed to\nServiceClients, has been moved to a stable interface in tempest.lib.\n\nChange-Id: Iad1ba3e9498ee2263e497c079d1524fa7ad12439'}]",4,338486,1b1206cf8024f8dedae0769b40a3e1b8b7b79908,33,4,7,1921,,,0,"Use tempest-plugin service client registration

Tempest exposes a new optional interface in the plugin
class, to automatically register service clients implemented in
a plugin. Along with this the former client manager, renamed to
ServiceClients, has been moved to a stable interface in tempest.lib.

Change-Id: Iad1ba3e9498ee2263e497c079d1524fa7ad12439",git fetch https://review.opendev.org/openstack/monasca-api refs/changes/86/338486/7 && git format-patch -1 --stdout FETCH_HEAD,"['monasca_tempest_tests/services/monasca_client.py', 'monasca_tempest_tests/tests/api/base.py', 'monasca_tempest_tests/plugin.py', 'monasca_tempest_tests/services/__init__.py', 'monasca_tempest_tests/clients.py']",5,4683a4568f177998ead102de2c0d28f875cea4c6,bp/client-manager-refactor,"from tempest import config from tempest import service_clients CONF = config.CONF class Manager(service_clients.ServiceClients): """"""Tempest stable service clients and loaded plugins service clients"""""" def __init__(self, credentials): # Identity settings if CONF.identity.auth_version == 'v2': identity_uri = CONF.identity.uri else: identity_uri = CONF.idenity.uri_v3 # Stable client settings parameters = {} for service in ['compute', 'network', 'image']: parameters[service] = config.service_client_config(service) super(Manager, self).__init__(credentials, identity_uri, client_parameters=parameters)","from tempest import clients from monasca_tempest_tests.services import monasca_client class Manager(clients.Manager): def __init__(self, credentials=None, service=None): super(Manager, self).__init__(credentials, service) self.monasca_client = monasca_client.MonascaClient(self.auth_provider)",51,14
openstack%2Fstorlets~master~I8e32e55fb684b77367489c945801ad6b78f0a1c2,openstack/storlets,master,I8e32e55fb684b77367489c945801ad6b78f0a1c2,WIP: add test for long running tasks to find which part should be fixed,NEW,2016-12-06 06:45:23.000000000,2017-12-18 04:32:02.000000000,,"[{'_account_id': 4608}, {'_account_id': 11317}]","[{'number': 1, 'created': '2016-12-06 06:45:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/7dcd9853dd33e29c6abbf413f20c7f4fad2ace5c', 'message': 'WIP: add test for long running tasks to find which part should be fixed\n\nChange-Id: I8e32e55fb684b77367489c945801ad6b78f0a1c2\n'}, {'number': 2, 'created': '2016-12-06 06:49:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/c17cbd051ac51180362febe4de564ba14c8fae55', 'message': 'WIP: add test for long running tasks to find which part should be fixed\n\nChange-Id: I8e32e55fb684b77367489c945801ad6b78f0a1c2\n'}, {'number': 3, 'created': '2016-12-06 07:26:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/a97f3f668ab26c16318fea107068c27a056303e6', 'message': 'WIP: add test for long running tasks to find which part should be fixed\n\nChange-Id: I8e32e55fb684b77367489c945801ad6b78f0a1c2\n'}, {'number': 4, 'created': '2016-12-06 08:02:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/69297faf92633178e940abb390255e4d4d0436f8', 'message': 'WIP: add test for long running tasks to find which part should be fixed\n\nChange-Id: I8e32e55fb684b77367489c945801ad6b78f0a1c2\n'}, {'number': 5, 'created': '2016-12-06 08:32:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/edbc3394018df8f912535fde3a3522fb62464529', 'message': 'WIP: add test for long running tasks to find which part should be fixed\n\nChange-Id: I8e32e55fb684b77367489c945801ad6b78f0a1c2\n'}, {'number': 6, 'created': '2016-12-06 10:21:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/62a2aac139baebda5a2c7e20f7e25c9c47852892', 'message': 'WIP: add test for long running tasks to find which part should be fixed\n\nChange-Id: I8e32e55fb684b77367489c945801ad6b78f0a1c2\n'}, {'number': 7, 'created': '2016-12-09 02:05:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/79d76d034a2d4884f5ad3b951a64907ad7687d87', 'message': 'WIP: add test for long running tasks to find which part should be fixed\n\nChange-Id: I8e32e55fb684b77367489c945801ad6b78f0a1c2\n'}, {'number': 8, 'created': '2016-12-09 11:07:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/19257fc69eaa2a7b61f6890da129f172cf107e1f', 'message': 'WIP: add test for long running tasks to find which part should be fixed\n\nChange-Id: I8e32e55fb684b77367489c945801ad6b78f0a1c2\n'}, {'number': 9, 'created': '2017-01-04 02:51:12.000000000', 'files': ['StorletSamples/python/storlet_samples/long/__init__.py', 'tests/functional/python/test_long_run_storlet.py', 'StorletSamples/python/storlet_samples/long/source.txt', 'StorletSamples/python/storlet_samples/long/long.py'], 'web_link': 'https://opendev.org/openstack/storlets/commit/6f1e1b858a849da5d7cc9c9ca5f15ce496d02b3d', 'message': 'WIP: add test for long running tasks to find which part should be fixed\n\nChange-Id: I8e32e55fb684b77367489c945801ad6b78f0a1c2\n'}]",0,407342,6f1e1b858a849da5d7cc9c9ca5f15ce496d02b3d,21,2,9,4608,,,0,"WIP: add test for long running tasks to find which part should be fixed

Change-Id: I8e32e55fb684b77367489c945801ad6b78f0a1c2
",git fetch https://review.opendev.org/openstack/storlets refs/changes/42/407342/6 && git format-patch -1 --stdout FETCH_HEAD,"['StorletSamples/python/long/long.py', 'StorletSamples/python/long/source.txt', 'tests/functional/python/test_long_run_storlet.py']",3,7dcd9853dd33e29c6abbf413f20c7f4fad2ace5c,long-running,"# Copyright (c) 2010-2016 OpenStack Foundation # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. from swiftclient import client from tests.functional.python import StorletPythonFunctionalTest class TestSimpleStorlet(StorletPythonFunctionalTest): def setUp(self): self.storlet_log = 'simple.log' self.content = 'abcdefghijklmonp' self.additional_headers = {} super(TestSimpleStorlet, self).setUp('long', 'long.py', 'long.LongRunStorlet', 'myobjects', 'source.txt') def test_get(self): resp = dict() req_headers = {'X-Run-Storlet': self.storlet_name} headers, content = client.get_object( self.url, self.token, self.container, self.storlet_file, response_dict=resp, headers=req_headers) self.assertEqual(200, resp['status']) self.assertEqual('long', headers['x-object-meta-test']) self.assertEqual(self.content, content) def test_put_get_delete(self): objname = self.storlet_file + '-put' resp = dict() req_headers = {'X-Run-Storlet': self.storlet_name} client.put_object( self.url, self.token, self.container, objname, self.content, response_dict=resp, headers=req_headers) self.assertEqual(201, resp['status']) resp = dict() headers, content = client.get_object( self.url, self.token, self.container, objname, response_dict=resp) self.assertEqual(200, resp['status']) self.assertEqual('long', headers['x-object-meta-test']) self.assertEqual(self.content, content) resp = dict() client.delete_object( self.url, self.token, self.container, objname, response_dict=resp) self.assertEqual(204, resp['status']) class TestSimpleStorletOnProxy(TestSimpleStorlet): def setUp(self): super(TestSimpleStorletOnProxy, self).setUp() self.additional_headers = {'X-Storlet-Run-On-Proxy': ''} ",,120,0
openstack%2Fpython-aodhclient~master~I882f713e3bed50f2eb72e2e374a7803710966029,openstack/python-aodhclient,master,I882f713e3bed50f2eb72e2e374a7803710966029,Create generic alarm api cli,NEW,2016-12-20 10:18:02.000000000,2017-12-18 04:31:24.000000000,,[{'_account_id': 19122}],"[{'number': 1, 'created': '2016-12-20 10:18:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-aodhclient/commit/1e1fa9513d5d8b579b9c3a5936f43f7293236124', 'message': 'Create generic alarm api cli\n\nAdding generic alarm support in aodh client including tests.\nThe new alarm has a new property called metadata, which will include the external information for the alarm.\n\nImplements: blueprint/generic-alarm\nChange-Id: I882f713e3bed50f2eb72e2e374a7803710966029\n'}, {'number': 2, 'created': '2016-12-20 11:32:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-aodhclient/commit/2f1251a7e10332f2ccabdb4cd6f10d8e3000c1b0', 'message': 'Create generic alarm api cli\n\nAdding generic alarm support in aodh client including tests.\nThe new alarm has a new property called metadata, which will include the external information for the alarm.\n\nImplements: blueprint/generic-alarm\n\nChange-Id: I882f713e3bed50f2eb72e2e374a7803710966029\n'}, {'number': 3, 'created': '2016-12-20 16:17:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-aodhclient/commit/b8dc55954f0d81d1b09dea05a5e71ff0a388e7ff', 'message': 'Create generic alarm api cli\n\nAdding generic alarm support in aodh client including tests.\nThe new alarm has a new property called metadata, which will include the external information for the alarm.\n\nImplements: blueprint/generic-alarm\n\nChange-Id: I882f713e3bed50f2eb72e2e374a7803710966029\n'}, {'number': 4, 'created': '2016-12-21 14:26:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-aodhclient/commit/b1158f8864c8d416476f6927526f62bbe7c7d863', 'message': 'Create generic alarm api cli\n\nAdding generic alarm support in aodh client including tests.\nGeneric alarms will include metadata fields and will post immediate alarm\nnotifications upon state change, but do not require evaluation as these are\ntriggered externally.\n\nImplements: blueprint generic-alarm\n\nChange-Id: I882f713e3bed50f2eb72e2e374a7803710966029\n'}, {'number': 5, 'created': '2016-12-22 09:43:39.000000000', 'files': ['aodhclient/tests/functional/test_alarm.py', 'aodhclient/v2/alarm.py', 'aodhclient/tests/unit/test_alarm_cli.py', 'aodhclient/v2/alarm_cli.py', 'aodhclient/tests/unit/test_alarm_manager.py'], 'web_link': 'https://opendev.org/openstack/python-aodhclient/commit/6899332f43c6fbc013766d725e804ecfa434212f', 'message': 'Create generic alarm api cli\n\nAdding generic alarm support in aodh client including tests.\nGeneric alarms will include metadata fields and will post immediate alarm\nnotifications upon state change, but do not require evaluation as these are\ntriggered externally.\n\nImplements: blueprint generic-alarm\n\nChange-Id: I882f713e3bed50f2eb72e2e374a7803710966029\n'}]",0,413008,6899332f43c6fbc013766d725e804ecfa434212f,14,1,5,19122,,,0,"Create generic alarm api cli

Adding generic alarm support in aodh client including tests.
Generic alarms will include metadata fields and will post immediate alarm
notifications upon state change, but do not require evaluation as these are
triggered externally.

Implements: blueprint generic-alarm

Change-Id: I882f713e3bed50f2eb72e2e374a7803710966029
",git fetch https://review.opendev.org/openstack/python-aodhclient refs/changes/08/413008/4 && git format-patch -1 --stdout FETCH_HEAD,"['aodhclient/tests/functional/test_alarm.py', 'aodhclient/v2/alarm.py', 'aodhclient/v2/alarm_cli.py']",3,1e1fa9513d5d8b579b9c3a5936f43f7293236124,bp/generic-alarm,"ALARM_TYPES = ['threshold', 'event', 'composite', 'generic', generic_group = parser.add_argument_group('generic alarm') generic_group.add_argument( '--metadata', metavar='<METADATA>', dest='metadata', help='Metadata of the generic alarm') alarm['generic_rule'] = (utils.dict_from_parsed_args(parsed_args, [])) ","ALARM_TYPES = ['threshold', 'event', 'composite',",93,1
openstack%2Fironic~master~Idb739bbf5da03cd8c423921deece2520c593dfdb,openstack/ironic,master,Idb739bbf5da03cd8c423921deece2520c593dfdb,Do not execute conductor periodic tasks if they are disabled,NEW,2016-12-28 17:47:50.000000000,2017-12-18 04:30:35.000000000,,"[{'_account_id': 7711}, {'_account_id': 10118}, {'_account_id': 12356}, {'_account_id': 13295}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}]","[{'number': 1, 'created': '2016-12-28 17:47:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5774bab5a18277754500a52b23dcafc4b6268b5d', 'message': 'Do not execute conductor periodic tasks if they are disabled\n\nDo not try to execute  _check_deploy_timeouts and\n_check_cleanwait_timeouts periodical tasks if they are\ndisabled via interval config value.\n\nChange-Id: Idb739bbf5da03cd8c423921deece2520c593dfdb\n'}, {'number': 2, 'created': '2016-12-29 13:28:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e4a69fd99091ba0dab946a1df0767258c178d254', 'message': 'Do not execute conductor periodic tasks if they are disabled\n\nDo not try to execute  _check_deploy_timeouts and\n_check_cleanwait_timeouts periodical tasks if they are\ndisabled via interval config value. Fix invalid zero value\nof Swift timeout if _check_deploy_timeouts disabled.\nUse futurist parameter for enable or disable _send_sensor_data\nperiodical task. Help for conductor periodical options\nchanged to consistent state.\n\nCloses-Bug: 1653112\nChange-Id: Idb739bbf5da03cd8c423921deece2520c593dfdb\n'}, {'number': 3, 'created': '2016-12-29 13:31:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9971a7a1b826b2e587c7b855c7bb1f89a13d64a0', 'message': 'Do not execute conductor periodic tasks if they are disabled\n\nDo not try to execute  _check_deploy_timeouts and\n_check_cleanwait_timeouts periodical tasks if they are\ndisabled via interval config value. Fix invalid zero value\nof Swift timeout if _check_deploy_timeouts disabled.\nUse futurist parameter for enable or disable _send_sensor_data\nperiodical task. Help for conductor periodical options\nchanged to consistent state.\n\nCloses-Bug: 1653112\nChange-Id: Idb739bbf5da03cd8c423921deece2520c593dfdb\n'}, {'number': 4, 'created': '2016-12-30 12:20:12.000000000', 'files': ['ironic/conf/conductor.py', 'ironic/conductor/manager.py', 'etc/ironic/ironic.conf.sample', 'ironic/tests/unit/conductor/test_utils.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/conductor/utils.py', 'releasenotes/notes/invalid-swift-timeout-51c870aadb749342.yaml'], 'web_link': 'https://opendev.org/openstack/ironic/commit/862a58da6251e5c0a67bcdb6346641d3f2b1c2cf', 'message': 'Do not execute conductor periodic tasks if they are disabled\n\nDo not try to execute  _check_deploy_timeouts and\n_check_cleanwait_timeouts periodical tasks if they are\ndisabled via interval config value. Fix invalid zero value\nof Swift timeout if _check_deploy_timeouts disabled.\nUse futurist parameter for enable or disable _send_sensor_data\nperiodical task. Help for conductor periodical options\nchanged to consistent state.\n\nCloses-Bug: 1653112\nChange-Id: Idb739bbf5da03cd8c423921deece2520c593dfdb\n'}]",13,415517,862a58da6251e5c0a67bcdb6346641d3f2b1c2cf,29,7,4,7711,,,0,"Do not execute conductor periodic tasks if they are disabled

Do not try to execute  _check_deploy_timeouts and
_check_cleanwait_timeouts periodical tasks if they are
disabled via interval config value. Fix invalid zero value
of Swift timeout if _check_deploy_timeouts disabled.
Use futurist parameter for enable or disable _send_sensor_data
periodical task. Help for conductor periodical options
changed to consistent state.

Closes-Bug: 1653112
Change-Id: Idb739bbf5da03cd8c423921deece2520c593dfdb
",git fetch https://review.opendev.org/openstack/ironic refs/changes/17/415517/4 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/conductor/manager.py', 'ironic/tests/unit/conductor/test_utils.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/conductor/utils.py']",4,5774bab5a18277754500a52b23dcafc4b6268b5d,bug/1653112,"from ironic.conf import conductor as conductor_opts def get_default_opt_value(opt_name): """"""Get default value of config option from conductor group."""""" for opt in conductor_opts.opts: if opt_name == opt.name: return opt.default else: raise exception.NotFound(_('Config option %s is not found in ' '""conductor"" group.') % opt_name)",,46,16
openstack%2Fironic~master~I045ec8a10bc0fb06359399a444c854278c46db4b,openstack/ironic,master,I045ec8a10bc0fb06359399a444c854278c46db4b,Remove node update from conductor change_node_power_state(),NEW,2016-08-23 15:23:59.000000000,2017-12-18 04:30:33.000000000,,"[{'_account_id': 7711}, {'_account_id': 12356}, {'_account_id': 13295}, {'_account_id': 14525}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19339}, {'_account_id': 20311}]","[{'number': 1, 'created': '2016-08-23 15:23:59.000000000', 'files': ['ironic/conductor/manager.py', 'ironic/tests/unit/conductor/test_utils.py', 'ironic/conductor/utils.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/bbd0fb0ba357502e949af162c2c2c17b9b06e178', 'message': ""Remove node update from conductor change_node_power_state()\n\nThere is inconsistency between change node.target_power_state when\npower operation is executed from ironic internally or via API. If\nAPI request used actual operation can not be started if error occurs\nduring spawning the worker thread.\nThis patch removes node update from conductor's change_node_power_state()\nmethod, this update always will be done via utils.node_power_action().\nThere is no conflict with RFC 7231,\nhttps://tools.ietf.org/html/rfc7231#section-6.3.3\n\nCo-Authored-By: Mario Villaplana <mario.villaplana@gmail.com>\nChange-Id: I045ec8a10bc0fb06359399a444c854278c46db4b\n""}]",3,359287,bbd0fb0ba357502e949af162c2c2c17b9b06e178,13,8,1,7711,,,0,"Remove node update from conductor change_node_power_state()

There is inconsistency between change node.target_power_state when
power operation is executed from ironic internally or via API. If
API request used actual operation can not be started if error occurs
during spawning the worker thread.
This patch removes node update from conductor's change_node_power_state()
method, this update always will be done via utils.node_power_action().
There is no conflict with RFC 7231,
https://tools.ietf.org/html/rfc7231#section-6.3.3

Co-Authored-By: Mario Villaplana <mario.villaplana@gmail.com>
Change-Id: I045ec8a10bc0fb06359399a444c854278c46db4b
",git fetch https://review.opendev.org/openstack/ironic refs/changes/87/359287/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/conductor/manager.py', 'ironic/tests/unit/conductor/test_utils.py', 'ironic/conductor/utils.py']",3,bbd0fb0ba357502e949af162c2c2c17b9b06e178,target-power-state-conductor," # Set the target_power_state and clear any last_error, if we're # starting a new operation. This will expose to other processes # and clients that work is in progress. node['target_power_state'] = target_state node['last_error'] = None node.save()def power_state_error_handler(e, node): """"""Handle spawning error for node power state change. ""an action on node %(node)s, keeping node's "" ""power state as %(power_state)s.""), {'node': node.uuid, 'power_state': node.power_state})"," node['last_error'] = None # Set the target_power_state and clear any last_error, if we're # starting a new operation. This will expose to other processes # and clients that work is in progress. if node['target_power_state'] != target_state: node['target_power_state'] = target_state node['last_error'] = None node.save() def power_state_error_handler(e, node, power_state): """"""Set the node's power states if error occurs. :param power_state: the power state to set on the node. node.power_state = power_state node.target_power_state = states.NOSTATE ""an action on node %(node)s, setting node's "" ""power state back to %(power_state)s.""), {'node': node.uuid, 'power_state': power_state})",14,32
openstack%2Fswift~master~I905f681fdb68e003a8a72f48e8e7fac686125de8,openstack/swift,master,I905f681fdb68e003a8a72f48e8e7fac686125de8,Refactoring the expiring objects feature,NEW,2015-12-01 22:17:05.000000000,2017-12-18 04:29:42.000000000,,"[{'_account_id': 1179}, {'_account_id': 4608}, {'_account_id': 7433}, {'_account_id': 13052}, {'_account_id': 13297}]","[{'number': 1, 'created': '2015-12-01 22:17:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/5d50740c2b71591479c432ae45957fdaeef63c18', 'message': 'Refactoring the expiring objects feature\n\nThis change allows for expired objects to be deletedi from disk\nby the object auditor. Also, the container replicator will be\nremoving expired objects out of the container databases before\nreplicating. By changing from a expiring objects daemon to this\napproach the work for the expiring objects feature is spread out.\n\nDocImpact\nImplements: spec expiring_objects_rework\nChange-Id: I905f681fdb68e003a8a72f48e8e7fac686125de8\n'}, {'number': 2, 'created': '2016-01-05 22:44:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/01997f9baf8c96cf52d309a69d6f00f1c740f371', 'message': 'Refactoring the expiring objects feature\n\nThis change allows for expired objects to be deleted from disk\nby the object auditor. Also, the container replicator will be\nremoving expired objects out of the container databases before\nreplicating.\n\nFor pre-existing container databases, upon the first PUT object\nrequst after a code update the container db will be updated with the new\nexpired table. This allowes for a easy transition to the new\ndesign for expiring objects.\n\nDocImpact\nImplements: spec expiring_objects_rework\nChange-Id: I905f681fdb68e003a8a72f48e8e7fac686125de8\n'}, {'number': 3, 'created': '2016-01-06 17:25:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/094009ba25a11b9b02b2c8c9dfbc2e71cd513085', 'message': 'Refactoring the expiring objects feature\n\nThis change allows for expired objects to be deleted from disk\nby the object auditor. Also, the container replicator will be\nremoving expired objects out of the container databases before\nreplicating.\n\nFor pre-existing container databases, upon the first PUT object\nrequst after a code update the container db will be updated with the new\nexpired table. This allowes for a easy transition to the new\ndesign for expiring objects.\n\nDocImpact\nImplements: spec expiring_objects_rework\nChange-Id: I905f681fdb68e003a8a72f48e8e7fac686125de8\n'}, {'number': 4, 'created': '2016-01-07 19:22:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2be427673f0b8fc28d04bce12ee8ee57b2c542e5', 'message': 'Refactoring the expiring objects feature\n\nThis change allows for expired objects to be deleted from disk\nby the object auditor. Also, the container replicator will be\nremoving expired objects out of the container databases before\nreplicating.\n\nFor pre-existing container databases, upon the first PUT object\nrequst after a code update the container db will be updated with the new\nexpired table. This allowes for a easy transition to the new\ndesign for expiring objects.\n\nDocImpact\nImplements: spec expiring_objects_rework\nChange-Id: I905f681fdb68e003a8a72f48e8e7fac686125de8\n'}, {'number': 5, 'created': '2016-01-11 18:48:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/46b6417b7362ec63387b97573ca23805f7616536', 'message': 'Refactoring the expiring objects feature\n\nThis change allows for expired objects to be deleted from disk\nby the object auditor. Also, the container replicator will be\nremoving expired objects out of the container databases before\nreplicating.\n\nFor pre-existing container databases, upon the first PUT object\nrequst after a code update the container db will be updated with the new\nexpired table. This allowes for a easy transition to the new\ndesign for expiring objects.\n\nDocImpact\nImplements: spec expiring_objects_rework\nChange-Id: I905f681fdb68e003a8a72f48e8e7fac686125de8\n'}, {'number': 6, 'created': '2016-01-11 21:54:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0824db004d7f5dabd4fbd75e9e242898f5c141aa', 'message': 'Refactoring the expiring objects feature\n\nThis change allows for expired objects to be deleted from disk\nby the object auditor. Also, the container replicator will be\nremoving expired objects out of the container databases before\nreplicating.\n\nFor pre-existing container databases, upon the first PUT object\nrequst after a code update the container db will be updated with the new\nexpired table. This allowes for a easy transition to the new\ndesign for expiring objects.\n\nDocImpact\nImplements: spec expiring_objects_rework\nChange-Id: I905f681fdb68e003a8a72f48e8e7fac686125de8\n'}, {'number': 7, 'created': '2017-01-11 18:55:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8fb7c0adfaa7290a87e91a56afd817c20b7f46ef', 'message': 'Refactoring the expiring objects feature\n\nThis change allows for expired objects to be deleted from disk\nby the object auditor. Also, the container replicator will be\nremoving expired objects out of the container databases before\nreplicating.\n\nFor pre-existing container databases, upon the first PUT object\nrequst after a code update the container db will be updated with the new\nexpired table. This allowes for a easy transition to the new\ndesign for expiring objects.\n\nDocImpact\nImplements: spec expiring_objects_rework\nChange-Id: I905f681fdb68e003a8a72f48e8e7fac686125de8\n'}, {'number': 8, 'created': '2017-01-11 19:03:25.000000000', 'files': ['swift/obj/server.py', 'test/unit/container/test_replicator.py', 'swift/obj/auditor.py', 'test/probe/test_expiring_objects.py', 'test/unit/obj/test_server.py', 'swift/container/replicator.py', 'test/unit/container/test_backend.py', 'test/unit/obj/test_auditor.py', 'swift/container/server.py', 'swift/common/db_replicator.py', 'doc/source/overview_expiring_objects.rst', 'swift/container/backend.py', 'swift/obj/diskfile.py', 'test/probe/test_object_expirer.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/1052de90306b6295c53af1ed89c2acd3767fdffc', 'message': 'Refactoring the expiring objects feature\n\nThis change allows for expired objects to be deleted from disk\nby the object auditor. Also, the container replicator will be\nremoving expired objects out of the container databases before\nreplicating.\n\nFor pre-existing container databases, upon the first PUT object\nrequst after a code update the container db will be updated with the new\nexpired table. This allowes for a easy transition to the new\ndesign for expiring objects.\n\nDocImpact\nImplements: spec expiring_objects_rework\nCo-Authored-By: Richard Hawkins <hurricanerix@gmail.com>\nChange-Id: I905f681fdb68e003a8a72f48e8e7fac686125de8\n'}]",12,252085,1052de90306b6295c53af1ed89c2acd3767fdffc,31,5,8,13297,,,0,"Refactoring the expiring objects feature

This change allows for expired objects to be deleted from disk
by the object auditor. Also, the container replicator will be
removing expired objects out of the container databases before
replicating.

For pre-existing container databases, upon the first PUT object
requst after a code update the container db will be updated with the new
expired table. This allowes for a easy transition to the new
design for expiring objects.

DocImpact
Implements: spec expiring_objects_rework
Co-Authored-By: Richard Hawkins <hurricanerix@gmail.com>
Change-Id: I905f681fdb68e003a8a72f48e8e7fac686125de8
",git fetch https://review.opendev.org/openstack/swift refs/changes/85/252085/4 && git format-patch -1 --stdout FETCH_HEAD,"['swift/obj/server.py', 'test/unit/container/test_replicator.py', 'swift/obj/auditor.py', 'test/probe/test_expiring_objects.py', 'test/unit/obj/test_server.py', 'swift/container/replicator.py', 'test/unit/container/test_backend.py', 'test/unit/obj/test_auditor.py', 'swift/container/server.py', 'swift/common/db_replicator.py', 'doc/source/overview_expiring_objects.rst', 'swift/container/backend.py', 'swift/obj/diskfile.py', 'test/probe/test_object_expirer.py']",14,5d50740c2b71591479c432ae45957fdaeef63c18,expiring_objects_refactor,,"#!/usr/bin/python -u # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. import random import uuid import unittest from nose import SkipTest from swift.common.internal_client import InternalClient from swift.common.manager import Manager from swift.common.utils import Timestamp from test.probe.common import ReplProbeTest, ENABLED_POLICIES from test.probe.test_container_merge_policy_index import BrainSplitter from swiftclient import client class TestObjectExpirer(ReplProbeTest): def setUp(self): if len(ENABLED_POLICIES) < 2: raise SkipTest('Need more than one policy') self.expirer = Manager(['object-expirer']) self.expirer.start() err = self.expirer.stop() if err: raise SkipTest('Unable to verify object-expirer service') conf_files = [] for server in self.expirer.servers: conf_files.extend(server.conf_files()) conf_file = conf_files[0] self.client = InternalClient(conf_file, 'probe-test', 3) super(TestObjectExpirer, self).setUp() self.container_name = 'container-%s' % uuid.uuid4() self.object_name = 'object-%s' % uuid.uuid4() self.brain = BrainSplitter(self.url, self.token, self.container_name, self.object_name) def test_expirer_object_split_brain(self): old_policy = random.choice(ENABLED_POLICIES) wrong_policy = random.choice([p for p in ENABLED_POLICIES if p != old_policy]) # create an expiring object and a container with the wrong policy self.brain.stop_primary_half() self.brain.put_container(int(old_policy)) self.brain.put_object(headers={'X-Delete-After': 2}) # get the object timestamp metadata = self.client.get_object_metadata( self.account, self.container_name, self.object_name, headers={'X-Backend-Storage-Policy-Index': int(old_policy)}) create_timestamp = Timestamp(metadata['x-timestamp']) self.brain.start_primary_half() # get the expiring object updates in their queue, while we have all # the servers up Manager(['object-updater']).once() self.brain.stop_handoff_half() self.brain.put_container(int(wrong_policy)) # don't start handoff servers, only wrong policy is available # make sure auto-created containers get in the account listing Manager(['container-updater']).once() # this guy should no-op since it's unable to expire the object self.expirer.once() self.brain.start_handoff_half() self.get_to_final_state() # validate object is expired found_in_policy = None metadata = self.client.get_object_metadata( self.account, self.container_name, self.object_name, acceptable_statuses=(4,), headers={'X-Backend-Storage-Policy-Index': int(old_policy)}) self.assertTrue('x-backend-timestamp' in metadata) self.assertEqual(Timestamp(metadata['x-backend-timestamp']), create_timestamp) # but it is still in the listing for obj in self.client.iter_objects(self.account, self.container_name): if self.object_name == obj['name']: break else: self.fail('Did not find listing for %s' % self.object_name) # clear proxy cache client.post_container(self.url, self.token, self.container_name, {}) # run the expirier again after replication self.expirer.once() # object is not in the listing for obj in self.client.iter_objects(self.account, self.container_name): if self.object_name == obj['name']: self.fail('Found listing for %s' % self.object_name) # and validate object is tombstoned found_in_policy = None for policy in ENABLED_POLICIES: metadata = self.client.get_object_metadata( self.account, self.container_name, self.object_name, acceptable_statuses=(4,), headers={'X-Backend-Storage-Policy-Index': int(policy)}) if 'x-backend-timestamp' in metadata: if found_in_policy: self.fail('found object in %s and also %s' % (found_in_policy, policy)) found_in_policy = policy self.assertTrue('x-backend-timestamp' in metadata) self.assertTrue(Timestamp(metadata['x-backend-timestamp']) > create_timestamp) if __name__ == ""__main__"": unittest.main() ",368,808
openstack%2Fmonasca-persister~master~If611dee0fd6ec6e07dbe5025fef5b079edaca5d4,openstack/monasca-persister,master,If611dee0fd6ec6e07dbe5025fef5b079edaca5d4,Add unit tests for utils.py,NEW,2016-08-18 12:34:56.000000000,2017-12-18 04:29:30.000000000,,"[{'_account_id': 11809}, {'_account_id': 14273}, {'_account_id': 15027}, {'_account_id': 16168}, {'_account_id': 20033}, {'_account_id': 20873}]","[{'number': 1, 'created': '2016-08-18 12:34:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/36a6d3c64c1ce63ef1774c65257f7ebd8f0a946d', 'message': 'Add unit tests and cleanup utils.py\n\nAdd tests for utils.py that pass with the original version and turn on\ncoverage checking.\n\nThe test message object for parse_measurement_message was copied from an\nactual Kafka message.\n\nCleanup utils.py for maintainability.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 2, 'created': '2016-08-18 12:37:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/2c356573c1f479cd0453ff9c706d63054ae7a7d2', 'message': 'Add unit tests and cleanup utils.py\n\nAdd tests for utils.py that pass with the original version and turn on\ncoverage checking.\n\nThe test message object for parse_measurement_message was copied from an\nactual Kafka message.\n\nCleanup utils.py for maintainability.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 3, 'created': '2016-08-18 14:21:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/53554706c3a3985e82352062e8dbb36969e3af81', 'message': 'Add unit tests and cleanup utils.py\n\nAdd tests for utils.py that pass with the original version.\nTurn on coverage checking.\nClean up utils.py.\n\nThe fake message objects for testing the methods parse_measurement_message and\nparse_alarm_state_hist_message were copied from actual Kafka messages.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 4, 'created': '2016-08-19 08:30:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/883bf6b2fd432f5f9a103dc2e34051b0c1bfad38', 'message': 'Add unit tests and cleanup utils.py\n\nAdd tests for utils.py that pass with the original version.\nTurn on coverage checking.\nClean up utils.py.\n\nThe fake message objects for testing the methods parse_measurement_message and\nparse_alarm_state_hist_message were copied from actual Kafka messages.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 5, 'created': '2016-08-19 11:45:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/647a265f338aeb9b0e80f1d09286df524a91e346', 'message': 'Add unit tests and cleanup utils.py\n\nAdd tests for utils.py that pass with the original version.\nTurn on coverage checking.\nClean up utils.py.\n\nThe fake message objects for testing the methods parse_measurement_message and\nparse_alarm_state_hist_message were copied from actual Kafka messages.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 6, 'created': '2016-08-22 06:10:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/acb80114930eb04522bc21fd333ed22f96aee384', 'message': 'Add unit tests and cleanup utils.py\n\nAdd tests for utils.py that pass with the original version.\nTurn on coverage checking.\nClean up utils.py.\n\nThe fake message objects for testing the methods parse_measurement_message and\nparse_alarm_state_hist_message were copied from actual Kafka messages.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 8, 'created': '2016-08-23 16:33:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/620657d17ef2ec05dfa833de57972808aac25b00', 'message': 'Add unit tests and cleanup utils.py\n\nAdd tests for utils.py that pass with the original version.\nTurn on coverage checking.\nClean up utils.py.\n\nThe fake message objects for testing the methods parse_measurement_message and\nparse_alarm_state_hist_message were copied from actual Kafka messages.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 9, 'created': '2016-08-29 05:35:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/aaca7c9d243a2eb60bca491c00f006900aabf162', 'message': 'Add unit tests and cleanup utils.py\n\nAdd tests for utils.py that pass with the original version.\nTurn on coverage checking.\nClean up utils.py.\n\nThe fake message objects for testing the methods parse_measurement_message and\nparse_alarm_state_hist_message were copied from actual Kafka messages.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 10, 'created': '2016-08-30 06:09:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/19d324bdfcabf13dec72e66eb558bd688ff46a46', 'message': 'Add unit tests and cleanup utils.py\n\nAdd tests for utils.py that pass with the original version.\nTurn on coverage checking.\nClean up utils.py.\n\nThe fake message objects for testing the methods parse_measurement_message and\nparse_alarm_state_hist_message were copied from actual Kafka messages.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 11, 'created': '2016-08-31 07:44:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/34b4b870c2d2c3ed1c5edccd9caf3b0ca8ea65dd', 'message': 'Add unit tests and cleanup utils.py\n\nAdd tests for utils.py that pass with the original version.\nTurn on coverage checking.\nClean up utils.py.\n\nThe fake message objects for testing the methods parse_measurement_message and\nparse_alarm_state_hist_message were copied from actual Kafka messages.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 12, 'created': '2016-08-31 07:50:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/421f7acbb9a21fb01bb87278cff608464761deed', 'message': 'Add unit tests for utils.py\n\nAdd tests for utils.py.\nTurn on coverage checking.\n\nThe fake message objects for testing the methods parse_measurement_message and\nparse_alarm_state_hist_message were copied from actual Kafka messages.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 13, 'created': '2016-08-31 09:05:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/1603ce9688dd81ce49e1781ce056bc7523e92554', 'message': 'Add unit tests for utils.py\n\nAdd tests for utils.py.\nTurn on coverage checking.\n\nThe fake message objects for testing the methods parse_measurement_message and\nparse_alarm_state_hist_message were copied from actual Kafka messages.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 14, 'created': '2016-08-31 11:00:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/cc7356d260883d49f4c8420ca0ca02e54c4648bb', 'message': 'Add unit tests for utils.py\n\nAdd tests for utils.py.\nTurn on coverage checking.\n\nThe fake message objects for testing the methods parse_measurement_message and\nparse_alarm_state_hist_message were copied from actual Kafka messages.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}, {'number': 15, 'created': '2016-09-05 08:01:01.000000000', 'files': ['test-requirements.txt', 'monasca_persister/repositories/utils.py', 'monasca_persister/tests/test_utils.py', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/331d1bdef92c3c8b50795ff654b3447dc7d3e13d', 'message': 'Add unit tests for utils.py\n\nAdd tests for utils.py.\nTurn on coverage checking.\n\nThe fake message objects for testing the methods parse_measurement_message and\nparse_alarm_state_hist_message were copied from actual Kafka messages.\n\nChange-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4\n'}]",23,357169,331d1bdef92c3c8b50795ff654b3447dc7d3e13d,56,6,14,20873,,,0,"Add unit tests for utils.py

Add tests for utils.py.
Turn on coverage checking.

The fake message objects for testing the methods parse_measurement_message and
parse_alarm_state_hist_message were copied from actual Kafka messages.

Change-Id: If611dee0fd6ec6e07dbe5025fef5b079edaca5d4
",git fetch https://review.opendev.org/openstack/monasca-persister refs/changes/69/357169/13 && git format-patch -1 --stdout FETCH_HEAD,"['test-requirements.txt', 'monasca_persister/repositories/utils.py', 'monasca_persister/tests/test_utils.py', 'tox.ini']",4,36a6d3c64c1ce63ef1774c65257f7ebd8f0a946d,cleanup, nosetests --with-coverage --cover-package=monasca_persister/. --cover-erase, nosetests,115,23
openstack%2Fdiskimage-builder~master~I9486b1356eac74272e79655c9024d86ee486628c,openstack/diskimage-builder,master,I9486b1356eac74272e79655c9024d86ee486628c,Sanitize distro elements for minimal vs cloud,NEW,2015-08-12 06:20:03.000000000,2017-12-18 04:29:04.000000000,,"[{'_account_id': 360}, {'_account_id': 4190}, {'_account_id': 4328}, {'_account_id': 5263}, {'_account_id': 6476}, {'_account_id': 6488}, {'_account_id': 6928}, {'_account_id': 7118}, {'_account_id': 9369}, {'_account_id': 10035}, {'_account_id': 13505}]","[{'number': 1, 'created': '2015-08-12 06:20:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/90c9e1a4f1432c7c50cde9766583de15edfb6824', 'message': 'Sanitize distro elements for minimal vs cloud\n\nWe now have two elements for most of our distros which do different\nthings. The naming for what these different elements do is very unclear\nto a user who is not intimately familiar with the elements. Lets rename\nour distro elements into *-cloudimage or *-packageinstall and document\nthe differences.\n\nChange-Id: I9486b1356eac74272e79655c9024d86ee486628c\n'}, {'number': 2, 'created': '2015-08-12 06:21:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/3ae42b46ee9ac2655ad683e49463a1263f4eec09', 'message': 'Sanitize distro elements for minimal vs cloud\n\nWe now have two elements for most of our distros which do different\nthings. The naming for what these different elements do is very unclear\nto a user who is not intimately familiar with the elements. Lets rename\nour distro elements into *-cloudimage or *-packageinstall and document\nthe differences. This also allows us to make the -packageinstall\nelements provide the packages which -minimal leaves out and which users\ngenerally expect to have (like ssh).\n\nChange-Id: I9486b1356eac74272e79655c9024d86ee486628c\n'}, {'number': 3, 'created': '2015-08-12 15:38:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/dc9061c296f2a327b24e265348e0908da688864f', 'message': 'Sanitize distro elements for minimal vs cloud\n\nWe now have two elements for most of our distros which do different\nthings. The naming for what these different elements do is very unclear\nto a user who is not intimately familiar with the elements. Lets rename\nour distro elements into *-cloudimage or *-packageinstall and document\nthe differences. This also allows us to make the -packageinstall\nelements provide the packages which -minimal leaves out and which users\ngenerally expect to have (like ssh).\n\nChange-Id: I9486b1356eac74272e79655c9024d86ee486628c\n'}, {'number': 4, 'created': '2015-08-12 15:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/a4092a7017b1f938371a3fa3fbfe8519d1682210', 'message': 'Sanitize distro elements for minimal vs cloud\n\nWe now have two elements for most of our distros which do different\nthings. The naming for what these different elements do is very unclear\nto a user who is not intimately familiar with the elements. Lets rename\nour distro elements into *-cloudimage or *-packageinstall and document\nthe differences. This also allows us to make the -packageinstall\nelements provide the packages which -minimal leaves out and which users\ngenerally expect to have (like ssh).\n\nChange-Id: I9486b1356eac74272e79655c9024d86ee486628c\n'}, {'number': 5, 'created': '2015-10-20 02:18:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/46272f4fb7e3d5a2e1e621a0cbba31ac34a75bb1', 'message': 'Sanitize distro elements for minimal vs cloud\n\nWe now have two elements for most of our distros which do different\nthings. The naming for what these different elements do is very unclear\nto a user who is not intimately familiar with the elements. Lets rename\nour distro elements into *-cloudimage or *-packageinstall and document\nthe differences. This also allows us to make the -packageinstall\nelements provide the packages which -minimal leaves out and which users\ngenerally expect to have (like ssh).\n\nCo-Authored-By: Augustina Ragwitz <aragwitz+lp@pobox.com>\nChange-Id: I9486b1356eac74272e79655c9024d86ee486628c\n'}, {'number': 6, 'created': '2015-12-26 22:14:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/24bcec546521e377bdc75886d79964e6190a59b9', 'message': 'Sanitize distro elements for minimal vs cloud\n\nWe now have two elements for most of our distros which do different\nthings. The naming for what these different elements do is very unclear\nto a user who is not intimately familiar with the elements. Lets rename\nour distro elements into *-cloudimage or *-packageinstall and document\nthe differences. This also allows us to make the -packageinstall\nelements provide the packages which -minimal leaves out and which users\ngenerally expect to have (like ssh).\n\nCo-Authored-By: Augustina Ragwitz <aragwitz+lp@pobox.com>\nChange-Id: I9486b1356eac74272e79655c9024d86ee486628c\n'}, {'number': 7, 'created': '2016-02-15 12:20:28.000000000', 'files': ['elements/centos7-cloudimage/element-deps', 'doc/source/developer/developing_elements.rst', 'elements/fedora/README.rst', 'elements/ubuntu/README.rst', 'elements/centos-packageinstall/README.rst', 'elements/fedora-packageinstall/element-deps', 'elements/fedora-cloudimage/element-deps', 'doc/source/user_guide/supported_distros.rst', 'elements/ubuntu-minimal/README.rst', 'elements/ubuntu-cloudimage/README.rst', 'elements/ubuntu-packageinstall/element-deps', 'elements/fedora-cloudimage/README.rst', 'elements/debian-packageinstall/element-deps', 'elements/centos-cloudimage/README.rst', 'elements/fedora-cloudimage/test-elements/build-succeeds/element-deps', 'elements/ubuntu-packageinstall/README.rst', 'elements/centos-packageinstall/element-deps', 'elements/debian-packageinstall/README.rst', 'elements/fedora-packageinstall/README.rst', 'elements/fedora-minimal/README.rst', 'elements/centos-minimal/README.rst', 'elements/centos/README.rst', 'elements/centos7/README.rst', 'elements/fedora-cloudimage/test-elements/build-succeeds/README.rst', 'elements/centos-cloudimage/element-deps', 'elements/ubuntu-cloudimage/element-deps', 'elements/ubuntu-packageinstall/package-installs.yaml', 'elements/centos7-cloudimage/README.rst', 'elements/apt-sources/test-elements/test-sources/element-deps', 'elements/debian-packageinstall/package-installs.yaml', 'elements/debian-minimal/README.rst'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/3f0536405ff67af67c2b9c6e92362662717b6914', 'message': 'Sanitize distro elements for minimal vs cloud\n\nWe now have two elements for most of our distros which do different\nthings. The naming for what these different elements do is very unclear\nto a user who is not intimately familiar with the elements. Lets rename\nour distro elements into *-cloudimage or *-packageinstall and document\nthe differences. This also allows us to make the -packageinstall\nelements provide the packages which -minimal leaves out and which users\ngenerally expect to have (like ssh).\n\nCo-Authored-By: Augustina Ragwitz <aragwitz+lp@pobox.com>\nChange-Id: I9486b1356eac74272e79655c9024d86ee486628c\n'}]",4,211859,3f0536405ff67af67c2b9c6e92362662717b6914,52,11,7,10035,,,0,"Sanitize distro elements for minimal vs cloud

We now have two elements for most of our distros which do different
things. The naming for what these different elements do is very unclear
to a user who is not intimately familiar with the elements. Lets rename
our distro elements into *-cloudimage or *-packageinstall and document
the differences. This also allows us to make the -packageinstall
elements provide the packages which -minimal leaves out and which users
generally expect to have (like ssh).

Co-Authored-By: Augustina Ragwitz <aragwitz+lp@pobox.com>
Change-Id: I9486b1356eac74272e79655c9024d86ee486628c
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/59/211859/1 && git format-patch -1 --stdout FETCH_HEAD,"['elements/centos7-cloudimage/element-deps', 'elements/fedora-packageinstall/README.rst', 'elements/centos-packageinstall/README.rst', 'elements/fedora-packageinstall/element-deps', 'elements/fedora-cloudimage/element-deps', 'doc/source/user_guide/supported_distros.rst', 'elements/ubuntu-cloudimage/README.rst', 'elements/ubuntu-packageinstall/element-deps', 'elements/fedora-cloudimage/README.rst', 'elements/centos-cloudimage/element-deps', 'elements/centos-cloudimage/README.rst', 'elements/ubuntu-cloudimage/element-deps', 'elements/ubuntu-packageinstall/README.rst', 'elements/ubuntu-packageinstall/package-installs.yaml', 'elements/centos7-cloudimage/README.rst', 'elements/centos-packageinstall/element-deps']",16,90c9e1a4f1432c7c50cde9766583de15edfb6824,sanitize-distros,centos-minimal ,,91,5
openstack%2Fswift~master~I0d60a1d452bbbbb8b1a4263916a668c6cb6b355b,openstack/swift,master,I0d60a1d452bbbbb8b1a4263916a668c6cb6b355b,The object server returns the policy index,NEW,2016-11-06 23:55:33.000000000,2017-12-18 04:29:02.000000000,,"[{'_account_id': 1179}, {'_account_id': 12279}, {'_account_id': 13052}, {'_account_id': 15343}]","[{'number': 1, 'created': '2016-11-06 23:55:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0e7217f4b0924ee0be431d5876348580b14c009b', 'message': 'The object server returns the policy index\n\nThe X-Backend-Storage-Policy-Index header will be included\nin the object server responses.\nIt will be included in any exception responses as well.\nThe response relies on the result of get_name_and_placement.\nThe header is included in the\nGET, HEAD, POST, PUT and REPLICATE verbs.\nThe policy index of the source of a copy will not be used\nfor the following PUT request.\n\nChange-Id: I0d60a1d452bbbbb8b1a4263916a668c6cb6b355b\nCloses-Bug: 1634382\n'}, {'number': 2, 'created': '2016-11-08 18:37:57.000000000', 'files': ['swift/obj/server.py', 'swift/common/middleware/copy.py', 'test/unit/obj/test_server.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/7ac9166713a1b32164182180c42740c52b83058f', 'message': 'The object server returns the policy index\n\nThe X-Backend-Storage-Policy-Index header will be included\nin the object server responses.\nIt will be included in any exception responses as well.\nThe response relies on the result of get_name_and_placement.\nThe header is included in the\nGET, HEAD, POST, PUT and REPLICATE verbs.\nThe policy index of the source of a copy will not be used\nfor the following PUT request.\n\nChange-Id: I0d60a1d452bbbbb8b1a4263916a668c6cb6b355b\nCloses-Bug: 1634382\n'}]",0,394212,7ac9166713a1b32164182180c42740c52b83058f,8,4,2,23739,,,0,"The object server returns the policy index

The X-Backend-Storage-Policy-Index header will be included
in the object server responses.
It will be included in any exception responses as well.
The response relies on the result of get_name_and_placement.
The header is included in the
GET, HEAD, POST, PUT and REPLICATE verbs.
The policy index of the source of a copy will not be used
for the following PUT request.

Change-Id: I0d60a1d452bbbbb8b1a4263916a668c6cb6b355b
Closes-Bug: 1634382
",git fetch https://review.opendev.org/openstack/swift refs/changes/12/394212/2 && git format-patch -1 --stdout FETCH_HEAD,"['swift/obj/server.py', 'swift/common/middleware/copy.py', 'test/unit/obj/test_server.py']",3,0e7217f4b0924ee0be431d5876348580b14c009b,bug/1634382," def _set_policy_index_header_if_necessary(self, req, policy): if policy is not None: req.headers.update({ 'X-Backend-Storage-Policy-Index': int(policy)}) if int(policy) == 1: req.headers.update({ 'X-Object-Sysmeta-Ec-Frag-Index': '0'}) 'X-Backend-Storage-Policy-Index': POLICIES[0], 'X-Backend-Storage-Policy-Index': POLICIES[0], 'X-Backend-Storage-Policy-Index': POLICIES[0], 'X-Backend-Storage-Policy-Index': POLICIES[0], 'X-Backend-Storage-Policy-Index': POLICIES[0], 'X-Backend-Storage-Policy-Index': POLICIES[0], 'X-Backend-Storage-Policy-Index': POLICIES[0], 'X-Backend-Storage-Policy-Index': POLICIES[0], 'X-Backend-Storage-Policy-Index': POLICIES[0], 'X-Backend-Storage-Policy-Index': POLICIES[0], 'X-Backend-Storage-Policy-Index': POLICIES[0], def test_POST_storage_policy_index(self): def do_test(policy): expected_policy = POLICIES[0] if policy is None else policy start = time() req = Request.blank('/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'PUT'}, headers={'X-Timestamp': normalize_timestamp(start), 'Content-Length': 0, 'Content-Type': 'plain/text'}) self._set_policy_index_header_if_necessary(req, policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 201) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], expected_policy) req = Request.blank('/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'POST'}) self._set_policy_index_header_if_necessary(req, policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 400) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], expected_policy) timestamp = utils.Timestamp(start + 0.00001) req = Request.blank('/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'POST'}, headers={'X-Timestamp': normalize_timestamp(timestamp)}) self._set_policy_index_header_if_necessary(req, policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 202) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], expected_policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 409) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], expected_policy) timestamp = utils.Timestamp(start + 0.00002) req = Request.blank('/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'POST'}, headers={'X-Timestamp': normalize_timestamp(timestamp)}) self._set_policy_index_header_if_necessary(req, policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 202) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], expected_policy) do_test(None) do_test(POLICIES[0]) do_test(POLICIES[1]) def test_PUT_test_storage_policy_index(self): req = Request.blank( '/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'PUT'}, headers={'X-Timestamp': normalize_timestamp(time()), 'Content-Type': 'text/plain'}) req.body = 'test' resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 201) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], POLICIES[0]) req = Request.blank( '/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'PUT'}, headers={'X-Timestamp': 'invalid', 'Content-Type': 'text/plain'}) req.body = 'test' resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 400) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], POLICIES[0]) req = Request.blank( '/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'PUT'}, headers={'X-Timestamp': normalize_timestamp(time()), 'Content-Type': 'text/plain', 'ETag': 'invalid'}) req.body = 'test' resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 422) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], POLICIES[0]) ec_policy = POLICIES[1] req = Request.blank( '/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'PUT'}, headers={'X-Timestamp': normalize_timestamp(time()), 'X-Backend-Storage-Policy-Index': int(ec_policy), 'X-Object-Sysmeta-Ec-Frag-Index': '0', 'Content-Type': 'text/plain'}) req.body = 'test' resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 201) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], ec_policy) ec_policy = POLICIES[1] req = Request.blank( '/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'PUT'}, headers={'X-Timestamp': 'invalid', 'X-Backend-Storage-Policy-Index': int(ec_policy), 'X-Object-Sysmeta-Ec-Frag-Index': '0', 'Content-Type': 'text/plain'}) req.body = 'test' resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 400) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], ec_policy) req = Request.blank( '/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'PUT'}, headers={'X-Timestamp': normalize_timestamp(time()), 'X-Backend-Storage-Policy-Index': int(ec_policy), 'X-Object-Sysmeta-Ec-Frag-Index': '0', 'Content-Type': 'text/plain', 'ETag': 'invalid'}) req.body = 'test' resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 422) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], ec_policy) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], POLICIES[0]) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], POLICIES[0]) def test_GET_HEAD_storage_policy_index_response(self): def do_test(policy, other_policy): expected_policy = POLICIES[0] if policy is None else policy start = time() req = Request.blank('/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'PUT'}, headers={ 'X-Timestamp': normalize_timestamp(start), 'Content-Type': 'application/x-test'}) self._set_policy_index_header_if_necessary(req, policy) req.body = 'VERIFY' resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 201) # GET HEAD the object req = Request.blank('/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'GET'}) self._set_policy_index_header_if_necessary(req, policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 200) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], expected_policy) req = Request.blank('/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'HEAD'}) self._set_policy_index_header_if_necessary(req, policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 200) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], expected_policy) # object oo does not exist req = Request.blank('/sda1/p/a/c/oo', environ={'REQUEST_METHOD': 'GET'}) self._set_policy_index_header_if_necessary(req, policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 404) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], expected_policy) req = Request.blank('/sda1/p/a/c/oo', environ={'REQUEST_METHOD': 'HEAD'}) self._set_policy_index_header_if_necessary(req, policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 404) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], expected_policy) # Object o was created with another policy req = Request.blank('/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'GET'}) self._set_policy_index_header_if_necessary(req, other_policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 404) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], other_policy) req = Request.blank('/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'HEAD'}) self._set_policy_index_header_if_necessary(req, other_policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 404) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], other_policy) timestamp = utils.Timestamp(start + 0.00001) req = Request.blank('/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'DELETE'}, headers={'X-Timestamp': normalize_timestamp(timestamp)}) self._set_policy_index_header_if_necessary(req, policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 204) do_test(POLICIES[0], POLICIES[1]) do_test(POLICIES[1], POLICIES[0]) do_test(None, POLICIES[1]) def test_DELETE_policy_index_response(self): def do_test(policy): start = time() expected_policy = POLICIES[0] if policy is None else policy req = Request.blank( '/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'PUT'}, headers={'X-Timestamp': normalize_timestamp( utils.Timestamp(start + 0.00001)), 'Content-Length': 0, 'Content-Type': 'text/plain', }) self._set_policy_index_header_if_necessary(req, policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 201) req = Request.blank( '/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'DELETE'}, headers={'X-Timestamp': normalize_timestamp( utils.Timestamp(start + 0.00002))}) self._set_policy_index_header_if_necessary(req, policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 204) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], expected_policy) req = Request.blank( '/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'DELETE'}, headers={'X-Timestamp': normalize_timestamp( utils.Timestamp(start + 0.00002))}) self._set_policy_index_header_if_necessary(req, policy) resp = req.get_response(self.object_controller) self.assertEqual(resp.status_int, 404) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], expected_policy) do_test(POLICIES[0]) do_test(POLICIES[1]) do_test(None) self.assertEqual(resp.headers['X-Backend-Storage-Policy-Index'], POLICIES[0]) self.assertEqual( resp.headers['X-Backend-Storage-Policy-Index'], POLICIES[0]) '404 - ""-"" ""-"" ""-"" 2.0000 ""-"" 1234 0'])"," '404 - ""-"" ""-"" ""-"" 2.0000 ""-"" 1234 -'])",361,42
openstack%2Fdiskimage-builder~feature%2Fv2~I6e8097e188b7a62107e4b5222f25b39727322671,openstack/diskimage-builder,feature/v2,I6e8097e188b7a62107e4b5222f25b39727322671,Remove deprecated map-packages,NEW,2016-10-06 16:02:42.000000000,2017-12-18 04:28:55.000000000,,"[{'_account_id': 7118}, {'_account_id': 21741}]","[{'number': 1, 'created': '2016-10-06 16:02:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/33455fb5f109f5ef749d794495f4f079f66f477f', 'message': 'Remove deprecated map-packages\n\nmap-packages has been deprecated for some time. Lets remove it as part\nof v2.\n\nChange-Id: I6e8097e188b7a62107e4b5222f25b39727322671\n'}, {'number': 2, 'created': '2016-11-01 20:30:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/f4750eb25ab06e55ea114a76c6a0048c3a62b203', 'message': 'Remove deprecated map-packages\n\nmap-packages has been deprecated for some time. Lets remove it as part\nof v2.\n\nChange-Id: I6e8097e188b7a62107e4b5222f25b39727322671\n'}, {'number': 3, 'created': '2016-11-01 23:09:49.000000000', 'files': ['elements/redhat-common/bin/map-packages', 'elements/zypper/bin/install-packages', 'elements/zypper/bin/map-packages', 'elements/rhel/bin/map-packages', 'elements/yum/bin/install-packages'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/e9c91ff55c52462e5f166747f6b50f272935f22c', 'message': 'Remove deprecated map-packages\n\nmap-packages has been deprecated for some time. Lets remove it as part\nof v2.\n\nChange-Id: I6e8097e188b7a62107e4b5222f25b39727322671\n'}]",0,383068,e9c91ff55c52462e5f166747f6b50f272935f22c,14,2,3,10035,,,0,"Remove deprecated map-packages

map-packages has been deprecated for some time. Lets remove it as part
of v2.

Change-Id: I6e8097e188b7a62107e4b5222f25b39727322671
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/68/383068/3 && git format-patch -1 --stdout FETCH_HEAD,"['elements/redhat-common/bin/map-packages', 'elements/rhel/bin/map-packages', 'elements/opensuse/bin/map-packages']",3,33455fb5f109f5ef749d794495f4f079f66f477f,383068,,"#!/usr/bin/env python # dib-lint: disable=indent # dib-lint indent requirements causes issue with pep8 # Copyright 2013 SUSE Linux Products GmbH # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from __future__ import print_function import sys # Manually maintained for brevity; consider making this compiled from # distromatch or other rich data sources. # Debian name on the left, openSUSE on the right. package_map = { 'arping': 'iputils', 'augeas-tools': 'augeas', 'build-essential': 'make automake gcc gcc-c++ kernel-devel', 'default-jre': 'java', 'extlinux': 'syslinux', 'grub-pc': 'grub2', 'libapache2-mod-wsgi': 'apache2-mod_wsgi', 'libc6-dev': 'glibc-devel', 'libffi-dev': 'libffi-devel', 'libmysql-java': 'mysql-connector-java', 'libmysqlclient-dev': 'libmysqlclient-devel', 'libssl-dev': 'openssl-devel', 'libvirt-bin': 'libvirt', 'libxml2-dev': 'libxml2-devel', 'libxslt-dev': 'libxslt-devel', 'libz-dev': 'zlib-devel', 'linux-image-generic': 'kernel-default', 'mysql-client-5.5': 'mariadb-client', 'mysql-server-5.5': 'mariadb', 'openjdk-7-jre-headless': 'java-1_7_0-openjdk-headless', 'openssh-client': 'openssh', 'openvswitch-common': 'openvswitch', 'openvswitch-datapath-dkms': 'openvswitch-kmp', 'openvswitch-switch': 'openvswitch-switch', 'python-dev': 'python-devel', 'python-libvirt': 'libvirt-python', 'python-memcache': 'python-python-memcached', 'python-mysqldb': 'python-mysql', 'python-pyopenssl': 'python-pyOpenSSL', 'qemu-utils': 'qemu-tools', 'tftpd-hpa': 'tftp', # openstack related package remappings 'openstack-heat-common': 'openstack-heat', 'openstack-neutron-ml2': 'openstack-neutron', 'openstack-neutron-openvswitch': 'openstack-neutron-openvswitch-agent', } print(""WARNING: map-packages is deprecated. Please use the pkg-map element."", file=sys.stderr) for arg in sys.argv[1:]: print(package_map.get(arg, arg)) sys.exit(0) ",0,209
openstack%2Fstackviz~master~I1ab055efb9bc5653f98be165cd2be3ac9cc5cd27,openstack/stackviz,master,I1ab055efb9bc5653f98be165cd2be3ac9cc5cd27,Fix docs and update its theme,MERGED,2017-06-05 03:15:59.000000000,2017-12-18 04:28:26.000000000,2017-12-18 04:28:26.000000000,"[{'_account_id': 3}, {'_account_id': 5689}, {'_account_id': 8556}, {'_account_id': 9725}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-06-05 03:15:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/stackviz/commit/ea691b94afd4751051348a0f75fd03ceabbfab4d', 'message': 'Fix docs and update its theme\n\nThis commit fixes the stackviz docs to show some manual pages and some\ntiny sectioning. And this commit also updates to use the new theme.\n\nChange-Id: I1ab055efb9bc5653f98be165cd2be3ac9cc5cd27\n'}, {'number': 2, 'created': '2017-06-05 04:38:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/stackviz/commit/06e4a8d9aa49e4b38559e977d76053d848cef0ce', 'message': 'Fix docs and update its theme\n\nThis commit fixes the stackviz docs to show some manual pages and some\ntiny sectioning. And this commit also updates to use the new theme.\n\nChange-Id: I1ab055efb9bc5653f98be165cd2be3ac9cc5cd27\n'}, {'number': 3, 'created': '2017-12-01 07:31:35.000000000', 'files': ['doc/source/index.rst', 'doc/source/man/stackviz-export.rst', 'test-requirements.txt', 'doc/source/conf.py', 'doc/source/man/stackviz-front.rst'], 'web_link': 'https://opendev.org/openstack/stackviz/commit/6fb81ff38a352e84821d410fe1adbb587a4d5e52', 'message': 'Fix docs and update its theme\n\nThis commit fixes the stackviz docs to show some manual pages and some\ntiny sectioning. And this commit also updates to use the new theme.\nThere are still remaining weird indentations and some other tiny\nformatting bugs. However, we can fix them in following patches.\n\nChange-Id: I1ab055efb9bc5653f98be165cd2be3ac9cc5cd27\n'}]",6,470825,6fb81ff38a352e84821d410fe1adbb587a4d5e52,14,5,3,5689,,,0,"Fix docs and update its theme

This commit fixes the stackviz docs to show some manual pages and some
tiny sectioning. And this commit also updates to use the new theme.
There are still remaining weird indentations and some other tiny
formatting bugs. However, we can fix them in following patches.

Change-Id: I1ab055efb9bc5653f98be165cd2be3ac9cc5cd27
",git fetch https://review.opendev.org/openstack/stackviz refs/changes/25/470825/3 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/index.rst', 'doc/source/man/stackviz-export.rst', 'test-requirements.txt', 'doc/source/conf.py', 'doc/source/man/stackviz-front.rst']",5,ea691b94afd4751051348a0f75fd03ceabbfab4d,fix-docs," - :code:`list` returns `config.json` using GET. - :code:`get(id)` calls :code:`list`, then iterates through all the available datasets for the requested id number. Rejects if not found. - :code:`raw(dataset)` returns `<dataset>_raw.json` file using GET. - :code:`details(dataset)` returns `<dataset>_details.json` file using GET. - :code:`tree(dataset)` returns `<dataset>_tree.json` file using GET. - :code:`dstat(dataset)` returns `dstat_log.csv` file using GET, if available."," - :code:`list` returns `config.json` using GET. - :code:`get(id)` calls :code:`list`, then iterates through all the available datasets for the requested id number. Rejects if not found. - :code:`raw(dataset)` returns `<dataset>_raw.json` file using GET. - :code:`details(dataset)` returns `<dataset>_details.json` file using GET. - :code:`tree(dataset)` returns `<dataset>_tree.json` file using GET. - :code:`dstat(dataset)` returns `dstat_log.csv` file using GET, if available.",25,15
openstack%2Frally~master~Icf7b01ff9d3893b5e34c8a7356d8274dcb24eb02,openstack/rally,master,Icf7b01ff9d3893b5e34c8a7356d8274dcb24eb02,Add CeilometerSamples.list_and_get_matched_samples,NEW,2016-10-31 00:15:33.000000000,2017-12-18 04:28:09.000000000,,"[{'_account_id': 10475}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 18404}, {'_account_id': 23094}, {'_account_id': 23435}]","[{'number': 1, 'created': '2016-10-31 00:15:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/bf5f425c21c94a8bb6b924161d30fe8b7cf8ea02', 'message': 'Add ceilometer.get_sample\n\nFetches detailed information of a sample.\n\nChange-Id: Icf7b01ff9d3893b5e34c8a7356d8274dcb24eb02\n'}, {'number': 2, 'created': '2016-10-31 23:33:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b1c79b0122d589e0e4339ac118c5e4b9c12daba7', 'message': 'Add ceilometer.get_sample\n\nFetches detailed information about sample.\n\nChange-Id: Icf7b01ff9d3893b5e34c8a7356d8274dcb24eb02\n'}, {'number': 3, 'created': '2016-11-04 07:14:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b2f6346e8ac2334c8584fa51fb42ed8348b2d907', 'message': 'Add CeilometerSamples.list_and_get_matched_samples\n\nList and get samples that matched fields from context and args\n\nChange-Id: Icf7b01ff9d3893b5e34c8a7356d8274dcb24eb02\n'}, {'number': 4, 'created': '2016-11-08 03:33:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/94bf00f76e158dd9ae042222ad8367ea00dbedc4', 'message': 'Add CeilometerSamples.list_and_get_matched_samples\n\nList and get samples that matched fields from context and args\n\nChange-Id: Icf7b01ff9d3893b5e34c8a7356d8274dcb24eb02\n'}, {'number': 5, 'created': '2016-11-24 01:19:38.000000000', 'files': ['samples/tasks/scenarios/ceilometer/list-and-get-matched-samples.yaml', 'rally/plugins/openstack/scenarios/ceilometer/samples.py', 'rally-jobs/rally-keystone-api-v2.yaml', 'rally/plugins/openstack/scenarios/ceilometer/utils.py', 'tests/unit/plugins/openstack/scenarios/ceilometer/test_samples.py', 'tests/unit/plugins/openstack/scenarios/ceilometer/test_utils.py', 'rally-jobs/rally.yaml', 'samples/tasks/scenarios/ceilometer/list-and-get-matched-samples.json'], 'web_link': 'https://opendev.org/openstack/rally/commit/002cb4a56465cccb9bb80b6e8686948c926a5b2c', 'message': 'Add CeilometerSamples.list_and_get_matched_samples\n\nList and get samples that matched fields from context and args\n\nChange-Id: Icf7b01ff9d3893b5e34c8a7356d8274dcb24eb02\n'}]",5,391688,002cb4a56465cccb9bb80b6e8686948c926a5b2c,39,6,5,18404,,,0,"Add CeilometerSamples.list_and_get_matched_samples

List and get samples that matched fields from context and args

Change-Id: Icf7b01ff9d3893b5e34c8a7356d8274dcb24eb02
",git fetch https://review.opendev.org/openstack/rally refs/changes/88/391688/4 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/openstack/scenarios/ceilometer/utils.py', 'tests/unit/plugins/openstack/scenarios/ceilometer/test_utils.py']",2,bf5f425c21c94a8bb6b924161d30fe8b7cf8ea02,ceilometer.create_and_get_sample," def test__get_sample(self): self.assertEqual(self.clients(""ceilometer"").samples.get.return_value, self.scenario._get_sample(""sample-id"")) self.clients(""ceilometer"").samples.get.assert_called_once_with( ""sample-id"") self._test_atomic_action_timer(self.scenario.atomic_actions(), ""ceilometer.get_sample"") ",,17,0
openstack%2Frally~master~I8e1088eded81561a687b17102a464a5875843d6c,openstack/rally,master,I8e1088eded81561a687b17102a464a5875843d6c,Add several scenarios about volume backup,NEW,2016-10-27 05:06:09.000000000,2017-12-18 04:28:07.000000000,,"[{'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 23435}, {'_account_id': 23668}]","[{'number': 1, 'created': '2016-10-27 05:06:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/7a708f86a31e4bdb9cc35fd2ab8fd0d9f79fdd11', 'message': 'Add several scenarios about volume backup\n\n1.CreateAndExportVolumeBackup\n2.CreateAndGetVolumeBackup\n3.CreateAndResetStateVolumeBackup\n\nChange-Id: I8e1088eded81561a687b17102a464a5875843d6c\n'}, {'number': 2, 'created': '2017-01-16 06:48:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8f0c498b3e451ce4cc599965cd71e775c8464d51', 'message': 'Add several scenarios about volume backup\n\n1.CreateAndExportVolumeBackup\n2.CreateAndGetVolumeBackup\n3.CreateAndResetStateVolumeBackup\n\nChange-Id: I8e1088eded81561a687b17102a464a5875843d6c\n'}, {'number': 3, 'created': '2017-01-17 01:32:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0451e6b0842ffbcd1057c65aef5818a416b02700', 'message': 'Add several scenarios about volume backup\n\n1.CreateAndExportVolumeBackup\n2.CreateAndGetVolumeBackup\n3.CreateAndResetStateVolumeBackup\n\nChange-Id: I8e1088eded81561a687b17102a464a5875843d6c\n'}, {'number': 4, 'created': '2017-01-17 05:24:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/118d9c711da3da7d30bb9027ab261d57cc64ac02', 'message': 'Add several scenarios about volume backup\n\n1.CreateAndExportVolumeBackup\n2.CreateAndGetVolumeBackup\n3.CreateAndResetStateVolumeBackup\n\nChange-Id: I8e1088eded81561a687b17102a464a5875843d6c\n'}, {'number': 5, 'created': '2017-01-17 07:12:42.000000000', 'files': ['samples/tasks/scenarios/cinder/create-and-export-volume-backup.json', 'tests/unit/plugins/openstack/scenarios/cinder/test_volume_backups.py', 'samples/tasks/scenarios/cinder/create-and-get-volume-backup.yaml', 'rally/plugins/openstack/scenarios/cinder/volume_backups.py', 'samples/tasks/scenarios/cinder/create-and-reset-state-volume-backup.json', 'rally/plugins/openstack/scenarios/cinder/utils.py', 'samples/tasks/scenarios/cinder/create-and-get-volume-backup.json', 'rally-jobs/cinder.yaml', 'samples/tasks/scenarios/cinder/create-and-export-volume-backup.yaml', 'tests/unit/plugins/openstack/scenarios/cinder/test_utils.py', 'samples/tasks/scenarios/cinder/create-and-reset-state-volume-backup.yaml'], 'web_link': 'https://opendev.org/openstack/rally/commit/b91767e9095e3d2fde484296b2fc91c77a84414e', 'message': 'Add several scenarios about volume backup\n\n1.CreateAndExportVolumeBackup\n2.CreateAndGetVolumeBackup\n3.CreateAndResetStateVolumeBackup\n\nChange-Id: I8e1088eded81561a687b17102a464a5875843d6c\n'}]",7,391024,b91767e9095e3d2fde484296b2fc91c77a84414e,39,4,5,23435,,,0,"Add several scenarios about volume backup

1.CreateAndExportVolumeBackup
2.CreateAndGetVolumeBackup
3.CreateAndResetStateVolumeBackup

Change-Id: I8e1088eded81561a687b17102a464a5875843d6c
",git fetch https://review.opendev.org/openstack/rally refs/changes/24/391024/2 && git format-patch -1 --stdout FETCH_HEAD,"['samples/tasks/scenarios/cinder/create-and-export-volume-backup.json', 'tests/unit/plugins/openstack/scenarios/cinder/test_volume_backups.py', 'samples/tasks/scenarios/cinder/create-and-get-volume-backup.yaml', 'rally/plugins/openstack/scenarios/cinder/volume_backups.py', 'samples/tasks/scenarios/cinder/create-and-reset-state-volume-backup.json', 'rally/plugins/openstack/scenarios/cinder/utils.py', 'samples/tasks/scenarios/cinder/create-and-get-volume-backup.json', 'rally-jobs/cinder.yaml', 'samples/tasks/scenarios/cinder/create-and-export-volume-backup.yaml', 'tests/unit/plugins/openstack/scenarios/cinder/test_utils.py', 'samples/tasks/scenarios/cinder/create-and-reset-state-volume-backup.yaml']",11,7a708f86a31e4bdb9cc35fd2ab8fd0d9f79fdd11,cinder.ExportVolumeBackup,"--- CinderVolumeBackups.create_and_reset_state_volume_backup: - args: size: 1 do_delete: True create_volume_kwargs: {} create_backup_kwargs: {} runner: type: ""constant"" times: 3 concurrency: 2 context: users: tenants: 2 users_per_tenant: 2 roles: - ""admin"" sla: failure_rate: max: 0 ",,501,0
openstack%2Fdiskimage-builder~feature%2Fv2~If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45,openstack/diskimage-builder,feature/v2,If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45,Add support for building images capable of UEFI,NEW,2016-10-31 14:15:39.000000000,2017-12-18 04:27:42.000000000,,"[{'_account_id': 9237}, {'_account_id': 10035}, {'_account_id': 18781}, {'_account_id': 23310}]","[{'number': 1, 'created': '2016-10-31 14:15:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/1e048a43a7472e72031006845b5df409d2c002b5', 'message': 'Add support for building images capable of UEFI\n\nThis change adds support for to build images using diskimage-builder\nwhich are capable to boot in UEFI based systems. Currently RHEL 7, CentOS 7,\nUbuntu, Debian and Fedora images have been tested and verified for\nx86_64/amd64 architectures.\n\nTo build an UEFI capable image,\n\n1. Two Partions were created with GPT partition table. First partition\n   contains the EFI System Partition(ESP), while the second partition is\n   the root file system. parted is used for creating the partitions.\n\n2. Bootloader element pkg-map has been added with grub-efi which\n   installs dependent packages for default and redhat distributions.\n\n3. As part of installing grub boot loader, the ESP partition is mounted\n   to /boot/efi directory of the root file system so that grub will\n   install required grubx64.efi and other dependent packages in ESP\n   partition.\n\n4. Grub configuration files are modified to work with respective\n   distributions.\n\nChange-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\nSigned-off-by: Purandhar Sairam Mannidi <mannidi@hpe.com>\n\nConflicts:\n\tdoc/source/user_guide/supported_distros.rst\n\nChange-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\n'}, {'number': 2, 'created': '2016-11-01 23:16:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/fe7ca5703f604a32e26a4eb6b158f8217cb643b0', 'message': 'Add support for building images capable of UEFI\n\nThis change adds support for to build images using diskimage-builder\nwhich are capable to boot in UEFI based systems. Currently RHEL 7, CentOS 7,\nUbuntu, Debian and Fedora images have been tested and verified for\nx86_64/amd64 architectures.\n\nTo build an UEFI capable image,\n\n1. Two Partions were created with GPT partition table. First partition\n   contains the EFI System Partition(ESP), while the second partition is\n   the root file system. parted is used for creating the partitions.\n\n2. Bootloader element pkg-map has been added with grub-efi which\n   installs dependent packages for default and redhat distributions.\n\n3. As part of installing grub boot loader, the ESP partition is mounted\n   to /boot/efi directory of the root file system so that grub will\n   install required grubx64.efi and other dependent packages in ESP\n   partition.\n\n4. Grub configuration files are modified to work with respective\n   distributions.\n\nChange-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\nSigned-off-by: Purandhar Sairam Mannidi <mannidi@hpe.com>\n\nConflicts:\n\tdoc/source/user_guide/supported_distros.rst\n\nChange-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\n'}, {'number': 3, 'created': '2016-11-02 15:29:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/162a420f7cb8bfd7cc529d982e2cb6b462366b0d', 'message': 'Add support for building images capable of UEFI\n\nThis change adds support for to build images using diskimage-builder\nwhich are capable to boot in UEFI based systems. Currently RHEL 7, CentOS 7,\nUbuntu, Debian and Fedora images have been tested and verified for\nx86_64/amd64 architectures.\n\nTo build an UEFI capable image,\n\n1. Two Partions were created with GPT partition table. First partition\n   contains the EFI System Partition(ESP), while the second partition is\n   the root file system. parted is used for creating the partitions.\n\n2. Bootloader element pkg-map has been added with grub-efi which\n   installs dependent packages for default and redhat distributions.\n\n3. As part of installing grub boot loader, the ESP partition is mounted\n   to /boot/efi directory of the root file system so that grub will\n   install required grubx64.efi and other dependent packages in ESP\n   partition.\n\n4. Grub configuration files are modified to work with respective\n   distributions.\n\nChange-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\nSigned-off-by: Purandhar Sairam Mannidi <mannidi@hpe.com>\n\nConflicts:\n\tdoc/source/user_guide/supported_distros.rst\n\nChange-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\n'}, {'number': 4, 'created': '2016-11-18 09:28:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/054b4b9b26ca954cf4ece635208c0dc9d95e4a5b', 'message': 'Add support for building images capable of UEFI\n\nThis change adds support for to build images using diskimage-builder\nwhich are capable to boot in UEFI based systems. Currently RHEL 7, CentOS 7,\nUbuntu, Debian and Fedora images have been tested and verified for\nx86_64/amd64 architectures.\n\nTo build an UEFI capable image,\n\n1. Two Partions were created with GPT partition table. First partition\n   contains the EFI System Partition(ESP), while the second partition is\n   the root file system. parted is used for creating the partitions.\n\n2. Bootloader element pkg-map has been added with grub-efi which\n   installs dependent packages for default and redhat distributions.\n\n3. As part of installing grub boot loader, the ESP partition is mounted\n   to /boot/efi directory of the root file system so that grub will\n   install required grubx64.efi and other dependent packages in ESP\n   partition.\n\n4. Grub configuration files are modified to work with respective\n   distributions.\n\nChange-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\nSigned-off-by: Purandhar Sairam Mannidi <mannidi@hpe.com>\n\nConflicts:\n\tdoc/source/user_guide/supported_distros.rst\n\nChange-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\n'}, {'number': 5, 'created': '2016-11-18 09:42:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/35edf7da04c36530809a0a4a91cc295391add3d6', 'message': 'Add support for building images capable of UEFI\n\nThis change adds support for to build images using diskimage-builder\nwhich are capable to boot in UEFI based systems. Currently RHEL 7, CentOS 7,\nUbuntu, Debian and Fedora images have been tested and verified for\nx86_64/amd64 architectures.\n\nTo build an UEFI capable image,\n\n1. Two Partions were created with GPT partition table. First partition\n   contains the EFI System Partition(ESP), while the second partition is\n   the root file system. parted is used for creating the partitions.\n\n2. Bootloader element pkg-map has been added with grub-efi which\n   installs dependent packages for default and redhat distributions.\n\n3. As part of installing grub boot loader, the ESP partition is mounted\n   to /boot/efi directory of the root file system so that grub will\n   install required grubx64.efi and other dependent packages in ESP\n   partition.\n\n4. Grub configuration files are modified to work with respective\n   distributions.\n\nChange-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\nSigned-off-by: Purandhar Sairam Mannidi <mannidi@hpe.com>\n\nConflicts:\n\tdoc/source/user_guide/supported_distros.rst\n\nChange-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\n'}, {'number': 6, 'created': '2016-11-24 07:14:10.000000000', 'files': ['diskimage_builder/elements/bootloader/finalise.d/50-bootloader', 'diskimage_builder/elements/bootloader/pkg-map', 'diskimage_builder/elements/vm/environment.d/50-vm', 'doc/source/user_guide/building_an_image.rst', 'diskimage_builder/elements/vm/README.rst', 'diskimage_builder/lib/disk-image-create', 'doc/source/user_guide/supported_distros.rst'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/baafc2ac3c80a2009e3a4cdb45c5fc075488420f', 'message': 'Add support for building images capable of UEFI\n\nThis change adds support for to build images using diskimage-builder\nwhich are capable to boot in UEFI based systems. Currently RHEL 7, CentOS 7,\nUbuntu, Debian and Fedora images have been tested and verified for\nx86_64/amd64 architectures.\n\nTo build an UEFI capable image,\n\n1. Two Partions were created with GPT partition table. First partition\n   contains the EFI System Partition(ESP), while the second partition is\n   the root file system. parted is used for creating the partitions.\n\n2. Bootloader element pkg-map has been added with grub-efi which\n   installs dependent packages for default and redhat distributions.\n\n3. As part of installing grub boot loader, the ESP partition is mounted\n   to /boot/efi directory of the root file system so that grub will\n   install required grubx64.efi and other dependent packages in ESP\n   partition.\n\n4. Grub configuration files are modified to work with respective\n   distributions.\n\nChange-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\nSigned-off-by: Purandhar Sairam Mannidi <mannidi@hpe.com>\n\nConflicts:\n\tdoc/source/user_guide/supported_distros.rst\n\nChange-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\n'}]",0,391850,baafc2ac3c80a2009e3a4cdb45c5fc075488420f,37,4,6,10035,,,0,"Add support for building images capable of UEFI

This change adds support for to build images using diskimage-builder
which are capable to boot in UEFI based systems. Currently RHEL 7, CentOS 7,
Ubuntu, Debian and Fedora images have been tested and verified for
x86_64/amd64 architectures.

To build an UEFI capable image,

1. Two Partions were created with GPT partition table. First partition
   contains the EFI System Partition(ESP), while the second partition is
   the root file system. parted is used for creating the partitions.

2. Bootloader element pkg-map has been added with grub-efi which
   installs dependent packages for default and redhat distributions.

3. As part of installing grub boot loader, the ESP partition is mounted
   to /boot/efi directory of the root file system so that grub will
   install required grubx64.efi and other dependent packages in ESP
   partition.

4. Grub configuration files are modified to work with respective
   distributions.

Change-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45
Signed-off-by: Purandhar Sairam Mannidi <mannidi@hpe.com>

Conflicts:
	doc/source/user_guide/supported_distros.rst

Change-Id: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/50/391850/1 && git format-patch -1 --stdout FETCH_HEAD,"['elements/bootloader/finalise.d/50-bootloader', 'elements/vm/README.rst', 'elements/vm/environment.d/50-vm', 'elements/vm/block-device.d/10-partition', 'elements/bootloader/pkg-map', 'doc/source/user_guide/building_an_image.rst', 'doc/source/user_guide/supported_distros.rst']",7,1e048a43a7472e72031006845b5df409d2c002b5,dib_uefi_work," - Centos 6, 7 - Debian 8 (""jessie"") - Fedora 20, 21, 22 - RHEL 6, 7 - Ubuntu 14.04 (""trusty"") - Gentoo Distributions which are supported as a target for a BIOS boot image: Distributions which are supported as a target for an UEFI capable image: - Fedora 22 - Ubuntu 14.04 (""trusty"") - RHEL 7 - Centos 7 - Debian 8 (""jessie"") Note: Currently the images for x86_64/amd64 architectures are tested.","- Centos 6, 7 - Debian 8 (""jessie"") - Fedora 20, 21, 22 - RHEL 6, 7 - Ubuntu 14.04 (""trusty"") - Gentoo Distributions which are supported as a target for an image:",189,14
openstack%2Fironic-python-agent~master~Icc685427234ab3797a79464a1622f9f4bf87fc96,openstack/ironic-python-agent,master,Icc685427234ab3797a79464a1622f9f4bf87fc96,Convert _get_route_source to use the socket module,NEW,2017-01-12 15:48:32.000000000,2017-12-18 04:27:20.000000000,,"[{'_account_id': 1926}, {'_account_id': 10342}]","[{'number': 1, 'created': '2017-01-12 15:48:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/f18fbdc71ce79add6ac166fa69448021d8937e0b', 'message': 'Convert _get_route_source to use the socket module\n\nUse the socket library instead of the ""ip"" command.\n\nTODO:\n o This doesn\'t work with link-local IPv6 addresses, befor commiting\n   to this address we need to verify if this will be a problem or not.\n   If we are to support link local address sin6_scope_id needs to be\n   passed into the socket.\n\nChange-Id: Icc685427234ab3797a79464a1622f9f4bf87fc96\n'}, {'number': 2, 'created': '2017-01-12 15:51:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/62e7661324d45375f75add551968f9cfe6cf6026', 'message': 'Convert _get_route_source to use the socket module\n\nUse the socket library instead of the ""ip"" command.\n\nTODO:\n o This doesn\'t work with link-local IPv6 addresses, befor commiting\n   to this address we need to verify if this will be a problem or not.\n   If we are to support link local address sin6_scope_id needs to be\n   passed into the socket.\n\nChange-Id: Icc685427234ab3797a79464a1622f9f4bf87fc96\n'}, {'number': 3, 'created': '2017-01-19 10:57:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/42745f4bc4421dd214d63a41cc47d16c8bb1b018', 'message': 'Convert _get_route_source to use the socket module\n\nUse the socket library instead of the ""ip"" command.\n\nTODO:\n o This doesn\'t work with link-local IPv6 addresses, befor commiting\n   to this address we need to verify if this will be a problem or not.\n   If we are to support link local address sin6_scope_id needs to be\n   passed into the socket.\n\nChange-Id: Icc685427234ab3797a79464a1622f9f4bf87fc96\n'}, {'number': 4, 'created': '2017-01-19 15:25:12.000000000', 'files': ['ironic_python_agent/agent.py', 'ironic_python_agent/tests/unit/test_agent.py'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/d5ea1d476a5a8e47d85419f8528b1038efde9803', 'message': 'Convert _get_route_source to use the socket module\n\nUse the socket library instead of the ""ip"" command.\n\nTODO:\n o This doesn\'t work with link-local IPv6 addresses, befor commiting\n   to this address we need to verify if this will be a problem or not.\n   If we are to support link local address sin6_scope_id needs to be\n   passed into the socket.\n\nChange-Id: Icc685427234ab3797a79464a1622f9f4bf87fc96\n'}]",0,419534,d5ea1d476a5a8e47d85419f8528b1038efde9803,13,2,4,1926,,,0,"Convert _get_route_source to use the socket module

Use the socket library instead of the ""ip"" command.

TODO:
 o This doesn't work with link-local IPv6 addresses, befor commiting
   to this address we need to verify if this will be a problem or not.
   If we are to support link local address sin6_scope_id needs to be
   passed into the socket.

Change-Id: Icc685427234ab3797a79464a1622f9f4bf87fc96
",git fetch https://review.opendev.org/openstack/ironic-python-agent refs/changes/34/419534/2 && git format-patch -1 --stdout FETCH_HEAD,"['ironic_python_agent/agent.py', 'ironic_python_agent/tests/unit/test_agent.py']",2,f18fbdc71ce79add6ac166fa69448021d8937e0b,," @mock.patch.object(socket.socket, 'connect') @mock.patch.object(socket.socket, 'getsockname') def test_get_route_source_ipv4(self, mock_getsockname, mock_connect): mock_getsockname.return_value = (""1.2.3.4"", 0) source = self.agent._get_route_source('1.2.3.1') @mock.patch.object(socket.socket, 'connect') @mock.patch.object(socket.socket, 'getsockname') def test_get_route_source_ipv6(self, mock_getsockname, mock_connect): mock_getsockname.return_value = (""1:2::3:4"", 0, 0, 0) source = self.agent._get_route_source('1:2::3:1') @mock.patch.object(socket.socket, 'connect') def test_get_route_source_unreachable(self, mock_connect, mock_log): mock_connect.side_effect = socket.error('Network is unreachable') source = self.agent._get_route_source('1.2.3.1') @mock.patch.object(socket.socket, 'connect') @mock.patch.object(socket.socket, 'getsockname') def test_route_with_ip(self, mock_getsockname, mock_connect, mock_exec, mock_gethostbyname): mock_getsockname.return_value = (""192.168.122.56"", 0) @mock.patch.object(socket.socket, 'connect') @mock.patch.object(socket.socket, 'getsockname') def test_route_with_ipv6(self, mock_getsockname, mock_connect, mock_exec, mock_gethostbyname): mock_getsockname.return_value = (""fc00:101::4"", 0, 0, 0) @mock.patch.object(socket.socket, 'connect') @mock.patch.object(socket.socket, 'getsockname') def test_route_with_host(self, mock_getsockname, mock_connect, mock_exec, mock_gethostbyname): mock_getsockname.return_value = (""192.168.122.56"", 0) @mock.patch.object(socket.socket, 'connect') @mock.patch.object(socket.socket, 'getsockname') def test_route_retry(self, mock_getsockname, mock_connect, mock_sleep, mock_exec, mock_gethostbyname): mock_getsockname.side_effect = [ socket.error(""XXX""), socket.error(""XXX""), (""192.168.122.56"", 0) self.assertEqual(3, mock_getsockname.call_count) @mock.patch.object(socket.socket, 'connect') @mock.patch.object(socket.socket, 'getsockname') def test_route_failed(self, mock_getsockname, mock_connect, mock_sleep, mock_exec, mock_gethostbyname): mock_getsockname.side_effect = socket.error(""XXX"") mock_getsockname.assert_called() self.assertEqual(5, mock_getsockname.call_count)","from oslo_concurrency import processutils @mock.patch.object(utils, 'execute', autospec=True) def test_get_route_source_ipv4(self, mock_execute): mock_execute.return_value = ('XXX src 1.2.3.4 XXX\n cache', None) source = self.agent._get_route_source('XXX') @mock.patch.object(utils, 'execute', autospec=True) def test_get_route_source_ipv6(self, mock_execute): mock_execute.return_value = ('XXX src 1:2::3:4 metric XXX\n cache', None) source = self.agent._get_route_source('XXX') @mock.patch.object(utils, 'execute', autospec=True) def test_get_route_source_indexerror(self, mock_execute, mock_log): mock_execute.return_value = ('XXX src \n cache', None) source = self.agent._get_route_source('XXX') def test_route_with_ip(self, mock_exec, mock_gethostbyname): mock_exec.return_value = ( """"""1.2.1.2 via 192.168.122.1 dev eth0 src 192.168.122.56 cache """""", """" ) mock_exec.assert_called_once_with('ip', 'route', 'get', '1.2.1.2') def test_route_with_ipv6(self, mock_exec, mock_gethostbyname): mock_exec.return_value = ( """"""fc00:101::1 dev br-ctlplane src fc00:101::4 metric 0 cache """""", """" ) mock_exec.assert_called_once_with('ip', 'route', 'get', 'fc00:1111::1') def test_route_with_host(self, mock_exec, mock_gethostbyname): mock_exec.return_value = ( """"""1.2.1.2 via 192.168.122.1 dev eth0 src 192.168.122.56 cache """""", """" ) mock_exec.assert_called_once_with('ip', 'route', 'get', '1.2.1.2') def test_route_retry(self, mock_sleep, mock_exec, mock_gethostbyname): mock_exec.side_effect = [ processutils.ProcessExecutionError('boom'), ( ""Error: some error text"", """" ), ( """"""1.2.1.2 via 192.168.122.1 dev eth0 src 192.168.122.56 cache """""", """" ) mock_exec.assert_called_with('ip', 'route', 'get', '1.2.1.2') self.assertEqual(3, mock_exec.call_count) def test_route_failed(self, mock_sleep, mock_exec, mock_gethostbyname): mock_exec.side_effect = processutils.ProcessExecutionError('boom') mock_exec.assert_called_with('ip', 'route', 'get', '1.2.1.2') self.assertEqual(5, mock_exec.call_count)",59,64
openstack%2Fironic~master~I39d6bd46ad900c064fdf8b6df208b7baeac06f38,openstack/ironic,master,I39d6bd46ad900c064fdf8b6df208b7baeac06f38,DNM: Test patch to verify bond,NEW,2016-10-19 12:16:55.000000000,2017-12-18 04:27:09.000000000,,"[{'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 14525}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 22255}]","[{'number': 1, 'created': '2016-10-19 12:16:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a4ad7f09a085380bfacb37a0127a1ea23241db4e', 'message': 'DNM: Test patch to verify bond\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 2, 'created': '2016-10-19 12:38:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/da1e518d4cb0d247ba419cdcd149e1fbf5f7c369', 'message': 'DNM: Test patch to verify bond\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 3, 'created': '2016-10-19 13:06:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e25ce92366b5fd16ca603c5fd7b771618c4b0c3b', 'message': 'DNM: Test patch to verify bond\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 4, 'created': '2016-10-19 13:12:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3b60ae04cbcec13a833d2297c1e45c9c24721e21', 'message': 'DNM: Test patch to verify bond\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 5, 'created': '2016-10-19 13:14:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9b6f8af1fb2d7b8b0d71fe37a2531eee247701be', 'message': 'DNM: Test patch to verify bond\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 6, 'created': '2016-10-19 14:34:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d73d252dcdfc24486993da33c274c7979ee174fd', 'message': 'DNM: Test patch to verify bond\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 7, 'created': '2016-10-19 15:24:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/75c24b99504f4906ee638519f985e5d1c5d409de', 'message': 'DNM: Test patch to verify bond\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 8, 'created': '2016-10-19 17:42:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/52d8bcb5941652d3b56a1dd0f8b17843b54a67ad', 'message': 'DNM: Test patch to verify bond\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 9, 'created': '2016-10-20 12:51:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d168dbfe0b17e20281909f89bf70bff1b87739bf', 'message': 'DNM: Test patch to verify bond\n\nNova portgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 10, 'created': '2016-10-20 14:10:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/06cba1ac6fb0006d4188f3b5a23ba4bdb0e3fcb5', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 11, 'created': '2016-10-20 15:05:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c70e1f3db034307467e7aa2840f2412d5d9ccd49', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 12, 'created': '2016-10-20 19:42:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/4f97e916cae6f761483a733d624304c6e27395e7', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 13, 'created': '2016-10-21 08:21:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5ba0b147b6b2bb5292e2f34cd9f37f794bca9f51', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 14, 'created': '2016-10-31 11:43:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fa7230ec57475bcbee15a8cf7adc4d549eb03609', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 15, 'created': '2016-11-03 11:37:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e8cdd509d7c15c549a129024761e1b0e1d6e7055', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 16, 'created': '2016-11-03 14:10:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/45c039c87c77320b0faed4db889e2275445582f1', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 17, 'created': '2016-11-03 14:27:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5458653eb141f393da8f39d7f1a249e2f772efca', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 18, 'created': '2016-11-07 09:19:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2c40827de82e3067f58499870d617a3702ec1492', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 19, 'created': '2016-11-08 17:39:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/4e15370d7417a2ecb818a219d62e4f58da5443fe', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 20, 'created': '2016-11-08 19:55:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1c8aeb8a7c562f3c91d25d7b0b54cd3bf71b2c31', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 21, 'created': '2016-11-09 11:38:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3c12da7ab47f9e48a10958f884faa5e5badb7c0f', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 22, 'created': '2016-11-10 16:08:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c76b1300cf51514b3a21ecc56b06155715004c01', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 23, 'created': '2016-11-11 10:58:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fc96b06c40aef0a50e6f261b331a306e73521b00', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 24, 'created': '2016-11-16 12:36:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bd4deb44d559551fd052e67dd9dbb4bd52d6ea64', 'message': 'DNM: Test patch to verify bond\n\nNova\nportgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 25, 'created': '2016-12-20 12:41:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d4471e87bfb133c17ae565d4f5cedcf9b86988f6', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 26, 'created': '2016-12-20 14:11:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/aa3bf8257b56c4b8ed895012d05136e4b7cdbe13', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\n#Depends-#On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 27, 'created': '2016-12-20 16:53:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2db8724fa3fb39955ec66a37793a0ca89cfa399a', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\n#Depends-#On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 28, 'created': '2016-12-20 16:58:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/62051775ec90e88479437e38c9ee3730f289cc50', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\n#Depends-#On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 29, 'created': '2016-12-21 10:16:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/281bddc9eee0bd9417d9815a5a7966be3bd6813e', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\n#Depends-#On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 30, 'created': '2016-12-21 12:00:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/96134e76a8464d6023957b7ba8e0e0d55c2cbbde', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 31, 'created': '2016-12-21 12:54:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0d5161c55965b0f39656f258db4f52b8cb95d914', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 32, 'created': '2016-12-21 13:32:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e3a6725a200e57f430ff8757566f26b8feb8c49d', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 33, 'created': '2017-01-16 09:36:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5da2e08c2e39285e245b973945e2e12f70935007', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 34, 'created': '2017-01-16 12:50:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1b7727614022d0cab5672537966ffb257530a9a2', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 35, 'created': '2017-01-16 14:06:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3998dddd5e2fd53028d2ec3d26bf25318e856e4e', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 36, 'created': '2017-01-16 14:32:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5398678d055100ee747088e45ee84e3b82509069', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 37, 'created': '2017-01-16 16:37:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9e709f6971f4f6a84613bd8d9b5fad5974ea7558', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}, {'number': 38, 'created': '2017-01-18 19:11:46.000000000', 'files': ['devstack/lib/ironic', 'devstack/upgrade/upgrade.sh'], 'web_link': 'https://opendev.org/openstack/ironic/commit/9dc644feb2f7518908f92f051653d304e7490cbc', 'message': 'DNM: Test patch to verify bond\n\nNova\nattach/detach patch\nDepends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777\n\nconfig drive patch\nDepends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0\n\nChange-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38\n'}]",0,388660,9dc644feb2f7518908f92f051653d304e7490cbc,145,6,38,14525,,,0,"DNM: Test patch to verify bond

Nova
attach/detach patch
Depends-On: I4d70423ca978885a982c7eb5bd1efcc024d2b777

config drive patch
Depends-On: Ic9bf6ca9e3de0068a289cbe6760cd8c4526ec7e0

Change-Id: I39d6bd46ad900c064fdf8b6df208b7baeac06f38
",git fetch https://review.opendev.org/openstack/ironic refs/changes/60/388660/30 && git format-patch -1 --stdout FETCH_HEAD,['devstack/lib/ironic'],1,a4ad7f09a085380bfacb37a0127a1ea23241db4e,bug/1618754,"IRONIC_USE_BOND=$(trueorfalse True IRONIC_USE_BOND)if [[ ""$IRONIC_USE_BOND"" == 'True' && ""$IMAGE_URLS"" != *""cirros-disk-bonding-static.qcow2""* ]]; then IMAGE_URLS+=""https://www.dropbox.com/s/owby2p2kln1mojm/cirros-disk-bonding-static.qcow2"" fi",IRONIC_USE_BOND=$(trueorfalse False IRONIC_USE_BOND),4,1
openstack%2Fdiskimage-builder~master~Iedf121c34aefc00696bc974019833fc286a7b8ac,openstack/diskimage-builder,master,Iedf121c34aefc00696bc974019833fc286a7b8ac,Move tests in to unit specific dir and fix cover,NEW,2016-12-21 00:04:58.000000000,2017-12-18 04:26:53.000000000,,"[{'_account_id': 7118}, {'_account_id': 10035}, {'_account_id': 21741}]","[{'number': 1, 'created': '2016-12-21 00:04:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/c9db5cef9f8c587d9c1874754ecd0870290157b4', 'message': 'Move tests in to unit specific dir and fix cover\n\nWe need the ability to run some of our tests with sudo permissions. This\nis generally a bad practice for unit tests, but this shouldnt prevent us\nfrom writing gaining this type of test coverage. To deal with this, lets\nmove our unit tests in to a unit test specific dir so we can have a\nspecific tox target for them.\n\nWhen conducting this move it was noticed that our cover target is\ncompletely broken. Fixing it up as part of pointing it at the new unit\ntest dir.\n\nChange-Id: Iedf121c34aefc00696bc974019833fc286a7b8ac\n'}, {'number': 2, 'created': '2016-12-21 00:24:51.000000000', 'files': ['diskimage_builder/tests/unit/__init__.py', 'diskimage_builder/tests/unit/test_loggingconfig.py', 'elements/cache-url/tests/test_cache_url.py', 'diskimage_builder/tests/unit/test_elementdeps.py', 'diskimage_builder/tests/unit/test_elements.py', 'diskimage_builder/tests/unit/test_no_dup_filenames.py', 'tox.ini', 'diskimage_builder/tests/unit/base.py'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/5b601db7af456690f0266146be2e017a5a870e85', 'message': 'Move tests in to unit specific dir and fix cover\n\nWe need the ability to run some of our tests with sudo permissions. This\nis generally a bad practice for unit tests, but this shouldnt prevent us\nfrom writing gaining this type of test coverage. To deal with this, lets\nmove our unit tests in to a unit test specific dir so we can have a\nspecific tox target for them.\n\nWhen conducting this move it was noticed that our cover target is\ncompletely broken. Fixing it up as part of pointing it at the new unit\ntest dir.\n\nChange-Id: Iedf121c34aefc00696bc974019833fc286a7b8ac\n'}]",0,413277,5b601db7af456690f0266146be2e017a5a870e85,12,3,2,10035,,,0,"Move tests in to unit specific dir and fix cover

We need the ability to run some of our tests with sudo permissions. This
is generally a bad practice for unit tests, but this shouldnt prevent us
from writing gaining this type of test coverage. To deal with this, lets
move our unit tests in to a unit test specific dir so we can have a
specific tox target for them.

When conducting this move it was noticed that our cover target is
completely broken. Fixing it up as part of pointing it at the new unit
test dir.

Change-Id: Iedf121c34aefc00696bc974019833fc286a7b8ac
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/77/413277/2 && git format-patch -1 --stdout FETCH_HEAD,"['diskimage_builder/tests/unit/__init__.py', 'diskimage_builder/tests/unit/test_loggingconfig.py', 'diskimage_builder/tests/unit/test_elementdeps.py', 'diskimage_builder/tests/unit/test_elements.py', 'diskimage_builder/tests/unit/test_no_dup_filenames.py', 'tox.ini', 'diskimage_builder/tests/unit/base.py']",7,c9db5cef9f8c587d9c1874754ecd0870290157b4,,,,7,5
openstack%2Fironic~master~I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86,openstack/ironic,master,I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86,Ironic devstack portgroup support,NEW,2016-10-04 12:17:07.000000000,2017-12-18 04:26:38.000000000,,"[{'_account_id': 10118}, {'_account_id': 11929}, {'_account_id': 12356}, {'_account_id': 14525}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 22255}]","[{'number': 1, 'created': '2016-10-04 12:17:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6a80b736c0dee42e88b157ebc8728f0d29b16784', 'message': 'WIP: devstack portgroup support\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 2, 'created': '2016-10-04 14:53:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f6f403aa3a3baca0348697e34fe61d4166f6144a', 'message': 'WIP: devstack portgroup support\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 3, 'created': '2016-10-05 13:34:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b0ea6eee710e24e3e4d5cc423af14f2b380a2a49', 'message': 'WIP: devstack portgroup support\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 4, 'created': '2016-10-05 14:06:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2672ef313962db3c1a0c40ef1acc269f471a1cf8', 'message': 'WIP: devstack portgroup support\n\nNova portgroups patch:\nDepends-On: I60bfc588a0d5c79bc9d5b02286c3dc2afc660892\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 5, 'created': '2016-10-06 16:08:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fd2959a9d057a0b20b7cf81bf2c420de2b61c622', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nNova portgroups patch:\nDepends-On: I60bfc588a0d5c79bc9d5b02286c3dc2afc660892\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 6, 'created': '2016-10-07 08:23:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2ff041752690fff273327bad658702af21618d90', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nNova portgroups patch:\nDepends-On: I60bfc588a0d5c79bc9d5b02286c3dc2afc660892\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 7, 'created': '2016-10-07 10:50:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8e9a9a3e30dc7c355068ead65dce5d2a1b90b411', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nNova portgroups patch:\nDepends-On: I60bfc588a0d5c79bc9d5b02286c3dc2afc660892\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 8, 'created': '2016-10-19 12:16:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/842907c3e4f8bb3cc29e182fe1452e402032703b', 'message': 'WIP: Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nNova portgroups patch:\nDepends-On: I60bfc588a0d5c79bc9d5b02286c3dc2afc660892\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 9, 'created': '2016-10-19 15:23:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/464636f3fb2c97c9606deb557640eabfa4d4fadf', 'message': 'WIP: Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nNova portgroups patch:\nDepends-On: Ie0cfd3100d43a55c6eb6b449b75c4f62be5ce2c4\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 10, 'created': '2016-10-20 12:51:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/4f5d89b8b489bd3c2ef4809c5d3f3760d2d3c735', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 11, 'created': '2016-10-31 11:43:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/012d1bfd1b1673d07f75a1d23006e918d28a3d49', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 12, 'created': '2016-11-03 11:37:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/255b8462bbcb5d86187382a4a7f85446cf0dcbdc', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 13, 'created': '2016-11-03 14:10:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7c0c180476e3a51c7ad57142e2c6568c2caa533c', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 14, 'created': '2016-11-03 14:27:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3d89f1ec01b229c2125e711c6b42afd9622880b5', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 15, 'created': '2016-11-07 09:19:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/887edb55a271b9c97218806fa93964600cde2557', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 16, 'created': '2016-11-08 17:39:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a37d665cd6f7f8c027d61bbcb563f366b1a984c8', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 17, 'created': '2016-11-08 19:55:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/04fc5ba4bb52753e9ae9cc62cb2cb60b63e671ff', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 18, 'created': '2016-11-09 11:38:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bfac21d0942aec55ce12ab09e63c64ef5340caf5', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 19, 'created': '2016-11-10 16:08:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fad7edcd4c9388795c2e1ab88e55a990e66c2560', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 20, 'created': '2016-11-11 10:58:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/97be62acdad336fe849ca78aa67c969d37237d3e', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 21, 'created': '2016-11-16 12:36:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/37cad80af512ed5ade6b6f76a5bf86f938485d05', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 22, 'created': '2016-12-20 12:41:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8f10ef1a90d1e559d8254389a337e666877afbf5', 'message': 'WIP: Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 23, 'created': '2016-12-20 16:53:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8a9e6ba64635d399af390444520b3536943503b9', 'message': 'WIP: Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 24, 'created': '2016-12-20 16:58:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e64ad09567b891b5f8f3ae8607ec0d7905c712e7', 'message': 'WIP: Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 25, 'created': '2016-12-21 10:16:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ade793016a349ad244473c270458e0f5b11d5184', 'message': 'WIP: Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 26, 'created': '2016-12-21 12:00:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5271bcbfabf37d412af6a574ad3f14b6aa3eb55f', 'message': 'WIP: Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 27, 'created': '2016-12-21 12:54:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f62b6f012452872c1cd9243df5791542ce2158d7', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 28, 'created': '2017-01-13 14:16:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ba8e2c6d900e4a4a7dab99e2e137e27c877be93f', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 29, 'created': '2017-01-13 22:52:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/04a60432b28ec082d3a963c6f2e3a102519adf45', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 30, 'created': '2017-01-16 09:36:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/375bf8b482b67534c99c08d1c37de0b1bf7e8866', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 31, 'created': '2017-01-16 12:50:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/edc8339eb8f1e6dbe6ed21c61d8b822901c729cb', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 32, 'created': '2017-01-16 14:06:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9d82ee54ee7ef855e194c5981b1b14c4ce4de1a5', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 33, 'created': '2017-01-16 14:32:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/088ac92e7e730910e2d859bb7fad38628eb21c77', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}, {'number': 34, 'created': '2017-01-17 07:04:52.000000000', 'files': ['devstack/lib/ironic', 'devstack/files/hooks/qemu', 'devstack/tools/ironic/templates/vm.xml', 'devstack/files/rpms/ironic', 'devstack/files/debs/ironic', 'devstack/tools/ironic/scripts/configure-vm.py', 'devstack/tools/ironic/scripts/create-node.sh'], 'web_link': 'https://opendev.org/openstack/ironic/commit/10fa4ff323dcc53cffd038d10b80afddca7032cd', 'message': 'Ironic devstack portgroup support\n\nThis change enables to devstack setup environment with\nportgroup support.\n\nPartial-bug: #1618754\nChange-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86\n'}]",4,381743,10fa4ff323dcc53cffd038d10b80afddca7032cd,160,8,34,14525,,,0,"Ironic devstack portgroup support

This change enables to devstack setup environment with
portgroup support.

Partial-bug: #1618754
Change-Id: I30186e1a8b6cdf1989657fa5ae3ddb35a5a19f86
",git fetch https://review.opendev.org/openstack/ironic refs/changes/43/381743/32 && git format-patch -1 --stdout FETCH_HEAD,"['devstack/lib/ironic', 'devstack/files/hooks/qemu', 'devstack/tools/ironic/templates/vm.xml', 'devstack/files/rpms/ironic', 'devstack/files/debs/ironic', 'devstack/tools/ironic/scripts/configure-vm.py', 'devstack/tools/ironic/scripts/create-node.sh']",7,6a80b736c0dee42e88b157ebc8728f0d29b16784,bug/1618754,"while getopts ""n:c:m:d:a:b:e:p:f:l:B:"" arg; do B) USE_BOND=$OPTARG;;LIBVIRT_NIC_DRIVER=${LIBVIRT_NIC_DRIVER:-""e1000""}vm_opts="""" if [[ -n ""USE_BOND"" ]]; then vm_opts+=""--use-bond True"" fi --emulator $EMULATOR --bridge br-$NAME --disk-format $DISK_FORMAT $VM_LOGGING \ $vm_opts >&2","while getopts ""n:c:m:d:a:b:e:p:f:l:"" arg; doLIBVIRT_NIC_DRIVER=${LIBVIRT_NIC_DRIVER:-""virtio""} --emulator $EMULATOR --bridge br-$NAME --disk-format $DISK_FORMAT $VM_LOGGING >&2",59,14
openstack%2Fswift~master~I8e3892f7dc8ef6f7d54692519e9e951559823fd0,openstack/swift,master,I8e3892f7dc8ef6f7d54692519e9e951559823fd0,add policy info to swift-get-nodes,NEW,2016-09-19 19:19:51.000000000,2017-12-18 04:26:19.000000000,,"[{'_account_id': 9625}, {'_account_id': 12261}, {'_account_id': 13052}, {'_account_id': 18334}, {'_account_id': 23733}]","[{'number': 1, 'created': '2016-09-19 19:19:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/d4e94eb4da98f3a34a5919b8aa9f90f0a1a1f527', 'message': 'add policy info to swift-get-nodes\n\nAdded policy info to print_item_locations()\nThe policy type and number of replicas or data+parity\ncould be helpful information when looking at the list\nof devices where data will be stored.\n\nChange-Id: I8e3892f7dc8ef6f7d54692519e9e951559823fd0\nSigned-off-by: Thiago da Silva <thiago@redhat.com>\n'}, {'number': 2, 'created': '2016-09-20 01:20:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2d0691a06d5c44ce8a81338dbc5babadb363baa7', 'message': 'add policy info to swift-get-nodes\n\nAdded policy info to print_item_locations()\nThe policy type and number of replicas or data+parity\ncould be helpful information when looking at the list\nof devices where data will be stored.\n\nChange-Id: I8e3892f7dc8ef6f7d54692519e9e951559823fd0\nSigned-off-by: Thiago da Silva <thiago@redhat.com>\n'}, {'number': 3, 'created': '2016-10-05 19:22:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/492310d94578ba6d1c700945a4a965f6ce584e3e', 'message': 'add policy info to swift-get-nodes\n\nAdded policy info to print_item_locations()\nThe policy type and number of replicas or data+parity\ncould be helpful information when looking at the list\nof devices where data will be stored.\n\nChange-Id: I8e3892f7dc8ef6f7d54692519e9e951559823fd0\nSigned-off-by: Thiago da Silva <thiago@redhat.com>\n'}, {'number': 4, 'created': '2016-11-16 16:08:27.000000000', 'files': ['swift/cli/info.py', 'test/unit/cli/test_info.py', 'test/unit/__init__.py', 'test/unit/helpers.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/da5751b28d4a28c8d3a320d728afc435deeb0eb1', 'message': 'add policy info to swift-get-nodes\n\nAdded policy info to print_item_locations()\nThe policy type and number of replicas or data+parity\ncould be helpful information when looking at the list\nof devices where data will be stored.\n\nChange-Id: I8e3892f7dc8ef6f7d54692519e9e951559823fd0\nSigned-off-by: Thiago da Silva <thiago@redhat.com>\n'}]",2,372716,da5751b28d4a28c8d3a320d728afc435deeb0eb1,19,5,4,9625,,,0,"add policy info to swift-get-nodes

Added policy info to print_item_locations()
The policy type and number of replicas or data+parity
could be helpful information when looking at the list
of devices where data will be stored.

Change-Id: I8e3892f7dc8ef6f7d54692519e9e951559823fd0
Signed-off-by: Thiago da Silva <thiago@redhat.com>
",git fetch https://review.opendev.org/openstack/swift refs/changes/16/372716/4 && git format-patch -1 --stdout FETCH_HEAD,['swift/cli/info.py'],1,d4e94eb4da98f3a34a5919b8aa9f90f0a1a1f527,add_policy_info,"from swift.common.storage_policy import POLICIES, ECStoragePolicy policy = None if obj: if not policy or not policy.object_ring: policy = POLICIES.get_by_index(policy_index) policy.load_ring(swift_dir) print('Policy type\t%s' % policy.policy_type) if isinstance(policy, ECStoragePolicy): print('Data + Parity\t%s+%s\n\n' % ( policy.ec_ndata, policy.ec_nparity)) else: print('Replicas\t%s\n\n' % policy.object_ring.replica_count) ",from swift.common.storage_policy import POLICIES,14,1
openstack%2Fswift~master~I9c306a0ef24eab622653ad36d00d966dfd29eaf9,openstack/swift,master,I9c306a0ef24eab622653ad36d00d966dfd29eaf9,Retrieve recon data from base port and add --exclude-zeroweight.,NEW,2016-07-18 09:11:01.000000000,2017-12-18 04:26:17.000000000,,"[{'_account_id': 1179}, {'_account_id': 4608}, {'_account_id': 6968}, {'_account_id': 7847}, {'_account_id': 12050}, {'_account_id': 13052}, {'_account_id': 21239}]","[{'number': 1, 'created': '2016-07-18 09:11:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/930db5fbc2a457a7bcaffd5d38fc882228f0f549', 'message': 'Prevent recon to check each ports for each host and skip zero weight\ndeviecs in default.\n\nSwift-recon checks all object ports of each host for the same result\nwhen server_per_port enabled, so just use one of object ports should\nbe enough.\nIgnore zero weight deviecs when list all host entries from ringfile\nin default, use the option `--zeroweight` (`-w`) to include zero weight\ndevices to host entries.\n\nChange-Id: I9c306a0ef24eab622653ad36d00d966dfd29eaf9\n'}, {'number': 2, 'created': '2016-07-18 09:20:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0d10cdd5fe0997820640f906d141ff647cb43814', 'message': 'Retrieve recon data from one of object ports and skip zero weight deviecs.\n\nSwift-recon retrieves the same result from all object ports of each host\nwhen server_per_port enabled, so just use one of object ports should\nbe enough.\nIgnore zero weight deviecs when list all host entries from ringfile\nin default, use the option `--zeroweight` (`-w`) to include zero weight\ndevices to host entries.\n\nChange-Id: I9c306a0ef24eab622653ad36d00d966dfd29eaf9\n'}, {'number': 3, 'created': '2016-07-26 03:02:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a84bc553bc1e79c25b107554462a9ed619fc305e', 'message': 'Retrieve recon data from one of object ports and add --exclude-zeroweight.\n\nSwift-recon retrieves the same result from all object ports of each host\nwhen server_per_port enabled, so just use one of object ports should\nbe enough.\nAdd `--exclude-zeroweight` (`-w`) to exclude zero weight deviecs when list\nall host entries from ringfile. All zero weight drives will in host\nlist unless you enabled this option.\n\nChange-Id: I9c306a0ef24eab622653ad36d00d966dfd29eaf9\n'}, {'number': 4, 'created': '2016-07-27 17:48:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a8992bdc0e61d0d6179b39b791cc5c30d7cabdb6', 'message': 'Retrieve recon data from base port and add --exclude-zeroweight.\n\nSwift-recon retrieves the same result from all object ports of each host\nwhen server_per_port enabled, so just use base_bind port to retrieve it\nand it should be enough.\nAdd `--exclude-zeroweight` (`-w`) to exclude zero weight deviecs when list\nall host entries from ringfile. All zero weight drives will in host\nlist unless you enabled this option.\n\nChange-Id: I9c306a0ef24eab622653ad36d00d966dfd29eaf9\n'}, {'number': 5, 'created': '2016-07-30 15:06:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2d52e83e15c9aa2686c007788ceccda387d02675', 'message': 'Retrieve recon data from base port and add --exclude-zeroweight.\n\nSwift-recon retrieves the same result from all object ports of each host\nwhen server_per_port enabled, so just use bind_port to retrieve it\nand it should be enough.\nAdd `--exclude-zeroweight` (`-w`) to exclude zero weight deviecs when list\nall host entries from ringfile. All zero weight drives will in host\nlist unless you enabled this option.\n\nChange-Id: I9c306a0ef24eab622653ad36d00d966dfd29eaf9\n'}, {'number': 6, 'created': '2016-07-30 15:26:16.000000000', 'files': ['test/unit/cli/test_recon.py', 'doc/manpages/swift-recon.1', 'swift/common/middleware/recon.py', 'swift/cli/recon.py', 'doc/source/admin_guide.rst'], 'web_link': 'https://opendev.org/openstack/swift/commit/f083ebd76ef2a81bc76469b8692d0e5fb285a06f', 'message': 'Retrieve recon data from base port and add --exclude-zeroweight.\n\nSwift-recon retrieves the same result from all object ports of each host\nwhen servers_per_port enabled, so try to find the bind_port from\n`recon/bindport` of each running servers, that can make sure the\nbehavior is similar as normal (servers_per_port disabled).\n\nAdd `--exclude-zeroweight` (`-w`) to exclude zero weight deviecs when list\nall hosts from ring files. All zero weight drives will in the list unless\nyou enabled this option.\n\nChange-Id: I9c306a0ef24eab622653ad36d00d966dfd29eaf9\n'}]",13,343514,f083ebd76ef2a81bc76469b8692d0e5fb285a06f,26,7,6,12050,,,0,"Retrieve recon data from base port and add --exclude-zeroweight.

Swift-recon retrieves the same result from all object ports of each host
when servers_per_port enabled, so try to find the bind_port from
`recon/bindport` of each running servers, that can make sure the
behavior is similar as normal (servers_per_port disabled).

Add `--exclude-zeroweight` (`-w`) to exclude zero weight deviecs when list
all hosts from ring files. All zero weight drives will in the list unless
you enabled this option.

Change-Id: I9c306a0ef24eab622653ad36d00d966dfd29eaf9
",git fetch https://review.opendev.org/openstack/swift refs/changes/14/343514/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/cli/test_recon.py', 'swift/cli/recon.py']",2,930db5fbc2a457a7bcaffd5d38fc882228f0f549,343514," def get_hosts(self, region_filter, zone_filter, zeroweight_filter, swift_dir, ring_names): :param zeroweight_filter: Include zero weight devices if True if not zeroweight_filter: devs = [d for d in devs if d['weight'] > 0] unique_hosts = {'ip': [], 'dev': []} for d in devs: if d['ip'] not in unique_hosts['ip']: unique_hosts['ip'].append(d['ip']) unique_hosts['dev'].append(d) return set((d['ip'], d['port']) for d in unique_hosts['dev']) args.add_option('--zeroweight', '-w', action=""store_true"", help=""Include zero weight devices"") options.zeroweight,"," def get_hosts(self, region_filter, zone_filter, swift_dir, ring_names): return set((d['ip'], d['port']) for d in devs)",68,35
openstack%2Frally~master~Iefab5e90d509ac8862b2d0b0f4e6f725a4535e58,openstack/rally,master,Iefab5e90d509ac8862b2d0b0f4e6f725a4535e58,Add cinder.CreateAndMigrateVolume scenario,NEW,2016-10-24 03:24:14.000000000,2017-12-18 04:26:12.000000000,,"[{'_account_id': 1736}, {'_account_id': 14817}, {'_account_id': 23435}]","[{'number': 1, 'created': '2016-10-24 03:24:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/be6d51407887abff57db3c969c4eedbe48b36bf5', 'message': 'Add cinder.CreateAndMigrateVolume scenario\n\ncreate a volume, then migrate the volume to new host\n\nChange-Id: Iefab5e90d509ac8862b2d0b0f4e6f725a4535e58\n'}, {'number': 2, 'created': '2016-11-01 03:06:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/52059c261262a6054ad69e425156ed74d2e238bb', 'message': 'Add cinder.CreateAndMigrateVolume scenario\n\ncreate a volume, then migrate the volume to new host\n\nChange-Id: Iefab5e90d509ac8862b2d0b0f4e6f725a4535e58\n'}, {'number': 3, 'created': '2016-11-14 03:44:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5b646bee622a3e3cb7b334875e5bf941f887b534', 'message': 'Add cinder.CreateAndMigrateVolume scenario\n\ncreate a volume, then migrate the volume to new host\n\nChange-Id: Iefab5e90d509ac8862b2d0b0f4e6f725a4535e58\n'}, {'number': 4, 'created': '2016-12-06 02:23:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6ba4767c8a3e31901b93e6b609fc6ba3dfe6488c', 'message': 'Add cinder.CreateAndMigrateVolume scenario\n\ncreate a volume, then migrate the volume to new host\n\nChange-Id: Iefab5e90d509ac8862b2d0b0f4e6f725a4535e58\n'}, {'number': 5, 'created': '2017-01-24 05:27:53.000000000', 'files': ['samples/tasks/scenarios/cinder/create-and-migrate-volume.json', 'tests/unit/plugins/openstack/scenarios/cinder/test_volumes.py', 'rally/plugins/openstack/scenarios/cinder/utils.py', 'rally-jobs/cinder.yaml', 'tests/unit/plugins/openstack/scenarios/cinder/test_utils.py', 'rally/plugins/openstack/scenarios/cinder/volumes.py', 'samples/tasks/scenarios/cinder/create-and-migrate-volume.yaml'], 'web_link': 'https://opendev.org/openstack/rally/commit/e2dfd84d67a1d8ce3d94d2d917858a48cb6feae3', 'message': 'Add cinder.CreateAndMigrateVolume scenario\n\ncreate a volume, then migrate the volume to new host\n\nChange-Id: Iefab5e90d509ac8862b2d0b0f4e6f725a4535e58\n'}]",2,390145,e2dfd84d67a1d8ce3d94d2d917858a48cb6feae3,22,3,5,23435,,,0,"Add cinder.CreateAndMigrateVolume scenario

create a volume, then migrate the volume to new host

Change-Id: Iefab5e90d509ac8862b2d0b0f4e6f725a4535e58
",git fetch https://review.opendev.org/openstack/rally refs/changes/45/390145/5 && git format-patch -1 --stdout FETCH_HEAD,"['samples/tasks/scenarios/cinder/create-and-migrate-volume.json', 'tests/unit/plugins/openstack/scenarios/cinder/test_volumes.py', 'rally/plugins/openstack/scenarios/cinder/utils.py', 'rally-jobs/cinder.yaml', 'tests/unit/plugins/openstack/scenarios/cinder/test_utils.py', 'rally/plugins/openstack/scenarios/cinder/volumes.py', 'samples/tasks/scenarios/cinder/create-and-migrate-volume.yaml']",7,be6d51407887abff57db3c969c4eedbe48b36bf5,cinder.CreateAndMigrateVolume,"--- CinderVolumes.create_and_migrate_volume: - args: size: 1 create_volume_kwargs: {} migrate_volume_kwargs: {} runner: type: ""constant"" times: 3 concurrency: 2 context: users: tenants: 2 users_per_tenant: 2 sla: failure_rate: max: 0 ",,211,2
openstack%2Fkolla~master~I0b10769e3dcd33f22cfe503c79ad73301f3e8827,openstack/kolla,master,I0b10769e3dcd33f22cfe503c79ad73301f3e8827,Install all python packages in global-requirements.txt file,ABANDONED,2016-11-06 05:04:27.000000000,2017-12-18 04:26:01.000000000,,"[{'_account_id': 1390}, {'_account_id': 2834}, {'_account_id': 7488}, {'_account_id': 16233}]","[{'number': 1, 'created': '2016-11-06 05:04:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/d8400040462a37d0e948686ecf6fba9a49530f33', 'message': 'Install all python packages in global-requirements.txt file\n\nInstall all python packages to reduce the total size of images.\n\nChange-Id: I0b10769e3dcd33f22cfe503c79ad73301f3e8827\n'}, {'number': 2, 'created': '2016-11-07 07:37:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/7a3e081d86b2dea1cea08098028ee9a7b2e565e1', 'message': 'Install all python packages in global-requirements.txt file\n\nInstall all python packages to reduce the total size of images.\n\nChange-Id: I0b10769e3dcd33f22cfe503c79ad73301f3e8827\n'}, {'number': 3, 'created': '2016-11-07 07:39:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/3c0065738a7dad3bfa01d5d113bd326c74c75d78', 'message': 'Install all python packages in global-requirements.txt file\n\nInstall all python packages to reduce the total size of images.\n\nChange-Id: I0b10769e3dcd33f22cfe503c79ad73301f3e8827\n'}, {'number': 4, 'created': '2017-01-18 10:39:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/7476724b8482c32abff3170bb27f66f428afb93c', 'message': 'Install all python packages in global-requirements.txt file\n\nInstall all python packages to reduce the total size of images.\n\nChange-Id: I0b10769e3dcd33f22cfe503c79ad73301f3e8827\n'}, {'number': 5, 'created': '2017-01-21 08:59:03.000000000', 'files': ['docker/openstack-base/Dockerfile.j2', 'tools/gate_run.sh'], 'web_link': 'https://opendev.org/openstack/kolla/commit/36ffd1f990f2c56dfdaac1bb164e6c20296822cb', 'message': 'Install all python packages in global-requirements.txt file\n\nInstall all python packages to reduce the total size of images.\n\nChange-Id: I0b10769e3dcd33f22cfe503c79ad73301f3e8827\n'}]",1,394150,36ffd1f990f2c56dfdaac1bb164e6c20296822cb,20,4,5,7488,,,0,"Install all python packages in global-requirements.txt file

Install all python packages to reduce the total size of images.

Change-Id: I0b10769e3dcd33f22cfe503c79ad73301f3e8827
",git fetch https://review.opendev.org/openstack/kolla refs/changes/50/394150/5 && git format-patch -1 --stdout FETCH_HEAD,"['docker/openstack-base/Dockerfile.j2', 'docker/macros.j2']",2,d8400040462a37d0e948686ecf6fba9a49530f33,,{% if packages %}{% endif %},,5,104
openstack%2Fironic~master~I9383b603e93ad4101cfdc6eabc27fe96a78929af,openstack/ironic,master,I9383b603e93ad4101cfdc6eabc27fe96a78929af,Add previous config name to Multitenancy doc,NEW,2017-01-23 08:30:06.000000000,2017-12-18 04:25:50.000000000,,"[{'_account_id': 6618}, {'_account_id': 7882}, {'_account_id': 12356}, {'_account_id': 19339}]","[{'number': 1, 'created': '2017-01-23 08:30:06.000000000', 'files': ['doc/source/deploy/multitenancy.rst'], 'web_link': 'https://opendev.org/openstack/ironic/commit/40004588a388c2a84aa24eb7fdfc19ae2173f0de', 'message': 'Add previous config name to Multitenancy doc\n\nconfig options ""cleaning_network"" and ""provisioning_network"" were\npreviously named ""cleaning_network_uuid"" and ""provisioning_network_uuid"".\nThis patch set adds this information.\n\nChange-Id: I9383b603e93ad4101cfdc6eabc27fe96a78929af\n'}]",1,423984,40004588a388c2a84aa24eb7fdfc19ae2173f0de,8,4,1,7882,,,0,"Add previous config name to Multitenancy doc

config options ""cleaning_network"" and ""provisioning_network"" were
previously named ""cleaning_network_uuid"" and ""provisioning_network_uuid"".
This patch set adds this information.

Change-Id: I9383b603e93ad4101cfdc6eabc27fe96a78929af
",git fetch https://review.opendev.org/openstack/ironic refs/changes/84/423984/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/deploy/multitenancy.rst'],1,40004588a388c2a84aa24eb7fdfc19ae2173f0de,fix-doc-multitenancy," requires that ``provisioning_network`` (Prior to Ocata, this option was named 'provisioning_network_uuid') and ``cleaning_network`` (Prior to Ocata, this option was named 'cleaning_network_uuid') configuration options are set to a valid neutron network UUIDs or names,"," requires that ``provisioning_network`` and ``cleaning_network`` configuration options are set to a valid neutron network UUIDs or names,",5,2
openstack%2Fdiskimage-builder~feature%2Fv2~I076e5216da1df97a7863b24a0f7597dc9d335f34,openstack/diskimage-builder,feature/v2,I076e5216da1df97a7863b24a0f7597dc9d335f34,Add multi partition func test,NEW,2016-10-06 15:52:53.000000000,2017-12-18 04:25:48.000000000,,"[{'_account_id': 4162}, {'_account_id': 6133}, {'_account_id': 7118}, {'_account_id': 10035}, {'_account_id': 12459}, {'_account_id': 21741}]","[{'number': 1, 'created': '2016-10-06 15:52:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/54f86d51fee34b2e40eb6e121c47999e3843c0e9', 'message': 'Add multi partition func test\n\nChange-Id: I076e5216da1df97a7863b24a0f7597dc9d335f34\n'}, {'number': 2, 'created': '2016-10-17 00:54:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/38e6afdb2fe69cc8964c8ea9ef481116ff1058d9', 'message': 'Add multi partition func test\n\nChange-Id: I076e5216da1df97a7863b24a0f7597dc9d335f34\n'}, {'number': 3, 'created': '2016-10-31 16:28:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/6ce7a72873178d3df740bf4c28cd367ce02b7e4d', 'message': 'Add multi partition func test\n\nChange-Id: I076e5216da1df97a7863b24a0f7597dc9d335f34\n'}, {'number': 4, 'created': '2016-11-02 14:02:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/22e6e86f62034aedbe4dfc8c95f22e959e9efae1', 'message': 'Add multi partition func test\n\nChange-Id: I076e5216da1df97a7863b24a0f7597dc9d335f34\n'}, {'number': 5, 'created': '2016-11-04 15:17:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/5d658ff3152c9f69a7ec9ec131b40104e655de0d', 'message': 'Add multi partition func test\n\nChange-Id: I076e5216da1df97a7863b24a0f7597dc9d335f34\n'}, {'number': 6, 'created': '2016-12-15 18:59:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/b596a576fb5327558a2b82b6e351de12e5f821af', 'message': 'Add multi partition func test\n\nChange-Id: I076e5216da1df97a7863b24a0f7597dc9d335f34\n'}, {'number': 7, 'created': '2016-12-15 19:02:14.000000000', 'files': ['elements/ubuntu-minimal/test-elements/multi-partition-device/environment.d/10-block-device-layout'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/26c9df4025943b4b40a482ad050887a45c923a7d', 'message': 'Add multi partition func test\n\nChange-Id: I076e5216da1df97a7863b24a0f7597dc9d335f34\n'}]",0,383055,26c9df4025943b4b40a482ad050887a45c923a7d,37,6,7,10035,,,0,"Add multi partition func test

Change-Id: I076e5216da1df97a7863b24a0f7597dc9d335f34
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/55/383055/1 && git format-patch -1 --stdout FETCH_HEAD,['elements/ubuntu-minimal/test-elements/multi-partition-device/environment.d/10-block-device-layout'],1,54f86d51fee34b2e40eb6e121c47999e3843c0e9,block-device/refactor/303-partitioning,"export DIB_BLOCK_DEVICE_CONFIG='[ [[""local_loop"", {""name"": ""rootdisk""}, {""name"": ""datadisk""}]], [[""partitioning"", {""rootdisk"": { ""label"": ""mbr"", ""partitions"": [{""name"": ""part-01"", ""flags"": [""boot""], ""size"": ""1GiB""}, {""name"": ""rd-partition1"", ""size"": ""100%""}]}}, {""datadisk"": { ""label"": ""mbr"", ""partitions"": [{""name"": ""data0"", ""size"": ""33%""}, {""name"": ""data1"", ""size"": ""33%""}, {""name"": ""data2"", ""size"": ""33%""}]}}]]]' ",,22,0
openstack%2Fdiskimage-builder~feature%2Fv2~I22b529c1ddf4a4c6d44eb7a1c0d8f12491cc7c9f,openstack/diskimage-builder,feature/v2,I22b529c1ddf4a4c6d44eb7a1c0d8f12491cc7c9f,DNM: Fixups for block device patch,NEW,2016-12-15 18:58:33.000000000,2017-12-18 04:25:45.000000000,,[],"[{'number': 1, 'created': '2016-12-15 18:58:33.000000000', 'files': ['diskimage_builder/block_device/level0/localloop.py', 'diskimage_builder/tests/functional/test_blockdevice_mbr.py', 'diskimage_builder/block_device/level1/mbr.py', 'diskimage_builder/block_device/level1/partitioning.py'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/387f2dc633ab17d93657dd85a005fae8111e112a', 'message': 'DNM: Fixups for block device patch\n\nChange-Id: I22b529c1ddf4a4c6d44eb7a1c0d8f12491cc7c9f\n'}]",0,411462,387f2dc633ab17d93657dd85a005fae8111e112a,5,0,1,10035,,,0,"DNM: Fixups for block device patch

Change-Id: I22b529c1ddf4a4c6d44eb7a1c0d8f12491cc7c9f
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/62/411462/1 && git format-patch -1 --stdout FETCH_HEAD,"['diskimage_builder/block_device/level0/localloop.py', 'diskimage_builder/tests/functional/test_blockdevice_mbr.py', 'diskimage_builder/block_device/level1/mbr.py', 'diskimage_builder/block_device/level1/partitioning.py']",4,387f2dc633ab17d93657dd85a005fae8111e112a,block-device/refactor/306-partitioning, assert self.label == 'mbr', if self.label != 'mbr': assert False,13,9
openstack%2Frally~master~I2ae2a6cc97d8628bb22ceb44f36e6e6ca7da94fa,openstack/rally,master,I2ae2a6cc97d8628bb22ceb44f36e6e6ca7da94fa,Added auto_assign_nic option to nova/seccgroup,NEW,2017-01-25 22:08:38.000000000,2017-12-18 04:25:28.000000000,,[{'_account_id': 14817}],"[{'number': 1, 'created': '2017-01-25 22:08:38.000000000', 'files': ['rally/plugins/openstack/scenarios/nova/security_group.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/9143d00dec7ca4ba144a2ec92d479a2dbc101569', 'message': 'Added auto_assign_nic option to nova/seccgroup\n\nIn Nova plugins auto_assign_nic only works for Server scenario.\nAt the same time, boot_and_delete_with_secgroup is the part of\nsecurity_group module. Added same option there.\n\nChange-Id: I2ae2a6cc97d8628bb22ceb44f36e6e6ca7da94fa\n'}]",0,425437,9143d00dec7ca4ba144a2ec92d479a2dbc101569,4,1,1,11782,,,0,"Added auto_assign_nic option to nova/seccgroup

In Nova plugins auto_assign_nic only works for Server scenario.
At the same time, boot_and_delete_with_secgroup is the part of
security_group module. Added same option there.

Change-Id: I2ae2a6cc97d8628bb22ceb44f36e6e6ca7da94fa
",git fetch https://review.opendev.org/openstack/rally refs/changes/37/425437/1 && git format-patch -1 --stdout FETCH_HEAD,['rally/plugins/openstack/scenarios/nova/security_group.py'],1,9143d00dec7ca4ba144a2ec92d479a2dbc101569,auto_ass_nic_fix_sec_group," def run(self, image, flavor, auto_assign_nic=False, security_group_count, server = self._boot_server(image, flavor, auto_assign_nic=auto_assign_nic def run(self, image, flavor, auto_assign_nic=False, security_group_count=1, server = self._boot_server(image, flavor, auto_assign_nic=auto_assign_nic, **kwargs)"," def run(self, image, flavor, security_group_count, server = self._boot_server(image, flavor, def run(self, image, flavor, security_group_count=1, server = self._boot_server(image, flavor, **kwargs)",5,4
openstack%2Fironic~master~Iec6b4bc32faa4cd0130cd0c465d6eb02ebf24bd3,openstack/ironic,master,Iec6b4bc32faa4cd0130cd0c465d6eb02ebf24bd3,WIP host ipxe static assets in devstack on :80,NEW,2017-01-23 23:44:11.000000000,2017-12-18 04:25:23.000000000,,"[{'_account_id': 10118}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}]","[{'number': 1, 'created': '2017-01-23 23:44:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7957fd0738cb9f6bd16e0e23dccff2d23aaa5d1e', 'message': 'WIP host ipxe static assets in devstack on :80\n\nChange-Id: Iec6b4bc32faa4cd0130cd0c465d6eb02ebf24bd3\n'}, {'number': 2, 'created': '2017-01-24 22:28:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/99da159542f7c9db03606efafcfe8c3ed713c42b', 'message': 'WIP host ipxe static assets in devstack on :80\n\nChange-Id: Iec6b4bc32faa4cd0130cd0c465d6eb02ebf24bd3\n'}, {'number': 3, 'created': '2017-01-25 00:24:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5f0ff13596ecf9cc047064f4e6f37e7f0a574e3d', 'message': 'WIP host ipxe static assets in devstack on :80\n\nChange-Id: Iec6b4bc32faa4cd0130cd0c465d6eb02ebf24bd3\n'}, {'number': 4, 'created': '2017-01-25 23:57:01.000000000', 'files': ['devstack/lib/ironic', 'devstack/files/apache-ironic.template'], 'web_link': 'https://opendev.org/openstack/ironic/commit/028d4eea035210b5b28b28ee0be67a3218cb0889', 'message': 'WIP host ipxe static assets in devstack on :80\n\nChange-Id: Iec6b4bc32faa4cd0130cd0c465d6eb02ebf24bd3\n'}]",0,424374,028d4eea035210b5b28b28ee0be67a3218cb0889,22,4,4,23883,,,0,"WIP host ipxe static assets in devstack on :80

Change-Id: Iec6b4bc32faa4cd0130cd0c465d6eb02ebf24bd3
",git fetch https://review.opendev.org/openstack/ironic refs/changes/74/424374/4 && git format-patch -1 --stdout FETCH_HEAD,"['devstack/lib/ironic', 'devstack/files/apache-ironic.template']",2,7957fd0738cb9f6bd16e0e23dccff2d23aaa5d1e,bug/1593510," WSGIScriptAlias %WEBROOT% %IRONIC_API_DIR%/openstack_dashboard/wsgi/django.wsgi WSGIDaemonProcess ironic-api user=%USER% group=%GROUP% processes=3 threads=10 home=%IRONIC_API_DIR% display-name=%{GROUP} WSGIApplicationGroup %{GLOBAL} SetEnv APACHE_RUN_USER %USER% SetEnv APACHE_RUN_GROUP %GROUP% WSGIProcessGroup ironic-api DocumentRoot %IRONIC_API_DIR%/.blackhole/ Alias %WEBROOT%/media %IRONIC_API_DIR%/openstack_dashboard/static Alias %WEBROOT%/static %IRONIC_API_DIR%/static RedirectMatch ""^/$"" ""%WEBROOT%/"" <Directory /> Options FollowSymLinks <Directory %IRONIC_API_DIR%/> Options Indexes FollowSymLinks <IfVersion >= 2.4> Require all granted </IfVersion> <IfVersion < 2.4> Order allow,deny Allow from all </IfVersion> Alias /ironic %IRONIC_API_DIR%/keystone-wsgi-public <Location /identity> SetHandler wsgi-script Options +ExecCGI WSGIProcessGroup ironic-public WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On </Location>"," DocumentRoot ""%HTTPROOT%"" <Directory ""%HTTPROOT%""> Options Indexes FollowSymLinks Order allow,deny Allow from all Require all granted",39,7
openstack%2Fsalt-formula-kubernetes~master~I3020cf59876253982fb989995a4d7602ed281e0c,openstack/salt-formula-kubernetes,master,I3020cf59876253982fb989995a4d7602ed281e0c,copy kubernetes-server certs from master,NEW,2017-01-26 09:46:39.000000000,2017-12-18 04:25:21.000000000,,[{'_account_id': 24859}],"[{'number': 1, 'created': '2017-01-26 09:46:39.000000000', 'files': ['kubernetes/master/controller.sls', 'kubernetes/master/init.sls'], 'web_link': 'https://opendev.org/openstack/salt-formula-kubernetes/commit/72a5617b63fdaaa6ab7c2b4019d33bace31cc361', 'message': 'copy kubernetes-server certs from master\n\nChange-Id: I3020cf59876253982fb989995a4d7602ed281e0c\n'}]",0,425601,72a5617b63fdaaa6ab7c2b4019d33bace31cc361,5,1,1,24859,,,0,"copy kubernetes-server certs from master

Change-Id: I3020cf59876253982fb989995a4d7602ed281e0c
",git fetch https://review.opendev.org/openstack/salt-formula-kubernetes refs/changes/01/425601/1 && git format-patch -1 --stdout FETCH_HEAD,"['kubernetes/master/controller.sls', 'kubernetes/master/init.sls']",2,72a5617b63fdaaa6ab7c2b4019d33bace31cc361,certs,- kubernetes.master.setup ,- kubernetes.master.setup,12,1
openstack%2Fswift~master~I5804de385a748c6f880a0a4383eba59a1b98e3c8,openstack/swift,master,I5804de385a748c6f880a0a4383eba59a1b98e3c8,WIP Swift multinode; Do not merge,NEW,2017-01-27 03:47:20.000000000,2017-12-18 04:25:19.000000000,,"[{'_account_id': 13052}, {'_account_id': 19587}]","[{'number': 1, 'created': '2017-01-27 03:47:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/31b1774d417c3fedcb76b55d17e6f805f660963d', 'message': 'WIP Swift multinode; Do not merge\n\nPlease do not merge.  Attempting to test an infra change\ntesting a swift multinode gate job.\n\nChange-Id: I5804de385a748c6f880a0a4383eba59a1b98e3c8\n'}, {'number': 2, 'created': '2017-01-27 03:49:11.000000000', 'files': ['do-not-merge'], 'web_link': 'https://opendev.org/openstack/swift/commit/1dc92d7b3ac436c001807c1e9fd7937c61b4442d', 'message': 'WIP Swift multinode; Do not merge\n\nPlease do not merge.  Attempting to test an infra change\ntesting a swift multinode gate job.\n\nChange-Id: I5804de385a748c6f880a0a4383eba59a1b98e3c8\nDepends-On: Iad575cf23e59551faf4bf863b59f7e6eb4a80493\n'}]",0,426043,1dc92d7b3ac436c001807c1e9fd7937c61b4442d,9,2,2,19587,,,0,"WIP Swift multinode; Do not merge

Please do not merge.  Attempting to test an infra change
testing a swift multinode gate job.

Change-Id: I5804de385a748c6f880a0a4383eba59a1b98e3c8
Depends-On: Iad575cf23e59551faf4bf863b59f7e6eb4a80493
",git fetch https://review.opendev.org/openstack/swift refs/changes/43/426043/2 && git format-patch -1 --stdout FETCH_HEAD,['do-not-merge'],1,31b1774d417c3fedcb76b55d17e6f805f660963d,,,,0,0
openstack%2Fdiskimage-builder~master~I9372e195913798a851c96e62eee89029e067baa1,openstack/diskimage-builder,master,I9372e195913798a851c96e62eee89029e067baa1,Dont generate qcow2's in tests,NEW,2016-12-08 22:49:22.000000000,2017-12-18 04:25:14.000000000,,[{'_account_id': 7118}],"[{'number': 1, 'created': '2016-12-08 22:49:22.000000000', 'files': ['tests/run_functests.sh'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/a86a20d602c3c310487ab83c2a62f7853d418c6a', 'message': ""Dont generate qcow2's in tests\n\nIt is unnecessary for us to wait on qcow2 conversion after every image,\nand this is a pretty expensive task. Instead we should have an\nindividual test to get coverage of this and then only use the shortest\npath for verifying our elements.\n\nChange-Id: I9372e195913798a851c96e62eee89029e067baa1\n""}]",0,408840,a86a20d602c3c310487ab83c2a62f7853d418c6a,9,1,1,10035,,,0,"Dont generate qcow2's in tests

It is unnecessary for us to wait on qcow2 conversion after every image,
and this is a pretty expensive task. Instead we should have an
individual test to get coverage of this and then only use the shortest
path for verifying our elements.

Change-Id: I9372e195913798a851c96e62eee89029e067baa1
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/40/408840/1 && git format-patch -1 --stdout FETCH_HEAD,['tests/run_functests.sh'],1,a86a20d602c3c310487ab83c2a62f7853d418c6a,, $DIB_CMD -x -t tar ${use_tmp_flag} -o $dest_dir/image -n $element $test_element 2>&1 \," $DIB_CMD -x -t tar,qcow2 ${use_tmp_flag} -o $dest_dir/image -n $element $test_element 2>&1 \",1,1
openstack%2Fswift~master~I2d09253b824f011de789aa8f0fe02ede8aeeb937,openstack/swift,master,I2d09253b824f011de789aa8f0fe02ede8aeeb937,Add more tests for diskfile consolidate_hashes,NEW,2017-01-17 17:08:07.000000000,2017-12-18 04:24:28.000000000,,"[{'_account_id': 1179}, {'_account_id': 7847}, {'_account_id': 12261}, {'_account_id': 12279}, {'_account_id': 13052}]","[{'number': 1, 'created': '2017-01-17 17:08:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/59898b31d4d90d3c1a009ee25208b25c7f02440b', 'message': 'Add more tests for diskfile consolidate_hashes\n\nAdds unit tests direct on the consolidate_hashes method,\nin particular to verify previously untested behaviour\nwhen:\n\n- hashes.pkl does not exist but an invalidation file does\n- hashes.pkl exists but cannot be read\n\nThe latter case results in a valid hashes.pkl never being written,\nwhich may be considered a bug to be fixed in a later patch but\nthis patch is simply increasing test coverage of the existing code.\n\nChange-Id: I2d09253b824f011de789aa8f0fe02ede8aeeb937\n'}, {'number': 2, 'created': '2017-01-18 16:16:27.000000000', 'files': ['test/unit/obj/test_diskfile.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/32383103d1079c8f4d2269c9cca2ef651e37a0c1', 'message': 'Add more tests for diskfile consolidate_hashes\n\nAdds unit tests direct on the consolidate_hashes method,\nin particular to verify previously untested behaviour\nwhen:\n\n- hashes.pkl does not exist but an invalidation file does\n- hashes.pkl exists but cannot be read\n\nThe latter case can result in any suffix hashes in the unreadable\nfile not being included in a new hashes.pkl when new suffix hashes\nare calculated, which may be considered a bug to be fixed in a later\npatch, but this patch is simply increasing test coverage of the\nexisting code.\n\nChange-Id: I2d09253b824f011de789aa8f0fe02ede8aeeb937\n'}]",13,421385,32383103d1079c8f4d2269c9cca2ef651e37a0c1,19,5,2,7847,,,0,"Add more tests for diskfile consolidate_hashes

Adds unit tests direct on the consolidate_hashes method,
in particular to verify previously untested behaviour
when:

- hashes.pkl does not exist but an invalidation file does
- hashes.pkl exists but cannot be read

The latter case can result in any suffix hashes in the unreadable
file not being included in a new hashes.pkl when new suffix hashes
are calculated, which may be considered a bug to be fixed in a later
patch, but this patch is simply increasing test coverage of the
existing code.

Change-Id: I2d09253b824f011de789aa8f0fe02ede8aeeb937
",git fetch https://review.opendev.org/openstack/swift refs/changes/85/421385/2 && git format-patch -1 --stdout FETCH_HEAD,['test/unit/obj/test_diskfile.py'],1,59898b31d4d90d3c1a009ee25208b25c7f02440b,p-consolidate-hashes-tests," def test_consolidate_hashes_with_existing_hashes_pkl_file(self): @contextmanager def fake_open(path, mode): # only break the open of the hashes file to read if path == hashes_file and mode == 'rb': raise OSError(errno.EPERM) with open(path, mode) as fd: yield fd for policy in self.iter_policies(): df_mgr = self.df_router[policy] # create something to hash df = df_mgr.get_diskfile('sda1', '0', 'a', 'c', 'o', policy=policy) df.delete(self.ts()) suffix_dir = os.path.dirname(df._datadir) suffix = os.path.basename(suffix_dir) part_dir = os.path.dirname(suffix_dir) hashes_file = os.path.join(part_dir, diskfile.HASH_FILE) invalidations_file = os.path.join( part_dir, diskfile.HASH_INVALIDATIONS_FILE) with mock.patch.object(df_mgr, '_hash_suffix', return_value='fake hash'): df_mgr.get_hashes('sda1', '0', [], policy) # sanity check - hashes.pkl exists self.assertTrue(os.path.exists(hashes_file)) with open(hashes_file, 'rb') as f: pickled_hashes = pickle.load(f) self.assertEqual({suffix: 'fake hash'}, pickled_hashes) # OSError opening hashes.pkl with mock.patch('swift.obj.diskfile.write_pickle') \ as fake_write_pickle: with mock.patch('swift.obj.diskfile.open', fake_open): hashes = df_mgr.consolidate_hashes(part_dir) self.assertEqual({}, hashes) # verify there is still the same hashes.pkl (BUT return value of {} # can result in no rewrite of hashes.pkl in _get_hashes ? ) self.assertTrue(os.path.exists(hashes_file)) self.assertEqual(0, fake_write_pickle.call_count) with open(hashes_file, 'rb') as f: pickled_hashes = pickle.load(f) self.assertEqual({suffix: 'fake hash'}, pickled_hashes) # verify there is now an empty invalidations file self.assertTrue(os.path.exists(invalidations_file)) self.assertEqual(0, os.path.getsize(invalidations_file)) # valid data in hashes.pkl with mock.patch('swift.obj.diskfile.write_pickle') \ as fake_write_pickle: hashes = df_mgr.consolidate_hashes(part_dir) self.assertEqual({suffix: 'fake hash'}, hashes) # verify there is still the same hashes.pkl self.assertTrue(os.path.exists(hashes_file)) self.assertEqual(0, fake_write_pickle.call_count) with open(hashes_file, 'rb') as f: pickled_hashes = pickle.load(f) self.assertEqual({suffix: 'fake hash'}, pickled_hashes) # verify there is now an empty invalidations file self.assertTrue(os.path.exists(invalidations_file)) self.assertEqual(0, os.path.getsize(invalidations_file)) # valid data in hashes.pkl, invalidations file has entry df_mgr.invalidate_hash(suffix_dir) with open(invalidations_file) as ivf: invalidations = ivf.read() self.assertEqual(""%s\n"" % suffix, invalidations) # sanity check hashes = df_mgr.consolidate_hashes(part_dir) self.assertEqual({suffix: None}, hashes) # verify hashes.pkl has been updated self.assertTrue(os.path.exists(hashes_file)) with open(hashes_file, 'rb') as f: pickled_hashes = pickle.load(f) self.assertEqual({suffix: None}, pickled_hashes) # verify there is now an empty invalidations file self.assertTrue(os.path.exists(invalidations_file)) self.assertEqual(0, os.path.getsize(invalidations_file)) # invalid pickle data in hashes.pkl with open(hashes_file, 'wb') as f: f.write('') with mock.patch('swift.obj.diskfile.write_pickle') \ as fake_write_pickle: hashes = df_mgr.consolidate_hashes(part_dir) self.assertEqual(None, hashes) # verify there is still the same invalid hashes.pkl (that's ok, # return value of None will cause _get_hashes to recalc and rewrite # hashes.pkl) self.assertTrue(os.path.exists(hashes_file)) self.assertEqual(0, fake_write_pickle.call_count) with open(hashes_file, 'rb') as f: self.assertEqual('', f.read()) # verify there is now an empty invalidations file self.assertTrue(os.path.exists(invalidations_file)) self.assertEqual(0, os.path.getsize(invalidations_file)) def test_consolidate_hashes_with_no_existing_hashes_pkl_file(self): for policy in self.iter_policies(): df_mgr = self.df_router[policy] # create something to hash df = df_mgr.get_diskfile('sda1', '0', 'a', 'c', 'o', policy=policy) df.delete(self.ts()) suffix_dir = os.path.dirname(df._datadir) suffix = os.path.basename(suffix_dir) part_dir = os.path.dirname(suffix_dir) hashes_file = os.path.join(part_dir, diskfile.HASH_FILE) invalidations_file = os.path.join( part_dir, diskfile.HASH_INVALIDATIONS_FILE) # no pre-existing hashes.pkl or invalidations file hashes = df_mgr.consolidate_hashes(part_dir) self.assertIsNone(hashes) # verify there is still no hashes.pkl self.assertFalse(os.path.exists(hashes_file)) # verify there is still no invalidations file self.assertFalse(os.path.exists(invalidations_file)) # no pre-existing hashes.pkl but (unexpected) invalidations file # that raises exception when opened to clear with open(invalidations_file, 'wb') as ivf: ivf.write(""%s\n"" % suffix) with mock.patch('swift.obj.diskfile.open', side_effect=OSError(errno.EPERM)): self.assertRaises(OSError, df_mgr.consolidate_hashes, part_dir) self.assertIsNone(hashes) # verify there is still no hashes.pkl self.assertFalse(os.path.exists(hashes_file)) # sanity check - invalidations file was not cleared self.assertTrue(os.path.exists(invalidations_file)) with open(invalidations_file, 'rb') as ivf: self.assertEqual(""%s\n"" % suffix, ivf.read()) # no pre-existing hashes.pkl but (unexpected) invalidations file hashes = df_mgr.consolidate_hashes(part_dir) self.assertIsNone(hashes) # verify there is still no hashes.pkl self.assertFalse(os.path.exists(hashes_file)) # verify invalidations file was cleared self.assertTrue(os.path.exists(invalidations_file)) with open(invalidations_file, 'rb') as ivf: self.assertEqual("""", ivf.read()) def test_get_hashes_pickle_not_a_file(self): for policy in self.iter_policies(): df_mgr = self.df_router[policy] part_path = os.path.join(self.devices, self.existing_device, diskfile.get_data_dir(policy), '0') os.makedirs(part_path) # create a pre-existing dir named hashes.pkl!! hashes_file = os.path.join(part_path, diskfile.HASH_FILE) os.mkdir(hashes_file) with mock.patch('swift.obj.diskfile.write_pickle')\ as fake_write_pickle: hashes = df_mgr.get_hashes(self.existing_device, '0', [], policy) self.assertEqual(hashes, {}) # but no attempt to write a pickle file self.assertEqual(0, fake_write_pickle.call_count) self.assertTrue(os.path.isdir(hashes_file)) hashes_file = os.path.join(part_path, diskfile.HASH_FILE) open(hashes_file, 'w').close() # hashes.pkl is written with open(hashes_file) as f: self.assertEqual(hashes, pickle.load(f)) # so next call will not need to write it... with mock.patch('swift.obj.diskfile.write_pickle')\ as fake_write_pickle: df_mgr.get_hashes(self.existing_device, '0', [], policy) self.assertEqual(0, fake_write_pickle.call_count)"," open(os.path.join(part_path, diskfile.HASH_FILE), 'w').close()",174,1
openstack%2Fdiskimage-builder~feature%2Fv2~I0f79224538c8aa89792895cbea7b91ff7ec48b5f,openstack/diskimage-builder,feature/v2,I0f79224538c8aa89792895cbea7b91ff7ec48b5f,Build wholedisk images capable of UEFI secure boot,NEW,2016-11-11 13:39:26.000000000,2017-12-18 04:24:23.000000000,,"[{'_account_id': 10035}, {'_account_id': 11076}, {'_account_id': 24232}]","[{'number': 1, 'created': '2016-11-11 13:39:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/0fa6639de97e40a6281ee1214c91c65ee4d19594', 'message': 'Build wholedisk images capable of UEFI secure boot\n\nAdds capability to build wholedisk images which can boot in secured\nenvironments. Currently it supports ubuntu 14.04, fedora 22, centos7\nand rhel7 distros.\n\nDepends-On: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\nChange-Id: I0f79224538c8aa89792895cbea7b91ff7ec48b5f\n'}, {'number': 2, 'created': '2017-01-17 16:55:41.000000000', 'files': ['diskimage_builder/elements/bootloader/finalise.d/50-bootloader', 'diskimage_builder/elements/bootloader/pkg-map'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/2d29de3234d0ff73f9669545af9d42bef11f6e35', 'message': 'Build wholedisk images capable of UEFI secure boot\n\nAdds capability to build wholedisk images which can boot in secured\nenvironments. Currently it supports ubuntu 14.04, fedora 22, centos7\nand rhel7 distros.\n\nDepends-On: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\nChange-Id: I0f79224538c8aa89792895cbea7b91ff7ec48b5f\n'}]",1,396629,2d29de3234d0ff73f9669545af9d42bef11f6e35,8,3,2,18781,,,0,"Build wholedisk images capable of UEFI secure boot

Adds capability to build wholedisk images which can boot in secured
environments. Currently it supports ubuntu 14.04, fedora 22, centos7
and rhel7 distros.

Depends-On: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45
Change-Id: I0f79224538c8aa89792895cbea7b91ff7ec48b5f
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/29/396629/2 && git format-patch -1 --stdout FETCH_HEAD,"['diskimage_builder/elements/bootloader/finalise.d/50-bootloader', 'diskimage_builder/elements/bootloader/pkg-map']",2,0fa6639de97e40a6281ee1214c91c65ee4d19594,dib_secure_boot," ""grub-efi"": ""grub2-efi grub2-efi-modules"", ""grub-signed"": """", ""shim"": ""shim"", ""shim-signed"": ""shim-signed"" ""grub-efi"": ""grub-efi"", ""grub-signed"": ""grub-efi-amd64-signed"", ""shim"": ""shim"", ""shim-signed"": ""shim-signed"""," ""grub-efi"": ""grub2-efi grub2-efi-modules"" ""grub-efi"": ""grub-efi""",48,7
openstack%2Fdiskimage-builder~master~I578afa9d917b88ec413b9a216eee949e4c71048b,openstack/diskimage-builder,master,I578afa9d917b88ec413b9a216eee949e4c71048b,Add dib-shell command,NEW,2016-09-03 15:56:25.000000000,2017-12-18 04:24:04.000000000,,"[{'_account_id': 10035}, {'_account_id': 12459}]","[{'number': 1, 'created': '2016-09-03 15:56:25.000000000', 'files': ['diskimage_builder/dib_shell.py', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/9d4b1c22cb0cd4af53a965d23b9a9ab8bd3fa28d', 'message': 'Add dib-shell command\n\nA command which mounts a diskimage and allows a user to run commands\ninside. Useful for debug / editing.\n\nChange-Id: I578afa9d917b88ec413b9a216eee949e4c71048b\n'}]",0,365265,9d4b1c22cb0cd4af53a965d23b9a9ab8bd3fa28d,6,2,1,10035,,,0,"Add dib-shell command

A command which mounts a diskimage and allows a user to run commands
inside. Useful for debug / editing.

Change-Id: I578afa9d917b88ec413b9a216eee949e4c71048b
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/65/365265/1 && git format-patch -1 --stdout FETCH_HEAD,"['diskimage_builder/dib_shell.py', 'setup.cfg']",2,9d4b1c22cb0cd4af53a965d23b9a9ab8bd3fa28d,, [entry_points] console_scripts = dib-shell = diskimage_builder.dib_shell:main,,103,0
openstack%2Fdiskimage-builder~master~Id6cba7536441d11a98d860181dc55879535a5188,openstack/diskimage-builder,master,Id6cba7536441d11a98d860181dc55879535a5188,Allow disabling manifests,NEW,2016-08-22 21:20:36.000000000,2017-12-18 04:24:02.000000000,,"[{'_account_id': 7118}, {'_account_id': 10035}]","[{'number': 1, 'created': '2016-08-22 21:20:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/9d86435e2a9267ba6ab55f64468599df48c1cc20', 'message': ""Allow disabling manifests\n\nIt's totally reasonable for a user to disable the creation of a\nmanifests dir.\n\nChange-Id: Id6cba7536441d11a98d860181dc55879535a5188\n""}, {'number': 2, 'created': '2016-10-10 15:27:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/6e87478c4c38c2341b4f06bea15febb098093d33', 'message': ""Allow disabling manifests\n\nIt's totally reasonable for a user to disable the creation of a\nmanifests dir.\n\nChange-Id: Id6cba7536441d11a98d860181dc55879535a5188\n""}, {'number': 3, 'created': '2016-12-13 16:25:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/3341d1ef4ff938062abd534244d9523cd49ed25d', 'message': ""Allow disabling manifests\n\nIt's totally reasonable for a user to disable the creation of a\nmanifests dir.\n\nChange-Id: Id6cba7536441d11a98d860181dc55879535a5188\n""}, {'number': 4, 'created': '2016-12-13 16:28:33.000000000', 'files': ['elements/dpkg/finalise.d/99-write-dpkg-manifest', 'elements/manifests/extra-data.d/20-manifest-dir', 'elements/manifests/README.rst', 'elements/manifests/environment.d/14-manifests', 'elements/manifests/cleanup.d/01-copy-manifests-dir'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/48ebb42061b4d0d573245ab7205da48d4805c9f5', 'message': ""Allow disabling manifests\n\nIt's totally reasonable for a user to disable the creation of a\nmanifests dir.\n\nChange-Id: Id6cba7536441d11a98d860181dc55879535a5188\n""}]",0,358865,48ebb42061b4d0d573245ab7205da48d4805c9f5,17,2,4,10035,,,0,"Allow disabling manifests

It's totally reasonable for a user to disable the creation of a
manifests dir.

Change-Id: Id6cba7536441d11a98d860181dc55879535a5188
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/65/358865/4 && git format-patch -1 --stdout FETCH_HEAD,"['elements/manifests/extra-data.d/20-manifest-dir', 'elements/manifests/README.rst', 'elements/manifests/environment.d/14-manifests', 'elements/manifests/cleanup.d/01-copy-manifests-dir']",4,9d86435e2a9267ba6ab55f64468599df48c1cc20,356582,"if [ ""$DIB_MANIFEST_DISABLED"" != ""0"" -a -d $TMP_MOUNT_PATH/${DIB_MANIFEST_IMAGE_DIR} ]; then",if [ -d $TMP_MOUNT_PATH/${DIB_MANIFEST_IMAGE_DIR} ]; then,8,2
openstack%2Ftacker~master~I64625113357493253cb950206b8f4fdbbee16f5b,openstack/tacker,master,I64625113357493253cb950206b8f4fdbbee16f5b,Refactor heat stack name to use VNF name,NEW,2016-06-17 04:26:43.000000000,2017-12-18 04:23:54.000000000,,"[{'_account_id': 2874}, {'_account_id': 10487}, {'_account_id': 13485}, {'_account_id': 16034}, {'_account_id': 18955}, {'_account_id': 20066}]","[{'number': 1, 'created': '2016-06-17 04:26:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/c511486f4b657c0bccd4225dcdb7dc33a84fcb5a', 'message': 'Refactor heat stack name to use device name\n\nThis fix removes the class path, file name and uuid based heat stack\nnames and replaces it with just the vnf names.\n\nCloses-Bug: #1590215\nPartially implements: blueprint infra-driver-refactor\n\nChange-Id: I64625113357493253cb950206b8f4fdbbee16f5b\n'}, {'number': 2, 'created': '2016-06-17 04:34:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/8d651016b3af8caab140b36ba4fa92ef22f3d2be', 'message': 'Refactor heat stack name to use device name\n\nThis fix removes the class path, file name and uuid based heat stack\nnames and replaces it with just the vnf names.\n\nCloses-Bug: #1590215\nPartially implements: blueprint infra-driver-refactor\n\nChange-Id: I64625113357493253cb950206b8f4fdbbee16f5b\n'}, {'number': 3, 'created': '2016-06-20 23:58:35.000000000', 'files': ['tacker/tests/unit/vm/test_plugin.py', 'tacker/tests/functional/vnfm/test_vnf_monitoring.py', 'tacker/vm/plugin.py', 'tacker/tests/unit/vm/infra_drivers/heat/test_heat.py', 'tacker/vm/infra_drivers/heat/heat.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/ebe38b32fd457398ac9e5cdea2666938b3ddf833', 'message': 'Refactor heat stack name to use VNF name\n\nThis fix removes the class path, file name and uuid based heat stack\nnames and replaces it with just the VNF names. Also restricts VNF\nnames to contain only alphnumeric and ""_.-"" characters\n\nCloses-Bug: #1590215\nPartially implements: blueprint infra-driver-refactor\n\nChange-Id: I64625113357493253cb950206b8f4fdbbee16f5b\n'}]",9,330882,ebe38b32fd457398ac9e5cdea2666938b3ddf833,18,6,3,13485,,,0,"Refactor heat stack name to use VNF name

This fix removes the class path, file name and uuid based heat stack
names and replaces it with just the VNF names. Also restricts VNF
names to contain only alphnumeric and ""_.-"" characters

Closes-Bug: #1590215
Partially implements: blueprint infra-driver-refactor

Change-Id: I64625113357493253cb950206b8f4fdbbee16f5b
",git fetch https://review.opendev.org/openstack/tacker refs/changes/82/330882/3 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/tests/unit/vm/infra_drivers/heat/test_heat.py', 'tacker/vm/infra_drivers/heat/heat.py']",2,c511486f4b657c0bccd4225dcdb7dc33a84fcb5a,bug/1590215, name = device['name'], if 'key_name' in vdu_dict: properties['key_name'] = vdu_dict['key_name'] name = (__name__ + '_' + self.__class__.__name__ + '-' + device['id']),7,19
openstack%2Fswift~master~I4aa4ab331a5d0345660e1fe79b66ec8294087928,openstack/swift,master,I4aa4ab331a5d0345660e1fe79b66ec8294087928,Fix the errors in swift/common/ring/builder.py and swift/cli/ringbuilder.py,NEW,2016-04-02 03:20:22.000000000,2017-12-18 04:23:35.000000000,,"[{'_account_id': 6968}, {'_account_id': 10068}, {'_account_id': 13052}, {'_account_id': 18675}, {'_account_id': 20508}, {'_account_id': 20683}, {'_account_id': 21246}]","[{'number': 1, 'created': '2016-04-02 03:20:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/39860a701ade127cc240ff851d632b721df84361', 'message': 'fixed released for Bug #1558754\n\nChange-Id: I4aa4ab331a5d0345660e1fe79b66ec8294087928\n'}, {'number': 2, 'created': '2016-04-04 14:54:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f6e5f3a866f4498484063c001556d16840892b19', 'message': 'Fix the errors in swift/common/ring/builder.py and swift/cli/ringbuilder.py\n\nCloses-Bug: 1558754\nChange-Id: I4aa4ab331a5d0345660e1fe79b66ec8294087928\n'}, {'number': 3, 'created': '2016-04-07 01:30:54.000000000', 'files': ['swift/common/ring/builder.py', 'swift/cli/ringbuilder.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/8f6ba68e99da4f8c4797e6eae6f1055569440372', 'message': 'Fix the errors in swift/common/ring/builder.py and swift/cli/ringbuilder.py\n\nCloses-Bug: 1558754\nChange-Id: I4aa4ab331a5d0345660e1fe79b66ec8294087928\n'}]",3,300710,8f6ba68e99da4f8c4797e6eae6f1055569440372,20,7,3,21246,,,0,"Fix the errors in swift/common/ring/builder.py and swift/cli/ringbuilder.py

Closes-Bug: 1558754
Change-Id: I4aa4ab331a5d0345660e1fe79b66ec8294087928
",git fetch https://review.opendev.org/openstack/swift refs/changes/10/300710/2 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/ring/builder.py', 'swift/cli/ringbuilder.py']",2,39860a701ade127cc240ff851d632b721df84361,bug/1558754," flags = ""DEL"" builder.set_remove_flag(flags) if builder.min_part_seconds_left > 0 and builder.remove_flags and not options.force:", if builder.min_part_seconds_left > 0 and not options.force:,8,1
openstack%2Fmasakari~master~I962e64d9ef8df7929bca80c573381e07cb78809f,openstack/masakari,master,I962e64d9ef8df7929bca80c573381e07cb78809f,Fixes some current typo errors on masakari project.,NEW,2017-01-25 07:21:33.000000000,2017-12-18 04:23:33.000000000,,[],"[{'number': 1, 'created': '2017-01-25 07:21:33.000000000', 'files': ['masakari/api/validation/parameter_types.py', 'masakari/ha/api.py', 'masakari/api/api_version_request.py', 'masakari/api/openstack/wsgi.py', 'masakari/hacking/checks.py'], 'web_link': 'https://opendev.org/openstack/masakari/commit/fce1ee0f7e66a85b02d2d4d6c226f90602b6c9b5', 'message': 'Fixes some current typo errors on masakari project.\n\nChange-Id: I962e64d9ef8df7929bca80c573381e07cb78809f\n'}]",0,425016,fce1ee0f7e66a85b02d2d4d6c226f90602b6c9b5,4,0,1,24456,,,0,"Fixes some current typo errors on masakari project.

Change-Id: I962e64d9ef8df7929bca80c573381e07cb78809f
",git fetch https://review.opendev.org/openstack/masakari refs/changes/16/425016/1 && git format-patch -1 --stdout FETCH_HEAD,"['masakari/api/validation/parameter_types.py', 'masakari/ha/api.py', 'masakari/api/api_version_request.py', 'masakari/api/openstack/wsgi.py', 'masakari/hacking/checks.py']",5,fce1ee0f7e66a85b02d2d4d6c226f90602b6c9b5,typo-bug-masakari-20172501,"# with [, "", '. Otherwise, checking of string","# with [, "", '. Otherwise checking of string",13,13
openstack%2Fstorlets~master~I950cf33d8820572815e2ab91638a22369afedb79,openstack/storlets,master,I950cf33d8820572815e2ab91638a22369afedb79,WIP: Add gzip stream example for python strolet app,NEW,2016-12-09 11:01:31.000000000,2017-12-18 04:23:23.000000000,,[{'_account_id': 4608}],"[{'number': 1, 'created': '2016-12-09 11:01:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/0ab529534dcb5d473fa9a483de03859327c7f76c', 'message': 'WIP: Add gzip stream example for python strolet app\n\nTODO:\nadd functional tests\n\nChange-Id: I950cf33d8820572815e2ab91638a22369afedb79\n'}, {'number': 2, 'created': '2016-12-09 12:09:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/510e29a2eb05dfb11acc7c0063d3ebff8910c0bc', 'message': 'WIP: Add gzip stream example for python strolet app\n\nTODO:\nadd functional tests\n\nChange-Id: I950cf33d8820572815e2ab91638a22369afedb79\n'}, {'number': 3, 'created': '2016-12-09 14:15:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/4bbc8198b5e86e398ee79cb67807f2d29dbfaf9a', 'message': 'WIP: Add gzip stream example for python strolet app\n\nTODO:\nadd functional tests\n\nChange-Id: I950cf33d8820572815e2ab91638a22369afedb79\n'}, {'number': 4, 'created': '2017-01-04 02:56:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/943c3d71b052e3a6cff882a471fcabe8ba484da8', 'message': 'WIP: Add gzip stream example for python strolet app\n\nTODO:\nadd functional tests\n\nChange-Id: I950cf33d8820572815e2ab91638a22369afedb79\n'}, {'number': 5, 'created': '2017-01-04 04:35:45.000000000', 'files': ['tests/unit/StorletSamples/test_gzip.py', 'StorletSamples/python/storlet_samples/gzip/gzip.py', 'StorletSamples/python/storlet_samples/gzip/__init__.py', 'tests/unit/StorletSamples/test_simple.py', 'tests/unit/StorletSamples/__init__.py'], 'web_link': 'https://opendev.org/openstack/storlets/commit/1fda799d30010872f3188b3d21c6faae7d1c1a08', 'message': 'WIP: Add gzip stream example for python strolet app\n\nTODO:\nadd functional tests\n\nChange-Id: I950cf33d8820572815e2ab91638a22369afedb79\n'}]",0,409081,1fda799d30010872f3188b3d21c6faae7d1c1a08,15,1,5,4608,,,0,"WIP: Add gzip stream example for python strolet app

TODO:
add functional tests

Change-Id: I950cf33d8820572815e2ab91638a22369afedb79
",git fetch https://review.opendev.org/openstack/storlets refs/changes/81/409081/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/StorletSamples/test_gzip.py', 'StorletSamples/python/storlet_samples/gzip/gzip.py', 'StorletSamples/python/storlet_samples/gzip/__init__.py', 'tests/unit/StorletSamples/test_simple.py', 'tests/unit/StorletSamples/__init__.py']",5,0ab529534dcb5d473fa9a483de03859327c7f76c,gzip,"# Copyright (c) 2015, 2016 OpenStack Foundation. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. class FakeStorletFile(object): def __init__(self): self._call_closed = False def close(self): self._call_closed = True @property def closed(self): return self._call_closed class FakeStorletFileIn(FakeStorletFile): def __init__(self, input_string, metadata): super(FakeStorletFileIn, self).__init__() self._input_string = input_string self._metadata = metadata self._pos = 0 def read(self, size=None): if size is None: ret_val = self._input_string[self._pos:] self._pos = len(self._input_string) else: ret_val = self._input_string[self._pos:self._pos + size] self._pos = self._pos + size return ret_val def get_metadata(self): return self._metadata class FakeStorletFileOut(FakeStorletFile): def __init__(self): super(FakeStorletFileOut, self).__init__() self._output_string = [] self._metadata = {} def write(self, data): self._output_string.append(data) def set_metadata(self, metadata): self._metadata.update(metadata) def read(self): return ''.join(self._output_string) ",,223,48
openstack%2Fpython-ironicclient~master~I3f344c45ec80eb00e1f93842b4a90f0a9cb0e030,openstack/python-ironicclient,master,I3f344c45ec80eb00e1f93842b4a90f0a9cb0e030,Fix OSC port create and port set commands,NEW,2016-10-10 08:22:45.000000000,2017-12-18 04:23:15.000000000,,"[{'_account_id': 10239}, {'_account_id': 11655}, {'_account_id': 14614}, {'_account_id': 17270}, {'_account_id': 20675}, {'_account_id': 22724}]","[{'number': 1, 'created': '2016-10-10 08:22:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/691f18ea103ba570fabf74fd0c927109209c7972', 'message': 'Fix OSC port create and port set commands\n\nChanging the optional parameters to positional\nones as it should be for command\n`openstack baremetal port create`.\n\nAdded mutually exclusive group to parser for\ncommand `openstack baremetal port set`,\nso that no way to specify only port and this\nshould be shown in help message.\n\nChange-Id: I3f344c45ec80eb00e1f93842b4a90f0a9cb0e030\nCloses-Bug: 1631326\n'}, {'number': 2, 'created': '2016-10-10 09:24:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/8516f319dcf69124d0b40474c3f54d0850ab55c4', 'message': 'Fix OSC port create and port set commands\n\nChanging the optional parameters to positional\nones as it should be for command\n`openstack baremetal port create`.\n\nAdded mutually exclusive group to parser for\ncommand `openstack baremetal port set`,\nso that no way to specify only port and this\nshould be shown in help message.\n\nImportant note: Did not touch the command\nopenstack baremetal node create,\nbecause for now for backward compatibility\n`--driver` should not be changed to positional\nargument (see\nironicclient/osc/v1/baremetal_port.py#L42-L44)\n\nChange-Id: I3f344c45ec80eb00e1f93842b4a90f0a9cb0e030\nCloses-Bug: 1631326\n'}, {'number': 3, 'created': '2017-01-04 13:11:00.000000000', 'files': ['ironicclient/osc/v1/baremetal_port.py', 'ironicclient/tests/unit/osc/v1/test_baremetal_port.py'], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/9037e75e7d643237a6357d2016197112b4d10d20', 'message': 'Fix OSC port create and port set commands\n\nChanging the optional parameters to positional\nones as it should be for command\n`openstack baremetal port create`.\n\nAdded mutually exclusive group to parser for\ncommand `openstack baremetal port set`,\nso that no way to specify only port and this\nshould be shown in help message.\n\nImportant note: Did not touch the command\nopenstack baremetal node create,\nbecause for now for backward compatibility\n`--driver` should not be changed to positional\nargument (see\nironicclient/osc/v1/baremetal_port.py#L42-L44)\n\nChange-Id: I3f344c45ec80eb00e1f93842b4a90f0a9cb0e030\nCloses-Bug: 1631326\n'}]",6,384345,9037e75e7d643237a6357d2016197112b4d10d20,18,6,3,22724,,,0,"Fix OSC port create and port set commands

Changing the optional parameters to positional
ones as it should be for command
`openstack baremetal port create`.

Added mutually exclusive group to parser for
command `openstack baremetal port set`,
so that no way to specify only port and this
should be shown in help message.

Important note: Did not touch the command
openstack baremetal node create,
because for now for backward compatibility
`--driver` should not be changed to positional
argument (see
ironicclient/osc/v1/baremetal_port.py#L42-L44)

Change-Id: I3f344c45ec80eb00e1f93842b4a90f0a9cb0e030
Closes-Bug: 1631326
",git fetch https://review.opendev.org/openstack/python-ironicclient refs/changes/45/384345/3 && git format-patch -1 --stdout FETCH_HEAD,"['ironicclient/osc/v1/baremetal_port.py', 'ironicclient/tests/unit/osc/v1/test_baremetal_port.py']",2,691f18ea103ba570fabf74fd0c927109209c7972,bug/1631326," baremetal_fakes.baremetal_uuid, baremetal_fakes.baremetal_uuid, def test_baremetal_port_set_only_port_specified(self): arglist = [baremetal_fakes.baremetal_port_uuid] verifylist = [('port', baremetal_fakes.baremetal_port_uuid)] self.assertRaises(osctestutils.ParserException, self.check_parser, self.cmd, arglist, verifylist) def test_baremetal_port_set_many(self): arglist = [ baremetal_fakes.baremetal_port_uuid, '--address', baremetal_fakes.baremetal_port_address, '--extra', 'foo=bar' ] verifylist = [ ('port', baremetal_fakes.baremetal_port_uuid), ('address', baremetal_fakes.baremetal_port_address), ('extra', ['foo=bar']) ] self.assertRaises(osctestutils.ParserException, self.check_parser, self.cmd, arglist, verifylist)"," '--node', baremetal_fakes.baremetal_uuid, '--node', baremetal_fakes.baremetal_uuid,",30,9
openstack%2Fironic~master~Iebe8f86f93044c0354832518b39db84a683485e4,openstack/ironic,master,Iebe8f86f93044c0354832518b39db84a683485e4,Add a possibility to send sensors data in standalone mode,NEW,2016-12-14 13:38:26.000000000,2017-12-18 04:23:05.000000000,,"[{'_account_id': 6618}, {'_account_id': 7711}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10342}, {'_account_id': 12356}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 19593}, {'_account_id': 22724}]","[{'number': 1, 'created': '2016-12-14 13:38:26.000000000', 'files': ['ironic/conductor/manager.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/tests/unit/conductor/mgr_utils.py', 'releasenotes/notes/sensors-standalone-mode-eef6ac63ad5dc207.yaml'], 'web_link': 'https://opendev.org/openstack/ironic/commit/f1a87b480fc2b27c0717c90470230ac05b75d945', 'message': 'Add a possibility to send sensors data in standalone mode\n\nCurrently send sensors data ironic feature can be used only with\nnova, because periodical task does not select nodes without instance\nuuid. This patch change filter to all nodes with ACTIVE state.\n\nCloses-Bug: 1649884\nChange-Id: Iebe8f86f93044c0354832518b39db84a683485e4\n'}]",5,410760,f1a87b480fc2b27c0717c90470230ac05b75d945,25,11,1,7711,,,0,"Add a possibility to send sensors data in standalone mode

Currently send sensors data ironic feature can be used only with
nova, because periodical task does not select nodes without instance
uuid. This patch change filter to all nodes with ACTIVE state.

Closes-Bug: 1649884
Change-Id: Iebe8f86f93044c0354832518b39db84a683485e4
",git fetch https://review.opendev.org/openstack/ironic refs/changes/60/410760/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/conductor/manager.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/tests/unit/conductor/mgr_utils.py', 'releasenotes/notes/sensors-standalone-mode-eef6ac63ad5dc207.yaml']",4,f1a87b480fc2b27c0717c90470230ac05b75d945,bug/1649884,"--- fixes: - Add ability to send sensors data in standalone ironic mode. This fix changes behavior of periodical task, only nodes in ACTIVE state will be processed. ",,17,8
openstack%2Frally~master~I620162bce02e870d2c478020d885ca9e15b3bede,openstack/rally,master,I620162bce02e870d2c478020d885ca9e15b3bede,[RaaS][DO-NOT-MERGE] RaaS demonstration,NEW,2016-07-01 16:08:35.000000000,2017-12-18 04:22:48.000000000,,"[{'_account_id': 12395}, {'_account_id': 13995}, {'_account_id': 14817}, {'_account_id': 18903}, {'_account_id': 22960}]","[{'number': 1, 'created': '2016-07-01 16:08:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1c5cb99e9605c98d1dac57666f05f210bba9038d', 'message': '[RaaS][DO-NOT-MERGE] Introduce RaaS demo\n\nThis demonstrates how Rally-as-a-Service can be implemented.\n\nHow to try this:\n\n1. Play with rally/aas/demo_cli.py without --location argument\n   to get data from local Rally Service\n\n2. Launch rally/aas/demo_server.py in another terminal\n   and try rally/aas/demo_cli.py with --location 127.0.0.1:8000\n\n3. For using Rally as a Lib, simply run python and try:\n\n    >>> from rally import api\n    >>> api.setup()\n    >>> api.Task.list()  # or another method\n\n4. For using Rally Service as a Lib:\n\n4.1. Local Service\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> svc.Task.list()  # or another method\n\n4.2. Remote Service (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> svc.Task.list()  # or another method\n\nImplements: rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}, {'number': 2, 'created': '2016-07-01 16:15:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c496c50501ae32fc81f319925e38aa6271216f82', 'message': '[RaaS][DO-NOT-MERGE] Introduce RaaS demo\n\nThis demonstrates how Rally-as-a-Service can be implemented.\n\nHow to try this:\n\n1. Play with rally/aas/demo_cli.py without --location argument\n   to get data from local Rally Service\n\n2. Launch rally/aas/demo_server.py in another terminal\n   and try rally/aas/demo_cli.py with --location 127.0.0.1:8000\n\n3. For using Rally as a Lib, simply run python and try:\n\n    >>> from rally import api\n    >>> api.setup()\n    >>> api.Task.list()  # or another method\n\n4. For using Rally Service as a Lib:\n\n4.1. Local Service\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> svc.Task.list()  # or another method\n\n4.2. Remote Service (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> svc.Task.list()  # or another method\n\nImplements: bp/rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}, {'number': 3, 'created': '2016-07-05 14:00:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/f1a7c129113dadb021c3b8f86a34eb9773172aca', 'message': '[RaaS][DO-NOT-MERGE] Introduce RaaS demo\n\nThis demonstrates how Rally-as-a-Service can be implemented.\n\nHow to try this:\n\n1. Play with rally/aas/demo_cli.py without --location argument\n   to get data from local Rally Service\n\n2. Launch rally/aas/demo_server.py in another terminal\n   and try rally/aas/demo_cli.py with --location 127.0.0.1:8000\n\n3.1. Local Service\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> svc.Task._list()  # or another method\n\n3.2. Remote Service (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> svc.Task._list()  # or another method\n\n4. Finally, now it is simpler to use API (static|class)methods\n   as-a-lib. Try this out:\n\n    >>> from rally import api\n    >>> api.setup()  # set up DB connection\n    >>> api.Task.list()  # or another method\n\nImplements: #rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}, {'number': 4, 'created': '2016-07-07 14:42:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/3c08194200a1f52be0af4d4e261ee33c9be230d5', 'message': '[RaaS][DO-NOT-MERGE] Introduce RaaS demo\n\nThis demonstrates how Rally-as-a-Service can be implemented.\n\nKey ideas:\n\n  * New approach proposes using API instancemethods,\n    in opposite to current static API classmethods and\n    staticmethods\n\n  * New class rally.api.API is a base class for all APIs.\n    A set of APIs instancemethods is introduced.\n    Existing static APIs remain working and can be gradually\n    replaced\n\n  * New class rally.aas.service.Service provides single endpoint\n    for all Rally API\n\n  * The main benefit of using Service and APIs instances is\n    having ability to work with unlimited number of remote Rally\n    services as well as with local Rally, within single process.\n    This approach also gives flexibility for future\n    improvements\n\nHow to try this:\n\n  * Local Service via CLI\n\n    Play with rally/aas/demo_cli.py without --location argument\n    to get data from local Rally Service\n\n  * Remote Service via CLI\n\n    Launch rally/aas/demo_server.py in another terminal\n    and try rally/aas/demo_cli.py with additional argument\n    --location http://127.0.0.1:8000\n\n  * Local Service as a Lib\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n  * Remote Service as a Lib (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n  * Using Rally API explicitly\n\n   - Legacy classmethods and staticmethods\n     (deprecated, local Rally only):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api.Task.list()\n     >>> deployments = api.Deployment.list()\n\n   - New instancemethods, for local Rally:\n\n     >>> from rally import api\n     >>> tasks = api.Task()._list()\n     >>> deployments = api.Deployment()._list()\n\n   - New instancemethods, for remote Rally:\n\n     >>> from rally import api\n     >>> location = ""http://127.0.0.1:8000""\n     >>> tasks = api.Task(location)._list()\n     >>> deployments = api.Deployment(location)._list()\n\nImplements: blueprint rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}, {'number': 5, 'created': '2016-07-07 15:03:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e608e78b24262696fca4ecf76bd865800ca41ab2', 'message': '[RaaS][DO-NOT-MERGE] Introduce RaaS demo\n\nThis demonstrates how Rally-as-a-Service can be implemented.\n\nKey ideas:\n\n  * New approach proposes using API instancemethods,\n    in opposite to current static API classmethods and\n    staticmethods\n\n  * New class rally.api.API is a base class for all APIs.\n    A set of APIs instancemethods is introduced.\n    Existing static APIs remain working and can be gradually\n    replaced\n\n  * New class rally.aas.service.Service provides single endpoint\n    for all Rally API\n\n  * The main benefit of using Service and APIs instances is\n    having ability to work with unlimited number of remote Rally\n    services as well as with local Rally, within single process.\n    This approach also gives flexibility for future\n    improvements\n\nHow to try this:\n\n 1. Local Service via CLI\n\n    Play with rally/aas/demo_cli.py without --location argument\n    to get data from local Rally Service\n\n 2. Remote Service via CLI\n\n    Launch rally/aas/demo_server.py in another terminal\n    and try rally/aas/demo_cli.py with additional argument\n    --location http://127.0.0.1:8000\n\n 3. Local Service as a Lib\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n 4. Remote Service as a Lib (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n 5. Using Rally API explicitly\n\n   * Legacy classmethods and staticmethods\n     (deprecated, local Rally only):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api.Task.list()\n     >>> deployments = api.Deployment.list()\n\n   * New instancemethods, for local Rally:\n\n     >>> from rally import api\n     >>> tasks = api.Task()._list()\n     >>> deployments = api.Deployment()._list()\n\n   * New instancemethods, for remote Rally\n     (demo server must be running):\n\n     >>> from rally import api\n     >>> location = ""http://127.0.0.1:8000""\n     >>> tasks = api.Task(location)._list()\n     >>> deployments = api.Deployment(location)._list()\n\n  6. Finally, lets make a fun and use Rally Service\n     as proxy to another Rally service:\n\n   run demo server (port 8000 by default):\n\n        $ ./rally/aas/demo_server\n\n   then run another demo server on port 8001 and\n   location pointed to first service:\n\n        $ ./rally/aas/demo_server 8001 http://127.0.0.1:8000\n\n   now we can query service http://127.0.0.1:8000 from\n   ./rally/aas/demo_cli:\n\n     direct connect:\n         <command args> --location http://127.0.0.1:8000\n\n     via proxy service:\n         <command args> --location http://127.0.0.1:8001\n\nImplements: blueprint rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}, {'number': 6, 'created': '2016-07-07 15:18:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/af2bb43325c0d204d7ca6652e34a00296a2f735c', 'message': '[RaaS][DO-NOT-MERGE] Introduce RaaS demo\n\nThis demonstrates how Rally-as-a-Service can be implemented.\n\nKey ideas:\n\n  * New approach proposes using API instancemethods,\n    in opposite to current static API classmethods and\n    staticmethods\n\n  * New class rally.api.API is a base class for all APIs.\n    A set of APIs instancemethods is introduced.\n    Existing static APIs remain working and can be gradually\n    replaced\n\n  * New class rally.aas.service.Service provides single endpoint\n    for all Rally API\n\n  * The main benefit of using Service and APIs instances is\n    having ability to work with unlimited number of remote Rally\n    services as well as with local Rally, within single process.\n    This approach also gives flexibility for future\n    improvements\n\nHow to try this:\n\n 1. Local Service via CLI\n\n    Play with rally/aas/demo_cli.py without --location argument\n    to get data from local Rally Service\n\n 2. Remote Service via CLI\n\n    Launch rally/aas/demo_server.py in another terminal\n    and try rally/aas/demo_cli.py with additional argument\n    --location http://127.0.0.1:8000\n\n 3. Local Service as a Lib\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n 4. Remote Service as a Lib (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n 5. Using Rally API explicitly\n\n   * Legacy classmethods and staticmethods\n     (deprecated, local Rally only):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api.Task.list()\n     >>> deployments = api.Deployment.list()\n\n   * New instancemethods, for local Rally:\n\n     >>> from rally import api\n     >>> tasks = api.Task()._list()\n     >>> deployments = api.Deployment()._list()\n\n   * New instancemethods, for remote Rally\n     (demo server must be running):\n\n     >>> from rally import api\n     >>> location = ""http://127.0.0.1:8000""\n     >>> tasks = api.Task(location)._list()\n     >>> deployments = api.Deployment(location)._list()\n\n  6. Finally, lets make a fun and use Rally Service\n     as proxy to another Rally Service:\n\n   run demo server (port 8000 by default):\n\n        $ ./rally/aas/demo_server\n\n   then run another demo server on port 8001 and\n   location pointed to first service:\n\n        $ ./rally/aas/demo_server 8001 http://127.0.0.1:8000\n\n   now we can query service http://127.0.0.1:8000 from\n   ./rally/aas/demo_cli:\n\n     direct connect:\n         <command args> --location http://127.0.0.1:8000\n\n     via proxy service:\n         <command args> --location http://127.0.0.1:8001\n\nImplements: blueprint rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}, {'number': 7, 'created': '2016-07-11 10:45:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/163959c2acbcc23eea20b57e9634874972a8599c', 'message': '[RaaS][DO-NOT-MERGE] RaaS demonstration\n\nThis shows how Rally-as-a-Service can be implemented.\n\nKey ideas:\n\n  * New approach proposes using API instancemethods,\n    in opposite to current static API classmethods and\n    staticmethods\n\n  * Move oslo_config.cfg.CONF initialization\n    from rally.cli.cliutils to rally.api.API.init_config()\n\n  * New class rally.api.API is a base class for all APIs.\n    A set of APIs instancemethods is introduced.\n    Existing static APIs remain working and can be gradually\n    replaced\n\n  * New class rally.aas.service.Service provides single endpoint\n    for all Rally API\n\n  * The main benefit of using Service and APIs instances is\n    having ability to work with unlimited number of remote Rally\n    services as well as with local Rally, within single process.\n    This approach also gives flexibility for future\n    improvements\n\nHow to try this:\n\n 1. Local Service via CLI\n\n    Play with rally/aas/demo_cli.py without --location argument\n    to get data from local Rally Service\n\n 2. Remote Service via CLI\n\n    Launch rally/aas/demo_server.py in another terminal\n    and try rally/aas/demo_cli.py with additional argument\n    --location http://127.0.0.1:8000\n\n 3. Local Service as a Lib\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n 4. Remote Service as a Lib (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n 5. Using Rally API explicitly\n\n   * Legacy classmethods and staticmethods\n     (deprecated, local Rally only):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api.Task.list()\n     >>> deployments = api.Deployment.list()\n\n   * With API instancemethods, for local Rally:\n\n     >>> from rally import api\n     >>> tasks = api.Task()._list()\n     >>> deployments = api.Deployment()._list()\n\n   * With API instancemethods, for remote Rally\n     (demo server must be running):\n\n     >>> from rally import api\n     >>> location = ""http://127.0.0.1:8000""\n     >>> tasks = api.Task(location)._list()\n     >>> deployments = api.Deployment(location)._list()\n\n  6. Finally, lets make a fun and use Rally Service\n     as proxy to another Rally Service:\n\n   run demo server (port 8000 by default):\n\n        $ ./rally/aas/demo_server\n\n   then run another demo server on port 8001 and\n   location pointed to first service:\n\n        $ ./rally/aas/demo_server 8001 http://127.0.0.1:8000\n\n   now we can query service http://127.0.0.1:8000 from\n   ./rally/aas/demo_cli:\n\n     direct connect:\n         <command args> --location http://127.0.0.1:8000\n\n     via proxy service:\n         <command args> --location http://127.0.0.1:8001\n\nImplements: blueprint rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}, {'number': 8, 'created': '2016-07-11 12:18:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d94c17731b08efd13fdc9cbe70024b103057aff2', 'message': '[RaaS][DO-NOT-MERGE] RaaS demonstration\n\nThis shows how Rally-as-a-Service can be implemented.\n\nKey ideas:\n\n  * New approach proposes using API instancemethods,\n    in opposite to current static API classmethods and\n    staticmethods\n\n  * Move oslo_config.cfg.CONF initialization\n    from rally.cli.cliutils to rally.api.API.init_config()\n\n  * New class rally.api.API is a base class for all APIs.\n    A set of APIs instancemethods is introduced.\n    Existing static APIs remain working and can be gradually\n    replaced\n\n  * New class rally.aas.service.Service provides single endpoint\n    for all Rally API\n\n  * The main benefit of using Service and APIs instances is\n    having ability to work with unlimited number of remote Rally\n    services as well as with local Rally, within single process.\n    This approach also gives flexibility for future\n    improvements\n\nHow to try this:\n\n 1. Local Service via CLI\n\n    Play with rally/aas/demo_cli without --location argument\n    to get data from local Rally Service\n\n 2. Remote Service via CLI\n\n    Launch rally/aas/demo_server in another terminal\n    and try rally/aas/demo_cli with additional argument\n    --location http://127.0.0.1:8000\n\n 3. Local Service as a Lib\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n 4. Remote Service as a Lib (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n 5. Using Rally API explicitly\n\n   * Legacy classmethods and staticmethods\n     (deprecated, local Rally only):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api.Task.list()\n     >>> deployments = api.Deployment.list()\n\n   * With API instancemethods, for local Rally:\n\n     >>> from rally import api\n     >>> tasks = api.Task()._list()\n     >>> deployments = api.Deployment()._list()\n\n   * With API instancemethods, for remote Rally\n     (demo server must be running):\n\n     >>> from rally import api\n     >>> location = ""http://127.0.0.1:8000""\n     >>> tasks = api.Task(location)._list()\n     >>> deployments = api.Deployment(location)._list()\n\n  6. Finally, lets make a fun and use Rally Service\n     as proxy to another Rally Service:\n\n   run demo server (port 8000 by default):\n\n        $ ./rally/aas/demo_server\n\n   then run another demo server on port 8001 and\n   location pointed to first service:\n\n        $ ./rally/aas/demo_server 8001 http://127.0.0.1:8000\n\n   now we can query service http://127.0.0.1:8000 from\n   ./rally/aas/demo_cli:\n\n     direct connect:\n         <command args> --location http://127.0.0.1:8000\n\n     via proxy service:\n         <command args> --location http://127.0.0.1:8001\n\nImplements: blueprint rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}, {'number': 9, 'created': '2016-09-08 16:27:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/fec3d3f97a6ebe693c723cc06e67694e701fddac', 'message': '[RaaS][DO-NOT-MERGE] RaaS demonstration\n\nThis shows how Rally-as-a-Service can be implemented.\n\nKey ideas:\n\n  * New approach proposes using API instancemethods,\n    in opposite to current static API classmethods and\n    staticmethods\n\n  * Move oslo_config.cfg.CONF initialization\n    from rally.cli.cliutils to rally.api.API.init_config()\n\n  * New class rally.api.API is a base class for all APIs.\n    A set of APIs instancemethods is introduced.\n    Existing static APIs remain working and can be gradually\n    replaced\n\n  * New class rally.aas.service.Service provides single endpoint\n    for all Rally API\n\n  * The main benefit of using Service and APIs instances is\n    having ability to work with unlimited number of remote Rally\n    services as well as with local Rally, within single process.\n    This approach also gives flexibility for future\n    improvements\n\nHow to try this:\n\n 1. Local Service via CLI\n\n    Play with rally/aas/demo_cli without --location argument\n    to get data from local Rally Service\n\n 2. Remote Service via CLI\n\n    Launch rally/aas/demo_server in another terminal\n    and try rally/aas/demo_cli with additional argument\n    --location http://127.0.0.1:8000\n\n 3. Local Service as a Lib\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n 4. Remote Service as a Lib (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n 5. Using Rally API explicitly\n\n   * Legacy classmethods and staticmethods\n     (deprecated, local Rally only):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api.Task.list()\n     >>> deployments = api.Deployment.list()\n\n   * With API instancemethods, for local Rally:\n\n     >>> from rally import api\n     >>> tasks = api.Task()._list()\n     >>> deployments = api.Deployment()._list()\n\n   * With API instancemethods, for remote Rally\n     (demo server must be running):\n\n     >>> from rally import api\n     >>> location = ""http://127.0.0.1:8000""\n     >>> tasks = api.Task(location)._list()\n     >>> deployments = api.Deployment(location)._list()\n\n  6. Finally, lets make a fun and use Rally Service\n     as proxy to another Rally Service:\n\n   run demo server (port 8000 by default):\n\n        $ ./rally/aas/demo_server\n\n   then run another demo server on port 8001 and\n   location pointed to first service:\n\n        $ ./rally/aas/demo_server 8001 http://127.0.0.1:8000\n\n   now we can query service http://127.0.0.1:8000 from\n   ./rally/aas/demo_cli:\n\n     direct connect:\n         <command args> --location http://127.0.0.1:8000\n\n     via proxy service:\n         <command args> --location http://127.0.0.1:8001\n\nImplements: blueprint rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}, {'number': 10, 'created': '2016-09-08 16:29:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6b963f3fa64d5033228545474cb63b37eb9ec567', 'message': '[RaaS][DO-NOT-MERGE] RaaS demonstration\n\nThis shows how Rally-as-a-Service can be implemented.\n\nKey ideas:\n\n  * New approach proposes using API instancemethods,\n    in opposite to current static API classmethods and\n    staticmethods\n\n  * Move oslo_config.cfg.CONF initialization\n    from rally.cli.cliutils to rally.api.API.init_config()\n\n  * New class rally.api.API is a base class for all APIs.\n    A set of APIs instancemethods is introduced.\n    Existing static APIs remain working and can be gradually\n    replaced\n\n  * New class rally.aas.service.Service provides single endpoint\n    for all Rally API\n\n  * The main benefit of using Service and APIs instances is\n    having ability to work with unlimited number of remote Rally\n    services as well as with local Rally, within single process.\n    This approach also gives flexibility for future\n    improvements\n\nHow to try this:\n\n 1. Local Service via CLI\n\n    Play with rally/aas/demo_cli without --location argument\n    to get data from local Rally Service\n\n 2. Remote Service via CLI\n\n    Launch rally/aas/demo_server in another terminal\n    and try rally/aas/demo_cli with additional argument\n    --location http://127.0.0.1:8000\n\n 3. Local Service as a Lib\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n 4. Remote Service as a Lib (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> tasks = svc.Task._list()\n    >>> deployments = svc.Deployment._list()\n\n 5. Using Rally API explicitly\n\n   * Legacy classmethods and staticmethods\n     (deprecated, local Rally only):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api.Task.list()\n     >>> deployments = api.Deployment.list()\n\n   * With API instancemethods, for local Rally:\n\n     >>> from rally import api\n     >>> tasks = api.Task()._list()\n     >>> deployments = api.Deployment()._list()\n\n   * With API instancemethods, for remote Rally\n     (demo server must be running):\n\n     >>> from rally import api\n     >>> location = ""http://127.0.0.1:8000""\n     >>> tasks = api.Task(location)._list()\n     >>> deployments = api.Deployment(location)._list()\n\n  6. Finally, lets make a fun and use Rally Service\n     as proxy to another Rally Service:\n\n   run demo server (port 8000 by default):\n\n        $ ./rally/aas/demo_server\n\n   then run another demo server on port 8001 and\n   location pointed to first service:\n\n        $ ./rally/aas/demo_server 8001 http://127.0.0.1:8000\n\n   now we can query service http://127.0.0.1:8000 from\n   ./rally/aas/demo_cli:\n\n     direct connect:\n         <command args> --location http://127.0.0.1:8000\n\n     via proxy service:\n         <command args> --location http://127.0.0.1:8001\n\nImplements: blueprint rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}, {'number': 11, 'created': '2016-09-08 16:40:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/3a52c2c479b6d4a86dbf610f45edad22922b8a51', 'message': '[RaaS][DO-NOT-MERGE] RaaS demonstration\n\nThis shows how Rally-as-a-Service can be implemented.\n\nKey ideas:\n\n  * Move oslo_config.cfg.CONF initialization\n    from rally.cli.cliutils to rally.api.API.init_config()\n\n  * New class rally.api.API is a base class for all APIs\n\n  * New approach proposes using API instance methods, in\n    opposite to static methods. Existing API classes (Task,\n    Deployment, Verification) are now used as instances\n\n  * New class rally.aas.service.Service provides single\n    endpoint for all Rally API.\n    The main benefit of using Service and APIs instances is\n    having ability to work with unlimited number of remote Rally\n    services as well as with local Rally, within single process.\n    This approach also gives flexibility for future\n    improvements\n\nHow to try this:\n\n 1. Local Service via CLI\n\n    Play with rally/aas/demo_cli without --location argument\n    to get data from local Rally Service\n\n 2. Remote Service via CLI\n\n    Launch rally/aas/demo_server in another terminal\n    and try rally/aas/demo_cli with additional argument\n    --location http://127.0.0.1:8000\n\n 3. Local Service as a Lib\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> tasks = svc.Task.list()\n    >>> deployments = svc.Deployment.list()\n\n 4. Remote Service as a Lib (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> tasks = svc.Task.list()\n    >>> deployments = svc.Deployment.list()\n\n 5. Using Rally API explicitly\n\n   * Legacy usage (does not support location):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api.Task.list()\n     >>> deployments = api.Deployment.list()\n\n   * With API instancemethods, for local Rally:\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api._Task().list()\n     >>> deployments = api._Deployment().list()\n\n   * With API instancemethods, for remote Rally\n     (demo server must be running):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> location = ""http://127.0.0.1:8000""\n     >>> tasks = api.Task(location).list()\n     >>> deployments = api.Deployment(location).list()\n\n  6. Finally, lets make a fun and use Rally Service\n     as proxy to another Rally Service:\n\n   run demo server (port 8000 by default):\n\n        $ ./rally/aas/demo_server\n\n   then run another demo server on port 8001 and\n   location pointed to first service:\n\n        $ ./rally/aas/demo_server 8001 http://127.0.0.1:8000\n\n   now we can query service http://127.0.0.1:8000 from\n   ./rally/aas/demo_cli:\n\n     direct connect:\n         <command args> --location http://127.0.0.1:8000\n\n     via proxy service:\n         <command args> --location http://127.0.0.1:8001\n\nImplements: blueprint rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}, {'number': 12, 'created': '2016-09-09 13:12:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/545b19951372ee0f40aa6247dac87d944a7e77cb', 'message': '[RaaS][DO-NOT-MERGE] RaaS demonstration\n\nThis shows how Rally-as-a-Service can be implemented.\n\nKey ideas:\n\n  * Move oslo_config.cfg.CONF initialization\n    from rally.cli.cliutils to rally.api.API.init_config()\n\n  * New class rally.api.API is a base class for all APIs\n\n  * New approach proposes using API instance methods, in\n    opposite to static methods. Existing API classes (Task,\n    Deployment, Verification) are now used as instances\n\n  * New class rally.aas.service.Service provides single\n    endpoint for all Rally API.\n    The main benefit of using Service and APIs instances is\n    having ability to work with unlimited number of remote Rally\n    services as well as with local Rally, within single process.\n    This approach also gives flexibility for future\n    improvements\n\nHow to try this:\n\n 1. Local Service via CLI\n\n    Play with rally/aas/demo_cli without --location argument\n    to get data from local Rally Service\n\n 2. Remote Service via CLI\n\n    Launch rally/aas/demo_server in another terminal\n    and try rally/aas/demo_cli with additional argument\n    --location http://127.0.0.1:8000\n\n 3. Local Service as a Lib\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> tasks = svc.Task.list()\n    >>> deployments = svc.Deployment.list()\n\n 4. Remote Service as a Lib (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> tasks = svc.Task.list()\n    >>> deployments = svc.Deployment.list()\n\n 5. Using Rally API explicitly\n\n   * Legacy usage (does not support location):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api.Task.list()\n     >>> deployments = api.Deployment.list()\n\n   * With API instancemethods, for local Rally:\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api._Task().list()\n     >>> deployments = api._Deployment().list()\n\n   * With API instancemethods, for remote Rally\n     (demo server must be running):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> location = ""http://127.0.0.1:8000""\n     >>> tasks = api.Task(location).list()\n     >>> deployments = api.Deployment(location).list()\n\n  6. Finally, lets make a fun and use Rally Service\n     as proxy to another Rally Service:\n\n   run demo server (port 8000 by default) and another demo server\n   on port 8001 and location pointed to first service:\n\n        $ ./rally/aas/demo_server\n        $ ./rally/aas/demo_server 8001 http://127.0.0.1:8000\n\n   now we can query service http://127.0.0.1:8000 via proxy service:\n\n        $ ./rally/aas/demo_cli <command args> --location http://127.0.0.1:8001\n\nImplements: blueprint rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}, {'number': 13, 'created': '2017-02-04 09:46:59.000000000', 'files': ['rally/cli/cliutils.py', 'rally/api.py', 'tests/unit/test_api.py', 'rally/aas/demo_server', 'rally/aas/service.py', 'rally/aas/__init__.py', 'rally/aas/demo_cli'], 'web_link': 'https://opendev.org/openstack/rally/commit/e6aea724a938344d9cb190abdcfd171c6c667c0b', 'message': '[RaaS][DO-NOT-MERGE] RaaS demonstration\n\nThis shows how Rally-as-a-Service can be implemented.\n\nKey ideas:\n\n  * Move oslo_config.cfg.CONF initialization\n    from rally.cli.cliutils to rally.api.API.init_config()\n\n  * New class rally.api.API is a base class for all APIs\n\n  * New approach proposes using API instance methods, in\n    opposite to static methods. Existing API classes (Task,\n    Deployment, Verification) are now used as instances\n\n  * New class rally.aas.service.Service provides single\n    endpoint for all Rally API.\n    The main benefit of using Service and APIs instances is\n    having ability to work with unlimited number of remote Rally\n    services as well as with local Rally, within single process.\n    This approach also gives flexibility for future\n    improvements\n\nHow to try this:\n\n 1. Local Service via CLI\n\n    Play with rally/aas/demo_cli without --location argument\n    to get data from local Rally Service\n\n 2. Remote Service via CLI\n\n    Launch rally/aas/demo_server in another terminal\n    and try rally/aas/demo_cli with additional argument\n    --location http://127.0.0.1:8000\n\n 3. Local Service as a Lib\n\n    >>> from rally.aas import service\n    >>> svc = service.Service()\n    >>> tasks = svc.Task.list()\n    >>> deployments = svc.Deployment.list()\n\n 4. Remote Service as a Lib (demo server must be running)\n\n    >>> from rally.aas import service\n    >>> svc = service.Service(""http://127.0.0.1:8000"")\n    >>> tasks = svc.Task.list()\n    >>> deployments = svc.Deployment.list()\n\n 5. Using Rally API explicitly\n\n   * Legacy usage (does not support location):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api.Task.list()\n     >>> deployments = api.Deployment.list()\n\n   * With API instancemethods, for local Rally:\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> tasks = api._Task().list()\n     >>> deployments = api._Deployment().list()\n\n   * With API instancemethods, for remote Rally\n     (demo server must be running):\n\n     >>> from rally import api\n     >>> api.API.init_config()  # set up DB connection, once\n     >>> location = ""http://127.0.0.1:8000""\n     >>> tasks = api.Task(location).list()\n     >>> deployments = api.Deployment(location).list()\n\n  6. Finally, lets make a fun and use Rally Service\n     as proxy to another Rally Service:\n\n   run demo server (port 8000 by default) and another demo server\n   on port 8001 and location pointed to first service:\n\n        $ ./rally/aas/demo_server\n        $ ./rally/aas/demo_server 8001 http://127.0.0.1:8000\n\n   now we can query service http://127.0.0.1:8000 via proxy service:\n\n        $ ./rally/aas/demo_cli <command args> --location http://127.0.0.1:8001\n\nImplements: blueprint rally-as-a-service\nChange-Id: I620162bce02e870d2c478020d885ca9e15b3bede\n'}]",11,336636,e6aea724a938344d9cb190abdcfd171c6c667c0b,37,5,13,10475,,,0,"[RaaS][DO-NOT-MERGE] RaaS demonstration

This shows how Rally-as-a-Service can be implemented.

Key ideas:

  * Move oslo_config.cfg.CONF initialization
    from rally.cli.cliutils to rally.api.API.init_config()

  * New class rally.api.API is a base class for all APIs

  * New approach proposes using API instance methods, in
    opposite to static methods. Existing API classes (Task,
    Deployment, Verification) are now used as instances

  * New class rally.aas.service.Service provides single
    endpoint for all Rally API.
    The main benefit of using Service and APIs instances is
    having ability to work with unlimited number of remote Rally
    services as well as with local Rally, within single process.
    This approach also gives flexibility for future
    improvements

How to try this:

 1. Local Service via CLI

    Play with rally/aas/demo_cli without --location argument
    to get data from local Rally Service

 2. Remote Service via CLI

    Launch rally/aas/demo_server in another terminal
    and try rally/aas/demo_cli with additional argument
    --location http://127.0.0.1:8000

 3. Local Service as a Lib

    >>> from rally.aas import service
    >>> svc = service.Service()
    >>> tasks = svc.Task.list()
    >>> deployments = svc.Deployment.list()

 4. Remote Service as a Lib (demo server must be running)

    >>> from rally.aas import service
    >>> svc = service.Service(""http://127.0.0.1:8000"")
    >>> tasks = svc.Task.list()
    >>> deployments = svc.Deployment.list()

 5. Using Rally API explicitly

   * Legacy usage (does not support location):

     >>> from rally import api
     >>> api.API.init_config()  # set up DB connection, once
     >>> tasks = api.Task.list()
     >>> deployments = api.Deployment.list()

   * With API instancemethods, for local Rally:

     >>> from rally import api
     >>> api.API.init_config()  # set up DB connection, once
     >>> tasks = api._Task().list()
     >>> deployments = api._Deployment().list()

   * With API instancemethods, for remote Rally
     (demo server must be running):

     >>> from rally import api
     >>> api.API.init_config()  # set up DB connection, once
     >>> location = ""http://127.0.0.1:8000""
     >>> tasks = api.Task(location).list()
     >>> deployments = api.Deployment(location).list()

  6. Finally, lets make a fun and use Rally Service
     as proxy to another Rally Service:

   run demo server (port 8000 by default) and another demo server
   on port 8001 and location pointed to first service:

        $ ./rally/aas/demo_server
        $ ./rally/aas/demo_server 8001 http://127.0.0.1:8000

   now we can query service http://127.0.0.1:8000 via proxy service:

        $ ./rally/aas/demo_cli <command args> --location http://127.0.0.1:8001

Implements: blueprint rally-as-a-service
Change-Id: I620162bce02e870d2c478020d885ca9e15b3bede
",git fetch https://review.opendev.org/openstack/rally refs/changes/36/336636/5 && git format-patch -1 --stdout FETCH_HEAD,"['rally/cli/cliutils.py', 'rally/api.py', 'rally/aas/demo_cli.py', 'rally/aas/service.py', 'rally/aas/__init__.py', 'rally/aas/demo_server.py']",6,1c5cb99e9605c98d1dac57666f05f210bba9038d,bp/rally-as-a-service,"#!/usr/bin/env python2 # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """""" Simple RaaS server, just for demonstration. In real use this should be replaced with more powerful server. """""" import urlparse import simplejson import SocketServer from SimpleHTTPServer import SimpleHTTPRequestHandler from rally.aas import service svc = service.Service() class SimpleHTTPHandler(object): def __init__(self, request, client_address, server): req = request.makefile(""rb"", -1).readline(65535).split() if req: method, uri = req[0], urlparse.urlparse(req[1]) print ""%s:%d"" % client_address, method, req[1] action = uri.path.rstrip(""/"") or ""/"" args = {k:v.pop() for k, v in urlparse.parse_qs(uri.query).items()} body = None # Not is use in this demo response = svc.dispatch(method, action, args, body) request.makefile(""wb"", 0).write(response) if __name__ == ""__main__"": port = 8000 SocketServer.TCPServer.allow_reuse_address = True # Good for demo server = SocketServer.TCPServer(("""", port), SimpleHTTPHandler) print ""Simple RaaS HTTP server is started on port %d"" % port try: server.serve_forever() except KeyboardInterrupt: pass ",,289,17
openstack%2Foctavia~master~If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6,openstack/octavia,master,If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6,Active-Active Topology - added disable arp,NEW,2017-01-25 13:38:36.000000000,2017-12-18 04:22:36.000000000,,"[{'_account_id': 16923}, {'_account_id': 21138}, {'_account_id': 21920}]","[{'number': 1, 'created': '2017-01-25 13:38:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/afdd3a11a1053d41f3aa502ab0cb96d9013df8de', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 2, 'created': '2017-01-25 15:55:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/05c855ccd3b793eca20693ff15b25b85fa884328', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 3, 'created': '2017-01-25 16:12:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/302ea6b628c11709efe3827c4c9f463c914b496e', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 4, 'created': '2017-01-26 08:56:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/64c13f2721d12f44f9425322ba1977702b83edcf', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 5, 'created': '2017-01-26 13:43:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cff5ebf88e8252758d86a1031fc0f28808e5ede1', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 6, 'created': '2017-01-26 14:56:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c2e0c294a6eb2af767c32ef6eef0a265e8437b87', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 7, 'created': '2017-01-29 10:42:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e264a30477893a7ef26895bc80e293bb9249ecc9', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 8, 'created': '2017-01-29 15:26:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3509957abd2fbaf336405afbcb6561e9bf6fb3e2', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 9, 'created': '2017-01-29 17:04:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bbb3b30df546d66e116c62c53b3bc7242b0dd680', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 10, 'created': '2017-01-30 09:01:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e273187b921d32ddbd84cdf73c59db31c577500e', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 11, 'created': '2017-01-30 09:19:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e205e8000e28ff3945a259fbe4d8722496c2d352', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 12, 'created': '2017-01-30 12:44:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/aad72044aeaf38ba9e9c8e1e909c25424b7f16b9', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 13, 'created': '2017-01-30 15:13:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b55e10af849903d9748df52463ff4fc8277595a2', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 14, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3bbc293ad119e50eaa54c88b9a888df84114dafe', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}, {'number': 15, 'created': '2017-02-06 09:50:41.000000000', 'files': ['octavia/amphorae/backends/agent/api_server/server.py', 'octavia/amphorae/backends/agent/api_server/arp.py', 'octavia/controller/worker/tasks/amphora_driver_tasks.py', 'elements/amphora-agent/source-repository-amphora-agent', 'octavia/amphorae/drivers/haproxy/rest_api_driver.py', 'octavia/amphorae/drivers/driver_base.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/59051425b5c2865796c98828d9fd6b47563548f7', 'message': 'Active-Active Topology - added disable arp\n\nChange-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6\n'}]",0,425175,59051425b5c2865796c98828d9fd6b47563548f7,42,3,15,24261,,,0,"Active-Active Topology - added disable arp

Change-Id: If04219c9cf2f7c4c71eca72b5a3ac1f53ca85ab6
",git fetch https://review.opendev.org/openstack/octavia refs/changes/75/425175/14 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/amphorae/backends/agent/api_server/server.py', 'octavia/distributor/drivers/ovs_driver/rest_api_driver.py', 'octavia/amphorae/backends/agent/api_server/arp.py', 'octavia/amphorae/cluster_manager/drivers/active_active/active_active_driver.py', 'octavia/controller/worker/tasks/amphora_driver_tasks.py', 'octavia/amphorae/drivers/haproxy/rest_api_driver.py', 'octavia/amphorae/drivers/driver_base.py']",7,afdd3a11a1053d41f3aa502ab0cb96d9013df8de,fix_distributor_agent," def post_disable_arp(self, amphora, amphora_mac): """"""Called after amphora added to network :param amphora: amphora object, needs id and mac :type amphora: object :param port: contains information of the plugged port :type port: octavia.network.data_models.Port """""" pass def post_enable_arp(self, amphora, amphora_mac): """"""Called after amphora added to network :param amphora: amphora object, needs id and mac :type amphora: object :param port: contains information of the plugged port :type port: octavia.network.data_models.Port """""" pass ",,195,25
openstack%2Fdiskimage-builder~master~Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91,openstack/diskimage-builder,master,Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91,Move hook generation in to python,NEW,2016-01-22 04:59:10.000000000,2017-12-18 04:22:19.000000000,,"[{'_account_id': 7118}, {'_account_id': 10035}, {'_account_id': 11105}, {'_account_id': 12459}, {'_account_id': 21741}]","[{'number': 1, 'created': '2016-01-22 04:59:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/dada9b81389d34e62f0a2f8258ed3d62c1047e4c', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 2, 'created': '2016-01-22 05:10:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/41e80a37cd57b6c31f76c6f5885aa150b0d08cc2', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 3, 'created': '2016-01-29 07:39:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/f497c764fff25d60e603b6b96fc90e30b2b87280', 'message': 'WIP: Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 4, 'created': '2016-01-29 07:54:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/7f5ec56c205b74cbaaff06f62aaa1011cbb2f12f', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 5, 'created': '2016-01-29 07:55:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/45a48402deff454f32a3d125110c240bb9aea8cf', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 6, 'created': '2016-02-16 07:06:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/2bf69712fcb8150b6b32aa17015e4ef840d527c9', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 7, 'created': '2016-02-16 07:10:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/13fb58db15db82d00e9caceb2daa30b3d418707e', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 8, 'created': '2016-02-16 07:43:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/40604f6f8ff2de1d821684e1052bd11559911ba7', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 9, 'created': '2016-04-03 06:13:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/bcab9d529399ccf166672a5306c2a72321c5dd28', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 10, 'created': '2016-04-03 15:51:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/48e62d916817d116ffa8a2d8e176c37779f4db9f', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 11, 'created': '2016-04-24 15:20:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/5f16efba7195aac60c68f3fe98c4c8b18cb5957e', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 12, 'created': '2016-04-24 15:21:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/5f28f7d7d58ff9a62efc62707d7d9cc073580d44', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 13, 'created': '2016-06-15 19:32:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/b738c6c93ed8cf3fdaa2a85acc7e55846055aedb', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 14, 'created': '2016-06-16 04:07:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/b5cae7a15d393e74d22070242d01cb8dcb8d3679', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 15, 'created': '2016-06-16 04:14:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/d053803057361f6d301e8dab228c0f6a2ec64139', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 16, 'created': '2016-06-28 16:23:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/3ad713fdf71326f9fe135763de6630cb94f4a83a', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 17, 'created': '2016-06-28 16:26:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/4ef72212dcdbb2f70f99d4cebe4abc81be3f2c7b', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 18, 'created': '2016-07-12 22:56:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/4e0ae10b2bf2119ad2583cd52f7a90305764bb19', 'message': 'WIP: Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 19, 'created': '2016-10-08 04:17:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/8259155bcfd1fad55d3c8fc3dd2b42fae02a4204', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 20, 'created': '2016-10-08 04:29:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/f9e10eb68128f52f0239efedca3bc43d71959ede', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}, {'number': 21, 'created': '2016-10-08 16:29:31.000000000', 'files': ['diskimage_builder/tests/fixtures/elements/simple-os/install.d/90-simple-os-install', 'diskimage_builder/tests/test_generate_hooks.py', 'diskimage_builder/generate_hooks.py', 'diskimage_builder/tests/fixtures/elements/simple-os/element-provides', 'lib/common-functions', 'setup.cfg', 'diskimage_builder/tests/fixtures/elements/simple-os/root.d/10-simple-os-root'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/202650ed5fb1f6eb6ec357b4dfadfc06a715840b', 'message': 'Move hook generation in to python\n\nIn order to support remote elements (from a uri) hook generation needs\nmore logic. Lets move this code over to python first so we can perform\nthe complex logic there.\n\nChange-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91\n'}]",23,271139,202650ed5fb1f6eb6ec357b4dfadfc06a715840b,94,5,21,10035,,,0,"Move hook generation in to python

In order to support remote elements (from a uri) hook generation needs
more logic. Lets move this code over to python first so we can perform
the complex logic there.

Change-Id: Iab3cf9ad2efb05b40d7a0f4859a52023d2e9be91
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/39/271139/16 && git format-patch -1 --stdout FETCH_HEAD,"['diskimage_builder/tests/fixtures/elements/simple-os/install.d/90-simple-os-install', 'diskimage_builder/tests/test_generate_hooks.py', 'diskimage_builder/generate_hooks.py', 'diskimage_builder/tests/fixtures/elements/simple-os/element-provides', 'lib/common-functions', 'setup.cfg', 'diskimage_builder/tests/fixtures/elements/simple-os/root.d/10-simple-os-root', 'bin/generate-hooks']",8,dada9b81389d34e62f0a2f8258ed3d62c1047e4c,feature/remote-elements,#!/usr/bin/env python import sys from diskimage_builder.generate_hooks import main sys.exit(main(sys.argv)) ,,125,30
openstack%2Frally~master~Ifa28f9cc72a02c0df23ff6e32bec201697e53cb2,openstack/rally,master,Ifa28f9cc72a02c0df23ff6e32bec201697e53cb2,Add CinderVolumes.create_unmanage_and_manage_snapshot,NEW,2017-01-17 06:47:43.000000000,2017-12-18 04:22:16.000000000,,"[{'_account_id': 14817}, {'_account_id': 22960}, {'_account_id': 23435}]","[{'number': 1, 'created': '2017-01-17 06:47:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d2e76b222e799b214e883225c0d76852c866fdf4', 'message': 'Add CinderVolumes.create_unmanage_and_manage_snapshot\n\nThis scenario first creates a volume, then creates a snapshot for it,\nthen unmanages and manages snapshot, finally deletes the snapshot\nand volume.\n\nChange-Id: Ifa28f9cc72a02c0df23ff6e32bec201697e53cb2\n'}, {'number': 2, 'created': '2017-02-06 03:27:34.000000000', 'files': ['tests/unit/plugins/openstack/scenarios/cinder/test_volumes.py', 'samples/tasks/scenarios/cinder/create-unmanage-and-manage-snapshot.json', 'rally/plugins/openstack/scenarios/cinder/utils.py', 'rally-jobs/cinder.yaml', 'tests/unit/plugins/openstack/scenarios/cinder/test_utils.py', 'rally/plugins/openstack/scenarios/cinder/volumes.py', 'samples/tasks/scenarios/cinder/create-unmanage-and-manage-snapshot.yaml'], 'web_link': 'https://opendev.org/openstack/rally/commit/c67326eaf4dc5da3e13803008f2be98512659377', 'message': 'Add CinderVolumes.create_unmanage_and_manage_snapshot\n\nThis scenario first creates a volume, then creates a snapshot for it,\nthen unmanages and manages snapshot, finally deletes the snapshot\nand volume.\n\nChange-Id: Ifa28f9cc72a02c0df23ff6e32bec201697e53cb2\n'}]",1,421093,c67326eaf4dc5da3e13803008f2be98512659377,11,3,2,18404,,,0,"Add CinderVolumes.create_unmanage_and_manage_snapshot

This scenario first creates a volume, then creates a snapshot for it,
then unmanages and manages snapshot, finally deletes the snapshot
and volume.

Change-Id: Ifa28f9cc72a02c0df23ff6e32bec201697e53cb2
",git fetch https://review.opendev.org/openstack/rally refs/changes/93/421093/2 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/plugins/openstack/scenarios/cinder/test_volumes.py', 'samples/tasks/scenarios/cinder/create-unmanage-and-manage-snapshot.json', 'rally/plugins/openstack/scenarios/cinder/utils.py', 'rally-jobs/cinder.yaml', 'tests/unit/plugins/openstack/scenarios/cinder/test_utils.py', 'rally/plugins/openstack/scenarios/cinder/volumes.py', 'samples/tasks/scenarios/cinder/create-unmanage-and-manage-snapshot.yaml']",7,d2e76b222e799b214e883225c0d76852c866fdf4,cinder.import_export_snapshot," CinderVolumes.create_unmanage_and_manage_snapshot: - args: size: 1 runner: type: ""constant"" times: 3 concurrency: 2 context: users: tenants: 2 users_per_tenant: 2 sla: failure_rate: max: 0 ",,192,0
openstack%2Frally~master~Ib46965035072fc6871b3bd1750c085361f1519cd,openstack/rally,master,Ib46965035072fc6871b3bd1750c085361f1519cd,New scenario to create Neutron and Nova resources for a tenant,NEW,2016-05-09 12:05:36.000000000,2017-12-18 04:22:14.000000000,,"[{'_account_id': 6172}, {'_account_id': 7369}, {'_account_id': 9545}, {'_account_id': 10061}, {'_account_id': 10068}, {'_account_id': 10475}, {'_account_id': 12395}, {'_account_id': 14168}, {'_account_id': 14817}, {'_account_id': 20535}, {'_account_id': 21528}]","[{'number': 1, 'created': '2016-05-09 12:05:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/7f7753d7b662ca23cc41c4a70b67a8fea371eba9', 'message': 'New scenario to create Neutron and Nova resources for a tenant.\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 2, 'created': '2016-05-09 13:17:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/80afaa67de7d3ae612f7acde4a643bd11e255b3a', 'message': 'New scenario to create Neutron and Nova resources for a tenant\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 3, 'created': '2016-05-16 07:06:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/7e3102173d95b1880f7069fb88e1d699eb8ff935', 'message': 'added ut cases and addressed review comments\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 4, 'created': '2016-05-17 06:11:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a3e56d62fdcc611f56b4e49ec8af6b2cca2d120c', 'message': 'improved coverage and updated scenario in rally-neutron.yaml\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 5, 'created': '2016-05-17 11:04:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/534e8d5aa9b529a133337e77a06317c700b1be8b', 'message': 'improved coverage\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 6, 'created': '2016-05-18 10:34:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/678a107dd3eb175b57da957ab29a8ef8b2e59bb5', 'message': 'more ut coverage to clear cover-gate\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 7, 'created': '2016-05-25 08:53:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4af92589a2a9f632d5f1877a2ff8afd58f1ea7b4', 'message': 'rally-ci recheck\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 8, 'created': '2016-05-25 12:25:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e90bccd299762e8867eabafff79ee6e7e18f6378', 'message': 'rally-ci recheck\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 9, 'created': '2016-05-27 07:27:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0c1797b1837e0844e4d47f99379e867af02b3978', 'message': 'New scenario to create Neutron and Nova resources for a tenant\n\nFollowing changes committed.\n - mock for generate_cidr\n - enhanced couple of cases\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 10, 'created': '2016-05-27 11:55:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6df0b3d04975d26b224d2fb7d052b7bff2d417fd', 'message': 'New scenario to create Neutron and Nova resources for a tenant\n\nFollowing changes are committed.\n - incorporated code-review comment\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 11, 'created': '2016-05-30 07:12:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/9ab3cd3cc2b7a89f99f66683a5d5f847b64de6d3', 'message': 'New scenario to create Neutron and Nova resources for a tenant\n\nretriggering ci job\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 12, 'created': '2016-07-05 09:18:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1cd21c0cda1092ac1109bf5d9fff8c22abd36e22', 'message': 'New scenario to create Neutron and Nova resources for a tenant\n\nAddressed the code review comments with following changes:\n - Removed schema validation and added scenario specific validator\n - Renamed scenario function to ""create_and_delete_resources""\n - Merged Network, Subnet and Router creation under one function\n - Detailed description of the input parameters\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 13, 'created': '2016-07-11 10:39:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/564f06c00752307796302712102784fa92c2d2e3', 'message': 'New scenario to create Neutron and Nova resources for a tenant\n\nrebased patch-set 12\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 14, 'created': '2016-07-18 09:54:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/58886ac3a1d17d510f2977b1d98ed69498cf34ed', 'message': 'New scenario to create Neutron and Nova resources for a tenant\n\nFollowing changes have been made:\n - rebased code base.\n - rally-neutron.yaml updated with latest code.\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 15, 'created': '2016-07-21 06:06:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/12127f7e1ea6b795653efb43289987e8e9e66c1c', 'message': 'New scenario to create Neutron and Nova resources for a tenant\n\nAddressed the code review comments with following changes:\n - removed the unnecessary logging statements.\n - refactored code to remove duplicate codes in VM creation functions.\n - changed the class name from TenantScenario to ProjectScenario.\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}, {'number': 16, 'created': '2016-08-09 10:36:30.000000000', 'files': ['samples/tasks/scenarios/tenant_centric/create-neutron-nova-resources.json', 'tests/unit/plugins/openstack/scenarios/tenant_centric/test_utils.py', 'rally/plugins/openstack/scenarios/tenant_centric/utils.py', 'tests/unit/plugins/openstack/scenarios/tenant_centric/test_neutron_nova_resources.py', 'rally-jobs/rally-neutron.yaml', 'rally/plugins/openstack/scenarios/tenant_centric/__init__.py', 'tests/unit/plugins/openstack/scenarios/tenant_centric/__init__.py', 'samples/tasks/scenarios/tenant_centric/create-neutron-nova-resources.yaml', 'rally/plugins/openstack/scenarios/tenant_centric/neutron_nova_resources.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/a950e2ee25cb8ae66a1976fdc079ef503d3368e0', 'message': 'New scenario to create Neutron and Nova resources for a tenant\n\nAddressed the code review comments with following changes:\n - Merged boot_vms_in_sequence and boot_vms_in_oneshot into as single method.\n\nChange-Id: Ib46965035072fc6871b3bd1750c085361f1519cd\nImplements: blueprint neutron-nova-resource-creation\n'}]",79,314070,a950e2ee25cb8ae66a1976fdc079ef503d3368e0,82,11,16,20535,,,0,"New scenario to create Neutron and Nova resources for a tenant

Addressed the code review comments with following changes:
 - Merged boot_vms_in_sequence and boot_vms_in_oneshot into as single method.

Change-Id: Ib46965035072fc6871b3bd1750c085361f1519cd
Implements: blueprint neutron-nova-resource-creation
",git fetch https://review.opendev.org/openstack/rally refs/changes/70/314070/16 && git format-patch -1 --stdout FETCH_HEAD,"['samples/tasks/scenarios/tenant_centric/create-neutron-nova-resources.json', 'rally/plugins/openstack/scenarios/tenant_centric/utils.py', 'rally/plugins/openstack/scenarios/tenant_centric/__init__.py', 'samples/tasks/scenarios/tenant_centric/create-neutron-nova-resources.yaml', 'rally/plugins/openstack/scenarios/tenant_centric/neutron_nova_resources.py']",5,7f7753d7b662ca23cc41c4a70b67a8fea371eba9,bp/neutron-nova-resource-creation,"# Copyright 2016: IBM Corp. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import copy import time import jsonschema from oslo_config import cfg from rally.common import logging from rally import consts from rally import exceptions from rally.plugins.openstack import scenario from rally.plugins.openstack.scenarios.tenant_centric import utils from rally.plugins.openstack.wrappers import network as network_wrapper from rally.task import atomic from rally.task import types from rally.task import utils as task_utils from rally.task import validation LOG = logging.getLogger(__name__) CONF = cfg.CONF class NeutronNovaResources(utils.TenantScenario): """"""Benchmark scenario for Neutron and Nova resources."""""" @types.set(image=types.ImageResourceType, flavor=types.FlavorResourceType) @validation.image_valid_on_flavor(""flavor"", ""image"") @validation.required_services(consts.Service.NEUTRON, consts.Service.NOVA) @validation.required_openstack(users=True) @scenario.configure(context={""cleanup"": [""nova"", ""neutron""]}) def resource_creation(self, resource_config, image, flavor, **kwargs): """"""Create Neutron and Nova resources. This scenario creates configurable number of Neutron resources ( networks, subnets, routers, floatingips etc) and boots VMs over all these Neutron networks. It measures creation time of all these newly created resources. The scenario is also capable of assigning floatingips and provider network IP for VMs if configured. Finally, the scenario measures the time it take to ping VM IPs and ssh login to VMs using respective IPs. :param resource_config: dict, contains resource configuration options :param image: image to be used to boot an instance :param flavor: flavor to be used to boot an instance :param kwargs: Optional additional arguments for server creation """""" LOG.debug(""resource config is %s"", resource_config) self.resource_config = self.process_config_data(resource_config) self.networks = {} self.subnets = {} self.routers = {} self.router_ifaces = {} self.sgs = {} self.fips = {} self.vm_list = [] self.vm_ips = {} self.vm_ports = {} ### # Create Network # self.create_network_resources() ### # Create Subnet # self.create_subnet_resources() self.dhcp_port_status_check() ### # Create Router # self.create_router_resources() ### # Create security group # self.create_sg_resources() ### # Create Servers # self.create_vm_resources(image, flavor, **kwargs) ### # Create floatingIp # self.create_floatingip_resources() ### # Ping test # self.ping_vm_resources() ### # ssh loging test # self.ssh_login_vm_resources() ### # Delete resources # self.delete_all_resources(**kwargs) def create_network_resources(self): """"""Creates network resources. """""" net_count = self.resource_config.get(""networks"").get(""count"") create_args = {""network"": {""name"": """"}} xtras = self.resource_config.get(""networks"").get(""extra_args"", {}) create_args[""network""].update(xtras) # Atomic operation # action_name = ""create_%d_networks"" % (net_count) with atomic.ActionTimer(self, action_name): for ni in range(net_count): create_args[""network""][""name""] = self.generate_random_name() netObj = self.clients(""neutron"").create_network(create_args) self.networks[netObj[""network""][""id""]] = netObj[""network""] LOG.debug(""Network [%s] creation done"", len(self.networks)) def create_subnet_resources(self): """"""Creates subnet resources. """""" sbnt_per_net = self.resource_config.get(""subnets"").get(""count"") cidr_pre = self.resource_config.get(""subnets"").get(""cidr_prefix"") create_args = { ""subnet"": { ""name"": """", ""network_id"": """", ""cidr"": """", ""ip_version"": 4 } } xtras = self.resource_config.get(""subnets"").get(""extra_args"", {}) create_args[""subnet""].update(xtras) # Atomic operation # action_name = ""create_%d_subnets"" % (len(self.networks) * sbnt_per_net) # with atomic.ActionTimer(self, action_name): for netid in self.networks.keys(): create_args[""subnet""][""network_id""] = netid for si in range(sbnt_per_net): cidr = network_wrapper.generate_cidr(start_cidr=cidr_pre) create_args[""subnet""][""name""] = self.generate_random_name() create_args[""subnet""][""cidr""] = cidr subObj = self.clients(""neutron"").create_subnet(create_args) self.subnets[subObj[""subnet""][""id""]] = subObj[""subnet""] # -------------------------------------------------------------------------- # Sync all the network object to get subnet info too. # network_ids = self.networks.keys() for netid in network_ids: netObj = self.clients(""neutron"").show_network(netid) self.networks[netid] = netObj[""network""] LOG.debug(""Subnet [%s] creation done"", len(self.subnets)) def dhcp_port_status_check(self): """"""Verifies the Dhcp port status as ACTIVE. """""" dhcp_config = self.resource_config.get( ""subnets"").get(""dhcp_port_status_check"", {}) test_enabled = dhcp_config.get(""enable"") if test_enabled is False: LOG.info(""Dhcp port status check is disabled"") return port_existence_timeout = dhcp_config.get(""port_existence_timeout"") port_check_interval = dhcp_config.get(""port_existence_check_interval"") status_check_timeout = dhcp_config.get(""status_check_timeout"") status_check_interval = dhcp_config.get(""status_check_interval"") # Atomic operation # action_name = ""dhcp_port_status_check"" # with atomic.ActionTimer(self, action_name): for netid in self.networks.keys(): dhcp_port_id = self.get_dhcp_port_id( netid, timeout=port_existence_timeout, check_interval=port_check_interval) self.check_port_status( dhcp_port_id, status=""ACTIVE"", timeout=status_check_timeout, check_interval=status_check_interval) LOG.debug(""Dhcp port status check is done"") def create_router_resources(self): """"""Create router resources. """""" rtr_count = self.resource_config.get(""routers"").get(""count"") if rtr_count <= 0: LOG.info(""Router creation skipped."") return # ------------------------------------------------------------------------ # Create Routers # create_args = { ""router"": { ""name"": """" } } extras = self.resource_config.get(""routers"").get(""extra_args"", {}) create_args[""router""].update(extras) if self.resource_config.get(""floating_ips"").get(""enable"") is True: extern_netid = self.resource_config.get( ""floating_ips"").get(""external_network_id"") create_args[""router""][""external_gateway_info""] = { ""network_id"": extern_netid } # Atomic operation # action_name = ""create_%d_routers"" % (rtr_count) # with atomic.ActionTimer(self, action_name): for ri in range(rtr_count): create_args[""router""][""name""] = self.generate_random_name() rtrObj = self.clients(""neutron"").create_router(create_args) self.routers[rtrObj[""router""][""id""]] = rtrObj[""router""] # ------------------------------------------------------------------------ # Create Router interfaces # This connects all the networks to the available routers. # All subnets of a network are attached to the same router. # rtr_iface_args = {""subnet_id"": """"} rtr_id_list = self.routers.keys() # Atomic operation # action_name = ""create_%d_router_interfaces"" % (len(self.subnets)) # with atomic.ActionTimer(self, action_name): use_rtr = 0 for netid, netObj in self.networks.items(): rtrid = rtr_id_list[use_rtr] use_rtr = (use_rtr + 1) % rtr_count for sbntid in netObj[""subnets""]: rtr_iface_args[""subnet_id""] = sbntid self.clients(""neutron"").add_interface_router( rtrid, rtr_iface_args) self.router_ifaces.setdefault(rtrid, []).append(sbntid) LOG.debug(""Router [%s] creation done"", len(self.routers)) def create_sg_resources(self): """"""Create security group resources. """""" enabled = self.resource_config.get(""security_groups"").get(""enable"") if not enabled: LOG.info(""Security groups is not enabled."") return groups = self.resource_config.get(""security_groups"").get(""groups"") if len(groups) > 0: LOG.info(""Creating configured security groups"") self.sg_create_groups(groups) return # No user provided security groups. # We create one group with predefined rules. # LOG.info(""Creating one predefined security group"") def_rules = [ { ""ip_protocol"": ""tcp"", ""to_port"": 65535, ""from_port"": 1, ""ip_range"": {""cidr"": ""0.0.0.0/0""} }, { ""ip_protocol"": ""udp"", ""to_port"": 65535, ""from_port"": 1, ""ip_range"": {""cidr"": ""0.0.0.0/0""} }, { ""ip_protocol"": ""icmp"", ""to_port"": -1, ""from_port"": -1, ""ip_range"": {""cidr"": ""0.0.0.0/0""} }, ] predef_group = {""ssh_icmp_allow"": def_rules} self.sg_create_groups(predef_group) def sg_create_groups(self, sg_groups): """"""Create security groups from the given configuration data. """""" sg_count = len(sg_groups) sg_rules = {} # ------------------------------------------------------------------- # Create security groups # # Atomic operation # action_name = ""create_%d_securitygroups"" % (sg_count) with atomic.ActionTimer(self, action_name): for sg_name, rules in sg_groups.items(): sg_desc = ""rally create - "" + sg_name sg = self.clients(""nova"").security_groups.create( sg_name, sg_desc) self.sgs[sg.id] = sg sg_rules[sg.id] = rules # ------------------------------------------------------------------ # Create security group rules # # Atomic operation # action_name = ""create_securitygroup_rules"" with atomic.ActionTimer(self, action_name): for sgid, rules in sg_rules.items(): for rule in rules: self.clients(""nova"").security_group_rules.create( sgid, from_port=rule[""from_port""], to_port=rule[""to_port""], ip_protocol=rule[""ip_protocol""], cidr=rule[""ip_range""][""cidr""]) LOG.debug(""Security Group [%s] creation done"", len(self.sgs)) def create_vm_resources(self, image, flavor, **kwargs): """"""Create VM resources. """""" self.sg_name_list = [sg.id for k, sg in self.sgs.items()] provider_netid = None prov_net_enabled = self.resource_config.get( ""provider_network"").get(""enable"") if prov_net_enabled is True: provider_netid = self.resource_config.get( ""provider_network"").get(""network_id"") boot_pattern = self.resource_config.get( ""instances"").get(""boot_pattern"") if boot_pattern in [""sequential""]: self.boot_vms_in_sequence(image, flavor, provider_netid, **kwargs) elif boot_pattern in [""oneshot""]: self.boot_vms_in_oneshot(image, flavor, provider_netid, **kwargs) else: self.boot_vms_in_parallel(image, flavor, provider_netid, **kwargs) self.learn_vms_public_private_ips() LOG.debug(""VM [%s] creation done"", len(self.vm_list)) def boot_vms_in_parallel(self, image, flavor, provider_netid, **kwargs): """"""Boots all VM resources in parallel. """""" vm_count = self.resource_config.get(""instances"").get(""count"") name_prefix = self.generate_random_name() net_id_list = self.networks.keys() net_count = len(net_id_list) # Atomic operation # action_name = ""boot_%d_vms_in_parallel"" % (vm_count) with atomic.ActionTimer(self, action_name): net_index = 0 for vmi in range(vm_count): vm_nics = [] # private network vm_nics.append({""net-id"": net_id_list[net_index]}) net_index = (net_index + 1) % net_count # provider network if provider_netid: vm_nics.append({""net-id"": provider_netid}) self.clients(""nova"").servers.create( ""%s_%d"" % (name_prefix, vmi), image, flavor, security_groups=self.sg_name_list, nics=vm_nics, **kwargs) # All VMs are booted. Now wait for them become active. # servers = filter( lambda server: server.name.startswith(name_prefix), self.clients(""nova"").servers.list()) time.sleep(CONF.benchmark.nova_server_boot_prepoll_delay) servers = [task_utils.wait_for_status( server, ready_statuses=[""ACTIVE""], update_resource=task_utils.get_from_manager(), timeout=CONF.benchmark.nova_server_boot_timeout, check_interval=CONF.benchmark.nova_server_boot_poll_interval ) for server in servers] self.vm_list += servers LOG.debug(""boot_vms_in_parallel done"") def boot_vms_in_sequence(self, image, flavor, provider_netid, **kwargs): """"""Boots all VM resources in sequence. """""" vm_count = self.resource_config.get(""instances"").get(""count"") net_id_list = self.networks.keys() net_count = len(net_id_list) # Atomic operation # action_name = ""boot_%d_vms_in_sequence"" % (vm_count) with atomic.ActionTimer(self, action_name): net_index = 0 for vmi in range(vm_count): vm_nics = [] # private network vm_nics.append({""net-id"": net_id_list[net_index]}) net_index = (net_index + 1) % net_count # provider network if provider_netid: vm_nics.append({""net-id"": provider_netid}) name_prefix = self.generate_random_name() self.clients(""nova"").servers.create( ""%s_%d"" % (name_prefix, vmi), image, flavor, security_groups=self.sg_name_list, nics=vm_nics, **kwargs) servers = filter( lambda server: server.name.startswith(name_prefix), self.clients(""nova"").servers.list()) time.sleep(CONF.benchmark.nova_server_boot_prepoll_delay) servers = [task_utils.wait_for_status( server, ready_statuses=[""ACTIVE""], update_resource=task_utils.get_from_manager(), timeout=CONF.benchmark.nova_server_boot_timeout, check_interval=CONF.benchmark. nova_server_boot_poll_interval ) for server in servers] self.vm_list += servers LOG.debug(""boot_vms_in_sequence done"") def boot_vms_in_oneshot(self, image, flavor, provider_netid, **kwargs): """"""Boots all VM resources using one single nova API call. """""" vm_count = self.resource_config.get(""instances"").get(""count"") name_prefix = self.generate_random_name() vm_nics = [] # private network private_netid = self.networks.keys()[0] vm_nics.append({""net-id"": private_netid}) # provider network if provider_netid: vm_nics.append({""net-id"": provider_netid}) # Atomic operation # action_name = ""boot_%d_vms_in_oneshot"" % (vm_count) with atomic.ActionTimer(self, action_name): self.clients(""nova"").servers.create( name_prefix, image, flavor, min_count=vm_count, max_count=vm_count, security_groups=self.sg_name_list, nics=vm_nics, **kwargs) servers = filter( lambda server: server.name.startswith(name_prefix), self.clients(""nova"").servers.list()) time.sleep(CONF.benchmark.nova_server_boot_prepoll_delay) servers = [task_utils.wait_for_status( server, ready_statuses=[""ACTIVE""], update_resource=task_utils.get_from_manager(), timeout=CONF.benchmark.nova_server_boot_timeout, check_interval=CONF.benchmark.nova_server_boot_poll_interval ) for server in servers] self.vm_list += servers LOG.debug(""boot_vms_in_oneshot done"") def learn_vms_public_private_ips(self): """"""This function learns private/public IPs of each vms. """""" # This map is going to store different IPs of each vm. # element # key -> vm_id, # value -> {""name"": """",""private"": [],""provider"": """",""floating"": []} # # self.vm_ips[ vm_id ] = {""name"": ""vm1"", # ""private"" :[ ""10.20.30.40"" ], # ""provider"" : ""192,168.100.40"", # ""floating"":[""100.200.30.40""]} private_networks_name = [n[""name""] for n in self.networks.values()] LOG.debug(""private netwroks %s"", private_networks_name) provider_networks_name = [ self.resource_config.get(""provider_network"").get(""name"")] LOG.debug(""provider netwroks %s"", provider_networks_name) for vm in self.vm_list: # private IP # ip_info = { ""name"": vm.name, ""private"": [], ""floating"": [], ""provider"": None } vm_addresses_map = vm.addresses LOG.debug(""VM '%s' addresses are %s"", vm.name, vm_addresses_map) for net_name, addrs_list in vm_addresses_map.items(): if net_name in private_networks_name: for addr_map in addrs_list: if addr_map[""OS-EXT-IPS:type""] in [""fixed""]: ip_info[""private""].append(addr_map.get(""addr"")) elif net_name in provider_networks_name: for addr_map in addrs_list: if addr_map[""OS-EXT-IPS:type""] in [""fixed""]: ip_info[""provider""] = addr_map.get(""addr"") self.vm_ips[vm.id] = ip_info LOG.debug(""VM '%s'. Learnt ips are %s"", vm.name, ip_info) LOG.debug(""VMs public private ip learning done"") def learn_vm_port_ids(self): LOG.debug(""Learning VM's private port ids"") MAX_ATTEMPT = 5 # Object :> [ (port_id, net_id), ] # # self.vm_ports[ vm_id ] = { (port_uuid, network_uuid) } pvt_net_ids = self.networks.keys() for vm in self.vm_list: vm_info = [] attachment_list = [] attempt_count = 0 while len(attachment_list) <= 0 and attempt_count < MAX_ATTEMPT: attachment_list = self.clients( ""nova"").servers.interface_list(vm) attempt_count += 1 LOG.debug(""vm %s attempt_count %d, attachment_list %s"", vm.name, attempt_count, attachment_list) if len(attachment_list) <= 0: continue LOG.info(""VM %s attachemts are %s"", vm.name, attachment_list) for attachment in attachment_list: LOG.debug(""VM attachment.net_id %s pvt_net_ids %s"", attachment.net_id, pvt_net_ids) if attachment.net_id in pvt_net_ids: vm_info.append((attachment.port_id, attachment.net_id)) if len(vm_info) > 0: self.vm_ports[vm.id] = vm_info LOG.info(""all VM interfaces %s"", self.vm_ports) LOG.debug(""VMs private port id learning done"") def create_floatingip_resources(self): enabled = self.resource_config.get(""floating_ips"").get(""enable"") fip_count = self.resource_config.get(""floating_ips"").get(""count"") if enabled is False or fip_count <= 0: LOG.info(""Not creating FloatingIP"") return self.learn_vm_port_ids() ext_netid = self.resource_config.get( ""floating_ips"").get(""external_network_id"") create_args = { ""floatingip"": { ""floating_network_id"": ext_netid, ""port_id"": None, } } # Atomic operation # action_name = ""create_%d_floatingips"" % (fip_count) # count = 0 with atomic.ActionTimer(self, action_name): for vm_id, vm_ports in self.vm_ports.items(): vm_ips = self.vm_ips[vm_id] vm_name = vm_ips.get(""name"", None) if vm_ports is None or len(vm_ports) == 0: LOG.debug(""VM %s has no private ip"", vm_name) continue vm_pvt_port_id, pvt_net_id = vm_ports[0] create_args[""floatingip""][""port_id""] = vm_pvt_port_id fipObj = self.clients(""neutron"").create_floatingip(create_args) self.fips[fipObj[""floatingip""][""id""]] = fipObj[""floatingip""] vm_ips[""floating""].append( fipObj[""floatingip""][""floating_ip_address""]) count += 1 if count >= fip_count: break LOG.debug(""FloatingIp [%s] creation done"", len(self.fips)) def ping_vm_resources(self): ping_test = self.resource_config.get(""instances"").get(""ping_test"") enabled = ping_test.get(""enable"") if enabled is False: LOG.info(""ping test is not enabled..."") return timeout = self.resource_config.get( ""instances"").get(""ping_test"").get(""timeout"") if len(self.fips) > 0: fip_ips = [o[""floating_ip_address""] for o in self.fips.values()] action_name = ""ping_%d_fip_interfaces"" % (len(fip_ips)) self.ping_all_ips(fip_ips, timeout, action_name) prov_ips = [o[""provider""] for o in self.vm_ips.values() if o[""provider""] is not None] if len(prov_ips) > 0: action_name = ""ping_%d_provider_interfaces"" % (len(prov_ips)) self.ping_all_ips(prov_ips, timeout, action_name) LOG.debug(""ping vm test complete"") def ping_all_ips(self, ip_list, timeout, action_name): with atomic.ActionTimer(self, action_name): self.ping_ips_in_roundrobin(ip_list, timeout=timeout) def ssh_login_vm_resources(self): ssh_config = self.resource_config.get(""instances"").get(""ssh_test"") enabled = ssh_config.get(""enable"") if enabled is False: LOG.info(""ssh login test is not enabled..."") return if len(self.fips) > 0: fip_ips = [o[""floating_ip_address""] for o in self.fips.values()] action_name = ""ssh_login_%d_fip_interfaces"" % (len(fip_ips)) self.ssh_login_to_vms(fip_ips, ssh_config, action_name) prov_ips = [o[""provider""] for o in self.vm_ips.values() if o[""provider""] is not None] if len(prov_ips) > 0: action_name = ""ssh_login_%d_provider_interfaces"" % (len(prov_ips)) self.ssh_login_to_vms(prov_ips, ssh_config, action_name) LOG.debug(""ssh login vm test complete"") def ssh_login_to_vms(self, ip_list, config, action_name): # timeout = config.get(""timeout"") user = config.get(""username"") paswd = config.get(""password"") ssh_port = config.get(""ssh_port"", 22) with atomic.ActionTimer(self, action_name): for ip in ip_list: ssh = self.login_to_server(ip, ssh_port, user, paswd) ssh.close() def delete_all_resources(self, **kwargs): cleanup = self.resource_config.get(""benchmark_cleanup"") if cleanup is False: LOG.info(""no resource cleanup from scenario"") return # Delete FloatingIPs # num_fip = len(self.fips) if num_fip > 0: action_name = ""delete_%s_floatingips"" % (num_fip) with atomic.ActionTimer(self, action_name): disassoc_body = {""floatingip"": {}} for fip_id in self.fips.keys(): self.clients(""neutron"").update_floatingip( fip_id, disassoc_body) self.clients(""neutron"").delete_floatingip(fip_id) # Delete VMs # num_vms = len(self.vm_list) if num_vms > 0: action_name = ""delete_%s_vms"" % (num_vms) with atomic.ActionTimer(self, action_name): self.delete_servers(self.vm_list) # Delete SecurityGroups # num_sgs = len(self.sgs) if num_sgs > 0: action_name = ""delete_%s_securitygroups"" % (num_sgs) with atomic.ActionTimer(self, action_name): for sg_id in self.sgs.keys(): self.clients(""nova"").security_groups.delete(sg_id) # Delete Routers # num_rtr = len(self.routers) if num_rtr > 0: action_name = ""delete_%s_routers"" % (num_rtr) with atomic.ActionTimer(self, action_name): del_iface = {""subnet_id"": """"} for rtr_id in self.routers.keys(): # remove gateway # self.clients(""neutron"").remove_gateway_router(rtr_id) # # remove all interfaces # sbntid_list = self.router_ifaces.get(rtr_id, []) for sbnt_id in sbntid_list: del_iface[""subnet_id""] = sbnt_id self.clients(""neutron"").remove_interface_router( rtr_id, del_iface) # Remove the router # self.clients(""neutron"").delete_router(rtr_id) # Delete Subnets # num_sbnts = len(self.subnets) if num_sbnts > 0: action_name = ""delete_%s_subnets"" % (num_sbnts) with atomic.ActionTimer(self, action_name): for sbnt_id in self.subnets.keys(): self.clients(""neutron"").delete_subnet(sbnt_id) # Delete Networks # num_nets = len(self.networks) if num_nets > 0: action_name = ""delete_%s_networks"" % (num_nets) with atomic.ActionTimer(self, action_name): for net_id in self.networks.keys(): self.clients(""neutron"").delete_network(net_id) LOG.debug(""All resources are deleted."") def process_config_data(self, resource_config): """"""This function varifies the resource configuration data with it's schema. Also, it assumes default values for missing parameters. """""" resource_config_schema = { ""$schema"": consts.JSON_SCHEMA, ""type"": ""object"", ""properties"": { ""networks"": { ""type"": ""object"", ""properties"": { ""count"": { ""type"": ""integer"", ""minimum"": 1, }, ""extra_args"": { ""type"": ""object"", }, }, }, ""subnets"": { ""type"": ""object"", ""properties"": { ""count"": { ""type"": ""integer"", ""minimum"": 1, }, ""extra_args"": { ""type"": ""object"", }, ""cidr_prefix"": { ""type"": ""string"", }, ""dhcp_port_status_check"": { ""type"": ""object"", ""properties"": { ""enable"": { ""type"": ""boolean"", }, ""port_existence_timeout"": { ""type"": ""integer"", }, ""port_existence_check_interval"": { ""type"": ""integer"", }, ""status_check_timeout"": { ""type"": ""integer"", }, ""status_check_interval"": { ""type"": ""integer"", }, } }, }, }, ""routers"": { ""type"": ""object"", ""properties"": { ""count"": { ""type"": ""integer"", }, ""extra_args"": { ""type"": ""object"", }, }, }, ""floating_ips"": { ""type"": ""object"", ""properties"": { ""count"": { ""type"": ""integer"", }, ""external_network_id"": { ""type"": ""string"", }, }, }, ""provider_network"": { ""type"": ""object"", ""properties"": { ""netwrok_id"": { ""type"": ""string"", }, }, }, ""security_groups"": { ""type"": ""object"", ""properties"": { ""enable"": { ""type"": ""boolean"", }, ""groups"": { ""type"": ""object"", }, }, }, ""instances"": { ""type"": ""object"", ""properties"": { ""count"": { ""type"": ""integer"", ""minimum"": 1, }, ""extra_args"": { ""type"": ""object"", }, ""boot_pattern"": { ""type"": ""string"", }, ""external_connectivity"": { ""type"": ""string"", }, ""ping_test"": { ""type"": ""object"", ""properties"": { ""enable"": { ""type"": ""boolean"", }, ""timeout"": { ""type"": ""integer"", }, } }, ""ssh_test"": { ""type"": ""object"", ""properties"": { ""enable"": { ""type"": ""boolean"", }, ""timeout"": { ""type"": ""integer"", }, ""port"": { ""type"": ""integer"", }, ""username"": { ""type"": ""string"", }, ""password"": { ""type"": ""string"", }, } }, }, }, ""benchmark_cleanup"": { ""type"": ""boolean"", } }, ""required"": [""networks"", ""instances""], ""additionalProperties"": False } jsonschema.validate(resource_config, resource_config_schema) # Schema validation passed. # Now assign default values if not given in configuration. # def_config = { ""networks"": { ""count"": 1, ""extra_args"": {}, }, ""subnets"": { ""count"": 1, ""extra_args"": {}, ""cidr_prefix"": ""10.20.0.0/24"", ""dhcp_port_status_check"": { ""enable"": False, ""port_existence_timeout"": 120, ""port_existence_check_interval"": 1, ""status_check_timeout"": 300, ""status_check_interval"": 1, }, }, ""routers"": { ""count"": 1, ""extra_args"": {}, }, ""floating_ips"": { ""count"": 1, ""external_network_id"": None, }, ""provider_network"": { ""network_id"": None, }, ""security_groups"": { ""enable"": True, ""groups"": {}, }, ""instances"": { ""count"": 1, ""extra_args"": {}, # boot_pattern - parallel|sequential|oneshot # ""boot_pattern"": ""parallel"", # # external_connectivity - provider|external|both|none # ""external_connectivity"": ""none"", ""ping_test"": { ""enable"": False, ""timeout"": 120, }, ""ssh_test"": { ""enable"": False, ""timeout"": 120, ""port"": 22, ""username"": ""root"", ""password"": ""root123"", }, }, ""benchmark_cleanup"": False, } new_config = copy.deepcopy(resource_config) set_default_config_values(def_config, new_config) xtern_conn = new_config.get(""instances"").get(""external_connectivity"") # Provider Network validations # new_config[""provider_network""][""enable""] = False if xtern_conn in [""provider"", ""both""]: new_config[""provider_network""][""enable""] = True prov_net_id = new_config.get(""provider_network"").get(""network_id"") if prov_net_id is None: raise exceptions.InvalidTaskException( message=""Provider network id not configured."") net_obj = self.clients(""neutron"").show_network(prov_net_id) new_config[""provider_network""][""name""] = net_obj[""network""][""name""] # FloatingIP validations new_config[""floating_ips""][""enable""] = False if xtern_conn in [""external"", ""both""]: net_id = new_config.get(""floating_ips"").get(""external_network_id"") if net_id is None: raise exceptions.InvalidTaskException( message=""External network id not configured."") cnt = new_config.get(""floating_ips"").get(""count"") if cnt <= 0: raise exceptions.InvalidTaskException( message=""FloatingIp count value is zero."") new_config[""floating_ips""][""enable""] = True LOG.info(""Modified configuration is %s"", new_config) return new_config def set_default_config_values(ref_obj, new_obj): for key, ref_val in ref_obj.items(): new_obj.setdefault(key, ref_val) if isinstance(ref_val, dict): new_val = new_obj.get(key, {}) set_default_config_values(ref_val, new_val) ",,1450,0
openstack%2Fpython-zunclient~master~I80087a291368386ce311050c83c77a79d45e2f5a,openstack/python-zunclient,master,I80087a291368386ce311050c83c77a79d45e2f5a,Switch to oslo_log,NEW,2017-01-19 09:49:44.000000000,2017-12-18 04:21:59.000000000,,[],"[{'number': 1, 'created': '2017-01-19 09:49:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-zunclient/commit/314c58a403d1b923fd8c972b13b098dd180319a8', 'message': 'Switch to oslo_log\n\nZunclient is currently using python logging module to\nimplement logging functionality.\nThis patch made a switch to oslo.log(logging for openstack projects)\n\nReference:-\nhttp://docs.openstack.org/developer/oslo.log\n\nChange-Id: I80087a291368386ce311050c83c77a79d45e2f5a\n'}, {'number': 2, 'created': '2017-01-20 06:26:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-zunclient/commit/1d49661a448b0c9e34c8e09ee1f618ffa8dc0997', 'message': 'Switch to oslo_log\n\nZunclient is currently using python logging module to\nimplement logging functionality.\nThis patch made a switch to oslo.log(logging for openstack\n projects)\n\nReference:-\nhttp://docs.openstack.org/developer/oslo.log\n\nChange-Id: I80087a291368386ce311050c83c77a79d45e2f5a\n'}, {'number': 3, 'created': '2017-02-08 10:22:15.000000000', 'files': ['requirements.txt', 'zunclient/common/httpclient.py', 'zunclient/shell.py', 'zunclient/osc/v1/containers.py', 'zunclient/osc/plugin.py'], 'web_link': 'https://opendev.org/openstack/python-zunclient/commit/b39d408d9bd6b5696d5990154ebce3ee41dae7d3', 'message': 'Switch to oslo_log\n\nZunclient is currently using python logging module to\nimplement logging functionality.\nThis patch made a switch to oslo.log(logging for openstack\n projects)\n\nReference:-\nhttp://docs.openstack.org/developer/oslo.log\n\nChange-Id: I80087a291368386ce311050c83c77a79d45e2f5a\n'}]",0,422510,b39d408d9bd6b5696d5990154ebce3ee41dae7d3,7,0,3,22020,,,0,"Switch to oslo_log

Zunclient is currently using python logging module to
implement logging functionality.
This patch made a switch to oslo.log(logging for openstack
 projects)

Reference:-
http://docs.openstack.org/developer/oslo.log

Change-Id: I80087a291368386ce311050c83c77a79d45e2f5a
",git fetch https://review.opendev.org/openstack/python-zunclient refs/changes/10/422510/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'zunclient/common/httpclient.py', 'zunclient/shell.py', 'zunclient/osc/v1/containers.py', 'zunclient/osc/plugin.py']",5,314c58a403d1b923fd8c972b13b098dd180319a8,I80087a291368386ce311050c83c77a79d45e2f5a,from oslo_log import log as logging,import logging ,5,6
openstack%2Fswift~master~I4ed85f9fb300ac95e4ae91029c3190b266a6ca0f,openstack/swift,master,I4ed85f9fb300ac95e4ae91029c3190b266a6ca0f,Pretend *some* parts min_part_hours_passed,NEW,2016-04-29 19:00:39.000000000,2017-12-18 04:21:57.000000000,,"[{'_account_id': 1179}, {'_account_id': 6968}, {'_account_id': 13052}, {'_account_id': 20508}]","[{'number': 1, 'created': '2016-04-29 19:00:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/69ade993b9f203c0453d5e408b68e1508bf11922', 'message': 'Pretend *some* parts min_part_hours_passed\n\nIt came to my attention that some old rings may be moving parts ""too""\naggressively when faced against the new rebalance algorithm.\n\nIt\'s not so much that the balance or dispersion is getting *bad* - but\nmore so it\'s FIXING ALL THE THINGS in one go - even though no weights\nare being changed.\n\nTo control the movement of parts the new pretend_min_part_hours_locked\ncommand will reset all the last_part_moves so that it *feels* like the\nring can\'t rebalance *anything* even thought it would really like to.\n\nIn order to allow only a *subset* of parts to come out of the lock at\na time (so the operator can control how many parts move) the\npretend_min_part_hours_passed command now accepts an optional ratio\nargument which can be less that 100% - it will choose the least\nrecently moved parts and make them movable.\n\nThis is an evolution of a previous idea to allow operators to control\nthe number of parts moved per rebalance - but adapted with an eye\ntowards respecting essential un-avoidable part movement from failed\ndevices can still be performed as needed (min_part_hours is already\nignored for failed devices!).\n\nI found it sort of interesting to play around with on the command line\nto functionally observe the effect of min_part_hours - so it might be\nuseful as a general tool in dev.\n\nAs always using pretend_min_part_hours on production rings carries\nsome risk.  You need to closely monitor your rebalance cycle as you\nare circumventing some safety measures in the rings partition\nplacement algorithm.  But if you move slowly and carefully you might\nbe able to pull one out of the fire!\n\nN.B. lp bug #1558754 was closed but not really fixed, from the command\nline you may need to use --force to bypass the mis-guided\npre-rebalance check, let the RingBuilder do it\'s job and rebalance\nyour unlocked parts.\n\nChange-Id: I4ed85f9fb300ac95e4ae91029c3190b266a6ca0f\n'}, {'number': 2, 'created': '2016-05-02 16:18:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c709bfc52809338c8fc8300826a3ad13e129060d', 'message': 'Pretend *some* parts min_part_hours_passed\n\nIt came to my attention that some old rings may be moving parts ""too""\naggressively when faced against the new rebalance algorithm.\n\nIt\'s not so much that the balance or dispersion is getting *bad* - but\nmore so it\'s FIXING ALL THE THINGS in one go - even though no weights\nare being changed.\n\nTo control the movement of parts the new pretend_min_part_hours_locked\ncommand will reset all the last_part_moves so that it *feels* like the\nring can\'t rebalance *anything* even thought it would really like to.\n\nIn order to allow only a *subset* of parts to come out of the lock at\na time (so the operator can control how many parts move) the\npretend_min_part_hours_passed command now accepts an optional ratio\nargument which can be less that 100% - it will choose the least\nrecently moved parts and make them movable.\n\nThis is an evolution of a previous idea to allow operators to control\nthe number of parts moved per rebalance - but adapted with an eye\ntowards respecting essential un-avoidable part movement from failed\ndevices can still be performed as needed (min_part_hours is already\nignored for failed devices!).\n\nI found it sort of interesting to play around with on the command line\nto functionally observe the effect of min_part_hours - so it might be\nuseful as a general tool in dev.\n\nAs always using pretend_min_part_hours on production rings carries\nsome risk.  You need to closely monitor your rebalance cycle as you\nare circumventing some safety measures in the rings partition\nplacement algorithm.  But if you move slowly and carefully you might\nbe able to pull one out of the fire!\n\nN.B. lp bug #1558754 was closed but not really fixed, from the command\nline you may need to use --force to bypass the mis-guided\npre-rebalance check, let the RingBuilder do it\'s job and rebalance\nyour unlocked parts.\n\nChange-Id: I4ed85f9fb300ac95e4ae91029c3190b266a6ca0f\n'}, {'number': 3, 'created': '2016-05-02 18:36:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8e08439d821da19913e612ca8ae1028acdc28b51', 'message': 'Pretend *some* parts min_part_hours_passed\n\nIt came to my attention that some old rings may be moving parts ""too""\naggressively when faced against the new rebalance algorithm.\n\nIt\'s not so much that the balance or dispersion is getting *bad* - but\nmore so it\'s FIXING ALL THE THINGS in one go - even though no weights\nare being changed.\n\nTo control the movement of parts the new pretend_min_part_hours_locked\ncommand will reset all the last_part_moves so that it *feels* like the\nring can\'t rebalance *anything* even though it would really like to.\n\nIn order to allow only a *subset* of parts to come out of the lock at\na time (so the operator can control how many parts move) the\npretend_min_part_hours_passed command now accepts an optional ratio\nargument which can be less that 100% - it will choose the least\nrecently moved parts and make them movable.\n\nThis is an evolution of a previous idea to allow operators to control\nthe number of parts moved per rebalance - but adapted with an eye\ntowards respecting essential un-avoidable part movement.  Since\nmin_part_hours is already ignored for failed devices (and other\nessential part assignment like duplicate-part/multiple-replica to\ndevice or replica count adjustment) limiting part movement with\npretend_min_part_hours_[locked|passed] will safely allow devices to be\nfailed and removed from the ring using normal swift-ring-builder\ncommands.\n\nI found it sort of interesting to play around with on the command line\nto functionally observe the effect of min_part_hours - so it might be\nuseful as a general tool in dev.\n\nAs always using pretend_min_part_hours on production rings carries\nsome risk.  You need to closely monitor your rebalance cycle as you\nare circumventing some safety measures in the rings partition\nplacement algorithm.  But if you move slowly and carefully you might\nbe able to pull one out of the fire!\n\nN.B. lp bug #1558754 was closed but not really fixed, from the command\nline you may need to use --force to bypass the mis-guided\npre-rebalance check, let the RingBuilder do it\'s job and rebalance\nyour unlocked parts.\n\nChange-Id: I4ed85f9fb300ac95e4ae91029c3190b266a6ca0f\n'}, {'number': 4, 'created': '2016-05-05 22:40:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/dfa8821215454693f7fddcd97b602efce655b309', 'message': 'Pretend *some* parts min_part_hours_passed\n\nIt came to my attention that some old rings may be moving parts ""too""\naggressively when faced against the new rebalance algorithm.\n\nIt\'s not so much that the balance or dispersion is getting *bad* - but\nmore so it\'s FIXING ALL THE THINGS in one go - even though no weights\nare being changed.\n\nTo control the movement of parts the new pretend_min_part_hours_locked\ncommand will reset all the last_part_moves so that it *feels* like the\nring can\'t rebalance *anything* even though it would really like to.\n\nIn order to allow only a *subset* of parts to come out of the lock at\na time (so the operator can control how many parts move) the\npretend_min_part_hours_passed command now accepts an optional ratio\nargument which can be less that 100% - it will choose the least\nrecently moved parts and make them movable.\n\nThis is an evolution of a previous idea to allow operators to control\nthe number of parts moved per rebalance - but adapted with an eye\ntowards respecting essential un-avoidable part movement.  Since\nmin_part_hours is already ignored for failed devices (and other\nessential part assignment like duplicate-part/multiple-replica to\ndevice or replica count adjustment) limiting part movement with\npretend_min_part_hours_[locked|passed] will safely allow devices to be\nfailed and removed from the ring using normal swift-ring-builder\ncommands.\n\nI found it sort of interesting to play around with on the command line\nto functionally observe the effect of min_part_hours - so it might be\nuseful as a general tool in dev.\n\nAs always using pretend_min_part_hours on production rings carries\nsome risk.  You need to closely monitor your rebalance cycle as you\nare circumventing some safety measures in the rings partition\nplacement algorithm.  But if you move slowly and carefully you might\nbe able to pull one out of the fire!\n\nN.B. lp bug #1558754 was closed but not really fixed, from the command\nline you may need to use --force to bypass the mis-guided\npre-rebalance check, let the RingBuilder do it\'s job and rebalance\nyour unlocked parts.\n\nChange-Id: I4ed85f9fb300ac95e4ae91029c3190b266a6ca0f\n'}, {'number': 5, 'created': '2017-01-14 00:10:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/26419ed1c4fdbd938430b2a3d9011c5b7d13ab96', 'message': 'Pretend *some* parts min_part_hours_passed\n\nIt came to my attention that some old rings may be moving parts ""too""\naggressively when faced against the new rebalance algorithm.\n\nIt\'s not so much that the balance or dispersion is getting *bad* - but\nmore so it\'s FIXING ALL THE THINGS in one go - even though no weights\nare being changed.\n\nTo control the movement of parts the new pretend_min_part_hours_locked\ncommand will reset all the last_part_moves so that it *feels* like the\nring can\'t rebalance *anything* even though it would really like to.\n\nIn order to allow only a *subset* of parts to come out of the lock at\na time (so the operator can control how many parts move) the\npretend_min_part_hours_passed command now accepts an optional ratio\nargument which can be less than 100% - it will choose the least\nrecently moved parts and make them movable.\n\nThis is an evolution of a previous idea to allow operators to control\nthe number of parts moved per rebalance - but adapted with an eye\ntowards respecting essential un-avoidable part movement.  Since\nmin_part_hours is already ignored for failed devices (and other\nessential part assignment like duplicate-part/multiple-replica to\ndevice or replica count adjustment) limiting part movement with\npretend_min_part_hours_[locked|passed] will safely allow devices to be\nfailed and removed from the ring using normal swift-ring-builder\ncommands.\n\nI found it sort of interesting to play around with on the command line\nto functionally observe the effect of min_part_hours - so it might be\nuseful as a general tool in dev.\n\nAs always using pretend_min_part_hours on production rings carries\nsome risk.  You need to closely monitor your rebalance cycle as you\nare circumventing some safety measures in the rings partition\nplacement algorithm.  But if you move slowly and carefully you might\nbe able to pull one out of the fire!\n\nChange-Id: I4ed85f9fb300ac95e4ae91029c3190b266a6ca0f\n'}, {'number': 6, 'created': '2017-01-16 17:45:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/60605a7464886af4624ff97fe7192f794b348dc9', 'message': 'Pretend *some* parts min_part_hours_passed\n\nIt came to my attention that some old rings may be moving parts ""too""\naggressively when faced against the new rebalance algorithm.\n\nIt\'s not so much that the balance or dispersion is getting *bad* - but\nmore so it\'s FIXING ALL THE THINGS in one go - even though no weights\nare being changed.\n\nTo control the movement of parts the new pretend_min_part_hours_locked\ncommand will reset all the last_part_moves so that it *feels* like the\nring can\'t rebalance *anything* even though it would really like to.\n\nIn order to allow only a *subset* of parts to come out of the lock at\na time (so the operator can control how many parts move) the\npretend_min_part_hours_passed command now accepts an optional ratio\nargument which can be less than 100% - it will choose the least\nrecently moved parts and make them movable.\n\nThis is an evolution of a previous idea to allow operators to control\nthe number of parts moved per rebalance - but adapted with an eye\ntowards respecting essential un-avoidable part movement.  Since\nmin_part_hours is already ignored for failed devices (and other\nessential part assignment like duplicate-part/multiple-replica to\ndevice or replica count adjustment) limiting part movement with\npretend_min_part_hours_[locked|passed] will safely allow devices to be\nfailed and removed from the ring using normal swift-ring-builder\ncommands.\n\nI found it sort of interesting to play around with on the command line\nto functionally observe the effect of min_part_hours - so it might be\nuseful as a general tool in dev.\n\nAs always using pretend_min_part_hours on production rings carries\nsome risk.  You need to closely monitor your rebalance cycle as you\nare circumventing some safety measures in the rings partition\nplacement algorithm.  But if you move slowly and carefully you might\nbe able to pull one out of the fire!\n\nChange-Id: I4ed85f9fb300ac95e4ae91029c3190b266a6ca0f\n'}, {'number': 7, 'created': '2017-01-17 15:51:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/b9f4d7e6979ff829f54b2ae24f92c73e0ba22c7a', 'message': 'Pretend *some* parts min_part_hours_passed\n\nIt came to my attention that some old rings may be moving parts ""too""\naggressively when faced against the new rebalance algorithm.\n\nIt\'s not so much that the balance or dispersion is getting *bad* - but\nmore so it\'s FIXING ALL THE THINGS in one go - even though no weights\nare being changed.\n\nTo control the movement of parts the new pretend_min_part_hours_locked\ncommand will reset all the last_part_moves so that it *feels* like the\nring can\'t rebalance *anything* even though it would really like to.\n\nIn order to allow only a *subset* of parts to come out of the lock at\na time (so the operator can control how many parts move) the\npretend_min_part_hours_passed command now accepts an optional ratio\nargument which can be less than 100% - it will choose the least\nrecently moved parts and make them movable.\n\nThis is an evolution of a previous idea to allow operators to control\nthe number of parts moved per rebalance - but adapted with an eye\ntowards respecting essential un-avoidable part movement.  Since\nmin_part_hours is already ignored for failed devices (and other\nessential part assignment like duplicate-part/multiple-replica to\ndevice or replica count adjustment) limiting part movement with\npretend_min_part_hours_[locked|passed] will safely allow devices to be\nfailed and removed from the ring using normal swift-ring-builder\ncommands.\n\nI found it sort of interesting to play around with on the command line\nto functionally observe the effect of min_part_hours - so it might be\nuseful as a general tool in dev.\n\nAs always using pretend_min_part_hours on production rings carries\nsome risk.  You need to closely monitor your rebalance cycle as you\nare circumventing some safety measures in the rings partition\nplacement algorithm.  But if you move slowly and carefully you might\nbe able to pull one out of the fire!\n\nChange-Id: I4ed85f9fb300ac95e4ae91029c3190b266a6ca0f\n'}, {'number': 8, 'created': '2017-01-17 20:55:48.000000000', 'files': ['test/unit/common/ring/test_utils.py', 'swift/common/ring/builder.py', 'test/unit/common/ring/test_builder.py', 'swift/common/ring/utils.py', 'swift/cli/ringbuilder.py', 'test/unit/cli/test_ringbuilder.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/c411c4038a73e4e0e243e8714b96cc1dd7908a98', 'message': 'Pretend *some* parts min_part_hours_passed\n\nIt came to my attention that some old rings may be moving parts ""too""\naggressively when faced against the new rebalance algorithm.\n\nIt\'s not so much that the balance or dispersion is getting *bad* - but\nmore so it\'s FIXING ALL THE THINGS in one go - even though no weights\nare being changed.\n\nTo control the movement of parts the new pretend_min_part_hours_locked\ncommand will reset all the last_part_moves so that it *feels* like the\nring can\'t rebalance *anything* even though it would really like to.\n\nIn order to allow only a *subset* of parts to come out of the lock at\na time (so the operator can control how many parts move) the\npretend_min_part_hours_passed command now accepts an optional ratio\nargument which can be less than 100% - it will choose the least\nrecently moved parts and make them movable.\n\nThis is an evolution of a previous idea to allow operators to control\nthe number of parts moved per rebalance - but adapted with an eye\ntowards respecting essential un-avoidable part movement.  Since\nmin_part_hours is already ignored for failed devices (and other\nessential part assignment like duplicate-part/multiple-replica to\ndevice or replica count adjustment) limiting part movement with\npretend_min_part_hours_[locked|passed] will safely allow devices to be\nfailed and removed from the ring using normal swift-ring-builder\ncommands.\n\nI found it sort of interesting to play around with on the command line\nto functionally observe the effect of min_part_hours - so it might be\nuseful as a general tool in dev.\n\nAs always using pretend_min_part_hours on production rings carries\nsome risk.  You need to closely monitor your rebalance cycle as you\nare circumventing some safety measures in the rings partition\nplacement algorithm.  But if you move slowly and carefully you might\nbe able to pull one out of the fire!\n\nChange-Id: I4ed85f9fb300ac95e4ae91029c3190b266a6ca0f\n'}]",12,311226,c411c4038a73e4e0e243e8714b96cc1dd7908a98,30,4,8,1179,,,0,"Pretend *some* parts min_part_hours_passed

It came to my attention that some old rings may be moving parts ""too""
aggressively when faced against the new rebalance algorithm.

It's not so much that the balance or dispersion is getting *bad* - but
more so it's FIXING ALL THE THINGS in one go - even though no weights
are being changed.

To control the movement of parts the new pretend_min_part_hours_locked
command will reset all the last_part_moves so that it *feels* like the
ring can't rebalance *anything* even though it would really like to.

In order to allow only a *subset* of parts to come out of the lock at
a time (so the operator can control how many parts move) the
pretend_min_part_hours_passed command now accepts an optional ratio
argument which can be less than 100% - it will choose the least
recently moved parts and make them movable.

This is an evolution of a previous idea to allow operators to control
the number of parts moved per rebalance - but adapted with an eye
towards respecting essential un-avoidable part movement.  Since
min_part_hours is already ignored for failed devices (and other
essential part assignment like duplicate-part/multiple-replica to
device or replica count adjustment) limiting part movement with
pretend_min_part_hours_[locked|passed] will safely allow devices to be
failed and removed from the ring using normal swift-ring-builder
commands.

I found it sort of interesting to play around with on the command line
to functionally observe the effect of min_part_hours - so it might be
useful as a general tool in dev.

As always using pretend_min_part_hours on production rings carries
some risk.  You need to closely monitor your rebalance cycle as you
are circumventing some safety measures in the rings partition
placement algorithm.  But if you move slowly and carefully you might
be able to pull one out of the fire!

Change-Id: I4ed85f9fb300ac95e4ae91029c3190b266a6ca0f
",git fetch https://review.opendev.org/openstack/swift refs/changes/26/311226/8 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/ring/builder.py', 'test/unit/common/ring/test_utils.py', 'test/unit/common/ring/test_builder.py', 'swift/common/ring/utils.py', 'swift/cli/ringbuilder.py', 'test/unit/cli/test_ringbuilder.py']",6,69ade993b9f203c0453d5e408b68e1508bf11922,bug/1558754,"import time # but you're still crazy and you still get a *warning* self.assertTrue('Warning overload is greater than 100%' in out) def test_pretend_min_part_hours_passed(self): # sample ring self.run_srb(""create"", 6, 3, 24) self.run_srb(""add"", ""r1z1-10.1.1.1:2345/sda"", 100.0, ""r1z1-10.1.1.1:2345/sdb"", 100.0, ""r1z1-10.1.1.1:2345/sdc"", 100.0, ""r1z1-10.1.1.1:2345/sdd"", 100.0) out, err = self.run_srb(""rebalance"") self.assertFalse(err) self.run_srb(""add"", ""r1z1-10.1.1.1:2345/sde"", 100.0) out, err = self.run_srb(""rebalance"") # min part hours locked! self.assertIn(""No partitions could be reassigned"", out) out, err = self.run_srb(""pretend_min_part_hours_passed"", ""50%"") self.assertIn(""Unlocked 32 parts"", out) # see lp bug #1558754 - this *should* rebalance w/o --force out, err = self.run_srb(""rebalance"") self.assertIn(""No partitions could be reassigned"", out) # work around lp bug #1558754 out, err = self.run_srb(""rebalance"", ""--force"") self.assertIn(""Reassigned 32"", out) def test_pretend_min_part_hours_locked(self): # sample ring self.run_srb(""create"", 6, 3, 24) self.run_srb(""add"", ""r1z1-10.1.1.1:2345/sda"", 100.0, ""r1z1-10.1.1.1:2345/sdb"", 100.0, ""r1z1-10.1.1.1:2345/sdc"", 100.0, ""r1z1-10.1.1.1:2345/sdd"", 100.0) out, err = self.run_srb(""rebalance"") with mock.patch('swift.common.ring.builder.time') as mock_time: mock_time.return_value = time.time() + 60 out, err = self.run_srb() remaining_line = None for line in out.splitlines(): if 'remaining' in line: remaining_line = line break self.assertIn('(23:59:00 remaining)', remaining_line) # lock it up! out, err = self.run_srb(""pretend_min_part_hours_locked"") out, err = self.run_srb() remaining_line = None for line in out.splitlines(): if 'remaining' in line: remaining_line = line break self.assertIn('(1 day, 0:00:00 remaining)', remaining_line) # wait it out! mock_time.return_value += 86400 out, err = self.run_srb() remaining_line = None for line in out.splitlines(): if 'remaining' in line: remaining_line = line break self.assertIn('(0:00:00 remaining)', remaining_line) ", self.assertTrue('Warning overload is greater than 100%' not in out),341,18
openstack%2Fdevstack-gate~master~I976ebf019fc3b05b0e42bd245dc5d53a71497413,openstack/devstack-gate,master,I976ebf019fc3b05b0e42bd245dc5d53a71497413,DNM - What happens if pydistutils.cfg is invalid,NEW,2016-09-19 12:36:47.000000000,2017-12-18 04:21:54.000000000,,[],"[{'number': 1, 'created': '2016-09-19 12:36:47.000000000', 'files': ['functions.sh'], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/22d31e77583e805f3d68965fc71e627cc28ed95f', 'message': 'DNM - What happens if pydistutils.cfg is invalid\n\nChange-Id: I976ebf019fc3b05b0e42bd245dc5d53a71497413\n'}]",0,372451,22d31e77583e805f3d68965fc71e627cc28ed95f,4,0,1,2,,,0,"DNM - What happens if pydistutils.cfg is invalid

Change-Id: I976ebf019fc3b05b0e42bd245dc5d53a71497413
",git fetch https://review.opendev.org/openstack/devstack-gate refs/changes/51/372451/1 && git format-patch -1 --stdout FETCH_HEAD,['functions.sh'],1,22d31e77583e805f3d68965fc71e627cc28ed95f,, cat > ~/.pydistutils.cfg <<EOF [easy_install] index_url = http://example.org allow_hosts = *.example.org EOF ,,6,0
openstack%2Fzun~master~If6e88cf6bdbb58b1ac340ac93e3678e3764390c4,openstack/zun,master,If6e88cf6bdbb58b1ac340ac93e3678e3764390c4,Moving inspect image from containers driver to image driver,NEW,2016-11-30 05:28:02.000000000,2017-12-18 04:21:06.000000000,,"[{'_account_id': 9775}, {'_account_id': 10206}, {'_account_id': 11536}, {'_account_id': 12407}, {'_account_id': 16277}, {'_account_id': 21785}]","[{'number': 1, 'created': '2016-11-30 05:28:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/0b76e6eaf6c1c834d35593f083e9a72992d3ad56', 'message': 'Moving inspect image from containers driver to image driver\n\nThis patch moves te inspect image from zun/container/docker/driver.py\nto zun/image/docker/driver.py\n\nChange-Id: If6e88cf6bdbb58b1ac340ac93e3678e3764390c4\n'}, {'number': 2, 'created': '2016-11-30 06:42:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/1a48503bf6c70763be49753b04f02f4e650e1d18', 'message': 'Moving inspect image from containers driver to image driver\n\nThis patch moves te inspect image from zun/container/docker/driver.py\nto zun/image/docker/driver.py\n\nChange-Id: If6e88cf6bdbb58b1ac340ac93e3678e3764390c4\n'}, {'number': 3, 'created': '2016-11-30 14:35:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/1db0fcbf251599ad58d27e0304684e94241244cc', 'message': 'Moving inspect image from containers driver to image driver\n\nThis patch moves te inspect image from zun/container/docker/driver.py\nto zun/image/docker/driver.py\n\nChange-Id: If6e88cf6bdbb58b1ac340ac93e3678e3764390c4\n'}, {'number': 4, 'created': '2016-12-01 10:00:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/287b497667d57abf2a5b7acc64b3a33d7199d95d', 'message': 'Moving inspect image from containers driver to image driver\n\nThis patch moves te inspect image from zun/container/docker/driver.py\nto zun/image/docker/driver.py\n\nChange-Id: If6e88cf6bdbb58b1ac340ac93e3678e3764390c4\n'}, {'number': 5, 'created': '2016-12-01 16:42:38.000000000', 'files': ['zun/image/docker/driver.py', 'zun/container/docker/driver.py', 'zun/compute/manager.py'], 'web_link': 'https://opendev.org/openstack/zun/commit/10c0814c83a33182a5242a28e77edcbbca404c1f', 'message': 'Moving inspect image from containers driver to image driver\n\nThis patch moves te inspect image from zun/container/docker/driver.py\nto zun/image/docker/driver.py\n\nChange-Id: If6e88cf6bdbb58b1ac340ac93e3678e3764390c4\n'}]",0,404546,10c0814c83a33182a5242a28e77edcbbca404c1f,19,6,5,21785,,,0,"Moving inspect image from containers driver to image driver

This patch moves te inspect image from zun/container/docker/driver.py
to zun/image/docker/driver.py

Change-Id: If6e88cf6bdbb58b1ac340ac93e3678e3764390c4
",git fetch https://review.opendev.org/openstack/zun refs/changes/46/404546/2 && git format-patch -1 --stdout FETCH_HEAD,"['zun/image/docker/driver.py', 'zun/container/docker/driver.py', 'zun/image/driver.py']",3,0b76e6eaf6c1c834d35593f083e9a72992d3ad56,," def inspect_image(self, image, image_path=None): """"""Inspect Image."""""" raise NotImplementedError()",,14,10
openstack%2Fswift~master~I74a8a4e0f8b1064563db17063d28b6ac03ce4978,openstack/swift,master,I74a8a4e0f8b1064563db17063d28b6ac03ce4978,Follow the patch 371150,ABANDONED,2017-11-02 09:26:52.000000000,2017-12-18 04:20:55.000000000,,"[{'_account_id': 4608}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-11-02 09:26:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/13d21f6a77b29bf2b4f9e0a91ecc6db8795963ce', 'message': 'Follow the patch 371150\n\nAdd a test for https://review.openstack.org/#/c/371150/\n\nThis is working in progress.\n\nChange-Id: I74a8a4e0f8b1064563db17063d28b6ac03ce4978\n'}, {'number': 2, 'created': '2017-11-02 13:04:24.000000000', 'files': ['test/unit/proxy/controllers/test_obj.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/db93ea1331ec69e1280a688599d1ccddb4e6500c', 'message': 'Follow the patch 371150\n\nAdd a test for https://review.openstack.org/#/c/371150/\n\nThis is working in progress.\n\nChange-Id: I74a8a4e0f8b1064563db17063d28b6ac03ce4978\n'}]",0,517224,db93ea1331ec69e1280a688599d1ccddb4e6500c,6,2,2,4608,,,0,"Follow the patch 371150

Add a test for https://review.openstack.org/#/c/371150/

This is working in progress.

Change-Id: I74a8a4e0f8b1064563db17063d28b6ac03ce4978
",git fetch https://review.opendev.org/openstack/swift refs/changes/24/517224/2 && git format-patch -1 --stdout FETCH_HEAD,['test/unit/proxy/controllers/test_obj.py'],1,13d21f6a77b29bf2b4f9e0a91ecc6db8795963ce,bug/1560574," def test_GET_not_found_even_only_one_404_newer(self): # if proxy receives a 404, it keeps waiting for other connections until # max number of nodes in hopes of finding an object, but if 404 is # more recent than a 200, then it should ignore 200 and return 404 req = swift.common.swob.Request.blank('/v1/a/c/o') codes = [404] + [200] * ( self.replicas + self.obj_ring.max_more_nodes - 1) ts_iter = iter( [2] + [1] * (self.replicas + self.obj_ring.max_more_nodes - 1)) with set_http_connect(*codes, timestamps=ts_iter): resp = req.get_response(self.app) self.assertEqual(resp.status_int, 404) ",,13,0
openstack%2Fopenstack-helm-infra~master~Icb06a6570683b7accebc142f75901530c6359180,openstack/openstack-helm-infra,master,Icb06a6570683b7accebc142f75901530c6359180,Update Prometheus to version 2.0,MERGED,2017-12-02 23:43:33.000000000,2017-12-18 04:19:21.000000000,2017-12-18 04:19:21.000000000,"[{'_account_id': 17591}, {'_account_id': 20466}, {'_account_id': 22348}, {'_account_id': 23928}]","[{'number': 1, 'created': '2017-12-02 23:43:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/3d8cdf441d561580b8c26a4ad15ab0e6bc3f205e', 'message': 'WIP: Test prometheus 2.0\n\nTest the prometheus 2.0 image to verify changes required for\nsupport in the prometheus chart\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 2, 'created': '2017-12-03 00:19:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/819b66ea0a68b8ec57a577acdbfca0ab124d997a', 'message': 'WIP: Test prometheus 2.0\n\nTest the prometheus 2.0 image to verify changes required for\nsupport in the prometheus chart\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 3, 'created': '2017-12-08 15:56:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/a89aec0d13a5ae56cca1398ee9cee5cb4eca3a7b', 'message': 'WIP: Test prometheus 2.0\n\nTest the prometheus 2.0 image to verify changes required for\nsupport in the prometheus chart\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 4, 'created': '2017-12-08 17:42:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/f469090d822ff2f9481e556f26d6513caaeaeaef', 'message': 'WIP: Test prometheus 2.0\n\nTest the prometheus 2.0 image to verify changes required for\nsupport in the prometheus chart\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 5, 'created': '2017-12-08 22:27:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/46a9af296ff298a1600e496d27bbac7ec2247aa7', 'message': 'WIP: Test prometheus 2.0\n\nTest the prometheus 2.0 image to verify changes required for\nsupport in the prometheus chart\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 6, 'created': '2017-12-08 22:38:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/de09e43aaaaeef2349b51843244f93a6d056709e', 'message': 'WIP: Test prometheus 2.0\n\nTest the prometheus 2.0 image to verify changes required for\nsupport in the prometheus chart\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 7, 'created': '2017-12-08 23:01:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/3d75026587df650f699e910d07b2b94660a395b2', 'message': 'WIP: Test prometheus 2.0\n\nTest the prometheus 2.0 image to verify changes required for\nsupport in the prometheus chart\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 8, 'created': '2017-12-08 23:10:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/4f822fdabdfaadf4b21dc0b6120c576dfaa127d7', 'message': 'WIP: Test prometheus 2.0\n\nTest the prometheus 2.0 image to verify changes required for\nsupport in the prometheus chart\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 9, 'created': '2017-12-10 20:12:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/1aa569c5beca30919ab5a037e7bb95c1bbbbaf08', 'message': 'Update Prometheus to version 2.0\n\nUpdates the Prometheus chart to use version 2.0 by default. This\nintroduces a change in the rules format (to yaml), and changes the\nflags required for the storage layer.\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 10, 'created': '2017-12-10 20:15:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/04f309c35402d4e26cdb4fa532de73335ccab0c8', 'message': 'Update Prometheus to version 2.0\n\nUpdates the Prometheus chart to use version 2.0 by default. This\nintroduces a change in the rules format (to yaml), and changes the\nflags required for the storage layer.\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 11, 'created': '2017-12-10 20:16:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/e68b6f6f6e37336c43e769a83eec6b29d9c18814', 'message': 'Update Prometheus to version 2.0\n\nUpdates the Prometheus chart to use version 2.0 by default. This\nintroduces a change in the rules format (to yaml), and changes the\nflags required for the storage layer.\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 12, 'created': '2017-12-10 21:02:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/7ca0e65b938f23586ad5e1956a02c74b07d1ea01', 'message': 'Update Prometheus to version 2.0\n\nUpdates the Prometheus chart to use version 2.0 by default. This\nintroduces a change in the rules format (to yaml), and changes the\nflags required for the storage layer.\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 13, 'created': '2017-12-10 22:27:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/d6ff5ce677d71855f99cbd9bc059f384ce3a475b', 'message': 'Update Prometheus to version 2.0\n\nUpdates the Prometheus chart to use version 2.0 by default. This\nintroduces a change in the rules format (to yaml), and changes the\nflags required for the storage layer.\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 14, 'created': '2017-12-11 00:00:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/a261c85d7347c02edeca5e338e6ba2e54a488540', 'message': 'Update Prometheus to version 2.0\n\nUpdates the Prometheus chart to use version 2.0 by default. This\nintroduces a change in the rules format (to yaml), and changes the\nflags required for the storage layer.\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 15, 'created': '2017-12-11 00:01:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/c269a0ddb6d5d7349767bcd4b4c2982c129f1aba', 'message': 'Update Prometheus to version 2.0\n\nUpdates the Prometheus chart to use version 2.0 by default. This\nintroduces a change in the rules format (to yaml), and changes the\nflags required for the storage layer.\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 16, 'created': '2017-12-11 00:10:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/d83224a412b765c01d09d67bc848c9c270f7ed3f', 'message': 'Update Prometheus to version 2.0\n\nUpdates the Prometheus chart to use version 2.0 by default. This\nintroduces a change in the rules format (to yaml), and changes the\nflags required for the storage layer.\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 17, 'created': '2017-12-12 15:32:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/edcff7f82574b7489a264a62575541a9aa8a8178', 'message': 'Update Prometheus to version 2.0\n\nUpdates the Prometheus chart to use version 2.0 by default. This\nintroduces a change in the rules format (to yaml), and changes the\nflags required for the storage layer.\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 18, 'created': '2017-12-17 20:19:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/0d1e90c48e58da078eb9f1a54e193b8fa3e5d611', 'message': 'Update Prometheus to version 2.0\n\nUpdates the Prometheus chart to use version 2.0 by default. This\nintroduces a change in the rules format (to yaml), and changes the\nflags required for the storage layer.\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}, {'number': 19, 'created': '2017-12-17 20:47:09.000000000', 'files': ['alertmanager/values.yaml', 'prometheus/templates/bin/_prometheus.sh.tpl', 'prometheus/values.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/ca6322da876ec187fa2268cf43e20a8b639de91f', 'message': 'Update Prometheus to version 2.0\n\nUpdates the Prometheus chart to use version 2.0 by default. This\nintroduces a change in the rules format (to yaml), and changes the\nflags required for the storage layer.\n\nChange-Id: Icb06a6570683b7accebc142f75901530c6359180\n'}]",0,524930,ca6322da876ec187fa2268cf43e20a8b639de91f,38,4,19,17591,,,0,"Update Prometheus to version 2.0

Updates the Prometheus chart to use version 2.0 by default. This
introduces a change in the rules format (to yaml), and changes the
flags required for the storage layer.

Change-Id: Icb06a6570683b7accebc142f75901530c6359180
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/30/524930/18 && git format-patch -1 --stdout FETCH_HEAD,['prometheus/values.yaml'],1,3d8cdf441d561580b8c26a4ad15ab0e6bc3f205e,prom2.0, prometheus: docker.io/prom/prometheus:v2.0.0, prometheus: docker.io/prom/prometheus:v1.7.1,1,1
openstack%2Fironic~master~I77b2e72345314fb0977130bc9a63838f5489e788,openstack/ironic,master,I77b2e72345314fb0977130bc9a63838f5489e788,Move add_node_capability to tests folder,NEW,2016-09-30 13:28:49.000000000,2017-12-18 04:08:20.000000000,,"[{'_account_id': 6618}, {'_account_id': 10118}, {'_account_id': 14525}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 20311}]","[{'number': 1, 'created': '2016-09-30 13:28:49.000000000', 'files': ['ironic/tests/unit/drivers/modules/msftocs/test_management.py', 'ironic/drivers/utils.py', 'ironic/tests/unit/drivers/modules/utils.py', 'ironic/tests/unit/drivers/test_utils.py', 'ironic/tests/unit/drivers/modules/irmc/test_management.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/ecd943f528991bc37a884bffbd8eb464b9d0ef48', 'message': 'Move add_node_capability to tests folder\n\nadd_node_capability() function is used only in tests. Move it to\nironic/tests/unit/drivers/modules/utils.py\n\nChange-Id: I77b2e72345314fb0977130bc9a63838f5489e788\n'}]",0,380290,ecd943f528991bc37a884bffbd8eb464b9d0ef48,13,7,1,14525,,,0,"Move add_node_capability to tests folder

add_node_capability() function is used only in tests. Move it to
ironic/tests/unit/drivers/modules/utils.py

Change-Id: I77b2e72345314fb0977130bc9a63838f5489e788
",git fetch https://review.opendev.org/openstack/ironic refs/changes/90/380290/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/tests/unit/drivers/modules/msftocs/test_management.py', 'ironic/drivers/utils.py', 'ironic/tests/unit/drivers/modules/utils.py', 'ironic/tests/unit/drivers/test_utils.py', 'ironic/tests/unit/drivers/modules/irmc/test_management.py']",5,ecd943f528991bc37a884bffbd8eb464b9d0ef48,add_node_capability,"from ironic.tests.unit.drivers.modules import utils as drivers_utils drivers_utils.add_node_capability(task.node, 'boot_mode', boot_mode) drivers_utils.add_node_capability(task.node, 'boot_mode', 'uefi')","from ironic.drivers import utils as driver_utils driver_utils.add_node_capability(task, 'boot_mode', boot_mode) driver_utils.add_node_capability(task, 'boot_mode', 'uefi')",35,56
openstack%2Fironic~master~Ib05958a36aaf79caadf93ff7713bfd113dfb3141,openstack/ironic,master,Ib05958a36aaf79caadf93ff7713bfd113dfb3141,Copy editing of ironic docs,NEW,2015-06-17 20:52:48.000000000,2017-12-18 04:08:18.000000000,,"[{'_account_id': 9382}, {'_account_id': 9383}, {'_account_id': 12459}, {'_account_id': 13589}, {'_account_id': 14920}, {'_account_id': 14943}, {'_account_id': 16272}]","[{'number': 1, 'created': '2015-06-17 20:52:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6ef47dab3397d7ab9ac33416b2c0215c345b0851', 'message': 'Copy editing of ironic docs\n\nChanged mentions of Ironic to Bare metal service per conventions listed here: https://wiki.openstack.org/wiki/Documentation/Conventions\n\nChange-Id: Ib05958a36aaf79caadf93ff7713bfd113dfb3141\n'}, {'number': 2, 'created': '2015-06-18 18:28:02.000000000', 'files': ['doc/source/drivers/vbox.rst'], 'web_link': 'https://opendev.org/openstack/ironic/commit/7e8365139c28d19905e8abbccc73572de81b6cbb', 'message': 'Copy editing of ironic docs\n\nChanged mentions of ironic to Bare metal service per conventions:\nhttps://wiki.openstack.org/wiki/Documentation/Conventions\n\nChange-Id: Ib05958a36aaf79caadf93ff7713bfd113dfb3141\n'}]",2,192889,7e8365139c28d19905e8abbccc73572de81b6cbb,17,7,2,13990,,,0,"Copy editing of ironic docs

Changed mentions of ironic to Bare metal service per conventions:
https://wiki.openstack.org/wiki/Documentation/Conventions

Change-Id: Ib05958a36aaf79caadf93ff7713bfd113dfb3141
",git fetch https://review.opendev.org/openstack/ironic refs/changes/89/192889/2 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/drivers/vbox.rst'],1,6ef47dab3397d7ab9ac33416b2c0215c345b0851,,"VirtualBox drivers can be used to test Bare metal service by using VirtualBox VMs toBare metal service provides support via the ``pxe_ssh`` and ``agent_ssh`` drivers for usingthe VirtualBox host. These drivers are primarily intended for Bare metal service developers ``/etc/ironic/ironic.conf``, and restart Bare metal service conductor. Bare metal service (during provisioning), VirtualBox will automatically pop up a","VirtualBox drivers can be used to test Ironic by using VirtualBox VMs toIronic provides support via the ``pxe_ssh`` and ``agent_ssh`` drivers for usingthe VirtualBox host. These drivers are primarily intended for Ironic developers ``/etc/ironic/ironic.conf``, and restart Ironic conductor. Ironic (during provisioning), VirtualBox will automatically pop up a",5,5
openstack%2Fkuryr-libnetwork~master~Id8aacf28225004745bad52ca87552ec9a4fb9f84,openstack/kuryr-libnetwork,master,Id8aacf28225004745bad52ca87552ec9a4fb9f84,Enable release notes translation,NEW,2017-01-19 05:28:43.000000000,2017-12-18 04:07:38.000000000,,"[{'_account_id': 1923}, {'_account_id': 6598}, {'_account_id': 9820}, {'_account_id': 11343}, {'_account_id': 13912}, {'_account_id': 14352}, {'_account_id': 14867}, {'_account_id': 15967}, {'_account_id': 17589}]","[{'number': 1, 'created': '2017-01-19 05:28:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr-libnetwork/commit/46c9152ffc3872dcd221969c4bf1d065dfbc6dfa', 'message': ""Enable release notes translation\n\nReleasenote translation publishing is being prepared. 'locale_dirs'\nneeds to be defined in conf.py to generate translated version of the\nrelease notes.\n\nNote that this repository might not get translated release notes - or\nno translations at all - but we add the entry here nevertheless to\nprepare for it.\n\nChange-Id: Id8aacf28225004745bad52ca87552ec9a4fb9f84\n""}, {'number': 2, 'created': '2017-01-19 07:03:31.000000000', 'files': ['releasenotes/source/conf.py'], 'web_link': 'https://opendev.org/openstack/kuryr-libnetwork/commit/d5fbbf299b3b24ecd5ff445a722854bf9fc3842c', 'message': ""Enable release notes translation\n\nReleasenote translation publishing is being prepared. 'locale_dirs'\nneeds to be defined in conf.py to generate translated version of the\nrelease notes.\n\nNote that this repository might not get translated release notes - or\nno translations at all - but we add the entry here nevertheless to\nprepare for it.\n\nChange-Id: Id8aacf28225004745bad52ca87552ec9a4fb9f84\n""}]",1,422394,d5fbbf299b3b24ecd5ff445a722854bf9fc3842c,12,9,2,13912,,,0,"Enable release notes translation

Releasenote translation publishing is being prepared. 'locale_dirs'
needs to be defined in conf.py to generate translated version of the
release notes.

Note that this repository might not get translated release notes - or
no translations at all - but we add the entry here nevertheless to
prepare for it.

Change-Id: Id8aacf28225004745bad52ca87552ec9a4fb9f84
",git fetch https://review.opendev.org/openstack/kuryr-libnetwork refs/changes/94/422394/2 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/source/conf.py'],1,46c9152ffc3872dcd221969c4bf1d065dfbc6dfa,,"# texinfo_appendices = [] # If false, no module index is generated. # texinfo_domain_indices = True # How to display URL addresses: 'footnote', 'no', or 'inline'. # texinfo_show_urls = 'footnote' # If true, do not generate a @detailmenu in the ""Top"" node's menu. # texinfo_no_detailmenu = False # -- Options for Internationalization output ------------------------------ locale_dirs = ['locale/'] ",,13,0
openstack%2Fswift~master~I7dec8d43ab7ffcd5bdd0ddb87733f5a9a39c81ec,openstack/swift,master,I7dec8d43ab7ffcd5bdd0ddb87733f5a9a39c81ec,change account_autocreate to default to true,NEW,2016-10-13 12:04:52.000000000,2017-12-18 04:07:31.000000000,,"[{'_account_id': 12261}, {'_account_id': 12279}, {'_account_id': 12954}, {'_account_id': 13052}, {'_account_id': 20403}, {'_account_id': 22348}]","[{'number': 1, 'created': '2016-10-13 12:04:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a4896be0ed9f10b13e5e63f100b7e89d6d40d527', 'message': 'change account_autocreate to default to true\n\n""account_autocreate = true"" is more common than the default false setting.\nThis patch is to make ""account_autocreate = true"" as default setting.\n\nChange-Id: I7dec8d43ab7ffcd5bdd0ddb87733f5a9a39c81ec\nCloses-bug: #1629711\n'}, {'number': 2, 'created': '2016-10-13 15:23:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/361779147175bbc723e25abee4f83953ef13c39b', 'message': 'change account_autocreate to default to true\n\n""account_autocreate = true"" is more common than the default false setting.\nThis patch is to make ""account_autocreate = true"" as default setting.\n\nChange-Id: I7dec8d43ab7ffcd5bdd0ddb87733f5a9a39c81ec\nCloses-bug: #1629711\n'}, {'number': 3, 'created': '2016-10-14 12:26:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8784bda9a56277c62226f32ba6e0bf9d02bf4f11', 'message': 'change account_autocreate to default to true\n\n""account_autocreate = true"" is more common than the default false setting.\nThis patch is to make ""account_autocreate = true"" as default setting.\n\nChange-Id: I7dec8d43ab7ffcd5bdd0ddb87733f5a9a39c81ec\nCloses-bug: #1629711\n'}, {'number': 4, 'created': '2017-03-18 04:02:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/de87e67a5b9e232f5f786931fb2d4c79218b3eed', 'message': 'change account_autocreate to default to true\n\n""account_autocreate = true"" is more common than the default false setting.\nThis patch is to make ""account_autocreate = true"" as default setting.\n\nChange-Id: I7dec8d43ab7ffcd5bdd0ddb87733f5a9a39c81ec\nCloses-bug: #1629711\n'}, {'number': 5, 'created': '2017-07-07 02:21:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/85cb65148222dac087c5f3f06488686113f96cde', 'message': 'change account_autocreate to default to true\n\n""account_autocreate = true"" is more common than the default false setting.\nThis patch is to make ""account_autocreate = true"" as default setting.\n\nChange-Id: I7dec8d43ab7ffcd5bdd0ddb87733f5a9a39c81ec\nCloses-bug: #1629711\n'}, {'number': 6, 'created': '2017-12-18 02:34:15.000000000', 'files': ['doc/source/deployment_guide.rst', 'test/unit/proxy/controllers/test_account.py', 'test/unit/proxy/test_server.py', 'etc/proxy-server.conf-sample', 'doc/manpages/proxy-server.conf.5', 'swift/proxy/server.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/0215cb47299c2a6f151e07ab7190a794bd9d7296', 'message': 'change account_autocreate to default to true\n\n""account_autocreate = true"" is more common than the default false setting.\nThis patch is to make ""account_autocreate = true"" as default setting.\n\nChange-Id: I7dec8d43ab7ffcd5bdd0ddb87733f5a9a39c81ec\nCloses-bug: #1629711\n'}]",5,385946,0215cb47299c2a6f151e07ab7190a794bd9d7296,28,6,6,20403,,,0,"change account_autocreate to default to true

""account_autocreate = true"" is more common than the default false setting.
This patch is to make ""account_autocreate = true"" as default setting.

Change-Id: I7dec8d43ab7ffcd5bdd0ddb87733f5a9a39c81ec
Closes-bug: #1629711
",git fetch https://review.opendev.org/openstack/swift refs/changes/46/385946/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/deployment_guide.rst', 'test/unit/proxy/test_server.py', 'etc/proxy-server.conf-sample', 'doc/manpages/proxy-server.conf.5', 'swift/proxy/server.py']",5,a4896be0ed9f10b13e5e63f100b7e89d6d40d527,bug/1629711," config_true_value(conf.get('account_autocreate', 'true'))"," config_true_value(conf.get('account_autocreate', 'no'))",23,19
openstack%2Fpython-ironicclient~master~If75a22884426144494290d84630bdffe35ca07f7,openstack/python-ironicclient,master,If75a22884426144494290d84630bdffe35ca07f7,Add functional tests for VIFs in OSC plugin,NEW,2017-02-08 13:18:00.000000000,2017-12-18 04:07:21.000000000,,"[{'_account_id': 14614}, {'_account_id': 20772}]","[{'number': 1, 'created': '2017-02-08 13:18:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/1f0ecf156b64c1fd3c8d1ad19aec24a68b12a5bc', 'message': '[WiP] Add functional tests for VIFs in OSC plugin\n\nAdd tests for three commands:\n  baremetal node vif attach\n  baremetal node vif detach\n  baremetal node vif list\n\nChange-Id: If75a22884426144494290d84630bdffe35ca07f7\nCloses-Bug: #1662878\n'}, {'number': 2, 'created': '2017-02-08 15:11:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/06caab30add720dbaa09d57d9513c64fc762a99f', 'message': '[WiP] Add functional tests for VIFs in OSC plugin\n\nAdd tests for three commands:\n  baremetal node vif attach\n  baremetal node vif detach\n  baremetal node vif list\n\nChange-Id: If75a22884426144494290d84630bdffe35ca07f7\nCloses-Bug: #1662878\n'}, {'number': 3, 'created': '2017-02-10 13:55:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/00c03cf90a8f56c8376681db41330d7f31c1c095', 'message': '[WiP] Add functional tests for VIFs in OSC plugin\n\nAdd basic tests for three commands:\n  baremetal node vif attach\n  baremetal node vif detach\n  baremetal node vif list\n\nChange-Id: If75a22884426144494290d84630bdffe35ca07f7\nCloses-Bug: #1662878\n'}, {'number': 4, 'created': '2017-02-13 16:11:46.000000000', 'files': ['ironicclient/tests/functional/osc/v1/base.py', 'ironicclient/tests/functional/osc/v1/test_baremetal_node_vif.py'], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/d3f524f3ffdb00465db0f42394a790d36e6d8e89', 'message': 'Add functional tests for VIFs in OSC plugin\n\nAdd basic tests for three commands:\n  baremetal node vif attach\n  baremetal node vif detach\n  baremetal node vif list\n\nTest VIFs attached to nodes with ports\nand nodes with port groups.\n\nChange-Id: If75a22884426144494290d84630bdffe35ca07f7\nCloses-Bug: #1662878\n'}]",0,430904,d3f524f3ffdb00465db0f42394a790d36e6d8e89,11,2,4,14614,,,0,"Add functional tests for VIFs in OSC plugin

Add basic tests for three commands:
  baremetal node vif attach
  baremetal node vif detach
  baremetal node vif list

Test VIFs attached to nodes with ports
and nodes with port groups.

Change-Id: If75a22884426144494290d84630bdffe35ca07f7
Closes-Bug: #1662878
",git fetch https://review.opendev.org/openstack/python-ironicclient refs/changes/04/430904/4 && git format-patch -1 --stdout FETCH_HEAD,['ironicclient/tests/functional/osc/v1/test_baremetal_node_vif.py'],1,1f0ecf156b64c1fd3c8d1ad19aec24a68b12a5bc,bug/1662878,"# Copyright (c) 2017 Mirantis, Inc. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from tempest.lib.common.utils import data_utils from ironicclient.tests.functional.osc.v1 import base class BaremetalNodeVIFTests(base.TestCase): """"""Functional tests for baremetal node VIF commands."""""" def setUp(self): super(BaremetalNodeVIFTests, self).setUp() self.api = ' --os-baremetal-api-version 1.28' self.node = self.node_create() self.port = self.port_create(self.node['uuid']) self.port_group = self.port_group_create(self.node['uuid'], params=self.api) self.openstack('baremetal port set --port-group {0} {1}' .format(self.port_group['uuid'], self.port['uuid'])) def test_vif_attach(self): """"""Attach VIF to a node. Test steps: 1) Generate fake Neutron port UUID. 2) Create VIF attachment to the node. 3) Check in port and portgroup. """""" neutron_port_uuid = data_utils.rand_uuid() tenant_vif_key = 'tenant_vif_port_id' # tenant_vif_info = {u'tenant_vif_port_id': # u'{}'.format(neutron_port_uuid)} self.openstack('baremetal node vif attach {0} {1}' .format(self.node['uuid'], neutron_port_uuid)) self.assertEqual(self.port['internal_info'][tenant_vif_key], neutron_port_uuid) self.assertEqual(self.port_group['internal_info'][tenant_vif_key], neutron_port_uuid) def test_vifs_list(self): """"""List VIFs of a node."""""" pass def test_vifs_list(self): """"""Detach VIF from node."""""" pass ",,58,0
openstack%2Fironic~master~I5475b5ff4f1e97ad20fc493535a9cc9af1c26d73,openstack/ironic,master,I5475b5ff4f1e97ad20fc493535a9cc9af1c26d73,ETAGs for subresource requests,NEW,2017-01-25 16:13:55.000000000,2017-12-18 04:07:02.000000000,,"[{'_account_id': 10118}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 19604}, {'_account_id': 22724}]","[{'number': 1, 'created': '2017-01-25 16:13:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/536b7550a5880936e47dc7f266b1187d35c707a7', 'message': 'ETAGs for subresource requests\n\nWhen the user requests a list of subresources\nit is logically if the response would contain\nboth types of etags in its body: etag of the resource\n(parent etag) and appropriate etags for subresources.\nIncluding parent etag of related parent resource to\nthe response body.\n\nChange-Id: I5475b5ff4f1e97ad20fc493535a9cc9af1c26d73\n'}, {'number': 2, 'created': '2017-01-26 12:47:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7ece2e8e6d2a30511a129b5f99c68f16f80e9523', 'message': 'ETAGs for subresource requests\n\nWhen the user requests a list of subresources\nit is logically if the response would contain\nboth types of etags in its body: etag of the resource\n(parent etag) and appropriate etags for subresources.\nIncluding parent etag of related parent resource to\nthe response body.\n\nChange-Id: I5475b5ff4f1e97ad20fc493535a9cc9af1c26d73\n'}, {'number': 3, 'created': '2017-02-09 12:28:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/60b4d820767df179eec9dcc6e34dd4149b45dd46', 'message': 'ETAGs for subresource requests\n\nWhen the user requests a list of subresources\nit is logically if the response would contain\nboth types of etags in its body: etag of the resource\n(parent etag) and appropriate etags for subresources.\nIncluding parent etag of related parent resource to\nthe response body.\n\nChange-Id: I5475b5ff4f1e97ad20fc493535a9cc9af1c26d73\n'}, {'number': 4, 'created': '2017-02-09 16:14:58.000000000', 'files': ['ironic/api/controllers/v1/port.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/971b51ee322edec773f3f7d6cf7e0a52f85191be', 'message': 'ETAGs for subresource requests\n\nWhen the user requests a list of subresources\nit is logically if the response would contain\nboth types of etags in its body: etag of the resource\n(parent etag) and appropriate etags for subresources.\nIncluding parent etag of related parent resource to\nthe response body.\n\nChange-Id: I5475b5ff4f1e97ad20fc493535a9cc9af1c26d73\n'}]",0,425260,971b51ee322edec773f3f7d6cf7e0a52f85191be,29,7,4,22724,,,0,"ETAGs for subresource requests

When the user requests a list of subresources
it is logically if the response would contain
both types of etags in its body: etag of the resource
(parent etag) and appropriate etags for subresources.
Including parent etag of related parent resource to
the response body.

Change-Id: I5475b5ff4f1e97ad20fc493535a9cc9af1c26d73
",git fetch https://review.opendev.org/openstack/ironic refs/changes/60/425260/1 && git format-patch -1 --stdout FETCH_HEAD,['ironic/api/controllers/v1/port.py'],1,536b7550a5880936e47dc7f266b1187d35c707a7,subresources," etag = wtypes.text """"""Entity tag for subresource requests"""""" etag = kwargs.get('parent_etag') if etag: self.etag = etag parent_etag = kwargs.get('parent_etag') if parent_etag: collection.etag = parent_etag sort_dir=sort_dir, parent_etag=node.etag)", sort_dir=sort_dir),13,1
openstack%2Fheat~master~Ib16328b1b69cd9a10f41fbc0dce1c2b0e6d7e664,openstack/heat,master,Ib16328b1b69cd9a10f41fbc0dce1c2b0e6d7e664,Add tempest ids for functional and scenario tests,ABANDONED,2017-09-22 05:08:54.000000000,2017-12-18 04:05:33.000000000,,"[{'_account_id': 3}, {'_account_id': 12404}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-09-22 05:08:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/2c443ff4841bdf07871c5d8fc6526943302cf97c', 'message': 'Add tempest ids for functional and scenario tests\n\nThis patch add tempest ids for all functional and scenario tests\nin integration tests.\n\nChange-Id: Ib16328b1b69cd9a10f41fbc0dce1c2b0e6d7e664\n'}, {'number': 2, 'created': '2017-09-23 23:40:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/6775bf3d7b4a6519de1bca7173813a9c807f18be', 'message': 'Add tempest ids for functional and scenario tests\n\nThis patch add tempest ids for all functional and scenario tests\nin integration tests.\n\nChange-Id: Ib16328b1b69cd9a10f41fbc0dce1c2b0e6d7e664\n'}, {'number': 3, 'created': '2017-12-18 03:54:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/85aebe3cae5d8cff8aa0fe07cc36d898dd3f32a4', 'message': 'Add tempest ids for functional and scenario tests\n\nThis patch add tempest ids for all functional and scenario tests\nin integration tests.\n\nDepends-On: I6d6e025f60867e5128704f54e5e793762f6c1b8a\nChange-Id: Ib16328b1b69cd9a10f41fbc0dce1c2b0e6d7e664\n'}, {'number': 4, 'created': '2017-12-18 03:56:18.000000000', 'files': ['heat_integrationtests/functional/test_aws_stack.py', 'heat_integrationtests/functional/test_immutable_parameters.py', 'heat_integrationtests/functional/test_os_wait_condition.py', 'heat_integrationtests/functional/test_heat_autoscaling.py', 'heat_integrationtests/functional/test_instance_group.py', 'heat_integrationtests/functional/test_cancel_update.py', 'heat_integrationtests/functional/test_create_update_neutron_trunk.py', 'heat_integrationtests/functional/test_update_restricted.py', 'heat_integrationtests/functional/test_templates.py', 'heat_integrationtests/functional/test_conditional_exposure.py', 'heat_integrationtests/functional/test_encrypted_parameter.py', 'heat_integrationtests/functional/test_preview_update.py', 'heat_integrationtests/scenario/test_server_cfn_init.py', 'heat_integrationtests/functional/test_translation.py', 'heat_integrationtests/scenario/test_server_software_config.py', 'heat_integrationtests/functional/test_nova_server_networks.py', 'heat_integrationtests/functional/test_create_update.py', 'heat_integrationtests/functional/test_default_parameters.py', 'heat_integrationtests/functional/test_template_resource.py', 'heat_integrationtests/scenario/test_volumes.py', 'heat_integrationtests/functional/test_create_update_neutron_port.py', 'heat_integrationtests/functional/test_create_update_neutron_subnet.py', 'heat_integrationtests/functional/test_admin_actions.py', 'heat_integrationtests/functional/test_resource_group.py', 'heat_integrationtests/functional/test_snapshot_restore.py', 'heat_integrationtests/functional/test_notifications.py', 'heat_integrationtests/functional/test_purge.py', 'heat_integrationtests/functional/test_validation.py', 'heat_integrationtests/functional/test_remote_stack.py', 'heat_integrationtests/functional/test_encryption_vol_type.py', 'heat_integrationtests/functional/test_swiftsignal_update.py', 'heat_integrationtests/functional/test_software_deployment_group.py', 'heat_integrationtests/functional/test_resource_chain.py', 'heat_integrationtests/functional/test_env_merge.py', 'heat_integrationtests/functional/test_autoscaling.py', 'heat_integrationtests/scenario/test_server_signal.py', 'heat_integrationtests/functional/test_delete.py', 'heat_integrationtests/functional/test_replace_deprecated.py', 'heat_integrationtests/functional/test_conditions.py', 'heat_integrationtests/functional/test_nested_get_attr.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/1efc9ffb5da331fc4df5b9bb2527f4eb9f06f67f', 'message': 'Add tempest ids for functional and scenario tests\n\nThis patch add tempest ids for all functional and scenario tests\nin integration tests.\n\nDepends-On: I6d6e025f60867e5128704f54e5e793762f6c1b8a\nChange-Id: Ib16328b1b69cd9a10f41fbc0dce1c2b0e6d7e664\n'}]",0,506525,1efc9ffb5da331fc4df5b9bb2527f4eb9f06f67f,10,3,4,12404,,,0,"Add tempest ids for functional and scenario tests

This patch add tempest ids for all functional and scenario tests
in integration tests.

Depends-On: I6d6e025f60867e5128704f54e5e793762f6c1b8a
Change-Id: Ib16328b1b69cd9a10f41fbc0dce1c2b0e6d7e664
",git fetch https://review.opendev.org/openstack/heat refs/changes/25/506525/2 && git format-patch -1 --stdout FETCH_HEAD,"['heat_integrationtests/functional/test_aws_stack.py', 'heat_integrationtests/functional/test_event_sinks.py', 'heat_integrationtests/functional/test_hooks.py', 'heat_integrationtests/functional/test_immutable_parameters.py', 'heat_integrationtests/functional/test_os_wait_condition.py', 'heat_integrationtests/functional/test_update_restricted.py', 'heat_integrationtests/functional/test_templates.py', 'heat_integrationtests/functional/test_unicode_template.py', 'heat_integrationtests/functional/test_conditional_exposure.py', 'heat_integrationtests/functional/test_stack_events.py', 'heat_integrationtests/scenario/test_server_software_config.py', 'heat_integrationtests/functional/test_nova_server_networks.py', 'heat_integrationtests/functional/test_external_ref.py', 'heat_integrationtests/functional/test_default_parameters.py', 'heat_integrationtests/functional/test_preview.py', 'heat_integrationtests/functional/test_create_update_neutron_port.py', 'heat_integrationtests/functional/test_create_update_neutron_subnet.py', 'heat_integrationtests/functional/test_snapshot_restore.py', 'heat_integrationtests/functional/test_notifications.py', 'heat_integrationtests/functional/test_validation.py', 'heat_integrationtests/functional/test_encryption_vol_type.py', 'heat_integrationtests/functional/test_env_merge.py', 'heat_integrationtests/functional/test_autoscaling.py', 'heat_integrationtests/scenario/test_autoscaling_lbv2.py', 'heat_integrationtests/functional/test_reload_on_sighup.py', 'heat_integrationtests/functional/test_replace_deprecated.py', 'heat_integrationtests/functional/test_conditions.py', 'heat_integrationtests/functional/test_nested_get_attr.py', 'heat_integrationtests/functional/test_heat_autoscaling.py', 'heat_integrationtests/functional/test_instance_group.py', 'heat_integrationtests/functional/test_stack_outputs.py', 'heat_integrationtests/functional/test_cancel_update.py', 'heat_integrationtests/functional/test_create_update_neutron_trunk.py', 'heat_integrationtests/functional/test_software_config.py', 'heat_integrationtests/scenario/test_aodh_alarm.py', 'heat_integrationtests/functional/test_waitcondition.py', 'heat_integrationtests/common/remote_client.py', 'heat_integrationtests/functional/test_encrypted_parameter.py', 'heat_integrationtests/functional/test_preview_update.py', 'heat_integrationtests/scenario/test_autoscaling_lb.py', 'heat_integrationtests/scenario/test_server_cfn_init.py', 'heat_integrationtests/functional/test_translation.py', 'heat_integrationtests/scenario/test_base_resources.py', 'heat_integrationtests/functional/test_create_update.py', 'heat_integrationtests/functional/test_template_resource.py', 'heat_integrationtests/scenario/test_volumes.py', 'heat_integrationtests/functional/test_admin_actions.py', 'heat_integrationtests/functional/test_resource_group.py', 'heat_integrationtests/functional/test_purge.py', 'heat_integrationtests/functional/test_remote_stack.py', 'heat_integrationtests/functional/test_resources_list.py', 'heat_integrationtests/functional/test_swiftsignal_update.py', 'heat_integrationtests/functional/test_software_deployment_group.py', 'heat_integrationtests/functional/test_resource_chain.py', 'heat_integrationtests/functional/test_template_validate.py', 'heat_integrationtests/functional/test_stack_tags.py', 'heat_integrationtests/scenario/test_server_signal.py', 'heat_integrationtests/functional/test_delete.py', 'heat_integrationtests/functional/test_lbaasv2.py']",59,2c443ff4841bdf07871c5d8fc6526943302cf97c,add-tempest-ids,from tempest.lib import decorators @decorators.idempotent_id('d71895be-17aa-46ea-b007-1d47e57d0841') @decorators.idempotent_id('f56f11be-d0c5-4e2e-912d-c88c7269f5bb'),,329,8
openstack%2Fswift~master~I19529f8c43690cf076bfa235fe0433adb8e901f6,openstack/swift,master,I19529f8c43690cf076bfa235fe0433adb8e901f6,Fix order of arguments in assertEqual (patch 1/2),NEW,2017-02-16 16:01:09.000000000,2017-12-18 04:05:21.000000000,,"[{'_account_id': 330}, {'_account_id': 10068}, {'_account_id': 13052}]","[{'number': 1, 'created': '2017-02-16 16:01:09.000000000', 'files': ['test/unit/obj/test_diskfile.py', 'test/unit/common/test_utils.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/97c756fd037f2137f6a2371ced47d09f52cd4e40', 'message': 'Fix order of arguments in assertEqual (patch 1/2)\n\nSome tests used incorrect order assertEqual(observed, expected).\n\nThe correct order expected by testtools is\nassertEqual(expected, observed).\n\nThis patch includes fixes for test_diskfile and about half of the fixes for common/test_utils.py, up-to and including test_get_hmac().\n\nPartial-Bug: #1259292\nChange ID: 11bd6c82ecac8572cf9538a4d92a0361b382f1de\n\nChange-Id: I19529f8c43690cf076bfa235fe0433adb8e901f6\nSigned-off-by: Nick Miethe <nimiethe@cisco.com>\n'}]",0,434985,97c756fd037f2137f6a2371ced47d09f52cd4e40,6,3,1,25174,,,0,"Fix order of arguments in assertEqual (patch 1/2)

Some tests used incorrect order assertEqual(observed, expected).

The correct order expected by testtools is
assertEqual(expected, observed).

This patch includes fixes for test_diskfile and about half of the fixes for common/test_utils.py, up-to and including test_get_hmac().

Partial-Bug: #1259292
Change ID: 11bd6c82ecac8572cf9538a4d92a0361b382f1de

Change-Id: I19529f8c43690cf076bfa235fe0433adb8e901f6
Signed-off-by: Nick Miethe <nimiethe@cisco.com>
",git fetch https://review.opendev.org/openstack/swift refs/changes/85/434985/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/obj/test_diskfile.py', 'test/unit/common/test_utils.py']",2,97c756fd037f2137f6a2371ced47d09f52cd4e40,bug/1259292," self.assertEqual(expected, result) self.assertEqual(expected, result) self.assertEqual(expected, result) self.assertEqual(expected, result) self.assertEqual(expected, result) self.assertEqual(expected, result) self.assertEqual(expected, pid_file) self.assertEqual('test', contents) self.assertEqual('test2', contents ) self.assertEqual(False, os.path.exists(file_name)) self.assertEqual(None, utils.remove_file(file_name)) self.assertEqual(None, utils.remove_file(file_name)) self.assertEqual('0', utils.human_readable(0)) self.assertEqual('1', utils.human_readable(1)) self.assertEqual('10', utils.human_readable(10)) self.assertEqual('100', utils.human_readable(100)) self.assertEqual('999', utils.human_readable(999)) self.assertEqual('1Ki', utils.human_readable(1024)) self.assertEqual('1Ki', utils.human_readable(1535)) self.assertEqual('2Ki', utils.human_readable(1536)) self.assertEqual('1023Ki', utils.human_readable(1047552)) self.assertEqual('1023Ki', utils.human_readable(1048063)) self.assertEqual('1Mi', utils.human_readable(1048064)) self.assertEqual('1Mi', utils.human_readable(1048576)) self.assertEqual('1Gi', utils.human_readable(1073741824)) self.assertEqual('1Ti', utils.human_readable(1099511627776)) self.assertEqual('1Pi', utils.human_readable(1125899906842624)) self.assertEqual('1Ei', utils.human_readable(1152921504606846976)) self.assertEqual('1Zi', utils.human_readable(1180591620717411303424)) self.assertEqual('1Yi', utils.human_readable(1208925819614629174706176)) self.assertEqual('1024Yi', utils.human_readable(1237940039285380274899124224)) self.assertEqual('127.0.0.1', utils.rsync_ip('127.0.0.1')) utils.rsync_ip('[fe80:0000:0000:0000:0202:b3ff:fe1e:8329]', 'fe80:0000:0000:0000:0202:b3ff:fe1e:8329')) utils.rsync_ip('[::ffff:192.0.2.128]', '::ffff:192.0.2.128')) '127.0.0.1', utils.rsync_module_interpolation('{ip}', fake_device)) '11', utils.rsync_module_interpolation('{port}', fake_device)) '127.0.0.2', utils.rsync_module_interpolation('{replication_ip}', fake_device)) '12', fake_device)) '1', utils.rsync_module_interpolation('{region}', fake_device)) '2', utils.rsync_module_interpolation('{zone}', fake_device)) 'sda1', utils.rsync_module_interpolation('{device}', fake_device)) 'just_a_string', utils.rsync_module_interpolation('{meta}', fake_device)) '127.0.0.2::object', fake_device)) '127.0.0.1::container11', fake_device)) '127.0.0.2::object_sda1', utils.rsync_module_interpolation( '{replication_ip}::object_{device}', fake_device)) '127.0.0.3::object_12', '127.0.0.3::object_{replication_port}', fake_device)) self.assertEqual(0, fallocate(0, 1, 0, ctypes.c_uint64(500))) self.assertEqual(0, fallocate(0, 1, 0, ctypes.c_uint64(0))) self.assertEqual(0, fallocate(0, 1, 0, ctypes.c_uint64(0))) self.assertEqual(0, fallocate(0, 1, 0, ctypes.c_uint64(1))) self.assertEqual(0, fallocate(0, 1, 0, ctypes.c_uint64(99))) self.assertEqual(0, fallocate(0, 1, 0, ctypes.c_uint64(49))) self.assertEqual(0, fallocate(0, 1, 0, ctypes.c_uint64(999))) self.assertEqual([1234, 1, 0, 0], utils._sys_fallocate.last_call) self.assertEqual([1234, 1, 0, 0], utils._sys_fallocate.last_call) self.assertEqual([1234, 1, 0, 1], utils._sys_fallocate.last_call) self.assertEqual([1234, 1, 0, 10 * 1024 * 1024 * 1024], utils._sys_fallocate.last_call) self.assertEqual(34, len(trans_id)) self.assertEqual('tx', trans_id[:2]) self.assertEqual('-', trans_id[23]) self.assertEqual(int(fake_time), int(trans_id[24:], 16)) self.assertEqual(41, len(trans_id)) self.assertEqual('tx', trans_id[:2]) self.assertEqual('-suffix', trans_id[34:]) self.assertEqual('-', trans_id[23]) self.assertEqual(int(fake_time), int(trans_id[24:34], 16)) self.assertEqual(None, ts) self.assertEqual(1366428678, ts) 'Sat Apr 20 03:31:18 2013 UTC', time.asctime(time.gmtime(ts)) + ' UTC') self.assertEqual(1366428678, ts) 'Sat Apr 20 03:31:18 2013 UTC', time.asctime(time.gmtime(ts)) + ' UTC') self.assertEqual(None, ts) self.assertEqual(None, ts) self.assertEqual(None, ts) self.assertEqual(10, fallocate_value) self.assertEqual(10, fallocate_value) self.assertEqual('Error: ab% is an invalid value for ' 'fallocate_reserve.', str(exc)) self.assertEqual('Error: ab is an invalid value for ' 'fallocate_reserve.', str(exc)) self.assertEqual('Error: 1%% is an invalid value for ' 'fallocate_reserve.', str(exc)) self.assertEqual('Error: 10.0 is an invalid value for ' 'fallocate_reserve.', str(exc)) self.assertEqual(10.5, fallocate_value) self.assertEqual(10.000, fallocate_value) self.assertEqual(""test string"", f.read()) self.assertEqual(""test string"", f.read()) self.assertEqual(""test string\nanother string"", f.read()) self.assertEqual(""test string\nanother string"", f.read()) self.assertEqual(('text/plain', []), utils.parse_content_type('text/plain')) self.assertEqual(('text/plain', [('charset', 'utf-8')]), utils.parse_content_type('text/plain;charset=utf-8')) ('text/plain', [('hello', '""world""'), ('charset', 'utf-8')]), utils.parse_content_type('text/plain;hello=""world"";charset=utf-8')) ('text/plain', [('hello', '""world""'), ('a', 'b')]), utils.parse_content_type('text/plain; hello=""world""; a=b')) ('text/plain', [('x', r'""\""""'), ('a', 'b')]), utils.parse_content_type(r'text/plain; x=""\""""; a=b')) ('text/plain', [('x', ''), ('a', 'b')]), utils.parse_content_type(r'text/plain; x; a=b')) ('text/plain', [('x', r'""\""""'), ('a', '')]), utils.parse_content_type(r'text/plain; x=""\""""; a')) self.assertEqual(15, listing_dict['bytes']) self.assertEqual('text/plain;hello=""world""', listing_dict['content_type']) self.assertEqual(1234, listing_dict['bytes']) self.assertEqual('text/plain;hello=""world""', listing_dict['content_type']) self.assertEqual(after, utils.clean_content_type(before)) 'b17f6ff8da0e251737aa9e3ee69a881e3e092e2f', utils.get_hmac('GET', '/path', 1, 'abc'))"," self.assertEqual(result, expected) self.assertEqual(result, expected) self.assertEqual(result, expected) self.assertEqual(result, expected) self.assertEqual(result, expected) self.assertEqual(result, expected) self.assertEqual(pid_file, expected) self.assertEqual(contents, 'test') self.assertEqual(contents, 'test2') self.assertEqual(os.path.exists(file_name), False) self.assertEqual(utils.remove_file(file_name), None) self.assertEqual(utils.remove_file(file_name), None) self.assertEqual(utils.human_readable(0), '0') self.assertEqual(utils.human_readable(1), '1') self.assertEqual(utils.human_readable(10), '10') self.assertEqual(utils.human_readable(100), '100') self.assertEqual(utils.human_readable(999), '999') self.assertEqual(utils.human_readable(1024), '1Ki') self.assertEqual(utils.human_readable(1535), '1Ki') self.assertEqual(utils.human_readable(1536), '2Ki') self.assertEqual(utils.human_readable(1047552), '1023Ki') self.assertEqual(utils.human_readable(1048063), '1023Ki') self.assertEqual(utils.human_readable(1048064), '1Mi') self.assertEqual(utils.human_readable(1048576), '1Mi') self.assertEqual(utils.human_readable(1073741824), '1Gi') self.assertEqual(utils.human_readable(1099511627776), '1Ti') self.assertEqual(utils.human_readable(1125899906842624), '1Pi') self.assertEqual(utils.human_readable(1152921504606846976), '1Ei') self.assertEqual(utils.human_readable(1180591620717411303424), '1Zi') self.assertEqual(utils.human_readable(1208925819614629174706176), '1Yi') self.assertEqual(utils.human_readable(1237940039285380274899124224), '1024Yi') self.assertEqual(utils.rsync_ip('127.0.0.1'), '127.0.0.1') utils.rsync_ip('fe80:0000:0000:0000:0202:b3ff:fe1e:8329'), '[fe80:0000:0000:0000:0202:b3ff:fe1e:8329]') utils.rsync_ip('::ffff:192.0.2.128'), '[::ffff:192.0.2.128]') utils.rsync_module_interpolation('{ip}', fake_device), '127.0.0.1') utils.rsync_module_interpolation('{port}', fake_device), '11') utils.rsync_module_interpolation('{replication_ip}', fake_device), '127.0.0.2') fake_device), '12') utils.rsync_module_interpolation('{region}', fake_device), '1') utils.rsync_module_interpolation('{zone}', fake_device), '2') utils.rsync_module_interpolation('{device}', fake_device), 'sda1') utils.rsync_module_interpolation('{meta}', fake_device), 'just_a_string') fake_device), '127.0.0.2::object') fake_device), '127.0.0.1::container11') utils.rsync_module_interpolation( '{replication_ip}::object_{device}', fake_device), '127.0.0.2::object_sda1') '127.0.0.3::object_{replication_port}', fake_device), '127.0.0.3::object_12') self.assertEqual(fallocate(0, 1, 0, ctypes.c_uint64(500)), 0) self.assertEqual(fallocate(0, 1, 0, ctypes.c_uint64(0)), 0) self.assertEqual(fallocate(0, 1, 0, ctypes.c_uint64(0)), 0) self.assertEqual(fallocate(0, 1, 0, ctypes.c_uint64(1)), 0) self.assertEqual(fallocate(0, 1, 0, ctypes.c_uint64(99)), 0) self.assertEqual(fallocate(0, 1, 0, ctypes.c_uint64(49)), 0) self.assertEqual(fallocate(0, 1, 0, ctypes.c_uint64(999)), 0) self.assertEqual(utils._sys_fallocate.last_call, [1234, 1, 0, 0]) self.assertEqual(utils._sys_fallocate.last_call, [1234, 1, 0, 0]) self.assertEqual(utils._sys_fallocate.last_call, [1234, 1, 0, 1]) self.assertEqual(utils._sys_fallocate.last_call, [1234, 1, 0, 10 * 1024 * 1024 * 1024]) self.assertEqual(len(trans_id), 34) self.assertEqual(trans_id[:2], 'tx') self.assertEqual(trans_id[23], '-') self.assertEqual(int(trans_id[24:], 16), int(fake_time)) self.assertEqual(len(trans_id), 41) self.assertEqual(trans_id[:2], 'tx') self.assertEqual(trans_id[34:], '-suffix') self.assertEqual(trans_id[23], '-') self.assertEqual(int(trans_id[24:34], 16), int(fake_time)) self.assertEqual(ts, None) self.assertEqual(ts, 1366428678) time.asctime(time.gmtime(ts)) + ' UTC', 'Sat Apr 20 03:31:18 2013 UTC') self.assertEqual(ts, 1366428678) time.asctime(time.gmtime(ts)) + ' UTC', 'Sat Apr 20 03:31:18 2013 UTC') self.assertEqual(ts, None) self.assertEqual(ts, None) self.assertEqual(ts, None) self.assertEqual(fallocate_value, 10) self.assertEqual(fallocate_value, 10) self.assertEqual(str(exc), 'Error: ab% is an invalid value for ' 'fallocate_reserve.') self.assertEqual(str(exc), 'Error: ab is an invalid value for ' 'fallocate_reserve.') self.assertEqual(str(exc), 'Error: 1%% is an invalid value for ' 'fallocate_reserve.') self.assertEqual(str(exc), 'Error: 10.0 is an invalid value for ' 'fallocate_reserve.') self.assertEqual(fallocate_value, 10.5) self.assertEqual(fallocate_value, 10.000) self.assertEqual(f.read(), ""test string"") self.assertEqual(f.read(), ""test string"") self.assertEqual(f.read(), ""test string\nanother string"") self.assertEqual(f.read(), ""test string\nanother string"") self.assertEqual(utils.parse_content_type('text/plain'), ('text/plain', [])) self.assertEqual(utils.parse_content_type('text/plain;charset=utf-8'), ('text/plain', [('charset', 'utf-8')])) utils.parse_content_type('text/plain;hello=""world"";charset=utf-8'), ('text/plain', [('hello', '""world""'), ('charset', 'utf-8')])) utils.parse_content_type('text/plain; hello=""world""; a=b'), ('text/plain', [('hello', '""world""'), ('a', 'b')])) utils.parse_content_type(r'text/plain; x=""\""""; a=b'), ('text/plain', [('x', r'""\""""'), ('a', 'b')])) utils.parse_content_type(r'text/plain; x; a=b'), ('text/plain', [('x', ''), ('a', 'b')])) utils.parse_content_type(r'text/plain; x=""\""""; a'), ('text/plain', [('x', r'""\""""'), ('a', '')])) self.assertEqual(listing_dict['bytes'], 15) self.assertEqual(listing_dict['content_type'], 'text/plain;hello=""world""') self.assertEqual(listing_dict['bytes'], 1234) self.assertEqual(listing_dict['content_type'], 'text/plain;hello=""world""') self.assertEqual(utils.clean_content_type(before), after) utils.get_hmac('GET', '/path', 1, 'abc'), 'b17f6ff8da0e251737aa9e3ee69a881e3e092e2f')",133,136
openstack%2Fpython-ironicclient~master~I906b065a94bcef1403944190e7db57d88cd23d87,openstack/python-ironicclient,master,I906b065a94bcef1403944190e7db57d88cd23d87,ETAGs for node in ironicclient,NEW,2016-11-21 16:42:06.000000000,2017-12-18 04:04:34.000000000,,"[{'_account_id': 19213}, {'_account_id': 22724}]","[{'number': 1, 'created': '2016-11-21 16:42:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/f594ef9abebf6f06a2de82c02874f4cd2c286f4b', 'message': 'WIP: Full-featured objects. POC\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource. To\navoid caching, the concept of full-fetured objects\nhas been proposed.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 2, 'created': '2016-11-22 15:06:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/737a48b3f543b74f81d069df7ca4b00b1a7a00ed', 'message': 'WIP: Full-featured objects. POC\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource. To\navoid caching, the concept of full-fetured objects\nhas been proposed.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 3, 'created': '2016-11-22 15:57:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/5dad4f9a5e371cb26fd090b3345dc03803da319d', 'message': 'WIP: Full-featured objects. POC\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource. To\navoid caching, the concept of full-fetured objects\nhas been proposed.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 4, 'created': '2016-11-25 13:43:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/e0f6b58735d184fd7eae729c9386f2a2332a6ff1', 'message': 'WIP: Full-featured objects. POC\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource. To\navoid caching, the concept of full-fetured objects\nhas been proposed.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 5, 'created': '2016-11-30 10:28:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/381e045178fe15ee119f089b2d3997eeb5b58760', 'message': 'POC: Full-featured objects with ETAGs supporting\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource. To\navoid caching, the concept of full-fetured objects\nhas been proposed.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 6, 'created': '2016-12-16 15:56:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/d3b84b4a3188cd71e5a8c9106030fbe7738362b8', 'message': '[WIP] POC: Full-featured objects with ETAGs supporting\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 7, 'created': '2016-12-19 17:41:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/4f0793c258245eab6b8e75a475d7886d889e3fe9', 'message': '[WIP] POC: Full-featured objects with ETAGs supporting\n\nCherry-picked.\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 8, 'created': '2016-12-20 09:07:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/4f36f0aede39e57bca9b505bfae2a1f49b1c997f', 'message': '[WIP] POC: Full-featured objects with ETAGs supporting\n\nCherry-picked.\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 9, 'created': '2016-12-20 10:27:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/4b4d34f1b2d951d879257f7f4799e3d046c56c1d', 'message': '[WIP] ETAGs for node in ironicclient\n\nCherry-picked.\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 10, 'created': '2016-12-23 12:04:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/f6ecd6116e3c1703a825182578a6c295a7fa3b30', 'message': '[WIP] ETAGs for node in ironicclient\n\nCherry-picked.\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\nDepends-On: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\n'}, {'number': 11, 'created': '2016-12-26 17:21:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/d25fffc53718eaab7a1004e4e088fd7d1e6db730', 'message': '[WIP] ETAGs for node in ironicclient\n\nCherry-picked.\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\nDepends-On: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\n'}, {'number': 12, 'created': '2016-12-28 15:01:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/3f1e5f9c4ec8b47b5c02179d7293efc1603a3446', 'message': '[WIP] ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own representation, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\nDepends-On: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\n'}, {'number': 13, 'created': '2016-12-29 13:08:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/1a159f9f110956633055edf26aed0f67b71e65a8', 'message': '[WIP] ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own representation, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\nDepends-On: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\n'}, {'number': 14, 'created': '2017-01-03 14:58:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/d16507297f0c14d9dc169c41d0eeaa0ee8ac82ac', 'message': '[WIP] ETAGs for node in ironicclient\n\nWork in progress. As the spec is not approved,\nthis is the investigation of the best way to\nimplement ETAGs storing in ironic CLI.\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\nobject can be updated through Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nAttempt to use the object representation instead of caching.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\nDepends-On: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\n'}, {'number': 15, 'created': '2017-01-04 09:52:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/c72d1fdc555f01395271ca4b7b8046793b810c56', 'message': '[WIP] ETAGs for node in ironicclient\n\nAs the spec is not approved,this is the\ninvestigation of the best way to implement\nETAGs storing in ironic CLI.\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\nobject can be updated through Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nAttempt to use the object representation instead of caching.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\nDepends-On: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\n'}, {'number': 16, 'created': '2017-01-05 13:07:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/6d23fc9a30518788712f15de6e79c998985561d1', 'message': '[WIP] ETAGs for node in ironicclient\n\nAs the spec is not approved,this is the\ninvestigation of the best way to implement\nETAGs storing in ironic CLI.\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\nobject can be updated through Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nAttempt to use the object representation instead of caching.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 17, 'created': '2017-01-06 11:38:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/d775cca2c9498eaea88f5104eb5614e384aa2b7e', 'message': '[WIP] ETAGs for node in ironicclient\n\nAs the spec is not approved,this is the\ninvestigation of the best way to implement\nETAGs storing in ironic CLI.\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\nobject can be updated through Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\n\nAttempt to use the object representation instead of caching.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 18, 'created': '2017-01-12 16:03:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/27bfbcdd3417cf1afbc55aab437598329778d881', 'message': '[WIP] ETAGs for node in ironicclient\n\nCherry-picked.\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\nDepends-On: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\n'}, {'number': 19, 'created': '2017-01-16 15:45:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/d4abf29c39ccdc666bbf86a80c5705177cf739c9', 'message': '[WIP] ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 20, 'created': '2017-01-27 12:26:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/07ce22ddb60d456195d02a2ea5afd6ecf1632a8a', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 21, 'created': '2017-01-31 10:59:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/aac3c3d6ba71319fc0bfb241f541786c7da535fa', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 22, 'created': '2017-01-31 13:22:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/0e13af42a9cd8101935fb63204f62c9cee113089', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 23, 'created': '2017-01-31 13:54:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/36d21d1f069770c12afcd15a9ab5f2d9bf5b69b2', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 24, 'created': '2017-01-31 14:01:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/3232e4877b886e661300891e6b9f30eede62a259', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 25, 'created': '2017-01-31 14:38:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/604f5c204fa67d7547fce4fee4ee04fc86e2ae38', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 26, 'created': '2017-01-31 15:35:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/db0ecab8182e96dd7ef7aee0ee835827ebce1c1c', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 27, 'created': '2017-02-09 12:31:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/19bd9f643b7c41f3690a931440e195ce9ebee4c4', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 28, 'created': '2017-02-09 16:25:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/9eb29a06aec7ca49516ba24f669d4a12fe7b5ab8', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 29, 'created': '2017-02-13 12:43:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/6ba4909c8de87de3631a76883b35ecfff39a47df', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user does get request to Ironic API, the Resource\ncan update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\nThe etag is common for entire resource including all\nsubresources.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 30, 'created': '2017-02-15 13:57:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/8cec79f295372caf0e2d1a8ec5074afcc397106f', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user script does get request to Ironic API,\nthe Resource can update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 31, 'created': '2017-02-15 13:58:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/d51fd182565c955b5837338268b0b4b1e250f433', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user script does get request to Ironic API,\nthe Resource can update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\n\nAdded etag option to update operation from shell as proposition\nto use etags in shell somehow.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}, {'number': 32, 'created': '2017-02-17 14:08:43.000000000', 'files': ['ironicclient/v1/node.py', 'ironicclient/v1/node_shell.py', 'ironicclient/tests/unit/v1/test_node_shell.py', 'ironicclient/common/http.py', 'ironicclient/tests/unit/v1/test_node.py', 'ironicclient/tests/unit/utils.py', 'ironicclient/v1/resource_fields.py', 'ironicclient/common/utils.py', 'ironicclient/common/base.py'], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/50ec633bd19d57698c0720a6f870817616d00d86', 'message': 'ETAGs for node in ironicclient\n\nThere is an issue in the ETAG spec that client\nshould store representation of the resource.\n\nThe full-featured Resource should do user actions\nwith itself and call Manager to do the work.\nWhen user script does get request to Ironic API,\nthe Resource can update own etag, got from Manager.\nWhen doing update request, the user can go with etag\nstored in Resource object. If this etag is old, the client\nwill fail and force the user either to stop updating or\ndo get request again to update the etag.\n\nAdded etag option to update operation from shell as proposition\nto use etags in shell somehow.\n\nChange-Id: I906b065a94bcef1403944190e7db57d88cd23d87\nRelated-Bug: 1605728\n'}]",5,400335,50ec633bd19d57698c0720a6f870817616d00d86,77,2,32,22724,,,0,"ETAGs for node in ironicclient

There is an issue in the ETAG spec that client
should store representation of the resource.

The full-featured Resource should do user actions
with itself and call Manager to do the work.
When user script does get request to Ironic API,
the Resource can update own etag, got from Manager.
When doing update request, the user can go with etag
stored in Resource object. If this etag is old, the client
will fail and force the user either to stop updating or
do get request again to update the etag.

Added etag option to update operation from shell as proposition
to use etags in shell somehow.

Change-Id: I906b065a94bcef1403944190e7db57d88cd23d87
Related-Bug: 1605728
",git fetch https://review.opendev.org/openstack/python-ironicclient refs/changes/35/400335/8 && git format-patch -1 --stdout FETCH_HEAD,"['ironicclient/v1/node.py', 'ironicclient/v1/resource_fields.py']",2,f594ef9abebf6f06a2de82c02874f4cd2c286f4b,bug/1605728, 'etag': 'ETag',,109,4
openstack%2Fkuryr~master~Ie74b4e31229aab897fa852a08c79a326baec1b6a,openstack/kuryr,master,Ie74b4e31229aab897fa852a08c79a326baec1b6a,Add devref for overlapping-cidrs,NEW,2016-06-08 07:59:06.000000000,2017-12-18 04:04:16.000000000,,"[{'_account_id': 1923}, {'_account_id': 6598}, {'_account_id': 11343}, {'_account_id': 12069}, {'_account_id': 13555}, {'_account_id': 14352}, {'_account_id': 15967}, {'_account_id': 19726}, {'_account_id': 20566}]","[{'number': 1, 'created': '2016-06-08 07:59:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr/commit/f93f15bddb226dc37e2ecc40c733747e93491c3f', 'message': 'Add devref for overlapping-cidrs\n\nThis patch describes about handling of overlapping cidrs and\napproach for integration of libnetwork address-spaces.\n\nChange-Id: Ie74b4e31229aab897fa852a08c79a326baec1b6a\nRelated: blueprint address-scopes-spaces\n'}, {'number': 2, 'created': '2016-06-08 08:05:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr/commit/bbe86247dd77acf6abd14d6ee28721af2111b5c7', 'message': 'Add devref for overlapping-cidrs\n\nThis patch describes about handling of overlapping cidrs and\napproach for integration of libnetwork address-spaces.\n\nChange-Id: Ie74b4e31229aab897fa852a08c79a326baec1b6a\nRelated: blueprint address-scopes-spaces\n'}, {'number': 3, 'created': '2016-06-13 11:10:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr/commit/cd9c7973a01934031bd4a24a25fcf7c208daacc1', 'message': 'Add devref for overlapping-cidrs\n\nThis patch describes about handling of overlapping cidrs and\napproach for integration of libnetwork address-spaces.\n\nChange-Id: Ie74b4e31229aab897fa852a08c79a326baec1b6a\nRelated: blueprint address-scopes-spaces\n'}, {'number': 4, 'created': '2016-06-13 11:17:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr/commit/aaf11b1cd89e1bff7588bbfcd3f2cace99301b4b', 'message': 'Add devref for overlapping-cidrs\n\nThis patch describes about handling of overlapping cidrs and\napproach for integration of libnetwork address-spaces.\n\nChange-Id: Ie74b4e31229aab897fa852a08c79a326baec1b6a\nRelated: blueprint address-scopes-spaces\n'}, {'number': 5, 'created': '2016-06-14 09:03:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr/commit/c2780a9ea1f181989347c5ba4b44d69f35948f4c', 'message': 'Add devref for overlapping-cidrs\n\nThis patch describes about handling of overlapping cidrs and\napproach for integration of libnetwork address-spaces.\n\nChange-Id: Ie74b4e31229aab897fa852a08c79a326baec1b6a\nRelated: blueprint address-scopes-spaces\n'}, {'number': 6, 'created': '2016-06-14 10:46:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr/commit/01517996b4e5c058d6bc4c850dc4ca9b0df170ce', 'message': 'Add devref for overlapping-cidrs\n\nThis patch describes about handling of overlapping cidrs and\napproach for integration of libnetwork address-spaces.\n\nChange-Id: Ie74b4e31229aab897fa852a08c79a326baec1b6a\nRelated: blueprint address-scopes-spaces\n'}, {'number': 7, 'created': '2016-07-19 06:18:15.000000000', 'files': ['doc/source/devref/overlapping_cidrs.rst', 'doc/source/devref/index.rst'], 'web_link': 'https://opendev.org/openstack/kuryr/commit/b8f4da0f034f8f45b076ac62dd1ce73ca4fb2fe4', 'message': 'Add devref for overlapping-cidrs\n\nThis patch describes about handling of overlapping cidrs and\napproach for integration of libnetwork address-spaces.\n\nChange-Id: Ie74b4e31229aab897fa852a08c79a326baec1b6a\nRelated: blueprint address-scopes-spaces\n'}]",54,326894,b8f4da0f034f8f45b076ac62dd1ce73ca4fb2fe4,22,9,7,15967,,,0,"Add devref for overlapping-cidrs

This patch describes about handling of overlapping cidrs and
approach for integration of libnetwork address-spaces.

Change-Id: Ie74b4e31229aab897fa852a08c79a326baec1b6a
Related: blueprint address-scopes-spaces
",git fetch https://review.opendev.org/openstack/kuryr refs/changes/94/326894/7 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/devref/overlapping_cidrs.rst'],1,f93f15bddb226dc37e2ecc40c733747e93491c3f,bp/address-scopes-spaces," .. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ================= Overlapping CIDRs ================= https://blueprints.launchpad.net/kuryr/+spec/address-scopes-spaces This spec describes the implementation approach for handling docker/libnetwork request for creation of subnets with same cidr. Problem Description =================== In docker, notion of network address namespaces(non-overlapping cidrs) is managed in the form of 'addressSpaces'. Currently docker UI(cli or Rest APIs) does not provide an option to create networks belonging to their choice of address-space. Just after remote driver activation, libnetwork queries capability scope and default address-space from remote network driver and remote ipam driver using /NetworkDriver.GetCapabilities and /IpamDriver.GetDefaultAddressSpaces. Based on capability received from network driver,(by design [1]) either localDefaultAddressSpace or globalDefaultAddressSpace is passed to remote drivers along with all pool creation requests and network creation requests. But these was a bug in libnetwork and this was not being passed, which recently got fixed[2]. Docker/libnetwork then passes these network creation requests to Kuryr drivers transparently with the assumption that remote drivers will take care of addressing conflicts(overlapping cidrs) with their own implementations of address-spaces. Now, problem arises when docker user tries to create two docker networks with subnets having same cidr and Kuryr as remote network driver. Current Kuryr implementation has not considered/used libnetwork address-spaces. Kuryr ipam driver creates per-network address pools(neutron subnetpools). Ipam driver returns PoolID to libnetwork in response to each pool creation request. But in /NetworkDriver.CreateNetwork api to Kuryr, libnetwork does not pass PoolIds sent previously by ipam driver rather libnetwork passes addressSpace that this network belongs to. This seems to be by design and chances are very high that PoolIds will not be passed through libnetwork apis. Summarizing the problem, libnetwork assumes addressScopes to be common/association attribute between network driver and ipam driver for figuring out the correct subnet for a network in case there are multiple subnets(belonging to multiple subnetpools) with same cidr. Proposed Change =============== basic idea ---------- The idea is quite simple: * Map one-to-one docker address-spaces with neutron address-scopes, [3]. * To support backward compatibility and networking backends who do not support address-scopes yet, query docker out-of-band for poolID using docker client. Detailed design --------------- It will be determined and stored in a global boolean variable if neutron supports addressScopes extension(same as we do currently for 'tag' extension). At pool creation, if libnetwork has passed non-default address-space and addressScopes extension not present, exception will be raised. Otherwise a corresponding neutron address-scope will be created. At network creation, if address-space that is received in libnetwork api is default, get network info from docker using docker-client api. From the received response, get PoolID using cidr. This PoolID will be used to create neutron subnet using --subnetpool option. At request address, libnetwork passes PoolID to ipam driver api. Subnets can be filtered with cidr and poolID now. Challenges ---------- Currently, docker network api does not show PoolIDs. PR in docker is under review for this enhancement. See [4] & [5] References ========== [1] https://github.com/docker/libnetwork/issues/489 [2] https://github.com/docker/libnetwork/pull/1202 [3] http://specs.openstack.org/openstack/neutron-specs/specs/liberty/address-scopes.html [4] https://github.com/docker/docker/pull/23163 [5] https://github.com/docker/engine-api/pull/247 ",,94,0
openstack%2Fmonasca-agent~master~I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682,openstack/monasca-agent,master,I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682,Spark monitoring plugin agent,NEW,2016-09-27 13:34:57.000000000,2017-12-18 04:04:06.000000000,,"[{'_account_id': 5559}, {'_account_id': 9276}, {'_account_id': 11580}, {'_account_id': 11809}, {'_account_id': 16168}, {'_account_id': 17681}, {'_account_id': 20033}]","[{'number': 1, 'created': '2016-09-27 13:34:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/9e6280bd0e2bc69878a33abd7f8320bf23e65872', 'message': 'WIP Spark monitoring plugin agent\n\nUnit tests depends on real Spark history server with at least 2\napplications\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 2, 'created': '2016-09-27 17:58:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/2bcc4d9f0d0be684cb9f32ff241bb6106127a31c', 'message': 'WIP Spark monitoring plugin agent\n\nBasic API access\n\nUnit tests depends on real Spark history server with at least 2\napplications\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 3, 'created': '2016-09-30 21:18:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/176f57d2e17744ea589ad019b6a15b8ebfbba961', 'message': 'WIP Spark monitoring plugin agent\n\nBasic API access\nUnit tests depends on real Spark history server with at least 2\napplications\nFetch data from api having last check as minDate\nImproving timestamp and get application duration\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 4, 'created': '2016-09-30 22:22:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/47197c63afe340979bc32077b65dcac522c0c741', 'message': 'WIP Spark monitoring plugin agent\n\nBasic API access\nUnit tests depends on real Spark history server with at least 2\napplications\nFetch data from api having last check as minDate\nImproving timestamp and get application duration\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 5, 'created': '2016-09-30 22:32:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/1d52867789ae0932aefd21ad4ec5e6275bb65602', 'message': 'WIP Spark monitoring plugin agent\n\nBasic API access\nUnit tests depends on real Spark history server with at least 2\napplications\nFetch data from api having last check as minDate\nImproving timestamp and get application duration\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 6, 'created': '2016-10-03 21:41:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/8bcd14d11b183f34aef0ba612c9151f900e3960f', 'message': 'WIP Spark monitoring plugin agent\n\nBasic API access\nUnit tests depends on real Spark history server with at least 2\napplications\nFetch data from api having last check as minDate\nImproving timestamp and get application duration\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 7, 'created': '2016-10-04 13:58:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/ccbaa0da7236ba146037e6b1feb261faabc600a3', 'message': 'WIP Spark monitoring plugin agent\n\nBasic API access\nUnit tests depends on real Spark history server with at least 2\napplications\nFetch data from api having last check as minDate\nImproving timestamp and get application duration\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 8, 'created': '2016-10-04 17:28:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/7247bd4fe005ae025f2f45df3af128ab08190bf3', 'message': 'WIP Spark monitoring plugin agent\n\nBasic API access\nUnit tests depends on real Spark history server with at least 2\napplications\nFetch data from api having last check as minDate\nImproving timestamp and get application duration\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 9, 'created': '2016-10-04 18:05:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/694982887eee5ed98a121b3cf1e43ea8d0887d8f', 'message': 'WIP Spark monitoring plugin agent\n\nBasic API access\nUnit tests depends on real Spark history server with at least 2\napplications\nFetch data from api having last check as minDate\nImproving timestamp and get application duration\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 10, 'created': '2016-10-05 21:03:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/e7ceee9ad604f228a9f23399583c82cf6c3e17c3', 'message': 'WIP Spark monitoring plugin agent\n\nBasic API access\nUnit tests depends on real Spark history server with at least 2\napplications\nFetch data from api having last check as minDate\nImproving timestamp and get application duration\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 11, 'created': '2016-10-06 21:07:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/da92909f2c398511e9bea5693219d10f464577a4', 'message': 'WIP Spark monitoring plugin agent\n\nBasic API access\nUnit tests depends on real Spark history server with at least 2\napplications\nFetch data from api having last check as minDate\nImproving timestamp and get application duration\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 12, 'created': '2016-10-07 13:52:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/953256d41dc03622dedc53b715724c44a4d4a9fd', 'message': 'Spark monitoring plugin agent\n\nUse spark-history API to collect data\nFetch data from API using the last check date as filter for collect\nfinished applications\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 13, 'created': '2016-10-07 18:39:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/d02681e024ab424f7e454f062102e8d14635a8b0', 'message': 'Spark monitoring plugin agent\n\nUse spark-history API to collect data\nFetch data from API using the last check date as filter for collect\nfinished applications\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 14, 'created': '2016-10-10 13:40:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/e2ee739020fbb15041d1ab6a9d6dda9b562b68ee', 'message': 'Spark monitoring plugin agent\n\nUse spark-history API to collect data\nFetch data from API using the last check date as filter for collect\nfinished applications\nSimple plugin detection\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 15, 'created': '2016-10-10 15:50:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/a2b281348b5903d6e9a54f6575aaf3a1eeadbfff', 'message': 'Spark monitoring plugin agent\n\nUse spark-history API to collect data\nFetch data from API using the last check date as filter for collect\nfinished applications\nSimple plugin detection\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 16, 'created': '2016-10-13 20:27:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/2abc1510768266a830259a1ac1474cf88b9e48de', 'message': 'Spark monitoring plugin agent\n\nUse spark-history API to collect data\nFetch data from API using the last check date as filter for collect\nfinished applications\nSimple plugin detection\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 17, 'created': '2017-02-03 16:31:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/657698c958bdcaa5b997d6e509f2198c43fa6d9e', 'message': 'Spark monitoring plugin agent\n\nUse spark-history API to collect data\nFetch data from API using the last check date as filter for collect\nfinished applications\nSimple plugin detection\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 18, 'created': '2017-02-03 18:34:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/9e502d3bbc9215da95dc91e0460d2bc3e26999b1', 'message': 'Spark monitoring plugin agent\n\nUse spark-history API to collect data\nFetch data from API using the last check date as filter for collect\nfinished applications\nSimple plugin detection\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 19, 'created': '2017-02-06 19:03:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/a05fdf9ddf8e2dc32dc34d152f209f3eff0d4a65', 'message': 'Spark monitoring plugin agent\n\nUse spark-history API to collect data\nFetch data from API using the last check date as filter for collect\nfinished applications\nSimple plugin detection\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 20, 'created': '2017-02-07 21:10:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/7f718ebc2fdf0777f5e1371970c219e8ac9b8558', 'message': 'Spark monitoring plugin agent\n\nUse spark-history API to collect data\nFetch data from API using the last check date as filter for collect\nfinished applications\nSimple plugin detection\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 21, 'created': '2017-02-08 16:03:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/e11b978b44bb86202b21e1b61730a0a8bbc66425', 'message': 'Spark monitoring plugin agent\n\nUse spark-history API to collect data\nFetch data from API using the last check date as filter for collect\nfinished applications\nSimple plugin detection\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 22, 'created': '2017-02-21 05:38:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/8e777c9d439ce9bf10b90192b846cb4491a00bdb', 'message': 'Spark monitoring plugin agent\n\nUse spark-history API to collect data\nFetch data from API using the last check date as filter for collect\nfinished applications\nSimple plugin detection\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}, {'number': 23, 'created': '2017-02-21 14:43:03.000000000', 'files': ['monasca_setup/detection/utils.py', 'tests/detection/test_spark.py', 'monasca_agent/collector/checks_d/spark.py', 'tests/checks_d/test_spark.py', 'conf.d/spark.yaml.example', 'tests/checks_d/fixtures/test_spark.json', 'monasca_setup/detection/plugins/spark.py', 'docs/Plugins.md', 'monasca_setup/detection/plugins/kafka_consumer.py'], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/c393d446354e29072a3fbadd8b91f80be90cfa63', 'message': 'Spark monitoring plugin agent\n\nUse spark-history API to collect data\nFetch data from API using the last check date as filter for collect\nfinished applications\nSimple plugin detection\n\nChange-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682\n'}]",43,377698,c393d446354e29072a3fbadd8b91f80be90cfa63,87,7,23,5559,,,0,"Spark monitoring plugin agent

Use spark-history API to collect data
Fetch data from API using the last check date as filter for collect
finished applications
Simple plugin detection

Change-Id: I90b5fe44b58e7995aceaf90c5b6fa43b54a2e682
",git fetch https://review.opendev.org/openstack/monasca-agent refs/changes/98/377698/17 && git format-patch -1 --stdout FETCH_HEAD,"['conf.d/spark.yaml.example', 'monasca_agent/collector/checks_d/spark.py', 'tests/checks_d/test_spark.py', 'tests/checks_d/fixtures/test_spark.json']",4,9e6280bd0e2bc69878a33abd7f8320bf23e65872,spark_agent_plugin,"{ ""applications"": [ { ""attempts"": [ { ""attemptId"": ""1"", ""completed"": true, ""endTime"": ""2016-09-26T17:03:03.787GMT"", ""sparkUser"": ""ubuntu"", ""startTime"": ""2016-09-26T16:50:32.952GMT"" } ], ""id"": ""application_1474894502291_0024"", ""name"": ""Application-1"" }, { ""attempts"": [ { ""attemptId"": ""1"", ""completed"": true, ""endTime"": ""2016-09-26T16:49:23.241GMT"", ""sparkUser"": ""ubuntu"", ""startTime"": ""2016-09-26T16:36:48.652GMT"" } ], ""id"": ""application_1474894502291_0023"", ""name"": ""Application-2"" }, { ""attempts"": [ { ""attemptId"": ""1"", ""completed"": true, ""endTime"": ""2016-09-26T16:35:37.832GMT"", ""sparkUser"": ""ubuntu"", ""startTime"": ""2016-09-26T16:21:58.800GMT"" } ], ""id"": ""application_1474894502291_0022"", ""name"": ""Application-3"" } ] } ",,135,0
openstack%2Ftacker~master~Icc685950971f8b0178e799bdc88aefee95781f20,openstack/tacker,master,Icc685950971f8b0178e799bdc88aefee95781f20,Function to upload OpenWRT sample VNFD during Tacker devstack installation,NEW,2016-03-08 08:13:45.000000000,2017-12-18 04:03:53.000000000,,"[{'_account_id': 2874}, {'_account_id': 13380}, {'_account_id': 13485}, {'_account_id': 15755}, {'_account_id': 18955}, {'_account_id': 19690}]","[{'number': 1, 'created': '2016-03-08 08:13:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/3cc60979dc92972aca6a763419f06273a8c6c9bc', 'message': 'Function to upload OpenWRT sample VNFD during Tacker devstack installation\n\nAdding this function will automatically download\nand upload the openwrt sample vnfd to Tacker VNF Catalog when the\ntacker plugin is enabled in the local.conf file.\n\nChange-Id: Icc685950971f8b0178e799bdc88aefee95781f20\nCloses-Bug: #1543377\n'}, {'number': 2, 'created': '2016-03-09 06:21:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/d41c2f3b6196aa84f20e701158ba09d4a9da49de', 'message': 'Function to upload OpenWRT sample VNFD during Tacker devstack installation\n\nAdding this function will automatically download\nand upload the openwrt sample vnfd to Tacker VNF Catalog when the\ntacker plugin is enabled in the local.conf file.\n\nChange-Id: Icc685950971f8b0178e799bdc88aefee95781f20\nCloses-Bug: #1543377\n'}, {'number': 3, 'created': '2016-03-10 11:01:37.000000000', 'files': ['devstack/lib/tacker', 'devstack/plugin.sh', 'devstack/samples/openwrt.yaml'], 'web_link': 'https://opendev.org/openstack/tacker/commit/674587bb91ddc847e0e1386b6577c9685608af0f', 'message': 'Function to upload OpenWRT sample VNFD during Tacker devstack installation\n\nAdding this function will automatically download\nand upload the openwrt sample vnfd to Tacker VNF Catalog when the\ntacker plugin is enabled in the local.conf file.\n\nChange-Id: Icc685950971f8b0178e799bdc88aefee95781f20\nCloses-Bug: #1543377\n'}]",7,289776,674587bb91ddc847e0e1386b6577c9685608af0f,18,6,3,19690,,,0,"Function to upload OpenWRT sample VNFD during Tacker devstack installation

Adding this function will automatically download
and upload the openwrt sample vnfd to Tacker VNF Catalog when the
tacker plugin is enabled in the local.conf file.

Change-Id: Icc685950971f8b0178e799bdc88aefee95781f20
Closes-Bug: #1543377
",git fetch https://review.opendev.org/openstack/tacker refs/changes/76/289776/1 && git format-patch -1 --stdout FETCH_HEAD,"['devstack/lib/tacker', 'devstack/plugin.sh', 'devstack/samples/openwrt.yaml']",3,3cc60979dc92972aca6a763419f06273a8c6c9bc,, user_data_format: RAW user_data: | #!/bin/sh sed -i -e 's/option type bridge/ /g' /etc/config/network /etc/config/network reload ,,24,0
openstack%2Fswift~master~Ib7affafcf7c13193b523e983711461f153a6bc93,openstack/swift,master,Ib7affafcf7c13193b523e983711461f153a6bc93,Proxy server controller tests should be reorganized,NEW,2016-12-28 14:42:38.000000000,2017-12-18 04:03:28.000000000,,"[{'_account_id': 1179}, {'_account_id': 7847}, {'_account_id': 9625}, {'_account_id': 13052}, {'_account_id': 20764}]","[{'number': 1, 'created': '2016-12-28 14:42:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/12eea7cb5e2fa396e2c410a04e757203971afd50', 'message': 'Proxy server controller tests should be reoganized\n\nFollowing tests were moved:\n- @patch_policies within test_server.TestObjectController -> controllers/test_obj.py\n- @patch_policies within test_server.TestContainerController -> controllers/test_container.py\n- @patch_policies within test_server.TestAccountController -> controllers/test_account.py\n\n- Lots of refactor and cleanup\n\nCloses-Bug: #1646963\n\nChange-Id: Ib7affafcf7c13193b523e983711461f153a6bc93\nSigned-off-by: Sachin Patil <psachin@redhat.com>\n'}, {'number': 2, 'created': '2017-01-02 04:53:15.000000000', 'files': ['test/unit/proxy/test_mem_server.py', 'test/unit/proxy/controllers/test_account.py', 'test/unit/proxy/test_server.py', 'test/unit/proxy/controllers/test_obj.py', 'test/unit/proxy/controllers/test_container.py', 'test/unit/proxy/__init__.py', 'test/unit/common/middleware/test_copy.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/16c36ce82259fdf82686492d3447dc84b2070ee7', 'message': 'Proxy server controller tests should be reorganized\n\nFollowing tests were moved:\n- @patch_policies within test_server.TestObjectController -> controllers/test_obj.py\n- @patch_policies within test_server.TestContainerController -> controllers/test_container.py\n- @patch_policies within test_server.TestAccountController -> controllers/test_account.py\n\n- Lots of refactor and cleanup\n\nCloses-Bug: #1646963\n\nChange-Id: Ib7affafcf7c13193b523e983711461f153a6bc93\nSigned-off-by: Sachin Patil <psachin@redhat.com>\n'}]",0,415473,16c36ce82259fdf82686492d3447dc84b2070ee7,11,5,2,20764,,,0,"Proxy server controller tests should be reorganized

Following tests were moved:
- @patch_policies within test_server.TestObjectController -> controllers/test_obj.py
- @patch_policies within test_server.TestContainerController -> controllers/test_container.py
- @patch_policies within test_server.TestAccountController -> controllers/test_account.py

- Lots of refactor and cleanup

Closes-Bug: #1646963

Change-Id: Ib7affafcf7c13193b523e983711461f153a6bc93
Signed-off-by: Sachin Patil <psachin@redhat.com>
",git fetch https://review.opendev.org/openstack/swift refs/changes/73/415473/2 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/proxy/test_mem_server.py', 'test/unit/proxy/controllers/test_account.py', 'test/unit/proxy/test_server.py', 'test/unit/proxy/controllers/test_obj.py', 'test/unit/proxy/controllers/test_container.py', 'test/unit/proxy/__init__.py', 'test/unit/common/middleware/test_copy.py']",7,12eea7cb5e2fa396e2c410a04e757203971afd50,bug/1646963,"from test.unit.proxy.controllers.test_obj import ( set_http_connect_contextmgr, PatchedObjControllerApp) with set_http_connect_contextmgr( *status_codes, body_iter=body_iter, headers=headers, expect_headers=expect_headers, give_connect=capture_conn): with set_http_connect_contextmgr( *status_codes, body_iter=body_iter, headers=headers, expect_headers=expect_headers):","from test.unit.proxy.controllers.test_obj import set_http_connect, \ PatchedObjControllerApp with set_http_connect(*status_codes, body_iter=body_iter, headers=headers, expect_headers=expect_headers, give_connect=capture_conn): with set_http_connect(*status_codes, body_iter=body_iter, headers=headers, expect_headers=expect_headers):",4436,2350
openstack%2Fdevstack-gate~master~I35d7c1a3d07ee6c45057cf23705e137bfbbb28d2,openstack/devstack-gate,master,I35d7c1a3d07ee6c45057cf23705e137bfbbb28d2,SERVICE_HOST should not be 127.0.0.1,NEW,2016-03-17 09:08:38.000000000,2017-12-18 04:03:16.000000000,,"[{'_account_id': 1106}, {'_account_id': 4146}, {'_account_id': 12053}]","[{'number': 1, 'created': '2016-03-17 09:08:38.000000000', 'files': ['devstack-vm-gate.sh'], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/72e8742794bb70f42519f618c3b44ed1ef759191', 'message': 'SERVICE_HOST should not be 127.0.0.1\n\nERVICE_HOST should not be 127.0.0.1 in devstack-vm-gate.sh.\nIf SERVICE_HOST is 127.0.0.1, then services in a vm can not\naccess these OpenStack services.\n\nChange-Id: I35d7c1a3d07ee6c45057cf23705e137bfbbb28d2\n'}]",0,293890,72e8742794bb70f42519f618c3b44ed1ef759191,8,3,1,12053,,,0,"SERVICE_HOST should not be 127.0.0.1

ERVICE_HOST should not be 127.0.0.1 in devstack-vm-gate.sh.
If SERVICE_HOST is 127.0.0.1, then services in a vm can not
access these OpenStack services.

Change-Id: I35d7c1a3d07ee6c45057cf23705e137bfbbb28d2
",git fetch https://review.opendev.org/openstack/devstack-gate refs/changes/90/293890/1 && git format-patch -1 --stdout FETCH_HEAD,['devstack-vm-gate.sh'],1,72e8742794bb70f42519f618c3b44ed1ef759191,bug,,SERVICE_HOST=127.0.0.1,0,1
openstack%2Fmonasca-api~master~Ie8c5d5f3af4d67b33b9b87aad07b245930a02fed,openstack/monasca-api,master,Ie8c5d5f3af4d67b33b9b87aad07b245930a02fed,Add v3 notification endpoints,NEW,2017-02-14 23:55:52.000000000,2017-12-18 04:02:37.000000000,,"[{'_account_id': 14273}, {'_account_id': 14517}, {'_account_id': 16168}]","[{'number': 1, 'created': '2017-02-14 23:55:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/c656a10184fb07dfbf722fe9e151f99abadcd99d', 'message': 'Add v3 notification endpoints\n\nChange-Id: Ie8c5d5f3af4d67b33b9b87aad07b245930a02fed\n'}, {'number': 2, 'created': '2017-02-15 15:18:40.000000000', 'files': ['monasca_api/v3/common/validation.py', 'monasca_api/v3/common/utils.py', 'monasca_api/v3/notification_types.py', 'monasca_api/v3/notifications.py'], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/e8865b522169e312a0516dbb232afb1acef622d1', 'message': 'Add v3 notification endpoints\n\nChange-Id: Ie8c5d5f3af4d67b33b9b87aad07b245930a02fed\n'}]",2,433985,e8865b522169e312a0516dbb232afb1acef622d1,10,3,2,14517,,,0,"Add v3 notification endpoints

Change-Id: Ie8c5d5f3af4d67b33b9b87aad07b245930a02fed
",git fetch https://review.opendev.org/openstack/monasca-api refs/changes/85/433985/2 && git format-patch -1 --stdout FETCH_HEAD,"['monasca_api/v3/common/validation.py', 'monasca_api/v3/common/utils.py', 'monasca_api/v3/notification_types.py', 'monasca_api/v3/notifications.py']",4,c656a10184fb07dfbf722fe9e151f99abadcd99d,v3notifications,"# (C) Copyright 2014-2017 Hewlett Packard Enterprise Development LP # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import falcon from monasca_common.simport import simport from oslo_config import cfg from oslo_log import log from monasca_api.common.repositories import exceptions from monasca_api.common import exceptions as http_exceptions from monasca_api.v3.common import auth from monasca_api.v3.common import pagination from monasca_api.v3.common import utils from monasca_api.v3.common import validation LOG = log.getLogger(__name__) DEFAULT_AUTHORIZED_ROLES = cfg.CONF.security.default_authorized_roles GET_NOTIFICATION_AUTHORIZED_ROLES = (cfg.CONF.security.default_authorized_roles + cfg.CONF.security.read_only_authorized_roles) class Notifications(object): def __init__(self): super(Notifications, self).__init__() self._region = cfg.CONF.region self._notifications_repo = simport.load( cfg.CONF.repositories.notifications_driver)() self._notification_method_type_repo = simport.load( cfg.CONF.repositories.notification_method_type_driver)() self.valid_periods = cfg.CONF.valid_notification_periods def _validate_notification_method_type_exist(self, nmt): notification_methods = self._notification_method_type_repo.list_notification_method_types() exists = nmt.upper() in notification_methods if not exists: LOG.warning(""Found no notification method type {} . Did you install/enable the plugin for that type?"" .format(nmt)) raise falcon.HTTPBadRequest('Bad Request', ""Not a valid notification method type {} "".format(nmt)) def _validate_name_not_conflicting(self, tenant_id, name, expected_id=None): notification = self._notifications_repo.find_notification_by_name(tenant_id, name) if notification: if not expected_id: LOG.warning(""Found existing notification method for {} with tenant_id {}"".format(name, tenant_id)) raise exceptions.AlreadyExistsException( ""A notification method with the name {} already exists"".format(name)) found_notification_id = notification['id'] if found_notification_id != expected_id: LOG.warning(""Found existing notification method for {} with tenant_id {} with unexpected id {}"" .format(name, tenant_id, found_notification_id)) raise exceptions.AlreadyExistsException( ""A notification method with name {} already exists with id {}"" .format(name, found_notification_id)) def _validate_notification(self, req, notification): """"""Validates the notification :param req: the original request :param notification: An event object. :raises falcon.HTTPBadRequest :raises monasca_api.common.repositories.exceptions.AlreadyExistsException """""" name = notification['name'] if not isinstance(name, basestring): raise http_exceptions.HTTPUnprocessableEntityError( 'Unproccessable Entity', 'Invalid type for name: {} is not a string type'.format(name)) if len(name) < 1 or len(name) > 255: raise http_exceptions.HTTPUnprocessableEntityError( 'Unprocessable Entity' 'Invalid length for name: {} is not between 1 and 255'.format(name)) address = notification['address'] if not isinstance(address, basestring): raise http_exceptions.HTTPUnprocessableEntityError( 'Unprocessable Entity', 'Invalid type for name: {} is not a string type'.format(address)) if len(address) < 1 or len(address) > 255: raise http_exceptions.HTTPUnprocessableEntityError( 'Unprocessable Entity', 'Invalid length for name: {} is not between 1 and 255'.format(address)) _type = notification['type'] if not isinstance(_type, basestring): raise http_exceptions.HTTPUnprocessableEntityError( 'Unprocessable Entity', 'Invalid type for name: {} is not a string type'.format(_type)) notification['type'] = _type.upper() try: period = int(notification['period']) assert period > 0 notification['period'] = period except Exception: raise http_exceptions.HTTPUnprocessableEntityError( 'Unprocessable Entity', 'Period must be a positive integer') self._validate_notification_method_type_exist(notification['type']) self._validate_name_not_conflicting(req.tenant_id, notification['name']) def _build_notification_result(self, notification_row): result = { u'id': notification_row['id'], u'name': notification_row['name'], u'type': notification_row['type'], u'address': notification_row['address'], u'period': notification_row['period'] } return result @utils.exception_translator @auth.Authorize(authorized_roles=DEFAULT_AUTHORIZED_ROLES) def on_post(self, req, res): notification = utils.parse_json_body(req, required_fields=['name', 'type', 'address'], defaults={'period': 0}, validation=self._validate_notification) self._validate_notification(req.tenant_id, notification) notification_id = self._notifications_repo.create_notification( req.tenant_id, notification['name'], notification['type'], notification['address'], notification['period']) notification['id'] = notification_id pagination.add_links_to_resource(notification, req.uri) res.body = utils.dumps_json_utf8(notification) res.status = falcon.HTTP_201 @utils.exception_translator @auth.Authorize(authorized_roles=GET_NOTIFICATION_AUTHORIZED_ROLES) def on_get(self, req, res, notification_method_id=None): if notification_method_id is None: sort_by = req.get_param_as_list('sort_by', default=[]) offset = req.get_param_as_int('offset') limit = req.limit allowed_sort_by = {'id', 'name', 'type', 'address', 'updated_at', 'created_at'} validation.validate_sort_by(sort_by, allowed_sort_by) # TODO(Ryan) move this formatting to repo level rows = self._notifications_repo.list_notifications(req.tenant_id, sort_by, offset, limit) results = [self._build_notification_result(row) for row in rows] pagination.add_links_to_resource_list(results, req.uri) paginated_results = pagination.paginate(results, req.uri, limit) res.body = utils.dumps_json_utf8(paginated_results) res.status = falcon.HTTP_200 else: row = self._notifications_repo.list_notification(req.tenant_id, notification_method_id) result = self._build_notification_result(row) req_uri_no_id = req.uri.replace('/' + notification_method_id, """") paginated_result = pagination.add_links_to_resource(result, req_uri_no_id) res.body = utils.dumps_json_utf8(paginated_result) res.status = falcon.HTTP_200 @utils.exception_translator @auth.Authorize(authorized_roles=DEFAULT_AUTHORIZED_ROLES) def on_delete(self, req, res, notification_method_id): self._notifications_repo.delete_notification(req.tenant_id, notification_method_id) res.status = falcon.HTTP_204 @utils.exception_translator @auth.Authorize(authorized_roles=DEFAULT_AUTHORIZED_ROLES) def on_put(self, req, res, notification_method_id): # verify the notification id exists before validating anything else self._notifications_repo.list_notification(req.tenant_id, notification_method_id) notification = utils.parse_json_body(req, required_fields=['name', 'type', 'address', 'period'], validation=self._validate_notification) self._validate_notification(req.tenant_id, notification) self._notifications_repo.update_notification(notification_method_id, req.tenant_id, notification['name'], notification['type'], notification['address'], notification['period']) notification['id'] = notification_method_id pagination.add_links_to_resource(notification, req.uri) res.body = utils.dumps_json_utf8(notification) res.status = falcon.HTTP_200 @utils.exception_translator @auth.Authorize(authorized_roles=DEFAULT_AUTHORIZED_ROLES) def on_patch(self, req, res, notification_method_id): # verify notification id exists before validating further old_notification = self._notifications_repo.list_notification(req.tenant_id, notification_method_id) notification = utils.parse_json_body(req, defaults={'name': old_notification['name'], 'type': old_notification['type'], 'address': old_notification['address'], 'period': old_notification['period']}, validation=self._validate_notification) self._validate_notification(req.tenant_id, notification) self._notifications_repo.update_notification(notification_method_id, req.tenant_id, notification['name'], notification['notification_type'], notification['address'], notification['period']) notification['id'] = notification_method_id pagination.add_links_to_resource(notification, req.uri) res.body = utils.dumps_json_utf8(notification) res.status = falcon.HTTP_200 ",,331,0
openstack%2Fswift~master~I172f97749f01da399fb70e9b12792b7ac2c0d1e8,openstack/swift,master,I172f97749f01da399fb70e9b12792b7ac2c0d1e8,Allow custom swift configuration directory,NEW,2016-11-04 23:16:19.000000000,2017-12-18 04:02:08.000000000,,"[{'_account_id': 1179}, {'_account_id': 7233}, {'_account_id': 7847}, {'_account_id': 12261}, {'_account_id': 13052}, {'_account_id': 18334}]","[{'number': 1, 'created': '2016-11-04 23:16:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/085a49645f255177808e8e168fa326ec6782a62f', 'message': 'Allow custom swift configuration directory\n\nSWIFT_ROOT environment variable allows to override default swift_dir\nwhich is /etc/swift. When not set, defaults to /etc/swift\n\nThis patch was done in the context of setting up multiple Swift\nclusters on shared hardware. This can also be used to run\nSwift using a non default swift configuration directory (i.e. other\nthan /etc/swift)\n\nChange-Id: I172f97749f01da399fb70e9b12792b7ac2c0d1e8\n'}, {'number': 2, 'created': '2016-11-07 15:24:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c4111f94f9d73264e5c58640032c5a3f977c4647', 'message': 'Allow custom swift configuration directory\n\nSWIFT_ROOT environment variable allows to override default swift_dir\nwhich is /etc/swift. When not set, defaults to /etc/swift\n\nThis patch was done in the context of setting up multiple Swift\nclusters on shared hardware. This can also be used to run\nSwift using a non default swift configuration directory (i.e. other\nthan /etc/swift)\n\nChange-Id: I172f97749f01da399fb70e9b12792b7ac2c0d1e8\n'}, {'number': 3, 'created': '2016-11-09 19:26:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/73df886e8bb9abae67e015488dff048504b4a423', 'message': 'Allow custom swift configuration directory\n\nSWIFT_ROOT environment variable allows to override default swift_dir\nwhich is /etc/swift. When not set, defaults to /etc/swift\n\nThis patch was done in the context of setting up multiple Swift\nclusters on shared hardware. This can also be used to run\nSwift using a non default swift configuration directory (i.e. other\nthan /etc/swift)\n\nChange-Id: I172f97749f01da399fb70e9b12792b7ac2c0d1e8\n'}, {'number': 4, 'created': '2016-11-14 18:43:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/143f58e0990990c21acb4f5ed316be5e6d93efde', 'message': 'Allow custom swift configuration directory\n\nIntroducing SWIFT_ROOT environment variable that allows to override\ndefault swift_dir which is /etc/swift. When not set, defaults to\n/etc/swift\n\nThis patch was done in the context of setting up multiple Swift\nclusters on shared hardware. This can also be used to run\nSwift using a non default swift configuration directory (i.e. other\nthan /etc/swift)\n\nChange-Id: I172f97749f01da399fb70e9b12792b7ac2c0d1e8\n'}, {'number': 5, 'created': '2016-11-22 22:54:19.000000000', 'files': ['test/unit/common/middleware/test_memcache.py', 'test/unit/cli/test_recon.py', 'swift/common/utils.py', 'test/unit/common/middleware/test_recon.py', 'doc/saio/bin/remakerings', 'swift/common/manager.py', 'swift/common/middleware/memcache.py', 'swift/proxy/server.py', 'test/unit/common/test_utils.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/7945ade2643abd2240a405951ebd76f9d6f5f7fc', 'message': 'Allow custom swift configuration directory\n\nIntroducing SWIFT_ROOT environment variable that allows to override\ndefault swift_dir which is /etc/swift. When not set, defaults to\n/etc/swift\n\nThis patch was done in the context of setting up multiple Swift\nclusters on shared hardware. This can also be used to run\nSwift using a non default swift configuration directory (i.e. other\nthan /etc/swift)\n\nNote: Please export SWIFT_ROOT (e.g. export SWIFT_ROOT=/etc/myswiftdir)\nto test the changes.\n\nChange-Id: I172f97749f01da399fb70e9b12792b7ac2c0d1e8\n'}]",39,393952,7945ade2643abd2240a405951ebd76f9d6f5f7fc,29,6,5,18334,,,0,"Allow custom swift configuration directory

Introducing SWIFT_ROOT environment variable that allows to override
default swift_dir which is /etc/swift. When not set, defaults to
/etc/swift

This patch was done in the context of setting up multiple Swift
clusters on shared hardware. This can also be used to run
Swift using a non default swift configuration directory (i.e. other
than /etc/swift)

Note: Please export SWIFT_ROOT (e.g. export SWIFT_ROOT=/etc/myswiftdir)
to test the changes.

Change-Id: I172f97749f01da399fb70e9b12792b7ac2c0d1e8
",git fetch https://review.opendev.org/openstack/swift refs/changes/52/393952/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/common/middleware/test_memcache.py', 'test/unit/cli/test_recon.py', 'swift/common/utils.py', 'test/unit/__init__.py', 'test/unit/common/middleware/test_recon.py', 'swift/common/manager.py', 'swift/common/middleware/memcache.py', 'swift/proxy/server.py']",8,085a49645f255177808e8e168fa326ec6782a62f,393952," register_swift_info, SWIFT_ROOT swift_dir = conf.get('swift_dir', SWIFT_ROOT)"," register_swift_info swift_dir = conf.get('swift_dir', '/etc/swift')",42,22
openstack%2Fswift~master~Ifdcd4298f0a287edd647945f2bc32d0430d22563,openstack/swift,master,Ifdcd4298f0a287edd647945f2bc32d0430d22563,test.unit.cli.test_ringbuilder should consisently use run_srb,NEW,2017-02-23 19:54:10.000000000,2017-12-18 04:01:41.000000000,,"[{'_account_id': 330}, {'_account_id': 13052}, {'_account_id': 25174}]","[{'number': 1, 'created': '2017-02-23 19:54:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/5d62ec527484d2510b123e0a545d251f5406bf0e', 'message': 'test.unit.cli.test_ringbuilder should consisently use run_srb\n\nPlaces where we manually make lists of args and manually pass\nthem to ringbuilder.main and expect them to return 0 should *definitely*\nbe rewritten to use run_srb. run_srb should be updated to return exit codes\nand all uses should reflect this.\n\nThis updates run_srb to return exit_codes and updates current\nuses of function. Preparation for replacement of\nassertSystemExit(...ringbuilder.main...).\n\nChange-Id: Ifdcd4298f0a287edd647945f2bc32d0430d22563\nPartial-Bug: #1656440\nSigned-off-by: Nick Miethe <nimiethe@cisco.com>\n'}, {'number': 2, 'created': '2017-02-23 20:10:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/54d4e2831e1cee0967fe83c626f74e988ca7a63f', 'message': 'test.unit.cli.test_ringbuilder should consisently use run_srb\n\nPlaces where we manually make lists of args and manually pass\nthem to ringbuilder.main and expect them to return 0 should *definitely*\nbe rewritten to use run_srb. run_srb should be updated to return exit codes\nand all uses should reflect this.\n\nThis updates run_srb to return exit_codes and updates current\nuses of function. Preparation for replacement of\nassertSystemExit(...ringbuilder.main...).\n\nChange-Id: Ifdcd4298f0a287edd647945f2bc32d0430d22563\nPartial-Bug: #1656440\nSigned-off-by: Nick Miethe <nimiethe@cisco.com>\n'}, {'number': 3, 'created': '2017-02-27 14:40:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/fb3940fff3a60b7594e78350822cdd7177b55fca', 'message': 'test.unit.cli.test_ringbuilder should consisently use run_srb\n\nPlaces where we manually make lists of args and manually pass\nthem to ringbuilder.main and expect them to return 0 should *definitely*\nbe rewritten to use run_srb. run_srb should be updated to return exit codes\nand all uses should reflect this.\n\nThis makes use of the new run_srb on functions up to\ntest_search_device_ipv4_new_format.\n\nChange-Id: Ifdcd4298f0a287edd647945f2bc32d0430d22563\nPartial-Bug: #1656440\nSigned-off-by: Nick Miethe <nimiethe@cisco.com>\n'}, {'number': 4, 'created': '2017-02-27 14:42:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e0e32f895a3c91d5c709873f0e7dbdadc65d7bfc', 'message': 'test.unit.cli.test_ringbuilder should consisently use run_srb\n\nPlaces where we manually make lists of args and manually pass\nthem to ringbuilder.main and expect them to return 0 should *definitely*\nbe rewritten to use run_srb. run_srb should be updated to return exit codes\nand all uses should reflect this.\n\nThis makes use of the new run_srb on functions up to\ntest_search_device_ipv4_new_format.\n\nChange-Id: Ifdcd4298f0a287edd647945f2bc32d0430d22563\nPartial-Bug: #1656440\nSigned-off-by: Nick Miethe <nimiethe@cisco.com>\n'}, {'number': 5, 'created': '2017-02-27 14:44:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7f3ae17d4ea05784367238c637ffa573e34f0b55', 'message': 'test.unit.cli.test_ringbuilder should consisently use run_srb\n\nPlaces where we manually make lists of args and manually pass\nthem to ringbuilder.main and expect them to return 0 should *definitely*\nbe rewritten to use run_srb. run_srb should be updated to return exit codes\nand all uses should reflect this.\n\nThis makes use of the new run_srb on functions up to\ntest_search_device_ipv4_new_format.\n\nChange-Id: Ifdcd4298f0a287edd647945f2bc32d0430d22563\nPartial-Bug: #1656440\nSigned-off-by: Nick Miethe <nimiethe@cisco.com>\n'}, {'number': 6, 'created': '2017-02-27 17:06:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1488d57cadb15df3705559969446f5ab37751449', 'message': 'test.unit.cli.test_ringbuilder should consisently use run_srb\n\nPlaces where we manually make lists of args and manually pass\nthem to ringbuilder.main and expect them to return 0 should *definitely*\nbe rewritten to use run_srb. run_srb should be updated to return exit codes\nand all uses should reflect this.\n\nThis makes use of the new run_srb on functions up to\ntest_search_device_ipv4_new_format.\n\nChange-Id: Ifdcd4298f0a287edd647945f2bc32d0430d22563\nPartial-Bug: #1656440\nSigned-off-by: Nick Miethe <nimiethe@cisco.com>\n'}, {'number': 7, 'created': '2017-02-27 17:07:17.000000000', 'files': ['test/unit/cli/test_ringbuilder.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/59bcd8b1ed763bb8b7a2626a9bd60ba72667ccc7', 'message': 'test.unit.cli.test_ringbuilder should consisently use run_srb\n\nPlaces where we manually make lists of args and manually pass\nthem to ringbuilder.main and expect them to return 0 should *definitely*\nbe rewritten to use run_srb. run_srb should be updated to return exit codes\nand all uses should reflect this.\n\nThis makes use of the new run_srb on functions up to\ntest_search_device_ipv4_new_format.\n\nChange-Id: Ifdcd4298f0a287edd647945f2bc32d0430d22563\nPartial-Bug: #1656440\nSigned-off-by: Nick Miethe <nimiethe@cisco.com>\n'}]",0,437587,59bcd8b1ed763bb8b7a2626a9bd60ba72667ccc7,20,3,7,25174,,,0,"test.unit.cli.test_ringbuilder should consisently use run_srb

Places where we manually make lists of args and manually pass
them to ringbuilder.main and expect them to return 0 should *definitely*
be rewritten to use run_srb. run_srb should be updated to return exit codes
and all uses should reflect this.

This makes use of the new run_srb on functions up to
test_search_device_ipv4_new_format.

Change-Id: Ifdcd4298f0a287edd647945f2bc32d0430d22563
Partial-Bug: #1656440
Signed-off-by: Nick Miethe <nimiethe@cisco.com>
",git fetch https://review.opendev.org/openstack/swift refs/changes/87/437587/2 && git format-patch -1 --stdout FETCH_HEAD,"['CHANGELOG', '.mailmap', 'AUTHORS', 'test/unit/cli/test_ringbuilder.py', 'releasenotes/notes/2_13_0_release-875e1fb1ef59f015.yaml']",5,5d62ec527484d2510b123e0a545d251f5406bf0e,bug/1656440,"--- features: - > Improved performance by eliminating an unneeded directory structure hash. - > Optimized the common case for hashing filesystem trees, thus eliminating a lot of extraneous disk I/O. - > Updated the `hashes.pkl` file format to include timestamp information for race detection. Also simplified hashing logic to prevent race conditions and optimize for the common case. - > The erasure code reconstructor will now shuffle work jobs across all disks instead of going disk-by-disk. This eliminates single-disk I/O contention and allows continued scaling as concurrency is increased. - > Erasure code reconstruction handles moving data from handoff nodes better. Instead of moving the data to another handoff, it waits until it can be moved to a primary node. - > Temporary URLs now support one common form of ISO 8601 timestamps in addition to Unix seconds-since-epoch timestamps. The ISO 8601 format accepted is '%Y-%m-%dT%H:%M:%SZ'. This makes TempURLs more user-friendly to produce and consume. - > Listing containers in accounts with json or xml now includes a `last_modified` time. This does not change any on-disk data, but simply exposes the value to offer consistency with the object listings on containers. - I/O priority is now supported on AArch64 architecture. upgrade: - If you upgrade and roll back, you must delete all `hashes.pkl` files. deprecations: - > If using erasure coding with ISA-L in rs_vand mode and 5 or more parity fragments, Swift will emit a warning. This is a configuration that is known to harm data durability. In a future release, this warning will be upgraded to an error unless the policy is marked as deprecated. All data in an erasure code storage policy using isa_l_rs_vand with 5 or more parity should be migrated as soon as possible. Please see https://bugs.launchpad.net/swift/+bug/1639691 for more information. - > The erasure code reconstructor `handoffs_first` option has been deprecated in favor of `handoffs_only`. `handoffs_only` is far more useful, and just like `handoffs_first` mode in the replicator, it gives the operator the option of forcing the consistency engine to focus solely on revert (handoff) jobs, thus improving the speed of rebalances. The `handoffs_only` behavior is somewhat consistent with the replicator's `handoffs_first` option (any error on any handoff in the replicator will make it essentially handoff only forever) but the `handoff_only` option does what you want and is named correctly in the reconstructor. - > The default for `object_post_as_copy` has been changed to False. The option is now deprecated and will be removed in a future release. If your cluster is still running with post-as-copy enabled, please update it to use the ""fast-post"" method. Future versions of Swift will not support post-as-copy, and future features will not be supported under post-as-copy. (""Fast-post"" is where `object_post_as_copy` is false). fixes: - > Fixed a bug where the ring builder would not allow removal of a device when min_part_seconds_left was greater than zero. - > PUT subrequests generated from a client-side COPY will now properly log the SSC (server-side copy) Swift source field. See https://docs.openstack.org/developer/swift/logs.html#swift-source for more information. - > Fixed a bug where an SLO download with a range request may have resulted in a 5xx series response. - > SLO manifest PUT requests can now be properly validated by sending an ETag header of the md5 sum of the concatenated md5 sums of the referenced segments. - Fixed the stats calculation in the erasure code reconstructor. - > Rings with min_part_hours set to zero will now only move one partition replica per rebalance, thus matching behavior when min_part_hours is greater than zero. other: - Various other minor bug fixes and improvements. ",,205,22
openstack%2Fzun~master~Ib58b3ab94d76b42368f47980f7e0d805c93e27a6,openstack/zun,master,Ib58b3ab94d76b42368f47980f7e0d805c93e27a6,Implement etcd db API for resource_provider,NEW,2017-02-15 07:56:30.000000000,2017-12-18 04:01:10.000000000,,"[{'_account_id': 12175}, {'_account_id': 13248}, {'_account_id': 16277}]","[{'number': 1, 'created': '2017-02-15 07:56:30.000000000', 'files': ['zun/tests/unit/db/test_resource_provider.py', 'zun/db/etcd/api.py', 'zun/db/etcd/models.py'], 'web_link': 'https://opendev.org/openstack/zun/commit/c6644e1d72db4aa10e8172dce231b48f4ab1989c', 'message': 'Implement etcd db API for resource_provider\n\nThis commit add etcd db model and implement etcd db API\nfor resource_provider.\n\nPart of blueprint expose-host-capabilities\n\nChange-Id: Ib58b3ab94d76b42368f47980f7e0d805c93e27a6\n'}]",3,434119,c6644e1d72db4aa10e8172dce231b48f4ab1989c,7,3,1,13248,,,0,"Implement etcd db API for resource_provider

This commit add etcd db model and implement etcd db API
for resource_provider.

Part of blueprint expose-host-capabilities

Change-Id: Ib58b3ab94d76b42368f47980f7e0d805c93e27a6
",git fetch https://review.opendev.org/openstack/zun refs/changes/19/434119/1 && git format-patch -1 --stdout FETCH_HEAD,"['zun/tests/unit/db/test_resource_provider.py', 'zun/db/etcd/api.py', 'zun/db/etcd/models.py']",3,c6644e1d72db4aa10e8172dce231b48f4ab1989c,bp/expose-host-capabilities," class ResourceProvider(Base): """"""Represents a resource provider."""""" _path = '/resource_providers' _fields = objects.ResourceProvider.fields.keys() def __init__(self, resource_provider_data): self.path = ResourceProvider.path() for f in ResourceProvider.fields(): setattr(self, f, None) self.id = 1 self.can_host = 0 self.update(resource_provider_data) @classmethod def path(cls): return cls._path @classmethod def fields(cls): return cls._fields",,318,0
openstack%2Ftacker~master~Ic528f3a8a6ac0e7f21074681ef75a0464b973793,openstack/tacker,master,Ic528f3a8a6ac0e7f21074681ef75a0464b973793,VNF REST APT DOC is modified to handle vim-id and vim-name argument in REST calls for VNF create operation Closes-bug: #1598257,NEW,2016-11-10 19:38:48.000000000,2017-12-18 04:01:01.000000000,,"[{'_account_id': 6348}, {'_account_id': 13380}, {'_account_id': 16237}]","[{'number': 1, 'created': '2016-11-10 19:38:48.000000000', 'files': ['doc/source/devref/mano_api.rst', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/tacker/commit/8e6b45ab45aded0979550b270bb10a5da775b594', 'message': 'VNF REST APT DOC is modified to handle vim-id and vim-name argument in REST\ncalls for VNF create operation\nCloses-bug: #1598257\n\nChange-Id: Ic528f3a8a6ac0e7f21074681ef75a0464b973793\n'}]",3,396372,8e6b45ab45aded0979550b270bb10a5da775b594,7,3,1,23480,,,0,"VNF REST APT DOC is modified to handle vim-id and vim-name argument in REST
calls for VNF create operation
Closes-bug: #1598257

Change-Id: Ic528f3a8a6ac0e7f21074681ef75a0464b973793
",git fetch https://review.opendev.org/openstack/tacker refs/changes/72/396372/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/devref/mano_api.rst', 'tox.ini']",2,8e6b45ab45aded0979550b270bb10a5da775b594,,"envlist = py35,py34,py26,py27,pep8,docs","envlist = py35,py34,py27,pep8,docs",2,1
openstack%2Fswift~master~I810845e506622d5aa156114cb374e3c9559bd349,openstack/swift,master,I810845e506622d5aa156114cb374e3c9559bd349,Better SSL support in functests,NEW,2016-09-23 22:41:00.000000000,2017-12-18 04:00:54.000000000,,"[{'_account_id': 1179}, {'_account_id': 13052}, {'_account_id': 15343}]","[{'number': 1, 'created': '2016-09-23 22:41:00.000000000', 'files': ['test/sample.conf', 'test/functional/__init__.py', 'test/functional/swift_test_client.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/3e96d94c677ee8e1cd16dfd8fb587683a1302c95', 'message': ""Better SSL support in functests\n\nIf you have a recent enough SSL and python 2.7.9ish installed on your\nmachine our httplib handling of insecure only half works, but the part\nthat *does* work emits warnings like mad.\n\nIn addition setting 4 different conf options instead of the auth url\nis pretty annoying so we'll prefer to set the auth_url in configs\ninstead.\n\nChange-Id: I810845e506622d5aa156114cb374e3c9559bd349\n""}]",4,375759,3e96d94c677ee8e1cd16dfd8fb587683a1302c95,9,3,1,1179,,,0,"Better SSL support in functests

If you have a recent enough SSL and python 2.7.9ish installed on your
machine our httplib handling of insecure only half works, but the part
that *does* work emits warnings like mad.

In addition setting 4 different conf options instead of the auth url
is pretty annoying so we'll prefer to set the auth_url in configs
instead.

Change-Id: I810845e506622d5aa156114cb374e3c9559bd349
",git fetch https://review.opendev.org/openstack/swift refs/changes/59/375759/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/sample.conf', 'test/functional/__init__.py', 'test/functional/swift_test_client.py']",3,3e96d94c677ee8e1cd16dfd8fb587683a1302c95,,"import sslimport requests.packages.urllib3 as urllib3 self.auth_ssl = config_true_value(config['auth_ssl']) if self.insecure: urllib3.disable_warnings() conn_class_kwargs = {} if self.insecure and self.conn_class == http_client.HTTPSConnection: conn_class_kwargs['context'] = ssl._create_unverified_context() port=self.storage_port, **conn_class_kwargs)"," self.auth_ssl = config['auth_ssl'] in ('on', 'true', 'yes', '1') port=self.storage_port) self.connection = self.conn_class(self.storage_host, port=self.storage_port) # self.connection.set_debuglevel(3)",39,26
openstack%2Frally~master~I6b9ea962d338417dfafae0ee6753b6b115a8877a,openstack/rally,master,I6b9ea962d338417dfafae0ee6753b6b115a8877a,Add SenlinClusters.list_clusters scenario,NEW,2016-10-25 12:06:37.000000000,2017-12-18 04:00:35.000000000,,"[{'_account_id': 9545}, {'_account_id': 10475}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 22960}, {'_account_id': 23094}, {'_account_id': 23839}]","[{'number': 1, 'created': '2016-10-25 12:06:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/22d56044d969554d0c201b4ff54571fa27aefe5e', 'message': 'Add SenlinClusters.list_clusters scenario\n\nChange-Id: I6b9ea962d338417dfafae0ee6753b6b115a8877a\n'}, {'number': 2, 'created': '2016-10-26 07:54:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/871c069b00b0849c6c9c0c7e70c2b3e1e710bf76', 'message': 'Add SenlinClusters.list_clusters scenario\n\nChange-Id: I6b9ea962d338417dfafae0ee6753b6b115a8877a\n'}, {'number': 3, 'created': '2016-11-07 11:55:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/478b66261f322a6b91e48f6db0fd3ea11630a292', 'message': 'Add SenlinClusters.list_clusters scenario\n\nadd sla and fix indentation\n\nChange-Id: I6b9ea962d338417dfafae0ee6753b6b115a8877a\n'}, {'number': 4, 'created': '2016-11-22 06:16:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/69f04c5bba77f127723ddd77231a387c33e7b008', 'message': 'Add SenlinClusters.list_clusters scenario\n\nlist the clusters.\nMeasure the ""senlin cluster-list"" command performance.\n\nChange-Id: I6b9ea962d338417dfafae0ee6753b6b115a8877a\n'}, {'number': 5, 'created': '2017-01-16 01:33:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1c713e904ac56f6a5996231bc3dfee9244b15abb', 'message': 'Add SenlinClusters.list_clusters scenario\n\nlist the clusters.\nMeasure the ""senlin cluster-list"" command performance.\n\nChange-Id: I6b9ea962d338417dfafae0ee6753b6b115a8877a\n'}, {'number': 6, 'created': '2017-01-16 01:35:06.000000000', 'files': ['samples/tasks/scenarios/senlin/list-clusters.json', 'tests/unit/plugins/openstack/scenarios/senlin/test_clusters.py', 'rally/plugins/openstack/scenarios/senlin/clusters.py', 'rally-jobs/rally-senlin.yaml', 'samples/tasks/scenarios/senlin/list-clusters.yaml'], 'web_link': 'https://opendev.org/openstack/rally/commit/035119decc40c99eb8cfd92834d87f2992616863', 'message': 'Add SenlinClusters.list_clusters scenario\n\nlist the clusters.\nMeasure the ""senlin cluster-list"" command performance.\n\nChange-Id: I6b9ea962d338417dfafae0ee6753b6b115a8877a\n'}]",21,390535,035119decc40c99eb8cfd92834d87f2992616863,47,7,6,23839,,,0,"Add SenlinClusters.list_clusters scenario

list the clusters.
Measure the ""senlin cluster-list"" command performance.

Change-Id: I6b9ea962d338417dfafae0ee6753b6b115a8877a
",git fetch https://review.opendev.org/openstack/rally refs/changes/35/390535/1 && git format-patch -1 --stdout FETCH_HEAD,"['samples/tasks/scenarios/senlin/list-clusters.json', 'tests/unit/plugins/openstack/scenarios/senlin/test_clusters.py', 'rally/plugins/openstack/scenarios/senlin/clusters.py', 'rally-jobs/rally-senlin.yaml', 'samples/tasks/scenarios/senlin/list-clusters.yaml']",5,22d56044d969554d0c201b4ff54571fa27aefe5e,senlin-list-clusters2,"--- SenlinClusters.list_clusters: - args: global_project: true runner: type: ""constant"" times: 1 concurrency: 1 context: users: tenants: 1 users_per_tenant: 1 ",,76,1
openstack%2Fswift~master~Iee8a3e222790e638cedd5171757613cbb67de0dc,openstack/swift,master,Iee8a3e222790e638cedd5171757613cbb67de0dc,Supporting Account ACL in keystoneauth,NEW,2016-08-17 20:58:41.000000000,2017-12-18 04:00:05.000000000,,"[{'_account_id': 7847}, {'_account_id': 9576}, {'_account_id': 13052}, {'_account_id': 15343}]","[{'number': 1, 'created': '2016-08-17 20:58:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/9e9dac85d1f39cdc5755f0b82d7789a7ef75a92d', 'message': 'WIP Investigating Account ACL\n\nChange-Id: Iee8a3e222790e638cedd5171757613cbb67de0dc\n'}, {'number': 2, 'created': '2016-08-19 20:06:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/cd8f2bbae466ba0250788d3d66dab35242c88840', 'message': 'WIP Investigating Account ACL\n\nChange-Id: Iee8a3e222790e638cedd5171757613cbb67de0dc'}, {'number': 3, 'created': '2016-08-22 18:16:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/d90e01a0d4a51c502be8fb7e86c2afe26d5f5197', 'message': 'WIP Investigating Account ACL\n\nChange-Id: Iee8a3e222790e638cedd5171757613cbb67de0dc\n'}, {'number': 4, 'created': '2016-08-23 21:47:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/41a0afe8f78a1434315c67253b3c3c7edac9a2eb', 'message': 'WIP Investigating Account ACL\n\nChange-Id: Iee8a3e222790e638cedd5171757613cbb67de0dc'}, {'number': 5, 'created': '2016-08-29 22:29:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/d962e05e3d661a6aa5c8616dc443caeede516c42', 'message': 'WIP Investigating Account ACL\n\nChange-Id: Iee8a3e222790e638cedd5171757613cbb67de0dc'}, {'number': 6, 'created': '2016-09-08 19:06:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6225938538775dcbefadb1c994d141aaebd4ec26', 'message': 'WIP Investigating Account ACL\n\nChange-Id: Iee8a3e222790e638cedd5171757613cbb67de0dc'}, {'number': 7, 'created': '2016-09-08 21:24:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e2e99e78ef5bbf12881eb50fb9e128911f264e0c', 'message': 'Supporting JSON format for Account ACL in keystoneauth\n\nThis patch adds the ability to store Account ACL in JSON format for\nKeystoneAuth. Right now it only supports storing user ids as discussed.\n\nIn addition, this patch extracted common code from TempAuth into\nAuthMixin. KeystoneAuth and TempAuth both inherit from AuthMixin.\n\nChange-Id: Iee8a3e222790e638cedd5171757613cbb67de0dc'}, {'number': 8, 'created': '2016-09-13 19:09:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6203cc7d2922573ce33b7f9b5ed3b718cc8b04b3', 'message': 'Supporting Account ACL in keystoneauth\n\nThis patch adds the ability to store Account ACL in JSON format for\nKeystoneAuth. Right now it only supports storing user ids. Keystone\nallow user names to change but not the user id. Basing our ACL on\nuser id makes a lot more sense. In the future, we can support a\ncombination of project and user ids.\n\nIn addition, this patch extracted common code from TempAuth into\nAuthMixin. KeystoneAuth and TempAuth both inherit from AuthMixin.\n\nChange-Id: Iee8a3e222790e638cedd5171757613cbb67de0dc\n'}, {'number': 9, 'created': '2016-09-14 20:50:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/16434e969a8a8e681ea3c997f99f28e317cebe44', 'message': 'Supporting Account ACL in keystoneauth\n\nThis patch adds the ability to store Account ACL in JSON format for\nKeystoneAuth. Right now it only supports storing user ids. Keystone\nallow user names to change but not the user id. Basing our ACL on\nuser id makes a lot more sense. In the future, we can support a\ncombination of project and user ids.\n\nIn addition, this patch extracted common code from TempAuth into\nAuthMixin. KeystoneAuth and TempAuth both inherit from AuthMixin.\n\nChange-Id: Iee8a3e222790e638cedd5171757613cbb67de0dc'}, {'number': 10, 'created': '2016-09-20 20:44:18.000000000', 'files': ['test/unit/common/middleware/test_keystoneauth.py', 'swift/common/middleware/authmixin.py', 'swift/common/middleware/tempauth.py', 'swift/common/middleware/keystoneauth.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/6440decbc61fcf8339ea03a07237939f62134b07', 'message': 'Supporting Account ACL in keystoneauth\n\nThis patch adds the ability to store Account ACL in JSON format for\nKeystoneAuth. Right now it only supports storing user ids. Keystone\nallow user names to change but not the user id. Basing our ACL on\nuser id makes a lot more sense. In the future, we can support a\ncombination of project and user ids.\n\nIn addition, this patch extracted common code from TempAuth into\nAuthMixin. KeystoneAuth and TempAuth both inherit from AuthMixin.\n\nChange-Id: Iee8a3e222790e638cedd5171757613cbb67de0dc'}]",25,356715,6440decbc61fcf8339ea03a07237939f62134b07,39,4,10,9576,,,0,"Supporting Account ACL in keystoneauth

This patch adds the ability to store Account ACL in JSON format for
KeystoneAuth. Right now it only supports storing user ids. Keystone
allow user names to change but not the user id. Basing our ACL on
user id makes a lot more sense. In the future, we can support a
combination of project and user ids.

In addition, this patch extracted common code from TempAuth into
AuthMixin. KeystoneAuth and TempAuth both inherit from AuthMixin.

Change-Id: Iee8a3e222790e638cedd5171757613cbb67de0dc",git fetch https://review.opendev.org/openstack/swift refs/changes/15/356715/10 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/utils.py', 'swift/common/middleware/tempauth.py', 'swift/account/server.py', 'swift/common/middleware/keystoneauth.py', 'swift/proxy/controllers/account.py']",5,9e9dac85d1f39cdc5755f0b82d7789a7ef75a92d,account-acl, import pdb; pdb.set_trace() import pdb; pdb.set_trace(),,199,1
openstack%2Fswift~master~Idd69a81b2802e12b2b069bac8ab8e9ca5cb63ab3,openstack/swift,master,Idd69a81b2802e12b2b069bac8ab8e9ca5cb63ab3,[WIP] Wait for each putter to be done before error handling,NEW,2017-02-23 10:32:00.000000000,2017-12-18 03:59:38.000000000,,"[{'_account_id': 12279}, {'_account_id': 13052}]","[{'number': 1, 'created': '2017-02-23 10:32:00.000000000', 'files': ['swift/proxy/controllers/obj.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/bd22c24110de6ff4aa3402d94a06b99cc17c9591', 'message': '[WIP] Wait for each putter to be done before error handling\n\non a PUT action on the EC storage policy, some socket leaks between\nthe proxy and object-server will appears (final CLOSE_WAIT state on\nthe proxy side). close() is called on each socket but following the\ndoc : ""Sockets are automatically closed when they are garbage-collected"",\nthere is still a reference to the socket. Calling wait() on each putter\nto ensure there is no more pending task before handling the disconnect\nerror, will correctly clear the socket.\n\nChange-Id: I29e5b91a6c5d23dde056f93c040152f6412e757f\nCloses-Bug: #1662159\n\n[WIP] close parts_iterators in case of EC error\n\nIn case of GET error in erasure coding (not enough working object-server)\nclose the working iterators.\n\nChange-Id: Idd69a81b2802e12b2b069bac8ab8e9ca5cb63ab3\nCloses-Bug: #1662159\n'}]",2,437321,bd22c24110de6ff4aa3402d94a06b99cc17c9591,6,2,1,24871,,,0,"[WIP] Wait for each putter to be done before error handling

on a PUT action on the EC storage policy, some socket leaks between
the proxy and object-server will appears (final CLOSE_WAIT state on
the proxy side). close() is called on each socket but following the
doc : ""Sockets are automatically closed when they are garbage-collected"",
there is still a reference to the socket. Calling wait() on each putter
to ensure there is no more pending task before handling the disconnect
error, will correctly clear the socket.

Change-Id: I29e5b91a6c5d23dde056f93c040152f6412e757f
Closes-Bug: #1662159

[WIP] close parts_iterators in case of EC error

In case of GET error in erasure coding (not enough working object-server)
close the working iterators.

Change-Id: Idd69a81b2802e12b2b069bac8ab8e9ca5cb63ab3
Closes-Bug: #1662159
",git fetch https://review.opendev.org/openstack/swift refs/changes/21/437321/1 && git format-patch -1 --stdout FETCH_HEAD,['swift/proxy/controllers/obj.py'],1,bd22c24110de6ff4aa3402d94a06b99cc17c9591,fix/1662159," for _getter, parts_iter in best_bucket.get_responses(): parts_iter.close() for putter in putters: putter.wait() for putter in putters: putter.wait() for putter in putters: putter.wait() for putter in putters: putter.wait()",,11,0
openstack%2Ftacker~master~I3e66d7d20c4aa3744324c22fc41253d103cc0bcb,openstack/tacker,master,I3e66d7d20c4aa3744324c22fc41253d103cc0bcb,Make tacker to register public url only openstack,NEW,2017-03-06 06:24:56.000000000,2017-12-18 03:59:36.000000000,,[{'_account_id': 2874}],"[{'number': 1, 'created': '2017-03-06 06:24:56.000000000', 'files': ['tacker/nfvo/drivers/vim/openstack_driver.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/f5eaa80b80e53cb63eb83252f9c58adc8af37759', 'message': 'Make tacker to register public url only openstack\n\nChange-Id: I3e66d7d20c4aa3744324c22fc41253d103cc0bcb\nCloses-bug: 1670264\n'}]",0,441758,f5eaa80b80e53cb63eb83252f9c58adc8af37759,4,1,1,2874,,,0,"Make tacker to register public url only openstack

Change-Id: I3e66d7d20c4aa3744324c22fc41253d103cc0bcb
Closes-bug: 1670264
",git fetch https://review.opendev.org/openstack/tacker refs/changes/58/441758/1 && git format-patch -1 --stdout FETCH_HEAD,['tacker/nfvo/drivers/vim/openstack_driver.py'],1,f5eaa80b80e53cb63eb83252f9c58adc8af37759,bug/1670264," #region_info = ks_client.regions.list() #region_list = [region.id for region in region_info] region_list = ['RegionOne',]", region_info = ks_client.regions.list() region_list = [region.id for region in region_info],3,2
openstack%2Fironic~master~Iec3cb35636252c29212d42bc6461d079a78a2a66,openstack/ironic,master,Iec3cb35636252c29212d42bc6461d079a78a2a66,WIP test to ironic API through WSGI,NEW,2017-02-22 21:15:26.000000000,2017-12-18 03:59:16.000000000,,"[{'_account_id': 10118}, {'_account_id': 12356}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 23330}]","[{'number': 1, 'created': '2017-02-22 21:15:26.000000000', 'files': ['foo'], 'web_link': 'https://opendev.org/openstack/ironic/commit/06f3e5b8b7a2e116afb364ef64df3e922aeba41b', 'message': 'WIP test to ironic API through WSGI\n\nIt run test on CI with WSGI\n\nDevstack-gate changes:\nDepends-On: Id9a79c7c79b220693264ca44b68b9cc46ee00ff2\n\nIronic changes:\nDepends-On: I9c5ad56e1acd292ff0f9cc9b460125fc420abda5\n\nChange-Id: Iec3cb35636252c29212d42bc6461d079a78a2a66\n'}]",0,437152,06f3e5b8b7a2e116afb364ef64df3e922aeba41b,51,7,1,23330,,,0,"WIP test to ironic API through WSGI

It run test on CI with WSGI

Devstack-gate changes:
Depends-On: Id9a79c7c79b220693264ca44b68b9cc46ee00ff2

Ironic changes:
Depends-On: I9c5ad56e1acd292ff0f9cc9b460125fc420abda5

Change-Id: Iec3cb35636252c29212d42bc6461d079a78a2a66
",git fetch https://review.opendev.org/openstack/ironic refs/changes/52/437152/1 && git format-patch -1 --stdout FETCH_HEAD,['foo'],1,06f3e5b8b7a2e116afb364ef64df3e922aeba41b,wsgi_check,,,0,0
openstack%2Foctavia~master~I711f26d62125c319cf410eb836dde0c54aac17ac,openstack/octavia,master,I711f26d62125c319cf410eb836dde0c54aac17ac,ACTIVE-ACTIVE Topology - Initial Cluster Manager,NEW,2016-12-01 09:55:49.000000000,2017-12-18 03:59:08.000000000,,"[{'_account_id': 16923}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-12-01 09:55:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e85634f9b1194263671b1005c4b407dc2374cadd', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 2, 'created': '2016-12-01 14:35:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f2a2b807d06dcdd8a3abb30b8235e3ea536eda45', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 3, 'created': '2016-12-05 13:28:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b3a0c5d724e0f7722be29166fda8d11b37e73ef3', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 4, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1b09f4524ce8d85bf000002597fa5efc1bc6f082', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 5, 'created': '2016-12-13 08:52:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/34001e7ec99701d6dfb7d5cd431e092664d6a582', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 6, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/dfc63adbe0db04f801924cd5af2ded737922a972', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 7, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cb9fc4a54134cedf36e4f06ef88b41fb60360df4', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 8, 'created': '2016-12-19 10:09:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/89dacfcd2af75278b4ab19b09636889642eefaa3', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 9, 'created': '2016-12-20 13:45:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cf7f7837f24271d08ac6bc092629b3d35fc214d2', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 10, 'created': '2017-01-04 12:44:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8bcce42a16c7572caa82fe12f7e26ec7a798f08e', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 11, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/00da3c21d9f8b1d8ec4deaf18688710fa3d5f914', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 12, 'created': '2017-01-05 15:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cafe79bc7a73ae02d213af498313ba36e329c290', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 13, 'created': '2017-01-08 10:06:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6560a8edc5f354898972d17ccca5cda04013c333', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 14, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1b0db3162b9064937b12e597bf274b5d7e9c4917', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 15, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ccd6b9768517fe464d583efd85f9b218a2f11913', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 16, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/be250fcf8435d903e262c9ba2f540d74c2921fb2', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 17, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1d03dca8165b8a42e4efe9d1089c46d03a7833c6', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 18, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7413d7bc0bccf0be5ab78e146f7c4e9eaa2b83cd', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 19, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ced1f7b97f80e1d50d29ea2eb970a48edc69a860', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 20, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/80c142c07d2dc500b01e94fad51ef440ca085fab', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 21, 'created': '2017-01-22 13:10:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e90be8e9beedd894f5defd36603348bd0d82e617', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 22, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3e1649ffdc427514159232ad2506c79b74f9c258', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 23, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f82a4c95843ab0133b804d5cecb25c4554e621ba', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 24, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/00733a15ce72ec96ad5a28d212001812e758d0eb', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 25, 'created': '2017-01-26 14:56:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/684b8711b3b6a8a76b9755999e11aa39013906d5', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 26, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0eb160f96b6ce5658ee288c017d4abcb90517817', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 27, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/530c3509c2b97a2ab96696fa5d4254bbe41d5ef6', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 28, 'created': '2017-02-01 18:43:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/471da904eb04283dc0d8f17495f27ec8886a3906', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 29, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a93a775d8d8edcddc08dad5ff42a37a5f351598c', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 30, 'created': '2017-02-05 13:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7e55377abd3865fae6374169be63d68e52c130c0', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 31, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/55d68a119faf9b01ef7e4a6203a11e3a88708384', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 32, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5218f473b9289e2cc9f4b723854bb61f28deaec4', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 33, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/697c53de51390942fa8e3da1d3a13e321de59d53', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 34, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/tests/unit/controller/worker/test_controller_worker.py', 'octavia/common/config.py', 'octavia/db/base_models.py', 'octavia/tests/unit/amphorae/cluster_manager/drivers/__init__.py', 'octavia/amphorae/cluster_manager/drivers/active_active/active_active_driver.py', 'octavia/tests/unit/amphorae/cluster_manager/drivers/active_active/__init__.py', 'octavia/tests/unit/amphorae/cluster_manager/drivers/noop_driver/__init__.py', 'octavia/controller/worker/flows/amphora_cluster_flows.py', 'octavia/db/migration/alembic_migrations/versions/e9573113afd2_create_amphora_cluster_table.py', 'octavia/tests/unit/amphorae/cluster_manager/__init__.py', 'octavia/common/constants.py', 'octavia/db/repositories.py', 'octavia/tests/unit/amphorae/cluster_manager/drivers/elastic/__init__.py', 'octavia/amphorae/cluster_manager/drivers/elastic/__init__.py', 'octavia/db/migration/alembic_migrations/versions/024eb25bb08f_add_roles_to_amphora.py', 'octavia/amphorae/cluster_manager/drivers/noop_driver/driver.py', 'octavia/amphorae/cluster_manager/__init__.py', 'octavia/db/models.py', 'devstack/plugin.sh', 'octavia/amphorae/cluster_manager/drivers/active_active/__init__.py', 'octavia/tests/unit/amphorae/cluster_manager/drivers/noop_driver/test_amphora_cluster_noop_driver.py', 'octavia/amphorae/cluster_manager/drivers/driver_base.py', 'octavia/tests/functional/db/test_repositories.py', 'octavia/common/data_models.py', 'octavia/controller/worker/flows/load_balancer_flows.py', 'octavia/amphorae/cluster_manager/drivers/__init__.py', 'octavia/controller/worker/controller_worker.py', 'octavia/tests/unit/controller/worker/flows/test_amphora_cluster_flows.py', 'octavia/amphorae/cluster_manager/drivers/elastic/elastic_driver.py', 'octavia/amphorae/cluster_manager/drivers/noop_driver/__init__.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/428a2119f293cfb1fc4966f10dffd478036709bc', 'message': 'ACTIVE-ACTIVE Topology - Initial Cluster Manager\n\nChange-Id: I711f26d62125c319cf410eb836dde0c54aac17ac\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}]",0,405238,428a2119f293cfb1fc4966f10dffd478036709bc,107,2,34,21138,,,0,"ACTIVE-ACTIVE Topology - Initial Cluster Manager

Change-Id: I711f26d62125c319cf410eb836dde0c54aac17ac
Implements: blueprint https://review.openstack.org/#/c/234639
",git fetch https://review.opendev.org/openstack/octavia refs/changes/38/405238/1 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/tests/unit/controller/worker/test_controller_worker.py', 'octavia/common/config.py', 'octavia/db/base_models.py', 'octavia/tests/unit/amphorae/cluster_manager/drivers/__init__.py', 'octavia/amphorae/cluster_manager/drivers/active_active/active_active_driver.py', 'octavia/tests/unit/amphorae/cluster_manager/drivers/active_active/__init__.py', 'octavia/tests/unit/amphorae/cluster_manager/drivers/noop_driver/__init__.py', 'octavia/controller/worker/flows/amphora_cluster_flows.py', 'octavia/db/migration/alembic_migrations/versions/e9573113afd2_create_amphora_cluster_table.py', 'octavia/tests/unit/amphorae/cluster_manager/__init__.py', 'octavia/common/constants.py', 'octavia/db/repositories.py', 'octavia/tests/unit/amphorae/cluster_manager/drivers/elastic/__init__.py', 'octavia/amphorae/cluster_manager/drivers/elastic/__init__.py', 'octavia/db/migration/alembic_migrations/versions/024eb25bb08f_add_roles_to_amphora.py', 'octavia/amphorae/cluster_manager/drivers/noop_driver/driver.py', 'octavia/amphorae/cluster_manager/__init__.py', 'octavia/db/models.py', 'octavia/amphorae/cluster_manager/drivers/active_active/__init__.py', 'octavia/tests/unit/amphorae/cluster_manager/drivers/noop_driver/test_amphora_cluster_noop_driver.py', 'octavia/amphorae/cluster_manager/drivers/driver_base.py', 'octavia/tests/functional/db/test_repositories.py', 'octavia/common/data_models.py', 'octavia/controller/worker/flows/load_balancer_flows.py', 'octavia/amphorae/cluster_manager/drivers/__init__.py', 'octavia/controller/worker/controller_worker.py', 'octavia/tests/unit/controller/worker/flows/test_amphora_cluster_flows.py', 'octavia/amphorae/cluster_manager/drivers/elastic/elastic_driver.py', 'octavia/amphorae/cluster_manager/drivers/noop_driver/__init__.py']",29,e85634f9b1194263671b1005c4b407dc2374cadd,06tmp03,,,848,16
openstack%2Foctavia~master~I77eaa31bd68a3a99247b521813b5acc08ccaf9e8,openstack/octavia,master,I77eaa31bd68a3a99247b521813b5acc08ccaf9e8,ACTIVE-ACTIVE Topology - distributor creation flow,NEW,2016-12-05 13:28:07.000000000,2017-12-18 03:59:05.000000000,,"[{'_account_id': 16923}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-12-05 13:28:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f7dffe8c043affe9ce198d4e4a1510f3cdd1c787', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 2, 'created': '2016-12-05 15:19:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/aac094fed26e829db0f0b9039c6992d01c1aaeb9', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 3, 'created': '2016-12-06 08:16:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/aaf95ce1faa9ef8ff168b8cde971541942016e61', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 4, 'created': '2016-12-06 09:43:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9e4ffc5bbca829deaa658b51d012cb8e4081fc3f', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 5, 'created': '2016-12-06 14:02:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/23ca37e017ef31b209228d98509c6a5ebc969260', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 6, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/db18660362d2dceb9964d92718bf56fbbeb3d42a', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 7, 'created': '2016-12-13 08:53:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/94862f561bedfade3c110376641399434d85883a', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 8, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/25616e50ab4a9cbfdcaa0c469c3ae2d0ac2d3d78', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 9, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/272d8b2ed655f23b449f1ef59aa4d5d628505660', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 10, 'created': '2017-01-04 12:50:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d503b8d4b2bfa87cb540274139d50b21833b5e01', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 11, 'created': '2017-01-04 12:58:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f73c3fc0b24ca01cf2c1ea5f77bd085e15d485cd', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 12, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9c0587d3eb487070fd32e88a40a83fb7b300e88d', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 13, 'created': '2017-01-05 15:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3915406587640c5470258fd9c5aa444c8188ab7e', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 14, 'created': '2017-01-08 10:06:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b79fb8c3c5c075257df95c6ac10ae82c610b9fd8', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 15, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/897787fa41e9e4dc59e8e05982b6ce321c0ae017', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 16, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f10cb2212a2a626a1ab3df0bc3255ad6cca94e93', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 17, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/4499fb1e8ddb1389ba5c4b3c5addc87041bcd876', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 18, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bf9a557b43d32ea9e1999de4340fcdb4e6cdd39f', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 19, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ce2b0bc4cf64f4c3166b0b069c341f94ff76b267', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 20, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8d7e08ac7d094e9816fd79b9f4bb3d603eb41371', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 21, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/21ecaafbd881641cbbdd3b1065ce41a2a4c8f38b', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 22, 'created': '2017-01-22 13:10:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5211ab151ef80bca3a6f1a60a16564c8e271acdf', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 23, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c641cff2c9fc7c759c61abadb46ec015d890ee85', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 24, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3aee38e46fbf3784e27afcbb43e4525cc021f5b7', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 25, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f132a9eeac250363af04a6594c4e3e3c07e26bb9', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 26, 'created': '2017-01-26 14:56:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c30de9d8a1dc80dbd7db8edd73b2b346a2439f55', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 27, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c60690b20b79b8a548acd3a315e2fc38effb8187', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 28, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/af864f8970dc1e9b1b235a479ffdb04c18017b16', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 29, 'created': '2017-02-01 18:43:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8f94789c0475eb129d7f86b846bb10f2ad137141', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 30, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6c74e119aa3ac5830016a3b84bdb89880dacfc95', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 31, 'created': '2017-02-05 13:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b4dd45318cb35c98026de1c970dc405a722e15fe', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 32, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1eabe9be87d8c6a804423d05117a15304cf14419', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 33, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/536889f914e4c16b16ace4224c79f7ed3e1a3dac', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 34, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/abc4382f283844fa388f53c5534157bca3160b2a', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}, {'number': 35, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/tests/unit/controller/worker/flows/test_distributor_flows.py', 'octavia/controller/worker/tasks/network_tasks.py', 'octavia/controller/worker/flows/distributor_flows.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/99b331e8199a6e5899d24b38b192a243d3c5e2d3', 'message': 'ACTIVE-ACTIVE Topology - distributor creation flow\n\nAdded distributor create flow\n\nChange-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8\n'}]",0,406953,99b331e8199a6e5899d24b38b192a243d3c5e2d3,109,2,35,21138,,,0,"ACTIVE-ACTIVE Topology - distributor creation flow

Added distributor create flow

Change-Id: I77eaa31bd68a3a99247b521813b5acc08ccaf9e8
",git fetch https://review.opendev.org/openstack/octavia refs/changes/53/406953/35 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/tests/unit/controller/worker/flows/test_distributor_flows.py', 'octavia/controller/worker/tasks/distributor_driver_tasks.py', 'octavia/controller/worker/flows/distributor_flows.py']",3,f7dffe8c043affe9ce198d4e4a1510f3cdd1c787,06tmp03,"# Copyright 2016 IBM Corp. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # from oslo_config import cfg from taskflow.patterns import linear_flow from taskflow import retry from octavia.common import constants from octavia.controller.worker.tasks import cert_task from octavia.controller.worker.tasks import compute_tasks from octavia.controller.worker.tasks import database_tasks CONF = cfg.CONF CONF.import_group('controller_worker', 'octavia.common.config') class DistributorFlows(object): def __init__(self): # for some reason only this has the values from the config file self.REST_DISTRIBUTOR_DRIVER = ( CONF.active_active_cluster.distributor_driver == 'distributor_rest_driver') def get_create_distributor_flow(self): """"""Creates a flow to create a distributor. Ideally that should be configurable in the config file - a db session needs to be placed into the flow :returns: The flow for creating the amphora """""" create_distributor_flow = linear_flow.Flow( constants.CREATE_DISTRIBUTOR_FLOW) create_distributor_flow.add(database_tasks.CreateDistributorInDB( provides=constants.DISTRIBUTOR_ID)) create_distributor_flow.add(cert_task.GenerateDistributorServerPEMTask( provides=constants.SERVER_PEM)) create_distributor_flow.add(compute_tasks.CertDistributorComputeCreate( requires=(constants.DISTRIBUTOR_ID, constants.SERVER_PEM), provides=constants.COMPUTE_ID)) create_distributor_flow.add(database_tasks.MarkDistributorBootingInDB( requires=(constants.DISTRIBUTOR_ID, constants.COMPUTE_ID))) wait_flow = linear_flow.Flow(constants.WAIT_FOR_DISTRIBUTOR, retry=retry.Times(CONF. controller_worker. amp_active_retries)) wait_flow.add(compute_tasks.DistributorComputeWait( requires=constants.COMPUTE_ID, provides=constants.COMPUTE_OBJ)) wait_flow.add(database_tasks.UpdateDistributorInfo( requires=(constants.DISTRIBUTOR_ID, constants.COMPUTE_OBJ), provides=constants.DISTRIBUTOR)) create_distributor_flow.add(wait_flow) create_distributor_flow.add(database_tasks.ReloadDistributor( requires=constants.DISTRIBUTOR_ID, provides=constants.DISTRIBUTOR)) create_distributor_flow.add(database_tasks.MarkDistributorReadyInDB( requires=constants.DISTRIBUTOR)) return create_distributor_flow",,133,0
openstack%2Foctavia~master~Id7fb2b17d700a86277d380aa2da7d96f2426eb55,openstack/octavia,master,Id7fb2b17d700a86277d380aa2da7d96f2426eb55,ACTIVE-ACTIVE - controller network tasks,NEW,2016-05-31 16:22:46.000000000,2017-12-18 03:59:03.000000000,,"[{'_account_id': 8335}, {'_account_id': 12040}, {'_account_id': 14591}, {'_account_id': 16923}, {'_account_id': 21138}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-05-31 16:22:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/96e76091f103dac3db75eb4edeeba417ad211af6', 'message': 'WIP: ACTIVE-ACTIVE - controller network tasks\n\nAdded methods to the controller network tasks to support\nplug/unplug VIP to/from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 2, 'created': '2016-06-20 14:12:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2199aa3551efa910e08af8969a53e06aa4cef70d', 'message': 'WIP: ACTIVE-ACTIVE - controller network tasks\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 3, 'created': '2016-06-21 05:53:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/4d3ec0994ef76149d3c9ab9fce6816c8bfb01ca1', 'message': 'WIP: ACTIVE-ACTIVE - controller network tasks\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 4, 'created': '2016-06-21 14:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9714adf4e9a3d935af5049cc710b09fe96982962', 'message': 'WIP: ACTIVE-ACTIVE - controller network tasks\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 5, 'created': '2016-07-13 20:26:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e8091447326535f0e1b380334e0c7f5462f12cbb', 'message': 'WIP: ACTIVE-ACTIVE - controller network tasks\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 6, 'created': '2016-07-19 15:05:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b2e9d99c74c5cb1cdcf0ded652fc41e448913adb', 'message': 'WIP: ACTIVE-ACTIVE - controller network tasks\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 7, 'created': '2016-07-19 15:24:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bafb426ff6dcdcfa9d0d72d95e948447dcdac90b', 'message': 'WIP: ACTIVE-ACTIVE - controller network tasks\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 8, 'created': '2016-07-20 11:42:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/fea008cdec461b63c5f6a1ceb9a27e860f4b0168', 'message': 'WIP: ACTIVE-ACTIVE - controller network tasks\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 9, 'created': '2016-07-20 13:33:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b5f7a1dfa2f5f9531f3236c1f823fc16302529cc', 'message': 'WIP: ACTIVE-ACTIVE - controller network tasks\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 10, 'created': '2016-08-23 16:03:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/64958969ffc95365ea739f11781ee3f92f62d7f0', 'message': 'WIP: ACTIVE-ACTIVE - controller network tasks\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 11, 'created': '2016-09-21 12:25:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/021c57335a2563b1c47fb14b54959a88e4e3d2cb', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 12, 'created': '2016-09-26 07:54:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2342d053a69183276fa2bc68461aa48c0371a509', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 13, 'created': '2016-09-26 08:02:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/aa9da5c12168cbb029d53995d20515a57ea0bbe4', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 14, 'created': '2016-09-27 13:05:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e4e54c133ff7956b30f7e9f4cdf42d70d95131f8', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 15, 'created': '2016-09-28 07:18:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/42bec5121978d6d36e6ce919daf13ca28e362e8d', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 16, 'created': '2016-10-09 12:02:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/af1f656108c2f9ae54d0f8dcb62b1a1fcc293f2c', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 17, 'created': '2016-10-09 13:41:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/97f47cd25fdb84d902352ad7605124902e559035', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 18, 'created': '2016-11-27 14:47:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c190538c12f275d46e0a55cccaa08bb5ec9040d1', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 19, 'created': '2016-11-28 10:46:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b9159fe6dc2fff77652c554e3784cdb80a7fceaa', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 20, 'created': '2016-11-28 11:51:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d0d3082b7553a8fe7f5c48336862e76261d441e5', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 21, 'created': '2016-11-30 15:17:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b99a69dd33d65e7639cb9e85c2670cf6e9b9570c', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 22, 'created': '2016-12-01 09:55:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cf946c12a1c480df78705eb563d667dc3092723d', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 23, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b53864298f5aa5338d605a0f0fdfc87f240783b3', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 24, 'created': '2016-12-13 08:52:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ed08178ac035e7adfa3883bb1db23031ed46d3fc', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 25, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/4daef1b6bf1891ea3670a2f79d49ef76794b3a1b', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 26, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f70e4cb65075a4473cd780467aaf796867f99c8a', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 27, 'created': '2016-12-15 13:22:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/08b228441a3d0003bafca165db905c6c6e0aa79f', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 28, 'created': '2016-12-15 15:36:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8c1bec056fa8b5c473b8189275db396ecf0bfaa4', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 29, 'created': '2016-12-18 11:15:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d45db8be7b8af09908cd066b755aa441700d9c1b', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 30, 'created': '2016-12-20 13:45:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/aa3560e896de129e07def02c222e655b5212f33c', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 31, 'created': '2016-12-20 15:13:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/03f94669c9e67872d4039454fe13dce5c02ff2be', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 32, 'created': '2016-12-21 07:41:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/91b4345729ddccc166bc714528d912e4d2ac1c5c', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 33, 'created': '2016-12-25 16:08:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ba22d4daba818a894bc42f8c6b12dc95d7472b6f', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 34, 'created': '2017-01-04 12:39:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/79baf7e6b9af8f070dde6c423c9d4bd18c236014', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 35, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/08c90d665e392114985b8e6b788007a4cce2d762', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 36, 'created': '2017-01-05 15:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a54377ada3b84dec76e7be9169bea992d00caf3b', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 37, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b0ba59b9cae095986281a8591147bf7990bb096a', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 38, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/49f48d8f53f971264b8553dd7404603468dbd474', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 39, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d4fd9e08fb5fcf8d7f672d2674d9bbaccd3080a0', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 40, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d54d978ef17727e6365f024674e66308b0f4f87d', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 41, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/941dfa2b69e7ab12748e0577690d73fe08f0031f', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 42, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0bdfc248ed46c5f534a026b307e424bfc09c539c', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 43, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f2c514c969b11e1f8b40be8a7419fe4580f8478b', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 44, 'created': '2017-01-22 13:10:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6a65bc123ef03c07a3eee8ab150032625921b58b', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 45, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6027e2fe5dcb551d9b6254ad8152e73eca2e845e', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 46, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0d0c0e098a0ca3be4ec55e4ba6c9f3ae0734f395', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 47, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f67be6c3a324f58b012778b03f9f672f8acc0fff', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 48, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/051066409cae677013e408924bd7a4d05bef161c', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 49, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6cd787b937d67b8cf445c196251c084b6feb16a1', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 50, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3f42b174fe8883f801c3f9e20ff0057b3d8a1113', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 51, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2dda0150108b04b8c985f465778579a4412e4935', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 52, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d86e1b4aca51aa7a1f6a68edabe275bb5f714fe5', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 53, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/186fe18146e263f02aad56e9ed0f199b63e2622a', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 54, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/tests/unit/controller/worker/tasks/test_network_tasks.py', 'octavia/controller/worker/tasks/network_tasks.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/6c4bfe6f69c912d7d701c40c39c52c52c9075d94', 'message': 'ACTIVE-ACTIVE - controller network tasks\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the network tasks to support\nplug/unplug VIP to or from distributor module\n\nChange-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}]",19,323481,6c4bfe6f69c912d7d701c40c39c52c52c9075d94,182,6,54,21138,,,0,"ACTIVE-ACTIVE - controller network tasks

Implements: blueprint https://review.openstack.org/#/c/234639

Added methods to the network tasks to support
plug/unplug VIP to or from distributor module

Change-Id: Id7fb2b17d700a86277d380aa2da7d96f2426eb55
Co-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>
Co-Authored-By: Valeria Perelman <perelman@il.ibm.com>
",git fetch https://review.opendev.org/openstack/octavia refs/changes/81/323481/12 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/tests/unit/controller/worker/tasks/test_network_tasks.py', 'octavia/controller/worker/tasks/network_tasks.py']",2,96e76091f103dac3db75eb4edeeba417ad211af6,06tmp03," #TODO: Lera - check if needed #from octavia.db import api as db_apis #from octavia.db import repositories as repo #self.loadbalncer_repo = repo.LoadBalancerRepository() #loadbalancer = self.loadbalncer_repo.get(db_apis.get_session(), # id=loadbalancer.id) class PlugDistributorVIP(BaseNetworkTask): """"""Task to plumb a VIP."""""" def execute(self, loadbalancer, distributor): """"""Plumb a vip to a distributor."""""" LOG.debug(""Plumbing Distributor VIP for loadbalancer id: %s"", loadbalancer.id) # TODO: REMOVE::: Bypass because the loadbalancer object # TODO: doesn't have amphore ... # TODO: Dean to check, something is wrong in the DB (I think) #from octavia.db import api as db_apis #from octavia.db import repositories as repo #self.loadbalncer_repo = repo.LoadBalancerRepository() #loadbalancer = self.loadbalncer_repo.get(db_apis.get_session(), # id=loadbalancer.id) self.network_driver.plug_distributor_vip( loadbalancer, distributor, loadbalancer.vip) def revert(self, loadbalancer, distributor, *args, **kwargs): """"""Handle a failure to plumb a vip."""""" LOG.warn(_LW(""Unable to plug VIP for loadbalancer id %s""), loadbalancer.id) self.network_driver.unplug_distributor_vip(loadbalancer, distributor, loadbalancer.vip) class UnplugDistributorVIP(BaseNetworkTask): """"""Task to unplug the vip from specific amphora ."""""" def execute(self, distributor, loadbalancer): """"""Unplug the vip."""""" LOG.debug(""UnplugAmporaVIP start"") try: self.network_driver.unplug_distributor_vip( loadbalancer, distributor, loadbalancer.vip) except Exception: LOG.exception(_LE(""Unable to unplug distributor vip from "" ""load balancer %s""), loadbalancer.id) class AllocateAmphoraVIP(BaseNetworkTask): """"""Task to allocate a VIP."""""" def execute(self, loadbalancer): """"""Allocate a vip to the loadbalancer."""""" LOG.debug(""Allocate_amphora_vip port_id %s, subnet_id %s,"" ""ip_address %s"", loadbalancer.vip.port_id, loadbalancer.vip.subnet_id, loadbalancer.vip.ip_address) return self.network_driver.allocate_amphora_vip(loadbalancer) def revert(self, result, loadbalancer, *args, **kwargs): """"""Handle a failure to allocate vip."""""" if isinstance(result, failure.Failure): LOG.exception(_LE(""Unable to allocate VIP"")) return vip = result LOG.warn(_LW(""Deallocating vip %s""), vip.ip_address) self.network_driver.deallocate_vip(vip) class GetAmphoraMacAddr(BaseNetworkTask): """""" Task to get Amphora MAC Address in order to register it to distributor. """""" def execute(self, amphora): #amphora_mac = amphorae_network_config[amphora.id].ha_port.\ # mac_address LOG.debug(""GetAmphoraMacAddr start"") vrrp_port = self.network_driver.get_port(amphora.vrrp_port_id) #amphora_mac = amphorae_network_config[amphora.id].vrrp_port.\ # mac_address amphora_mac = vrrp_port.mac_address LOG.debug(""GetAmphoraMacAddr: amphora_id:%s, mac:%s "", amphora.id, amphora_mac) return amphora_mac ",,142,0
openstack%2Foctavia~master~I03e5736b5e084b20949861dd4fd9ca8009870893,openstack/octavia,master,I03e5736b5e084b20949861dd4fd9ca8009870893,ACTIVE-ACTIVE - network driver related changes,NEW,2016-05-29 12:18:42.000000000,2017-12-18 03:59:00.000000000,,"[{'_account_id': 8335}, {'_account_id': 12040}, {'_account_id': 14591}, {'_account_id': 16923}, {'_account_id': 21138}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-05-29 12:18:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/81f39b164939c8e1fd3bdbcb8c9112ba4de7fa0a', 'message': 'WIP: ACTIVE-ACTIVE - network driver related changes\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 2, 'created': '2016-05-31 16:22:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/68bd797d224a82704fc7854deaccd3ada6984577', 'message': 'WIP: ACTIVE-ACTIVE - network driver related changes\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 3, 'created': '2016-06-20 14:12:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/97a4a4d7705f84463fe439ea1cdc575cad202178', 'message': 'WIP: ACTIVE-ACTIVE - network driver related changes\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 4, 'created': '2016-06-21 05:53:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9c9ddabf9bfbba247e966318b6a40877120e0515', 'message': 'WIP: ACTIVE-ACTIVE - network driver related changes\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 5, 'created': '2016-07-05 19:31:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/145faa9f456b7226577aaef86601b413b6b195b8', 'message': 'WIP: ACTIVE-ACTIVE - network driver related changes\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 6, 'created': '2016-07-12 17:28:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e4cb2c7ea65cf74e416a003c1ed9d710891e7202', 'message': 'WIP: ACTIVE-ACTIVE - network driver related changes\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 7, 'created': '2016-07-19 15:04:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b638b589ad4a80314c8a3f1b51539253f0c6b980', 'message': 'WIP: ACTIVE-ACTIVE - network driver related changes\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 8, 'created': '2016-07-19 15:24:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2e01e1d4b94d55de46f19e50fabf3bd679b6e23a', 'message': 'WIP: ACTIVE-ACTIVE - network driver related changes\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 9, 'created': '2016-07-20 11:42:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9bdf597101f837862daf82e37ed07b4703b38596', 'message': 'WIP: ACTIVE-ACTIVE - network driver related changes\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 10, 'created': '2016-08-23 16:03:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/69e1539c2993505b8aff858611d68af3c75bf17f', 'message': 'WIP: ACTIVE-ACTIVE - network driver related changes\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 11, 'created': '2016-09-20 14:18:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/661ffc1d6828247f235de1e1dda315295c94fd02', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 12, 'created': '2016-09-26 07:54:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/46a7ab764d37952e089582489742a4880f29cee1', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 13, 'created': '2016-09-26 08:02:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5849ff8e6e8f301dc6e998adea8aec8e83e40a8b', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 14, 'created': '2016-09-27 13:05:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/83cd4bc99cad042cdfcb846f463fbfeb745ba606', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 15, 'created': '2016-09-28 07:18:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8335ac7a266d6c0550930292da95d5de5a9cf6c1', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 16, 'created': '2016-10-09 12:02:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7d1bdd2575917003a590ee1802f3c099fa53276c', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 17, 'created': '2016-10-09 13:41:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/45809ed6d1dc6de1d82c9b4afd829ac98189dc40', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 18, 'created': '2016-11-27 14:47:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1d1ffe7b05ddcc55f31d4612f7b7bfad88d1bde9', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 19, 'created': '2016-11-28 10:46:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7e85d1d7e272d62daf769c8a5c027d94763d9bf6', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 20, 'created': '2016-11-28 11:51:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/65a8c4f7d6d5dc5bf31fb87b43c253c34ef465ea', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 21, 'created': '2016-11-30 15:17:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/fb7d511afaf66c74e73b316c87edec527a75ec39', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 22, 'created': '2016-12-01 09:55:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/be92d7dbed28052dc649a631033cb1f0343796d9', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 23, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3acd775136be91cf6e981f1616454dcadfc209bd', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 24, 'created': '2016-12-13 08:51:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b85908aaaa8831b265281f5f5a502ef291d30183', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 25, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ce7480bd446ada58e717464818670f5799a44a1e', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 26, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5d3bf781bcb0e6bc160c5f2d59e213f69a1ff493', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 27, 'created': '2016-12-18 11:15:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/18f5bb7321ab479262102abf9a078801a03f3fb6', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 28, 'created': '2016-12-20 13:45:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/40a9d618360e3ba76ad3584f7287611e08cea5dc', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 29, 'created': '2016-12-25 16:08:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5de4cec00ee9b36d7105d6313f0dfd205c6a4f4e', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 30, 'created': '2017-01-02 11:36:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7a7158771244f22135924768d0956baddc363427', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 31, 'created': '2017-01-04 12:39:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9384a3dedb9f308fcf3e5eedc256e37a46d34ace', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 32, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d6aa375ea80bcdb3ca0526166264eaddea3e33ec', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 33, 'created': '2017-01-05 15:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5b7a12fa7065f9f803db1ae7ff04966374c56d12', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 34, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/37c2865c9ad184f33385abbddd5627b5487be5c3', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 35, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/833bab46d1842b564afa125826b743bba818889d', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 36, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/75a8191aa0e5b7e2ef8af6e449645f0d7f87fc4f', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 37, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7b93bfd7a0c6a43bf6ebbfadbf219a39bcc17878', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 38, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9ef61700045fe24d5b0e2d5faa8dcea5590a5c41', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 39, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/12b0d96952fa5c68c1dd9f2f0234a42d252ae708', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 40, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/325b1a7ca3eb0b63a1302bb438a39a88e5d43628', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 41, 'created': '2017-01-22 13:10:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e3fdf4d6f17c84d98d26e84c0e5c621286cd671a', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 42, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0856248950c0beb697ef89d97cd3bd2254630467', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 43, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7129d1a6e730f4e5b6d0637213f46a9b428c357a', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 44, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/46c8b83a057897271392b44bc27d09328e32795b', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 45, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/65ac254fc533da2bb02a395bb973e3366395506e', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 46, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9c8cc51c39c174a7d7328c6da779e2f9d84eee35', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 47, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2a279bceaa428426b3470018caca8ccadb390c06', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 48, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1b970478d4506445cc6127259f78a662406111bb', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 49, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ab0e99fe126c2f61673e6b389a70955818cf98df', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 50, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2300287bf2718fc8eef551e14ece025cfc227d8d', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}, {'number': 51, 'created': '2017-03-06 15:52:04.000000000', 'files': ['specs/version0.5/network-driver-interface.rst', 'octavia/tests/unit/network/drivers/neutron/test_allowed_address_pairs.py', 'octavia/controller/worker/flows/amphora_flows.py', 'octavia/network/drivers/neutron/base.py', 'octavia/network/base.py', 'octavia/network/drivers/noop_driver/driver.py', 'octavia/network/drivers/neutron/allowed_address_pairs.py', 'octavia/controller/worker/controller_worker.py', 'specs/version0.5/controller-worker.rst', 'octavia/tests/unit/network/drivers/test_network_noop_driver.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/46658075501aadabcd6cc3b525e1acae2f51c182', 'message': 'ACTIVE-ACTIVE - network driver related changes\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nAdded methods to the neutron network driver to support\nplug/unplug VIP to/from distributor module\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I03e5736b5e084b20949861dd4fd9ca8009870893\n'}]",20,322494,46658075501aadabcd6cc3b525e1acae2f51c182,170,6,51,21138,,,0,"ACTIVE-ACTIVE - network driver related changes

Implements: blueprint https://review.openstack.org/#/c/234639

Added methods to the neutron network driver to support
plug/unplug VIP to/from distributor module

Co-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>
Co-Authored-By: Valeria Perelman <perelman@il.ibm.com>

Change-Id: I03e5736b5e084b20949861dd4fd9ca8009870893
",git fetch https://review.opendev.org/openstack/octavia refs/changes/94/322494/8 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/tests/unit/network/drivers/neutron/test_allowed_address_pairs.py', 'octavia/network/base.py', 'octavia/network/drivers/noop_driver/driver.py', 'octavia/network/drivers/neutron/allowed_address_pairs.py', 'octavia/tests/unit/network/drivers/test_network_noop_driver.py']",5,81f39b164939c8e1fd3bdbcb8c9112ba4de7fa0a,06tmp03," FAKE_UUID_4 = uuidutils.generate_uuid() self.distributor = models.Distributor() self.distributor.id = self.FAKE_UUID_4 def test_plug_distributor_vip(self): self.driver.plug_distributor_vip(self.load_balancer, self.distributor, self.vip) self.assertEqual((self.load_balancer, self.distributor, self.vip, 'plug_distributor_vip'), self.driver.driver.networkconfigconfig[( self.load_balancer.id, self.distributor.id, self.vip.ip_address)]) def test_unplug_distributor_vip(self): self.driver.unplug_distributor_vip(self.load_balancer, self.distributor, self.vip) self.assertEqual((self.load_balancer, self.distributor, self.vip, 'unplug_distributor_vip'), self.driver.driver.networkconfigconfig[( self.load_balancer.id, self.distributor.id, self.vip.ip_address)]) ",,129,38
openstack%2Foctavia~master~I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6,openstack/octavia,master,I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6,Active-Active Topology - Cluster DB Tasks,NEW,2016-12-12 12:22:18.000000000,2017-12-18 03:58:58.000000000,,"[{'_account_id': 16923}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-12-12 12:22:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9bb711ae422f217d1ca949532de2fc5e8cc0c9e3', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 2, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3c718afc6a6fb77b913d683dd7b9cf21f0000085', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 3, 'created': '2016-12-13 08:53:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0f3bb4817a15fe609ba24da1ff93d16740d4474d', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 4, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7971ab328d3734eb2ab3d4092226c6689dd6e83b', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 5, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2e5c49e55b18d5e066e0f81d42b89f06e0977b1d', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 6, 'created': '2017-01-04 13:03:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/80c1125f06ffcd653b6bfd843433476fd5fee7ae', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 7, 'created': '2017-01-04 15:09:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d0cf6f8d85a091f7972e55cf29dec8f4d0bf9877', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 8, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/774eaa129dee29582cf69581bf26ff4c3085316b', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 9, 'created': '2017-01-05 15:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/df2d2387e6747eb8279c62a422dcf3d562962633', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 10, 'created': '2017-01-08 10:06:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e63b5ec25cb934adf1a9b8fd1c7d1347121ef9dd', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 11, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a0dfdd5a0a83036b877c1933a76b2c2e3382e146', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 12, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/847b39e3e234bf4a21c885a4c2743eb2c80e2f09', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 13, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/37100d7c74592b0412230e09ae5e25ced116515b', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 14, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2aa34c0cdf0b50dd37b9c2034cf661c4f008ab89', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 15, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/004135403da89f17852c7d3e8240b1f4679d3266', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 16, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/98e7d149bbd01c2ef1da34d204cfe8e5a98da4ae', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 17, 'created': '2017-01-17 07:40:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/35ab282a05d257ed78054e051a4f2b30dd4bf0c0', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 18, 'created': '2017-01-17 10:46:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ed3f40a62c971388887741cfc3e2137b864b97bb', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 19, 'created': '2017-01-17 13:27:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6cc3d8518056f5728bfe3a2a4d64de62784ce871', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 20, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2c5e944a1fe06a8887ca8e44f181ada4c99b9476', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 21, 'created': '2017-01-22 13:10:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1e73ec08103e5a9a33ef2648935ed6fe88c51b7a', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 22, 'created': '2017-01-22 14:01:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/66ae6c959f756e1f6c978bdc91e8b8511a305fd8', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 23, 'created': '2017-01-23 09:26:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f8f1f2fcee33acdc6ae0e42a97f59fdae0f805ff', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 24, 'created': '2017-01-23 10:52:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/89d6f13bcd0527392cef13e0904d20ff76833ad4', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 25, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/88534edef4484c2d631254446febaa8526c6e755', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 26, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/38ca24a473fa967a2a81a542f7b10ed6a3a4bf19', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 27, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/617e285ef87d3e8fe60683c999b0d90c09bf667f', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\nTODO: add unit tests\n'}, {'number': 28, 'created': '2017-01-25 15:55:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ffd37535f4bc3e07fdec6a19fd26fdd65d52b59f', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 29, 'created': '2017-01-26 08:46:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/58ecbfc0b3496eff825d90fe6e630f02e177247c', 'message': 'Active-Active Topology - Cluster DB Tasks.\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 30, 'created': '2017-01-26 13:43:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/38a20ac1ab3ac80389111e7aba6a36e5582e43f4', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 31, 'created': '2017-01-26 14:56:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/803f3ba393a89f8e0383a8ef3d34dc11d20a5ff8', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 32, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/85934ba88e6856c04f6878db846b41d0f46455cf', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 33, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e307860c9638ed09500a8b9f6bedda6a5ec595e9', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 34, 'created': '2017-02-01 18:43:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a1335df57cc687ddfc0f0fb3d106a02e8ecc3699', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 35, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/422f692e179c4cb259228b0af7851d6fc718122c', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 36, 'created': '2017-02-05 13:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c1a800af1547b618e59a4528534348b87006cb44', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 37, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0bc566b925a2f2801a8181a20dd572e737da56d6', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 38, 'created': '2017-02-06 09:50:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/13cc770cbff9076dd1e88f052e20f7985b753e47', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 39, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bb61473771cfe023996526130df9a6d78b263b09', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 40, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/50d65118b4c7c11e6a126197305a5d9fff49059f', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}, {'number': 41, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/controller/worker/tasks/database_tasks.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/5fc2cf87f767e996b4deb250927ba744518fc1b1', 'message': 'Active-Active Topology - Cluster DB Tasks\n\nAdded DB tasks related to cluster\n\nChange-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6\n'}]",0,409764,5fc2cf87f767e996b4deb250927ba744518fc1b1,120,2,41,21138,,,0,"Active-Active Topology - Cluster DB Tasks

Added DB tasks related to cluster

Change-Id: I47358b12bbdd00f4f9f00f3f200a1ffdff82bcb6
",git fetch https://review.opendev.org/openstack/octavia refs/changes/64/409764/5 && git format-patch -1 --stdout FETCH_HEAD,['octavia/controller/worker/tasks/database_tasks.py'],1,9bb711ae422f217d1ca949532de2fc5e8cc0c9e3,06tmp03," self.amphora_cluster_repo = repo.AmphoraClusterRepository() class CreateAmphoraClusterInDB(BaseDatabaseTask): """"""Task to create an initial cluster in the Database."""""" def execute(self, distributor_id, loadbalancer_id, cluster_dict=None): """"""Creates an amphora_cluster record in the database. :param loadbalancer_id: id of load_balancer for the cluster :param cluster_dict: Dictionary representation of a cluster :return: string to represent octavia.common.data_models.AmphoraCluster """""" if not cluster_dict: cluster_dict = {} LOG.debug(""Creating amphora cluster for loadbalancer id: %s "", loadbalancer_id) amphora_cluster = self.repos.create_amphora_cluster_on_load_balancer( db_apis.get_session(), loadbalancer_id, cluster_dict) self.amphora_cluster_repo.associate(db_apis.get_session(), distributor_id, loadbalancer_id) return ""cluster-"" + str(loadbalancer_id) def revert(self, result, *args, **kwargs): if isinstance(result, failure.Failure): # This task's execute failed, so nothing needed to be done to # revert return # At this point the revert is being called because another task # executed after this failed so we will need to do something and # result is the cluster's id LOG.warn( _LW(""Reverting create cluster in DB for cluster id %s ""), result) # Delete the cluster for now. May want to just update LB status later self.amphora_repo.delete(db_apis.get_session(), id=result) class UpdateAmphoraClusterInDB(BaseDatabaseTask): """"""Update the Amphora Cluster in the DB. Since sqlalchemy will likely retry by itself always revert if it fails """""" def execute(self, loadbalancer_id, update_dict): """"""Update the amphora cluster in the DB :param loadbalancer_id: The load_balancer_id of cluster to be updated :param update_dict: The dictionary of updates to apply :returns: None """""" LOG.debug(""Update AmphoraCluster DB for "" ""loadbalancer_id: %s "", loadbalancer_id) self.cluster_repo.update(db_apis.get_session(), loadbalancer_id, **update_dict) def revert(self, amphora_cluster, *args, **kwargs): """"""Mark the cluster ERROR since the update couldn't happen :returns: None """""" LOG.warn(_LW(""Reverting update amphora_cluster in DB "" ""for amphora_cluster_id %s""), amphora_cluster.id) self.cluster_repo.update(db_apis.get_session(), amphora_cluster.id, enabled=0) class GetAmphoraClusterFromLoadbalancer(BaseDatabaseTask): """"""Task to pull the cluster from a loadbalancer."""""" def execute(self, loadbalancer): """""" :type loadbalancer: data_models.LoadBalancer :rtype: data_models.AmphoraCluster """""" return loadbalancer.amphora_cluster def revert(self, *args, **kwargs): pass class DeleteAmphoraClusterInDB(BaseDatabaseTask): """"""Delete the amphora_cluster in the DB. Since sqlalchemy will likely retry by itself always revert if it fails """""" def execute(self, loadbalancer): """"""Delete the amphora_cluster in DB :param loadbalancer: The load_balanacer of the cluster to delete :type loadbalancer: data_models.LoadBalancer :returns: None """""" LOG.debug(""DB delete amphora_cluster for load_balancer id: %s "", loadbalancer.id) self.cluster_repo.delete(db_apis.get_session(), load_balancer_id=loadbalancer.id) def revert(self, loadbalancer, *args, **kwargs): """"""Mark the amphora_cluster not enabled since the mark active couldn't happen :returns: None """""" LOG.debug(""Reverting amphora cluster delete in DB "" ""for cluster on loadbalancer with id %s"", loadbalancer.id) update_dict = {'enabled': False} self.cluster_repo.update(db_apis.get_session(), loadbalancer_id=loadbalancer.id, **update_dict)",,115,0
openstack%2Foctavia~master~I46cc315dba0be1e9ec640a65d750738340d81ab1,openstack/octavia,master,I46cc315dba0be1e9ec640a65d750738340d81ab1,ACTIVE-ACTIVE Topology - create shared distributor,NEW,2016-12-05 13:28:07.000000000,2017-12-18 03:58:55.000000000,,"[{'_account_id': 16923}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-12-05 13:28:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9c7827324bc8ead03eff054771fe4328cd096f42', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 2, 'created': '2016-12-05 15:19:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/89c8ff60425388824b1d889260ca5ebcfcf6b336', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 3, 'created': '2016-12-06 08:16:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ef00c75262068d9dd4235245b9afcfefce6df0d7', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 4, 'created': '2016-12-06 09:43:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e87d5bb466ea1738a0f38ec15e8ac934fdb25bcb', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 5, 'created': '2016-12-06 14:02:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/68e44dcbfc884670d04f735b8ff32e1294ae3f26', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 6, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6e365d7f47e0838d6ff040a5203896053b17cf9b', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 7, 'created': '2016-12-13 08:53:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f832883198f2b6de77f5346f1fd607d9e129d028', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 8, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/dbcd563ae948617ad689d91d17bfad44cba6e2d3', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 9, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f4a1cf27699d0f64d35fa12e1938feb00b047d17', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 10, 'created': '2017-01-04 13:03:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9710cc0db90f81bf6a2795ab985482195de77e7a', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 11, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/12db6f878bc4d2885b8891417edb6c7ca0323aff', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 12, 'created': '2017-01-05 15:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ceb23a796ae2f342bd630cc01445b17c716d386e', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 13, 'created': '2017-01-08 10:06:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b1a80030a06ee01bd73c22aeef0d4ad424574e24', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 14, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/48be5fd050174525acabf314564e6aa398ad7dc9', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 15, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c4a48a47ef9aa166ba263eccbeb66123d3cd9a96', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 16, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9ca6829956781ae8c17223febf8221227d6d8a19', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 17, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ae76e8cc4cfaf54c749a30ae95fb8c6d0ee1b781', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 18, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cd92fa653307358449d3e511463e0af304eafc37', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 19, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/971fb001fc652efd43831b64d507bd08bb141386', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 20, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e9956ccf854484ab05a0416a8d3d787087f33142', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 21, 'created': '2017-01-22 13:10:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/66e645a178bb16d67f00d0d0d5f5d04bd7b565d1', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 22, 'created': '2017-01-23 09:26:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8382332b88e9770414525256a0a116647f39be77', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 23, 'created': '2017-01-23 10:52:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bb6fd72b57a6c5459c9231205d329081c1904266', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 24, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5cdfe5c4a7a55c132751f82200775008954864f0', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 25, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6759852f70a8ef8b99fb28bbf2fb78a3cc4d4e2a', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 26, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/65dd796a1348ef4518d349041dcaa6d589d6dc8a', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 27, 'created': '2017-01-26 14:56:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cfa87e3336791d3ad129e968ce6f27517b1341ce', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 28, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/05e096e71675cc916eaa0ef01568f272f0b4b63c', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 29, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d0cc5d46cf46d005910a3ab10ff4d8a6e1e0ee41', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 30, 'created': '2017-02-01 18:43:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/10f1394098d23768c8e7f977407f6a5afb4b6ee2', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 31, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/99bbfa9e01646565d626ea1b3ab67ed6068217fa', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 32, 'created': '2017-02-05 13:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/51e5e865f1c5122ea6520cd3884c0cec5286a1d4', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 33, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/fa51e4cd7dd79327ce673160b074a5b061fec388', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 34, 'created': '2017-02-06 09:50:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b52a1e3919eb4f33280dc4c6b446aefac9476b88', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 35, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9004aef9e855398362986a62b030a648a9073c6b', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 36, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/922a6139be4e89d31d8f1a7060626da55e55c7fd', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}, {'number': 37, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/distributor/backend/agent/templates/__init__.py', 'octavia/controller/worker/tasks/database_tasks.py', 'octavia/amphorae/cluster_manager/drivers/active_active/active_active_driver.py', 'octavia/controller/worker/controller_worker.py', 'octavia/distributor/backend/agent/templates/distributor_agent_conf.template', 'octavia/tests/unit/controller/worker/flows/test_amphora_cluster_flows.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/57d499c49df895041a9e514d408c88175309c08e', 'message': 'ACTIVE-ACTIVE Topology - create shared distributor\n\nChange-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1\nImplements: blueprint https://review.openstack.org/#/c/234639\n'}]",0,406954,57d499c49df895041a9e514d408c88175309c08e,114,2,37,21138,,,0,"ACTIVE-ACTIVE Topology - create shared distributor

Change-Id: I46cc315dba0be1e9ec640a65d750738340d81ab1
Implements: blueprint https://review.openstack.org/#/c/234639
",git fetch https://review.opendev.org/openstack/octavia refs/changes/54/406954/21 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/distributor/backend/agent/templates/__init__.py', 'octavia/controller/worker/tasks/database_tasks.py', 'octavia/amphorae/cluster_manager/drivers/active_active/active_active_driver.py', 'octavia/controller/worker/flows/amphora_cluster_flows.py', 'octavia/controller/worker/controller_worker.py', 'octavia/distributor/backend/agent/templates/distributor_agent_conf.template']",6,9c7827324bc8ead03eff054771fe4328cd096f42,06tmp03,"{# Copyright 2015 Hewlett-Packard Development Company, L.P. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. #} [DEFAULT] debug = {{ debug }} [distributor] base_cert_dir = {{ base_cert_dir }} base_path = {{ base_path }} bind_host = {{ bind_host }} bind_port = {{ bind_port }} respawn_count = {{ respawn_count }} respawn_interval = {{ respawn_interval }} [health_manager] controller_ip_port_list = {{ controller_list|join(', ') }} heartbeat_interval = {{ heartbeat_interval }} heartbeat_key = {{ heartbeat_key }} [distributor_agent] agent_server_ca = {{ agent_server_ca }} agent_server_cert = {{ agent_server_cert }} agent_server_network_dir = {{ agent_server_network_dir }} distributor_id = {{ distributor_id }} ",,89,1
openstack%2Foctavia~master~I0501873213abcd20c3b6938029dcd783d5bade46,openstack/octavia,master,I0501873213abcd20c3b6938029dcd783d5bade46,Active-Active Topology - register/uregister amphorae tasks,NEW,2016-12-12 12:22:18.000000000,2017-12-18 03:58:53.000000000,,"[{'_account_id': 16923}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-12-12 12:22:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/69792a232623b6e12f5f3cf3b1864133eb3ad3f6', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 2, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c7f37690cdf9cba2629cf6cc90e5db587274f1f2', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 3, 'created': '2016-12-13 08:54:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/252615daea79d3ccec77ad730464776c832c17a5', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 4, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3e7712b343191bc05a76a4dd6cb0d085ad4d42d2', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 5, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f1cf109d14ec31b766e14e532ed7d0de01d17f86', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 6, 'created': '2017-01-04 13:03:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c015cadc8b2ed74bd4103396e37517d34a5c4f6d', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 7, 'created': '2017-01-04 15:09:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/83d2e3f0bcf8bb7a02795c282fee9d938660b8c5', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 8, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/84d1aac6cd45579f3f7d09fcc0d20415ca442fc0', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 9, 'created': '2017-01-05 15:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b96a249c9dbaddd2f918b7754c8880054a25cc3f', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 10, 'created': '2017-01-08 10:06:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7102fda99104da63387e2fa96215b1c17e252e2a', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 11, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/795c0eef2e6fc63b31ab364c548a707e731742a3', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 12, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/85d10fcf18c475b47ed64bfdebce6d94a3b379d5', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 13, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a30c365c270d3c61d8fae8b62fb17797fe989e9f', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 14, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bea12068a31112b5bf1ee7092a17c25db4dea345', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 15, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bf497e05f5f0a09ee31a12e09fce5619491e5a25', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 16, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b641f39ba2a4eabd20c52621cdc4e0daead6b50b', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 17, 'created': '2017-01-17 07:40:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/02f8230a08f12f8243e43cfc44fef806dc9f0e82', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 18, 'created': '2017-01-17 10:46:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/686d5b55aa0252131f53014a6c28bbdb46e5c196', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 19, 'created': '2017-01-17 13:27:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/157a6edcec6b1770321bb29e212807225f66d737', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 20, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/52a59e429b4e8240d8b3eda81fd5550bf40b9ace', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 21, 'created': '2017-01-22 13:10:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c993494d313299d7b444696f6d132f70f3680a8e', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 22, 'created': '2017-01-22 14:01:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/998f31125404e9b12af7648c837d769c125c4bea', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 23, 'created': '2017-01-23 09:26:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/20323a258761c237f8af8dd2773e344685f80bda', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 24, 'created': '2017-01-23 10:52:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/18a87e90cdc7ff7f7a08bffc09ddc4f1301481bd', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 25, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7bce7f21923d1486d2a2a3708d43bf44a2dbcaef', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 26, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0a1288355ca203fd8fa3caf69f024e995a11a9ad', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 27, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9dfdfa8cc2de8e7477afc9ff590f78bc3f3f2625', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 28, 'created': '2017-01-25 15:55:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8ae771f58c0a20f9c28708ae3058f66692c51f9d', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 29, 'created': '2017-01-25 16:12:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/55972f1ddc200fb77db754801f80cf720413074a', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 30, 'created': '2017-01-26 08:56:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8ff402e5851bec894caa416dc6aed7dc356f9925', 'message': 'Active-Active Topology - register/uregister amphorae tasks.\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 31, 'created': '2017-01-26 13:43:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1270c20b29c142d31d92c5b7dd02c7865971267b', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 32, 'created': '2017-01-26 14:56:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8ac8fc39cfd6d3fcbc7de990dd3b19de76040357', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 33, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/93aece78f2e62523d849c1fb48866520103f78ce', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 34, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1f1e9d885f3cd83601f8e86cc30f023f45f223ba', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 35, 'created': '2017-02-01 18:43:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1268504ed0e6607f6b2328448e12412c45f3c4fe', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 36, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/688dbde8ffc567cb658fd39c968d8d50427e3690', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 37, 'created': '2017-02-05 13:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6d4ef7a8b914e184d79aa715026e38b6c869eddd', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 38, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/63e5dd40250121e3865dac9a0e156b2fc699eb30', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 39, 'created': '2017-02-06 09:50:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d98ba91a4e39c860e3b3e6cd4ef812610af81154', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 40, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/4adce372f2f6459c3b0ce147a02ff8a576a06b75', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 41, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5f0075fbf80d500a3a8d1d752bbbbbb53ae4b182', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}, {'number': 42, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/controller/worker/tasks/distributor_driver_tasks.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/38ad54c3465837b4881b4824a2b8e270430c7f94', 'message': 'Active-Active Topology - register/uregister amphorae tasks\n\nDistributor Driver Tasks\nSupport distributor register/unregister amphorae\n\nChange-Id: I0501873213abcd20c3b6938029dcd783d5bade46\n'}]",0,409765,38ad54c3465837b4881b4824a2b8e270430c7f94,124,2,42,21138,,,0,"Active-Active Topology - register/uregister amphorae tasks

Distributor Driver Tasks
Support distributor register/unregister amphorae

Change-Id: I0501873213abcd20c3b6938029dcd783d5bade46
",git fetch https://review.opendev.org/openstack/octavia refs/changes/65/409765/23 && git format-patch -1 --stdout FETCH_HEAD,['octavia/controller/worker/tasks/distributor_driver_tasks.py'],1,69792a232623b6e12f5f3cf3b1864133eb3ad3f6,06tmp03,"class DistributorGetInfo(BaseDistributorTask): """"""Task to get information on a distributor."""""" def execute(self, distributor): """"""Execute get_info routine for a distributor."""""" self.distributor_driver.get_info(distributor) class DistributorRegisterAmphora(BaseDistributorTask): def execute(self, distributor, loadbalancer, amphora, alg_type, cluster_min_size): load_balancer = self.loadbalancer_repo.get(db_apis.get_session(), id=loadbalancer.id) self.distributor_driver.register_amphora(distributor, load_balancer, amphora, alg_type, cluster_min_size) def revert(self, result, loadbalancer, *args, **kwargs): if isinstance(result, failure.Failure): return LOG.warn(_LW(""Reverting register amphora."")) class DistributorUnregisterAmphora(BaseDistributorTask): def execute(self, distributor, loadbalancer, amphora, alg_type, cluster_min_size): load_balancer = self.loadbalancer_repo.get(db_apis.get_session(), id=loadbalancer.id) self.distributor_driver.register_amphora(distributor, load_balancer, amphora, alg_type, cluster_min_size) def revert(self, result, loadbalancer, *args, **kwargs): if isinstance(result, failure.Failure): return LOG.warn(_LW(""Reverting register amphora.""))",,38,0
openstack%2Foctavia~master~Ida69d6c83776e42ef06ac1657e4edcfdca21664d,openstack/octavia,master,Ida69d6c83776e42ef06ac1657e4edcfdca21664d,ACTIVE-ACTIVE Topology - Distributor related tasks,NEW,2016-12-05 13:28:07.000000000,2017-12-18 03:58:50.000000000,,"[{'_account_id': 16923}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-12-05 13:28:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ee38fcf99fc8e48dbbfd38c993e7a6a3b6145cd0', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 2, 'created': '2016-12-05 15:19:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/4d8dcf307ebd7ad1e16dbb32016dead37ff8d5ff', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 3, 'created': '2016-12-06 09:43:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8e6a9b26146664062d522bc748858f62c2ee1d05', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 4, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e0b758a952c70f793537e8f7e56afc18145f0b2c', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 5, 'created': '2016-12-13 08:53:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f239726112e82077fc9c70cecb10abd27f3f0d0a', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 6, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/48231d92b04c26cc6dadb2c1aaace25b5f3defbd', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 7, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/4883037ad84e1b85389b8e13dedc4355b11f842a', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 8, 'created': '2016-12-19 11:42:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8a068a0b54cbeb875374d977d8a62e7d62a68d5f', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 9, 'created': '2017-01-04 12:46:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/16e748fb681dd7ac14ae24dbaca386a50a9d5860', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 10, 'created': '2017-01-04 12:58:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/91f20348dce143b3ceecffe4c3a959f4609c31b3', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 11, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/eadbfd7a7c387a962f510715f4fa6f271b5c5638', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 12, 'created': '2017-01-05 15:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/89e28291db06e4052dc522cc134edf556819a315', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 13, 'created': '2017-01-08 10:06:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c022108b7248acbd89aba16aed7abe3f0bbe6301', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 14, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/506837125bf61ae708401a84fceb0304349aa807', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 15, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1dba1c9280c5852f88bda7cdaa5abba34bf17d2c', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 16, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/63e5a47f37537b10ffcdc4158cddca4f0f07c6fb', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 17, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3d0752a2a4f7915766d99674e0a778b455b1a0f2', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 18, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/470a36e2eedd6de380bd8a6b34b517bdeb9086ab', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 19, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/386cacee1a64e1c75d2aaf3d5b9947b252842332', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 20, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/319104af681b91c7a513fb8833c9d98703577377', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 21, 'created': '2017-01-22 13:10:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/4230832840823f08e595621d5144a07cf40bfae9', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 22, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7269f44cc18b8aa9e7994bc85abe7f0edbf79064', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 23, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/030f0dfb6496f5bb9316a39d848d38459628c1f0', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 24, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b51cc8b3dd8ed0cb2712b05ea38c2dcc7ee8406e', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 25, 'created': '2017-01-26 14:56:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f8eaa827650a7f5099fee3013b54aa02a9642c10', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 26, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/79bca70e1b75f1b5334fc26dc8a5d8d9e7fb80ec', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 27, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/aab9f8779a8dd3333067c305efc1a933c534d9d5', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 28, 'created': '2017-02-01 18:43:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/726ce18608441cfb6c9b865db45c674c501dc121', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 29, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/827810632be89633a93a31dcc9552be548290698', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 30, 'created': '2017-02-05 13:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/322811d6e56d336251479108aa355fefe6904a2c', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 31, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/501d69b3b0518992b351b4d8c1ad6e251302b653', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 32, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/39859c8e9d559821e5573ce5a34bf5bae6a6feb6', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 33, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/29bea8a3d44832400696f9971b10c8132521a62b', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}, {'number': 34, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/common/config.py', 'octavia/compute/compute_base.py', 'octavia/controller/worker/tasks/compute_tasks.py', 'octavia/compute/drivers/nova_driver.py', 'octavia/controller/worker/flows/distributor_flows.py', 'octavia/distributor/backend/agent/distributor_jinja_cfg.py', 'octavia/tests/unit/compute/drivers/test_compute_noop_driver.py', 'octavia/tests/unit/controller/worker/tasks/test_database_tasks.py', 'octavia/controller/worker/tasks/database_tasks.py', 'octavia/tests/unit/controller/worker/tasks/test_compute_tasks.py', 'octavia/tests/unit/compute/drivers/test_nova_driver.py', 'octavia/compute/drivers/noop_driver/driver.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/850f7c2374b2cef7549477db3af4bf6135d67c11', 'message': 'ACTIVE-ACTIVE Topology - Distributor related tasks\n\nAdded methods to the compute drivers;\nDB tasks and compute node related tasks\nto support distributor node creation\n\nChange-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d\n'}]",0,406951,850f7c2374b2cef7549477db3af4bf6135d67c11,106,2,34,21138,,,0,"ACTIVE-ACTIVE Topology - Distributor related tasks

Added methods to the compute drivers;
DB tasks and compute node related tasks
to support distributor node creation

Change-Id: Ida69d6c83776e42ef06ac1657e4edcfdca21664d
",git fetch https://review.opendev.org/openstack/octavia refs/changes/51/406951/11 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/common/config.py', 'octavia/compute/compute_base.py', 'octavia/controller/worker/tasks/compute_tasks.py', 'octavia/compute/drivers/nova_driver.py', 'octavia/controller/worker/flows/distributor_flows.py', 'octavia/distributor/backend/agent/distributor_jinja_cfg.py', 'octavia/tests/unit/compute/drivers/test_compute_noop_driver.py', 'octavia/tests/unit/controller/worker/tasks/test_database_tasks.py', 'octavia/controller/worker/tasks/database_tasks.py', 'octavia/controller/worker/tasks/distributor_driver_tasks.py', 'octavia/tests/unit/controller/worker/tasks/test_compute_tasks.py', 'octavia/tests/unit/compute/drivers/test_nova_driver.py', 'octavia/compute/drivers/noop_driver/driver.py']",13,ee38fcf99fc8e48dbbfd38c993e7a6a3b6145cd0,06tmp03," def build(self, name=""compute_name"", comp_flavor=None, LOG.debug(""Compute %s no-op, build name %s, comp_flavor %s, "" name, comp_flavor, image_id, image_tag, image_owner, self.computeconfig[(name, comp_flavor, image_id, image_tag, name, comp_flavor, def distributor_status(self, compute_id): LOG.debug(""Compute %s no-op, compute_id %s"", self.__class__.__name__, compute_id) self.computeconfig[compute_id] = (compute_id, 'distributor_status') return constants.UP def get_distributor(self, compute_id): LOG.debug(""Compute %s no-op, compute_id %s"", self.__class__.__name__, compute_id) self.computeconfig[compute_id] = (compute_id, 'get_distributor') return data_models.Distributor( compute_id=compute_id, status=constants.DISTRIBUTOR_ACTIVE, lb_network_ip='192.0.2.1' ) def build(self, name=""compute_name"", comp_flavor=None, compute_id = self.driver.build(name, comp_flavor, def distributor_status(self, compute_id): return self.driver.distributor_status(compute_id) def get_distributor(self, compute_id): return self.driver.get_distributor(compute_id) "," def build(self, name=""amphora_name"", amphora_flavor=None, LOG.debug(""Compute %s no-op, build name %s, amphora_flavor %s, "" name, amphora_flavor, image_id, image_tag, image_owner, self.computeconfig[(name, amphora_flavor, image_id, image_tag, name, amphora_flavor, def build(self, name=""amphora_name"", amphora_flavor=None, compute_id = self.driver.build(name, amphora_flavor,",948,106
openstack%2Foctavia~master~I421b24187cd9b2e12a6d63515cabdc3a12a07d04,openstack/octavia,master,I421b24187cd9b2e12a6d63515cabdc3a12a07d04,ACTIVE-ACTIVE Topology : create distributor network flow,NEW,2016-12-12 12:22:18.000000000,2017-12-18 03:58:48.000000000,,"[{'_account_id': 8871}, {'_account_id': 16923}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-12-12 12:22:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/47a7597552f0f717e5c9e29445587132e545b24d', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 2, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/fa46b11f43ff20b9bf08c61e6be2bc5928558a9d', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 3, 'created': '2016-12-13 08:53:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/29cdbdfdedc48909c014ebddc9b25dd359115719', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 4, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/699645cbb58ae32eb774b5aa0ae4cb1bcd5075ce', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 5, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6dc5abfd435fafa4663b958d6fdff1bd5a6d2351', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 6, 'created': '2017-01-04 13:03:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5131c66273b1cf971bc48a5de45aad14afeb0e08', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 7, 'created': '2017-01-04 15:09:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/23de59167f32f97fca59b591825ecf098ad40809', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 8, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f75743ad9ba095999e3206f50edbb96a6b0495f6', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 9, 'created': '2017-01-05 15:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f1c9d7e5d5352b32531cd8445bf51f6e83067e5f', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 10, 'created': '2017-01-08 10:06:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c65f1706b4c051cd894c3e3e7073ab68893d27f5', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 11, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7b6157e706f25f6621a19be4c94bddc86289d5b2', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 12, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b8993c6fde1e870a4b0b485661439087bb3a8d54', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 13, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d51206f728bfcfb87eb23758dbbf806b4b203396', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 14, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9f5f0f77b81b1ccc32202c846fef94151c85a7f3', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 15, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/985809e178cade8539113e15d8ef4b037c04b470', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 16, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e59c5a7ba8985b4ae00001d9d5fb16660c79a263', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 17, 'created': '2017-01-17 07:40:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7c4f6d3b086722277677862001b5de9c33c0b492', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 18, 'created': '2017-01-17 10:46:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9e194612a7f33405a1ff1c4414b37279fd969d13', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 19, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/aa26d1b895ad1570bca2572e787a6261af607bb5', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 20, 'created': '2017-01-22 14:01:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5934157050236e18621d57db84a90278b6df4965', 'message': 'ACTIVE-ACTIVE Topology - create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 21, 'created': '2017-01-22 16:55:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a0ae1fb81144f028eee15e1525bdcbdebce5f3a7', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 22, 'created': '2017-01-23 09:26:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e3d8b58d5f60cad110540e90f9928a5ed052615f', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 23, 'created': '2017-01-23 10:52:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ece3a0206e5c6884eeb6b621ca0275045f6757eb', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 24, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3bd28d333977be35e5a30bbeac836a8782852b7f', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 25, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9e6bcb36604f39f46fc85a6b85439068918b548c', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 26, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e20add79787f60a4afe9ce67fb44c1d6029a8190', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 27, 'created': '2017-01-26 14:56:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c7456f7310a559b0597fb45d1a807539853e24e1', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 28, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7b49e913ec4d31e56468c80b82c09b5048611505', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 29, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9c516234e106287622ea028bc2d3bbcb12bdcc5f', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 30, 'created': '2017-02-01 18:43:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6cbc1b4d47707c109e0d2225298db52117f7a583', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 31, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/35344949aee72feb1973b7427cbc62acc82698c8', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 32, 'created': '2017-02-05 13:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b3be21e6a65920f299dd2a71124684b5af3c4234', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 33, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/457239c3caf32c40541613123e2e96ee074cfb94', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 34, 'created': '2017-02-06 09:50:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/60003e66a25a47f5cf435768cdb9d003ae937f9a', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 35, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7c226640e21656a09c27ea9b92885f2379c0d38b', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 36, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cdd61811ddf3519370af74180aafccc77ddfe164', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}, {'number': 37, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/tests/unit/controller/worker/flows/test_distributor_flows.py', 'octavia/common/constants.py', 'octavia/controller/worker/tasks/distributor_driver_tasks.py', 'octavia/controller/worker/flows/distributor_flows.py', 'octavia/tests/unit/controller/worker/tasks/test_distributor_driver_tasks.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/70c4316186baa7f2e166633023ea5825e0fac092', 'message': 'ACTIVE-ACTIVE Topology : create distributor network flow\n\nAdds distributor network subflow for ACTIVE-ACTIVE topology.\nIn this flow VIP for new load balancer is allocated and the\ndistributor is plugged with the VIP\n\nChange-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04\n'}]",0,409763,70c4316186baa7f2e166633023ea5825e0fac092,113,3,37,21138,,,0,"ACTIVE-ACTIVE Topology : create distributor network flow

Adds distributor network subflow for ACTIVE-ACTIVE topology.
In this flow VIP for new load balancer is allocated and the
distributor is plugged with the VIP

Change-Id: I421b24187cd9b2e12a6d63515cabdc3a12a07d04
",git fetch https://review.opendev.org/openstack/octavia refs/changes/63/409763/32 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/common/constants.py', 'octavia/tests/unit/controller/worker/flows/test_distributor_flows.py', 'octavia/controller/worker/tasks/distributor_driver_tasks.py', 'octavia/controller/worker/flows/distributor_flows.py', 'octavia/distributor/drivers/noop_driver/driver.py', 'octavia/tests/unit/controller/worker/tasks/test_distributor_driver_tasks.py']",6,47a7597552f0f717e5c9e29445587132e545b24d,06tmp03,"import mock from oslo_config import cfg from oslo_utils import uuidutils from taskflow.types import failure from octavia.common import constants from octavia.common import data_models from octavia.controller.worker.tasks import distributor_driver_tasks from octavia.db import repositories as repo import octavia.tests.unit.base as base DISTRIBUTOR_ID = uuidutils.generate_uuid() COMPUTE_ID = uuidutils.generate_uuid() LISTENER_ID = uuidutils.generate_uuid() LB_ID = uuidutils.generate_uuid() MOCK_MAC_ADDR = 'fe:16:3e:00:95:5c' _distributor_mock = mock.MagicMock() _distributor_mock.id = DISTRIBUTOR_ID _distributor_mock.status = constants.DISTRIBUTOR_READY _load_balancer_mock = mock.MagicMock() _load_balancer_mock.id = LB_ID _LB_mock = mock.MagicMock() _session_mock = mock.MagicMock() @mock.patch('octavia.db.api.get_session', return_value=_session_mock) @mock.patch('octavia.controller.worker.tasks.amphora_driver_tasks.LOG') @mock.patch('oslo_utils.uuidutils.generate_uuid', return_value=DISTRIBUTOR_ID) @mock.patch('stevedore.driver.DriverManager.driver') class TestDistributorDriverTasks(base.TestCase): def setUp(self): super(TestDistributorDriverTasks, self).setUp() _LB_mock.id = LB_ID @mock.patch('octavia.db.repositories.LoadBalancerRepository.update') @mock.patch('octavia.db.repositories.LoadBalancerRepository.get', return_value=_LB_mock) def test_distributor_post_vip_plug(self, mock_loadbalancer_repo_update, mock_loadbalancer_repo_get, mock_driver, mock_generate_uuid, mock_log, mock_get_session): distributor_post_vip_plug_obj = \ distributor_driver_tasks.DistributorPostVIPPlug() distributor_post_vip_plug_obj.execute(_distributor_mock, _LB_mock, MOCK_MAC_ADDR, "" "", 0) mock_driver.post_vip_plug.assert_called_once_with( _distributor_mock, _LB_mock, MOCK_MAC_ADDR, "" "", 0) # Test revert distr = distributor_post_vip_plug_obj.revert(None, _LB_mock) repo.LoadBalancerRepository.update.assert_called_once_with( _session_mock, id=LB_ID, status=constants.ERROR) self.assertIsNone(distr)",,185,13
openstack%2Foctavia~master~Id99afa8674dd1c5ad30bfd15790941972065456c,openstack/octavia,master,Id99afa8674dd1c5ad30bfd15790941972065456c,ACTIVE-ACTIVE Topology - Distributor image creation,NEW,2016-11-28 09:39:55.000000000,2017-12-18 03:58:46.000000000,,"[{'_account_id': 8788}, {'_account_id': 10068}, {'_account_id': 16923}, {'_account_id': 21138}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-11-28 09:39:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8d3eb66d3e786d57b2bc29bf7f08df1890d66ebd', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 2, 'created': '2016-11-28 11:02:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3a2b9dada392eedf1939026bafe6ff131485e033', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 3, 'created': '2016-11-28 11:40:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6f41c8d4eb1c24a028709fa55d7952c7b25362ab', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 4, 'created': '2016-11-28 11:51:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/babd3f5a014ef02077867d06737e9aa52c71f8ae', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 5, 'created': '2016-11-30 15:17:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6663c29deda72840913237c064f1df27b936346e', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 6, 'created': '2016-12-01 09:55:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6b42eeac10f5b113d446bb21bc29c3978a565d92', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 7, 'created': '2016-12-05 11:33:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8a381fefc28fac1965fec8051d47e3ba3b79d44d', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 8, 'created': '2016-12-07 09:22:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3ee4729aae538080f49a72622fb1e58d8bf4125c', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 9, 'created': '2016-12-07 11:43:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f51cb81386fe14011285a0db3bb9165bde0aaa0e', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 10, 'created': '2016-12-07 14:50:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/4be62294a4b3ece6b8bf63cbe7605f856f8a4623', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 11, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/39823ce8c67a0be2b5a0139dd26f15bd83f7efd2', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 12, 'created': '2016-12-13 08:52:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/052b13a1cf62294fb4348b63ac93a1f5df003c1c', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 13, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9219bf6eb8938f122287f60ef753ccd123a2d197', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 14, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/45a6eb5ae19fc46e86df60a07b108e8ca65d657d', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 15, 'created': '2016-12-18 08:57:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0b42f717f1974dcf1744dc98583dfc13445ea453', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 16, 'created': '2016-12-18 11:15:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/53780903ac24036a46e4c15021abc7e38f486d1f', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 17, 'created': '2016-12-20 13:45:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/34047f16b7bb1877590bb5bf1e050ad5bf4d97cf', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 18, 'created': '2017-01-04 09:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5d0d0156e7ca86ac4b1e10a607a1e0a6b085df1d', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 19, 'created': '2017-01-04 12:41:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b3b39bebdf7d18fa90c43dc87089270b9a6677d1', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 20, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/01dc55b5323f0d53986b7f2ab55c9765f2a00776', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 21, 'created': '2017-01-05 15:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1ff721d49fe7707759af6d49b6cc3a9024407ca1', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 22, 'created': '2017-01-08 10:06:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/af1c6341594c189f6b47df9395f90f5393001048', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 23, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1200f1c6b9c4c0fdf334fe755a7188792d5bb530', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 24, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2a7c158aa0bc249f4ad4646aadc9ddedc824f926', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 25, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/88ed272b487f13100b3008a768bd8e7b06943dfc', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 26, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/64bdd25549fdf7df022ebab872fea78592eab38d', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 27, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/4793511b5e80d2ffc87cdacbf029fc941ab01645', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 28, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ed0928b5b9abeb24691d096eee42e60be3d53b56', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 29, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cf25d366a6822f75b4290130c091698de60b8d65', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 30, 'created': '2017-01-22 13:10:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/886cffcf24c528eb1abd6a4a22a184cde7e87c49', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 31, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3583df2821638a90b5daa9ac45921c8596edef88', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 32, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9d4d5dbb23429cdd77903e71f46cbb0b74f2bbf3', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 33, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6e713389ca0486a1173d395ba15818209deecd2a', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 34, 'created': '2017-01-26 14:56:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/21ec6b7087c41576a8bc70bf9a83a165c8f3106f', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 35, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8f82b20d77d49285aa3645a06bd7336a78b496e3', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 36, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ab928c09e4dc49b303524af5159ee28df0f20e68', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 37, 'created': '2017-02-01 18:43:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0f6aea27790c6315b6d55d7cd574fcf7ddbf7646', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 38, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ae35f83f201420db42b069d58d0753ab154e7615', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 39, 'created': '2017-02-05 13:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/841eb8a048da78e69a06100fb7577813b742e0e0', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 40, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/de8ca456d3dd6f1d7b8d262c0b7c0eb7aed284c1', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 41, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d1865a9e191e3dcd8ec982ee86334953231580ea', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 42, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/623831a2553f93a4c57630618804925f970f0c64', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}, {'number': 43, 'created': '2017-03-06 15:52:04.000000000', 'files': ['elements/distributor-agent/README.rst', 'elements/openvswitch-octavia/README.rst', 'octavia/common/config.py', 'octavia/distributor/backend/agent/api_server/server.py', 'diskimage-create/diskimage-create.sh', 'elements/openvswitch-octavia/install.d/77-run_setup_install', 'devstack/plugin.sh', 'elements/distributor-agent/install.d/75-run_setup_install', 'etc/octavia.conf', 'devstack/settings', 'etc/init/distributor-agent.conf', 'elements/distributor-agent/source-repository-distributor-agent', 'octavia/common/constants.py', 'elements/distributor-agent/svc-map', 'etc/initd/distributor-agent', 'octavia/cmd/distributor_agent.py', 'elements/distributor-agent/element-deps', 'elements/openvswitch-octavia/svc-map', 'octavia/opts.py', 'setup.cfg', 'elements/distributor-agent/package-installs.yaml'], 'web_link': 'https://opendev.org/openstack/octavia/commit/2f4640a8a902b253d7ef8e74d6e7a6448e9788aa', 'message': 'ACTIVE-ACTIVE Topology - Distributor image creation\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nFiles to support distributor image create\n\nChange-Id: Id99afa8674dd1c5ad30bfd15790941972065456c\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\nCo-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>\nCo-Authored-By: Abed Abu dbai <abeda@il.ibm.com>\n'}]",13,403594,2f4640a8a902b253d7ef8e74d6e7a6448e9788aa,144,5,43,24261,,,0,"ACTIVE-ACTIVE Topology - Distributor image creation

Implements: blueprint https://review.openstack.org/#/c/234639

Files to support distributor image create

Change-Id: Id99afa8674dd1c5ad30bfd15790941972065456c
Co-Authored-By: Valeria Perelman <perelman@il.ibm.com>
Co-Authored-By: Banashankar K Veerad <bkalebe@us.ibm.com>
Co-Authored-By: Abed Abu dbai <abeda@il.ibm.com>
",git fetch https://review.opendev.org/openstack/octavia refs/changes/94/403594/40 && git format-patch -1 --stdout FETCH_HEAD,"['elements/distributor-agent/README.rst', 'elements/openvswitch-octavia/README.rst', 'octavia/common/config.py', 'elements/openvswitch-octavia/source-repository-openvswitch-octavia', 'octavia/distributor/backend/agent/api_server/server.py', 'diskimage-create/diskimage-create.sh', 'elements/openvswitch-octavia/install.d/77-run_setup_install', 'devstack/plugin.sh', 'elements/distributor-agent/install.d/75-run_setup_install', 'etc/octavia.conf', 'devstack/settings', 'etc/init/distributor-agent.conf', 'elements/distributor-agent/source-repository-distributor-agent', 'octavia/common/constants.py', 'elements/distributor-agent/svc-map', 'etc/initd/distributor-agent', 'octavia/cmd/distributor_agent.py', 'elements/distributor-agent/element-deps', 'elements/openvswitch-octavia/svc-map', 'octavia/opts.py', 'setup.cfg', 'elements/distributor-agent/package-installs.yaml']",22,8d3eb66d3e786d57b2bc29bf7f08df1890d66ebd,06tmp03,libffi-dev: libssl-dev: ,,510,23
openstack%2Foctavia~master~I07b7847f670bcbbba3baa88a98eb42b242f3a6cb,openstack/octavia,master,I07b7847f670bcbbba3baa88a98eb42b242f3a6cb,ACTIVE-ACTIVE - distributor certificate tasks,NEW,2016-12-05 13:28:07.000000000,2017-12-18 03:58:43.000000000,,"[{'_account_id': 8871}, {'_account_id': 16923}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-12-05 13:28:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a6fd8f219b19fbb513295dd4db3a8fb080dc5d2c', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 2, 'created': '2016-12-05 15:19:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6689d4c1a4cf1fcd293d20ac250d85ca49d2d69c', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 3, 'created': '2016-12-06 09:43:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/768b449c3d471c81cb85560eee9b78d4849a14c9', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 4, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/fcd729e65538c27bca5644ee1bc3a4152fc5f3a1', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 5, 'created': '2016-12-13 08:53:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/92a6bd917c1e73269bbfb6e59a42a093b6ffa4c3', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 6, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3f22301d452fce61f2672a71a62e6d6a2c2b908d', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 7, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e130657b3c35311e8e81909e80af8e12312007ee', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 8, 'created': '2016-12-19 12:55:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d80bcf9729ba16a97769a96a92f5647bafe72dd9', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 9, 'created': '2016-12-19 15:27:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/33490227ecc835073dc9fe4280c279edcb6779b5', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 10, 'created': '2017-01-04 12:46:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/dacf678c8012b32da1f834f452555acd00ba4e22', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 11, 'created': '2017-01-04 12:58:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/76667f9057ba29d4335056cb622c6e42616d4588', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 12, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ecd3f12e806ac7989f2c10d5e37976b390a52049', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 13, 'created': '2017-01-05 15:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d3ac6caa7e455efe5f755e1eac533c2a5af37f1d', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 14, 'created': '2017-01-08 10:06:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/04a5dd26b7237e609d65ea95678567aa461abf71', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 15, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/80b3905d8b11801829bbd789847a55585060c75f', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 16, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9a0acec1831da0577b0f50940f446e28bc4dc80b', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 17, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/176cbf6790d66be827ae04d1313f55f20873e809', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 18, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/93c3af2327bc033f0fcd9033ee5edbb0792c54a8', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 19, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6aaafba0744d45a70d417a01e72b96ce5c5b7674', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 20, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bd73d0bdd8e47d64540d50fa5a339601ee4d1f80', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 21, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d6c2e43308d6580a735d382b1c99fd2909010458', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 22, 'created': '2017-01-22 13:10:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cdaa8b114d72bb196cd86972d27bd81c65a8db42', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 23, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5b9834a738e0e760aaca76ddcc5075cda574a204', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 24, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/07a4e7b819d8ed49225bf373804c50c2c18225b1', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 25, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a12ed97e7b0b0eafacc59d9ab71f4648d187b354', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 26, 'created': '2017-01-26 14:56:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3556bf7982b5a70ed90db3d9450643d9b673fa8b', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 27, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5477b7f110725b6aeeedcf9a7dcfa1b02666b3cf', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 28, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0d72060fbfd3c694ba168cb848218734fd807e34', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 29, 'created': '2017-02-01 18:43:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f5a6ac025d5d34c2694708cfdd0c5841cf2abbac', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 30, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ec0f44dd2111519e1ea680b5db9ad09346b8f7b4', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 31, 'created': '2017-02-05 13:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7d363c7efecd01398e94e9e646777125dc958013', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 32, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c556eb3c2dd183aef1576385df8e1fd2f03bd1d5', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 33, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/70bce07242f1461818989fa75d4559827683e392', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 34, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2b8b293b34cb904ffb48d1342079e0ba3a16fc47', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}, {'number': 35, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/tests/unit/controller/worker/tasks/test_cert_task.py', 'octavia/controller/worker/tasks/cert_task.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/680f01678d9c018e37a698a28d0aec55cba5c9c4', 'message': 'ACTIVE-ACTIVE - distributor certificate tasks\n\nAdded certificate related task to support\ndistributor node creation\n\nChange-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb\n'}]",0,406952,680f01678d9c018e37a698a28d0aec55cba5c9c4,114,3,35,21138,,,0,"ACTIVE-ACTIVE - distributor certificate tasks

Added certificate related task to support
distributor node creation

Change-Id: I07b7847f670bcbbba3baa88a98eb42b242f3a6cb
",git fetch https://review.opendev.org/openstack/octavia refs/changes/52/406952/33 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/tests/unit/controller/worker/tasks/test_cert_task.py', 'octavia/controller/worker/tasks/cert_task.py']",2,a6fd8f219b19fbb513295dd4db3a8fb080dc5d2c,06tmp03," class GenerateDistributorServerPEMTask(BaseCertTask): """"""Create the server certs for the agent comm Use the amphora_id for the CN """""" def execute(self, distributor_id): cert = self.cert_generator.generate_cert_key_pair( cn=distributor_id, validity=CERT_VALIDITY) return cert.certificate + cert.private_key",,27,0
openstack%2Foctavia~master~Ifd01d908edd4e245e18651db998c07b857d46190,openstack/octavia,master,Ifd01d908edd4e245e18651db998c07b857d46190,Active-Active Topology - Distributor open_flow back-end,NEW,2017-02-01 18:37:34.000000000,2017-12-18 03:58:41.000000000,,"[{'_account_id': 16923}, {'_account_id': 24261}]","[{'number': 1, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d289961b72f44e8de952bd443939f2ef0096a818', 'message': 'Active-Active Topology - Distributor open_flow back-end\n\nDistributor open_flow back-end with ovs state and heartbeat\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nChange-Id: Ifd01d908edd4e245e18651db998c07b857d46190\nCo-Authored-By:  Dean Lorenz <dean@il.ibm.com>\n'}, {'number': 2, 'created': '2017-02-01 18:43:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3ebb0c01168e93bb13d716ade17a81a578bd5b62', 'message': 'Active-Active Topology - Distributor open_flow back-end\n\nDistributor open_flow back-end with ovs state and heartbeat\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nChange-Id: Ifd01d908edd4e245e18651db998c07b857d46190\nCo-Authored-By:  Dean Lorenz <dean@il.ibm.com>\n'}, {'number': 3, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6b409c8b4be2cf9b4ce042d17c912ba998d46f07', 'message': 'Active-Active Topology - Distributor open_flow back-end\n\nDistributor open_flow back-end with ovs state and heartbeat\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nChange-Id: Ifd01d908edd4e245e18651db998c07b857d46190\nCo-Authored-By:  Dean Lorenz <dean@il.ibm.com>\n'}, {'number': 4, 'created': '2017-02-05 13:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b9ab773759affc5dda78bc3e8c026252a1b68a35', 'message': 'Active-Active Topology - Distributor open_flow back-end\n\nDistributor open_flow back-end with ovs state and heartbeat\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nChange-Id: Ifd01d908edd4e245e18651db998c07b857d46190\nCo-Authored-By:  Dean Lorenz <dean@il.ibm.com>\n'}, {'number': 5, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/10990d045d09ac25389d76cafad5b28261e0fd09', 'message': 'Active-Active Topology - Distributor open_flow back-end\n\nDistributor open_flow back-end with ovs state and heartbeat\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nChange-Id: Ifd01d908edd4e245e18651db998c07b857d46190\nCo-Authored-By:  Dean Lorenz <dean@il.ibm.com>\n'}, {'number': 6, 'created': '2017-02-06 09:50:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e1adb38947bf16a1b5d9766c28af6c6096e4770d', 'message': 'Active-Active Topology - Distributor open_flow back-end\n\nDistributor open_flow back-end with ovs state and heartbeat\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nChange-Id: Ifd01d908edd4e245e18651db998c07b857d46190\nCo-Authored-By:  Dean Lorenz <dean@il.ibm.com>\n'}, {'number': 7, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e0da32c969189b80d8f3673ff09b063ce5689793', 'message': 'Active-Active Topology - Distributor open_flow back-end\n\nDistributor open_flow back-end with ovs state and heartbeat\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nChange-Id: Ifd01d908edd4e245e18651db998c07b857d46190\nCo-Authored-By:  Dean Lorenz <dean@il.ibm.com>\n'}, {'number': 8, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/acd3cbd9a8c20bd784cce7cefd6b886c0e777662', 'message': 'Active-Active Topology - Distributor open_flow back-end\n\nDistributor open_flow back-end with ovs state and heartbeat\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nChange-Id: Ifd01d908edd4e245e18651db998c07b857d46190\nCo-Authored-By:  Dean Lorenz <dean@il.ibm.com>\n'}, {'number': 9, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/tests/unit/distributor/drivers/test_distributor_rest_api_driver.py', 'octavia/controller/healthmanager/update_db.py', 'octavia/distributor/backend/agent/api_server/distributor_info.py', 'octavia/distributor/backend/agent/api_server/server.py', 'octavia/distributor/drivers/ovs_driver/rest_api_driver.py', 'octavia/amphorae/cluster_manager/drivers/active_active/active_active_driver.py', 'octavia/distributor/backend/health_daemon/health_daemon.py', 'octavia/distributor/drivers/noop_driver/driver.py', 'octavia/tests/unit/distributor/drivers/test_distributor_noop_driver.py', 'octavia/distributor/backend/agent/api_server/open_flow.py', 'octavia/distributor/backend/agent/api_server/plug.py', 'octavia/common/constants.py', 'octavia/db/repositories.py', 'octavia/distributor/backend/health_daemon/__init__.py', 'octavia/cmd/distributor_agent.py', 'octavia/controller/worker/tasks/distributor_driver_tasks.py', 'octavia/distributor/drivers/driver_base.py', 'octavia/amphorae/drivers/health/heartbeat_udp.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/c9f8737063263ca69365679c8b76331766d63191', 'message': 'Active-Active Topology - Distributor open_flow back-end\n\nDistributor open_flow back-end with ovs state and heartbeat\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nChange-Id: Ifd01d908edd4e245e18651db998c07b857d46190\nCo-Authored-By:  Dean Lorenz <dean@il.ibm.com>\n'}]",0,427858,c9f8737063263ca69365679c8b76331766d63191,25,2,9,21138,,,0,"Active-Active Topology - Distributor open_flow back-end

Distributor open_flow back-end with ovs state and heartbeat
Implements: blueprint https://review.openstack.org/#/c/234639

Change-Id: Ifd01d908edd4e245e18651db998c07b857d46190
Co-Authored-By:  Dean Lorenz <dean@il.ibm.com>
",git fetch https://review.opendev.org/openstack/octavia refs/changes/58/427858/1 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/tests/unit/distributor/drivers/test_distributor_rest_api_driver.py', 'octavia/controller/healthmanager/update_db.py', 'octavia/distributor/backend/agent/api_server/distributor_info.py', 'octavia/distributor/backend/agent/api_server/server.py', 'octavia/distributor/drivers/ovs_driver/rest_api_driver.py', 'octavia/amphorae/cluster_manager/drivers/active_active/active_active_driver.py', 'octavia/distributor/backend/health_daemon/health_daemon.py', 'octavia/distributor/drivers/noop_driver/driver.py', 'octavia/tests/unit/distributor/drivers/test_distributor_noop_driver.py', 'octavia/distributor/backend/agent/api_server/open_flow.py', 'octavia/distributor/backend/agent/api_server/plug.py', 'octavia/common/constants.py', 'octavia/db/repositories.py', 'octavia/distributor/backend/health_daemon/__init__.py', 'octavia/cmd/distributor_agent.py', 'octavia/controller/worker/tasks/distributor_driver_tasks.py', 'octavia/distributor/drivers/driver_base.py', 'octavia/amphorae/drivers/health/heartbeat_udp.py']",18,d289961b72f44e8de952bd443939f2ef0096a818,06tmp03," def __init__(self, health_update, stats_update, distributor_update): self.distributo_update = distributor_update if self.distributo_update and 'distributor_id' in obj: self.executor.submit( self.distributo_update.update_distributor_health, obj) return"," def __init__(self, health_update, stats_update):",1740,226
openstack%2Foctavia~master~I424f8dd604c366c8abf4070c52858e741f434944,openstack/octavia,master,I424f8dd604c366c8abf4070c52858e741f434944,Active-Active Topology - LB create cluster impl,NEW,2017-01-29 12:02:34.000000000,2017-12-18 03:58:38.000000000,,"[{'_account_id': 16923}, {'_account_id': 24261}]","[{'number': 1, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/95c43bc662a375cb89933d6f3aa57a384b8635df', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 2, 'created': '2017-01-29 15:26:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1d26fc465b7c83cd14326f547ee1833d370bd754', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 3, 'created': '2017-01-29 17:04:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d60045ace7791cfe89cbbb530140e00ff733875b', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 4, 'created': '2017-01-30 09:01:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8a2bd607c00f82cd02ca32ee06aa428617f3932f', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 5, 'created': '2017-01-30 09:19:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a4f487e4a49360a2e1e5d708ebab9cb14fee25ee', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 6, 'created': '2017-01-30 12:44:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/22c56bf93be06bf1d384c69a28cbba791eb310d1', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 7, 'created': '2017-01-30 15:13:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bfb7e10a80c903a7576eed935883e43a7463e6b9', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 8, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/51f63a885038434bcd53a3b1947a7e531a47588e', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 9, 'created': '2017-02-01 18:43:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5b7cc8b5b649d659f226cfa02bc80bb0e60ced22', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 10, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/65b07ac81593e130c296e2426a4d0ba59565e83d', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 11, 'created': '2017-02-05 13:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/39c08e375a9573858ba90ff6931b0c9c5d9a44cd', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 12, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b2991a74f2a7d56e1e256b5578df0526c8bb34c8', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 13, 'created': '2017-02-06 09:50:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2db9b46071c7550e1615bfea189ca094dec8e6ce', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 14, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0ff9f2da9ca9b27251c2407d7872678efbc09cce', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 15, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f9977152df4ae97da52eb6ff1e5f839fa04f31ec', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}, {'number': 16, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/common/config.py', 'octavia/controller/worker/flows/amphora_flows.py', 'octavia/controller/worker/tasks/compute_tasks.py', 'octavia/amphorae/cluster_manager/drivers/active_active/active_active_driver.py', 'devstack/plugin.sh', 'octavia/controller/worker/tasks/network_tasks.py', 'elements/distributor-agent/install.d/75-run_setup_install', 'octavia/controller/worker/flows/amphora_cluster_flows.py', 'octavia/controller/worker/flows/distributor_flows.py', 'etc/octavia.conf', 'octavia/controller/worker/flows/load_balancer_flows.py', 'etc/init/distributor-agent.conf', 'octavia/distributor/backend/agent/distributor_jinja_cfg.py', 'octavia/common/constants.py', 'octavia/db/repositories.py', 'octavia/network/drivers/neutron/base.py', 'octavia/controller/worker/tasks/database_tasks.py', 'octavia/controller/worker/tasks/distributor_driver_tasks.py', 'octavia/tests/unit/controller/worker/tasks/test_compute_tasks.py', 'octavia/controller/worker/tasks/amphora_driver_tasks.py', 'octavia/controller/worker/controller_worker.py', 'octavia/tests/unit/controller/worker/flows/test_amphora_cluster_flows.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/a3895c562b432f5342d30f0b39a4bfc6cca5a2b8', 'message': 'Active-Active Topology - LB create cluster impl\n\nAdded LB Create supporting flows to the cluster\nmanager flow and to the Active-Active cluster manager\ndriver\n\nChange-Id: I424f8dd604c366c8abf4070c52858e741f434944\n'}]",0,426560,a3895c562b432f5342d30f0b39a4bfc6cca5a2b8,44,2,16,21138,,,0,"Active-Active Topology - LB create cluster impl

Added LB Create supporting flows to the cluster
manager flow and to the Active-Active cluster manager
driver

Change-Id: I424f8dd604c366c8abf4070c52858e741f434944
",git fetch https://review.opendev.org/openstack/octavia refs/changes/60/426560/8 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/common/config.py', 'octavia/controller/worker/flows/amphora_flows.py', 'octavia/controller/worker/tasks/compute_tasks.py', 'octavia/amphorae/cluster_manager/drivers/active_active/active_active_driver.py', 'devstack/plugin.sh', 'octavia/controller/worker/tasks/network_tasks.py', 'elements/distributor-agent/install.d/75-run_setup_install', 'octavia/controller/worker/flows/amphora_cluster_flows.py', 'octavia/controller/worker/flows/distributor_flows.py', 'etc/octavia.conf', 'octavia/distributor/backend/agent/api_server/__init__.py', 'octavia/controller/worker/flows/load_balancer_flows.py', 'etc/init/distributor-agent.conf', 'octavia/distributor/backend/agent/distributor_jinja_cfg.py', 'elements/distributor-agent/source-repository-distributor-agent', 'octavia/common/constants.py', 'octavia/db/repositories.py', 'octavia/network/drivers/neutron/base.py', 'octavia/controller/worker/tasks/database_tasks.py', 'octavia/controller/worker/tasks/distributor_driver_tasks.py', 'octavia/controller/worker/tasks/amphora_driver_tasks.py', 'octavia/controller/worker/controller_worker.py']",22,95c43bc662a375cb89933d6f3aa57a384b8635df,06tmp03," def _get_create_load_balancer_flows(self, load_balancer, topology): # if listeners exist then this was a request to create many resources # at once, so different logic will be needed. post_amp_prefix = 'post-amphora-association' if load_balancer.listeners: allocate_amphorae_flow, post_lb_amp_assoc_flow = ( self._lb_flows.get_create_load_balancer_graph_flows( topology, post_amp_prefix ) ) #TODO(Lera): case in which listeners exist??? elif topology == constants.TOPOLOGY_CLUSTER: allocate_amphorae_flow = ( self._amphora_cluster_flows. get_amphora_cluster_for_lb_subflow() ) post_lb_amp_assoc_flow = ( self._amphora_cluster_flows.get_post_cluster_for_lb_assoc_flow( )) else: allocate_amphorae_flow = ( self._lb_flows.get_create_load_balancer_flow( topology=topology ) ) post_lb_amp_assoc_flow = ( self._lb_flows.get_post_lb_amp_association_flow( prefix=post_amp_prefix, topology=topology)) return allocate_amphorae_flow, post_lb_amp_assoc_flow ", self._topology = CONF.controller_worker.loadbalancer_topology if self._topology == constants.TOPOLOGY_CLUSTER: self._amphora_cluster_flows = ( amphora_cluster_flows.AmphoraClusterFlows()),450,68
openstack%2Foctavia~master~I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c,openstack/octavia,master,I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c,ACTIVE-ACTIVE - tmp commit not for release,NEW,2017-02-05 11:57:32.000000000,2017-12-18 03:58:36.000000000,,"[{'_account_id': 16923}, {'_account_id': 24261}]","[{'number': 1, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/530d59d95fbf5e8d13ff1ab0d13cee746d512455', 'message': 'ACTIVE-ACTIVE - tmp commit not for release\n\nChange-Id: I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c\n'}, {'number': 2, 'created': '2017-02-05 12:15:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8fa844b21ac353fb3db17fd0c0810c60efa808af', 'message': 'ACTIVE-ACTIVE - tmp commit not for release\n\nChange-Id: I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c\n'}, {'number': 3, 'created': '2017-02-05 13:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d50b760065cadeab2ef63529ef34e5de6e6d5d4a', 'message': 'ACTIVE-ACTIVE - tmp commit not for release\n\nChange-Id: I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c\n'}, {'number': 4, 'created': '2017-02-06 12:47:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c322990791d129b2eec74313296262ae40964e89', 'message': 'ACTIVE-ACTIVE - tmp commit not for release\n\nChange-Id: I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c\n'}, {'number': 5, 'created': '2017-02-07 13:36:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a08089ceab25c81b32aa5a7dce7133e55023c58c', 'message': 'ACTIVE-ACTIVE - tmp commit not for release\n\nChange-Id: I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c\n'}, {'number': 6, 'created': '2017-02-13 13:49:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7efdbd9508c4d27c8db2ab04edd564f8c28cefe2', 'message': 'ACTIVE-ACTIVE - tmp commit not for release\n\nChange-Id: I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c\n'}, {'number': 7, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2a231abef43cdb4df6583d4a693c74c6c0e70fd5', 'message': 'ACTIVE-ACTIVE - tmp commit not for release\n\nChange-Id: I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c\n'}, {'number': 8, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a7747d86ced7efb5e39289d928a5e74c9e0d49a2', 'message': 'ACTIVE-ACTIVE - tmp commit not for release\n\nChange-Id: I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c\n'}, {'number': 9, 'created': '2017-03-02 13:14:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/431f702cc75de3be144aa7ef768cb47501e57930', 'message': 'ACTIVE-ACTIVE - tmp commit not for release\n\nChange-Id: I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c\n'}, {'number': 10, 'created': '2017-03-06 15:52:04.000000000', 'files': ['elements/distributor-agent/source-repository-distributor-agent', 'octavia/distributor/backend/agent/api_server/server.py', 'octavia/controller/worker/tasks/database_tasks.py', 'octavia/controller/worker/tasks/distributor_driver_tasks.py', 'elements/openvswitch-octavia/install.d/77-run_setup_install', 'devstack/plugin.sh', 'octavia/network/drivers/neutron/allowed_address_pairs.py', 'elements/distributor-agent/install.d/75-run_setup_install', 'octavia/distributor/backend/health_daemon/health_daemon.py', 'octavia/distributor/backend/agent/api_server/open_flow.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/8da5ec10cc321a7ab3cee748289689e6a0360a7c', 'message': 'ACTIVE-ACTIVE - tmp commit not for release\n\nChange-Id: I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c\n'}]",0,429369,8da5ec10cc321a7ab3cee748289689e6a0360a7c,28,2,10,21138,,,0,"ACTIVE-ACTIVE - tmp commit not for release

Change-Id: I2ca1498c7e432e6f78e41632bedc5ea5fca49b8c
",git fetch https://review.opendev.org/openstack/octavia refs/changes/69/429369/9 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/distributor/backend/agent/api_server/server.py', 'octavia/distributor/backend/health_daemon/health_daemon.py']",2,530d59d95fbf5e8d13ff1ab0d13cee746d512455,06tmp03, msg = {,"CONF.import_group('distributor_agent', 'octavia.common.config') CONF.import_group('distributor', 'octavia.common.config') msg = {'distributor-id': CONF.distributor_agent.distributor_id,",107,94
openstack%2Fkeystone-specs~master~I6ea7891511896ecb264f7bbe6b614c2dc599f49d,openstack/keystone-specs,master,I6ea7891511896ecb264f7bbe6b614c2dc599f49d,WIP -Alternative policy enforcement,NEW,2016-06-01 10:55:02.000000000,2017-12-18 03:58:34.000000000,,"[{'_account_id': 5046}, {'_account_id': 5707}, {'_account_id': 8866}, {'_account_id': 13055}, {'_account_id': 17860}, {'_account_id': 21465}]","[{'number': 1, 'created': '2016-06-01 10:55:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone-specs/commit/52a6516196c75d9eddb4b812d39c933690314e53', 'message': ""Alternative policy enforcement\n\nAdd policy enforcer to provide alternative policy enforcement logic. Current\nenforcement logic allow to check policy via http requests. New enforcer handle\nthat requests and send response with 'True' of 'False' value depending on the\ncredentials, action and target that comes with request.\n\nChange-Id: I6ea7891511896ecb264f7bbe6b614c2dc599f49d\n""}, {'number': 2, 'created': '2016-06-03 12:05:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone-specs/commit/72b0923d4acdb65fd8695b60aba5d4357d7a60fe', 'message': ""WIP - Alternative policy enforcement\n\nAdd policy enforcer to provide alternative policy enforcement logic. Current\nenforcement logic allow to check policy via http requests. New enforcer handle\nthese requests and send response with 'True' of 'False' value depending on the\ncredentials, action and target that comes with request.\n\nChange-Id: I6ea7891511896ecb264f7bbe6b614c2dc599f49d\n""}, {'number': 3, 'created': '2016-06-03 12:09:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone-specs/commit/660b6eba4e2aa58ab06e88e0bdfa8002c6ac3cfe', 'message': ""WIP - Alternative policy enforcement\n\nAdd policy enforcer to provide alternative policy enforcement logic. Current\nenforcement logic allow to check policy via http requests. New enforcer handle\nthese requests and send response with 'True' of 'False' value depending on the\ncredentials, action and target that comes with request.\n\nChange-Id: I6ea7891511896ecb264f7bbe6b614c2dc599f49d\n""}, {'number': 4, 'created': '2016-06-08 12:02:40.000000000', 'files': ['specs/keystone/backlog/policy-enforcement.rst'], 'web_link': 'https://opendev.org/openstack/keystone-specs/commit/22f3cb4b0e6a18ec1eee21ccafb8d91071da11f4', 'message': ""WIP -Alternative policy enforcement\n\nAdd policy enforcer to provide alternative policy enforcement logic. Current\nenforcement logic allow to check policy via http requests sended by\noslo.policy. New enforcer handle these requests and send response with 'True'\nof 'False' value depending on the credentials, action and target that comes\nwith request.\n\nChange-Id: I6ea7891511896ecb264f7bbe6b614c2dc599f49d\n""}]",16,323791,22f3cb4b0e6a18ec1eee21ccafb8d91071da11f4,20,6,4,21465,,,0,"WIP -Alternative policy enforcement

Add policy enforcer to provide alternative policy enforcement logic. Current
enforcement logic allow to check policy via http requests sended by
oslo.policy. New enforcer handle these requests and send response with 'True'
of 'False' value depending on the credentials, action and target that comes
with request.

Change-Id: I6ea7891511896ecb264f7bbe6b614c2dc599f49d
",git fetch https://review.opendev.org/openstack/keystone-specs refs/changes/91/323791/4 && git format-patch -1 --stdout FETCH_HEAD,['specs/keystone/backlog/policy-enforcement.rst'],1,52a6516196c75d9eddb4b812d39c933690314e53,,"============================== Alternative policy enforcement ============================== bp Add policy enforcer to provide alternative policy enforcement logic. Current enforcement logic allow to check policy via http requests. New enforcer handle that requests and send response with 'True' of 'False' value depending on the credentials, action and target that comes with request. Problem description =================== Currently policies are loaded statically from policy.json. Alternative policy enforcer allow to write URL in policy.json as rule for policy action and handle requests directed by this URL. It allow to implement dynamic policies. Proposed change =============== Description of idea ------------------- Alternative enforcement assume the existence of URL of policy enforcement controller in the rule field in plicy.json. For example: +--------------------------------------------------------------------------+ |identity:action | http://localhost:5000/policy_enforcement/identity:action| +--------------------------------------------------------------------------+ Information about credentials, target and action sends to the driver. Changes ------- 1. Class EnforcerV3 in keystone/policy/controllers. a) EnforcerV3 is controller for policy enforcement. b) policy_check method process POST requests. b) Takes policy action and context as arguments, parse credential and target information from query_string from context. c) Call enforce method from enforcement api. 2. Class PolicyEnforcementRouter in keystone/policy/routers. a) PolicyEnforcementRouter is policy enforcement router. b) URL mapped to /policy_enforcement/{policy_action}. 3. Class EnforcementManager in keystone/policy/core. a) EnforcementManager is policy enforcement manager, that provide enforcement api. b) enforce function perform enforcement method in driver, takes action, credentials and target as argument 4. Class Policy in keystone/policy/backends/static. a) Policy is driver for static policy enforcement, that preform enforcement using oslo.policy and custom policy file Implementation ============== Assignee(s) ----------- Primary assignee: - Mikhail Nikolaenko (mniolaenko) Work Items ========== 1. Implement policy enforcer for dynamic policies. ",,75,0
openstack%2Fstorlets~master~I0c6130b4501f4c027da4d98b42365849039778bb,openstack/storlets,master,I0c6130b4501f4c027da4d98b42365849039778bb,WIP: create the configration file for the agent side,NEW,2017-02-21 00:40:04.000000000,2017-12-18 03:58:26.000000000,,"[{'_account_id': 9816}, {'_account_id': 17901}]","[{'number': 1, 'created': '2017-02-21 00:40:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/7f47e632fc2a76c5376423a3209949bffddf53f1', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 2, 'created': '2017-02-21 05:37:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/7db9622d7cb2f59c2493c2e355591d6233f9f913', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 3, 'created': '2017-02-21 06:10:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/f752a4237064c58cbeec2e28407e539c1971c708', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 4, 'created': '2017-02-21 06:26:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/149f28a7f930cdf342e176fa70e01a32596d6e2b', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 5, 'created': '2017-02-21 13:48:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/e7d67d67734803eff79981ae087c4c39a08f7b0c', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 6, 'created': '2017-02-21 16:09:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/87bc53847e2259f06db7c45ff603390337c62448', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 7, 'created': '2017-02-22 14:37:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/5b576a77f0ce881d1073ddaa839cd07c48a5b0b9', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 8, 'created': '2017-02-22 16:17:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/96d2e9bdc3b44787474706dd841e06c84d98e779', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 9, 'created': '2017-02-22 22:15:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/33746123dae0f402ca485058ed411f704f00b507', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 10, 'created': '2017-02-22 22:28:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/6dde828651ec3d03da0334b7a927c031c9a7af73', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 11, 'created': '2017-02-23 14:41:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/6ddd715da36a5bb59184cf4f34ddb4b50c072d57', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 12, 'created': '2017-02-23 15:04:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/6157b85027d060d601e0ad6557d553eb604cb08b', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 13, 'created': '2017-02-23 16:37:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/99b49540e1cdc38ad77e628a09b23fe434ce5c2f', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 14, 'created': '2017-02-23 19:46:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/03863a62b917795364f5e243f6ba5a1b6a915733', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 15, 'created': '2017-02-23 21:12:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/7a10e8fd917e524992a10cc1126e4be413048196', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 16, 'created': '2017-02-28 09:31:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/c7e3c30568e5d78a16b39d3c42d4a9391dc0da82', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 17, 'created': '2017-03-01 07:26:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/950e5d65a49d03cba8f8ad00c288a9dddca5ac4e', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 18, 'created': '2017-03-01 07:40:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/20e67272ab62137ab1d3b4398764ac96a0ec1d2b', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 19, 'created': '2017-03-01 08:05:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/65cfe32db865276a0d186a64b3301f92ee574ae5', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 20, 'created': '2017-03-01 09:34:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/4788b4affa105ef6f0d2a82a95beacd0e4a84aaa', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 21, 'created': '2017-03-02 04:44:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/a757e61814bcd711b5327d1ffeb2656d27a98904', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 22, 'created': '2017-03-05 05:06:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/f57ff2e97ee50e9e391426b0a2ede0dc0544adae', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 23, 'created': '2017-03-05 06:31:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/4642668677bb96977d252f0fec8458d014e58434', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 24, 'created': '2017-03-07 05:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/d5c8b8a344441a5f1b5fcccb80cb1911cd9f2886', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}, {'number': 25, 'created': '2017-03-07 06:44:56.000000000', 'files': ['install/storlets/roles/common_templates/storlet-docker-gateway.conf-sample', 'storlets/agent/common/utils.py', 'storlets/agent/daemon/server.py', 'etc/storlet-daemon.conf', 'etc/SDaemon.conf-sample', 'install/storlets/roles/docker_storlet_engine_image/templates/ubuntu_16.04_jre8_storlets_Dockerfile', 'storlets/agent/daemon_factory/server.py', 'storlets/gateway/gateways/docker/runtime.py', 'install/storlets/templates/cluster_config', 'install/storlets/installation_vars.yml-sample', 'install/storlets/roles/docker_storlet_engine_image/files/init_container.sh', 'install/storlets/roles/common_templates/SDaemon.conf-sample', 'install/storlets/roles/common_templates/daemon-factory.conf-sample', 'tests/unit/agent/common/test_utils.py', 'install/storlets/roles/common_templates/storlet-daemon.conf-sample', 'scripts/restart_docker_container.c', 'etc/daemon-factory.conf-sample', 'install/storlets/roles/host_storlet_engine_configure/tasks/main.yml', 'etc/storlet-docker-gateway.conf-sample'], 'web_link': 'https://opendev.org/openstack/storlets/commit/5875015d372db5babde89e14aa33eea6c6a9f08c', 'message': 'WIP: create the configration file for the agent side\n\nChange-Id: I0c6130b4501f4c027da4d98b42365849039778bb\n'}]",0,436263,5875015d372db5babde89e14aa33eea6c6a9f08c,52,2,25,17901,,,0,"WIP: create the configration file for the agent side

Change-Id: I0c6130b4501f4c027da4d98b42365849039778bb
",git fetch https://review.opendev.org/openstack/storlets refs/changes/63/436263/20 && git format-patch -1 --stdout FETCH_HEAD,"['install/storlets/roles/common_templates/storlet-docker-agent.conf-sample', 'etc/storlet-docker-agent.conf-sample', 'install/storlets/templates/cluster_config', 'install/storlets/roles/host_storlet_engine_configure/tasks/main.yml']",4,7f47e632fc2a76c5376423a3209949bffddf53f1,create_agent_config_file,"- name: Create agent conf directory file: path: ""/etc/swift/storlet-docker-agent"" state: directory owner: ""{{ swift_run_time_user }}"" group: ""{{ swift_run_time_group }}"" mode: 0755 become: true - name: Copy agent conf template template: src: ../../common_templates/storlet-docker-agent.conf-sample dest: ""{{ storlet_agent_conf_file }}"" owner: ""{{ swift_run_time_user }}"" group: ""{{ swift_run_time_group }}"" mode: 0644 ",,40,0
openstack%2Fsahara-tests~master~Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e,openstack/sahara-tests,master,Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e,Added -ispublic and -isprotected flags for cluster in sahara-scenario,NEW,2016-03-29 17:21:28.000000000,2017-12-18 03:58:24.000000000,,"[{'_account_id': 7213}, {'_account_id': 10459}, {'_account_id': 13919}, {'_account_id': 21698}, {'_account_id': 22066}]","[{'number': 1, 'created': '2016-03-29 17:21:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/56124f9024365347ae4be21968715bd17f42dcb3', 'message': '[wip] ispublic isprotected\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 2, 'created': '2016-03-30 09:37:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/c2b46d8277da4633b08bed5d32ce77432bb4769b', 'message': '[wip] ispublic isprotected\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 3, 'created': '2016-03-30 10:42:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/cfa27abe8cd0bea0cb83ff09ca45731d6477805c', 'message': '[wip] ispublic isprotected\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 4, 'created': '2016-08-26 15:18:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/cc24372275d65ccfc15214e94a93aaf8789d8147', 'message': '[WIP] Added -ispublic and -isprotected flags for cluster\nin sahara-scenario\n\nPossibility to update cluster and make its public and/or\nprotected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 5, 'created': '2016-08-29 14:04:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/d86d5ee0d6151c8a04c86266445dc0833e9687ad', 'message': '[WIP] Added -ispublic and -isprotected flags for cluster\nin sahara-scenario\n\nPossibility to update cluster and make its public and/or\nprotected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 6, 'created': '2016-08-31 10:19:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/9fcdf2253876213e7a3a7ed7043dc4a0fefc6623', 'message': '[WIP] Added -ispublic and -isprotected flags for cluster\nin sahara-scenario\n\nPossibility to update cluster and make its public and/or\nprotected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 7, 'created': '2016-08-31 12:42:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/36b323053d11c7caaaa8f38bbcdc5535cf253e97', 'message': '[WIP] Added -ispublic and -isprotected flags for cluster\nin sahara-scenario\n\nPossibility to update cluster and make its public and/or\nprotected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 8, 'created': '2016-09-06 15:03:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/b50d219140af56ff97c6ae31d42e3181c5ce501c', 'message': '[WIP] Added -ispublic and -isprotected flags for cluster\nin sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 9, 'created': '2016-09-08 14:20:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/94693d322a4864eab649ecc993e91c208be9bedb', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 10, 'created': '2016-09-08 15:06:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/2a31033c968e71be2767c5d7ef7323ebc0f90b3f', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 11, 'created': '2016-09-09 06:05:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/158cdee900f683ea01977e1e30a65be67d2405d9', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 12, 'created': '2016-09-09 09:58:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/fe914fb116f52f59ea11e0122449df345b2464f3', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 13, 'created': '2016-09-22 11:36:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/5bdd2d5cd8a8d78191af5bb2ed08fac60be3b6d5', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 14, 'created': '2016-09-22 11:37:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/77b7bbc9c8f675875cbe141a50a03e9154a9f4ef', 'message': '[WIP]Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 15, 'created': '2016-09-22 14:02:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/63c51a757cce209cdfc5c0c4bd30f6fba329dc6a', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 16, 'created': '2016-09-30 12:59:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/3438dbdb82fd28abeeeba6d73132b460b3eff57d', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 17, 'created': '2016-10-05 10:30:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/0907e52f8aba4e9a9e299c4566e7eac45e6148aa', 'message': '[WIP] Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 18, 'created': '2016-10-06 14:11:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/05b247b03d8776dfe8a86fa48978bfab0d3374b7', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 19, 'created': '2016-10-07 09:47:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/287fd49776288e2030ec72e67d308a155f7188d0', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 20, 'created': '2016-10-07 13:06:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/597d7b5baba30e29fb47991e5f92af7ff9b8fd1b', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 21, 'created': '2016-10-07 13:11:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/9ee58655ea9b91db3f6b3f2c538e895cfe943465', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 22, 'created': '2016-11-01 14:45:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/64a3e9f9c04408739a015503048e18b004139453', 'message': '[WIP] Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 23, 'created': '2016-12-05 13:39:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/36e5539ffd11c50e3a45edc3b65812bf76157825', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 24, 'created': '2016-12-06 10:53:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/9b39b1220380828e57ccec2eca279740cd3031eb', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 25, 'created': '2016-12-09 12:26:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/a1cf032e6ca031b47578d2cc2dafb01677fc7eb6', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 26, 'created': '2016-12-14 07:39:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/bc6009c530b318df93c35c34cdbf93a27db468b7', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 27, 'created': '2016-12-19 10:13:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/ba6aa2784ea3f443ac2470acc10c423efae57840', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 28, 'created': '2017-01-13 09:52:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/15ed63d083052bc69810a2d1ce0afeaba0621b8d', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 29, 'created': '2017-01-23 09:29:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/eb0b858eeb621c0f45a90e02284534946920d258', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 30, 'created': '2017-03-07 11:49:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/77390b06f0f15e2221aaef3a922d7eb4eb1e4b61', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}, {'number': 31, 'created': '2017-03-07 12:16:22.000000000', 'files': ['sahara_tests/scenario/custom_checks/check_is_public_protected.py', 'sahara_tests/unit/scenario/templatevars_nodefault.ini', 'etc/scenario/gate/credentials.yaml.mako', 'doc/source/scenario.rst', 'sahara_tests/unit/scenario/test_base.py', 'sahara_tests/scenario/clients.py', 'sahara_tests/scenario/base.py', 'tools/gate/scenario/commons', 'sahara_tests/scenario/defaults/credentials.yaml.mako', 'sahara_tests/unit/scenario/test_runner.py', 'sahara_tests/scenario/validation.py', 'tools/gate/scenario/post_test_hook.sh', 'sahara_tests/unit/scenario/templatevars_complete.ini', 'releasenotes/notes/ispublic-isprotected-4a961840b2cff3ab.yaml', 'sahara_tests/unit/scenario/templatevars_incomplete.ini', 'sahara_tests/scenario/runner.py'], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/c3ab4d54b17076b7a62a2850544bc8bfce09d654', 'message': 'Added -ispublic and -isprotected flags for cluster in sahara-scenario\n\nAdded the possibility to update cluster and make it public\nand/or protected in sahara-scenario tests\n\nCo-Authored-By: Alina Nesterova <anesterova@mirantis.com>\n\nChange-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e\n'}]",34,298882,c3ab4d54b17076b7a62a2850544bc8bfce09d654,129,5,31,13919,,,0,"Added -ispublic and -isprotected flags for cluster in sahara-scenario

Added the possibility to update cluster and make it public
and/or protected in sahara-scenario tests

Co-Authored-By: Alina Nesterova <anesterova@mirantis.com>

Change-Id: Icdb053d0dbebf0c9d1bbc2129eff35e261522e2e
",git fetch https://review.opendev.org/openstack/sahara-tests refs/changes/82/298882/9 && git format-patch -1 --stdout FETCH_HEAD,"['sahara_tests/scenario/clients.py', 'sahara_tests/scenario/base.py']",2,56124f9024365347ae4be21968715bd17f42dcb3,ispublic," def _init_clients(self, login={}): username = login.get('os_username', self.credentials['os_username']) password = login.get('os_password', self.credentials['os_password']) tenant_name = login.get('os_tenant', self.credentials['os_tenant']) self.keystone = clients.KeystoneClient(session=session) self._check_ispublic_isprotected() @track_result(""Check ispublic-isprotected"") def _check_ispublic_isprotected(self): login = { ""os_tenant"": utils.rand_name('scenario-tenant'), ""os_username"": utils.rand_name('scenario-user'), ""os_password"": utils.rand_name('scenario-password') } tenant_id = self.keystone.create_tenant(login['os_tenant']) user_id = self.keystone.create_user(login['os_username'], login['os_password'], login['os_tenant']) self.sahara.update_cluster(self.cluster_id, is_public=True, is_protected=True) self._init_clients(login) try: cluster_id = self.sahara.get_cluster(self.cluster_id) except Exception as ex: raise ('Cluster {} unavailable from tenant {} for user {}'.format( self.cluster_id, login['os_tenant'], login['os_username'])) try: self.sahara.delete_cluster(self.cluster_id) except Exception as ex: pass ", def _init_clients(self): username = self.credentials['os_username'] password = self.credentials['os_password'] tenant_name = self.credentials['os_tenant'],57,4
openstack%2Fzun~master~Ia3ad31b9007c8df35dab61515aa0300f912efd06,openstack/zun,master,Ia3ad31b9007c8df35dab61515aa0300f912efd06,image_pull policy implemented for image,NEW,2017-02-09 08:18:24.000000000,2017-12-18 03:58:19.000000000,,"[{'_account_id': 8264}, {'_account_id': 10206}, {'_account_id': 11536}, {'_account_id': 12407}, {'_account_id': 16277}, {'_account_id': 20858}]","[{'number': 1, 'created': '2017-02-09 08:18:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/c2e5f7f3a29d715263efa25518820f56384fd007', 'message': ""Options default value added in image_driver pull\n\nzun create/run commands pull image if not exist So, two\nextra parameter passed image_pull_policy and image_driver\nin pull_image method but they don't have default value.\n\nWhile pull image using command:\n\n   zun pull repo:tag\nit pass only 3 argument in method  _do_image_pull for that\ni added default value for image_pull_policy and image_driver.\n\nCloses-Bug: #1663158\n\nChange-Id: Ia3ad31b9007c8df35dab61515aa0300f912efd06\n""}, {'number': 2, 'created': '2017-02-09 08:47:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/c2aaf188002f1ad726b6874afdbf2cd8bb6d6108', 'message': ""Options default value added in image_driver pull\n\nzun create/run commands pull image if not exist So, two\nextra parameter passed image_pull_policy and image_driver\nin pull_image method but they don't have default value.\n\nWhile pull image using command:\n\n   zun pull repo:tag\nit pass only 3 argument in method  _do_image_pull for that\ni added default value for image_pull_policy and image_driver.\n\nCloses-Bug: #1663158\n\nChange-Id: Ia3ad31b9007c8df35dab61515aa0300f912efd06\n""}, {'number': 3, 'created': '2017-02-13 08:34:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/cf4bc1644c3b71bcc30ecc5592017b4315966e41', 'message': 'image_pull policy implemented for image\n\nEarlier, zun pull command used to create database entry\nfor image without verifying. now it will first search\nif image exist then pull otherwise it will return ""image\nnot found""\n\nzun create/run commands pull image if not exist So, two\nextra parameter passed image_pull_policy and image_driver\nin pull_image method but they don\'t have default value.\n\nWhile pull image using command:\n\n   zun pull repo:tag\nit pass only 3 argument in method  _do_image_pull for that\ni added default value for image_pull_policy and image_driver\nto support zun pull command.\n\nCloses-Bug: #1663158\n\nChange-Id: Ia3ad31b9007c8df35dab61515aa0300f912efd06\n'}, {'number': 4, 'created': '2017-02-13 08:41:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/80a4a3b73489e628d71908de668be8518865f4da', 'message': 'image_pull policy implemented for image\n\nEarlier, zun pull command used to create database entry\nfor image without verifying. now it will first search\nif image exist then pull otherwise it will return ""image\nnot found""\n\nzun create/run commands pull image if not exist So, two\nextra parameter passed image_pull_policy and image_driver\nin pull_image method but they don\'t have default value.\n\nWhile pull image using command:\n\n   zun pull repo:tag\nit pass only 3 argument in method  _do_image_pull for that\ni added default value for image_pull_policy and image_driver\nto support zun pull command.\n\nCloses-Bug: #1663158\nCloses-Bug: #1647434\n\nChange-Id: Ia3ad31b9007c8df35dab61515aa0300f912efd06\n'}, {'number': 5, 'created': '2017-02-23 13:17:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/6f44770634a13d79d368420368cd3f269b8ef4be', 'message': 'image_pull policy implemented for image\n\nEarlier, zun pull command used to create database entry\nfor image without verifying. now it will first search\nif image exist then pull otherwise it will return ""image\nnot found""\n\nzun create/run commands pull image if not exist So, two\nextra parameter passed image_pull_policy and image_driver\nin pull_image method but they don\'t have default value.\n\nWhile pull image using command:\n\n   zun pull repo:tag\nit pass only 3 argument in method  _do_image_pull for that\ni added default value for image_pull_policy and image_driver\nto support zun pull command.\n\nCloses-Bug: #1663158\nCloses-Bug: #1647434\n\nChange-Id: Ia3ad31b9007c8df35dab61515aa0300f912efd06\n'}, {'number': 6, 'created': '2017-02-26 08:04:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/80ca566716eb8143570e75a72f409fd17605f213', 'message': 'image_pull policy implemented for image\n\nEarlier, zun pull command used to create database entry\nfor image without verifying. now it will first search\nif image exist then pull otherwise it will return ""image\nnot found""\n\nzun create/run commands pull image if not exist So, two\nextra parameter passed image_pull_policy and image_driver\nin pull_image method but they don\'t have default value.\n\nWhile pull image using command:\n\n   zun pull repo:tag\nit pass only 3 argument in method  _do_image_pull for that\ni added default value for image_pull_policy and image_driver\nto support zun pull command.\n\nCloses-Bug: #1663158\nCloses-Bug: #1647434\n\nChange-Id: Ia3ad31b9007c8df35dab61515aa0300f912efd06\n'}, {'number': 7, 'created': '2017-02-26 08:09:49.000000000', 'files': ['zun/compute/api.py', 'zun/compute/rpcapi.py', 'zun/compute/manager.py', 'zun/api/controllers/v1/images.py', 'zun/tests/unit/api/controllers/v1/test_images.py', 'zun/conf/image_driver.py'], 'web_link': 'https://opendev.org/openstack/zun/commit/5599b6ba69b75869629a1602f3ff8368baee2c11', 'message': 'image_pull policy implemented for image\n\nEarlier, zun pull command used to create database entry\nfor image without verifying. now it will first search\nif image exist then pull otherwise it will return ""image\nnot found""\n\nzun create/run commands pull image if not exist So, two\nextra parameter passed image_pull_policy and image_driver\nin pull_image method but they don\'t have default value.\n\nWhile pull image using command:\n\n   zun pull repo:tag\nit pass only 3 argument in method  _do_image_pull for that\ni added default value for image_pull_policy and image_driver\nto support zun pull command.\n\nCloses-Bug: #1663158\nCloses-Bug: #1647434\n\nChange-Id: Ia3ad31b9007c8df35dab61515aa0300f912efd06\n'}]",6,431370,5599b6ba69b75869629a1602f3ff8368baee2c11,23,6,7,20858,,,0,"image_pull policy implemented for image

Earlier, zun pull command used to create database entry
for image without verifying. now it will first search
if image exist then pull otherwise it will return ""image
not found""

zun create/run commands pull image if not exist So, two
extra parameter passed image_pull_policy and image_driver
in pull_image method but they don't have default value.

While pull image using command:

   zun pull repo:tag
it pass only 3 argument in method  _do_image_pull for that
i added default value for image_pull_policy and image_driver
to support zun pull command.

Closes-Bug: #1663158
Closes-Bug: #1647434

Change-Id: Ia3ad31b9007c8df35dab61515aa0300f912efd06
",git fetch https://review.opendev.org/openstack/zun refs/changes/70/431370/5 && git format-patch -1 --stdout FETCH_HEAD,['zun/image/driver.py'],1,c2e5f7f3a29d715263efa25518820f56384fd007,bug/1663158,"def pull_image(context, repo, tag, image_pull_policy='ifnotpresent', image_driver=None):"," def pull_image(context, repo, tag, image_pull_policy, image_driver):",2,2
openstack%2Fdiskimage-builder~master~I16d75b1dda94af8ba58ede209b53510b402632b1,openstack/diskimage-builder,master,I16d75b1dda94af8ba58ede209b53510b402632b1,Easy diskimage-builder development environment setup,NEW,2017-01-12 21:18:38.000000000,2017-12-18 03:58:09.000000000,,"[{'_account_id': 7118}, {'_account_id': 21741}]","[{'number': 1, 'created': '2017-01-12 21:18:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/9ec0e0eeaf071fb1f12a75d2d2f3fd077de8fc40', 'message': ""Easy diskimage-builder development environment setup\n\nAdd new scripts and elements to easily adapt an existing system\nor create an image with diskimage-builder that can be used\nfor development.\n\nThere are three flavors:\n'run': dib can run and create images\n'test': possible to run test cases\n'full': adds some other development tools like git-gerrit\n\nThis was tested using the follwing docker images:\nubuntu-xenial, ubuntu-yakkety, gentoo-latest, fedora-25\n\nThis patch creates preconditions which are needed to\ncreate test-images that are able to run diskimage-builder.\n\nChange-Id: I16d75b1dda94af8ba58ede209b53510b402632b1\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}, {'number': 2, 'created': '2017-01-13 00:22:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/65797cf33ca795aaf63921add44785efbb57c779', 'message': ""Easy diskimage-builder development environment setup\n\nAdd new scripts and elements to easily adapt an existing system\nor create an image with diskimage-builder that can be used\nfor development.\n\nThere are three flavors:\n'run': dib can run and create images\n'test': possible to run test cases\n'full': adds some other development tools like git-gerrit\n\nThis was tested using the follwing docker images:\nubuntu-xenial, ubuntu-yakkety, gentoo-latest, fedora-25\n\nThis patch creates preconditions which are needed to\ncreate test-images that are able to run diskimage-builder.\n\nChange-Id: I16d75b1dda94af8ba58ede209b53510b402632b1\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}, {'number': 3, 'created': '2017-01-13 18:01:14.000000000', 'files': ['releasenotes/notes/simple-dib-devel-setup-d212ed6ec7cd51ae.yaml', 'tests/elements/dibdevel-run/pkg-map', 'elements/pkg-map/bin/pkg-map', 'tests/elements/dibdevel-full/package-installs.yaml', 'tests/elements/dibdevel-run/post-install.d/80-fix-user', 'tests/elements/dibdevel-run/environment.d/05-set-environment', 'tests/elements/dibdevel-full/element-deps', 'tests/elements/dibdevel-run/element-deps', 'tests/elements/dibdevel-test/Readme.rst', 'bin/dib-bootstrap-devsetup', 'tests/elements/dibdevel-test/pkg-map', 'tests/elements/dibdevel-test/package-installs.yaml', 'tests/elements/dibdevel-full/Readme.rst', 'tests/elements/dibdevel-run/pre-install.d/10-portage-adaptions', 'tests/elements/dibdevel-test/element-deps', 'doc/source/developer/index.rst', 'tests/elements/dibdevel-run/Readme.rst', 'tests/elements/dibdevel-run/package-installs.yaml', 'elements/package-installs/bin/package-installs-v2'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/27372c843755eb10cdbabb235bf8dcb35b1268e7', 'message': ""Easy diskimage-builder development environment setup\n\nAdd new scripts and elements to easily adapt an existing system\nor create an image with diskimage-builder that can be used\nfor development.\n\nThere are three flavors:\n'run': dib can run and create images\n'test': possible to run test cases\n'full': adds some other development tools like git-gerrit\n\nThis was tested using the follwing docker images:\nubuntu-xenial, ubuntu-yakkety, gentoo-latest, fedora-25\n\nThis patch creates preconditions which are needed to\ncreate test-images that are able to run diskimage-builder.\n\nChange-Id: I16d75b1dda94af8ba58ede209b53510b402632b1\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}]",0,419655,27372c843755eb10cdbabb235bf8dcb35b1268e7,16,2,3,21741,,,0,"Easy diskimage-builder development environment setup

Add new scripts and elements to easily adapt an existing system
or create an image with diskimage-builder that can be used
for development.

There are three flavors:
'run': dib can run and create images
'test': possible to run test cases
'full': adds some other development tools like git-gerrit

This was tested using the follwing docker images:
ubuntu-xenial, ubuntu-yakkety, gentoo-latest, fedora-25

This patch creates preconditions which are needed to
create test-images that are able to run diskimage-builder.

Change-Id: I16d75b1dda94af8ba58ede209b53510b402632b1
Signed-off-by: Andreas Florath <andreas@florath.net>
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/55/419655/3 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/simple-dib-devel-setup-d212ed6ec7cd51ae.yaml', 'tests/elements/dibdevel-run/pkg-map', 'elements/pkg-map/bin/pkg-map', 'tests/elements/dibdevel-full/package-installs.yaml', 'tests/elements/dibdevel-run/post-install.d/80-fix-user', 'tests/elements/dibdevel-run/environment.d/05-set-environment', 'tests/elements/dibdevel-full/element-deps', 'tests/elements/dibdevel-run/element-deps', 'tests/elements/dibdevel-test/Readme.rst', 'bin/dib-bootstrap-devsetup', 'tests/elements/dibdevel-test/pkg-map', 'tests/elements/dibdevel-test/package-installs.yaml', 'tests/elements/dibdevel-full/Readme.rst', 'tests/elements/dibdevel-run/pre-install.d/10-portage-adaptions', 'tests/elements/dibdevel-test/element-deps', 'doc/source/developer/index.rst', 'tests/elements/dibdevel-run/Readme.rst', 'tests/elements/dibdevel-run/package-installs.yaml', 'elements/package-installs/bin/package-installs-v2']",19,9ec0e0eeaf071fb1f12a75d2d2f3fd077de8fc40,simple-dib-devel-setup,"import os parser.add_argument('--pkg-map-executable', default=os.getenv('DIB_PKG_MAP_EXEC', ""pkg-map""), help=""(Complete path) to pkg-map. Default: pkg-map"") parser.add_argument('--install-packages-executable', default=os.getenv('DIB_INSTALL_PACKAGES_EXEC', ""install-packages""), help=""(Complete path) to install-packages. "" ""Default: install-packages"") pkg_map_args = [args.pkg_map_executable, '--missing-ok', '--element', element, pkg] install_args = [args.install_packages_executable]"," pkg_map_args = ['pkg-map', '--missing-ok', '--element', element, pkg] install_args = [""install-packages""]",425,2
openstack%2Fdiskimage-builder~master~I79eb34685944eb5651aad823d51263ad14bfa495,openstack/diskimage-builder,master,I79eb34685944eb5651aad823d51263ad14bfa495,DOC:Delete a obsolete note in the doc of python-brickclient,NEW,2017-02-16 03:12:07.000000000,2017-12-18 03:58:07.000000000,,[{'_account_id': 21741}],"[{'number': 1, 'created': '2017-02-16 03:12:07.000000000', 'files': ['elements/python-brickclient/README.rst'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/d87635587617004825aafe1fbe6c6fd78f5bb137', 'message': 'DOC:Delete a obsolete note in the doc of python-brickclient\n\nIn the doc of the python-brickclient, noticing that there is a known\nbug.\nAs for now, it has been resolved and released, so that I think the note\ncan be deleted now.\n\nChange-Id: I79eb34685944eb5651aad823d51263ad14bfa495\n'}]",0,434600,d87635587617004825aafe1fbe6c6fd78f5bb137,6,1,1,19901,,,0,"DOC:Delete a obsolete note in the doc of python-brickclient

In the doc of the python-brickclient, noticing that there is a known
bug.
As for now, it has been resolved and released, so that I think the note
can be deleted now.

Change-Id: I79eb34685944eb5651aad823d51263ad14bfa495
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/00/434600/1 && git format-patch -1 --stdout FETCH_HEAD,['elements/python-brickclient/README.rst'],1,d87635587617004825aafe1fbe6c6fd78f5bb137,doc/delete-obsolete-note,,"* Currently the feature has a dependency on a known bug `<https://launchpad.net/bugs/1623549>`__, which has been resolved and will be part of the upstream with the next release of ``python-brick-cinderclient-ext``. Note: Current version of ``python-brick-cinderclient-ext`` i.e. 0.2.0 requires and update to be made in Line32 for ``/usr/share/python-brickclient/venv/lib/python2.7/site-packages/brick_cinderclient_ext/__init__.py``: update ``brick-python-cinderclient-ext`` to ``python-brick-cinderclient-ext``. ",0,10
openstack%2Fdiskimage-builder~master~If8e1f7dee04c07f8d4f20ebb5527ff13dfaa63f7,openstack/diskimage-builder,master,If8e1f7dee04c07f8d4f20ebb5527ff13dfaa63f7,[WIP] switch enable-serial-console element to ttyS2,NEW,2016-03-08 14:52:41.000000000,2017-12-18 03:57:59.000000000,,"[{'_account_id': 6928}, {'_account_id': 11105}]","[{'number': 1, 'created': '2016-03-08 14:52:41.000000000', 'files': ['elements/enable-serial-console/install.d/ttySx.conf'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/56162411578c85c323962130a308f2af5b2cbac4', 'message': '[WIP] switch enable-serial-console element to ttyS2\n\nOn some hardware, serial console needs to be directed to ttyS2.\n\nI\'m putting this patch up to track that work, and to solicit ideas on\nhow to make this more dynamic, rather than hard-coded in the element.\n\nPerhaps this element could take a cue from the kernel boot paramater\n""console="" value?\n\nChange-Id: If8e1f7dee04c07f8d4f20ebb5527ff13dfaa63f7\n'}]",0,289953,56162411578c85c323962130a308f2af5b2cbac4,8,2,1,2889,,,0,"[WIP] switch enable-serial-console element to ttyS2

On some hardware, serial console needs to be directed to ttyS2.

I'm putting this patch up to track that work, and to solicit ideas on
how to make this more dynamic, rather than hard-coded in the element.

Perhaps this element could take a cue from the kernel boot paramater
""console="" value?

Change-Id: If8e1f7dee04c07f8d4f20ebb5527ff13dfaa63f7
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/53/289953/1 && git format-patch -1 --stdout FETCH_HEAD,['elements/enable-serial-console/install.d/ttySx.conf'],1,56162411578c85c323962130a308f2af5b2cbac4,ttyS2," echo ""ttySx probing ttyS2"" >/dev/ttyS2 2>/dev/null && console_port=2"," echo ""ttySx probing ttyS1"" >/dev/ttyS1 2>/dev/null && console_port=1",1,1
openstack%2Fdiskimage-builder~master~I01b49a881be019b51f5e25659cb22354fa59bd5a,openstack/diskimage-builder,master,I01b49a881be019b51f5e25659cb22354fa59bd5a,Copy sysctl.d snippets,NEW,2016-12-06 22:40:09.000000000,2017-12-18 03:57:57.000000000,,"[{'_account_id': 7118}, {'_account_id': 11628}, {'_account_id': 12459}, {'_account_id': 21741}]","[{'number': 1, 'created': '2016-12-06 22:40:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/08487e84b527989f7f25c20c33d2a4ba061352d1', 'message': 'Copy sysctl.d snippets\n\nConvert the sysctl element to copy sysctl.d snippets from elements\ninto the final image.\n\nChange-Id: I01b49a881be019b51f5e25659cb22354fa59bd5a\n'}, {'number': 2, 'created': '2016-12-06 23:44:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/8417245746c4813d384d429d7df3dda8b6939af2', 'message': 'Copy sysctl.d snippets\n\nConvert the sysctl element to copy sysctl.d snippets from elements\ninto the final image.\n\nCo-Authored-By: Ian Wienand <iwienand@redhat.com>\nCo-Authored-By: Michael Johnson <johnsomor@gmail.com>\nChange-Id: I01b49a881be019b51f5e25659cb22354fa59bd5a\n'}, {'number': 3, 'created': '2016-12-07 00:20:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/676d203bc2184618ba43bd62b88900d1372e1c1b', 'message': 'Copy sysctl.d snippets\n\nConvert the sysctl element to copy sysctl.d snippets from elements\ninto the final image.\n\nCo-Authored-By: Ian Wienand <iwienand@redhat.com>\nCo-Authored-By: Michael Johnson <johnsomor@gmail.com>\nChange-Id: I01b49a881be019b51f5e25659cb22354fa59bd5a\n'}, {'number': 4, 'created': '2016-12-07 15:02:04.000000000', 'files': ['elements/sysctl/README.rst', 'elements/sysctl/extra-data.d/10-sysctl-copy'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/db26d08c8420256506459a26526c3fecb09c012c', 'message': 'Copy sysctl.d snippets\n\nConvert the sysctl element to copy sysctl.d snippets from elements\ninto the final image.\n\nCo-Authored-By: Ian Wienand <iwienand@redhat.com>\nCo-Authored-By: Michael Johnson <johnsomor@gmail.com>\nChange-Id: I01b49a881be019b51f5e25659cb22354fa59bd5a\n'}]",3,407739,db26d08c8420256506459a26526c3fecb09c012c,18,4,4,7118,,,0,"Copy sysctl.d snippets

Convert the sysctl element to copy sysctl.d snippets from elements
into the final image.

Co-Authored-By: Ian Wienand <iwienand@redhat.com>
Co-Authored-By: Michael Johnson <johnsomor@gmail.com>
Change-Id: I01b49a881be019b51f5e25659cb22354fa59bd5a
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/39/407739/4 && git format-patch -1 --stdout FETCH_HEAD,"['elements/sysctl/README.rst', 'elements/sysctl/extra-data.d/10-sysctl-copy']",2,08487e84b527989f7f25c20c33d2a4ba061352d1,sysctl-snippets,"#!/bin/bash if [ ${DIB_DEBUG_TRACE:-1} -gt 0 ]; then set -x fi set -eu set -o pipefail # copy all sysctl.d snippets to sysctl for ELEMENT in $IMAGE_ELEMENT ; do for DIR in ${ELEMENTS_PATH//:/ }; do if [ -d ""$DIR/$ELEMENT/sysctl.d/"" ]; then # TODO : we should probably check for conflicts, so you # know if two elements overwrite each other. sudo cp ""$DIR/$ELEMENT/sysctl.d/*"" ""$TMP_MOUNT_PATH/etc/sysctl.d"" fi done done ",,23,8
openstack%2Fdiskimage-builder~master~I207b4ce8bcfa694c1d67e738ac8c2dce96c38d61,openstack/diskimage-builder,master,I207b4ce8bcfa694c1d67e738ac8c2dce96c38d61,Make svc-map support overridden elements,NEW,2016-06-23 21:35:06.000000000,2017-12-18 03:57:55.000000000,,"[{'_account_id': 7118}, {'_account_id': 10035}, {'_account_id': 10342}, {'_account_id': 11105}, {'_account_id': 11655}, {'_account_id': 12459}, {'_account_id': 21741}]","[{'number': 1, 'created': '2016-06-23 21:35:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/809af2a65e0f2c65a47518217bf5bbd609d03529', 'message': 'Make svc-map support overridden elements\n\nWe want to be able to deprecate in-tree elements,\nbut keep them around and use a different version\nin one of the locations specified by ELEMENTS_PATH.\nsvc-map currently does not support this - it errors\ndue to duplicate service names.\n\nRefine the check such that an element is allowed to\nduplicate a service definition if the element providing\nthe definition is not in BASE_ELEMENT_DIR (i.e. it is not\nprovided with dib itself). Where multiple paths are\nprovided, check svc-maps in BASE_ELEMENT_DIR first, then\nin ELEMENTS_PATH.\n\nChange-Id: I207b4ce8bcfa694c1d67e738ac8c2dce96c38d61\n'}, {'number': 2, 'created': '2016-06-23 23:16:36.000000000', 'files': ['releasenotes/notes/svc-map-extra-paths-ab2c585dc69e3405.yaml', 'lib/common-defaults', 'elements/svc-map/tests/test_data_merge.py', 'elements/svc-map/extra-data.d/10-merge-svc-map-files'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/de8d2e4e546c757107652898c58fd8a3a91c6ed1', 'message': 'Make svc-map support overridden elements\n\nWe want to be able to deprecate in-tree elements,\nbut keep them around and use a different version\nin one of the locations specified by ELEMENTS_PATH.\nsvc-map currently does not support this - it errors\ndue to duplicate service names.\n\nRefine the check such that an element is allowed to\nduplicate a service definition if the element providing\nthe definition is not in BASE_ELEMENT_DIR (i.e. it is not\nprovided with dib itself). Where multiple paths are\nprovided, check svc-maps in BASE_ELEMENT_DIR first, then\nin ELEMENTS_PATH.\n\nChange-Id: I207b4ce8bcfa694c1d67e738ac8c2dce96c38d61\n'}]",10,333597,de8d2e4e546c757107652898c58fd8a3a91c6ed1,24,7,2,12459,,,0,"Make svc-map support overridden elements

We want to be able to deprecate in-tree elements,
but keep them around and use a different version
in one of the locations specified by ELEMENTS_PATH.
svc-map currently does not support this - it errors
due to duplicate service names.

Refine the check such that an element is allowed to
duplicate a service definition if the element providing
the definition is not in BASE_ELEMENT_DIR (i.e. it is not
provided with dib itself). Where multiple paths are
provided, check svc-maps in BASE_ELEMENT_DIR first, then
in ELEMENTS_PATH.

Change-Id: I207b4ce8bcfa694c1d67e738ac8c2dce96c38d61
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/97/333597/2 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/svc-map-extra-paths-ab2c585dc69e3405.yaml', 'lib/common-defaults', 'elements/svc-map/extra-data.d/10-merge-svc-map-files']",3,809af2a65e0f2c65a47518217bf5bbd609d03529,dont-complain-about-dupe-element-service,"def merge_data(source, destination, distro, element, allow_dupes): Merges two dictionaries and filters on distro family, or default (in order). If the servicename is already found, raise an exception unless it's redefined by an element of the same name not in BASE_ELEMENT_DIR. if allow_dupes: print(""WARNING: service %s is redefined in a duplicate "" ""element %s, assuming you intend to override the "" ""existing element."" % (servicename, element)) else: raise Exception(""%s already found in services list"" % servicename)def get_element_paths(): """"""Get paths to elements from the environment Return unique, canonicalized paths for the base element path and any additional paths in ELEMENTS_PATH. """""" # get canonical location of all element paths element_paths = [os.path.realpath(path) for path in os.environ.get(""ELEMENTS_PATH"").split(':')] base_element_dir = os.path.realpath(os.environ.get(""BASE_ELEMENT_DIR"")) # remove duplicate element paths when there are multiple element paths if len(element_paths) > 1: deduped_paths = [] for p in element_paths: if p not in deduped_paths: deduped_paths.append(p) element_paths = deduped_paths # move base_element_dir to first position so overridden elements # are evaluated later element_paths.pop(element_paths.index(base_element_dir)) element_paths.insert(0, base_element_dir) return (base_element_dir, element_paths) base_element_dir, element_paths = get_element_paths() seen_elements = set() allow_dupes = (element_path != base_element_dir and element in seen_elements) os.environ.get(""DISTRO_NAME""), element, allow_dupes) except Exception as err: print(""%s. Check %s for duplicate"" seen_elements.add(element)","def merge_data(source, destination, distro): Merges two dictionaries and filters on distro family, or default (in order) raise Exception(""%s already found in services list"" % servicename) element_paths = os.environ.get(""ELEMENTS_PATH"").split(':') os.environ.get(""DISTRO_NAME"")) except Exception as err: print(""%s. Check %s for duplicate""",57,9
openstack%2Fdiskimage-builder~master~Ib5fe4bdd3dc24df0fa4a4491c97c7a3a390ba383,openstack/diskimage-builder,master,Ib5fe4bdd3dc24df0fa4a4491c97c7a3a390ba383,Include systemd-sysv for debian stretch,NEW,2017-03-08 16:01:04.000000000,2017-12-18 03:57:28.000000000,,"[{'_account_id': 4162}, {'_account_id': 6476}, {'_account_id': 21741}]","[{'number': 1, 'created': '2017-03-08 16:01:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/5aa7485a87f2b091ab04e29b2ca1facfe5992991', 'message': 'Include systemd-sysv for debian stretch\n\nDebian does not include an init system in debootstrap by default\nstarting with stretch. For now, hardcode a fix. We need better long term\nsupport though.\n\nChange-Id: Ib5fe4bdd3dc24df0fa4a4491c97c7a3a390ba383\n'}, {'number': 2, 'created': '2017-03-08 16:09:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/48997c8a3c990daac6e30fddfaffa8ebc8589f7c', 'message': 'Include systemd-sysv for debian stretch\n\nDebian does not include an init system in debootstrap by default\nstarting with stretch. For now, hardcode a fix. We need better long term\nsupport though.\n\nChange-Id: Ib5fe4bdd3dc24df0fa4a4491c97c7a3a390ba383\n'}, {'number': 3, 'created': '2017-03-08 16:14:26.000000000', 'files': ['diskimage_builder/elements/debian-minimal/environment.d/10-debian-minimal.bash'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/6682ad1dd282ba2ad36f261baaefef30df7d394a', 'message': 'Include systemd-sysv for debian stretch\n\nDebian does not include an init system in debootstrap by default\nstarting with stretch. Include one.\n\nChange-Id: Ib5fe4bdd3dc24df0fa4a4491c97c7a3a390ba383\n'}]",2,443201,6682ad1dd282ba2ad36f261baaefef30df7d394a,13,3,3,2,,,0,"Include systemd-sysv for debian stretch

Debian does not include an init system in debootstrap by default
starting with stretch. Include one.

Change-Id: Ib5fe4bdd3dc24df0fa4a4491c97c7a3a390ba383
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/01/443201/1 && git format-patch -1 --stdout FETCH_HEAD,['diskimage_builder/elements/debootstrap/root.d/08-debootstrap'],1,5aa7485a87f2b091ab04e29b2ca1facfe5992991,,"# Do this more generically for things post stretch # zesty will also have this problem, but not sure what the right package to # include is. if [ ""$DIB_RELEASE"" = ""stretch""] ; then DIB_DEBOOTSTRAP_EXTRA_ARGS=${DIB_DEBOOTSTRAP_EXTRA_ARGS:---include=systemd-sysv} fi ",,7,0
openstack%2Fmonasca-api~master~I1bee0d585b2343e94748ff4fc2b4704ed4d67f7f,openstack/monasca-api,master,I1bee0d585b2343e94748ff4fc2b4704ed4d67f7f,Add v3 api metrics endpoints,NEW,2017-02-13 22:36:10.000000000,2017-12-18 03:57:13.000000000,,"[{'_account_id': 14517}, {'_account_id': 16168}, {'_account_id': 20033}]","[{'number': 1, 'created': '2017-02-13 22:36:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/4028844808a815df13a1e06459335d0680b9741c', 'message': 'Add v3 api endpoints\n\nChange-Id: I1bee0d585b2343e94748ff4fc2b4704ed4d67f7f\n'}, {'number': 2, 'created': '2017-02-13 23:52:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/7c888885e1b1a99ac05be8be90cbda5bdf9d921f', 'message': 'Add v3 api metrics endpoints\n\nChange-Id: I1bee0d585b2343e94748ff4fc2b4704ed4d67f7f\n'}, {'number': 3, 'created': '2017-02-15 15:47:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/92798c5c54f6572257c43fd7310c9661f9c89da4', 'message': 'Add v3 api metrics endpoints\n\nChange-Id: I1bee0d585b2343e94748ff4fc2b4704ed4d67f7f\n'}, {'number': 4, 'created': '2017-02-15 17:39:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/75e8316c56d8cc4abca88a70ea1e968c069a5677', 'message': 'Add v3 api metrics endpoints\n\nChange-Id: I1bee0d585b2343e94748ff4fc2b4704ed4d67f7f\n'}, {'number': 5, 'created': '2017-02-15 18:38:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/81c08e9350b0025bbf996b99e4604c659f98f6f3', 'message': 'Add v3 api metrics endpoints\n\nChange-Id: I1bee0d585b2343e94748ff4fc2b4704ed4d67f7f\n'}, {'number': 6, 'created': '2017-02-23 22:08:28.000000000', 'files': ['monasca_api/v3/metrics.py', 'monasca_api/v2/reference/notifications.py', 'monasca_api/v2/reference/resource.py', 'monasca_api/v2/reference/alarm_definitions.py', 'monasca_api/v2/common/validation.py', 'monasca_api/v2/reference/alarms.py', 'monasca_api/v3/common/pagination.py', 'monasca_api/tests/test_validation.py', 'monasca_api/v3/common/__init__.py', 'monasca_api/v2/reference/helpers.py', 'monasca_api/v2/reference/metrics.py', 'monasca_api/v3/common/auth.py', 'monasca_api/common/exceptions.py', 'monasca_api/v3/version_3.py', 'monasca_api/v3/common/validation.py', 'monasca_api/api/__init__.py', 'monasca_api/tests/test_query_helpers.py', 'monasca_api/v3/common/utils.py', 'monasca_api/v2/reference/__init__.py', 'monasca_api/v2/reference/versions.py', 'monasca_api/api/server.py', 'monasca_api/v3/__init__.py', 'monasca_api/api/core/request.py', 'monasca_api/tests/test_request.py'], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/85c42d000ec3ac31c13c23aa27a9fe98fa49aeda', 'message': 'Add v3 api metrics endpoints\n\nChange-Id: I1bee0d585b2343e94748ff4fc2b4704ed4d67f7f\n'}]",4,433314,85c42d000ec3ac31c13c23aa27a9fe98fa49aeda,27,3,6,14517,,,0,"Add v3 api metrics endpoints

Change-Id: I1bee0d585b2343e94748ff4fc2b4704ed4d67f7f
",git fetch https://review.opendev.org/openstack/monasca-api refs/changes/14/433314/4 && git format-patch -1 --stdout FETCH_HEAD,"['monasca_api/common/repositories/mysql/alarm_definitions_repository.py', 'monasca_api/common/repositories/mysql/notifications_repository.py', 'monasca_api/v3/metrics.py', 'monasca_api/v2/reference/notifications.py', 'monasca_api/v2/reference/resource.py', 'monasca_api/v2/reference/alarm_definitions.py', 'monasca_api/v3/common/alarming.py', 'monasca_api/v2/common/validation.py', 'monasca_api/v3/alarm_definitions.py', 'monasca_api/v2/reference/alarms.py', 'monasca_api/v3/common/pagination.py', 'monasca_api/tests/test_validation.py', 'monasca_api/v3/alarms.py', 'monasca_api/v3/common/__init__.py', 'monasca_api/v2/reference/helpers.py', 'monasca_api/v2/reference/metrics.py', 'monasca_api/common/repositories/mysql/notification_method_type_repository.py', 'monasca_api/v3/common/auth.py', 'monasca_api/common/exceptions.py', 'monasca_api/v3/version_3.py', 'monasca_api/common/repositories/mysql/alarms_repository.py', 'monasca_api/v3/common/validation.py', 'monasca_api/tests/test_query_helpers.py', 'monasca_api/v3/common/utils.py', 'monasca_api/v3/notification_types.py', 'monasca_api/v3/notifications.py', 'monasca_api/v2/reference/versions.py', 'monasca_api/api/server.py', 'monasca_api/v3/__init__.py', 'monasca_api/api/core/request.py', 'monasca_api/tests/test_request.py']",31,4028844808a815df13a1e06459335d0680b9741c,v3,from falcon import testingfrom monasca_api.common import exceptions,from falcon import testing from monasca_api.v2.common import exceptions,2811,1459
openstack%2Fsahara-tests~master~I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc,openstack/sahara-tests,master,I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc,Add yaml with all processes of CDH,NEW,2016-02-27 11:07:41.000000000,2017-12-18 03:57:00.000000000,,"[{'_account_id': 7213}, {'_account_id': 9740}, {'_account_id': 10459}, {'_account_id': 13919}]","[{'number': 1, 'created': '2016-02-27 11:07:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/61c7638f4f07117f8778b60c73cc6876c70e1819', 'message': ""Add yaml's with all processes CDH, Ambari\n\nAdd files with CDH and Ambari for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 2, 'created': '2016-03-09 09:15:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/5c02075137914b84538387c0d944ce89fac26dbd', 'message': ""[TEST]Add yaml's with all processes CDH, Ambari\n\nAdd files with CDH and Ambari for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 3, 'created': '2016-03-09 09:16:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/77aeba6d19194302c35af19a8c208625396e143f', 'message': ""[TEST]Add yaml's with all processes CDH, Ambari\n\nAdd files with CDH and Ambari for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 4, 'created': '2016-03-09 11:04:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/83e37c86c5d42f793e242a641e68441c00bbb488', 'message': ""[TEST]Add yaml's with all processes CDH, Ambari\n\nAdd files with CDH and Ambari for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 5, 'created': '2016-03-09 11:27:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/9b2261a7bb5f068fab12813fc4650c9c73e51652', 'message': ""[TEST]Add yaml's with all processes CDH, Ambari\n\nAdd files with CDH and Ambari for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 6, 'created': '2016-03-10 09:33:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/776b12ca83cc007cdd44d90b03f724863f53b802', 'message': ""[TEST]Add yaml's with all processes CDH, Ambari\n\nAdd files with CDH and Ambari for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 7, 'created': '2016-03-14 08:18:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/986dde60498750957d08d59910c2743bb88fae1e', 'message': ""[TEST]Add yaml's with all processes CDH\n\nAdd files with CDH for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 8, 'created': '2016-03-30 13:11:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/10e89f860489e9d756262ab3c14068023dafe817', 'message': ""[TEST]Add yaml's with all processes CDH\n\nAdd files with CDH for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 9, 'created': '2016-03-31 07:50:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/f291fea2cb4d265b51b755c1bc5721f37992bdc8', 'message': ""[TEST]Add yaml's with all processes CDH\n\nAdd files with CDH for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 10, 'created': '2016-03-31 14:14:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/58b8fcf530617be38e8ba607bb0cbb12af73d347', 'message': ""[WIP]Add yaml's with all processes CDH\n\nAdd files with CDH for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 11, 'created': '2016-04-28 14:12:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/d2a7e92df1aeb03b3c03868ec8d8892fcd258d07', 'message': ""[WIP]Add yaml's with all processes CDH\n\nAdd files with CDH for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 12, 'created': '2016-09-02 08:25:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/81226f28dff1e2504d57925b78d6ddce818e7da9', 'message': ""[WIP]Add yaml's with all processes CDH\n\nAdd files with CDH for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 13, 'created': '2016-09-02 08:26:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/abcddad3339e65c7cd746fe6c65018898f0d9671', 'message': ""[WIP]Add yaml's with all processes CDH\n\nAdd files with CDH for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 14, 'created': '2016-11-01 12:57:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/550e91fbb4c47cea2cec544029c42ef5ea4c6667', 'message': ""[WIP]Add yaml's with all processes CDH\n\nAdd files with CDH for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 15, 'created': '2016-11-09 12:43:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/d974e6b11e1fed97da9bc0b6c89774114fe398a5', 'message': ""[WIP]Add yaml's with all processes CDH\n\nAdd files with CDH for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n""}, {'number': 16, 'created': '2016-11-10 08:06:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/176fe7f69f6fdae6ac7755e29501e1bac568eac3', 'message': 'Add yaml with all processes of CDH\n\nAdd files with CDH for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n'}, {'number': 17, 'created': '2016-11-10 13:42:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/2101f2d1bd1812b37b54faba3940ecb2733b347d', 'message': 'Add yaml with all processes of CDH\n\nAdd files with CDH for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n'}, {'number': 18, 'created': '2016-11-11 07:10:42.000000000', 'files': ['sahara_tests/scenario/custom_checks/check_sentry.py', 'sahara_tests/scenario/defaults/cdh-5.7.0.yaml.mako'], 'web_link': 'https://opendev.org/openstack/sahara-tests/commit/780bb1c11c5bf3faaeef4b8e746575ff031e99e7', 'message': 'Add yaml with all processes of CDH\n\nAdd files with CDH for nightly jobs\n\nChange-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc\n'}]",1,285663,780bb1c11c5bf3faaeef4b8e746575ff031e99e7,74,4,18,13919,,,0,"Add yaml with all processes of CDH

Add files with CDH for nightly jobs

Change-Id: I9b7bd97925f32c062e64a9f0f0f4fa9c0919cabc
",git fetch https://review.opendev.org/openstack/sahara-tests refs/changes/63/285663/17 && git format-patch -1 --stdout FETCH_HEAD,"['etc/scenario/sahara-ci/cdh-5.4.0-full.yaml.mako', 'etc/scenario/sahara-ci/ambari-2.3-full.yaml.mako']",2,61c7638f4f07117f8778b60c73cc6876c70e1819,nightly-jobs,clusters: - plugin_name: ambari plugin_version: '2.3' image: ${ambari_2_1_image} node_group_templates: - name: master flavor: ${medium_flavor_id} node_processes: - Ambari - MapReduce History Server - Spark History Server - NameNode - ResourceManager - SecondaryNameNode - YARN Timeline Server - ZooKeeper - Kafka Broker - JournalNode auto_security_group: true - name: master-edp flavor: ${ci_flavor_id} node_processes: - Hive Metastore - HiveServer - Oozie auto_security_group: true - name: worker flavor: ${ci_flavor_id} node_processes: - DataNode - NodeManager volumes_per_node: 2 volumes_size: 2 auto_security_group: true - name: master-additional flavor: ${medium_flavor_id} node_processes: - Supervisor - Storm UI Server - Sqoop - Slider - Ranger Usersync - Ranger Admin - Nimbus - Knox Gateway - HBase RegionServer - HBase Master - Flume - Falcon Server - DRPC Server cluster_template: name: ambari21 node_group_templates: master: 1 master-edp: 1 worker: 3 cluster_configs: HDFS: dfs.datanode.du.reserved: 0 custom_checks: check_kafka: zookeeper_process: ZooKeeper kafka_process: Kafka Broker spark_flow: - type: Spark main_lib: type: database source: etc/edp-examples/edp-spark/spark-kafka-example.jar args: - '{zookeeper_list}' - '{topic}' - '{timeout}' timeout: 30 cluster: name: ${cluster_name} scenario: - run_jobs - kafka edp_jobs_flow: - java_job - spark_pi ,,182,0
openstack%2Fpython-barbicanclient~stable%2Fpike~Ibcbd0e6337025119d41241787ab965c5b3d0046d,openstack/python-barbicanclient,stable/pike,Ibcbd0e6337025119d41241787ab965c5b3d0046d,Updated from global requirements,MERGED,2017-09-21 01:05:47.000000000,2017-12-18 03:56:59.000000000,2017-12-18 03:56:59.000000000,"[{'_account_id': 21797}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-09-21 01:05:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/3e42c3735f6650cbda41faf963b4acfac29be82d', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 2, 'created': '2017-09-27 14:10:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/51c63f6aef38638358dbc8de3475bfc044fa4389', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 3, 'created': '2017-09-27 14:24:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/a74cbf870c24c8f817f4329ee937f278140c29fb', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 4, 'created': '2017-09-27 14:37:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/52f5fcd12fecab7ac3c2a89696d35b4740bccb31', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 5, 'created': '2017-09-27 15:02:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/fc32ad24dd801fe4e5f200706bb84aa4c751a924', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 6, 'created': '2017-09-27 15:16:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/a8d80dedf75a8230f320545767ea630088f78b16', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 7, 'created': '2017-09-27 17:33:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/5003555ac567bc825039e550bd6c31c66dabbfba', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 8, 'created': '2017-09-28 06:17:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/4f1690f0d8c2ec1b7f18936f40c3b6043b776a20', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 9, 'created': '2017-09-28 06:35:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/eae2823e0cd4ed6d0d135ea0f35b34ea412ad2df', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 10, 'created': '2017-10-04 22:56:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/6c07dd3e5ec9d070624d2265d99761cfd25950d2', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 11, 'created': '2017-10-04 23:09:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/7e26f9a27604013fc40127df7b999c8998c95edc', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 12, 'created': '2017-10-05 05:56:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/e7c9ff242409702ef2410374e80ba17215124e72', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 13, 'created': '2017-10-05 06:27:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/2c7d1076039a040782b07ab1e02cd3cfccdef3d7', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 14, 'created': '2017-10-10 20:43:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/be5caf3384461e8d561e0c69e0f66500be837905', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 15, 'created': '2017-10-11 00:27:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/7165b930a2632ae34f04e7d058290a18a7956a27', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 16, 'created': '2017-10-11 00:40:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/bf3bc27bc8dd1a9f9794a342b5d3e9b772b38fea', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 17, 'created': '2017-10-11 00:53:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/e20f526acfa1a712e3ecd9341e51403d16e3cb94', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 18, 'created': '2017-10-27 13:43:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/68e3b524d854457cbcddc81c08cae5342650e3fb', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 19, 'created': '2017-10-27 22:00:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/1a24aa8db012e899c2aa56c6580ab2452043a9d5', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 20, 'created': '2017-11-02 04:08:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/31fab4bbb34569dd55058184e9875fe156c2e55f', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 21, 'created': '2017-11-06 03:34:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/18ca794bffc8d0424b9a5b200edcb3c282914db0', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 22, 'created': '2017-11-06 03:37:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/c18a664e51339907d011a63a525b9fbcddffc803', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 23, 'created': '2017-11-06 03:38:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/e1c4fc36d79ee9e744e6dd68c28e9c35b027032a', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 24, 'created': '2017-11-06 03:51:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/b71d7ee7b0dbcbfd59e0b30d943cd4a395a5289f', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 25, 'created': '2017-11-06 03:58:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/7513220adf621bce4813d7b5bcdd01a51729d792', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 26, 'created': '2017-11-14 21:06:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/44a49522f994ff48f7d6b08096224dca05b99285', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}, {'number': 27, 'created': '2017-11-15 01:49:56.000000000', 'files': ['test-requirements.txt'], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/6dcb1319611f97524f84b79a7ebd723e3ea5440e', 'message': 'Updated from global requirements\n\nChange-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d\n'}]",0,505874,6dcb1319611f97524f84b79a7ebd723e3ea5440e,57,2,27,11131,,,0,"Updated from global requirements

Change-Id: Ibcbd0e6337025119d41241787ab965c5b3d0046d
",git fetch https://review.opendev.org/openstack/python-barbicanclient refs/changes/74/505874/17 && git format-patch -1 --stdout FETCH_HEAD,['test-requirements.txt'],1,3e42c3735f6650cbda41faf963b4acfac29be82d,openstack/requirements,python-openstackclient>=3.11.0 # Apache-2.0,"python-openstackclient!=3.10.0,>=3.3.0 # Apache-2.0",1,1
openstack%2Fironic-python-agent~master~Ibf742198e83ae13f90767b28cc1858f0a17c3a95,openstack/ironic-python-agent,master,Ibf742198e83ae13f90767b28cc1858f0a17c3a95,Store image on disk when bigger than RAM size,NEW,2017-02-07 21:10:26.000000000,2017-12-18 03:56:53.000000000,,"[{'_account_id': 10342}, {'_account_id': 20902}]","[{'number': 1, 'created': '2017-02-07 21:10:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/8e8bf7d0638c4289fcbba09ea6a855f50c6c421d', 'message': 'Store image on disk when bigger than RAM size\n\nInstallation of big disk images on servers with a small amount of RAM\nis actually failing. The download starts and fails when the memory\ncapacity is hit. When using raw images, the image could still be\nstreamed and written on the disk but this uses a bigger amount of\nnetwork bandwidth and storage space on Glance.\n\nImage is cached on disk only when there is not enough space in RAM and\nthe image is not a partition image.  It is stored at the very end of\nthe block device where the OS will be installed by making a loop device\non this one.\n\nChange-Id: Ibf742198e83ae13f90767b28cc1858f0a17c3a95\n'}, {'number': 2, 'created': '2017-02-08 15:47:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/65a7ecca328e9933e4f23fd06cbf0d263c5e0110', 'message': 'Store image on disk when bigger than RAM size\n\nInstallation of big disk images on servers with a small amount of RAM\nis actually failing. The download starts and fails when the memory\ncapacity is hit. When using raw images, the image could still be\nstreamed and written on the disk but this uses a bigger amount of\nnetwork bandwidth and storage space on Glance.\n\nImage is cached on disk only when there is not enough space in RAM and\nthe image is not a partition image.  It is stored at the very end of\nthe block device where the OS will be installed by making a loop device\non this one.\n\nChange-Id: Ibf742198e83ae13f90767b28cc1858f0a17c3a95\nPartial-bug: 1661328\n'}, {'number': 3, 'created': '2017-02-08 16:04:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/ca8aafee958953735e9cfe965d653c0b804a2c95', 'message': 'Store image on disk when bigger than RAM size\n\nInstallation of big disk images on servers with a small amount of RAM\nis actually failing. The download starts and fails when the memory\ncapacity is hit. When using raw images, the image could still be\nstreamed and written on the disk but this uses a bigger amount of\nnetwork bandwidth and storage space on Glance.\n\nImage is cached on disk only when there is not enough space in RAM and\nthe image is not a partition image.  It is stored at the very end of\nthe block device where the OS will be installed by making a loop device\non this one.\n\nChange-Id: Ibf742198e83ae13f90767b28cc1858f0a17c3a95\nCloses-bug: 1661328\n'}, {'number': 4, 'created': '2017-02-08 18:51:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/c349b139a47e090cb3f867acba7d26bae9528376', 'message': 'Store image on disk when bigger than RAM size\n\nInstallation of big disk images on servers with a small amount of RAM\nis actually failing. The download starts and fails when the memory\ncapacity is hit. When using raw images, the image could still be\nstreamed and written on the disk but this uses a bigger amount of\nnetwork bandwidth and storage space on Glance.\n\nImage is cached on disk only when there is not enough space in RAM and\nthe image is not a partition image.  It is stored at the very end of\nthe block device where the OS will be installed by making a loop device\non this one.\n\nChange-Id: Ibf742198e83ae13f90767b28cc1858f0a17c3a95\nCloses-bug: 1661328\n'}, {'number': 5, 'created': '2017-03-10 22:48:08.000000000', 'files': ['ironic_python_agent/shell/create_loop.sh', 'ironic_python_agent/shell/write_image.sh', 'ironic_python_agent/tests/unit/extensions/test_standby.py', 'ironic_python_agent/extensions/standby.py', 'ironic_python_agent/errors.py'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/2c9b64e90e388fbf9d65f2375ad02cec5aecdfcf', 'message': 'Store image on disk when bigger than RAM size\n\nInstallation of big disk images on servers with a small amount of RAM\nis actually failing. The download starts and fails when the memory\ncapacity is hit. When using raw images, the image could still be\nstreamed and written on the disk but this uses a bigger amount of\nnetwork bandwidth and storage space on Glance.\n\nImage is cached on disk only when there is not enough space in RAM and\nthe image is not a partition image.  It is stored at the very end of\nthe block device where the OS will be installed by making a loop device\non this one.\n\nChange-Id: Ibf742198e83ae13f90767b28cc1858f0a17c3a95\nCloses-bug: 1661328\n'}]",0,430442,2c9b64e90e388fbf9d65f2375ad02cec5aecdfcf,12,2,5,12060,,,0,"Store image on disk when bigger than RAM size

Installation of big disk images on servers with a small amount of RAM
is actually failing. The download starts and fails when the memory
capacity is hit. When using raw images, the image could still be
streamed and written on the disk but this uses a bigger amount of
network bandwidth and storage space on Glance.

Image is cached on disk only when there is not enough space in RAM and
the image is not a partition image.  It is stored at the very end of
the block device where the OS will be installed by making a loop device
on this one.

Change-Id: Ibf742198e83ae13f90767b28cc1858f0a17c3a95
Closes-bug: 1661328
",git fetch https://review.opendev.org/openstack/ironic-python-agent refs/changes/42/430442/5 && git format-patch -1 --stdout FETCH_HEAD,"['ironic_python_agent/shell/create_loop.sh', 'ironic_python_agent/tests/unit/extensions/test_standby.py', 'ironic_python_agent/extensions/standby.py', 'ironic_python_agent/errors.py']",4,8e8bf7d0638c4289fcbba09ea6a855f50c6c421d,bug/1661328,"class ImageFileTooLargeError(RESTError): """"""Error raised when an image cannot be stored on cache."""""" message = 'Error allocating space on cache for the image' def __init__(self, exit_code, stdout, stderr): details = ('Pre-allocating space for image failed with exit code ' '{}. stdout: {}. stderr: {}') details = details.format(exit_code, stdout, stderr) super(ImageFileTooLargeError, self).__init__(details) ",,167,3
openstack%2Fpython-tackerclient~master~I32b230019cea9e6d1613d1016dd22956b5163118,openstack/python-tackerclient,master,I32b230019cea9e6d1613d1016dd22956b5163118,Add Python 3.4 classifier and venv,NEW,2016-07-14 06:06:11.000000000,2017-12-18 03:56:48.000000000,,"[{'_account_id': 2874}, {'_account_id': 13380}, {'_account_id': 22132}]","[{'number': 1, 'created': '2016-07-14 06:06:11.000000000', 'files': ['setup.cfg'], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/b69abc02b113d43fa3827ee4694bea26770d34f1', 'message': 'Add Python 3.4 classifier and venv\n\nNow that there is a passing gate job, we can claim support for Python 3.4 in the classifier.\n\nChange-Id: I32b230019cea9e6d1613d1016dd22956b5163118\n'}]",1,341977,b69abc02b113d43fa3827ee4694bea26770d34f1,21,3,1,22132,,,0,"Add Python 3.4 classifier and venv

Now that there is a passing gate job, we can claim support for Python 3.4 in the classifier.

Change-Id: I32b230019cea9e6d1613d1016dd22956b5163118
",git fetch https://review.opendev.org/openstack/python-tackerclient refs/changes/77/341977/1 && git format-patch -1 --stdout FETCH_HEAD,['setup.cfg'],1,b69abc02b113d43fa3827ee4694bea26770d34f1,, Programming Language :: Python :: 2.7 Programming Language :: Python :: 3.4, Programming Language :: Python :: 2.7,2,1
openstack%2Fpython-tackerclient~master~Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2,openstack/python-tackerclient,master,Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2,Use mock instead of mox in python-tackerclient,NEW,2016-08-23 11:22:02.000000000,2017-12-18 03:56:39.000000000,,"[{'_account_id': 2874}, {'_account_id': 22132}, {'_account_id': 22863}]","[{'number': 1, 'created': '2016-08-23 11:22:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/3bcb79d2511c2e7612f8bc7863cd67a24ae5780e', 'message': 'Stop using mox from test_ssl/test_http.py\n\n1.Try to replace mox with mock in tackerclient/test/unit/\ntest_ssl/test_http.py.\n2.Add mock in test-requriements.txt.\n\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 2, 'created': '2016-08-23 11:39:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/8ed9ea4248f0bcd38202c3d04242b9601624fd56', 'message': 'Stop using mox from test_ssl/test_http.py\n\n1.Try to replace mox with mock in tackerclient/test/unit/\ntest_ssl/test_http.py.\n2.Add mock in test-requriements.txt.\n\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 3, 'created': '2016-08-23 11:56:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/e713fc616263302282f6a21545670cf62aa87f3c', 'message': 'Stop using mox from test_ssl/test_http.py\n\n1.Try to replace mox with mock in tackerclient/test/unit/\ntest_ssl/test_http.py.\n2.Add mock in test-requriements.txt.\n\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 4, 'created': '2016-08-23 12:35:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/f7f75495f42de8b03eaa310687d29d55c9d4b6b4', 'message': 'Stop using mox from test_ssl/test_http.py\n\n1.Try to replace mox with mock in tackerclient/test/unit/\ntest_ssl/test_http.py.\n2.Add mock in test-requriements.txt.\n\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 5, 'created': '2016-08-24 02:10:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/9227ca351d18a47cf619df917967616463f92f1e', 'message': 'Stop using mox from test_ssl/test_http.py\n\n1.Try to replace mox with mock in tackerclient/test/unit/\ntest_ssl/test_http.py.\n2.Add mock in test-requriements.txt.\n\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 6, 'created': '2016-08-24 02:43:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/af91a869f93a53270fc4349cbb6422965fb00e8e', 'message': 'Stop using mox from test_ssl/test_http.py\n\n1.Try to replace mox with mock in tackerclient/test/unit/\ntest_ssl/test_http.py.\n2.Add mock in test-requriements.txt.\n\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 7, 'created': '2016-08-24 06:35:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/ca1d1b3dee594066a00abc20949726499807d4dd', 'message': 'Stop using mox in tackerclient (1)\n\n1.Try to replace mox with mock in tackerclient/test/unit/\ntest_ssl/test_http.py.\n2.Add mock in test-requriements.txt.\n\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 8, 'created': '2016-09-22 06:23:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/df153775b8edbbc1d30f31ad565bd4a9119a7852', 'message': 'Stop using mox in tackerclient (1)\n\n1.Try to replace mox with mock in tackerclient/test/unit/\ntest_ssl/test_http.py\\test_shell.py\\test.ssl.py.\n2.Add mock in test-requriements.txt.\n\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 9, 'created': '2017-01-10 11:13:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/e850201d4df4f0a5c5fdd6b3d907dbc654b8e986', 'message': 'Use mock instead of in test_validate.py\n\nCo-Authored-By: shizhihui <zhihui.shi@easystack.cn>\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 10, 'created': '2017-01-10 11:23:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/8967bed69f5a29106dc4eea25491d78b7b9c5dbd', 'message': 'Use mock instead of mox in python-tackerclient\n\nThe files:\n    tackerclient/tests/unit/test_http.py\n    tackerclient/tests/unit/test_shell.py\n    tackerclient/tests/unit/test_ssl.py\n\nCo-Authored-By: shizhihui <zhihui.shi@easystack.cn>\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 11, 'created': '2017-01-10 11:28:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/6a790cb87ced06facf1851a23be4097090223aca', 'message': 'Use mock instead of mox in python-tackerclient\n\nThe files below:\n    tackerclient/tests/unit/test_http.py\n    tackerclient/tests/unit/test_shell.py\n    tackerclient/tests/unit/test_ssl.py\n\nCo-Authored-By: shizhihui <zhihui.shi@easystack.cn>\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 12, 'created': '2017-01-20 11:43:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/50b7a3b713f5df04c29d88436819a550bba8f65d', 'message': 'Use mock instead of mox in python-tackerclient\n\nIn addition, the cases diretory is moved up so that\nall unit tests can be run.\n\nCo-Authored-By: shizhihui <zhihui.shi@easystack.cn>\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 13, 'created': '2017-01-23 04:43:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/8dfa9cdf216afa62e4ab4a2817d12c34a4657c9b', 'message': 'Use mock instead of mox in python-tackerclient\n\nThe files below:\n    tackerclient/tests/unit/test_http.py\n    tackerclient/tests/unit/test_shell.py\n    tackerclient/tests/unit/test_ssl.py\n\nCo-Authored-By: shizhihui <zhihui.shi@easystack.cn>\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}, {'number': 14, 'created': '2017-03-08 08:44:39.000000000', 'files': ['tackerclient/tests/unit/test_http.py', 'tackerclient/tests/unit/test_shell.py', 'tackerclient/tests/unit/test_ssl.py'], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/8875be0266bdd619596c3a1450722b72267374a5', 'message': 'Use mock instead of mox in python-tackerclient\n\nThe files below:\n    tackerclient/tests/unit/test_http.py\n    tackerclient/tests/unit/test_shell.py\n    tackerclient/tests/unit/test_ssl.py\n\nCo-Authored-By: shizhihui <zhihui.shi@easystack.cn>\nChange-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2\n'}]",0,359132,8875be0266bdd619596c3a1450722b72267374a5,37,3,14,22132,,,0,"Use mock instead of mox in python-tackerclient

The files below:
    tackerclient/tests/unit/test_http.py
    tackerclient/tests/unit/test_shell.py
    tackerclient/tests/unit/test_ssl.py

Co-Authored-By: shizhihui <zhihui.shi@easystack.cn>
Change-Id: Iac0a1bef773b1c7e79a3ddceb1eb026529c3ecf2
",git fetch https://review.opendev.org/openstack/python-tackerclient refs/changes/32/359132/11 && git format-patch -1 --stdout FETCH_HEAD,"['tackerclient/tests/unit/test_http.py', 'test-requirements.txt', 'tackerclient/tests/unit/test_ssl.py']",3,3bcb79d2511c2e7612f8bc7863cd67a24ae5780e,mox-to-mock-1,"import mock @mock.patch.object(ClientManager, '__init__') @mock.pathc.object(openstack_shell.TackerShell, 'interact') def test_ca_cert_passed(self, mock_init, mock_interact): api_version=mock.ANY, auth_strategy=mock.ANY, auth_url=mock.ANY, service_type=mock.ANY, endpoint_type=mock.ANY, insecure=mock.ANY, password=mock.ANY, region_name=mock.ANY, tenant_id=mock.ANY, tenant_name=mock.ANY, token=mock.ANY, url=mock.ANY, username=mock.ANY, user_id=mock.ANY, log_credentials=mock.ANY, mock_interact.return_value = 0 mock_interact.assert_called_once_with() mock_init.assert_called_once_with( ca_cert=CA_CERT, api_version, auth_strategy, auth_url, service_type, endpoint_type, insecure, password, region_name, tenant_id, tenant_name, token, url, username, user_id, log_credentials, ) @mock.pathc.object(openstack_shell.TackerShell, 'interact') @mock.patch.object(ClientManager, '__init__') def test_ca_cert_passed_as_env_var(self, mock_init, mock_interact): api_version=mock.ANY, auth_strategy=mock.ANY, auth_url=mock.ANY, service_type=mock.ANY, endpoint_type=mock.ANY, insecure=mock.ANY, password=mock.ANY, region_name=mock.ANY, tenant_id=mock.ANY, tenant_name=mock.ANY, token=mock.ANY, url=mock.ANY, username=mock.ANY, user_id=mock.ANY, log_credentials=mock.ANY, mock_interact.return_value = 0 mock_interact.assert_called_once_with() mock_init.assert_called_once_with( ca_cert=CA_CERT, api_version, auth_strategy, auth_url, service_type, endpoint_type, insecure, password, region_name, tenant_id, tenant_name, token, url, username, user_id, log_credentials, ) with mock.patch.object(HTTPClient, '__init__'): HTTPClient.__init__( ca_cert=CA_CERT, # we are not really interested in other args auth_strategy=mock.ANY, auth_url=mock.ANY, endpoint_url=mock.ANY, insecure=mock.ANY, password=mock.ANY, region_name=mock.ANY, tenant_name=mock.ANY, token=mock.ANY, username=mock.ANY, ) mock_init.assert_called_once_with( ca_cert=CA_CERT, auth_strategy, auth_url, endpoint_url, insecure, password, region_name, tenant_name, token, username, ) with mock.patch.object(HTTPClient, 'request') as mock_request: mock_request.return_value = requests.exceptions.SSLError mock_request.aseert_called_once_with(URL, METHOD, headers)","import mox self.mox = mox.Mox() self.addCleanup(self.mox.UnsetStubs) def test_ca_cert_passed(self): self.mox.StubOutWithMock(ClientManager, '__init__') self.mox.StubOutWithMock(openstack_shell.TackerShell, 'interact') api_version=mox.IgnoreArg(), auth_strategy=mox.IgnoreArg(), auth_url=mox.IgnoreArg(), service_type=mox.IgnoreArg(), endpoint_type=mox.IgnoreArg(), insecure=mox.IgnoreArg(), password=mox.IgnoreArg(), region_name=mox.IgnoreArg(), tenant_id=mox.IgnoreArg(), tenant_name=mox.IgnoreArg(), token=mox.IgnoreArg(), url=mox.IgnoreArg(), username=mox.IgnoreArg(), user_id=mox.IgnoreArg(), log_credentials=mox.IgnoreArg(), openstack_shell.TackerShell.interact().AndReturn(0) self.mox.ReplayAll() self.mox.VerifyAll() def test_ca_cert_passed_as_env_var(self): self.mox.StubOutWithMock(ClientManager, '__init__') self.mox.StubOutWithMock(openstack_shell.TackerShell, 'interact') api_version=mox.IgnoreArg(), auth_strategy=mox.IgnoreArg(), auth_url=mox.IgnoreArg(), service_type=mox.IgnoreArg(), endpoint_type=mox.IgnoreArg(), insecure=mox.IgnoreArg(), password=mox.IgnoreArg(), region_name=mox.IgnoreArg(), tenant_id=mox.IgnoreArg(), tenant_name=mox.IgnoreArg(), token=mox.IgnoreArg(), url=mox.IgnoreArg(), username=mox.IgnoreArg(), user_id=mox.IgnoreArg(), log_credentials=mox.IgnoreArg(), openstack_shell.TackerShell.interact().AndReturn(0) self.mox.ReplayAll() self.mox.VerifyAll() self.mox.StubOutWithMock(HTTPClient, '__init__') HTTPClient.__init__( ca_cert=CA_CERT, # we are not really interested in other args auth_strategy=mox.IgnoreArg(), auth_url=mox.IgnoreArg(), endpoint_url=mox.IgnoreArg(), insecure=mox.IgnoreArg(), password=mox.IgnoreArg(), region_name=mox.IgnoreArg(), tenant_name=mox.IgnoreArg(), token=mox.IgnoreArg(), username=mox.IgnoreArg(), ) self.mox.ReplayAll() self.mox.VerifyAll() self.mox.StubOutWithMock(HTTPClient, 'request') HTTPClient.request( URL, METHOD, headers=mox.IgnoreArg() ).AndRaise(requests.exceptions.SSLError) self.mox.ReplayAll() self.mox.VerifyAll()",119,97
openstack%2Fbifrost~master~I8f7c103f56c4217f86b5b5da857ed8d8f99517f0,openstack/bifrost,master,I8f7c103f56c4217f86b5b5da857ed8d8f99517f0,WiP move bifrost testing to 'execute on remote',NEW,2017-03-15 16:58:08.000000000,2017-12-18 03:55:57.000000000,,"[{'_account_id': 9542}, {'_account_id': 22474}]","[{'number': 1, 'created': '2017-03-15 16:58:08.000000000', 'files': ['playbooks/roles/bifrost-unprovision-node-dynamic/tasks/main.yml', 'playbooks/roles/bifrost-create-vm-nodes/tasks/main.yml', 'doc/source/baremetal.json.example', 'doc/source/baremetal.csv.example', 'playbooks/roles/bifrost-create-vm-nodes/defaults/required_defaults_Debian.yml', 'playbooks/test-bifrost.yaml', 'playbooks/roles/ironic-delete-dynamic/tasks/main.yml', 'playbooks/roles/bifrost-deploy-nodes-dynamic/tasks/main.yml', 'scripts/test-bifrost.sh', 'playbooks/test-bifrost-create-vm.yaml', 'doc/source/baremetal.yml.example'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/df655f5370cadf02427f982f2a44a56a8db7b47c', 'message': ""WiP move bifrost testing to 'execute on remote'\n\nbifrost already aims to support installing and managing ironic on a\nremote host opposing to installing ironic locally where bifrost is\ninstalled itself.\n\nLet's move the testing procedure to execute under this assumption as well.\n\nChanges include:\n- moving *.example files out of inventory to not interfere with ansible\n  using multiple inventory sources\n- executing tasks locally is removed or added where needed\n- changes to test playbooks and test-bifrost.sh files to accomodate\n  for these changes\n- add packages needed to build virtualbmc and its dependencies to the\n  list of required packages in 'bifrost-create-vm-nodes' role (in\n  progress)\n\nThe only non-working on a remote target piece currently is ssh-ing to\nthe deployed VMs.\n\nChange-Id: I8f7c103f56c4217f86b5b5da857ed8d8f99517f0\n""}]",0,446067,df655f5370cadf02427f982f2a44a56a8db7b47c,5,2,1,9542,,,0,"WiP move bifrost testing to 'execute on remote'

bifrost already aims to support installing and managing ironic on a
remote host opposing to installing ironic locally where bifrost is
installed itself.

Let's move the testing procedure to execute under this assumption as well.

Changes include:
- moving *.example files out of inventory to not interfere with ansible
  using multiple inventory sources
- executing tasks locally is removed or added where needed
- changes to test playbooks and test-bifrost.sh files to accomodate
  for these changes
- add packages needed to build virtualbmc and its dependencies to the
  list of required packages in 'bifrost-create-vm-nodes' role (in
  progress)

The only non-working on a remote target piece currently is ssh-ing to
the deployed VMs.

Change-Id: I8f7c103f56c4217f86b5b5da857ed8d8f99517f0
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/67/446067/1 && git format-patch -1 --stdout FETCH_HEAD,"['playbooks/roles/bifrost-create-vm-nodes/tasks/main.yml', 'playbooks/roles/bifrost-unprovision-node-dynamic/tasks/main.yml', 'doc/source/baremetal.csv.example', 'doc/source/baremetal.json.example', 'playbooks/roles/bifrost-create-vm-nodes/defaults/required_defaults_Debian.yml', 'playbooks/test-bifrost.yaml', 'playbooks/roles/ironic-delete-dynamic/tasks/main.yml', 'playbooks/roles/bifrost-deploy-nodes-dynamic/tasks/main.yml', 'scripts/test-bifrost.sh', 'doc/source/baremetal.yml.example', 'playbooks/test-bifrost-create-vm.yaml']",11,df655f5370cadf02427f982f2a44a56a8db7b47c,venv-fixes,- hosts: target,- hosts: localhost connection: local,41,27
openstack%2Foctavia~master~I66f4d9b8b303576af4c9f56410ef010931bed077,openstack/octavia,master,I66f4d9b8b303576af4c9f56410ef010931bed077,ACTIVE-ACTIVE Topology OVS-based Distributor Driver,NEW,2016-05-17 17:07:06.000000000,2017-12-18 03:55:47.000000000,,"[{'_account_id': 8335}, {'_account_id': 11628}, {'_account_id': 12040}, {'_account_id': 13438}, {'_account_id': 16923}, {'_account_id': 17419}, {'_account_id': 21138}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-05-17 17:07:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/de20a093ccf6ba0fe8f73f7ac51353cba11a26cd', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\nTODO: add tests to the patch\n'}, {'number': 2, 'created': '2016-05-18 16:55:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/454bfcbe3e2116a92fa5f54033b5aaad653a0fb3', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\nTODO: add tests to the patch\n'}, {'number': 3, 'created': '2016-05-20 05:33:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a7ba050e2801d0f21576da1f150f269199753f99', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\nTODO: add tests to the patch\n'}, {'number': 4, 'created': '2016-05-20 06:42:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/88d4bcdab0383c76fd91c5ff653f951dc6e58b86', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\nTODO: add tests to the patch\n'}, {'number': 5, 'created': '2016-05-24 06:27:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d13cbe92de1ad28cfc0f7ccc8f4a466ee2c6bed9', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\nTODO: add tests to the patch\n'}, {'number': 6, 'created': '2016-05-24 12:40:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7fcea0a5a8466ca69dc34116465ffeece4fde372', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\nTODO: add tests to the patch\n'}, {'number': 7, 'created': '2016-05-26 11:35:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1a81cde37342f72de3ed463f2327012ea3a60b05', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\nTODO: add tests to the patch\n'}, {'number': 8, 'created': '2016-05-29 12:18:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ea2eda9375d52312879af49ce888eb5f59c472ae', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\nTODO: add tests to the patch\n'}, {'number': 9, 'created': '2016-05-31 16:22:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c500574f9985274ab891aafe8fa58e734dfc9c48', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\nTODO: add tests to the patch\n'}, {'number': 10, 'created': '2016-06-13 11:14:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/dd33fcf3395552cbeb65961177653ed39f7639ef', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\nTODO: add tests to the patch\n'}, {'number': 11, 'created': '2016-06-13 13:03:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cc370288ddc06cc31823923df969ee6672c6e50b', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\nTODO: add tests to the patch\n'}, {'number': 12, 'created': '2016-06-15 12:02:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a770d297056f77c165eeed76132eda3a30b3de0a', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 13, 'created': '2016-06-20 14:12:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0119cc6394fcf8c1b4fd32550cc210874b73258d', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 14, 'created': '2016-06-21 05:53:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/54094d6d944108730ecd6282fe8340287f063973', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 15, 'created': '2016-07-05 19:29:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/47e5e16f8fc3fb5c57422568411d74e327cc4095', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 16, 'created': '2016-07-12 17:28:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/fc2aa5d5b1aef8dbc1c4a3ae1ccd46bbf8ad7d88', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 17, 'created': '2016-07-19 15:04:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f4c6112b5d446019446ef43be4e7b04f1eeeefd2', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 18, 'created': '2016-07-19 15:24:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/540483c3e8ef52d8d357f4a46ac0eb62a22cd9a4', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 19, 'created': '2016-07-20 11:42:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9311b2ecc6a6062a396c01facb123319c3bfff8c', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 20, 'created': '2016-08-23 16:03:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/59c28899d174bf377f4563fd0a65202d8bf5887e', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 21, 'created': '2016-09-19 15:41:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9c112bc94bf419b19a60a8e0049edf29c384aafa', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 22, 'created': '2016-09-20 12:48:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8fece65dd30faf9893455095850778cd5d91f846', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 23, 'created': '2016-09-26 07:54:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d7ccb3073b68d332a3980e29043a08b425dad917', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 24, 'created': '2016-09-26 08:02:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/264d1889d001cdcd1a942e80013441a72a09e74e', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 25, 'created': '2016-09-27 13:05:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/af40f34e4d9803b021d022497f8da8d7ef54429c', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 26, 'created': '2016-10-09 12:02:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/171e864e4bd5cf1c55af20c79a66d0fa3c792b43', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 27, 'created': '2016-10-09 13:41:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a7bd0bb7023259be777fd1310cb4591201640074', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 28, 'created': '2016-11-27 14:47:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2bb6f1a3bbfb08cb8a2953367c0239f4abf7cc26', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 29, 'created': '2016-11-28 10:46:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2fb4eb46449fe1a93985bd972f247f1e8d496859', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 30, 'created': '2016-11-28 11:51:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c2d07a0a245dcd11b1ed51ebc486a5b851867298', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 31, 'created': '2016-11-30 15:17:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/03839d6a602a3f42731ee53b964eca25f2bbf228', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 32, 'created': '2016-12-01 09:55:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f67dc304f7cd8564a5f2944cac145da2b7a121ab', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 33, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c56a9baf841436e7f1638ec5f9b425a6632f8466', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 34, 'created': '2016-12-13 08:50:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ffa3d1da08265ea7914c2845910f892e58ed6a36', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 35, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7f2d5d793b0d9e10c7982cce8d928b08164b8930', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 36, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a78722ce6b096f61f0c2a0e5358cefcd532463a3', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 37, 'created': '2016-12-18 11:15:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/db0a5cb86006b5e0047c6e9896c52bfd68f5d0cc', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 38, 'created': '2016-12-20 13:45:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ed8d8fe3c2a82114e9e828e915a930f403a9f8ba', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 39, 'created': '2016-12-25 16:08:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6862fc9941eeee043b63c297ed5bcd8f00073e7e', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 40, 'created': '2017-01-02 11:39:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b1f86d57e97a37b1c28fe4a624ac520fc2bd9e28', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 41, 'created': '2017-01-02 15:04:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/875226f524096f41c5a6f3d02ee1e40feedbf87e', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 42, 'created': '2017-01-03 08:24:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f9ce2b18b664c052a7f01c7cd0176e52dd994c76', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 43, 'created': '2017-01-04 11:44:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/99fa378237ab58165fffdf4758139e8e8ecada9c', 'message': 'ACTIVE-ACTIVE Topology: OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 44, 'created': '2017-01-04 12:39:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c26d1918bb33b7271525290e7d40f64de455efc5', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 45, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/82eacb7671addedcc7e93e645d3aa54e8fd90bc4', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 46, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c6b10a13a9b1cf2a3d0136b58da88deb2225360a', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 47, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ac722a004054ba781529312304c757af2ea5e756', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 48, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1c5ff1270cf9c932c5bb872c19dd8df3b9427123', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 49, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1c625567a02a2a6247398d04f8d5e0069dc1429f', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 50, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f3c7a9de7bb70248af407029290ee4fbd68597a4', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 51, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c80d903da54bc1d8a0cc2f6444b66bb6d3e51006', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 52, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/dd7a349ddf8c04fc329e746bb1d2e221d8e0dc2c', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 53, 'created': '2017-01-22 13:10:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/95e4be6c3bc3666d205c4cbf34c25ad9adf6640e', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 54, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/09db904126be958cb52fc634579c62895a627cbe', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 55, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8dc2ae570fb541584726952c1a91c1705ff9ef12', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 56, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/03c44dd030d708a27c02bbf59b6f9e642670b9e4', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 57, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d1bf417962138d700d23fd9ef573606f2a2e692b', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 58, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/00cfb1663a61c5570dd7ab070d904ea803bdb920', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 59, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c1aab4fce96b28b04d681d8e3c9d6f82e94a8def', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 60, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/51906c5f18c69ab76c9d3f9c17a3435569d97432', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 61, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/373c6b2f7c01c4dec11cfe11dee3ca8c5457e4aa', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 62, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/114578980b397be20c58e0f362ac2de2bd5a490d', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}, {'number': 63, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/common/config.py', 'octavia/tests/unit/distributor/drivers/test_distributor_rest_api_driver.py', 'octavia/distributor/drivers/ovs_driver/rest_api_driver.py', 'octavia/distributor/drivers/ovs_driver/__init__.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/a23b8bb03e9126d5e3e87158189bc58940ef79da', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Driver\n\nImplements: blueprint https://review.openstack.org/#/c/234639\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n\nChange-Id: I66f4d9b8b303576af4c9f56410ef010931bed077\n'}]",32,317629,a23b8bb03e9126d5e3e87158189bc58940ef79da,210,8,63,21138,,,0,"ACTIVE-ACTIVE Topology OVS-based Distributor Driver

Implements: blueprint https://review.openstack.org/#/c/234639
Co-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>
Co-Authored-By: Valeria Perelman <perelman@il.ibm.com>

Change-Id: I66f4d9b8b303576af4c9f56410ef010931bed077
",git fetch https://review.opendev.org/openstack/octavia refs/changes/29/317629/15 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/distributor/drivers/ovs_driver/rest_api_driver.py', 'octavia/distributor/drivers/ovs_driver/__init__.py']",2,de20a093ccf6ba0fe8f73f7ac51353cba11a26cd,06tmp03,,,284,0
openstack%2Foctavia~master~Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e,openstack/octavia,master,Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e,ACTIVE-ACTIVE Topology OVS-based Distributor Backend,NEW,2016-05-24 12:40:03.000000000,2017-12-18 03:55:44.000000000,,"[{'_account_id': 8335}, {'_account_id': 12040}, {'_account_id': 16923}, {'_account_id': 24261}]","[{'number': 1, 'created': '2016-05-24 12:40:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/00c03e3d8ecea12434c16c2f08f2d5a7a6eb6201', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 2, 'created': '2016-05-24 14:46:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e3edf650c7e7de730a3ba2b414d4de2a98d1d4f7', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 3, 'created': '2016-05-26 11:35:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5340ba96d8210550d01438353e9c960ba5a0b462', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 4, 'created': '2016-05-29 12:18:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0949969982085fa2c13e61a1d4074233b8cb1eb4', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 5, 'created': '2016-05-31 16:22:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e6d7f876bad846acd35264ce1b06f55c14142fe0', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 6, 'created': '2016-06-15 12:08:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7662caa0d426787eabe0d76f3e49104da9461a16', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 7, 'created': '2016-06-20 14:12:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/08414f70447cc7612d923d6efb2a91103e075531', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 8, 'created': '2016-06-21 05:53:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/76bc95963ccfe7fd8c3448f8faf9e813b78aab97', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 9, 'created': '2016-07-05 19:30:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2e2bd92bc6a8aa43151311ea33c707848ff39869', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 10, 'created': '2016-07-12 17:28:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/349c9efe5757f7ffd03b08bbf55999d620dd846f', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 11, 'created': '2016-07-19 15:04:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bc7927f0395cb70859709be96e438943dd43fe89', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 12, 'created': '2016-07-19 15:24:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bd557bb66b3f355af33bc07fe3c62f6a520d30dc', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 13, 'created': '2016-07-20 11:42:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/84836e6a2f9b71be7164971ce022ded4cbda1091', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 14, 'created': '2016-08-23 16:03:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/6e213559a69eb19466b7fe9b812c4680255542d4', 'message': 'WIP: ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 15, 'created': '2016-09-20 12:36:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/890fab6e635b6e9e7d489de59038adf35de3ec95', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 16, 'created': '2016-09-20 12:48:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ee2fb0edec3501fe6f6bf7b04e7b6c3051d42648', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 17, 'created': '2016-09-26 07:54:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/143598987a3c597325c0b80f3f3a31e210160997', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 18, 'created': '2016-09-26 08:02:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/af6c93e2afe57518bb57f9384341479b7ec3d12a', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 19, 'created': '2016-09-27 13:05:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d1d5b9c2454be7beb946ad96c686a3fdad9b7246', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 20, 'created': '2016-10-09 12:02:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f77f1f42941f394576f467555d16feecea6da998', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 21, 'created': '2016-10-09 13:41:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0e39721672b0ad95e69e6e3dfc2690334baf9162', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 22, 'created': '2016-11-27 14:47:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bc70077ffc77276533e32c148861abb5919935f8', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 23, 'created': '2016-11-28 10:46:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f4176ce6884cd7d02ca5a03b6f0d9171c9f077b0', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 24, 'created': '2016-11-28 11:51:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/435533d949237674ea70cb2ceaf970161b81d943', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 25, 'created': '2016-11-30 15:17:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cd681108d7bf9ff4cb7a16d44ae4b95488d975c0', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 26, 'created': '2016-12-01 09:55:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ca9aac623dabaa9ce3ad9dba7132d99c779c29db', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 27, 'created': '2016-12-12 14:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b1c8df98020d94e73316cf9b63fdaf2fc510016f', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 28, 'created': '2016-12-13 08:51:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/3640b51e6e5d414b2ab0341bb79fe42741eb30e1', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 29, 'created': '2016-12-13 11:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2d56754b6c537f39127dcc6357a25689ea213171', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 30, 'created': '2016-12-14 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/20210938243385d546278326a214b1da79cabd47', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 31, 'created': '2016-12-18 11:15:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/186793688a16fdf4046fa8b45803487f85d78c0c', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 32, 'created': '2016-12-20 13:45:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e1655406a7755597b46dfccf0cd5eae599407ddc', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 33, 'created': '2016-12-25 16:08:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/192d457f2de2596b447f6b11e99e2e6c586ea630', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 34, 'created': '2017-01-02 15:04:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2e1d5edffe1313a8ea42134f843ac8f4a3adb827', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 35, 'created': '2017-01-03 12:29:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/37b0f4cb7fbf38aa42f99535db38bc419e10af8a', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 36, 'created': '2017-01-04 12:39:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/64574703de8fab2604be4fa253b3eabecb92c39e', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 37, 'created': '2017-01-05 11:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b892142a64b69bbb3a15e8af08f66b4031385158', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 38, 'created': '2017-01-08 12:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f7a72a833a406e360ac61eb20dc645e59a9e820c', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 39, 'created': '2017-01-08 15:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/25d633844a7406b63fa38bb97dcdc5c94a2de800', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 40, 'created': '2017-01-11 09:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7d9f0162ed5c05c4997ab349d8cdfaf07a2a04e6', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 41, 'created': '2017-01-12 11:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bc9379063734c9dbf06e1550d7e557f8a9f54b2e', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 42, 'created': '2017-01-15 09:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1b0d7d9dd724af3f9b5791cedf14050a0ef9f255', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 43, 'created': '2017-01-15 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d5668ff6e714dc0775c8675cd586ec673d23ed58', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 44, 'created': '2017-01-18 08:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/283da4631ead67f2dd46525c87077f8d824391a6', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 45, 'created': '2017-01-22 13:10:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/11a23070a2ccec3de3d9ec3fa8ae083ea1a4ec5a', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 46, 'created': '2017-01-23 13:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/0b4b7bd7cc3bf1deb94f2b7fa1b18af41843afa2', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 47, 'created': '2017-01-24 13:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/134a5122367a4631e68afb84473d5b65767fc39b', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 48, 'created': '2017-01-25 13:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/944ac4e668aa63cb946bb6ca5c8b427625603bec', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 49, 'created': '2017-01-29 12:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c5122602971183d256710b1576054d79102d9843', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 50, 'created': '2017-02-01 18:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ec1f155ab18456416f3f6944da45477b99bfbf2b', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 51, 'created': '2017-02-05 11:57:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/4e48d3528021ec1e79520c1ce498e211405ad824', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 52, 'created': '2017-02-05 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/759e71deee12d06ba1561350cb5b01a6a861e05a', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 53, 'created': '2017-02-19 16:00:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/06c77091206abfbf4957347e1677d8e25292619f', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 54, 'created': '2017-03-02 13:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9b1a7880ca56edd7e8a2985518d16031f3297c9c', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}, {'number': 55, 'created': '2017-03-06 15:52:04.000000000', 'files': ['octavia/tests/unit/distributor/backend/__init__.py', 'octavia/distributor/backend/agent/__init__.py', 'octavia/distributor/backend/agent/api_server/distributor_info.py', 'octavia/distributor/backend/agent/api_server/server.py', 'octavia/distributor/backend/__init__.py', 'octavia/distributor/backend/agent/api_server/distributor_data.py', 'octavia/distributor/backend/agent/api_server/open_flow.py', 'octavia/distributor/backend/agent/api_server/__init__.py', 'octavia/distributor/backend/agent/api_server/plug.py', 'octavia/common/constants.py', 'octavia/tests/unit/distributor/backend/agent/api_server/__init__.py', 'octavia/tests/unit/distributor/backend/agent/__init__.py', 'octavia/tests/unit/distributor/backend/agent/api_server/test_plug.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/c4516cb785f5cb5c208322a96b9ceaf6b9f0457d', 'message': 'ACTIVE-ACTIVE Topology OVS-based Distributor Backend\n\nImplements: blueprint https://review.openstack.org/#/c/234639\n\nInitial implementation of the distributor backend\nmissing actual OVS rules\n\nChange-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e\nCo-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>\nCo-Authored-By: Valeria Perelman <perelman@il.ibm.com>\n'}]",0,320422,c4516cb785f5cb5c208322a96b9ceaf6b9f0457d,174,4,55,21138,,,0,"ACTIVE-ACTIVE Topology OVS-based Distributor Backend

Implements: blueprint https://review.openstack.org/#/c/234639

Initial implementation of the distributor backend
missing actual OVS rules

Change-Id: Idc1366e1fa00c63c32a94f43f064dfb5cdfffe5e
Co-Authored-By: Kevin H. Tran <trankevi@us.ibm.com>
Co-Authored-By: Valeria Perelman <perelman@il.ibm.com>
",git fetch https://review.opendev.org/openstack/octavia refs/changes/22/320422/1 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/tests/unit/distributor/backend/__init__.py', 'octavia/distributor/backend/agent/__init__.py', 'octavia/distributor/backend/agent/api_server/distributor_info.py', 'octavia/distributor/backend/agent/api_server/server.py', 'octavia/distributor/backend/__init__.py', 'octavia/distributor/backend/agent/api_server/distributor_data.py', 'octavia/distributor/backend/agent/api_server/open_flow.py', 'octavia/distributor/backend/agent/api_server/__init__.py', 'octavia/distributor/backend/agent/api_server/plug.py', 'octavia/tests/unit/distributor/backend/agent/api_server/__init__.py', 'octavia/tests/unit/distributor/backend/agent/__init__.py', 'octavia/tests/unit/distributor/backend/agent/api_server/test_plug.py']",12,00c03e3d8ecea12434c16c2f08f2d5a7a6eb6201,06tmp03,"# Copyright 2015 Rackspace # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import mock import netifaces from octavia.distributor.backend.agent.api_server import plug import octavia.tests.unit.base as base @mock.patch.object(plug, ""netifaces"") class TestPlug(base.TestCase): def test__interface_by_mac_case_insensitive(self, mock_netifaces): mock_netifaces.AF_LINK = netifaces.AF_LINK mock_interface = 'eth0' mock_netifaces.interfaces.return_value = [mock_interface] mock_netifaces.ifaddresses.return_value = { netifaces.AF_LINK: [ {'addr': 'ab:cd:ef:00:ff:22'} ] } interface = plug._interface_by_mac('AB:CD:EF:00:FF:22') self.assertEqual('eth0', interface)",,512,0
openstack%2Ftacker~master~I6bb38bd709adb688b50e2f6ee2c9e6aea192ce9f,openstack/tacker,master,I6bb38bd709adb688b50e2f6ee2c9e6aea192ce9f,Make VIM name/description/is_default updatable,NEW,2016-10-28 11:37:50.000000000,2017-12-18 03:55:27.000000000,,"[{'_account_id': 2874}, {'_account_id': 6348}, {'_account_id': 22588}]","[{'number': 1, 'created': '2016-10-28 11:37:50.000000000', 'files': ['tacker/db/nfvo/nfvo_db.py', 'tacker/tests/unit/vm/nfvo/test_nfvo_plugin.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/7bbeb6c76f965a5a9bd1e6dcad45ed31051bb0c8', 'message': ""Make VIM name/description/is_default updatable\n\nNow Tacker does not support updating name and description of VIM.\nIn addition, API call to update is_default attribute from 'true' to\n'false' will be ignored, while updating is_default from 'false' to\n'true' works well.\n\nThis patch introduces a way to update these attributes by API to\nimprove operability.\n\nChange-Id: I6bb38bd709adb688b50e2f6ee2c9e6aea192ce9f\nPartial-Bug: 1637360\n""}]",2,391426,7bbeb6c76f965a5a9bd1e6dcad45ed31051bb0c8,8,3,1,22588,,,0,"Make VIM name/description/is_default updatable

Now Tacker does not support updating name and description of VIM.
In addition, API call to update is_default attribute from 'true' to
'false' will be ignored, while updating is_default from 'false' to
'true' works well.

This patch introduces a way to update these attributes by API to
improve operability.

Change-Id: I6bb38bd709adb688b50e2f6ee2c9e6aea192ce9f
Partial-Bug: 1637360
",git fetch https://review.opendev.org/openstack/tacker refs/changes/26/391426/1 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/db/nfvo/nfvo_db.py', 'tacker/tests/unit/vm/nfvo/test_nfvo_plugin.py']",2,7bbeb6c76f965a5a9bd1e6dcad45ed31051bb0c8,bug/1637360," placement_attr={'regions': ['RegionOne']}, is_default=True) 'name': 'new_name', 'description': 'new_description', 'is_default': False, vim_name = vim_dict['vim']['name'] vim_description = vim_dict['vim']['description'] vim_is_default = vim_dict['vim']['is_default'] self.assertEqual(vim_name, res['name']) self.assertEqual(vim_description, res['description']) self.assertEqual(vim_is_default, res['is_default'])", placement_attr={'regions': ['RegionOne']}),18,5
openstack%2Fswift~master~I9ba55614da9ea56f50f1a9674e4b820c7a30d039,openstack/swift,master,I9ba55614da9ea56f50f1a9674e4b820c7a30d039,WIP - Make EC GET defer nodes likely to have duplicate fragments,NEW,2017-03-15 16:20:02.000000000,2017-12-18 03:54:55.000000000,,"[{'_account_id': 7847}, {'_account_id': 13052}]","[{'number': 1, 'created': '2017-03-15 16:20:02.000000000', 'files': ['test/unit/__init__.py', 'test/unit/proxy/controllers/test_obj.py', 'swift/proxy/controllers/obj.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/eed0a9c61c3f414c5aa634063172c6d52baab0a2', 'message': 'WIP - Make EC GET defer nodes likely to have duplicate fragments\n\nTESTS FAILING - some of the unit test failing because they assert\nmore requests being made than now needed.\n\nAlso needs:\n- more testing of the new algorithm,\n- clean up of now unused code in NodeIter\n- some explanation on commit message\n\nChange-Id: I9ba55614da9ea56f50f1a9674e4b820c7a30d039\n'}]",0,446050,eed0a9c61c3f414c5aa634063172c6d52baab0a2,5,2,1,7847,,,0,"WIP - Make EC GET defer nodes likely to have duplicate fragments

TESTS FAILING - some of the unit test failing because they assert
more requests being made than now needed.

Also needs:
- more testing of the new algorithm,
- clean up of now unused code in NodeIter
- some explanation on commit message

Change-Id: I9ba55614da9ea56f50f1a9674e4b820c7a30d039
",git fetch https://review.opendev.org/openstack/swift refs/changes/50/446050/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/__init__.py', 'test/unit/proxy/controllers/test_obj.py', 'swift/proxy/controllers/obj.py']",3,eed0a9c61c3f414c5aa634063172c6d52baab0a2,p-ec-skipping-iter," self.skipped_nodes = [] def wrap_iter(self, node_iter): Wrap a node iterator to modify the usual iteration order: - if ec_ndata or more nodes have been consumed from node_iter, prefer any previously used node that may have a wanted alternate frag index in addition to the frag index that it first returned; - otherwise, defer any primary node from node_iter whose node index suggests that it would have a frag index that has already been found on another node (this is to be expected with EC policies having a duplication factor >1); - otherwise, return the next node from the node_iter if there is one - otherwise, return the earliest deferred node - otherwise, raise StopIteration :param node_iter: an iterator that returns a sequence node dicts :return: a wrapped iterator that also returns a sequence of node dicts def _next(): # track the number of nodes to which GET requests have been made # and selectively inject an alternate node, if we have one. self.node_iter_count += 1 nodes = self._get_alternate_nodes() if nodes: return nodes.pop(0).copy() while True: try: node = node_iter.next() except StopIteration: if self.skipped_nodes: return self.skipped_nodes.pop(0) return None # nothing left - return the sentinel frag_index = self.policy.get_backend_index( node.get('index', '')) if self.best_bucket and frag_index in self.best_bucket.gets: # this node is expected to have a frag_index that has # already been found so defer it until other nodes have # been consumed; handoffs are never deferred because they # will have frag_index of None self.skipped_nodes.append(node) else: return node return iter(_next, None) buckets = ECGetResponseCollection(policy) safe_iter = buckets.wrap_iter(GreenthreadSafeIterator(node_iter))"," def provide_alternate_node(self): Callback function that is installed in a NodeIter. Called on every call to NodeIter.next(), which means we can track the number of nodes to which GET requests have been made and selectively inject an alternate node, if we have one. :return: A dict describing a node to which the next GET request should be made. self.node_iter_count += 1 nodes = self._get_alternate_nodes() if nodes: return nodes.pop(0).copy() safe_iter = GreenthreadSafeIterator(node_iter) buckets = ECGetResponseCollection(policy) node_iter.set_node_provider(buckets.provide_alternate_node)",163,64
openstack%2Faodh~master~Id08da277fdb84b326b489c350a705fed3e22495c,openstack/aodh,master,Id08da277fdb84b326b489c350a705fed3e22495c,Enable events notification verification when evaluating event alarms,NEW,2016-06-03 02:03:08.000000000,2017-12-18 03:54:53.000000000,,"[{'_account_id': 1894}, {'_account_id': 6537}, {'_account_id': 8290}]","[{'number': 1, 'created': '2016-06-03 02:03:08.000000000', 'files': ['aodh/evaluator/event.py', 'aodh/evaluator/utils.py', 'aodh/tests/unit/evaluator/test_event.py'], 'web_link': 'https://opendev.org/openstack/aodh/commit/4678f95f98af2d32de8f3bde2ea5c7a96b480c92', 'message': 'Enable events notification verification when evaluating event alarms\n\nChange-Id: Id08da277fdb84b326b489c350a705fed3e22495c\n'}]",0,324981,4678f95f98af2d32de8f3bde2ea5c7a96b480c92,12,3,1,8290,,,0,"Enable events notification verification when evaluating event alarms

Change-Id: Id08da277fdb84b326b489c350a705fed3e22495c
",git fetch https://review.opendev.org/openstack/aodh refs/changes/81/324981/1 && git format-patch -1 --stdout FETCH_HEAD,"['aodh/evaluator/event.py', 'aodh/evaluator/utils.py', 'aodh/tests/unit/evaluator/test_event.py']",3,4678f95f98af2d32de8f3bde2ea5c7a96b480c92,verify-signature,"from aodh.evaluator import utils def setUp(self): super(TestEventAlarmEvaluate, self).setUp() self.conf.set_override('event_secret', '') 'traits': kwargs.get('traits', []), 'message_signature': kwargs.get('message_signature', '')} def test_fire_alarm_with_event_verification(self): self.conf.set_override('event_secret', 'test_secret') alarm = self._alarm(project='project1') event = self._event(traits=[['project_id', 1, 'project1']]) signature = utils.compute_signature(event, self.conf.event_secret) event.update(message_signature=signature) self._do_test_event_alarm( [alarm], [event], expect_db_queries=['project1'], expect_alarm_states={alarm.alarm_id: evaluator.ALARM}, expect_alarm_updates=[alarm], expect_notifications=[dict(alarm=alarm, event=event)])"," 'traits': kwargs.get('traits', [])}",145,1
openstack%2Fironic-python-agent~master~Ifef196ec774371474c15a2aede5c2d2509397d64,openstack/ironic-python-agent,master,Ifef196ec774371474c15a2aede5c2d2509397d64,Disable local disks shredding if agent_erase_devices_iterations is 0,NEW,2015-07-07 13:09:42.000000000,2017-12-18 03:53:30.000000000,,"[{'_account_id': 6550}, {'_account_id': 6618}, {'_account_id': 6773}, {'_account_id': 10239}, {'_account_id': 10380}, {'_account_id': 11076}, {'_account_id': 11655}, {'_account_id': 12459}, {'_account_id': 13295}, {'_account_id': 13719}]","[{'number': 1, 'created': '2015-07-07 13:09:42.000000000', 'files': ['ironic_python_agent/hardware.py'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/1afb44012b15cfe154931b99c6f2c0f004d1b03c', 'message': 'Disable local disks shredding if agent_erase_devices_iterations is 0\n\nIt is possible to define multiple steps in the cleaning phase.\nGenericHardwareManager defines erase_devices() step which execute a\nshred of local disks.\nIf there are custom hardware managers, we might want to skip the\nshredding step or even override it with hardware specific operations\nwhile executing other steps.\nDisabling cleaning, would not only disable the shredding step but all\ncustom steps as well.\n\nChange-Id: Ifef196ec774371474c15a2aede5c2d2509397d64\n'}]",2,199088,1afb44012b15cfe154931b99c6f2c0f004d1b03c,19,10,1,6550,,,0,"Disable local disks shredding if agent_erase_devices_iterations is 0

It is possible to define multiple steps in the cleaning phase.
GenericHardwareManager defines erase_devices() step which execute a
shred of local disks.
If there are custom hardware managers, we might want to skip the
shredding step or even override it with hardware specific operations
while executing other steps.
Disabling cleaning, would not only disable the shredding step but all
custom steps as well.

Change-Id: Ifef196ec774371474c15a2aede5c2d2509397d64
",git fetch https://review.opendev.org/openstack/ironic-python-agent refs/changes/88/199088/1 && git format-patch -1 --stdout FETCH_HEAD,['ironic_python_agent/hardware.py'],1,1afb44012b15cfe154931b99c6f2c0f004d1b03c,," if npasses: # If npasses is 0, shredding is not needed try: utils.execute('shred', '--force', '--zero', '--verbose', '--iterations', str(npasses), block_device.name) except (processutils.ProcessExecutionError, OSError) as e: msg = (""Erasing block device %(dev)s failed with error "" ""%(err)s "", {'dev': block_device.name, 'err': e}) LOG.error(msg) return False"," try: utils.execute('shred', '--force', '--zero', '--verbose', '--iterations', str(npasses), block_device.name) except (processutils.ProcessExecutionError, OSError) as e: msg = (""Erasing block device %(dev)s failed with error %(err)s "", {'dev': block_device.name, 'err': e}) LOG.error(msg) return False",9,8
openstack%2Fglance_store~master~I3582a6daa896418dcaca21eaa60b03628ecdd476,openstack/glance_store,master,I3582a6daa896418dcaca21eaa60b03628ecdd476,WIP: Use swift conf file instead of swift store auth opts,NEW,2017-03-24 23:22:13.000000000,2017-12-18 03:53:28.000000000,,[{'_account_id': 21722}],"[{'number': 1, 'created': '2017-03-24 23:22:13.000000000', 'files': ['glance_store/tests/unit/test_swift_store_utils.py', 'glance_store/tests/etc/glance-swift.conf', 'glance_store/_drivers/swift/utils.py', 'glance_store/tests/unit/test_opts.py', 'glance_store/_drivers/swift/store.py', 'glance_store/tests/unit/test_swift_store.py'], 'web_link': 'https://opendev.org/openstack/glance_store/commit/b59e404b49b84c1a194f620ca540700b9b623753', 'message': 'WIP: Use swift conf file instead of swift store auth opts\n\nThis patch attempts to remove the swift store auth opts that have\nbeen deprecated over 2 cycles. The swift params will now be picked\nfrom the file set with ``swift_store_config_file`` under the reference\nset via ``default_swift_reference``.\n\nChange-Id: I3582a6daa896418dcaca21eaa60b03628ecdd476\nCloses-Bug: #1675950\n'}]",0,449842,b59e404b49b84c1a194f620ca540700b9b623753,4,1,1,21722,,,0,"WIP: Use swift conf file instead of swift store auth opts

This patch attempts to remove the swift store auth opts that have
been deprecated over 2 cycles. The swift params will now be picked
from the file set with ``swift_store_config_file`` under the reference
set via ``default_swift_reference``.

Change-Id: I3582a6daa896418dcaca21eaa60b03628ecdd476
Closes-Bug: #1675950
",git fetch https://review.opendev.org/openstack/glance_store refs/changes/42/449842/1 && git format-patch -1 --stdout FETCH_HEAD,"['glance_store/tests/unit/test_swift_store_utils.py', 'glance_store/tests/etc/glance-swift.conf', 'glance_store/_drivers/swift/utils.py', 'glance_store/tests/unit/test_opts.py', 'glance_store/_drivers/swift/store.py', 'glance_store/tests/unit/test_swift_store.py']",6,b59e404b49b84c1a194f620ca540700b9b623753,bug/1675950,"SWIFT_CONF = {'swift_store_container': 'glance',def stub_out_swiftclient(stubs): swift_store_config_file=None, http:// in the swift auth_address value '.is_swift_store_config_file', mock.Mock(return_value=True)) '.is_swift_store_config_file', mock.Mock(return_value=True)) '.is_swift_store_config_file', '.is_swift_store_config_file', '.is_swift_store_config_file', '.is_swift_store_config_file', '.is_swift_store_config_file', '.is_swift_store_config_file', '.is_swift_store_config_file', mock.Mock(return_value=True)) '.is_swift_store_config_file', conf['default_swift_reference'] = 'ref-v1' self.store = Store(self.conf) conf['default_swift_reference'] = 'ref-v2' conf['default_swift_reference'] = 'ref-v3' project_domain_id='default', project_domain_name='ignored', user_domain_id='default', user_domain_name='ignored',) self.config(swift_store_endpoint_type='internalURL',","SWIFT_CONF = {'swift_store_auth_address': 'localhost:8080', 'swift_store_container': 'glance', 'swift_store_user': 'user', 'swift_store_key': 'key',def stub_out_swiftclient(stubs, swift_store_auth_version): # Check the auth version against the configured value if swift_store_auth_version != auth_version: msg = 'AUTHENTICATION failed (version mismatch)' raise swiftclient.ClientException(msg) self.config(swift_store_config_file=None) swift_store_config_file='not/none', http:// in the swift_store_auth_address config value '.is_multiple_swift_store_accounts_enabled', mock.Mock(return_value=False)) '.is_multiple_swift_store_accounts_enabled', mock.Mock(return_value=False)) '.is_multiple_swift_store_accounts_enabled', conf['swift_store_user'] = 'tenant:user' '.is_multiple_swift_store_accounts_enabled', conf['swift_store_user'] = 'tenant:user' '.is_multiple_swift_store_accounts_enabled', conf['swift_store_user'] = 'tenant:user' '.is_multiple_swift_store_accounts_enabled', conf['swift_store_user'] = 'tenant:user' '.is_multiple_swift_store_accounts_enabled', '.is_multiple_swift_store_accounts_enabled', '.is_multiple_swift_store_accounts_enabled', mock.Mock(return_value=False)) '.is_multiple_swift_store_accounts_enabled', self.config(swift_store_config_file=None) self.config(swift_store_config_file=None) self.config(swift_store_config_file=None) self.config(swift_store_config_file=None) self.config(swift_store_config_file=None) conf['swift_store_auth_version'] = '1' conf['swift_store_user'] = 'tenant:user1' stub_out_swiftclient(self.stubs, conf['swift_store_auth_version']) self.store = Store(self.conf) conf['swift_store_auth_version'] = '2' conf['swift_store_user'] = 'tenant:user1' conf['swift_store_auth_version'] = '3' conf['swift_store_user'] = 'tenant:user1' project_domain_id='default', project_domain_name=None, user_domain_id='default', user_domain_name=None,) self.config(swift_store_auth_version='1') self.config(swift_store_auth_version='2', swift_store_user='testuser', swift_store_key='testpass', swift_store_auth_address='testaddress', swift_store_endpoint_type='internalURL',",94,169
openstack%2Frally~master~Idfa3d073e1aac1acdd64c3f0a53f60415a13daee,openstack/rally,master,Idfa3d073e1aac1acdd64c3f0a53f60415a13daee,[Reports] Use flat scenarios menu in task report,NEW,2016-11-02 17:35:15.000000000,2017-12-18 03:53:20.000000000,,"[{'_account_id': 11748}, {'_account_id': 12395}, {'_account_id': 13340}, {'_account_id': 14817}]","[{'number': 1, 'created': '2016-11-02 17:35:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/03488d374a0c035f67122d00aecb8bef215964fd', 'message': '[Reports] Use flat scenarios menu in task report\n\nSwitch from two-level (grouping by class->method)\nto single-level (just scenario name) menu on task\nresults report.\n\nThis change is made because we are on our way to use only\nnew class-based scenarios, which can have arbitrary name,\nwithout <class>.<method> parts.\n\nChange-Id: Idfa3d073e1aac1acdd64c3f0a53f60415a13daee\n'}, {'number': 2, 'created': '2016-11-03 11:13:08.000000000', 'files': ['rally/ui/templates/task/report.html', 'rally/task/processing/plot.py', 'tests/unit/task/processing/test_plot.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/fdbaf5d2e69440ce175d2c9e6dad943c139fc64f', 'message': '[Reports] Use flat scenarios menu in task report\n\nSwitch from two-level (grouping by class->method)\nto single-level (just scenario name) menu on task\nresults report.\n\nThis change is made because we are on our way to use only\nnew class-based scenarios, which can have arbitrary name,\nwithout <class>.<method> parts.\n\nChange-Id: Idfa3d073e1aac1acdd64c3f0a53f60415a13daee\n'}]",1,392897,fdbaf5d2e69440ce175d2c9e6dad943c139fc64f,12,4,2,10475,,,0,"[Reports] Use flat scenarios menu in task report

Switch from two-level (grouping by class->method)
to single-level (just scenario name) menu on task
results report.

This change is made because we are on our way to use only
new class-based scenarios, which can have arbitrary name,
without <class>.<method> parts.

Change-Id: Idfa3d073e1aac1acdd64c3f0a53f60415a13daee
",git fetch https://review.opendev.org/openstack/rally refs/changes/97/392897/1 && git format-patch -1 --stdout FETCH_HEAD,"['rally/ui/templates/task/report.html', 'rally/task/processing/plot.py', 'tests/unit/task/processing/test_plot.py']",3,03488d374a0c035f67122d00aecb8bef215964fd,single-level-menu-in-task-report," ""name"": ""foo_scenario""}, {""name"": ""foo_scenario"", ""name_pos"": ""foo_scenario [2]"", {""foo_scenario"": [{""runner"": {""type"": ""constant""}}]}, for i in (""a"", ""b"", ""c"", ""b"", ""b"")] {""name_pos"": ""%s%s"" % (a[""key""][""name""], b)}) {""a"": [""kw_a""], ""b"": [""kw_b"", ""kw_b"", ""kw_b""], ""c"": [""kw_c""]}, tasks, [{""name_pos"": ""a0""}, {""name_pos"": ""b0""}, {""name_pos"": ""b1""}, {""name_pos"": ""b2""}, {""name_pos"": ""c0""}])"," ""name"": ""Foo.bar"", ""pos"": 0}, {""cls"": ""Foo"", ""met"": ""bar"", ""name"": ""bar [2]"", ""pos"": ""1"", {""Foo.bar"": [{""runner"": {""type"": ""constant""}}]}, for i in (""a"", ""b"", ""c"", ""b"")] {""cls"": ""%s_cls"" % a[""key""][""name""], ""name"": str(b), ""met"": ""dummy"", ""pos"": str(b)}) {""a"": [""kw_a""], ""b"": [""kw_b"", ""kw_b""], ""c"": [""kw_c""]}, tasks, [{""cls"": ""a_cls"", ""met"": ""dummy"", ""name"": ""0"", ""pos"": ""0""}, {""cls"": ""b_cls"", ""met"": ""dummy"", ""name"": ""0"", ""pos"": ""0""}, {""cls"": ""b_cls"", ""met"": ""dummy"", ""name"": ""1"", ""pos"": ""1""}, {""cls"": ""c_cls"", ""met"": ""dummy"", ""name"": ""0"", ""pos"": ""0""}])",51,82
openstack%2Ftacker~master~I3a47298c5a46d8942ed52664619bfeadad7f076c,openstack/tacker,master,I3a47298c5a46d8942ed52664619bfeadad7f076c,Remove unnecessary setUp function in testcase,NEW,2016-09-21 05:37:57.000000000,2017-12-18 03:52:34.000000000,,"[{'_account_id': 10487}, {'_account_id': 18955}, {'_account_id': 22132}]","[{'number': 1, 'created': '2016-09-21 05:37:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/427e6d53e3041dc402c81853dea0776a207b0ba9', 'message': 'Remove unnecessary setUp function in testcase\n\nIn testcase, setUp will be called automatically. This patch used to\nremove setUp functions that do nothing. Besides, it will keep code clean.\n\nChange-Id: I3a47298c5a46d8942ed52664619bfeadad7f076c\n'}, {'number': 2, 'created': '2016-12-09 12:12:34.000000000', 'files': ['tacker/tests/unit/test_api_v2.py', 'tacker/tests/unit/vm/test_toscautils.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/14808d360efa0822dbe4370397be45963d02b241', 'message': 'Remove unnecessary setUp function in testcase\n\nIn testcase, setUp will be called automatically. This patch used to\nremove setUp functions that do nothing. Besides, it will keep code clean.\n\nChange-Id: I3a47298c5a46d8942ed52664619bfeadad7f076c\n'}]",0,373661,14808d360efa0822dbe4370397be45963d02b241,11,3,2,22132,,,0,"Remove unnecessary setUp function in testcase

In testcase, setUp will be called automatically. This patch used to
remove setUp functions that do nothing. Besides, it will keep code clean.

Change-Id: I3a47298c5a46d8942ed52664619bfeadad7f076c
",git fetch https://review.opendev.org/openstack/tacker refs/changes/61/373661/1 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/tests/unit/test_api_v2.py', 'tacker/tests/unit/vm/test_toscautils.py']",2,427e6d53e3041dc402c81853dea0776a207b0ba9,rm_setup,," def setUp(self): super(TestToscaUtils, self).setUp() ",0,6
openstack%2Fkolla-ansible~master~I38661a0bc2163a7f72febd98b7ae6f51c5d45ad5,openstack/kolla-ansible,master,I38661a0bc2163a7f72febd98b7ae6f51c5d45ad5,Enable heat-api proxy header parsing,MERGED,2017-03-01 09:00:51.000000000,2017-12-18 03:52:15.000000000,2017-03-20 16:38:34.000000000,"[{'_account_id': 3}, {'_account_id': 1390}, {'_account_id': 19316}, {'_account_id': 21927}]","[{'number': 1, 'created': '2017-03-01 09:00:51.000000000', 'files': ['ansible/roles/heat/templates/heat.conf.j2'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/63e5c444dda43958843f7729056a254708e92002', 'message': 'Enable heat-api proxy header parsing\n\nheat-api kept redirecting clients to use http:// instead of https://\nwhen communicating with our https:// only loadbalancer\n\nPlease examine the logic for enabling it carefully, it\'s hard to know\nif it should be enabled or not, potenitially it could be a security\nrisk.\n\nBased on openstack-ansible-os_heat:\ncommit 4033a0f854cba6719c61812ef5b553e932a6c6c2\nAuthor: Kyle L. Henderson <kyleh@us.ibm.com>\n\n    Enable oslo_middleware proxy header parsing\n\n""Heat has moved to using oslo_middleware for the http proxy header\nparsing, however the default is to not parse the headers.  When\nthe external protocol differs from the internal protocol this\nparsing is required in order for heat to work properly since it\nwill return 302 redirects to the client during some operations\n(such as delete stack).\n\nAn example of this is when using haproxy with https configured\nfor the external protocol and http for the internal protocol.\nIf the oslo_middleware does not parse the headers, then any\n302 redirects would specify a url with http rather than\ncorrectly specifying https and the heat client would fail to\nconnect on the redirect url.""\n\nChange-Id: I38661a0bc2163a7f72febd98b7ae6f51c5d45ad5\n'}]",0,439468,63e5c444dda43958843f7729056a254708e92002,11,4,1,11221,,,0,"Enable heat-api proxy header parsing

heat-api kept redirecting clients to use http:// instead of https://
when communicating with our https:// only loadbalancer

Please examine the logic for enabling it carefully, it's hard to know
if it should be enabled or not, potenitially it could be a security
risk.

Based on openstack-ansible-os_heat:
commit 4033a0f854cba6719c61812ef5b553e932a6c6c2
Author: Kyle L. Henderson <kyleh@us.ibm.com>

    Enable oslo_middleware proxy header parsing

""Heat has moved to using oslo_middleware for the http proxy header
parsing, however the default is to not parse the headers.  When
the external protocol differs from the internal protocol this
parsing is required in order for heat to work properly since it
will return 302 redirects to the client during some operations
(such as delete stack).

An example of this is when using haproxy with https configured
for the external protocol and http for the internal protocol.
If the oslo_middleware does not parse the headers, then any
302 redirects would specify a url with http rather than
correctly specifying https and the heat client would fail to
connect on the redirect url.""

Change-Id: I38661a0bc2163a7f72febd98b7ae6f51c5d45ad5
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/68/439468/1 && git format-patch -1 --stdout FETCH_HEAD,['ansible/roles/heat/templates/heat.conf.j2'],1,63e5c444dda43958843f7729056a254708e92002,, {% if public_protocol != internal_protocol and kolla_external_fqdn != kolla_internal_fqdn %} [oslo_middleware] enable_proxy_headers_parsing = True {% endif %},,5,0
openstack%2Frally~master~I850afa1e32cc2753250609a9e6fff047f2a97f0f,openstack/rally,master,I850afa1e32cc2753250609a9e6fff047f2a97f0f,[WIP] Move OPTs in stand-alone module,NEW,2017-03-29 09:37:36.000000000,2017-12-18 03:52:06.000000000,,[{'_account_id': 14817}],"[{'number': 1, 'created': '2017-03-29 09:37:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/282d36bfc80766ada33040cf94b792022d516cec', 'message': '[WIP] Move OPTs in stand-alone module: nova, magnum, vm\n\nChange-Id: I850afa1e32cc2753250609a9e6fff047f2a97f0f\n'}, {'number': 2, 'created': '2017-03-29 10:11:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8c707c17204a5314780df9bc30bb2bac38b8c69d', 'message': '[WIP] Move OPTs in stand-alone module: nova, magnum, vm\n\nChange-Id: I850afa1e32cc2753250609a9e6fff047f2a97f0f\n'}, {'number': 3, 'created': '2017-03-29 11:50:08.000000000', 'files': ['rally/plugins/openstack/scenarios/ec2/utils.py', 'rally/common/opts.py', 'rally/plugins/openstack/verification/tempest/config.py', 'rally/plugins/openstack/context/glance/images.py', 'rally/plugins/openstack/scenarios/nova/utils.py', 'rally/plugins/openstack/verification/tempest/context.py', 'rally/plugins/openstack/scenarios/cinder/utils.py', 'rally/plugins/openstack/cleanup/resources.py', 'rally/plugins/openstack/scenarios/vm/utils.py', 'rally/plugins/openstack/scenarios/glance/utils.py', 'rally/plugins/openstack/scenarios/magnum/utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/e0b55c718fdef7515c0d67e7424aac3adc83a838', 'message': '[WIP] Move OPTs in stand-alone module\n\nInclude patches for: nova, magnum, vm, cleanup, glance,\ncinder, ec2, tempest\n\nChange-Id: I850afa1e32cc2753250609a9e6fff047f2a97f0f\n'}]",0,451290,e0b55c718fdef7515c0d67e7424aac3adc83a838,8,1,3,23094,,,0,"[WIP] Move OPTs in stand-alone module

Include patches for: nova, magnum, vm, cleanup, glance,
cinder, ec2, tempest

Change-Id: I850afa1e32cc2753250609a9e6fff047f2a97f0f
",git fetch https://review.opendev.org/openstack/rally refs/changes/90/451290/1 && git format-patch -1 --stdout FETCH_HEAD,"['rally/common/opts.py', 'rally/plugins/openstack/scenarios/nova/utils.py', 'rally/plugins/openstack/scenarios/magnum/utils.py']",3,282d36bfc80766ada33040cf94b792022d516cec,astarove-config-opts,"from rally.common import opts # MAGNUM_BENCHMARK_OPTS = [ # cfg.FloatOpt(""magnum_cluster_create_prepoll_delay"", # default=5.0, # help=""Time(in sec) to sleep after creating a # resource before "" # ""polling for the status.""), # cfg.FloatOpt(""magnum_cluster_create_timeout"", # default=1200.0, # help=""Time(in sec) to wait for magnum cluster to be "" # ""created.""), # cfg.FloatOpt(""magnum_cluster_create_poll_interval"", # default=1.0, # help=""Time interval(in sec) between checks when # waiting for "" # ""cluster creation.""), # ] # # CONF = cfg.CONF # benchmark_group = cfg.OptGroup(name=""benchmark"", title=""benchmark options"") # CONF.register_opts(MAGNUM_BENCHMARK_OPTS, group=benchmark_group) CONF = opts.get_opts(""magnum"")","MAGNUM_BENCHMARK_OPTS = [ cfg.FloatOpt(""magnum_cluster_create_prepoll_delay"", default=5.0, help=""Time(in sec) to sleep after creating a resource before "" ""polling for the status.""), cfg.FloatOpt(""magnum_cluster_create_timeout"", default=1200.0, help=""Time(in sec) to wait for magnum cluster to be "" ""created.""), cfg.FloatOpt(""magnum_cluster_create_poll_interval"", default=1.0, help=""Time interval(in sec) between checks when waiting for "" ""cluster creation.""), ] CONF = cfg.CONF benchmark_group = cfg.OptGroup(name=""benchmark"", title=""benchmark options"") CONF.register_opts(MAGNUM_BENCHMARK_OPTS, group=benchmark_group)",369,88
openstack%2Fswift~master~I4ae16789ccddaa005315b090f6f69e803edca966,openstack/swift,master,I4ae16789ccddaa005315b090f6f69e803edca966,Change Automated Tiering's derivative container to hidden accounts,NEW,2017-03-22 11:56:03.000000000,2017-12-18 03:51:44.000000000,,"[{'_account_id': 9625}, {'_account_id': 13052}, {'_account_id': 13852}]","[{'number': 1, 'created': '2017-03-22 11:56:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/d6882eda3b72642da8d663730ec390654292454c', 'message': 'Change Automated Tiering\'s derivative container to hidden accounts\n\nIn current implementation, automated tiering daemons make derivative\ncontainers in end-users\' account.\n\nBut this behavior causes the following problems:\n    1. Derivative container names can conflict with end-user\'s container names\n    2. Derivative containers are listed in result of GET account,\n       even though end-user doesn\'t make the containers\n    3. End-users can update objects in derivative containers,\n       even though they should be updated only by automated tiering daemons\n\nThis patch changes daemons to make derivative containers in hidden account.\nIf end-user\'s account name is \'acc\', hidden account \'.tiered_acc\' will be used.\n\nThis patch changes behavior of GET container response too.\nObject records in response of GET container request for tiered container\nwill have ""bytes"" and ""hash"" of entity object, even though the object is\nreplaced with link object.\n\nChange-Id: I4ae16789ccddaa005315b090f6f69e803edca966\n'}, {'number': 2, 'created': '2017-03-30 01:40:06.000000000', 'files': ['swift/common/middleware/automated_tiering.py', 'swift/container/server.py', 'swift/common/utils.py', 'etc/object-deliverer.conf-sample', 'swift/obj/addresser.py', 'etc/object-server.conf-sample'], 'web_link': 'https://opendev.org/openstack/swift/commit/0045510e2265a8478bf7dfa34fdb723e7adc1cd2', 'message': 'Change Automated Tiering\'s derivative container to hidden accounts\n\nIn current implementation, automated tiering daemons make derivative\ncontainers in end-users\' account.\n\nBut this behavior causes the following problems:\n    1. Derivative container names can conflict with end-user\'s container names\n    2. Derivative containers are listed in result of GET account,\n       even though end-user doesn\'t make the containers\n    3. End-users can update objects in derivative containers,\n       even though they should be updated only by automated tiering daemons\n\nThis patch changes daemons to make derivative containers in hidden account.\nIf end-user\'s account name is \'acc\', hidden account \'.tiered_acc\' will be used.\n\nThis patch changes behavior of GET container response too.\nObject records in response of GET container request for tiered container\nwill have ""bytes"" and ""hash"" of entity object, even though the object is\nreplaced with link object.\n\nChange-Id: I4ae16789ccddaa005315b090f6f69e803edca966\n'}]",0,448525,0045510e2265a8478bf7dfa34fdb723e7adc1cd2,8,3,2,14766,,,0,"Change Automated Tiering's derivative container to hidden accounts

In current implementation, automated tiering daemons make derivative
containers in end-users' account.

But this behavior causes the following problems:
    1. Derivative container names can conflict with end-user's container names
    2. Derivative containers are listed in result of GET account,
       even though end-user doesn't make the containers
    3. End-users can update objects in derivative containers,
       even though they should be updated only by automated tiering daemons

This patch changes daemons to make derivative containers in hidden account.
If end-user's account name is 'acc', hidden account '.tiered_acc' will be used.

This patch changes behavior of GET container response too.
Object records in response of GET container request for tiered container
will have ""bytes"" and ""hash"" of entity object, even though the object is
replaced with link object.

Change-Id: I4ae16789ccddaa005315b090f6f69e803edca966
",git fetch https://review.opendev.org/openstack/swift refs/changes/25/448525/2 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/middleware/automated_tiering.py', 'swift/container/server.py', 'swift/common/utils.py', 'etc/object-deliverer.conf-sample', 'swift/obj/addresser.py', 'etc/object-server.conf-sample']",6,d6882eda3b72642da8d663730ec390654292454c,automated-tiering,,derivative_container_prefix = DERIVATIVE_,193,100
openstack%2Frally~master~I690113117604325debda9afe29bd45a1ab949f9c,openstack/rally,master,I690113117604325debda9afe29bd45a1ab949f9c,Add name prefix argument to boot servers,NEW,2017-01-16 13:38:47.000000000,2017-12-18 03:51:34.000000000,,"[{'_account_id': 12395}, {'_account_id': 14817}]","[{'number': 1, 'created': '2017-01-16 13:38:47.000000000', 'files': ['rally/plugins/openstack/scenarios/nova/utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/40a584fa8b957c3ea3a167e6110c60ae069f0eb8', 'message': 'Add name prefix argument to boot servers\n\nAdd ability to pass name_prefix to boot_servers\nthat allows exteral code know exact names of servers\n\nChange-Id: I690113117604325debda9afe29bd45a1ab949f9c\n'}]",1,420720,40a584fa8b957c3ea3a167e6110c60ae069f0eb8,6,2,1,12712,,,0,"Add name prefix argument to boot servers

Add ability to pass name_prefix to boot_servers
that allows exteral code know exact names of servers

Change-Id: I690113117604325debda9afe29bd45a1ab949f9c
",git fetch https://review.opendev.org/openstack/rally refs/changes/20/420720/1 && git format-patch -1 --stdout FETCH_HEAD,['rally/plugins/openstack/scenarios/nova/utils.py'],1,40a584fa8b957c3ea3a167e6110c60ae069f0eb8,," auto_assign_nic=False, name_prefix=None, **kwargs): if name_prefix is None: name_prefix = self.generate_random_name() "," auto_assign_nic=False, **kwargs): name_prefix = self.generate_random_name()",4,2
openstack%2Fbifrost~master~I1db9f2e2612b313eef8b5784eed33bfe5150965c,openstack/bifrost,master,I1db9f2e2612b313eef8b5784eed33bfe5150965c,Do not overrride enable_venv by local lookup,NEW,2017-03-29 11:49:04.000000000,2017-12-18 03:51:24.000000000,,"[{'_account_id': 9542}, {'_account_id': 22474}]","[{'number': 1, 'created': '2017-03-29 11:49:04.000000000', 'files': ['playbooks/roles/bifrost-ironic-install/defaults/main.yml', 'playbooks/roles/bifrost-ironic-install/tasks/bootstrap.yml', 'playbooks/roles/bifrost-ironic-install/tasks/install.yml', 'playbooks/roles/bifrost-keystone-install/tasks/install.yml', 'playbooks/roles/bifrost-keystone-install/tasks/bootstrap.yml', 'playbooks/roles/bifrost-keystone-install/defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/d771c4cad8e7666c9bc2a09fa19f3ec49fd1d446', 'message': 'Do not overrride enable_venv by local lookup\n\ncurrently bifrost overrides the enable_venv var passed into playbooks\nwhen ansible itself is running in a venv.\n\nThese two should be independent, especially when running playbooks\nagainst a remote host.\n\nThis patch stops using local env lookups to set enable_venv var,\nand require the enable_venv var to always be passed explicitly to playbooks\nwhen installation of ironic and others in venv is desired.\nAdditionaly, the bifrost_venv_dir can be passed in to customize the\nlocation of the venv (/opt/stack/bifrost by default).\n\nOur test scripts already pass the enable_venv var to playbooks\nexplicitly.\n\nChange-Id: I1db9f2e2612b313eef8b5784eed33bfe5150965c\n'}]",0,451351,d771c4cad8e7666c9bc2a09fa19f3ec49fd1d446,11,2,1,9542,,,0,"Do not overrride enable_venv by local lookup

currently bifrost overrides the enable_venv var passed into playbooks
when ansible itself is running in a venv.

These two should be independent, especially when running playbooks
against a remote host.

This patch stops using local env lookups to set enable_venv var,
and require the enable_venv var to always be passed explicitly to playbooks
when installation of ironic and others in venv is desired.
Additionaly, the bifrost_venv_dir can be passed in to customize the
location of the venv (/opt/stack/bifrost by default).

Our test scripts already pass the enable_venv var to playbooks
explicitly.

Change-Id: I1db9f2e2612b313eef8b5784eed33bfe5150965c
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/51/451351/1 && git format-patch -1 --stdout FETCH_HEAD,"['playbooks/roles/bifrost-ironic-install/defaults/main.yml', 'playbooks/roles/bifrost-ironic-install/tasks/bootstrap.yml', 'playbooks/roles/bifrost-ironic-install/tasks/install.yml', 'playbooks/roles/bifrost-keystone-install/tasks/install.yml', 'playbooks/roles/bifrost-keystone-install/tasks/bootstrap.yml', 'playbooks/roles/bifrost-keystone-install/defaults/main.yml']",6,d771c4cad8e7666c9bc2a09fa19f3ec49fd1d446,no-venv-override,bifrost_venv_dir: /opt/stack/bifrost,"bifrost_venv_dir: ""{{ lookup('env', 'VENV') | default('/opt/stack/bifrost') }}""",2,23
openstack%2Ftacker~master~I47f46b4cbdd847281844c376db2fb9040d4c29e8,openstack/tacker,master,I47f46b4cbdd847281844c376db2fb9040d4c29e8,allow to boot vdu with block mapping,NEW,2017-01-18 16:09:46.000000000,2017-12-18 03:51:05.000000000,,"[{'_account_id': 2874}, {'_account_id': 12455}, {'_account_id': 13380}, {'_account_id': 18955}, {'_account_id': 24785}]","[{'number': 1, 'created': '2017-01-18 16:09:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/79705290e76f639f4653e246380bd05bcdc8be17', 'message': 'allow to boot vdu with block mapping\n\nChange-Id: I47f46b4cbdd847281844c376db2fb9040d4c29e8\nCloses-bug: #1649459\n'}, {'number': 2, 'created': '2017-01-18 16:18:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/5c397fa10ea526dc4120f644e4711bd969d75c16', 'message': 'allow to boot vdu with block mapping\n\nDepends-On: I3e3d62f4ad60fdb4ceddd46b70386d4623367c51\nChange-Id: I47f46b4cbdd847281844c376db2fb9040d4c29e8\nPartial-bug: #1649459\n'}, {'number': 3, 'created': '2017-01-19 01:32:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/8c3f0a127f3b439eeacb8eb5f6bb7fb3aa8bbc3f', 'message': 'allow to boot vdu with block mapping\n\nDepends-On: I3e3d62f4ad60fdb4ceddd46b70386d4623367c51\nChange-Id: I47f46b4cbdd847281844c376db2fb9040d4c29e8\nPartial-bug: #1649459\n'}, {'number': 4, 'created': '2017-01-19 01:37:28.000000000', 'files': ['tacker/vnfm/tosca/lib/tacker_nfv_defs.yaml', 'releasenotes/notes/boot-VDU-with-block-mapping-f4cc772599249547.yaml', 'samples/tosca-templates/vnfd/tosca-vnfd-block-device-mappingv2.yaml', 'tacker/vnfm/tosca/lib/tacker_defs.yaml'], 'web_link': 'https://opendev.org/openstack/tacker/commit/c6898617815a223f46b82f1cd8cf194f80922296', 'message': 'allow to boot vdu with block mapping\n\nDepends-On: I3e3d62f4ad60fdb4ceddd46b70386d4623367c51\nChange-Id: I47f46b4cbdd847281844c376db2fb9040d4c29e8\nPartial-bug: #1649459\n'}]",4,422066,c6898617815a223f46b82f1cd8cf194f80922296,20,5,4,2874,,,0,"allow to boot vdu with block mapping

Depends-On: I3e3d62f4ad60fdb4ceddd46b70386d4623367c51
Change-Id: I47f46b4cbdd847281844c376db2fb9040d4c29e8
Partial-bug: #1649459
",git fetch https://review.opendev.org/openstack/tacker refs/changes/66/422066/4 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/vnfm/tosca/lib/tacker_nfv_defs.yaml', 'samples/tosca-templates/vnfd/tosca-vnfd-block-device-mappingv2.yaml', 'tacker/vnfm/tosca/lib/tacker_defs.yaml']",3,79705290e76f639f4653e246380bd05bcdc8be17,bug/1649459, tosca.datatypes.tacker.BlockMapping: properties: volume_id: type: string required: true device_type: type: string required: true boot_index: type: integer required: false disk_bus: type: string required: false device_name: type: string required: false,,76,0
openstack%2Ftacker~master~Ib551c93dcc20e5b91ccc544c941339fe2dd7b681,openstack/tacker,master,Ib551c93dcc20e5b91ccc544c941339fe2dd7b681,Wrong value set when passing param to VNFD,NEW,2017-01-09 06:15:20.000000000,2017-12-18 03:51:03.000000000,,"[{'_account_id': 2874}, {'_account_id': 10487}, {'_account_id': 12455}, {'_account_id': 13380}, {'_account_id': 15950}, {'_account_id': 16034}, {'_account_id': 18955}, {'_account_id': 19339}, {'_account_id': 20560}]","[{'number': 1, 'created': '2017-01-09 06:15:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/e23e987eee40ac98c37f8cae8d9f811908806c8d', 'message': 'Wrong value set when passing param to VNFD\n\nIn a VNFD template where the template is parameterized according to:\n\ninputs:\n  connection-type:\n    type string\n\nand\n\nCP2:\n  type: {get_input : connection-type}\n\nThe same template without the parameter looks like:\n\nCP2:\n  type: sriov\n\nHowever when deploying the VNF ""sriov"" is not allowed as a value for\n""connection-type"" but the Heat binding.vnic_type values\n(direct,normal, ...) are suggested.\n\nSolution: Checking the value of param in type, if \'sriov\' found,\nreplace \'get_param\' to it to the string \'direct\'\n\nChange-Id: Ib551c93dcc20e5b91ccc544c941339fe2dd7b681\ncloses-bug: #1643576\n'}, {'number': 2, 'created': '2017-01-11 06:00:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/bbc939e18f1e7855b7cd79ff64c81939834b6310', 'message': 'Wrong value set when passing param to VNFD\n\nIn a VNFD template where the template is parameterized according to:\n\ninputs:\n  connection-type:\n    type string\n\nand\n\nCP2:\n  type: {get_input : connection-type}\n\nThe same template without the parameter looks like:\n\nCP2:\n  type: sriov\n\nHowever when deploying the VNF ""sriov"" is not allowed as a value for\n""connection-type"" but the Heat binding.vnic_type values\n(direct,normal, ...) are suggested.\n\nSolution: Checking the value of param in type, if \'sriov\' found,\nreplace \'get_param\' to it to the string \'direct\'\n\nChange-Id: Ib551c93dcc20e5b91ccc544c941339fe2dd7b681\ncloses-bug: #1643576\n'}, {'number': 3, 'created': '2017-01-16 06:28:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/21e1b2f78d9723107e0472c4b75b017487180e2e', 'message': 'Wrong value set when passing param to VNFD\n\nIn a VNFD template where the template is parameterized according to:\n\ninputs:\n  connection-type:\n    type string\n\nand\n\nCP2:\n  type: {get_input : connection-type}\n\nThe same template without the parameter looks like:\n\nCP2:\n  type: sriov\n\nHowever when deploying the VNF ""sriov"" is not allowed as a value for\n""connection-type"" but the Heat binding.vnic_type values\n(direct,normal, ...) are suggested.\n\nSolution: Checking the value of param in type, if \'sriov\' found,\nreplace \'get_param\' to it to the string \'direct\'\n\nChange-Id: Ib551c93dcc20e5b91ccc544c941339fe2dd7b681\ncloses-bug: #1643576\n'}, {'number': 4, 'created': '2017-02-14 19:43:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/cff83ee98ec677965a57c2410c66aa27588e2df5', 'message': 'Wrong value set when passing param to VNFD\n\nIn a VNFD template where the template is parameterized according to:\n\ninputs:\n  connection-type:\n    type string\n\nand\n\nCP2:\n  type: {get_input : connection-type}\n\nThe same template without the parameter looks like:\n\nCP2:\n  type: sriov\n\nHowever when deploying the VNF ""sriov"" is not allowed as a value for\n""connection-type"" but the Heat binding.vnic_type values\n(direct,normal, ...) are suggested.\n\nSolution: Checking the value of param in type, if \'sriov\' found,\nreplace \'get_param\' to it to the string \'direct\'\n\nChange-Id: Ib551c93dcc20e5b91ccc544c941339fe2dd7b681\ncloses-bug: #1643576\nCo-Authored-By: Dharmendra Kushwaha<dharmendra.kushwaha@nectechnologies.in>\n'}, {'number': 5, 'created': '2017-03-27 07:30:04.000000000', 'files': ['tacker/tests/unit/vnfm/infra_drivers/openstack/data/tosca_vnic_param.yaml', 'tacker/tests/unit/vnfm/infra_drivers/openstack/test_openstack.py', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/hot_tosca_vnic_param.yaml', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/hot_tosca_sriov_param.yaml', 'tacker/vnfm/tosca/utils.py', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/tosca_sriov_param.yaml'], 'web_link': 'https://opendev.org/openstack/tacker/commit/cb5684c8acb660418f39864a6f76408baa47fc2e', 'message': 'Wrong value set when passing param to VNFD\n\nIn a VNFD template where the template is parameterized according to:\n\ninputs:\n  connection-type:\n    type string\n\nand\n\nCP2:\n  type: {get_input : connection-type}\n\nThe same template without the parameter looks like:\n\nCP2:\n  type: sriov\n\nHowever when deploying the VNF ""sriov"" is not allowed as a value for\n""connection-type"" but the Heat binding.vnic_type values\n(direct,normal, ...) are suggested.\n\nSolution: Checking the value of param in type, if \'sriov\' found,\nreplace \'get_param\' to it to the string \'direct\'\n\nChange-Id: Ib551c93dcc20e5b91ccc544c941339fe2dd7b681\ncloses-bug: #1643576\nCo-Authored-By: Dharmendra Kushwaha<dharmendra.kushwaha@nectechnologies.in>\n'}]",6,417752,cb5684c8acb660418f39864a6f76408baa47fc2e,31,9,5,15950,,,0,"Wrong value set when passing param to VNFD

In a VNFD template where the template is parameterized according to:

inputs:
  connection-type:
    type string

and

CP2:
  type: {get_input : connection-type}

The same template without the parameter looks like:

CP2:
  type: sriov

However when deploying the VNF ""sriov"" is not allowed as a value for
""connection-type"" but the Heat binding.vnic_type values
(direct,normal, ...) are suggested.

Solution: Checking the value of param in type, if 'sriov' found,
replace 'get_param' to it to the string 'direct'

Change-Id: Ib551c93dcc20e5b91ccc544c941339fe2dd7b681
closes-bug: #1643576
Co-Authored-By: Dharmendra Kushwaha<dharmendra.kushwaha@nectechnologies.in>
",git fetch https://review.opendev.org/openstack/tacker refs/changes/52/417752/1 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/tests/unit/vm/infra_drivers/openstack/data/hot_tosca_sriov_param.yaml', 'tacker/tests/unit/vm/infra_drivers/heat/test_heat.py', 'tacker/tests/unit/vm/infra_drivers/openstack/test_openstack.py', 'tacker/tests/unit/vm/infra_drivers/openstack/data/tosca_sriov_param.yaml', 'tacker/vnfm/tosca/utils.py']",5,e23e987eee40ac98c37f8cae8d9f811908806c8d,(detached,"from toscaparser import functions if prop == p.name: if isinstance(p.value, functions.GetInput): tmpVal = p.value.result() else: tmpVal = p.value if tmpVal in convert_prop_values[nt.type][prop].keys(): v = convert_prop_values[nt.type][prop][tmpVal] p.value = v", if (prop == p.name and p.value in convert_prop_values[nt.type][prop].keys()): v = convert_prop_values[nt.type][prop][p.value] p.value = v,126,5
openstack%2Ftacker~master~Icf4b817ad8199f55c7880fe1e04f869998b5e3b6,openstack/tacker,master,Icf4b817ad8199f55c7880fe1e04f869998b5e3b6,Update policy definition in tacker-def.yaml,NEW,2016-08-03 08:19:35.000000000,2017-12-18 03:51:00.000000000,,[{'_account_id': 16511}],"[{'number': 1, 'created': '2016-08-03 08:19:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/07b75d1ee6356dd12546437a230f45c789c15182', 'message': 'Update policy definition in tacker-def.yaml\n\nSome policies are defined with ""action"" atrribtute in tacker-defs.yaml.\nWhen using tosca-parser for these policies, validation will be failed.\n\nChange-Id: Icf4b817ad8199f55c7880fe1e04f869998b5e3b6\nCloses-bug: #1609284\n'}, {'number': 2, 'created': '2016-08-31 11:12:06.000000000', 'files': ['tacker/vnfm/tosca/lib/tacker_defs.yaml'], 'web_link': 'https://opendev.org/openstack/tacker/commit/c8809d9b8117d2a6a32fe07413a6d3d8cb09077d', 'message': 'Update policy definition in tacker-def.yaml\n\nSome policies are defined with ""action"" atrribtute in tacker-defs.yaml.\nWhen using tosca-parser for these policies, validation will be failed.\n\nChange-Id: Icf4b817ad8199f55c7880fe1e04f869998b5e3b6\nCloses-bug: #1609284\n'}]",0,350436,c8809d9b8117d2a6a32fe07413a6d3d8cb09077d,7,1,2,20560,,,0,"Update policy definition in tacker-def.yaml

Some policies are defined with ""action"" atrribtute in tacker-defs.yaml.
When using tosca-parser for these policies, validation will be failed.

Change-Id: Icf4b817ad8199f55c7880fe1e04f869998b5e3b6
Closes-bug: #1609284
",git fetch https://review.opendev.org/openstack/tacker refs/changes/36/350436/1 && git format-patch -1 --stdout FETCH_HEAD,['tacker/vm/tosca/lib/tacker_defs.yaml'],1,07b75d1ee6356dd12546437a230f45c789c15182,bug/1609284, derived_from: tosca.policies.Placement description: respawn description: log_and_kill description: log, derived_from: tosca.policies.Root action: type: string action: respawn action: log_and_kill action: log,4,6
openstack%2Ftacker~master~I934083b2615494828430d5297012850f623ecd89,openstack/tacker,master,I934083b2615494828430d5297012850f623ecd89,Mistral workflow for vnf life cycle,NEW,2016-08-04 15:16:38.000000000,2017-12-18 03:50:55.000000000,,"[{'_account_id': 2874}, {'_account_id': 13380}, {'_account_id': 13485}, {'_account_id': 19999}]","[{'number': 1, 'created': '2016-08-04 15:16:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/a02d5b5bfed0208cc0163542806042fd3f24d4f6', 'message': 'Mistral workflow for vnf life cycle\n\nA Mistral workflow for vnf life cycle. i.e.\nvnfd create--> vnf create--> vnf active--> vnf delete--> vnfd delete\n\nChange-Id: I934083b2615494828430d5297012850f623ecd89\n'}, {'number': 2, 'created': '2016-09-14 15:38:59.000000000', 'files': ['samples/mistral/workflows/vnf_lifecycle.yaml', 'samples/mistral/workflows/input/create_vnfd.json'], 'web_link': 'https://opendev.org/openstack/tacker/commit/d1c9be8d15efb18cbf995370b426f9d8d542456b', 'message': 'Mistral workflow for vnf life cycle\n\nA Mistral workflow for vnf life cycle. i.e.\nvnfd create--> vnf create--> vnf active--> vnf delete--> vnfd delete\n\nChange-Id: I934083b2615494828430d5297012850f623ecd89\n'}]",0,351242,d1c9be8d15efb18cbf995370b426f9d8d542456b,8,4,2,18955,,,0,"Mistral workflow for vnf life cycle

A Mistral workflow for vnf life cycle. i.e.
vnfd create--> vnf create--> vnf active--> vnf delete--> vnfd delete

Change-Id: I934083b2615494828430d5297012850f623ecd89
",git fetch https://review.opendev.org/openstack/tacker refs/changes/42/351242/1 && git format-patch -1 --stdout FETCH_HEAD,"['samples/mistral/workflows/vnf_lifecycle.yaml', 'samples/mistral/workflows/input/create_vnfd.json']",2,a02d5b5bfed0208cc0163542806042fd3f24d4f6,,"{ ""body"":{ ""vnfd"":{ ""attributes"":{ ""vnfd"":""tosca_definitions_version: tosca_simple_profile_for_nfv_1_0_0\n\ndescription: Demo example\n\nmetadata:\n template_name: sample-tosca-vnfd\n\ntopology_template:\n node_templates:\n VDU1:\n type: tosca.nodes.nfv.VDU.Tacker\n properties:\n image: cirros-0.3.4-x86_64-uec\n flavor: m1.tiny\n availability_zone: nova\n mgmt_driver: noop\n config: |\n param0: key1\n param1: key2\n\n CP1:\n type: tosca.nodes.nfv.CP.Tacker\n properties:\n management: true\n anti_spoofing_protection: false\n requirements:\n - virtualLink:\n node: VL1\n - virtualBinding:\n node: VDU1\n\n CP2:\n type: tosca.nodes.nfv.CP.Tacker\n properties:\n anti_spoofing_protection: false\n requirements:\n - virtualLink:\n node: VL2\n - virtualBinding:\n node: VDU1\n\n CP3:\n type: tosca.nodes.nfv.CP.Tacker\n properties:\n anti_spoofing_protection: false\n requirements:\n - virtualLink:\n node: VL3\n - virtualBinding:\n node: VDU1\n\n VL1:\n type: tosca.nodes.nfv.VL\n properties:\n network_name: net_mgmt\n vendor: Tacker\n\n VL2:\n type: tosca.nodes.nfv.VL\n properties:\n network_name: net0\n vendor: Tacker\n\n VL3:\n type: tosca.nodes.nfv.VL\n properties:\n network_name: net1\n vendor: Tacker\n"" }, ""name"":""tacker-create-vnfd"" } } } ",,111,0
openstack%2Fswift~master~I538ab17319ce1422bf57949eacc4d7afd61db51e,openstack/swift,master,I538ab17319ce1422bf57949eacc4d7afd61db51e,Sharding: use container backends _record_to_dict to simplify _generate_object_list,NEW,2017-04-04 07:02:33.000000000,2017-12-18 03:50:24.000000000,,"[{'_account_id': 7233}, {'_account_id': 13052}]","[{'number': 1, 'created': '2017-04-04 07:02:33.000000000', 'files': ['swift/container/server.py', 'test/unit/container/test_sharder.py', 'swift/container/sharder.py', 'swift/container/backend.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/e82336065992e00661897f9da592752fb1797f1a', 'message': ""Sharding: use container backends _record_to_dict to simplify _generate_object_list\n\nThis change also adds a list_full_objects_dict_iter has added 2 things:\n 1. uses _record_to_dict as the row_transform_func that is passed to\n    _list_object_iter\n 2. passes a new flag 'full' to _iter_objects_iter that will include all\n    elements in the table being returned (in this case adds\n    storage_policy_index) as we can use _record_to_dict.\n\nIt also changes the pivot constaint RECORD_TYPE_PIVOT_NODE to\nRECORD_TYPE_PIVOT so it matches the naming of the object record type.\n\nChange-Id: I538ab17319ce1422bf57949eacc4d7afd61db51e\n""}]",1,453032,e82336065992e00661897f9da592752fb1797f1a,5,2,1,7233,,,0,"Sharding: use container backends _record_to_dict to simplify _generate_object_list

This change also adds a list_full_objects_dict_iter has added 2 things:
 1. uses _record_to_dict as the row_transform_func that is passed to
    _list_object_iter
 2. passes a new flag 'full' to _iter_objects_iter that will include all
    elements in the table being returned (in this case adds
    storage_policy_index) as we can use _record_to_dict.

It also changes the pivot constaint RECORD_TYPE_PIVOT_NODE to
RECORD_TYPE_PIVOT so it matches the naming of the object record type.

Change-Id: I538ab17319ce1422bf57949eacc4d7afd61db51e
",git fetch https://review.opendev.org/openstack/swift refs/changes/32/453032/1 && git format-patch -1 --stdout FETCH_HEAD,"['swift/container/server.py', 'test/unit/container/test_sharder.py', 'swift/container/sharder.py', 'swift/container/backend.py']",4,e82336065992e00661897f9da592752fb1797f1a,sharding-generate-obj-list,"RECORD_TYPE_PIVOT = 1 if record_type and record_type == RECORD_TYPE_PIVOT: if record['record_type'] == RECORD_TYPE_PIVOT: if record_type == RECORD_TYPE_PIVOT: full=False, row_transform_func=None): :param full: Include other fields not normally returned (like storage_policy_index if full: query += ', storage_policy_index ' def list_full_objects_dict_iter(self, *args, **kwargs): kwargs.setdefault('row_transform_func', ContainerBroker._record_to_dict) kwargs.setdefault('full', True) return self._list_objects_iter(*args, **kwargs) @staticmethod def _record_to_dict(rec, record_type=RECORD_TYPE_OBJECT): if item['record_type'] == RECORD_TYPE_PIVOT] record_type = RECORD_TYPE_PIVOT existing = ContainerBroker._record_to_dict( records.get(item_ident), record_type=record_type) 'record_type': RECORD_TYPE_PIVOT}) [ContainerBroker._record_to_dict(list(r) + [0], RECORD_TYPE_PIVOT)","RECORD_TYPE_PIVOT_NODE = 1 if record_type and record_type == RECORD_TYPE_PIVOT_NODE: if record['record_type'] == RECORD_TYPE_PIVOT_NODE: if record_type == RECORD_TYPE_PIVOT_NODE: row_transform_func=None): def _record_to_dict(self, rec, record_type=RECORD_TYPE_OBJECT): if item['record_type'] == RECORD_TYPE_PIVOT_NODE] record_type = RECORD_TYPE_PIVOT_NODE existing = self._record_to_dict(records.get(item_ident), record_type=record_type) 'record_type': RECORD_TYPE_PIVOT_NODE}) [self._record_to_dict(list(r) + [0], RECORD_TYPE_PIVOT_NODE)",200,69
openstack%2Fdevstack-gate~master~Ia020e791ef717e28fcba4a9fccbe4bfa9570ba9a,openstack/devstack-gate,master,Ia020e791ef717e28fcba4a9fccbe4bfa9570ba9a,Remove the EBTABLES_RACE_FIX added for Trusty,NEW,2017-03-27 08:28:39.000000000,2017-12-18 03:50:21.000000000,,"[{'_account_id': 7069}, {'_account_id': 7118}, {'_account_id': 7350}, {'_account_id': 19339}]","[{'number': 1, 'created': '2017-03-27 08:28:39.000000000', 'files': ['devstack-vm-gate.sh'], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/9468ad17b78a844ff2d2e71e473a7c0a5e12fb99', 'message': ""Remove the EBTABLES_RACE_FIX added for Trusty\n\nNow that we don't support Ubuntu Trusty anymore,\nall testing platforms have libvirt >= 1.2.11 so we can\nremove the ebtables race workaround.\n\nChange-Id: Ia020e791ef717e28fcba4a9fccbe4bfa9570ba9a\n""}]",0,450120,9468ad17b78a844ff2d2e71e473a7c0a5e12fb99,16,4,1,7350,,,0,"Remove the EBTABLES_RACE_FIX added for Trusty

Now that we don't support Ubuntu Trusty anymore,
all testing platforms have libvirt >= 1.2.11 so we can
remove the ebtables race workaround.

Change-Id: Ia020e791ef717e28fcba4a9fccbe4bfa9570ba9a
",git fetch https://review.opendev.org/openstack/devstack-gate refs/changes/20/450120/1 && git format-patch -1 --stdout FETCH_HEAD,['devstack-vm-gate.sh'],1,9468ad17b78a844ff2d2e71e473a7c0a5e12fb99,,," # set this until all testing platforms have libvirt >= 1.2.11 # see bug #1501558 localrc_set ""$localrc_file"" ""EBTABLES_RACE_FIX"" ""True""",0,3
openstack%2Fmonasca-agent~master~If049db17eb18e063d7e6b3c4de07ed7a32a86122,openstack/monasca-agent,master,If049db17eb18e063d7e6b3c4de07ed7a32a86122,Enhance CPU plugin,NEW,2016-11-23 15:18:05.000000000,2017-12-18 03:50:08.000000000,,"[{'_account_id': 2419}, {'_account_id': 11155}, {'_account_id': 11809}, {'_account_id': 15027}, {'_account_id': 16168}, {'_account_id': 18179}, {'_account_id': 24322}]","[{'number': 1, 'created': '2016-11-23 15:18:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/c9329fb0ca942d619aa75a0b362ea343e247efc2', 'message': 'Enhance CPU plugin\n* add frequency_multiplier to display correct Y axis in Grafana\n* separate guest cpu counters from user as those are interesting on HV\n* add detailed_stats option to have all CPU counters sent to Monasca\n\nChange-Id: If049db17eb18e063d7e6b3c4de07ed7a32a86122\n'}, {'number': 2, 'created': '2016-11-23 15:23:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/072c9475e20040d7a535e7550363e000f91d48d3', 'message': 'Enhance CPU plugin\n\n* add frequency_multiplier to display correct Y axis in Grafana\n* separate guest cpu counters from user as those are interesting on HV\n* add detailed_stats option to have all CPU counters sent to Monasca\n\nChange-Id: If049db17eb18e063d7e6b3c4de07ed7a32a86122\n'}, {'number': 3, 'created': '2016-11-23 16:31:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/9759d0cb44bdd792f9238a1006da7b51c0df8f9c', 'message': 'Enhance CPU plugin\n\n* add frequency_multiplier to display correct Y axis in Grafana\n* separate guest cpu counters from user as those are interesting on HV\n* add detailed_stats option to have all CPU counters sent to Monasca\n\nChange-Id: If049db17eb18e063d7e6b3c4de07ed7a32a86122\n'}, {'number': 4, 'created': '2016-11-25 13:11:42.000000000', 'files': ['monasca_agent/collector/checks_d/cpu.py', 'conf.d/cpu.yaml', 'docs/Plugins.md'], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/2af3f37180957d440e123709a105c921eb6ad582', 'message': 'Enhance CPU plugin\n\n* add frequency_multiplier to display correct Y axis in Grafana\n* separate guest cpu counters from user as those are interesting on HV\n* add detailed_stats option to have all CPU counters sent to Monasca\n\nChange-Id: If049db17eb18e063d7e6b3c4de07ed7a32a86122\n'}]",5,401308,2af3f37180957d440e123709a105c921eb6ad582,17,7,4,24322,,,0,"Enhance CPU plugin

* add frequency_multiplier to display correct Y axis in Grafana
* separate guest cpu counters from user as those are interesting on HV
* add detailed_stats option to have all CPU counters sent to Monasca

Change-Id: If049db17eb18e063d7e6b3c4de07ed7a32a86122
",git fetch https://review.opendev.org/openstack/monasca-agent refs/changes/08/401308/1 && git format-patch -1 --stdout FETCH_HEAD,['monasca_agent/collector/checks_d/cpu.py'],1,c9329fb0ca942d619aa75a0b362ea343e247efc2,401308," if not instance.get(""detailed_stats"", False): data = {'cpu.user_perc': cpu_stats.user + cpu_stats.nice, 'cpu.system_perc': cpu_stats.system + cpu_stats.irq + cpu_stats.softirq, 'cpu.wait_perc': cpu_stats.iowait, 'cpu.idle_perc': cpu_stats.idle, 'cpu.stolen_perc': cpu_stats.steal, 'cpu.percent': cpu_perc, 'cpu.idle_time': cpu_times.idle, 'cpu.wait_time': cpu_times.iowait, 'cpu.user_time': cpu_times.user + cpu_times.nice, 'cpu.system_time': cpu_times.system + cpu_times.irq + cpu_times.softirq, 'cpu.guest_time': cpu_times.guest + cpu_timest.guest_nice } if 'guest' in cpu_stats._fields and 'guest_nice' in cpu_stats._fields: data['cpu.guest_perc'] = cpu_stats.guest + cpu_stats.guest_nice data['cpu.user_perc'] = cpu_stats.user + cpu_stats.nice - cpu_stats.guest - cpu_stats.guest_nice data['cpu.guest_time'] = cpu_times.guest + cpu_times.guest_nice data['cpu.user_time'] = cpu_times.user + cpu_times.nice - cpu_times.guest - cpu_times.guest_nice else: data = {'cpu.user_perc': cpu_stats.user, 'cpu.nice_perc': cpu_stats.nice, 'cpu.system_perc': cpu_stats.system, 'cpu.irq_perc': cpu_stats.irq, 'cpu.softirq_perc': cpu_stats.softirq, 'cpu.wait_perc': cpu_stats.iowait, 'cpu.idle_perc': cpu_stats.idle, 'cpu.stolen_perc': cpu_stats.steal, 'cpu.percent': cpu_perc, 'cpu.user_time': cpu_times.user, 'cpu.nice_time': cpu_times.nice, 'cpu.system_time': cpu_times.system, 'cpu.irq_time': cpu_times.irq, 'cpu.softirq_time': cpu_times.softirq, 'cpu.wait_time': cpu_times.iowait, 'cpu.idle_time': cpu_times.idle, 'cpu.stolen_time': cpu_times.steal } if 'guest' in cpu_stats._fields and 'guest_nice' in cpu_stats._fields: data['cpu.guest_perc'] = cpu_stats.guest data['cpu.guest_nice_perc'] = cpu_stats.guest_nice data['cpu.user_perc'] = cpu_stats.user - cpu_stats.guest data['cpu.nice_perc'] = cpu_stats.nice - cpu_stats.guest_nice data['cpu.guest_time'] = cpu_times.guest data['cpu.guest_nice_time'] = cpu_times.guest_nice data['cpu.user_time'] = cpu_times.user - cpu_times.guest data['cpu.nice_time'] = cpu_times.nice - cpu_times.guest_nice self._add_cpu_freq(data,instance) def _add_cpu_freq(self, data, instance): data['cpu.frequency_mhz'] = cpu_freq * instance.get('frequency_multiplier', 1)"," data = {'cpu.user_perc': cpu_stats.user + cpu_stats.nice, 'cpu.system_perc': cpu_stats.system + cpu_stats.irq + cpu_stats.softirq, 'cpu.wait_perc': cpu_stats.iowait, 'cpu.idle_perc': cpu_stats.idle, 'cpu.stolen_perc': cpu_stats.steal, 'cpu.percent': cpu_perc, 'cpu.idle_time': cpu_times.idle, 'cpu.wait_time': cpu_times.iowait, 'cpu.user_time': cpu_times.user + cpu_times.nice, 'cpu.system_time': cpu_times.system + cpu_times.irq + cpu_times.softirq} self._add_cpu_freq(data) def _add_cpu_freq(self, data): data['cpu.frequency_mhz'] = cpu_freq",49,13
openstack%2Fmonasca-agent~master~I013153084017858c0edecccb9d87ab1a2c388d49,openstack/monasca-agent,master,I013153084017858c0edecccb9d87ab1a2c388d49,Revise SolidFire Monasca Check Plugin,NEW,2017-03-20 18:14:58.000000000,2017-12-18 03:50:06.000000000,,"[{'_account_id': 16168}, {'_account_id': 16627}]","[{'number': 1, 'created': '2017-03-20 18:14:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/35613668189daa4baea1a2640878c544b17ac2e9', 'message': 'Revise SolidFire Monasca Check Plugin\n\nThis second revision to the SolidFire Monasca plugin adds a bunch of metrics,\nsome additional performance logging, and the option to opt-out of intense\nmetric gathering. Users can now log general cluster health, cluster capacity\nand performance, and drive/node/volume performance.\n\nIt is recommended that volume monitor and possibly drive monitoring be disabled\non start installs. A third revision is in the works that will throttle back and\nchunk requests to avoid several second in duration API responses.\n\nChange-Id: I013153084017858c0edecccb9d87ab1a2c388d49\n'}, {'number': 2, 'created': '2017-03-20 20:06:45.000000000', 'files': ['conf.d/solidfire.yaml.example', 'docs/Plugins.md', 'monasca_agent/collector/checks_d/solidfire.py'], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/314eaedb2908cfcb47bf2188e54d57957217a3fd', 'message': 'Revise SolidFire Monasca Check Plugin\n\nThis second revision to the SolidFire Monasca plugin adds a bunch of metrics,\nsome additional performance logging, and the option to opt-out of intense\nmetric gathering. Users can now log general cluster health, cluster capacity\nand performance, and drive/node/volume performance.\n\nIt is recommended that volume monitor and possibly drive monitoring be disabled\non start installs. A third revision is in the works that will throttle back and\nchunk requests to avoid several second in duration API responses.\n\nChange-Id: I013153084017858c0edecccb9d87ab1a2c388d49\n'}]",8,447632,314eaedb2908cfcb47bf2188e54d57957217a3fd,9,2,2,16627,,,0,"Revise SolidFire Monasca Check Plugin

This second revision to the SolidFire Monasca plugin adds a bunch of metrics,
some additional performance logging, and the option to opt-out of intense
metric gathering. Users can now log general cluster health, cluster capacity
and performance, and drive/node/volume performance.

It is recommended that volume monitor and possibly drive monitoring be disabled
on start installs. A third revision is in the works that will throttle back and
chunk requests to avoid several second in duration API responses.

Change-Id: I013153084017858c0edecccb9d87ab1a2c388d49
",git fetch https://review.opendev.org/openstack/monasca-agent refs/changes/32/447632/2 && git format-patch -1 --stdout FETCH_HEAD,"['conf.d/solidfire.yaml.example', 'docs/Plugins.md', 'monasca_agent/collector/checks_d/solidfire.py']",3,35613668189daa4baea1a2640878c544b17ac2e9,monasca-solidfire-rev2,"import copydef timing(func): def stopwatch(*args, **kwargs): then = time.time() tmp = func(*args, **kwargs) delta = '%.2f' % ((time.time() - then)*1000) LOG.debug(""Function %s:%sms"" % (func.func_name, delta)) return tmp return stopwatch mock = {'metric_name': None, 'api_call': None, 'result_field': None, 'metric_type': None} cluster_stat_metrics = [ {'metric_name': 'solidfire.cluster.utilization', 'api_call': 'GetClusterStats', 'result_field': 'clusterUtilization', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.read_bytes', 'api_call': 'GetClusterStats', 'result_field': 'readBytes', 'metric_type': 'rate'}, {'metric_name': 'solidfire.cluster.write_bytes', 'api_call': 'GetClusterStats', 'result_field': 'writeBytes', 'metric_type': 'rate'}, {'metric_name': 'solidfire.cluster.read_ops', 'api_call': 'GetClusterStats', 'result_field': 'readOps', 'metric_type': 'rate'}, {'metric_name': 'solidfire.cluster.write_ops', 'api_call': 'GetClusterStats', 'result_field': 'writeOps', 'metric_type': 'rate'}] cluster_capacity_metrics = [ {'metric_name': 'solidfire.cluster.capacity.num_active_iscsi_sessions', 'api_call': 'GetClusterCapacity', 'result_field': 'activeSessions', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.active_block_bytes', 'api_call': 'GetClusterCapacity', 'result_field': 'activeBlockSpace', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.iops.avg_utc', 'api_call': 'GetClusterCapacity', 'result_field': 'averageIOPS', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.iops.avg_io_size', 'api_call': 'GetClusterCapacity', 'result_field': 'clusterRecentIOSIze', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.iops.avg_5_sec', 'api_call': 'GetClusterCapacity', 'result_field': 'currentIOPS', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.iops.max_available', 'api_call': 'GetClusterCapacity', 'result_field': 'maxIOPS', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.max_overprovisioned_bytes', 'api_call': 'GetClusterCapacity', 'result_field': 'maxOverProvisionableSpace', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.max_provisioned_bytes', 'api_call': 'GetClusterCapacity', 'result_field': 'maxProvisionableSpace', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.max_meta_bytes', 'api_call': 'GetClusterCapacity', 'result_field': 'maxUsedMetadataSpace', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.max_block_bytes', 'api_call': 'GetClusterCapacity', 'result_field': 'maxUsedSpace', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.non_zero_blocks', 'api_call': 'GetClusterCapacity', 'result_field': 'nonZeroBlocks', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.num_peak_iscsi_sessions', 'api_call': 'GetClusterCapacity', 'result_field': 'peakActiveSessions', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.iops.peak_utc', 'api_call': 'GetClusterCapacity', 'result_field': 'peakIOPS', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.provisioned_bytes', 'api_call': 'GetClusterCapacity', 'result_field': 'provisionedSpace', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.iops.total_ops', 'api_call': 'GetClusterCapacity', 'result_field': 'totalOps', 'metric_type': 'rate'}, {'metric_name': 'solidfire.cluster.capacity.unique_blocks', 'api_call': 'GetClusterCapacity', 'result_field': 'uniqueBlocks', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.unique_blocks_used_bytes', 'api_call': 'GetClusterCapacity', 'result_field': 'uniqueBlocksUsedSpace', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.active_meta_bytes', 'api_call': 'GetClusterCapacity', 'result_field': 'usedMetadataSpace', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.active_snapshot_bytes', 'api_call': 'GetClusterCapacity', 'result_field': 'usedMetadataSpaceinSnapshots', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.active_block_bytes', 'api_call': 'GetClusterCapacity', 'result_field': 'usedSpace', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.cluster.capacity.zero_blocks', 'api_call': 'GetClusterCapacity', 'result_field': 'zeroBlocks', 'metric_type': 'gauge'}] node_stat_metrics = [ {'metric_name': 'solidfire.node.cpu_usage', 'api_call': 'ListNodeStats', 'result_field': 'cpu', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.node.cluster_interface_bytes_recv', 'api_call': 'ListNodeStats', 'result_field': 'cBytesIn', 'metric_type': 'rate'}, {'metric_name': 'solidfire.node.cluster_interface_bytes_tx', 'api_call': 'ListNodeStats', 'result_field': 'cBytesOut', 'metric_type': 'rate'}, {'metric_name': 'solidfire.node.storage_interface_bytes_recv', 'api_call': 'ListNodeStats', 'result_field': 'sBytesIn', 'metric_type': 'rate'}, {'metric_name': 'solidfire.node.storage_interface_bytes_tx', 'api_call': 'ListNodeStats', 'result_field': 'sBytesOut', 'metric_type': 'rate'}, {'metric_name': 'solidfire.node.management_interface_bytes_recv', 'api_call': 'ListNodeStats', 'result_field': 'mBytesIn', 'metric_type': 'rate'}, {'metric_name': 'solidfire.node.management_interface_bytes_tx', 'api_call': 'ListNodeStats', 'result_field': 'mBytesOut', 'metric_type': 'rate'}, {'metric_name': 'solidfire.node.cluster_interface_network_utilization', 'api_call': 'ListNodeStats', 'result_field': 'networkUtilizationCluster', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.node.storage_interface_network_utilization', 'api_call': 'ListNodeStats', 'result_field': 'networkUtilizationStorage', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.node.used_memory', 'api_call': 'ListNodeStats', 'result_field': 'usedMemory', 'metric_type': 'gauge'}] drive_stat_metrics = [ {'metric_name': 'solidfire.drive.failed_die_count', 'api_call': 'GetDriveStats', 'result_field': 'failedDieCount', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.drive.remaining_life', 'api_call': 'GetDriveStats', 'result_field': 'lifeRemainingPercent', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.drive.lifetime_read_bytes', 'api_call': 'GetDriveStats', 'result_field': 'lifetimeReadBytes', 'metric_type': 'rate'}, {'metric_name': 'solidfire.drive.lifetime_write_bytes', 'api_call': 'GetDriveStats', 'result_field': 'lifetimeWriteBytes', 'metric_type': 'rate'}, {'metric_name': 'solidfire.drive.power_on_hours', 'api_call': 'GetDriveStats', 'result_field': 'powerOnHours', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.drive.reallocated_sectors', 'api_call': 'GetDriveStats', 'result_field': 'reallocatedSectors', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.drive.reserve_capacity_percent', 'api_call': 'GetDriveStats', 'result_field': 'reserveCapacityPercent', 'metric_type': 'gauge'}] volume_stat_metrics = [ {'metric_name': 'solidfire.volume.iops.current', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'actualIOPS', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.volume.iops.average_size_bytes', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'averageIOPSize', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.volume.qos.burst_credit', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'burstIOPSCredit', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.volume.iops.queue_depth', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'clientQueueDepth', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.volume.iops.latency_microsecond', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'latencyUSec', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.volume.iops.read_latency_microsecond', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'readLatencyUSec', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.volume.iops.utilization', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'volumeUtilization', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.volume.iops.write_latency_microsecond', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'writeLatencyUSec', 'metric_type': 'gauge'}, {'metric_name': 'solidfire.volume.iops.read_bytes', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'readBytes', 'metric_type': 'rate'}, {'metric_name': 'solidfire.volume.iops.write_bytes', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'writeBytes', 'metric_type': 'rate'}, {'metric_name': 'solidfire.volume.iops.read_rate', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'readOps', 'metric_type': 'rate'}, {'metric_name': 'solidfire.volume.iops.write_rate', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'writeOps', 'metric_type': 'rate'}, {'metric_name': 'solidfire.volume.qos.throttle', 'api_call': 'ListVolumeStatsByVolume', 'result_field': 'Throttle', 'metric_type': 'gauge'}] self.enabled_metrics = [{'name': 'GetClusterCapacity', 'method': '_get_cluster_capacity', 'metrics': self.cluster_capacity_metrics}, {'name': 'ListNodeStats', 'method': '_list_node_stats', 'metrics': self.node_stat_metrics}, {'name': 'GetDriveStats', 'method': '_get_drive_stats', 'metrics': self.drive_stat_metrics}, {'name': 'GetClusterStats', 'method': '_get_cluster_stats', 'metrics': self.cluster_stat_metrics}, {'name': 'GetVolumeStats', 'method': '_get_volume_stats', 'metrics': self.volume_stat_metrics}] @timing self.dimensions = {'service': 'solidfire', 'cluster': self.cluster} self._filter_metrics(instance) processed_metrics = [] # Iterate over enabled metrics # Real talk, we spend a lot of time waiting on responses. Good use case # for threading. for metric in self.enabled_metrics: if hasattr(self, metric.get('method')): result = getattr(self, metric.get('method'))(metric['metrics']) if result: LOG.debug(""%s: Raw Metrics: %s Processed Metrics: %s"" % (metric.get('name'), len(metric.get('metrics')), len(result))) processed_metrics.extend(result) else: LOG.error(""%s is not a valid metric category."" % metric.get('method')) for metric in processed_metrics: if metric.get('value'): num_metrics += 1 if metric.get('metric_type') == 'rate': self.rate(metric.get('metric_name'), metric.get('value'), metric.get('dimensions')) elif metric.get('metric_type') == 'gauge': self.gauge(metric.get('metric_name'), metric.get('value'), metric.get('dimensions')) LOG.debug(""Collected %s metrics"" % (num_metrics)) def _filter_metrics(self, instance): """"""Some metrics, like GetVolumeStats and GetDriveStats are incredibly expensive. For now we're enabling people to simply opt out of collecting this data. Next revision will include more intelligent metric gathering. """""" excluded_checks = instance.get('excluded_checks') if excluded_checks: LOG.debug(""Excluded checks: %s"" % excluded_checks) filtered_metrics = [entry for entry in self.enabled_metrics if entry.get('name') not in excluded_checks] self.enabled_metrics = filtered_metrics LOG.debug(""Filtered metrics: %s"" % ([val.get('name') for val in filtered_metrics])) @timing def _get_cluster_stats(self, metrics): processed_metrics = [] #TODO: Is it more sane to simply copy metrics upfront? What is more # efficient? for metric in metrics: cp = copy.copy(metric) tmp = res.get(cp.get('result_field')) cp['value'] = tmp cp['dimensions'] = self.dimensions processed_metrics.append(cp) return processed_metrics @timing def _get_cluster_capacity(self, metrics): processed_metrics = [] for metric in metrics: # It might be faster to copy the entire metrics array than # individual metrics. cp = copy.copy(metric) tmp = res.get(cp.get('result_field')) cp['value'] = tmp cp['dimensions'] = self.dimensions processed_metrics.append(cp) # Retrieve cluster space efficiency factors. eff_factors = self._get_cluster_capacity_helper(processed_metrics) processed_metrics.extend(eff_factors) return processed_metrics @timing def _get_cluster_capacity_helper(self, data): # Calculate capacity utilization factors from cluster capacity data. nzb = 'solidfire.cluster.capacity.non_zero_blocks' zb = 'solidfire.cluster.capacity.zero_blocks' ub = 'solidfire.cluster.capacity.unique_blocks' ubs = 'solidfire.cluster.capacity.unique_blocks_used_bytes' non_zero_blocks = next((x['value'] for x in data if x['metric_name'] == nzb), None) zero_blocks = next((x['value'] for x in data if x['metric_name'] == zb), None) unique_blocks = next((x['value'] for x in data if x['metric_name'] == ub), None) unique_blocks_space = next((x['value'] for x in data if x['metric_name'] == ubs), None) # Generate stubs to populate with factor metrics tmp = copy.copy(self.mock) tmp['api_call'] = 'GetClusterCapacity' tmp['metric_type'] = 'gauge' tmp['dimensions'] = self.dimensions stubs = [copy.copy(tmp) for val in range(4)] # Populate stubs with factors stubs[0]['metric_name'] = ( 'solidfire.cluster.capacity.thin_provision_factor') stubs[0]['value'] = thin_factor stubs[1]['metric_name'] = ( 'solidfire.cluster.capacity.deduplication_factor') stubs[1]['value'] = dedup_factor stubs[2]['metric_name'] = ( 'solidfire.cluster.capacity.compression_factor') stubs[2]['value'] = comp_factor stubs[3]['metric_name'] = ( 'solidfire.cluster.capacity.data_reduction_factor') stubs[3]['value'] = eff_factor return stubs @timing def _list_node_stats(self, metrics): res = (self.sf.issue_api_request('ListNodeStats', {}, '8.0') ['result']['nodeStats']['nodes']) processed_metrics = [] for node in res: node_dimensions = copy.copy(self.dimensions) node_dimensions['node_id'] = str(node.get('nodeID')) for metric in metrics: cp = copy.copy(metric) tmp = node.get(cp.get('result_field')) cp['value'] = tmp cp['dimensions'] = node_dimensions processed_metrics.append(cp) return processed_metrics def _drive_id_lookup(self, drive_list, serial): # Because we're dumb as a sack of rocks and don't include the very # helpful drive_id in the ListDriveHardware response. return next((str(drive['driveID']) for drive in drive_list if serial in drive['serial']), None) @timing def _get_drive_stats(self, metrics): # TODO: This was slow in testing and you don't need high resolution. # Rate limit this to something configurable with a reasonable default. drive_id_list = (self.sf.issue_api_request('ListDrives', {}, '8.0') ['result']['drives']) nodes = self.sf.issue_api_request('ListDriveHardware', {'force': True}, '8.0')['result']['nodes'] processed_metrics = [] for node in nodes: drives = node['result']['driveHardware'] # Some SolidFire node models create multiple partitions on a # physical drive. This yields multiple 'drive' objects with the # same serial number in the ListDriveHardware response. Filter by uniques = {d['serial']:d for d in drives}.values() for drive in uniques: drive_id = self._drive_id_lookup(drive_id_list, drive.get('serial')) if not drive_id: # Unfortunately ListDriveHardware returns NVME devices # which do not report usable drive wear stats. We filter # these devices out by matching them to the results of # ListDrives, which only returns meta/block service drives. # _drive_id_lookup will return None if it cannot find a # match, indicating a NVME drive. continue drive_dimensions = copy.copy(self.dimensions) drive_dimensions['drive_id'] = drive_id drive_dimensions['serial'] = drive.get('serial') drive_dimensions['node_id'] = str(node['nodeID']) for metric in metrics: metric_cp = copy.copy(metric) tmp = drive.get(metric_cp.get('result_field')) metric_cp['value'] = tmp # Should be fine using the same dimension instance across # all metrics for a drive metric_cp['dimensions'] = drive_dimensions processed_metrics.append(metric_cp) return processed_metrics @timing def _get_volume_stats(self, metrics): #TODO: This is NOT scalable. This should never be used in production. # Chunk volume stats into a manageable quantity so as to not timeout. vol_list = (self.sf.issue_api_request('ListVolumeStatsByVolume', {}, '8.0')['result']['volumeStats']) processed_metrics = [] for vol in vol_list: vol_dimensions = copy.copy(self.dimensions) vol_dimensions['vol_id'] = str(vol['volumeID']) for metric in metrics: metric_cp = copy.copy(metric) tmp = vol.get(metric_cp.get('result_field')) metric_cp['value'] = tmp metric_cp['dimensions'] = vol_dimensions processed_metrics.append(metric_cp) return processed_metrics data = {'solidfire.cluster.active_cluster_faults': len(res)} @timing"," dimensions = {'service': 'solidfire', 'cluster': self.cluster} data = {} # Query cluster for stats data.update(self._get_cluster_stats()) # Query for active cluster faults. data.update(self._list_cluster_faults()) # Query for cluster capacity info data.update(self._get_cluster_capacity()) for key, value in data.iteritems(): if data[key] is None: continue self.gauge(key, value, dimensions) num_metrics += 1 LOG.debug('Collected %s metrics' % (num_metrics)) def _get_cluster_stats(self): # Cluster utilization is the overall load. data = {'solidfire.cluster_utilization': res['clusterUtilization']} return data def _get_cluster_capacity(self): # Number of 4KiB blocks with data after the last garbage collection non_zero_blocks = res['nonZeroBlocks'] # Number of 4KiB blocks without data after the last garbage collection zero_blocks = res['zeroBlocks'] # Number of blocks(not always 4KiB) stored on block drives. unique_blocks = res['uniqueBlocks'] # Amount of space the unique blocks take on the block drives. unique_blocks_space = res['uniqueBlocksUsedSpace'] # Amount of space consumed by the block services, including cruft. active_block_space = res['activeBlockSpace'] # Maximum amount of bytes allocated to the block services. max_block_space = res['maxUsedSpace'] # Amount of space consumed by the metadata services. active_slice_space = res['usedMetadataSpace'] # Amount of space consumed by the metadata services for snapshots. active_snap_space = res['usedMetadataSpaceInSnapshots'] # Maximum amount of bytes allocated to the metadata services. max_slice_space = res['maxUsedMetadataSpace'] # Volume provisioned space prov_space = res['provisionedSpace'] # Max provisionable space if 100% metadata space used. max_prov_space = res['maxProvisionedSpace'] # Overprovision limit. max_overprov_space = res['maxOverProvisionableSpace'] # Number of active iSCSI sessions. iscsi_sessions = res['activeSessions'] # Average IOPS since midnight UTC. avg_iops = res['averageIOPS'] # Peak IOPS since midnight UTC. peak_iops = res['peakIOPS'] # Current IOPs over the last 5 seconds. current_iops = res['currentIOPS'] # Theoretical max IOPS max_iops = res['maxIOPS'] # Single-node clusters can report zero values for some divisors. data = {'solidfire.num_iscsi_sessions': iscsi_sessions, 'solidfire.iops.avg_utc': avg_iops, 'solidfire.iops.peak_utc': peak_iops, 'solidfire.iops.avg_5_sec': current_iops, 'solidfire.iops.max_available': max_iops, 'solidfire.provisioned_bytes': prov_space, 'solidfire.max_provisioned_bytes': max_prov_space, 'solidfire.max_overprovisioned_bytes': max_overprov_space, 'solidfire.max_block_bytes': max_block_space, 'solidfire.active_block_bytes': active_block_space, 'solidfire.max_meta_bytes': max_slice_space, 'solidfire.active_meta_bytes': active_slice_space, 'solidfire.active_snapshot_bytes': active_snap_space, 'solidfire.non_zero_blocks': non_zero_blocks, 'solidfire.zero_blocks': zero_blocks, 'solidfire.unique_blocks': unique_blocks, 'solidfire.unique_blocks_used_bytes': unique_blocks_space, 'solidfire.thin_provision_factor': thin_factor, 'solidfire.deduplication_factor': dedup_factor, 'solidfire.compression_factor': comp_factor, 'solidfire.data_reduction_factor': eff_factor } return data data = {'solidfire.active_cluster_faults': len(res)}",538,104
openstack%2Fpython-tackerclient~master~Id58ce2c69d1c58ea0b4b11f5154d610b475272ac,openstack/python-tackerclient,master,Id58ce2c69d1c58ea0b4b11f5154d610b475272ac,Introduce tacker vnfd-template-validate command.,NEW,2016-06-24 11:07:30.000000000,2017-12-18 03:49:44.000000000,,"[{'_account_id': 2874}, {'_account_id': 10487}, {'_account_id': 12455}, {'_account_id': 13380}, {'_account_id': 13485}, {'_account_id': 16034}, {'_account_id': 18401}, {'_account_id': 18955}, {'_account_id': 20986}, {'_account_id': 22061}]","[{'number': 1, 'created': '2016-06-24 11:07:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/458e4f2ece46713af202ab6f14a827a91c0e53fa', 'message': 'Introduce tacker vnfd-template-validate command.\n\nAdding a new command for exclusively validating the Tosca VNFD templates.\nPartial-Bug: #1483953\n\nChange-Id: Id58ce2c69d1c58ea0b4b11f5154d610b475272ac\n'}, {'number': 2, 'created': '2016-06-27 07:35:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/30a98cd7dd937c89df201ba12de7b9edd2df84d9', 'message': 'Introduce tacker vnfd-template-validate command.\n\nAdding a new command for exclusively validating the Tosca VNFD templates.\nPartial-Bug: #1483953\n\nChange-Id: Id58ce2c69d1c58ea0b4b11f5154d610b475272ac\n'}, {'number': 3, 'created': '2016-06-27 07:40:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/b77bceefdbe4877b64ceb6230c2ad3fac7436c19', 'message': 'Introduce tacker vnfd-template-validate command.\n\nAdding a new command for exclusively validating the Tosca VNFD templates.\nPartial-Bug: #1483953\n\nChange-Id: Id58ce2c69d1c58ea0b4b11f5154d610b475272ac\n'}, {'number': 4, 'created': '2016-06-28 09:58:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/93100b626cfe63869b600ac5169cc142c2be4298', 'message': 'Introduce tacker vnfd-template-validate command.\n\nAdding a new command for exclusively validating the Tosca VNFD templates.\nPartial-Bug: #1483953\nDepends-On: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n\nChange-Id: Id58ce2c69d1c58ea0b4b11f5154d610b475272ac\n'}, {'number': 5, 'created': '2016-07-04 12:42:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/1c74ee5e01b69f3a9585fe0e863fe359c6d811fb', 'message': 'Introduce tacker vnfd-template-validate command.\n\nAdding a new command for exclusively validating the Tosca VNFD templates.\nPartial-Bug: #1483953\nDepends-On: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n\nChange-Id: Id58ce2c69d1c58ea0b4b11f5154d610b475272ac\n'}, {'number': 6, 'created': '2016-07-05 05:55:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/aafc7aa3a2c7b488add331a6cba86174f7c27cb6', 'message': 'Introduce tacker vnfd-template-validate command.\n\nAdding a new command for exclusively validating the Tosca VNFD templates.\nPartial-Bug: #1483953\nDepends-On: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n\nChange-Id: Id58ce2c69d1c58ea0b4b11f5154d610b475272ac\n'}, {'number': 7, 'created': '2016-07-05 05:58:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/e6fe6e64523645f87f5a201aee5844531525088a', 'message': 'Introduce tacker vnfd-template-validate command.\n\nAdding a new command for exclusively validating the Tosca VNFD templates.\nPartial-Bug: #1483953\nDepends-On: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n\nChange-Id: Id58ce2c69d1c58ea0b4b11f5154d610b475272ac\n'}, {'number': 8, 'created': '2016-07-07 11:57:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/b2300c976e1db8a6cc01120adcebb28bc35085aa', 'message': 'Introduce tacker vnfd-template-validate command.\n\nAdding a new command to validate the TOSCA VNFD templates.\nPartial-Bug: #1483953\nDepends-On: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n\nChange-Id: Id58ce2c69d1c58ea0b4b11f5154d610b475272ac\n'}, {'number': 9, 'created': '2016-07-19 06:51:29.000000000', 'files': ['tackerclient/tacker/v1_0/__init__.py', 'tackerclient/tacker/v1_0/vm/vnfd.py', 'tackerclient/v1_0/client.py', 'tackerclient/shell.py'], 'web_link': 'https://opendev.org/openstack/python-tackerclient/commit/e1d3185cdc2dca4457aca3b699b5a39df22b4ffb', 'message': 'Introduce tacker vnfd-template-validate command.\n\nAdding a new command to validate the TOSCA VNFD templates.\nPartial-Bug: #1483953\nDepends-On: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02\n\nChange-Id: Id58ce2c69d1c58ea0b4b11f5154d610b475272ac\n'}]",25,333853,e1d3185cdc2dca4457aca3b699b5a39df22b4ffb,40,10,9,22061,,,0,"Introduce tacker vnfd-template-validate command.

Adding a new command to validate the TOSCA VNFD templates.
Partial-Bug: #1483953
Depends-On: I6822e3f7bc00e47ee2c75b0944bcb4637310fa02

Change-Id: Id58ce2c69d1c58ea0b4b11f5154d610b475272ac
",git fetch https://review.opendev.org/openstack/python-tackerclient refs/changes/53/333853/8 && git format-patch -1 --stdout FETCH_HEAD,"['tackerclient/tacker/v1_0/__init__.py', 'tackerclient/tacker/v1_0/vm/vnfd.py', 'tackerclient/v1_0/client.py', 'tackerclient/shell.py']",4,458e4f2ece46713af202ab6f14a827a91c0e53fa,template-validate," 'vnfd-template-validate': vnfd.ValidateTemplateVNFD,",,67,0
openstack%2Fkarbor~master~I1213d465ad89447ac61ba8f4ff1899152f0407d9,openstack/karbor,master,I1213d465ad89447ac61ba8f4ff1899152f0407d9,Add jsonschema validation for karbor triggers API,MERGED,2017-12-15 07:17:15.000000000,2017-12-18 03:49:39.000000000,2017-12-18 03:49:39.000000000,"[{'_account_id': 17151}, {'_account_id': 21224}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 07:17:15.000000000', 'files': ['karbor/tests/unit/api/v1/test_scheduled_operation.py', 'karbor/api/v1/triggers.py', 'karbor/tests/unit/api/v1/test_triggers.py', 'karbor/api/schemas/triggers.py'], 'web_link': 'https://opendev.org/openstack/karbor/commit/d75fba70b197f024d8a1f1ba9510477157dc1b78', 'message': 'Add jsonschema validation for karbor triggers API\n\nChange-Id: I1213d465ad89447ac61ba8f4ff1899152f0407d9\nPartial-Implements: bp karbor-json-schema-validation\n'}]",0,528176,d75fba70b197f024d8a1f1ba9510477157dc1b78,7,3,1,17151,,,0,"Add jsonschema validation for karbor triggers API

Change-Id: I1213d465ad89447ac61ba8f4ff1899152f0407d9
Partial-Implements: bp karbor-json-schema-validation
",git fetch https://review.opendev.org/openstack/karbor refs/changes/76/528176/1 && git format-patch -1 --stdout FETCH_HEAD,"['karbor/api/v1/triggers.py', 'karbor/tests/unit/api/v1/test_scheduled_operation.py', 'karbor/tests/unit/api/v1/test_triggers.py', 'karbor/api/schemas/triggers.py']",4,d75fba70b197f024d8a1f1ba9510477157dc1b78,bp/karbor-json-schema-validation,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """""" Schema for Karbor V1 Triggers API. """""" create = { 'type': 'object', 'properties': { 'type': 'object', 'trigger_info': { 'type': 'object', 'properties': { 'name': {'type': 'string'}, 'type': {'type': 'string'}, 'properties': { 'type': 'object', 'properties': { 'format': {'type': 'string'}, 'pattern': {'type': 'string'}, 'start_time': {'type': 'string'}, 'end_time': {'type': 'string'}, 'window': {'type': 'integer'}, }, 'required': ['format', 'pattern'], 'additionalProperties': False, }, }, 'required': ['name', 'type', 'properties'], 'additionalProperties': False, }, }, 'required': ['trigger_info'], 'additionalProperties': False, } update = { 'type': 'object', 'properties': { 'type': 'object', 'trigger_info': { 'type': 'object', 'properties': { 'name': {'type': 'string'}, 'type': {'type': 'string'}, 'properties': { 'type': 'object', 'properties': { 'format': {'type': 'string'}, 'pattern': {'type': 'string'}, 'start_time': {'type': 'string'}, 'end_time': {'type': 'string'}, 'window': {'type': 'integer'}, }, 'required': [], 'additionalProperties': False, }, }, 'required': [], 'additionalProperties': False, }, }, 'required': ['trigger_info'], 'additionalProperties': False, } ",,96,11
openstack%2Frally~master~Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7,openstack/rally,master,Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7,Add NeutronNetworks.create_and_list_subnetpools scenario,NEW,2016-10-14 03:09:27.000000000,2017-12-18 03:49:32.000000000,,"[{'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 18404}, {'_account_id': 21528}, {'_account_id': 22960}, {'_account_id': 23435}, {'_account_id': 23668}]","[{'number': 1, 'created': '2016-10-14 03:09:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8f25e9c35ea67138bbaec5a003cb1e0ca604c204', 'message': 'Add NeutronNetworks.create_and_list_subnetpools\n\nThe scenario creates a subnetpool and then\nlists all subnetpools.\n\nChange-Id: Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7\n'}, {'number': 2, 'created': '2016-10-18 06:40:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e181235a490a19861faadef6379b0bd4324d672c', 'message': 'Add NeutronNetworks.create_and_list_subnetpools scenario\n\nThe scenario creates a subnetpool and then lists all subnetpools.\nAlso add new neutron resource subnetpool and two atomic actions.\n\nChange-Id: Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7\n'}, {'number': 3, 'created': '2016-10-19 01:31:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d37dfe65c6340eda004cbdb3c70bbfabb09d8dd7', 'message': 'Add NeutronNetworks.create_and_list_subnetpools scenario\n\nThe scenario creates a subnetpool and then lists all subnetpools.\nAlso add new neutron resource subnetpool and two atomic actions.\n\nChange-Id: Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7\n'}, {'number': 4, 'created': '2016-11-09 09:35:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/f4bfae59e81e573eafa6cdeb885f66593b4b737b', 'message': 'Add NeutronNetworks.create_and_list_subnetpools scenario\n\nThe scenario creates a subnetpool and then lists all subnetpools.\nAlso add new neutron resource subnetpool and two atomic actions.\n\nChange-Id: Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7\n'}, {'number': 5, 'created': '2016-11-24 09:55:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/42b07e6adf663bff3ea44dbe1ba926136818de58', 'message': 'Add NeutronNetworks.create_and_list_subnetpools scenario\n\nThe scenario creates a subnetpool and then lists all subnetpools.\nAlso add new neutron resource subnetpool and two atomic actions.\n\nChange-Id: Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7\n'}, {'number': 6, 'created': '2016-12-07 05:37:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/84b4dc3c8ea037fe877024d1b8e5f2de41455c97', 'message': 'Add NeutronNetworks.create_and_list_subnetpools scenario\n\nThe scenario creates a subnetpool and then lists all subnetpools.\nAlso add new neutron resource subnetpool and two atomic actions.\n\nChange-Id: Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7\n'}, {'number': 7, 'created': '2017-03-03 01:56:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/128a4d926ddf998fadac9e310049f6f1e4ac52ab', 'message': 'Add NeutronNetworks.create_and_list_subnetpools scenario\n\nThe scenario creates a subnetpool and then lists all subnetpools.\nAlso add new neutron resource subnetpool and two atomic actions.\n\nChange-Id: Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7\n'}, {'number': 8, 'created': '2017-03-07 07:31:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5e8a8066b41ed54071b92351e700f491fd585a4f', 'message': 'Add NeutronNetworks.create_and_list_subnetpools scenario\n\nThe scenario creates a subnetpool and then lists all subnetpools.\nAlso add new neutron resource subnetpool and two atomic actions.\n\nChange-Id: Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7\n'}, {'number': 9, 'created': '2017-03-30 08:45:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d5beebc410ddbb3440887b06368da62b0d266521', 'message': 'Add NeutronNetworks.create_and_list_subnetpools scenario\n\nThe scenario creates a subnetpool and then lists all subnetpools.\nAlso add new neutron resource subnetpool and two atomic actions.\n\nChange-Id: Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7\n'}, {'number': 10, 'created': '2017-03-30 09:45:46.000000000', 'files': ['tests/ci/osresources.py', 'tests/unit/plugins/openstack/scenarios/neutron/test_network.py', 'samples/tasks/scenarios/neutron/create-and-list-subnetpools.yaml', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'rally/plugins/openstack/cleanup/resources.py', 'rally-jobs/rally-neutron.yaml', 'samples/tasks/scenarios/neutron/create-and-list-subnetpools.json', 'tests/unit/plugins/openstack/scenarios/neutron/test_utils.py', 'rally/plugins/openstack/scenarios/neutron/network.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/7667c280c7ba3bdcf8861fa2e59af780fee4c8de', 'message': 'Add NeutronNetworks.create_and_list_subnetpools scenario\n\nThe scenario creates a subnetpool and then lists all subnetpools.\nAlso add new neutron resource subnetpool and two atomic actions.\n\nCo-Authored-By: zhangzhang <zhangzh.fnst@cn.fujitsu.com>\n\nChange-Id: Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7\n'}]",20,386317,7667c280c7ba3bdcf8861fa2e59af780fee4c8de,51,7,10,23668,,,0,"Add NeutronNetworks.create_and_list_subnetpools scenario

The scenario creates a subnetpool and then lists all subnetpools.
Also add new neutron resource subnetpool and two atomic actions.

Co-Authored-By: zhangzhang <zhangzh.fnst@cn.fujitsu.com>

Change-Id: Ieb7380366c895ab023ed24cf57ffdcbb81db9ea7
",git fetch https://review.opendev.org/openstack/rally refs/changes/17/386317/8 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/plugins/openstack/scenarios/neutron/test_network.py', 'samples/tasks/scenarios/neutron/create-and-list-subnetpools.yaml', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'rally-jobs/rally-neutron.yaml', 'rally/plugins/openstack/cleanup/resources.py', 'samples/tasks/scenarios/neutron/create-and-list-subnetpools.json', 'rally/plugins/openstack/scenarios/neutron/network.py', 'tests/unit/plugins/openstack/scenarios/neutron/test_utils.py']",8,8f25e9c35ea67138bbaec5a003cb1e0ca604c204,NeutronNetworks.create_and_list_subnetpools," def test__list_subnetpools(self): result = self.scenario._list_subnetpools() self.assertEqual(self.clients(""neutron"").list_subnetpools.return_value, result) self._test_atomic_action_timer(self.scenario.atomic_actions(), ""neutron.list_subnetpools"") def test__create_subnetpool(self): expected_subnetpool = { ""subnetpool"": { ""name"": ""test_subnetpool"", ""prefixes"": [""10.1.0.0/18""] } } self.clients( ""neutron"").create_subnetpool.return_value = expected_subnetpool subnetpool_create_args = {""name"": ""test_subnetpool"", ""prefixes"": [""10.1.0.0/18""]} result_subnetpool = self.scenario._create_subnetpool( subnetpool_create_args) self.clients(""neutron"").create_subnetpool.assert_called_once_with( expected_subnetpool) self.assertEqual(result_subnetpool, expected_subnetpool) self._test_atomic_action_timer(self.scenario.atomic_actions(), ""neutron.create_subnetpool"") ",,169,1
openstack%2Fswift~master~I2641ed1a6fa7fdb1ff04539cb57edff2817facf0,openstack/swift,master,I2641ed1a6fa7fdb1ff04539cb57edff2817facf0,Refactor diskfile check implementation of automated tiering,NEW,2017-04-05 09:50:06.000000000,2017-12-18 03:49:10.000000000,,[{'_account_id': 13052}],"[{'number': 1, 'created': '2017-04-05 09:50:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a82027b348e3342c1fd0ff3d100021bd5e6fcc11', 'message': 'Refactor diskfile check implementation of automated tiering\n\nChange-Id: I2641ed1a6fa7fdb1ff04539cb57edff2817facf0\n'}, {'number': 2, 'created': '2017-04-07 06:57:29.000000000', 'files': ['swift/common/middleware/automated_tiering.py', 'swift/obj/addresser.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/ca09cedef83f02632435725d65e778709b511820', 'message': 'Refactor diskfile check implementation of automated tiering\n\nIn current autoamted tiering implementation, AddressingDiskFile\nclass and ConsistencyDiskFile class are used. AddressingDiskFile is\na class to make information about which object should be moved and\nwhere the objects should be moved to. ConsistencyDiskFile is class a\nclass to make information about which object is in inconsistent state\nand what tiering daemons should do to obtain consistency.  The two\nclasses can be implemented as dicts and implementation by dict will\nbe more simple than current implementation.\n\nThis patch refactors the AddressingDiskFile and the ConsistencyDiskFile.\nThis patch is preparation for https://review.openstack.org/#/c/454423/\n\nChange-Id: I2641ed1a6fa7fdb1ff04539cb57edff2817facf0\n'}]",0,453548,ca09cedef83f02632435725d65e778709b511820,7,1,2,14766,,,0,"Refactor diskfile check implementation of automated tiering

In current autoamted tiering implementation, AddressingDiskFile
class and ConsistencyDiskFile class are used. AddressingDiskFile is
a class to make information about which object should be moved and
where the objects should be moved to. ConsistencyDiskFile is class a
class to make information about which object is in inconsistent state
and what tiering daemons should do to obtain consistency.  The two
classes can be implemented as dicts and implementation by dict will
be more simple than current implementation.

This patch refactors the AddressingDiskFile and the ConsistencyDiskFile.
This patch is preparation for https://review.openstack.org/#/c/454423/

Change-Id: I2641ed1a6fa7fdb1ff04539cb57edff2817facf0
",git fetch https://review.opendev.org/openstack/swift refs/changes/48/453548/2 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/middleware/automated_tiering.py', 'swift/obj/addresser.py']",2,a82027b348e3342c1fd0ff3d100021bd5e6fcc11,(detached,"def entity_path_of(link_metadata): get entity object path from link metadata. :return: the entity object path return target_path for rule in self.addressing_rules: rule.initialize() if rule.has_optimized_search_range: delivery_info_generator =\ self.optimized_delivery_info_generator_of(rule) else: delivery_info_generator = self.delivery_info_generator_of(rule) for delivery_info in delivery_info_generator: run_pool.spawn(self.address, delivery_info) rule.finalize() def address(self, delivery_info): :param delivery_info: a dict about deilvery information successful = self.register_derivative_container(delivery_info) self.throw_delivery_task(delivery_info) def optimized_delivery_info_generator_of(self, addressing_rule): :return: a generator of dict with delivery information for filtered local diskfile in the original policy of the addressing_rule except DiskFileNotExist: # the object path is not local if df is None: continue delivery_info = self.delivery_info_of(df, addressing_rule) if delivery_info: yield delivery_info or None if the object is not local :raise DiskFileNotExist: if the diskfile is not exist node['device'], part, account, container, obj, policy) return None def delivery_info_generator_of(self, addressing_rule): :return: a generator of dict with delivery information for all local diskfile in the original policy of the addressing_rule df = diskfile_mgr.get_diskfile_from_audit_location(audit_location) delivery_info = self.delivery_info_of(df, addressing_rule) if delivery_info: yield delivery_info def delivery_info_of(self, df, addressing_rule): """""" :param df: a DiskFile instance :param addressing_rule: a AddressingRule subclass instance :return: if the df has object (not tombstone) and tiering daemons need to deliver the object, a dict with information about the object path and source object path and destination object path in other cases, None """""" try: with df.open(): metadata = HeaderKeyDict(df.get_metadata()) timestamp = df.data_timestamp except DiskFileNotExist: return None orig_path = metadata['name'] orig_acc, orig_con, obj =\ split_path(orig_path, 3, 3, rest_with_last=True) if is_link_metadata(metadata): src_path = entity_path_of(metadata) metadata = entity_metadata_of(metadata) else: src_path = orig_path src_acc, src_con, _ = split_path(src_path, 3, 3, rest_with_last=True) dst_policy =\ addressing_rule.determine_suitable_policy(metadata, src_con) # addressing rule determined current policy is suitable if dst_policy is None: return None if dst_policy == addressing_rule.original_policy: dst_acc = orig_acc dst_con = orig_con else: dst_acc = derivative_account_of(orig_acc) dst_con = derivative_container_of(orig_con, dst_policy) # addressing rule determined current policy is suitable if dst_con == src_con: return None return { 'original': { 'account': orig_acc, 'container': orig_con, 'object': obj, }, 'src': { 'account': src_acc, 'container': src_con, 'object': obj, }, 'dst': { 'account': dst_acc, 'container': dst_con, 'object': obj, 'policy': dst_policy, }, 'timestamp': timestamp, 'etag': metadata['ETag'], 'created_time': int(time.time()), } def register_derivative_container(self, delivery_info): :param delivery_info: a dict about delivery information if delivery_info['dst']['container'] ==\ delivery_info['original']['container']: con_name = DERIVATIVE_CONTAINERS obj_name = '/'.join(['', delivery_info['original']['account'], delivery_info['original']['container'], delivery_info['dst']['container']]) headers = { 'X-Size': 0, 'X-Etag': 'derivative_container', 'X-Timestamp': Timestamp(time.time()).internal, 'X-Content-Type': 'application/derivative_container'} return self.direct_put_to_tiering_info_account( con_name, obj_name, headers) def throw_delivery_task(self, delivery_info): :param delivery_info: a dict about delivery information con_name = DeliveryTask.task_container_name(delivery_info) obj_name = DeliveryTask.task_obj_name(delivery_info) headers = DeliveryTask.task_header(delivery_info) con_name, obj_name, headers) for consistency_info in self.consistency_info_generator(): if consistency_info['original']['is_base']: run_pool.spawn(self.throw_cleaning_task, consistency_info) if consistency_info['derivative']['is_base']: run_pool.spawn(self.throw_link_update_task, consistency_info) def consistency_info_generator(self): Consistency between original object path '/orig_a/orig_c/orig_o' and its derivative object path '/deri_a/deri_c/deri_o' is defined as satisfying all of the following conditions. Condition 1. If an link object is in original object path, it must refers an object in a derivative object path. Condition 2. If an object is in derivative object path, it must be refered by a link object in original object path. Condition 3. If a link object is in original object path and a refered object is in derivative object path, timestamp of .data file of the link object and the refered object must be equal. The condition 1 is ensured by deliverer, because deliverer follow the rules. Rule 1: Link objects must be made after refered objects. Rule 2: Refered objects must be deleted after link objects. But condition 2 and 3 can be not satisfied. Therefore, this method checks condition 2 and 3. If there are objects in derivative object path which are against the condition 2 or 3, this method yields dictionaries with information about the objects. :returns: a generator of dict with consistency information return (consistency_info for con in derivative_container_list for consistency_info in self.consistency_info_generator_of(con) if consistency_info is not None) def consistency_info_generator_of(self, derivative_container_dict): """""" :param derivative_container_dict: a dictionary about derivative container information """""" original_account, original_container, derivative_container =\ split_path(derivative_container_dict['name'], 3) derivative_account = derivative_account_of(original_account) original_container_headers, _ =\ self.direct_get_container_util( original_account, original_container) original_policy = POLICIES.get_by_index( original_container_headers['X-Storage-Policy-Index']) derivative_container_headers, derivative_objects =\ self.direct_get_container_util( derivative_account, derivative_container) derivative_policy = POLICIES.get_by_index( derivative_container_headers['X-Storage-Policy-Index']) def consistency_info_of(obj): return self.consistency_info_of( '/'.join(['', original_account, original_container, obj['name']]), original_policy, '/'.join(['', derivative_account, derivative_container, obj['name']]), derivative_policy, last_modified_date_to_timestamp(obj['last_modified'])) return (consistency_info_of(obj) for obj in derivative_objects) def consistency_info_of( self, original_path, original_policy, derivative_path, derivative_policy, derivative_timestamp): """""" :param original_path: an object path of original object :param original policy: a storage policy of the original path :param derivative_path: an object path of derivative object :param derivative_policy: a storage policy of the derivative path :param derivative_timestamp: a timestamp object of the derivative object without offset :return: if the original object is in local and tiering daemons need to obtain consistency, a dict with information about the original path and derivative path, in other cases, None """""" try: df = self.obj_path_to_local_diskfile( original_policy, original_path) if df is None: # this object path is not local. return None with df.open(): metadata = HeaderKeyDict(df.get_metadata()) original_timestamp = df.data_timestamp except DiskFileDeleted as err: metadata = HeaderKeyDict(err.metadata) original_timestamp = Timestamp(metadata['X-Timestamp']) except DiskFileNotExist: # the tombstone may be reclaimed. # this is dummy metadata and dummy timestamp metadata = HeaderKeyDict() original_timestamp = Timestamp(0) derivative_is_referred =( is_link_metadata(metadata) and entity_path_of(metadata) == derivative_path) timestamp_is_different =\ original_timestamp.normal != derivative_timestamp.normal # If ""derivative_is_referred"" is False, condition 2 is not satisfied. # If ""timestamp_is_different"" is True, condition 3 is not satisfied. if not derivative_is_referred or timestamp_is_different: # In automated tiering, daemons should objtain consistency base on # information of the newest object among original path and # derivative paths. The newest object should be judged by timestamp # with offsets. However, the derivative timestamp have no offset # because it is from container listing. Therefore, base object is # judged by timestamp without offset. If original timestamp is # equal to derivative timestamp without offsets, both of original # and derivative can be base because this method can't know which # has newer offset. return { 'original': { 'path': original_path, 'timestamp': original_timestamp, 'is_base': original_timestamp.normal >= derivative_timestamp.normal, }, 'derivative': { 'path': derivative_path, 'timestamp': derivative_timestamp, 'is_base': original_timestamp.normal <= derivative_timestamp.normal, 'policy': derivative_policy, }, 'created_time': int(time.time()), } return None def throw_cleaning_task(self, consistency_info): con_name = CleaningTask.task_container_name(consistency_info) obj_name = CleaningTask.task_obj_name(consistency_info) headers = CleaningTask.task_header(consistency_info) con_name, obj_name, headers) def throw_link_update_task(self, consistency_info): con_name = LinkUpdateTask.task_container_name(consistency_info) obj_name = LinkUpdateTask.task_obj_name(consistency_info) headers = LinkUpdateTask.task_header(consistency_info) con_name, obj_name, headers) def task_container_name(cls, task_info): ('%020d' % (task_info['created_time'] // HOUR * HOUR)) def task_header(cls, task_info): 'X-Timestamp': Timestamp(task_info['created_time']).internal, 'X-Content-Type': cls.content_type } def task_obj_name(cls, delivery_info): :param delivery_info: a dict about delivery information delivery_timestamp = cls.delivery_timestamp(delivery_info['timestamp']) return '/'.join(['', delivery_timestamp.internal, delivery_info['etag'], str(int(delivery_info['dst']['policy'])), delivery_info['original']['account'], delivery_info['original']['container'], delivery_info['src']['account'], delivery_info['src']['container'], delivery_info['dst']['account'], delivery_info['dst']['container'], delivery_info['original']['object']]) def task_obj_name(cls, consistency_info): Serialize a CleaningTask which is defined by an consistency info dict. :param consistency_info: a consistency info dict acc, con, obj = split_path( consistency_info['derivative']['path'], 3, 3, rest_with_last=True) '', consistency_info['original']['timestamp'].internal, str(int(consistency_info['derivative']['policy'])), acc, con, obj]) def task_obj_name(cls, consistency_info): Serialize a LinkUpdateTask which is defined by an consistency info dict. :param consistency_info: a consistency info dict orig_acc, orig_con, obj = split_path( consistency_info['original']['path'], 3, 3, rest_with_last=True) # original object name is equal to derivative object name deri_acc, deri_con, _ = split_path( consistency_info['derivative']['path'], 3, 3, rest_with_last=True) '', consistency_info['derivative']['timestamp'].internal, str(int(consistency_info['derivative']['policy'])), orig_acc, orig_con, deri_acc, deri_con, obj])","def entity_account_of(link_metadata): get entity account name from link metadata. :return: a account name in which the entity object is stored entity_account, _, _ = split_path(target_path, 3, 3, rest_with_last=True) return entity_account def entity_container_of(link_metadata): """""" get entity container name from link metadata. :param link_metadata: a metadata dict of a link object :return: a container name in which the entity object is stored """""" target_path = HeaderKeyDict(link_metadata)[TGT_OBJ_SYSMETA_SYMLINK_HDR] _, entity_container, _ = split_path(target_path, 3, 3, rest_with_last=True) return entity_container for addressing_rule in self.addressing_rules: addressing_rule.initialize() for addressing_diskfile in self.search_range_of(addressing_rule): if addressing_diskfile.is_need_to_be_delivered: run_pool.spawn(self.address, addressing_diskfile) addressing_rule.finalize() def address(self, addressing_diskfile): :param addressing_diskfile: a AddressingDiskFile instance successful = self.register_derivative_container(addressing_diskfile) self.throw_delivery_task(addressing_diskfile) def search_range_of(self, addressing_rule): """""" if addressing_rule.has_optimized_search_range: return self.optimized_search_range_of(addressing_rule) else: return self.all_local_diskfile_of(addressing_rule) def optimized_search_range_of(self, addressing_rule): """""" :param addressing_rule: a AddressingRule subclass instance :return: a generator of AddressingDiskFile if df: yield AddressingDiskFile(df, addressing_rule) except (DiskFileDeleted, DiskFileNotExist): or None if the object does not exist on the local node, local_node = None local_node = node if local_node is None: return None else: try: local_node['device'], part, account, container, obj, policy) except DiskFileNotExist: raise def all_local_diskfile_of(self, addressing_rule): :returns: a generator of all local AddressingDiskFile in the original policy of the addressing_rule try: df = diskfile_mgr.get_diskfile_from_audit_location( audit_location) yield AddressingDiskFile(df, addressing_rule) except (DiskFileNotExist, DiskFileDeleted): continue def register_derivative_container(self, addressing_diskfile): :param addressing_diskfile: a instance of AddressingDiskFile if addressing_diskfile.dst_container ==\ addressing_diskfile.original_container: else: container_name = DERIVATIVE_CONTAINERS obj_name = '/'.join([ '', addressing_diskfile.original_account, addressing_diskfile.original_container, addressing_diskfile.dst_container]) headers = { 'X-Size': 0, 'X-Etag': 'derivative_container', 'X-Timestamp': Timestamp(time.time()).internal, 'X-Content-Type': 'application/derivative_container'} return self.direct_put_to_tiering_info_account( container_name, obj_name, headers) def throw_delivery_task(self, addressing_diskfile): :param addressing_diskfile: a instance of AddressingDiskFile container_name =\ DeliveryTask.task_container_name(addressing_diskfile) obj_name = DeliveryTask.task_obj_name(addressing_diskfile) headers = DeliveryTask.task_header(addressing_diskfile) container_name, obj_name, headers) for consistency_diskfile in\ self.diskfiles_with_corresponding_objects(): if consistency_diskfile.is_need_to_clean_corresponding_object: run_pool.spawn(self.throw_cleaning_task, consistency_diskfile) if consistency_diskfile.is_need_to_update_link: run_pool.spawn( self.throw_link_update_task, consistency_diskfile) def diskfiles_with_corresponding_objects(self): :returns: a generator of ConsistencyDiskFile in the original policy which have correspoinding objects in derivative container. policy_cache = dict() for derivative_container_dict in derivative_container_list: original_account, original_container, derivative_container =\ split_path(derivative_container_dict['name'], 3) if (original_account, original_container) not in policy_cache: headers = self.direct_head_container_util( original_account, original_container) original_policy =\ POLICIES.get_by_index(headers['X-Storage-Policy-Index']) policy_cache[(original_account, original_container)] =\ original_policy else: original_policy =\ policy_cache[(original_account, original_container)] headers, objects = self.direct_get_container_util( derivative_account_of(original_account), derivative_container) for obj in objects: obj_path = '/'.join([ '', original_account, original_container, obj['name']]) try: df = self.obj_path_to_local_diskfile( original_policy, obj_path) if df is None: # this object path is not local. continue except DiskFileNotExist: df = None derivative_policy =\ POLICIES.get_by_index(headers['X-Storage-Policy-Index']) yield ConsistencyDiskFile( df, obj_path, derivative_policy, derivative_container, last_modified_date_to_timestamp(obj['last_modified'])) def throw_cleaning_task(self, consistency_diskfile): container_name =\ CleaningTask.task_container_name(consistency_diskfile) obj_name = CleaningTask.task_obj_name(consistency_diskfile) headers = CleaningTask.task_header(consistency_diskfile) container_name, obj_name, headers) def throw_link_update_task(self, consistency_diskfile): container_name =\ LinkUpdateTask.task_container_name(consistency_diskfile) obj_name = LinkUpdateTask.task_obj_name(consistency_diskfile) headers = LinkUpdateTask.task_header(consistency_diskfile) container_name, obj_name, headers) def task_container_name(cls, tiering_diskfile): ('%020d' % (tiering_diskfile.created_time // HOUR * HOUR)) def task_header(cls, tiering_diskfile): 'X-Timestamp': Timestamp(tiering_diskfile.created_time).internal, 'X-Content-Type': cls.content_type} def task_obj_name(cls, addressing_diskfile): :param addressing_diskfile: a AddressingDiskFile instance delivery_timestamp =\ cls.delivery_timestamp(addressing_diskfile.timestamp) return '/'.join([ '', delivery_timestamp.internal, addressing_diskfile.etag, str(int(addressing_diskfile.dst_policy)), addressing_diskfile.original_account, addressing_diskfile.original_container, addressing_diskfile.src_account, addressing_diskfile.src_container, addressing_diskfile.dst_account, addressing_diskfile.dst_container, addressing_diskfile.obj]) def task_obj_name(cls, consistency_diskfile): Serialize a CleaningTask which is defined by an ConsistencyDiskFile. :param consistency_diskfile: a ConsistencyDiskFile instance '', consistency_diskfile.timestamp.internal, str(int(consistency_diskfile.derivative_policy)), consistency_diskfile.derivative_account, consistency_diskfile.derivative_container, consistency_diskfile.obj]) def task_obj_name(cls, consistency_diskfile): Serialize a LinkUpdateTask which is defined by an ConsistencyDiskFile. :param consistency_diskfile: a ConsistencyDiskFile instance '', consistency_diskfile.derivative_timestamp.internal, str(int(consistency_diskfile.derivative_policy)), consistency_diskfile.original_account, consistency_diskfile.original_container, consistency_diskfile.derivative_account, consistency_diskfile.derivative_container, consistency_diskfile.obj])class AddressingDiskFile(object): """""" This is adapter class of object's DiskFile metadata for addressing. This class has following interfaces. original_account: Account name of the object. original_container: Container name of the object. obj: Name of the object. timestamp: Timestamp instance of the object's .data file. src_account: Account name of entity object. If the object is link object, this is the account name of the referred object. If the object is entity object, this is the account name of the object. src_container: Container name of entity object. If the object is link object, this is the container name of the referred object. If the object is entity object, this is the container name of the object. etag: Etag of entity object. metadata: Metadata of entity object. If the object is link object, the object's metadata is converted into this. If there is .meta file, X-Timestamp of this metadata is timestamp of the .meta file. dst_policy: Suitable storage policy instance of entity object determined by an AddressingRule. dst_account: Account name into which ObjectDeliverer deliver entity object. dst_container: Container name into which ObjectDeliverer deliver entity object. is_need_to_be_delivered: Entity object is need to be delivered by ObjectDeliverer or not. created_time: UNIX timestamp integer at this instance is created. """""" def __init__(self, df, addressing_rule): """""" :param df: a DiskFile instance :param addressing_rule: a AddressingRule subclass instance :raise DiskFileDeleted: if df is tombstone """""" try: with df.open(): self.metadata = HeaderKeyDict(df.get_metadata()) self.timestamp = df.data_timestamp except DiskFileDeleted: raise self.original_account, self.original_container, self.obj =\ split_path(self.metadata['name'], 3, 3, rest_with_last=True) if is_link_metadata(self.metadata): self.src_account = entity_account_of(self.metadata) self.src_container = entity_container_of(self.metadata) self.metadata = entity_metadata_of(self.metadata) else: self.src_account = self.original_account self.src_container = self.original_container self.etag = self.metadata['ETag'] self.dst_policy = addressing_rule.determine_suitable_policy( self.metadata, self.src_container) if self.dst_policy: if self.dst_policy == addressing_rule.original_policy: self.dst_account = self.original_account self.dst_container = self.original_container else: self.dst_account = derivative_account_of(self.original_account) self.dst_container = derivative_container_of( self.original_container, self.dst_policy) self.is_need_to_be_delivered =\ (self.dst_container != self.src_container) else: self.is_need_to_be_delivered = False self.created_time = int(time.time()) class ConsistencyDiskFile(object): """""" This is adapter class of object's/tombstone's DiskFile metadata for checking consistency. This class has following interfaces. original_account: Account name of the object/tombstone. original_container: Container name of the object/tombstone. obj: Name of the object/tombstone. timestamp: Timestamp instance of the object/tombstone. In object case, the timestamp is of .data file. derivative_policy: The storage policy instance of the derivative container. derivative_account: Account name of the object's/tombstone's derivative account derivative_container: Container name of the object's/tombstone's derivative container derivative_timestamp: The timestamp instance of '/<derivative_account>/ <derivative_container>/<obj>'. This timestamp doesn't have offset. is_need_to_clean_corresponding_object: '/<derivative_account>/ <derivative_container>/<obj>' is need to be cleaned or not. is_need_to_update_link: The link object is need to be updated or not. created_time: UNIX timestamp integer at this instance is created. Consistency between '/<original_account>/<original_container>/<obj>' and '/<derivative_account>/<derivative_container>/<obj>' is defined as satisfying one of the four following conditions. 1. Satisfying all of the two following conditions. 1-1. A entity object is in '<original_container>/<obj>'. 1-2. No object is in '<derivative_container>/<obj>'. 2. Satisfying all of the three following conditions. 2-1. A link object is in '<original_container>/<obj>'. 2-2. No object is in '<derivative_container>/<obj>'. 2-3. '<derivative_container>/<obj>' is not referred by the link object. 3. Satisfying all of the four following conditions. 3-1. A link object is in '<original_container>/<obj>'. 3-2. A entity object is in '<derivative_container>/<obj>'. 3-3. '<derivative_container>/<obj>' is referred by the link object. 3-4. The timestamp of .data file of the link object is equal to the timestamp of the entity object by ignoring offset of the timestamps. 4. Satisfying all of the two following conditions. 4-1. No object is in '<original_container>/<obj>'. 4-2. No object is in '<derivative_container>/<obj>'. .meta files' timestamps are not used in this definition, because .meta files are made in only original containers. In this definition, an object in '<derivative_container>/<obj>' must be referred by an link object and they have same timestamp. Then, ConsistencyDiskFile checks consistency by comparing referred container name and timestamp. """""" def __init__( self, df, obj_path, derivative_policy, derivative_container, derivative_timestamp): """""" :param df: a diskfile instance. this can be None when the diskfile is not exist :param obj_path: a object path string of the original object :param derivative_policy: a storage policy instance of the derivative container :param derivative_container: a derivative container name :param derivative_timestamp: a timestamp instance of an object in the derivative container without offset """""" if df: try: with df.open(): metadata = HeaderKeyDict(df.get_metadata()) self.timestamp = df.data_timestamp except DiskFileDeleted as err: metadata = HeaderKeyDict(err.metadata) self.timestamp = Timestamp(metadata['X-Timestamp']) except DiskFileNotExist: # the tombstone may be reclaimed. metadata = HeaderKeyDict() self.timestamp = Timestamp(0) else: # DiskFileNotExist is caught by obj_path_to_local_diskfile method. metadata = HeaderKeyDict() self.timestamp = Timestamp(0) self.original_account, self.original_container, self.obj =\ split_path(obj_path, 3, 3, rest_with_last=True) self.derivative_policy = derivative_policy self.derivative_account = derivative_account_of(self.original_account) self.derivative_container = derivative_container self.derivative_timestamp = derivative_timestamp if is_link_metadata(metadata): referred_container = entity_container_of(metadata) derivative_is_referred = derivative_container == referred_container else: derivative_is_referred = False if derivative_is_referred: # In this case, if (self.timestamp.normal > # self.derivative_timestamp.normal), there is inconsistency. But # ObjectAddresser doesn't throw any task, because entity object # with timestamp the same as the link object will overwrite the old # object in derivative container. This consideration is based on # the fact that ObjectDeliverer PUT link object after PUT entity # object with same timestamp. self.is_need_to_clean_corresponding_object = False # A ObjectDeliverer may have failed to update a link in # '/<original_account>/<original_container>/<obj>' self.is_need_to_update_link =\ (self.timestamp.normal < self.derivative_timestamp.normal) else: # In this case, if (self.timestamp.internal != # self.derivative_timestamp.internal) and (self.timestamp.normal == # self.derivative_timestamp.normal), ObjectAddresser cannot know # the order of these timestamp. Because, derivative_timestamp # doesn't have offset. Then, if (self.timestamp.normal == # self.derivative_timestamp.normal), ObjectAddresser throws task # for both cases of (self.timestamp.internal > # self.derivative_timestamp.internal) and (self.timestamp.internal # < self.derivative_timestamp.internal). # Someone made request PUT/DELETE to # '/<original_account>/<original_container>/<obj>'. self.is_need_to_clean_corresponding_object =\ (self.timestamp.normal >= self.derivative_timestamp.normal) # A ObjectDeliverer may have failed to update a link in # '/<original_account>/<original_container>/<obj>' self.is_need_to_update_link =\ (self.timestamp.normal <= self.derivative_timestamp.normal) self.created_time = int(time.time()) ",320,400
openstack%2Fswift~master~I188cd4fc7e94f5912999d2db572472423a607852,openstack/swift,master,I188cd4fc7e94f5912999d2db572472423a607852,Update implementation for DELETE of autoamted tiering,NEW,2017-04-07 02:07:33.000000000,2017-12-18 03:49:08.000000000,,[{'_account_id': 13052}],"[{'number': 1, 'created': '2017-04-07 02:07:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/49c5857aaa5b352ae974e970261a14ffcee33ae0', 'message': ""Update implementation for DELETE of autoamted tiering\n\nIn autoamted tiering, there will be some link objects and referred\nobjects. The link objects are in original (user-defined) container\nand the referred objects are in derivative (automatically made by\ndaemons) container. Swift end users can see objects in original\ncontainer. So if Swift end users want to DELETE tiered objects,\nthe uses can send DELETE requests to only objects in original\ncontainer. Therefore, tiering daemons should propagate the DELETE\nto the referred objects. In currenct automated tiering implementation,\nTiering daemons patrol object-servers' devices and if the daemons\nfind tombstones, the daemons try to send DELETE requests to\nobjects in derivative containers. But this implementation have a\nprpblem. The problem is that if there are too many objects in\nobject-servers, the patrol take long time greater than reclaim age\nof tombstones. In this case, tiering daemons cannot find tombstones,\nand there will be orphan (not referred) objects in derivative\ncontainers.\n\nThis patch changes the implementation by filtering patrol targets\nby comparing object lists from original containers and derivative\ncontainers.\n\nThis patch is for discussion.\nThis patch is not update tests for this change.\n\nChange-Id: I188cd4fc7e94f5912999d2db572472423a607852\n""}, {'number': 2, 'created': '2017-04-07 06:57:53.000000000', 'files': ['swift/obj/addresser.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/3014a485101accbd963d82f039778903f502afc4', 'message': ""Update implementation for DELETE of autoamted tiering\n\nIn autoamted tiering, there will be some link objects and referred\nobjects. The link objects are in original (user-defined) container\nand the referred objects are in derivative (automatically made by\ndaemons) container. Swift end users can see objects in original\ncontainer. So if Swift end users want to DELETE tiered objects,\nthe uses can send DELETE requests to only objects in original\ncontainer. Therefore, tiering daemons should propagate the DELETE\nto the referred objects. In currenct automated tiering implementation,\nTiering daemons patrol object-servers' devices and if the daemons\nfind tombstones, the daemons try to send DELETE requests to\nobjects in derivative containers. But this implementation have a\nprpblem. The problem is that if there are too many objects in\nobject-servers, the patrol take long time greater than reclaim age\nof tombstones. In this case, tiering daemons cannot find tombstones,\nand there will be orphan (not referred) objects in derivative\ncontainers.\n\nThis patch changes the implementation by filtering patrol targets\nby comparing object lists from original containers and derivative\ncontainers.\n\nThis patch is for discussion.\nThis patch is not update tests for this change.\n\nChange-Id: I188cd4fc7e94f5912999d2db572472423a607852\n""}]",0,454423,3014a485101accbd963d82f039778903f502afc4,7,1,2,14766,,,0,"Update implementation for DELETE of autoamted tiering

In autoamted tiering, there will be some link objects and referred
objects. The link objects are in original (user-defined) container
and the referred objects are in derivative (automatically made by
daemons) container. Swift end users can see objects in original
container. So if Swift end users want to DELETE tiered objects,
the uses can send DELETE requests to only objects in original
container. Therefore, tiering daemons should propagate the DELETE
to the referred objects. In currenct automated tiering implementation,
Tiering daemons patrol object-servers' devices and if the daemons
find tombstones, the daemons try to send DELETE requests to
objects in derivative containers. But this implementation have a
prpblem. The problem is that if there are too many objects in
object-servers, the patrol take long time greater than reclaim age
of tombstones. In this case, tiering daemons cannot find tombstones,
and there will be orphan (not referred) objects in derivative
containers.

This patch changes the implementation by filtering patrol targets
by comparing object lists from original containers and derivative
containers.

This patch is for discussion.
This patch is not update tests for this change.

Change-Id: I188cd4fc7e94f5912999d2db572472423a607852
",git fetch https://review.opendev.org/openstack/swift refs/changes/23/454423/1 && git format-patch -1 --stdout FETCH_HEAD,['swift/obj/addresser.py'],1,49c5857aaa5b352ae974e970261a14ffcee33ae0,(detached," than it of check_consistency_of_deleted_original_objects() and check_consistency_of_existing_original_objects(). Therefore, ObjectAddresser run the three methods independently. run_pool = GreenPool(size=3) self.forever, self.check_consistency_of_deleted_original_objects, 'checking consistency of deleted original objects') run_pool.spawn( self.forever, self.check_consistency_of_existing_original_objects, 'checking consistency of existing original objects') self.check_consistency_of_deleted_original_objects() self.check_consistency_of_existing_original_objects() contrainers list in check_consistency_of_deleted_original_objects() and check_consistency_of_existing_original_objects() method. If the registration is successful, throw a DeliveryTask. def check_consistency_of_deleted_original_objects(self): self._check_consistency(True) def check_consistency_of_existing_original_objects(self): self._check_consistency(False) def _check_consistency(self, check_deleted_objects): :param check_deleted_objects: if True, this method check consistency of DELETEd original objects and existing derivative objects, if False, this method check concurrency of existing original objects and existing derivative objects, for consistency_info in\ self.consistency_info_generator(check_deleted_objects): def consistency_info_generator(self, check_deleted_objects): :param check_deleted_objects: if True, this method check consistency of DELETEd original objects and existing derivative objects, if False, this method check concurrency of existing original objects and existing derivative objects, for consistency_info in self.consistency_info_generator_of(con, check_deleted_objects) def consistency_info_generator_of( self, derivative_container_dict, check_deleted_objects): original_container_headers, original_objects =\ existing_objects = set([obj['name'] for obj in original_objects]) return (consistency_info_of(obj) for obj in derivative_objects if (obj['name'] in existing_objects) != check_deleted_objects)"," than it of check_consistency(). Therefore, ObjectAddresser run the two methods independently. run_pool = GreenPool(size=2) self.forever, self.check_consistency, 'checking consistency') self.check_consistency() contrainers list in check_consistency() method. If the registration is successful, throw a DeliveryTask. def check_consistency(self): for consistency_info in self.consistency_info_generator(): def consistency_info_generator(self): for consistency_info in self.consistency_info_generator_of(con) def consistency_info_generator_of(self, derivative_container_dict): original_container_headers, _ =\ return (consistency_info_of(obj) for obj in derivative_objects)",46,14
openstack%2Fdiskimage-builder~master~I9c56ee72476994afa48e4162bc9858b085b876b6,openstack/diskimage-builder,master,I9c56ee72476994afa48e4162bc9858b085b876b6,Procedure to build UEFI based images,NEW,2016-04-19 09:07:53.000000000,2017-12-18 03:49:05.000000000,,"[{'_account_id': 7239}, {'_account_id': 10035}, {'_account_id': 10375}, {'_account_id': 11105}, {'_account_id': 12411}, {'_account_id': 12459}, {'_account_id': 21741}]","[{'number': 1, 'created': '2016-04-19 09:07:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/2ef51320b1663c1ad878df05d12ad58c4de6002b', 'message': 'Procedure to build UEFI based images\n\nThis change documents the procedure to build UEFI\nbased images\n\nChange-Id: I9c56ee72476994afa48e4162bc9858b085b876b6\n'}, {'number': 2, 'created': '2016-05-09 04:39:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/fa70f14783ac6b1092e9b3264c34c42f9bb93733', 'message': 'Procedure to build UEFI based images\n\nThis change documents the procedure to build UEFI\nbased images\n\nChange-Id: I9c56ee72476994afa48e4162bc9858b085b876b6\nDepends-On: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\n'}, {'number': 3, 'created': '2016-05-11 05:04:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/4213853170ec6442a6c1025b36752a4ee6bcd90c', 'message': 'Procedure to build UEFI based images\n\nThis change documents the procedure to build UEFI\nbased images\n\nChange-Id: I9c56ee72476994afa48e4162bc9858b085b876b6\nDepends-On: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\n'}, {'number': 4, 'created': '2017-04-07 12:02:04.000000000', 'files': ['README.rst'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/7e4121eb84272e4499cfbc6bf447a93ce1aa475c', 'message': 'Procedure to build UEFI based images\n\nThis change documents the procedure to build UEFI\nbased images\n\nChange-Id: I9c56ee72476994afa48e4162bc9858b085b876b6\nDepends-On: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45\n'}]",11,307655,7e4121eb84272e4499cfbc6bf447a93ce1aa475c,24,7,4,12411,,,0,"Procedure to build UEFI based images

This change documents the procedure to build UEFI
based images

Change-Id: I9c56ee72476994afa48e4162bc9858b085b876b6
Depends-On: If1fb0abe3507a0fe2b6f5266e0e2e5b32d628c45
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/55/307655/4 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,2ef51320b1663c1ad878df05d12ad58c4de6002b,287784,"To build an UEFI capable image, 1. Two Partions were created with GPT partition table. First partition contains the root file system, while the second partition is an EFI System Partition(ESP). parted is used for creating the partitions. 2. Bootloader element pkg-map has been added with grub-efi which installs dependent packages for default and redhat distributions. 3. As part of installing grub boot loader, the ESP partition is mounted to /boot/efi directory of the root file system so that grub will install required grubx64.efi and other dependent packages in ESP partition. 4. Grub configuration files are modified to work with respective distributions. ",,17,0
openstack%2Fqinling~master~I0a6abc98534dc95af5b985b6b6b4c0883263a221,openstack/qinling,master,I0a6abc98534dc95af5b985b6b6b4c0883263a221,Introduce etcd to qinling,MERGED,2017-12-14 01:33:14.000000000,2017-12-18 03:48:56.000000000,2017-12-18 03:48:56.000000000,"[{'_account_id': 6732}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-14 01:33:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/qinling/commit/4163b72b3a7061cac1a0df60af36961ec119e5f4', 'message': 'Fix execution concurrency bug\n\nChange-Id: I0a6abc98534dc95af5b985b6b6b4c0883263a221\n'}, {'number': 2, 'created': '2017-12-14 02:34:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/qinling/commit/6c9338a1552a9f603e6ae14db13438be4287fb8b', 'message': 'Fix execution concurrency bug\n\nChange-Id: I0a6abc98534dc95af5b985b6b6b4c0883263a221\n'}, {'number': 3, 'created': '2017-12-14 04:33:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/qinling/commit/68dad38d4ecbf32ffccfe7760f5a3ae98e76b7c5', 'message': 'Fix execution concurrency bug\n\nChange-Id: I0a6abc98534dc95af5b985b6b6b4c0883263a221\n'}, {'number': 4, 'created': '2017-12-16 07:15:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/qinling/commit/aa9ad847e4d39fa4fa11d009cea74b65e87024de', 'message': 'Introduce etcd to qinling\n\n- Qinling is using etcd for distributed locking, especially for invoking\n  functions simultaneously.\n- Get rid of function service and function worker mapping table\n- Use etcd to store function service url and workers\n\nChange-Id: I0a6abc98534dc95af5b985b6b6b4c0883263a221\n'}, {'number': 5, 'created': '2017-12-17 06:41:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/qinling/commit/e197e90a6afcbcb723119bc1ae7f7fed377bd36b', 'message': 'Introduce etcd to qinling\n\n- Qinling is using etcd for distributed locking, especially for invoking\n  functions simultaneously.\n- Get rid of function service and function worker mapping table\n- Use etcd to store function service url and workers\n\nChange-Id: I0a6abc98534dc95af5b985b6b6b4c0883263a221\n'}, {'number': 6, 'created': '2017-12-17 09:14:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/qinling/commit/cd7ade396140fa1f404b12b668a9fae9851ca35a', 'message': 'Introduce etcd to qinling\n\n- Qinling is using etcd for distributed locking, especially for invoking\n  functions simultaneously.\n- Get rid of function service and function worker mapping table\n- Use etcd to store function service url and workers\n\nChange-Id: I0a6abc98534dc95af5b985b6b6b4c0883263a221\n'}, {'number': 7, 'created': '2017-12-18 03:15:14.000000000', 'files': ['qinling/db/sqlalchemy/migration/alembic_migrations/versions/001_pike.py', 'qinling/engine/service.py', 'qinling/api/controllers/v1/runtime.py', 'qinling/db/api.py', 'qinling/api/controllers/v1/root.py', 'qinling/db/sqlalchemy/sqlite_lock.py', 'qinling/engine/utils.py', 'requirements.txt', 'qinling/utils/etcd_util.py', 'runtimes/python2/server.py', 'qinling/db/sqlalchemy/models.py', 'qinling/tests/unit/api/controllers/v1/test_function.py', 'qinling/api/controllers/v1/function.py', 'qinling/orchestrator/kubernetes/manager.py', 'qinling/engine/default_engine.py', 'qinling/tests/unit/base.py', 'qinling/cmd/launch.py', 'qinling/services/periodics.py', 'qinling/config.py', 'qinling/db/base.py', 'qinling_tempest_plugin/tests/api/test_executions.py', 'qinling/tests/unit/api/controllers/v1/test_runtime.py', 'qinling/api/controllers/v1/resources.py', 'qinling/db/sqlalchemy/api.py', 'qinling/tests/unit/api/controllers/v1/test_function_worker.py'], 'web_link': 'https://opendev.org/openstack/qinling/commit/072605fa63a96301ef3f6ae4db3a247babfde9e6', 'message': 'Introduce etcd to qinling\n\n- Qinling is using etcd for distributed locking, especially for invoking\n  functions simultaneously.\n- Get rid of function service and function worker mapping table\n- Use etcd to store function service url and workers\n\nChange-Id: I0a6abc98534dc95af5b985b6b6b4c0883263a221\n'}]",0,527830,072605fa63a96301ef3f6ae4db3a247babfde9e6,26,2,7,6732,,,0,"Introduce etcd to qinling

- Qinling is using etcd for distributed locking, especially for invoking
  functions simultaneously.
- Get rid of function service and function worker mapping table
- Use etcd to store function service url and workers

Change-Id: I0a6abc98534dc95af5b985b6b6b4c0883263a221
",git fetch https://review.opendev.org/openstack/qinling refs/changes/30/527830/4 && git format-patch -1 --stdout FETCH_HEAD,"['qinling/engine/default_engine.py', 'qinling/config.py', 'qinling_tempest_plugin/tests/api/test_executions.py', 'qinling/db/sqlalchemy/api.py']",4,4163b72b3a7061cac1a0df60af36961ec119e5f4,add-etcd, return _secure_query(models.FunctionWorkers).filter_by( function_id=function_id).with_for_update().all(), return _secure_query( models.FunctionWorkers).with_for_update().filter( models.FunctionWorkers.function_id == function_id).all(),59,29
openstack%2Fswift~master~I433f7bd0bb73f39ba20c32bbe7267c2beeb1371e,openstack/swift,master,I433f7bd0bb73f39ba20c32bbe7267c2beeb1371e,Fix default FakeRing max_more_nodes,NEW,2017-04-07 21:52:08.000000000,2017-12-18 03:48:44.000000000,,"[{'_account_id': 1179}, {'_account_id': 4608}, {'_account_id': 13052}]","[{'number': 1, 'created': '2017-04-07 21:52:08.000000000', 'files': ['test/unit/__init__.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/3974f5b06271e6f797c0964a3c61d21406838cc1', 'message': 'Fix default FakeRing max_more_nodes\n\nOur Fake should be a good Fake by default.  A good fake is going to have\npleanty of devices - just like real production rings.\n\nThis chagne is to demonstrate the regression from Related-Change, and\nrelated test fallout.\n\nChange-Id: I433f7bd0bb73f39ba20c32bbe7267c2beeb1371e\nRelated-Change-Id: I04132858f44b42ee7ecf3b7994cb22a19d001d70\n'}]",0,454898,3974f5b06271e6f797c0964a3c61d21406838cc1,5,3,1,1179,,,0,"Fix default FakeRing max_more_nodes

Our Fake should be a good Fake by default.  A good fake is going to have
pleanty of devices - just like real production rings.

This chagne is to demonstrate the regression from Related-Change, and
related test fallout.

Change-Id: I433f7bd0bb73f39ba20c32bbe7267c2beeb1371e
Related-Change-Id: I04132858f44b42ee7ecf3b7994cb22a19d001d70
",git fetch https://review.opendev.org/openstack/swift refs/changes/98/454898/1 && git format-patch -1 --stdout FETCH_HEAD,['test/unit/__init__.py'],1,3974f5b06271e6f797c0964a3c61d21406838cc1,," def __init__(self, replicas=3, max_more_nodes=None, part_power=0, base_port=1000): """""" Create a good FakeRing that feels *very* similar to rings your code might acctually encoutner. :param replicas: number of replicas :param max_more_nodes: limit get_more_nodes depth, default is replicas * 2 :param part_power: control # of parts and _part_shirt :param base_port: used creating stub devs """""" self._base_port = base_port if max_more_nodes is None: self.max_more_nodes = 2 * replicas else: self.max_more_nodes = max_more_nodes self._part_shift = 32 - part_power"," def __init__(self, replicas=3, max_more_nodes=0, part_power=0, base_port=1000): self._base_port = base_port self.max_more_nodes = max_more_nodes self._part_shift = 32 - part_power # 9 total nodes (6 more past the initial 3) is the cap, no matter if # this is set higher, or R^2 for R replicas",17,6
openstack%2Fdiskimage-builder~master~If1190857c4e2d1a44e76bef172c9bac3c22d83d4,openstack/diskimage-builder,master,If1190857c4e2d1a44e76bef172c9bac3c22d83d4,dib-lint: basic check for duplicate element names,NEW,2017-04-10 06:08:21.000000000,2017-12-18 03:48:30.000000000,,"[{'_account_id': 7118}, {'_account_id': 10118}]","[{'number': 1, 'created': '2017-04-10 06:08:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/228f0a3a9d48e083a3151c0a6d83f590346d3090', 'message': ""dib-lint: basic check for duplicate element names\n\nCheck that phases of different elements don't include the same script\nto avoid overwriting each other.\n\nChange-Id: If1190857c4e2d1a44e76bef172c9bac3c22d83d4\n""}, {'number': 2, 'created': '2017-04-10 07:10:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/7f80e14c5879b87c0360aca42313c5f82a2ae313', 'message': ""dib-lint: basic check for duplicate element names\n\nCheck that phases of different elements don't include the same script\nto avoid overwriting each other.\n\nChange-Id: If1190857c4e2d1a44e76bef172c9bac3c22d83d4\n""}, {'number': 3, 'created': '2017-04-10 07:24:41.000000000', 'files': ['diskimage_builder/elements/ubuntu/environment.d/10-ubuntu-distro-name.bash', 'diskimage_builder/elements/fedora-minimal/environment.d/11-yum-dnf.bash', 'bin/dib-lint', 'diskimage_builder/elements/fedora-minimal/environment.d/10-fedora-minimal-distro-name.bash', 'diskimage_builder/elements/ubuntu-core/environment.d/10-ubuntu-distro-name.bash', 'diskimage_builder/elements/opensuse-minimal/environment.d/10-opensuse-minimal-distro-name.bash', 'diskimage_builder/elements/ubuntu-minimal/environment.d/10-ubuntu-minimal-distro-name.bash', 'diskimage_builder/elements/yum/11-yum-dnf.bash'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/f3b4a8fd167300b14d4ce12faee3a6c9148c9b52', 'message': 'dib-lint: basic check for duplicate element names\n\nCheck that phases of different elements don\'t include the same script\nto avoid overwriting each other.\n\nThis revealed some issues.  There are elements that are clearly not\nsupposed to be used together, however I still think we\'re better off\nenforcing different names.\n\nThe ""-minimal"" builds have their ""distro-name"" environment files\nrenamed to have ""minimal"" in it and not conflict wit the non-minimal\nversions.\n\n11-yum-dnf.bash was duplicated between fedora-minimal and fedora.\nCentralise that in ""yum"" which is common to both.\n\nMove default in ubuntu-core from ""ubuntu""\'s distro-name.  ubuntu-core\ndoesn\'t check for missing values, so it seems sane to set this there.\n\nChange-Id: If1190857c4e2d1a44e76bef172c9bac3c22d83d4\n'}]",0,455126,f3b4a8fd167300b14d4ce12faee3a6c9148c9b52,11,2,3,7118,,,0,"dib-lint: basic check for duplicate element names

Check that phases of different elements don't include the same script
to avoid overwriting each other.

This revealed some issues.  There are elements that are clearly not
supposed to be used together, however I still think we're better off
enforcing different names.

The ""-minimal"" builds have their ""distro-name"" environment files
renamed to have ""minimal"" in it and not conflict wit the non-minimal
versions.

11-yum-dnf.bash was duplicated between fedora-minimal and fedora.
Centralise that in ""yum"" which is common to both.

Move default in ubuntu-core from ""ubuntu""'s distro-name.  ubuntu-core
doesn't check for missing values, so it seems sane to set this there.

Change-Id: If1190857c4e2d1a44e76bef172c9bac3c22d83d4
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/26/455126/2 && git format-patch -1 --stdout FETCH_HEAD,['bin/dib-lint'],1,228f0a3a9d48e083a3151c0a6d83f590346d3090,385608,"declare -A seen_scripts # Simple check for duplicate script names that run within the same # phase. These will copy over each other and fail. # # tricks: # - use negative character class [^/] as bash doesn't have non-greedy match # - only matching numeric numbered scripts # - can't seem to check key with -u # # Split the filename up into /elements/NAME/PHASE.d/nn-SCRIPT if [[ $i =~ /(elements)/([^/]+)/([^/]+\.d)/([0-9][0-9].*)$ ]]; then # PHASE/xx-SCRIPT is the key key=""${BASH_REMATCH[3]}/${BASH_REMATCH[4]}"" set +u if [[ -n ""${seen_scripts[$key]}"" ]]; then error ""Potential name conflict found: $key already provided by ${seen_scripts[$key]}"" fi set -u seen_scripts[$key]=${BASH_REMATCH[2]} fi ",,22,0
openstack%2Fironic-inspector~master~I4b15b954c6c64a8935a5a32537d878ceb5ebc602,openstack/ironic-inspector,master,I4b15b954c6c64a8935a5a32537d878ceb5ebc602,Functional tests: add API microversion tests,NEW,2017-03-30 07:45:41.000000000,2017-12-18 03:48:27.000000000,,"[{'_account_id': 10239}, {'_account_id': 13636}]","[{'number': 1, 'created': '2017-03-30 07:45:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/0f9938b43e04443c1d9d4e54c65ea78cf968d366', 'message': 'Functional tests: add API microversion tests\n\nChange-Id: I4b15b954c6c64a8935a5a32537d878ceb5ebc602\n'}, {'number': 2, 'created': '2017-03-30 07:49:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/c66f6d2dcc096a1768a1b72c443ce644dc7560f7', 'message': 'Functional tests: add API microversion tests\n\nChange-Id: I4b15b954c6c64a8935a5a32537d878ceb5ebc602\n'}, {'number': 3, 'created': '2017-04-04 10:10:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/0b9979e362ff87793e58778efd032d62fa13e39e', 'message': ""Functional tests: add API microversion tests\n\nSplit functional tests into 2 group:\n\n * API-specific tests\n   Contains simple tests for verify particular microversion behavior;\n * Current version tests\n   Tests scenarios based on current (maximum) API version.\n\nChange will allow to keep tests from second group up-to-date with API\nversion (save us from microversion mess in code) and check that we don't\nbreak something in the meantime.\n\nChange-Id: I4b15b954c6c64a8935a5a32537d878ceb5ebc602\n""}, {'number': 4, 'created': '2017-04-05 10:03:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/6d14b79b705a5654f37af9959757cf722df8659b', 'message': ""Functional tests: add API microversion tests\n\nSplit functional tests into 2 group:\n\n * API-specific tests\n   Contains simple tests for verify particular microversion behavior;\n * Current version tests\n   Tests scenarios based on current (maximum) API version.\n\nChange will allow to keep tests from second group up-to-date with API\nversion (save us from microversion mess in code) and check that we don't\nbreak something in the meantime.\n\nChange-Id: I4b15b954c6c64a8935a5a32537d878ceb5ebc602\n""}, {'number': 5, 'created': '2017-04-06 10:24:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/c819d559b3c757f7ed373866c05fc7ed1603e58b', 'message': ""Functional tests: add API microversion tests\n\nSplit functional tests into 2 group:\n\n * API-specific tests\n   Contains simple tests for verify particular microversion behavior;\n * Current version tests\n   Tests scenarios based on current (maximum) API version.\n\nChange will allow to keep tests from second group up-to-date with API\nversion (save us from microversion mess in code) and check that we don't\nbreak something in the meantime.\n\nChange-Id: I4b15b954c6c64a8935a5a32537d878ceb5ebc602\n""}, {'number': 6, 'created': '2017-04-10 10:17:40.000000000', 'files': ['ironic_inspector/test/functional/__init__.py', 'ironic_inspector/test/functional/test_scenarios.py', 'ironic_inspector/test/functional/base.py', 'ironic_inspector/test/functional/test_microversions.py', 'tox.ini', 'ironic_inspector/test/functional/main.py'], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/e8c6e807135f4a9e1ef3e62c203d9611a3784e06', 'message': ""Functional tests: add API microversion tests\n\nSplit functional tests into 2 group:\n\n * API-specific tests\n   Contains simple tests for verify particular microversion behavior;\n * Current version tests\n   Tests scenarios based on current (maximum) API version.\n\nChange will allow to keep tests from second group up-to-date with API\nversion (save us from microversion mess in code) and check that we don't\nbreak something in the meantime.\n\nChange-Id: I4b15b954c6c64a8935a5a32537d878ceb5ebc602\n""}]",3,451685,e8c6e807135f4a9e1ef3e62c203d9611a3784e06,17,2,6,13636,,,0,"Functional tests: add API microversion tests

Split functional tests into 2 group:

 * API-specific tests
   Contains simple tests for verify particular microversion behavior;
 * Current version tests
   Tests scenarios based on current (maximum) API version.

Change will allow to keep tests from second group up-to-date with API
version (save us from microversion mess in code) and check that we don't
break something in the meantime.

Change-Id: I4b15b954c6c64a8935a5a32537d878ceb5ebc602
",git fetch https://review.opendev.org/openstack/ironic-inspector refs/changes/85/451685/5 && git format-patch -1 --stdout FETCH_HEAD,"['ironic_inspector/test/functional/__init__.py', 'ironic_inspector/test/functional/test_scenarios.py', 'ironic_inspector/test/functional/base.py', 'ironic_inspector/test/functional/test_microversions.py', 'ironic_inspector/test/unit/test_main.py', 'tox.ini', 'ironic_inspector/test/functional/main.py']",7,0f9938b43e04443c1d9d4e54c65ea78cf968d366,refactor-api-microver-tests,"# Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. import eventlet # noqa eventlet.monkey_patch() import contextlib import shutil import tempfile import unittest import mock from oslo_config import cfg import requests from ironic_inspector.common import ironic as ir_utils from ironic_inspector import dbsync from ironic_inspector import main from ironic_inspector.test.functional import base @contextlib.contextmanager def mocked_server(): d = tempfile.mkdtemp() try: conf_file = base.get_test_conf_file() with mock.patch.object(ir_utils, 'get_client'): dbsync.main(args=['--config-file', conf_file, 'upgrade']) cfg.CONF.reset() cfg.CONF.unregister_opt(dbsync.command_opt) eventlet.greenthread.spawn_n(main.main, args=['--config-file', conf_file]) eventlet.greenthread.sleep(1) # Wait for service to start up to 30 seconds for i in range(10): try: requests.get('http://127.0.0.1:5050/v1') except requests.ConnectionError: if i == 9: raise print('Service did not start yet') eventlet.greenthread.sleep(3) else: break # start testing yield # Make sure all processes finished executing eventlet.greenthread.sleep(1) finally: shutil.rmtree(d) if __name__ == '__main__': with mocked_server(): suite = unittest.TestLoader().discover( 'ironic_inspector.test.functional') unittest.TextTestRunner(verbosity=2).run(suite) ",,399,300
openstack%2Frally~master~I554bdd680439f102f63590990ad2047e3bc548a9,openstack/rally,master,I554bdd680439f102f63590990ad2047e3bc548a9,[WIP] Adding Gnocchi API Benchmarks,NEW,2017-04-05 20:44:45.000000000,2017-12-18 03:48:25.000000000,,"[{'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 21143}, {'_account_id': 21528}]","[{'number': 1, 'created': '2017-04-05 20:44:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/f5c29ca264d38bf08ae42b317925ec05e1e16590', 'message': 'WIP Adding Gnocchi API Benchmarks\n\nChange-Id: I554bdd680439f102f63590990ad2047e3bc548a9\n'}, {'number': 2, 'created': '2017-04-05 20:45:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d7102b6455b20e20ae10799f654976df0ed15a54', 'message': '[WIP] Adding Gnocchi API Benchmarks\n\nChange-Id: I554bdd680439f102f63590990ad2047e3bc548a9\n'}, {'number': 3, 'created': '2017-04-06 12:07:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6b76aa34fb8d7d9d2ac0ce2e856d24e4d91734c6', 'message': '[WIP] Adding Gnocchi API Benchmarks\n\nNew Scenarios:\n+ Gnocchi.ArchivePolicyRuleList\n+ Gnocchi.ArchivePolicyRuleCreate\n+ Gnocchi.ArchivePolicyRuleCreateDelete\n+ Gnocchi.ArchivePolicyList\n+ Gnocchi.ArchivePolicyCreate\n+ Gnocchi.ArchivePolicyCreateDelete\n+ Gnocchi.CapabilitiesList\n+ Gnocchi.MeasuresAggregation\n+ Gnocchi.MeasuresShow\n+ Gnocchi.MetricList\n+ Gnocchi.MetricCreate\n+ Gnocchi.MetricCreateDelete\n+ Gnocchi.ResourceTypeList\n+ Gnocchi.ResourceTypeCreate\n+ Gnocchi.ResourceTypeCreateDelete\n+ Gnocchi.ResourceList\n+ Gnocchi.ResourceCreate\n+ Gnocchi.ResourceCreateDelete\n+ Gnocchi.Status\n\nChange-Id: I554bdd680439f102f63590990ad2047e3bc548a9\n'}, {'number': 4, 'created': '2017-04-06 17:16:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0c7121482f964e7a885b62ed6ca440216035d51c', 'message': '[WIP] Adding Gnocchi API Benchmarks\n\nNew Scenarios:\n+ Gnocchi.ArchivePolicyRuleList\n+ Gnocchi.ArchivePolicyRuleCreate\n+ Gnocchi.ArchivePolicyRuleCreateDelete\n+ Gnocchi.ArchivePolicyList\n+ Gnocchi.ArchivePolicyCreate\n+ Gnocchi.ArchivePolicyCreateDelete\n+ Gnocchi.CapabilitiesList\n+ Gnocchi.MeasuresAggregation\n+ Gnocchi.MeasuresShow\n+ Gnocchi.MetricList\n+ Gnocchi.MetricCreate\n+ Gnocchi.MetricCreateDelete\n+ Gnocchi.ResourceTypeList\n+ Gnocchi.ResourceTypeCreate\n+ Gnocchi.ResourceTypeCreateDelete\n+ Gnocchi.ResourceList\n+ Gnocchi.ResourceCreate\n+ Gnocchi.ResourceCreateDelete\n+ Gnocchi.Status\n\nChange-Id: I554bdd680439f102f63590990ad2047e3bc548a9\n'}, {'number': 5, 'created': '2017-04-06 19:20:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4d235c4cd6b4accd48b172238c2316b3e52cbfaa', 'message': '[WIP] Adding Gnocchi API Benchmarks\n\nNew Scenarios:\n+ Gnocchi.ArchivePolicyRuleList\n+ Gnocchi.ArchivePolicyRuleCreate\n+ Gnocchi.ArchivePolicyRuleCreateDelete\n+ Gnocchi.ArchivePolicyList\n+ Gnocchi.ArchivePolicyCreate\n+ Gnocchi.ArchivePolicyCreateDelete\n+ Gnocchi.CapabilitiesList\n+ Gnocchi.MeasuresAggregation\n+ Gnocchi.MeasuresShow\n+ Gnocchi.MetricList\n+ Gnocchi.MetricCreate\n+ Gnocchi.MetricCreateDelete\n+ Gnocchi.ResourceTypeList\n+ Gnocchi.ResourceTypeCreate\n+ Gnocchi.ResourceTypeCreateDelete\n+ Gnocchi.ResourceList\n+ Gnocchi.ResourceCreate\n+ Gnocchi.ResourceCreateDelete\n+ Gnocchi.Status\n\nChange-Id: I554bdd680439f102f63590990ad2047e3bc548a9\n'}, {'number': 6, 'created': '2017-04-06 19:42:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/432ade252f8e242760ca7e139682cb4ec81319bc', 'message': '[WIP] Adding Gnocchi API Benchmarks\n\nNew Scenarios:\n+ Gnocchi.ArchivePolicyRuleList\n+ Gnocchi.ArchivePolicyRuleCreate\n+ Gnocchi.ArchivePolicyRuleCreateDelete\n+ Gnocchi.ArchivePolicyList\n+ Gnocchi.ArchivePolicyCreate\n+ Gnocchi.ArchivePolicyCreateDelete\n+ Gnocchi.CapabilitiesList\n+ Gnocchi.MeasuresAggregation\n+ Gnocchi.MeasuresShow\n+ Gnocchi.MetricList\n+ Gnocchi.MetricCreate\n+ Gnocchi.MetricCreateDelete\n+ Gnocchi.ResourceTypeList\n+ Gnocchi.ResourceTypeCreate\n+ Gnocchi.ResourceTypeCreateDelete\n+ Gnocchi.ResourceList\n+ Gnocchi.ResourceCreate\n+ Gnocchi.ResourceCreateDelete\n+ Gnocchi.Status\n\nChange-Id: I554bdd680439f102f63590990ad2047e3bc548a9\n'}, {'number': 7, 'created': '2017-04-07 12:40:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/cba6fbf6b7b3337c5b8ae84803b335c45bd86800', 'message': '[WIP] Adding Gnocchi API Benchmarks\n\nNew Scenarios:\n+ Gnocchi.ArchivePolicyRuleList\n+ Gnocchi.ArchivePolicyRuleCreate\n+ Gnocchi.ArchivePolicyRuleCreateDelete\n+ Gnocchi.ArchivePolicyList\n+ Gnocchi.ArchivePolicyCreate\n+ Gnocchi.ArchivePolicyCreateDelete\n+ Gnocchi.CapabilitiesList\n+ Gnocchi.MeasuresAggregation\n+ Gnocchi.MeasuresShow\n+ Gnocchi.MetricList\n+ Gnocchi.MetricCreate\n+ Gnocchi.MetricCreateDelete\n+ Gnocchi.ResourceTypeList\n+ Gnocchi.ResourceTypeCreate\n+ Gnocchi.ResourceTypeCreateDelete\n+ Gnocchi.ResourceList\n+ Gnocchi.ResourceCreate\n+ Gnocchi.ResourceCreateDelete\n+ Gnocchi.Status\n\nAdded Gnocchi job under rally_jobs\n\nChange-Id: I554bdd680439f102f63590990ad2047e3bc548a9\n'}, {'number': 8, 'created': '2017-04-07 15:53:39.000000000', 'files': ['rally/plugins/openstack/scenarios/gnocchi/archive_policy.py', 'samples/tasks/scenarios/gnocchi/metric-create.json', 'rally/plugins/openstack/scenarios/gnocchi/resource.py', 'samples/tasks/scenarios/gnocchi/resource-type-create.yaml', 'samples/tasks/scenarios/gnocchi/measures-aggregation.yaml', 'samples/tasks/scenarios/gnocchi/metric-list.json', 'samples/tasks/scenarios/gnocchi/resource-type-list.yaml', 'rally/plugins/openstack/context/gnocchi/__init__.py', 'samples/tasks/scenarios/gnocchi/metric-create-delete.yaml', 'rally-jobs/rally-gnocchi.yaml', 'samples/tasks/scenarios/gnocchi/resource-create-delete.yaml', 'samples/tasks/scenarios/gnocchi/status.json', 'samples/tasks/scenarios/gnocchi/resource-create.json', 'tests/unit/plugins/openstack/scenarios/gnocchi/test_utils.py', 'samples/tasks/scenarios/gnocchi/archive-policy-rule-create-delete.yaml', 'rally/plugins/openstack/scenarios/gnocchi/measures.py', 'tests/unit/plugins/openstack/scenarios/gnocchi/test_archive_policy.py', 'tests/unit/plugins/openstack/scenarios/gnocchi/test_capablities.py', 'samples/tasks/scenarios/gnocchi/resource-type-create-delete.yaml', 'tests/unit/plugins/openstack/scenarios/gnocchi/test_resource_type.py', 'tests/unit/plugins/openstack/scenarios/gnocchi/__init__.py', 'rally/plugins/openstack/scenarios/gnocchi/__init__.py', 'samples/tasks/scenarios/gnocchi/measures-show.yaml', 'samples/tasks/scenarios/gnocchi/archive-policy-rule-create-delete.json', 'samples/tasks/scenarios/gnocchi/archive-policy-list.yaml', 'samples/tasks/scenarios/gnocchi/status.yaml', 'rally/plugins/openstack/scenarios/gnocchi/utils.py', 'samples/tasks/scenarios/gnocchi/capabilities-list.json', 'tests/unit/plugins/openstack/scenarios/gnocchi/test_archive_policy_rule.py', 'samples/tasks/scenarios/gnocchi/archive-policy-create-delete.json', 'samples/tasks/scenarios/gnocchi/resource-type-list.json', 'rally/plugins/openstack/scenarios/gnocchi/status.py', 'tests/unit/plugins/openstack/scenarios/gnocchi/test_resource.py', 'samples/tasks/scenarios/gnocchi/archive-policy-rule-list.json', 'samples/tasks/scenarios/gnocchi/resource-list.json', 'samples/tasks/scenarios/gnocchi/archive-policy-create.yaml', 'samples/tasks/scenarios/gnocchi/metric-create.yaml', 'samples/tasks/scenarios/gnocchi/metric-list.yaml', 'samples/tasks/scenarios/gnocchi/resource-type-create.json', 'tests/unit/plugins/openstack/scenarios/gnocchi/test_status.py', 'samples/tasks/scenarios/gnocchi/archive-policy-rule-create.json', 'samples/tasks/scenarios/gnocchi/metric-create-delete.json', 'samples/tasks/scenarios/gnocchi/resource-create.yaml', 'rally/plugins/openstack/scenarios/gnocchi/capabilities.py', 'samples/tasks/scenarios/gnocchi/measures-aggregation.json', 'samples/tasks/scenarios/gnocchi/resource-type-create-delete.json', 'rally/plugins/openstack/context/gnocchi/metric.py', 'rally/plugins/openstack/scenarios/gnocchi/archive_policy_rule.py', 'samples/tasks/scenarios/gnocchi/resource-create-delete.json', 'samples/tasks/scenarios/gnocchi/archive-policy-list.json', 'samples/tasks/scenarios/gnocchi/resource-list.yaml', 'samples/tasks/scenarios/gnocchi/archive-policy-rule-create.yaml', 'tests/unit/plugins/openstack/scenarios/gnocchi/test_measures.py', 'samples/tasks/scenarios/gnocchi/capabilities-list.yaml', 'tests/unit/plugins/openstack/scenarios/gnocchi/test_metric.py', 'rally/plugins/openstack/scenarios/gnocchi/resource_type.py', 'samples/tasks/scenarios/gnocchi/measures-show.json', 'rally/plugins/openstack/scenarios/gnocchi/metric.py', 'samples/tasks/scenarios/gnocchi/archive-policy-rule-list.yaml', 'samples/tasks/scenarios/gnocchi/archive-policy-create-delete.yaml', 'samples/tasks/scenarios/gnocchi/archive-policy-create.json'], 'web_link': 'https://opendev.org/openstack/rally/commit/3f18a911e9f388ac5da802a03f3104a4ee243d1c', 'message': '[WIP] Adding Gnocchi API Benchmarks\n\nNew Scenarios:\n+ Gnocchi.ArchivePolicyRuleList\n+ Gnocchi.ArchivePolicyRuleCreate\n+ Gnocchi.ArchivePolicyRuleCreateDelete\n+ Gnocchi.ArchivePolicyList\n+ Gnocchi.ArchivePolicyCreate\n+ Gnocchi.ArchivePolicyCreateDelete\n+ Gnocchi.CapabilitiesList\n+ Gnocchi.MeasuresAggregation\n+ Gnocchi.MeasuresShow\n+ Gnocchi.MetricList\n+ Gnocchi.MetricCreate\n+ Gnocchi.MetricCreateDelete\n+ Gnocchi.ResourceTypeList\n+ Gnocchi.ResourceTypeCreate\n+ Gnocchi.ResourceTypeCreateDelete\n+ Gnocchi.ResourceList\n+ Gnocchi.ResourceCreate\n+ Gnocchi.ResourceCreateDelete\n+ Gnocchi.Status\n\nAdded Gnocchi job under rally_jobs\n\nChange-Id: I554bdd680439f102f63590990ad2047e3bc548a9\n'}]",8,453861,3f18a911e9f388ac5da802a03f3104a4ee243d1c,29,4,8,21143,,,0,"[WIP] Adding Gnocchi API Benchmarks

New Scenarios:
+ Gnocchi.ArchivePolicyRuleList
+ Gnocchi.ArchivePolicyRuleCreate
+ Gnocchi.ArchivePolicyRuleCreateDelete
+ Gnocchi.ArchivePolicyList
+ Gnocchi.ArchivePolicyCreate
+ Gnocchi.ArchivePolicyCreateDelete
+ Gnocchi.CapabilitiesList
+ Gnocchi.MeasuresAggregation
+ Gnocchi.MeasuresShow
+ Gnocchi.MetricList
+ Gnocchi.MetricCreate
+ Gnocchi.MetricCreateDelete
+ Gnocchi.ResourceTypeList
+ Gnocchi.ResourceTypeCreate
+ Gnocchi.ResourceTypeCreateDelete
+ Gnocchi.ResourceList
+ Gnocchi.ResourceCreate
+ Gnocchi.ResourceCreateDelete
+ Gnocchi.Status

Added Gnocchi job under rally_jobs

Change-Id: I554bdd680439f102f63590990ad2047e3bc548a9
",git fetch https://review.opendev.org/openstack/rally refs/changes/61/453861/1 && git format-patch -1 --stdout FETCH_HEAD,"['samples/tasks/scenarios/gnocchi/metric-create.json', 'rally/plugins/openstack/scenarios/gnocchi/resource.py', 'samples/tasks/scenarios/gnocchi/resource-type-create.yaml', 'rally/plugins/openstack/scenarios/gnocchi/resource-type.py', 'samples/tasks/scenarios/gnocchi/measures-aggregation.yaml', 'samples/tasks/scenarios/gnocchi/metric-list.json', 'samples/tasks/scenarios/gnocchi/resource-type-list.yaml', 'rally/plugins/openstack/context/gnocchi/__init__.py', 'samples/tasks/scenarios/gnocchi/metric-create-delete.yaml', 'samples/tasks/scenarios/gnocchi/resource-create-delete.yaml', 'samples/tasks/scenarios/gnocchi/status.json', 'samples/tasks/scenarios/gnocchi/resource-create.json', 'samples/tasks/scenarios/gnocchi/archive-policy-rule-create-delete.yaml', 'rally/plugins/openstack/scenarios/gnocchi/measures.py', 'samples/tasks/scenarios/gnocchi/resource-type-create-delete.yaml', 'rally/plugins/openstack/scenarios/gnocchi/__init__.py', 'samples/tasks/scenarios/gnocchi/measures-show.yaml', 'samples/tasks/scenarios/gnocchi/archive-policy-rule-create-delete.json', 'samples/tasks/scenarios/gnocchi/archive-policy-list.yaml', 'samples/tasks/scenarios/gnocchi/status.yaml', 'rally/plugins/openstack/scenarios/gnocchi/utils.py', 'samples/tasks/scenarios/gnocchi/archive-policy-create-delete.json', 'samples/tasks/scenarios/gnocchi/resource-type-list.json', 'rally/plugins/openstack/scenarios/gnocchi/status.py', 'samples/tasks/scenarios/gnocchi/archive-policy-rule-list.json', 'samples/tasks/scenarios/gnocchi/resource-list.json', 'samples/tasks/scenarios/gnocchi/capablities-list.yaml', 'samples/tasks/scenarios/gnocchi/archive-policy-create.yaml', 'samples/tasks/scenarios/gnocchi/metric-create.yaml', 'samples/tasks/scenarios/gnocchi/metric-list.yaml', 'samples/tasks/scenarios/gnocchi/resource-type-create.json', 'samples/tasks/scenarios/gnocchi/archive-policy-rule-create.json', 'samples/tasks/scenarios/gnocchi/metric-create-delete.json', 'rally/plugins/openstack/scenarios/gnocchi/archive-policy-rule.py', 'samples/tasks/scenarios/gnocchi/resource-create.yaml', 'samples/tasks/scenarios/gnocchi/measures-aggregation.json', 'samples/tasks/scenarios/gnocchi/resource-type-create-delete.json', 'rally/plugins/openstack/scenarios/gnocchi/capablities.py', 'samples/tasks/scenarios/gnocchi/resource-create-delete.json', 'samples/tasks/scenarios/gnocchi/archive-policy-list.json', 'samples/tasks/scenarios/gnocchi/resource-list.yaml', 'samples/tasks/scenarios/gnocchi/archive-policy-rule-create.yaml', 'samples/tasks/scenarios/gnocchi/measures-show.json', 'rally/plugins/openstack/scenarios/gnocchi/metric.py', 'samples/tasks/scenarios/gnocchi/archive-policy-rule-list.yaml', 'samples/tasks/scenarios/gnocchi/capablities-list.json', 'rally/plugins/openstack/context/gnocchi/metrics.py', 'rally/plugins/openstack/scenarios/gnocchi/archive-policy.py', 'samples/tasks/scenarios/gnocchi/archive-policy-create-delete.yaml', 'samples/tasks/scenarios/gnocchi/archive-policy-create.json']",50,f5c29ca264d38bf08ae42b317925ec05e1e16590,gnocchi_scenarios,"{ ""Gnocchi.archive_policy_create"": [ { ""runner"": { ""type"": ""constant"", ""concurrency"": 2, ""times"": 10 }, ""args"": {}, ""context"": {} } ] }",,1015,0
openstack%2Fnetworking-odl~master~Idf2a400b8016dde7d7d60c80bb34e73a6b71b1fb,openstack/networking-odl,master,Idf2a400b8016dde7d7d60c80bb34e73a6b71b1fb,Ignore failure of a resource sync,ABANDONED,2017-11-16 11:00:03.000000000,2017-12-18 03:45:54.000000000,,"[{'_account_id': 333}, {'_account_id': 13912}, {'_account_id': 22348}, {'_account_id': 26507}]","[{'number': 1, 'created': '2017-11-16 11:00:03.000000000', 'files': ['networking_odl/tests/unit/journal/test_full_sync.py', 'networking_odl/journal/full_sync.py'], 'web_link': 'https://opendev.org/openstack/networking-odl/commit/d71d9476c2437f5df093256d2718979d8d95ec5f', 'message': 'Ignore failure of a resource sync\n\nif there is some problem in sync of a resource then full sync stops\nat that point and does not sync remaining resources. This patch\nignores the failures and continue to sync other resources.\n\nChange-Id: Idf2a400b8016dde7d7d60c80bb34e73a6b71b1fb\n'}]",7,520387,d71d9476c2437f5df093256d2718979d8d95ec5f,8,4,1,13912,,,0,"Ignore failure of a resource sync

if there is some problem in sync of a resource then full sync stops
at that point and does not sync remaining resources. This patch
ignores the failures and continue to sync other resources.

Change-Id: Idf2a400b8016dde7d7d60c80bb34e73a6b71b1fb
",git fetch https://review.opendev.org/openstack/networking-odl refs/changes/87/520387/1 && git format-patch -1 --stdout FETCH_HEAD,"['networking_odl/tests/unit/journal/test_full_sync.py', 'networking_odl/journal/full_sync.py']",2,d71d9476c2437f5df093256d2718979d8d95ec5f,dont_stop_on_fail,"from oslo_log import log as loggingLOG = logging.getLogger(__name__) try: _sync_resources(context, resource_type, handler) except Exception: LOG.exception(""Full sync for %s resource failed"", resource_type, exc_info=True)"," _sync_resources(context, resource_type, handler)",17,5
openstack%2Fnetworking-odl~master~Ibb73d206024a4063678229cf8d2db8d04fce84f9,openstack/networking-odl,master,Ibb73d206024a4063678229cf8d2db8d04fce84f9,Logs failure of journal record,ABANDONED,2017-11-22 06:59:58.000000000,2017-12-18 03:43:56.000000000,,"[{'_account_id': 13912}, {'_account_id': 22348}, {'_account_id': 26507}]","[{'number': 1, 'created': '2017-11-22 06:59:58.000000000', 'files': ['networking_odl/journal/journal.py'], 'web_link': 'https://opendev.org/openstack/networking-odl/commit/94aaaa89f23034c0aeb8fb129b0ec8cfde489fa6', 'message': 'Logs failure of journal record\n\nCurrently, if we have failure in recording of journal entry then\nnothing is logged, it makes debugging difficult with very generic\nmessage. This patch adds logging to make it easier to debug.\n\nChange-Id: Ibb73d206024a4063678229cf8d2db8d04fce84f9\n'}]",3,522138,94aaaa89f23034c0aeb8fb129b0ec8cfde489fa6,6,3,1,13912,,,0,"Logs failure of journal record

Currently, if we have failure in recording of journal entry then
nothing is logged, it makes debugging difficult with very generic
message. This patch adds logging to make it easier to debug.

Change-Id: Ibb73d206024a4063678229cf8d2db8d04fce84f9
",git fetch https://review.opendev.org/openstack/networking-odl refs/changes/38/522138/1 && git format-patch -1 --stdout FETCH_HEAD,['networking_odl/journal/journal.py'],1,94aaaa89f23034c0aeb8fb129b0ec8cfde489fa6,add_log_statement," LOG.debug(""Failed to record journal entry"", exc_info=True)",,1,0
openstack%2Fnetworking-odl~master~Ic3b7822567ec7b4e896f22bfdb8f1fc30236ed5b,openstack/networking-odl,master,Ic3b7822567ec7b4e896f22bfdb8f1fc30236ed5b,[discussion] Moving journal sync timer use case to periodic task,ABANDONED,2017-11-07 07:00:33.000000000,2017-12-18 03:41:42.000000000,,"[{'_account_id': 7921}, {'_account_id': 13912}, {'_account_id': 22348}, {'_account_id': 26507}]","[{'number': 1, 'created': '2017-11-07 07:00:33.000000000', 'files': ['networking_odl/journal/worker.py', 'networking_odl/journal/journal.py'], 'web_link': 'https://opendev.org/openstack/networking-odl/commit/b64b4daf96cade6efe933880c3361da801f036a6', 'message': '[discussion] Moving journal sync timer use case to periodic task\n\nJust to give idea, after current proposal, code will look something\nlike this. Complete implementation can be done after agreement.\n\nIt includes db locking for sync, so it might have impact on\nperformance.\n\nIt introduces simiplicity in the code.\n\nChange-Id: Ic3b7822567ec7b4e896f22bfdb8f1fc30236ed5b\n'}]",0,518255,b64b4daf96cade6efe933880c3361da801f036a6,5,4,1,13912,,,0,"[discussion] Moving journal sync timer use case to periodic task

Just to give idea, after current proposal, code will look something
like this. Complete implementation can be done after agreement.

It includes db locking for sync, so it might have impact on
performance.

It introduces simiplicity in the code.

Change-Id: Ic3b7822567ec7b4e896f22bfdb8f1fc30236ed5b
",git fetch https://review.opendev.org/openstack/networking-odl refs/changes/55/518255/1 && git format-patch -1 --stdout FETCH_HEAD,"['networking_odl/journal/worker.py', 'networking_odl/journal/journal.py']",2,b64b4daf96cade6efe933880c3361da801f036a6,creating_periodic_task_discussion,," self.event = threading.Event() self._odl_sync_thread = self._create_odl_sync_thread() self._odl_sync_thread_stop = threading.Event() if start_thread: self.start() def _create_odl_sync_thread(self): return threading.Thread(name='sync', target=self.run_sync_thread) def start(self): # Start the sync thread LOG.debug(""Starting a new sync thread"") if self._odl_sync_thread_stop.is_set(): self._odl_sync_thread_stop.clear() self._odl_sync_thread = self._create_odl_sync_thread() if not self._odl_sync_thread.is_alive(): self._odl_sync_thread.start() def stop(self, timeout=None): """"""Allows to stop the sync thread. Args: timeout (float): Time in seconds to wait for joining Returns: bool: True for success in stopping, False otherwise. """""" # Stop the sync thread LOG.debug(""Stopping the sync thread"") if self._odl_sync_thread.is_alive(): self._odl_sync_thread_stop.set() # Process the journal one last time before stopping. self.set_sync_event() self._odl_sync_thread.join(timeout) return not self._odl_sync_thread.is_alive() def set_sync_event(self): self.event.set() def run_sync_thread(self): while not self._odl_sync_thread_stop.is_set(): try: self.event.wait() self.event.clear() self.sync_pending_entries() except Exception: # Catch exceptions to protect the thread while running LOG.exception(""Error on run_sync_thread"") ",8,51
openstack%2Fkolla-ansible~master~I62512dc022426cc762ff603d8554e48651fa621f,openstack/kolla-ansible,master,I62512dc022426cc762ff603d8554e48651fa621f,"The notify ""Restart keystone containers"" is not correct",MERGED,2017-06-22 01:03:26.000000000,2017-12-18 03:33:20.000000000,2017-06-26 08:08:10.000000000,"[{'_account_id': 3}, {'_account_id': 894}, {'_account_id': 8157}, {'_account_id': 19316}, {'_account_id': 23717}]","[{'number': 1, 'created': '2017-06-22 01:03:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/d4b76023e2826a74f501f73ad57ed7b0488e567a', 'message': 'No handler named ""Restart keystone containers""\n\nWe should restart the keystone container and the keystone-fernet\ncontainer according to the context\n\nChange-Id: I62512dc022426cc762ff603d8554e48651fa621f\n'}, {'number': 2, 'created': '2017-06-22 10:08:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/f2b6f0e0489f8a4e2d39044e7fa6fd2119f0aa10', 'message': 'The notify ""Restart keystone containers"" is not correct\n\nNo handler named ""Restart keystone containers"", and we should restart\nthe keystone and the keystone-fernet container according to the context\n\nChange-Id: I62512dc022426cc762ff603d8554e48651fa621f\n'}, {'number': 3, 'created': '2017-06-23 00:52:06.000000000', 'files': ['ansible/roles/keystone/tasks/config.yml'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/77358dd920c1abab64d111496d94a77b9e5ca6c3', 'message': 'The notify ""Restart keystone containers"" is not correct\n\nNo handler named ""Restart keystone containers"", and we should restart\nthe keystone and the keystone-fernet container according to the context\n\nCloses-Bug: #1699924\n\nChange-Id: I62512dc022426cc762ff603d8554e48651fa621f\n'}]",1,476298,77358dd920c1abab64d111496d94a77b9e5ca6c3,18,5,3,22037,,,0,"The notify ""Restart keystone containers"" is not correct

No handler named ""Restart keystone containers"", and we should restart
the keystone and the keystone-fernet container according to the context

Closes-Bug: #1699924

Change-Id: I62512dc022426cc762ff603d8554e48651fa621f
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/98/476298/2 && git format-patch -1 --stdout FETCH_HEAD,['ansible/roles/keystone/tasks/config.yml'],1,d4b76023e2826a74f501f73ad57ed7b0488e567a,, - Restart keystone container - Restart keystone-fernet container, - Restart keystone containers,2,1
openstack%2Fironic~master~Icb754b162adb2f2709bf8db6890b3935b0cfc3dc,openstack/ironic,master,Icb754b162adb2f2709bf8db6890b3935b0cfc3dc,Allow get detailed VIF information,NEW,2017-01-17 21:50:39.000000000,2017-12-18 03:31:10.000000000,,"[{'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 14525}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 24828}]","[{'number': 1, 'created': '2017-01-17 21:50:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/23980c129d24570ef5a0eb200b739772ef47e12a', 'message': 'WIP: Allow to specify VIF fields and interface_type\n\nTODO....\n\nChange-Id: Icb754b162adb2f2709bf8db6890b3935b0cfc3dc\n'}, {'number': 2, 'created': '2017-01-30 17:53:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f8fe6d2a0c02918a93d5df70ef653b6c44d01388', 'message': 'WIP: Allow to VIF detail fields\n\nThis patch allows to get a list of VIFs attached to a node with details\nby adding detaul=True to vif_list request.\nAllow to specify interface_type when requesting vif_attach by adding\ninterface_type={port|portgroup|Unset}. If unset default behavior is\nkept (portgroups are picked with higher preference than ports.)\n\nBump API version to 1.29, RPC api version to 1.40.\n\nCloses-Bug: #1657170\n\nChange-Id: Icb754b162adb2f2709bf8db6890b3935b0cfc3dc\n'}, {'number': 3, 'created': '2017-02-01 14:25:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/df33b2ebdbc198c6e87e5123e94b098c5dfd6c72', 'message': 'Allow get detailed VIF information\n\nThis patch allows to get a detailed VIF information when getting list of\nattached VIFs to a node by adding detail=True parameter.\nAlso allow to strictly specify interface_type when requesting vif_attach\nby adding interface_type={port|portgroup}. If interface_type is not set\nin the request, default behavior is kept (portgroups are picked with\nhigher preference than ports.)\n\nBump API version to 1.32, RPC api version to 1.41.\n\nCloses-Bug: #1657170\n\nChange-Id: Icb754b162adb2f2709bf8db6890b3935b0cfc3dc\n'}, {'number': 4, 'created': '2017-03-31 13:48:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a1c588110a0c9628928f35f6cd7e7d2e19300a99', 'message': 'Allow get detailed VIF information\n\nThis patch allows to get a detailed VIF information when getting list of\nattached VIFs to a node by adding detail=True parameter.\nAlso allow to strictly specify interface_type when requesting vif_attach\nby adding interface_type={port|portgroup}. If interface_type is not set\nin the request, default behavior is kept (portgroups are picked with\nhigher preference than ports.)\n\nBump API version to 1.32, RPC api version to 1.41.\n\nCloses-Bug: #1657170\n\nChange-Id: Icb754b162adb2f2709bf8db6890b3935b0cfc3dc\n'}, {'number': 5, 'created': '2017-03-31 14:21:26.000000000', 'files': ['ironic/tests/unit/drivers/modules/network/test_common.py', 'ironic/tests/unit/conductor/test_rpcapi.py', 'ironic/drivers/base.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/tests/unit/drivers/test_base.py', 'ironic/tests/unit/api/v1/test_utils.py', 'ironic/conductor/rpcapi.py', 'ironic/conductor/manager.py', 'ironic/tests/unit/api/v1/test_nodes.py', 'ironic/api/controllers/v1/versions.py', 'ironic/drivers/modules/network/common.py', 'releasenotes/notes/allow-detail-vif-fields-190d2961073a055a.yaml', 'ironic/api/controllers/v1/utils.py', 'ironic/common/release_mappings.py', 'ironic/api/controllers/v1/node.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/67b400112cae5f87e97d1c8f075350f7d4f8a160', 'message': 'Allow get detailed VIF information\n\nThis patch allows to get a detailed VIF information when getting list of\nattached VIFs to a node by adding detail=True parameter.\nAlso allow to strictly specify interface_type when requesting vif_attach\nby adding interface_type={port|portgroup}. If interface_type is not set\nin the request, default behavior is kept (portgroups are picked with\nhigher preference than ports.)\n\nBump API version to 1.32, RPC api version to 1.41.\n\nCloses-Bug: #1657170\n\nChange-Id: Icb754b162adb2f2709bf8db6890b3935b0cfc3dc\n'}]",1,421544,67b400112cae5f87e97d1c8f075350f7d4f8a160,34,8,5,14525,,,0,"Allow get detailed VIF information

This patch allows to get a detailed VIF information when getting list of
attached VIFs to a node by adding detail=True parameter.
Also allow to strictly specify interface_type when requesting vif_attach
by adding interface_type={port|portgroup}. If interface_type is not set
in the request, default behavior is kept (portgroups are picked with
higher preference than ports.)

Bump API version to 1.32, RPC api version to 1.41.

Closes-Bug: #1657170

Change-Id: Icb754b162adb2f2709bf8db6890b3935b0cfc3dc
",git fetch https://review.opendev.org/openstack/ironic refs/changes/44/421544/4 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/conductor/manager.py', 'ironic/api/controllers/v1/versions.py', 'ironic/drivers/base.py', 'ironic/drivers/modules/network/common.py', 'ironic/api/controllers/v1/utils.py', 'ironic/api/controllers/v1/node.py', 'ironic/conductor/rpcapi.py']",7,23980c129d24570ef5a0eb200b739772ef47e12a,bug/1657170," def vif_list(self, context, node_id, fields=None, topic=None): :param fields: List of vif fields to return. return cctxt.call(context, 'vif_list', node_id=node_id, fields=fields)"," def vif_list(self, context, node_id, topic=None): return cctxt.call(context, 'vif_list', node_id=node_id)",57,21
openstack%2Ftacker~master~I8bf07e630098ae474cf36f3fabbf212481073f79,openstack/tacker,master,I8bf07e630098ae474cf36f3fabbf212481073f79,[WIP] Unify OpenStack VIM driver,NEW,2017-03-27 23:33:03.000000000,2017-12-18 03:31:02.000000000,,"[{'_account_id': 13380}, {'_account_id': 19644}]","[{'number': 1, 'created': '2017-03-27 23:33:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/8965554c5950ca7f28c7aa9465b6f61893aaa683', 'message': '[WIP] Unify OpenStack VIM driver\n\nCurrently OpenStack VIM driver code is spread across\nVNFM and NFVO layers. This is not the best way to organize\ncode in Tacker and it is confusing for new developers\nAlso this state make adding new VIM target type (like VMware)\ndifficult.\n\nThis patchset will move all the target VIM operations code\nunder a top level vim driver. This driver will provide, just\nlike it is now, concrete implementation for all abstract plugin\ninterfaces like VNFM, VNFFFG, NFVO.\n\nImplements: blueprint unified-openstack-vim-driver\nChange-Id: I8bf07e630098ae474cf36f3fabbf212481073f79\n'}, {'number': 2, 'created': '2017-03-29 00:40:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/1266cb1a49c4c1f7d7db5d9784e4c1f6b722efca', 'message': '[WIP] Unify OpenStack VIM driver\n\nCurrently OpenStack VIM driver code is spread across\nVNFM and NFVO layers. This is not the best way to organize\ncode in Tacker and it is confusing for new developers\nAlso this state make adding new VIM target type (like VMware)\ndifficult.\n\nThis patchset will move all the target VIM operations code\nunder a top level vim driver. This driver will provide, just\nlike it is now, concrete implementation for all abstract plugin\ninterfaces like VNFM, VNFFFG, NFVO.\n\nImplements: blueprint unified-openstack-vim-driver\nChange-Id: I8bf07e630098ae474cf36f3fabbf212481073f79\n'}, {'number': 3, 'created': '2017-03-31 21:14:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/d5b09963606ba7deae40960b432085bf8f68a4e3', 'message': '[WIP] Unify OpenStack VIM driver\n\nCurrently OpenStack VIM driver code is spread across\nVNFM and NFVO layers. This is not the best way to organize\ncode in Tacker and it is confusing for new developers\nAlso this state make adding new VIM target type (like VMware)\ndifficult.\n\nThis patchset will move all the target VIM operations code\nunder a top level vim driver. This driver will provide, just\nlike it is now, concrete implementation for all abstract plugin\ninterfaces like VNFM, VNFFFG, NFVO.\n\nImplements: blueprint unified-openstack-vim-driver\nChange-Id: I8bf07e630098ae474cf36f3fabbf212481073f79\n'}, {'number': 4, 'created': '2017-04-11 22:25:55.000000000', 'files': ['tacker/tests/unit/nfvo/drivers/vnffg/sfc_drivers/networking-sfc/test_n_sfc.py', 'tacker/tests/unit/vim/drivers/__init__.py', 'tacker/tests/unit/vim/__init__.py', 'tacker/vim/drivers/__init__.py', 'tacker/tests/unit/vim/drivers/test_openstack_driver.py', 'tacker/vim/__init__.py', 'tacker/nfvo/nfvo_plugin.py', 'tacker/vim/drivers/openstack/__init__.py', 'setup.cfg', 'tacker/vim/drivers/openstack/openstack_driver.py', 'tacker/vim/drivers/abstract_vim_driver.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/bbf30d949a90586e1f1e3703057598524d780c9c', 'message': '[WIP] Unify OpenStack VIM driver\n\nCurrently OpenStack VIM driver code is spread across\nVNFM and NFVO layers. This is not the best way to organize\ncode in Tacker and it is confusing for new developers\nAlso this state make adding new VIM target type (like VMware)\ndifficult.\n\nThis patchset will move all the target VIM operations code\nunder a top level vim driver. This driver will provide, just\nlike it is now, concrete implementation for all abstract plugin\ninterfaces like VNFM, VNFFFG, NFVO.\n\nImplements: blueprint unified-openstack-vim-driver\nChange-Id: I8bf07e630098ae474cf36f3fabbf212481073f79\n'}]",2,450487,bbf30d949a90586e1f1e3703057598524d780c9c,12,2,4,13380,,,0,"[WIP] Unify OpenStack VIM driver

Currently OpenStack VIM driver code is spread across
VNFM and NFVO layers. This is not the best way to organize
code in Tacker and it is confusing for new developers
Also this state make adding new VIM target type (like VMware)
difficult.

This patchset will move all the target VIM operations code
under a top level vim driver. This driver will provide, just
like it is now, concrete implementation for all abstract plugin
interfaces like VNFM, VNFFFG, NFVO.

Implements: blueprint unified-openstack-vim-driver
Change-Id: I8bf07e630098ae474cf36f3fabbf212481073f79
",git fetch https://review.opendev.org/openstack/tacker refs/changes/87/450487/4 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/vim/drivers/__init__.py', 'tacker/vim/__init__.py', 'tacker/vim/drivers/openstack/__init__.py', 'tacker/vim/drivers/openstack/openstack_driver.py']",4,8965554c5950ca7f28c7aa9465b6f61893aaa683,bp/unified-openstack-vim-driver,,,0,0
openstack%2Fironic~master~I8adee65fd2742ab915cd69daa2f5769955fe0f42,openstack/ironic,master,I8adee65fd2742ab915cd69daa2f5769955fe0f42,Add ability to time out on individual clean steps,NEW,2016-10-28 20:57:11.000000000,2017-12-18 03:30:55.000000000,,"[{'_account_id': 7711}, {'_account_id': 10118}, {'_account_id': 10239}, {'_account_id': 12356}, {'_account_id': 13295}, {'_account_id': 14525}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}]","[{'number': 1, 'created': '2016-10-28 20:57:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8d7bf37501f2e6b4bbf45a8fc9dcf79799f7295e', 'message': '[WIP] Add ability to time out on individual clean steps\n\nThis adds the ability to specify timeouts for individual clean steps.\n\nChange-Id: I8adee65fd2742ab915cd69daa2f5769955fe0f42\nCloses-Bug: #1611137\n'}, {'number': 2, 'created': '2016-10-31 21:24:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7eea5af5591b0771241aff63c33c20d0c1a08b36', 'message': '[WIP] Add ability to time out on individual clean steps\n\nThis adds the ability to specify timeouts for individual clean steps.\n\nChange-Id: I8adee65fd2742ab915cd69daa2f5769955fe0f42\nCloses-Bug: #1611137\n'}, {'number': 3, 'created': '2016-11-01 20:11:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/04dcd4f5c6b688159943fe1da6cb3e73cfa2955a', 'message': 'Add ability to time out on individual clean steps\n\nThis adds the ability to specify timeouts for individual clean steps.\n\nChange-Id: I8adee65fd2742ab915cd69daa2f5769955fe0f42\nCloses-Bug: #1611137\n'}, {'number': 4, 'created': '2016-11-01 21:53:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/30a1a20ba07b76b36c8b756a86b32e69d0d587d8', 'message': 'Add ability to time out on individual clean steps\n\nThis adds the ability to specify timeouts for individual clean steps.\n\nChange-Id: I8adee65fd2742ab915cd69daa2f5769955fe0f42\nCloses-Bug: #1611137\n'}, {'number': 5, 'created': '2016-11-02 20:42:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/593a9c9862037ccf2ebfd67b4c4f7e822c9b8008', 'message': 'Add ability to time out on individual clean steps\n\nThis adds the ability to specify timeouts for individual clean steps.\n\nChange-Id: I8adee65fd2742ab915cd69daa2f5769955fe0f42\nCloses-Bug: #1611137\n'}, {'number': 6, 'created': '2016-11-16 19:38:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ac35d7dca8429e5c923699d187c2b684ca393e44', 'message': 'Add ability to time out on individual clean steps\n\nThis adds the ability to specify timeouts for individual clean steps.\n\nChange-Id: I8adee65fd2742ab915cd69daa2f5769955fe0f42\nCloses-Bug: #1611137\n'}, {'number': 7, 'created': '2016-11-17 17:06:57.000000000', 'files': ['releasenotes/notes/add-clean-step-timeouts-745b780fc274bc6d.yaml', 'ironic/common/exception.py', 'ironic/conductor/manager.py', 'doc/source/deploy/cleaning.rst', 'ironic/drivers/base.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/conductor/utils.py', 'ironic/tests/unit/drivers/test_base.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/71e34e0451fc4e236c4b302440a497f3c6661f2a', 'message': 'Add ability to time out on individual clean steps\n\nThis adds the ability to specify timeouts for individual clean steps.\n\nChange-Id: I8adee65fd2742ab915cd69daa2f5769955fe0f42\nCloses-Bug: #1611137\n'}]",19,391554,71e34e0451fc4e236c4b302440a497f3c6661f2a,56,10,7,13295,,,0,"Add ability to time out on individual clean steps

This adds the ability to specify timeouts for individual clean steps.

Change-Id: I8adee65fd2742ab915cd69daa2f5769955fe0f42
Closes-Bug: #1611137
",git fetch https://review.opendev.org/openstack/ironic refs/changes/54/391554/7 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/conductor/manager.py', 'ironic/drivers/base.py', 'ironic/conductor/utils.py']",3,8d7bf37501f2e6b4bbf45a8fc9dcf79799f7295e,bug/1611137,# TODO(mariojv) Rename me if you overload this to be used for both sync and # async timeouts. Might want a separate method for different last_error though,,85,13
openstack%2Ftacker~master~I0f43a3f52ed7a71239ede3b77070b5fa858e13da,openstack/tacker,master,I0f43a3f52ed7a71239ede3b77070b5fa858e13da,Alarm monitor: Monasca support in Tacker,NEW,2016-12-27 12:26:31.000000000,2017-12-18 03:30:38.000000000,,"[{'_account_id': 2874}, {'_account_id': 10487}, {'_account_id': 13380}, {'_account_id': 13485}, {'_account_id': 18955}, {'_account_id': 19644}]","[{'number': 1, 'created': '2016-12-27 12:26:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/e26fa442d058274ff1e9328c805b3fcaf232ff7e', 'message': '[WIP]Alarm monitor: Monasca support in Tacker\n\nAlarm monitor intially integrated with OpenStack Ceilometer. This patch is proposed\nto leverage Monasca in Tacker. OpenStack Monasca is a highly scalable, performany,\nfault-tolerant monitoring solution.\nMonasca support is basically similar to Ceilometer support in Tacker. Currently,\nMonasca-based auto-scaling has been introduced in Heat. Therefore, the main motivation\nis to leverage  Monasca resources in Heat. Monasca will be responsible for monitoring\nand the backend policies like auto-scaling will be handled by Tacker.\n\nChange-Id: I0f43a3f52ed7a71239ede3b77070b5fa858e13da\n'}, {'number': 2, 'created': '2016-12-28 16:53:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/61fd1823ce362beb845076c83686983dbf0af48e', 'message': '[WIP]Alarm monitor: Monasca support in Tacker\n\nAlarm monitor initially integrated with OpenStack Ceilometer.\nThis patch is proposed to leverage Monasca in Tacker.\nOpenStack Monasca is a highly scalable, performany, fault-tolerant\nmonitoring solution. Monasca support is basically similar\nto Ceilometer support in Tacker. Currently, Monasca-based auto-scaling\nhas been introduced in Heat. Therefore, the main motivation\nis to leverage  Monasca resources in Heat. Monasca will be\nresponsible for monitoring and the backend policies like\nauto-scaling will be handled by Tacker.\n\nImplements blueprint: #monasca-alarm-monitor\nChange-Id: I0f43a3f52ed7a71239ede3b77070b5fa858e13da\n'}, {'number': 3, 'created': '2017-01-03 18:09:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/e34778efa6c192210e39384e700a654a9a2d4095', 'message': '[WIP] Alarm monitor: Monasca support in Tacker\n\nAlarm monitor initially integrated with OpenStack Ceilometer.\nThis patch is proposed to leverage Monasca in Tacker.\nOpenStack Monasca is a highly scalable, performant, fault-tolerant\nmonitoring solution. Monasca support is basically similar\nto Ceilometer support in Tacker. Currently, Monasca-based auto-scaling\nhas been introduced in Heat. Therefore, the main motivation\nis to leverage  Monasca resources in Heat. Monasca will be\nresponsible for monitoring and the backend policies like\nauto-scaling will be handled by Tacker.\n\nImplements blueprint: #monasca-alarm-monitor\nChange-Id: I0f43a3f52ed7a71239ede3b77070b5fa858e13da\n'}, {'number': 4, 'created': '2017-01-04 05:11:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/7af81f2825842e7a90332efef2c6b5cc0727f3e4', 'message': '[WIP] Alarm monitor: Monasca support in Tacker\n\nAlarm monitor initially integrated with OpenStack Ceilometer.\nThis patch is proposed to leverage Monasca in Tacker.\nOpenStack Monasca is a highly scalable, performant, fault-tolerant\nmonitoring solution. Monasca support is basically similar\nto Ceilometer support in Tacker. Currently, Monasca-based auto-scaling\nhas been introduced in Heat. Therefore, the main motivation\nis to leverage  Monasca resources in Heat. Monasca will be\nresponsible for monitoring and the backend policies like\nauto-scaling will be handled by Tacker.\n\nImplements blueprint: #monasca-alarm-monitor\nChange-Id: I0f43a3f52ed7a71239ede3b77070b5fa858e13da\n'}, {'number': 5, 'created': '2017-01-13 04:52:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/02e3b8bed4bad0b62709b999ab9e992b66eef2b4', 'message': 'Alarm monitor: Monasca support in Tacker\n\nAlarm monitor initially integrated with OpenStack Ceilometer.\nThis patch is proposed to leverage Monasca in Tacker.\nOpenStack Monasca is a highly scalable, performant, fault-tolerant\nmonitoring solution. Monasca support is basically similar\nto Ceilometer support in Tacker. Currently, Monasca-based auto-scaling\nhas been introduced in Heat. Therefore, the main motivation\nis to leverage  Monasca resources in Heat. Monasca will be\nresponsible for monitoring and the backend policies like\nauto-scaling will be handled by Tacker.\n\nImplements blueprint: #monasca-alarm-monitor\nChange-Id: I0f43a3f52ed7a71239ede3b77070b5fa858e13da\n'}, {'number': 6, 'created': '2017-02-07 06:48:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/1a075f325435c3c2ef7688555a00690a0f3cfd46', 'message': 'Alarm monitor: Monasca support in Tacker\n\nAlarm monitor initially integrated with OpenStack Ceilometer.\nThis patch is proposed to leverage Monasca in Tacker.\nOpenStack Monasca is a highly scalable, performant, fault-tolerant\nmonitoring solution. Monasca support is basically similar\nto Ceilometer support in Tacker. Currently, Monasca-based auto-scaling\nhas been introduced in Heat. Therefore, the main motivation\nis to leverage  Monasca resources in Heat. Monasca will be\nresponsible for monitoring and the backend policies like\nauto-scaling will be handled by Tacker.\n\nImplements blueprint: #monasca-alarm-monitor\nChange-Id: I0f43a3f52ed7a71239ede3b77070b5fa858e13da\n'}, {'number': 7, 'created': '2017-03-23 12:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/b4f26188a4d22570c20020f7f86a3cccec190794', 'message': '[WIP]Alarm monitor: Monasca support in Tacker\n\nAlarm monitor initially integrated with OpenStack Ceilometer.\nThis patch is proposed to leverage Monasca in Tacker.\nOpenStack Monasca is a highly scalable, performant, fault-tolerant\nmonitoring solution. Monasca support is basically similar\nto Ceilometer support in Tacker. Currently, Monasca-based auto-scaling\nhas been introduced in Heat. Therefore, the main motivation\nis to leverage  Monasca resources in Heat. Monasca will be\nresponsible for monitoring and the backend policies like\nauto-scaling will be handled by Tacker.\n\nImplements blueprint: #monasca-alarm-monitor\nChange-Id: I0f43a3f52ed7a71239ede3b77070b5fa858e13da\n'}, {'number': 8, 'created': '2017-03-23 15:26:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/143671d9736dab072fecc27108bf0ec5a5a74930', 'message': '[WIP]Alarm monitor: Monasca support in Tacker\n\nAlarm monitor initially integrated with OpenStack Ceilometer.\nThis patch is proposed to leverage Monasca in Tacker.\nOpenStack Monasca is a highly scalable, performant, fault-tolerant\nmonitoring solution. Monasca support is basically similar\nto Ceilometer support in Tacker. Currently, Monasca-based auto-scaling\nhas been introduced in Heat. Therefore, the main motivation\nis to leverage  Monasca resources in Heat. Monasca will be\nresponsible for monitoring and the backend policies like\nauto-scaling will be handled by Tacker.\n\nImplements blueprint: #monasca-alarm-monitor\nChange-Id: I0f43a3f52ed7a71239ede3b77070b5fa858e13da\n'}, {'number': 9, 'created': '2017-04-10 08:57:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/d62efd3b21166b99f7aee1ae94a52a8aea280499', 'message': 'Alarm monitor: Monasca support in Tacker\n\nAlarm monitor initially integrated with OpenStack Ceilometer.\nThis patch is proposed to leverage Monasca in Tacker.\nOpenStack Monasca is a highly scalable, performant, fault-tolerant\nmonitoring solution. Monasca support is basically similar\nto Ceilometer support in Tacker. Currently, Monasca-based auto-scaling\nhas been introduced in Heat. Therefore, the main motivation\nis to leverage  Monasca resources in Heat. Monasca will be\nresponsible for monitoring and the backend policies like\nauto-scaling will be handled by Tacker.\n\nImplements blueprint: #monasca-alarm-monitor\nChange-Id: I0f43a3f52ed7a71239ede3b77070b5fa858e13da\n'}, {'number': 10, 'created': '2017-04-10 09:11:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/8faa877a756f36b443a05819e4d3632cd94e8a9e', 'message': 'Alarm monitor: Monasca support in Tacker\n\nAlarm monitor initially integrated with OpenStack Ceilometer.\nThis patch is proposed to leverage Monasca in Tacker.\nOpenStack Monasca is a highly scalable, performant, fault-tolerant\nmonitoring solution. Monasca support is basically similar\nto Ceilometer support in Tacker. Currently, Monasca-based auto-scaling\nhas been introduced in Heat. Therefore, the main motivation\nis to leverage  Monasca resources in Heat. Monasca will be\nresponsible for monitoring and the backend policies like\nauto-scaling will be handled by Tacker.\n\nImplements blueprint: #monasca-alarm-monitor\nChange-Id: I0f43a3f52ed7a71239ede3b77070b5fa858e13da\n'}, {'number': 11, 'created': '2017-04-10 11:02:03.000000000', 'files': ['tacker/tests/unit/vnfm/infra_drivers/openstack/data/hot_tosca_alarm_monasca.yaml', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/tosca_ceilometer_alarm_scale.yaml', 'tacker/vnfm/monitor_drivers/alarm_driver/alarm_driver.py', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/tosca_ceilometer_alarm_metadata.yaml', 'samples/tosca-templates/vnfd/tosca-vnfd-ceilometer-alarm-respawn.yaml', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/hot_tosca_ceilometer_alarm_respawn.yaml', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/tosca_monasca_alarm_scale.yaml', 'tacker/vnfm/plugin.py', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/hot_tosca_ceilometer_alarm_scale.yaml', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/hot_monasca_alarm_scale_custom.yaml', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/hot_tosca_ceilometer_alarm_metadata.yaml', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/test_tosca_vnfd_ceilometer_alarm_respawn.yaml', 'tacker/tests/unit/db/utils.py', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/tosca_ceilometer_alarm_respawn.yaml', 'tacker/vnfm/monitor.py', 'samples/tosca-templates/vnfd/tosca-vnfd-monasca-alarm-scale.yaml', 'tacker/vnfm/infra_drivers/openstack/translate_template.py', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/hot_ceilometer_alarm_scale_custom.yaml', 'samples/tosca-templates/vnfd/tosca-vnfd-ceilometer-alarm-scale.yaml', 'tacker/vnfm/monitor_drivers/alarm_driver/__init__.py', 'tacker/tests/unit/vnfm/infra_drivers/openstack/data/test_tosca_vnfd_ceilometer_alarm_scale.yaml', 'tacker/tests/unit/vnfm/infra_drivers/openstack/test_openstack.py', 'setup.cfg', 'tacker/tests/unit/test_tosca_templates_under_samples.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/aeed28e5d40f7f538d7862268800d253551da973', 'message': 'Alarm monitor: Monasca support in Tacker\n\nAlarm monitor initially integrated with OpenStack Ceilometer.\nThis patch is proposed to leverage Monasca in Tacker.\nOpenStack Monasca is a highly scalable, performant, fault-tolerant\nmonitoring solution. Monasca support is basically similar\nto Ceilometer support in Tacker. Currently, Monasca-based auto-scaling\nhas been introduced in Heat. Therefore, the main motivation\nis to leverage  Monasca resources in Heat. Monasca will be\nresponsible for monitoring and the backend policies like\nauto-scaling will be handled by Tacker.\n\nImplements blueprint: #monasca-alarm-monitor\nChange-Id: I0f43a3f52ed7a71239ede3b77070b5fa858e13da\n'}]",0,415216,aeed28e5d40f7f538d7862268800d253551da973,29,6,11,20560,,,0,"Alarm monitor: Monasca support in Tacker

Alarm monitor initially integrated with OpenStack Ceilometer.
This patch is proposed to leverage Monasca in Tacker.
OpenStack Monasca is a highly scalable, performant, fault-tolerant
monitoring solution. Monasca support is basically similar
to Ceilometer support in Tacker. Currently, Monasca-based auto-scaling
has been introduced in Heat. Therefore, the main motivation
is to leverage  Monasca resources in Heat. Monasca will be
responsible for monitoring and the backend policies like
auto-scaling will be handled by Tacker.

Implements blueprint: #monasca-alarm-monitor
Change-Id: I0f43a3f52ed7a71239ede3b77070b5fa858e13da
",git fetch https://review.opendev.org/openstack/tacker refs/changes/16/415216/1 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/tests/unit/vm/infra_drivers/openstack/data/hot_tosca_ceilometer_alarm_metadata.yaml', 'tacker/tests/unit/vm/infra_drivers/openstack/data/hot_tosca_ceilometer_alarm_respawn.yaml', 'tacker/tests/unit/vm/infra_drivers/heat/test_heat.py', 'tacker/tests/unit/vm/infra_drivers/openstack/data/tosca_ceilometer_alarm_metadata.yaml', 'tacker/tests/unit/vm/infra_drivers/openstack/data/test_tosca_vnfd_ceilometer_alarm_scale.yaml', 'tacker/tests/unit/vm/infra_drivers/openstack/test_openstack.py', 'tacker/tests/unit/vm/infra_drivers/openstack/data/hot_tosca_ceilometer_alarm_scale.yaml', 'tacker/vnfm/infra_drivers/openstack/translate_template.py', 'samples/tosca-templates/vnfd/tosca-vnfd-ceilometer-alarm-scale.yaml', 'tacker/tests/unit/vm/infra_drivers/openstack/data/tosca_ceilometer_alarm_respawn.yaml', 'samples/tosca-templates/vnfd/tosca-vnfd-ceilometer-alarm-respawn.yaml', 'tacker/tests/unit/vm/infra_drivers/openstack/data/tosca_monasca_alarm_scale.yaml', 'tacker/tests/unit/vm/infra_drivers/openstack/data/hot_monasca_alarm_scale_custom.yaml', 'tacker/tests/unit/vm/infra_drivers/openstack/data/test_tosca_vnfd_ceilometer_alarm_respawn.yaml', 'tacker/tests/unit/vm/test_tosca_templates_under_samples.py', 'tacker/tests/unit/vm/infra_drivers/openstack/data/hot_ceilometer_alarm_scale_custom.yaml', 'tacker/tests/unit/db/utils.py', 'tacker/tests/unit/vm/infra_drivers/openstack/data/tosca_ceilometer_alarm_scale.yaml', 'tacker/tests/unit/vm/infra_drivers/openstack/data/hot_tosca_alarm_monasca.yaml', 'samples/tosca-templates/vnfd/tosca-vnfd-monasca-alarm-scale.yaml']",20,e26fa442d058274ff1e9328c805b3fcaf232ff7e,bp/monasca-support,tosca_definitions_version: tosca_simple_profile_for_nfv_1_0_0 description: sample-tosca-vnfd-scaling metadata: template_name: sample-tosca-vnfd-scaling topology_template: node_templates: VDU1: type: tosca.nodes.nfv.VDU.Tacker properties: image: cirros-0.3.4-x86_64-uec mgmt_driver: noop availability_zone: nova flavor: m1.tiny metadata: {scale_group: SG1} CP1: type: tosca.nodes.nfv.CP.Tacker properties: management: true anti_spoofing_protection: false requirements: - virtualLink: node: VL1 - virtualBinding: node: VDU1 VL1: type: tosca.nodes.nfv.VL properties: network_name: net_mgmt vendor: Tacker policies: - SP1: type: tosca.policies.tacker.Scaling properties: increment: 1 cooldown: 60 min_instances: 1 max_instances: 3 default_instances: 2 targets: [VDU1] - vdu_cpu_usage_monitoring_policy: type: tosca.policies.tacker.Alarming triggers: vdu_hcpu_usage_scaling_out: event_type: type: tosca.events.resource.utilization implementation: monasca metrics: cpu_util condition: threshold: 50 constraint: utilization greater_than 50% period: 600 evaluations: 1 method: avg comparison_operator: gt metadata: SG1 actions: [SP1] vdu_lcpu_usage_scaling_in: event_type: type: tosca.events.resource.utilization implementation: monasca metrics: cpu_util condition: threshold: 10 constraint: utilization less_than 10% period: 600 evaluations: 1 method: avg comparison_operator: lt metadata: SG1 actions: [SP1] ,,384,57
openstack%2Fmonasca-persister~master~I7c7acf235a1b87cfef406f0ef5daeb035d09c90a,openstack/monasca-persister,master,I7c7acf235a1b87cfef406f0ef5daeb035d09c90a,Add prometheus client in monasca persister,NEW,2016-12-13 20:42:00.000000000,2017-12-18 03:30:33.000000000,,"[{'_account_id': 15027}, {'_account_id': 16168}, {'_account_id': 17001}, {'_account_id': 18179}, {'_account_id': 20033}]","[{'number': 1, 'created': '2016-12-13 20:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/6ae6ec76c4c40c726d513afa357b9c4532339d23', 'message': 'Add prometheus client in monasca persister\n\nThis is just a simple example of using prometheus client in\nmonasca persister. Default prometheus client port is set to\n2000 and this code should reture 5 metrics:\nmessages_counter\nmessages_process_rate{pid=""11119"",topic=""alarm-state-transitions""}\nmessages_process_rate{pid=""11118"",topic=""metrics""}\nmessages_processed{pid=""11119"",topic=""alarm-state-transitions""}\nmessages_processed{pid=""11118"",topic=""metrics""}\n\nChange-Id: I7c7acf235a1b87cfef406f0ef5daeb035d09c90a\n'}, {'number': 2, 'created': '2017-01-03 17:26:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/3ecb27e756c2b8dd74ee8aba16632d889e08f8ce', 'message': 'Add prometheus client in monasca persister\n\nThis is just a simple example of using prometheus client in\nmonasca persister. Default prometheus client port is set to\n2000 and this code should reture 5 metrics:\nmessages_counter\nmessages_process_rate{pid=""11119"",topic=""alarm-state-transitions""}\nmessages_process_rate{pid=""11118"",topic=""metrics""}\nmessages_processed{pid=""11119"",topic=""alarm-state-transitions""}\nmessages_processed{pid=""11118"",topic=""metrics""}\n\nChange-Id: I7c7acf235a1b87cfef406f0ef5daeb035d09c90a\n'}, {'number': 3, 'created': '2017-01-03 17:47:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/b51f639f35130776b94989fc90c50e9f05c3e9c4', 'message': 'Add prometheus client in monasca persister\n\nThis is just a simple example of using prometheus client in\nmonasca persister. Default prometheus client port is set to\n2000 and this code should reture 5 metrics:\nmessages_counter\nmessages_process_rate{pid=""11119"",topic=""alarm-state-transitions""}\nmessages_process_rate{pid=""11118"",topic=""metrics""}\nmessages_processed{pid=""11119"",topic=""alarm-state-transitions""}\nmessages_processed{pid=""11118"",topic=""metrics""}\n\nChange-Id: I7c7acf235a1b87cfef406f0ef5daeb035d09c90a\n'}, {'number': 4, 'created': '2017-01-04 16:49:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/68fdd487c7c1c58c7f32cb5e917a6b8901726735', 'message': 'Add prometheus client in monasca persister\n\nThis is just a simple example of using prometheus client in\nmonasca persister. Default prometheus client port is set to\n2000 and this code should reture 5 metrics:\nmessages_counter\nmessages_process_rate{pid=""11119"",topic=""alarm-state-transitions""}\nmessages_process_rate{pid=""11118"",topic=""metrics""}\nmessages_processed{pid=""11119"",topic=""alarm-state-transitions""}\nmessages_processed{pid=""11118"",topic=""metrics""}\n\nChange-Id: I7c7acf235a1b87cfef406f0ef5daeb035d09c90a\n'}, {'number': 5, 'created': '2017-01-09 16:45:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/9ed5e0c2e401a772905a8fbb904127704ba6aeff', 'message': 'Add prometheus client in monasca persister\n\nThis is just a simple example of using prometheus client in\nmonasca persister. Default prometheus client port is set to\n2000 and this code should reture 5 metrics:\nmessages_counter\nmessages_process_rate{pid=""11119"",topic=""alarm-state-transitions""}\nmessages_process_rate{pid=""11118"",topic=""metrics""}\nmessages_processed{pid=""11119"",topic=""alarm-state-transitions""}\nmessages_processed{pid=""11118"",topic=""metrics""}\n\nChange-Id: I7c7acf235a1b87cfef406f0ef5daeb035d09c90a\n'}, {'number': 7, 'created': '2017-01-10 00:05:05.000000000', 'files': ['monasca_persister/persister.py', 'Dockerfile', 'monasca_persister/repositories/influxdb/abstract_repository.py', 'requirements.txt', 'monasca_persister/repositories/abstract_repository.py', 'monasca_persister/repositories/persister.py', 'persister.conf'], 'web_link': 'https://opendev.org/openstack/monasca-persister/commit/d3a0658d0702eea77709a3d6b754e2c3f12eecc7', 'message': 'Add prometheus client in monasca persister\n\nThis is just a simple example of using prometheus client in\nmonasca persister. Default prometheus client port is set to\n2000 and this code should reture 5 metrics:\nmessages_counter\nmessages_process_rate{pid=""11119"",topic=""alarm-state-transitions""}\nmessages_process_rate{pid=""11118"",topic=""metrics""}\nmessages_processed{pid=""11119"",topic=""alarm-state-transitions""}\nmessages_processed{pid=""11118"",topic=""metrics""}\n\nChange-Id: I7c7acf235a1b87cfef406f0ef5daeb035d09c90a\n'}]",0,410389,d3a0658d0702eea77709a3d6b754e2c3f12eecc7,26,5,6,18179,,,0,"Add prometheus client in monasca persister

This is just a simple example of using prometheus client in
monasca persister. Default prometheus client port is set to
2000 and this code should reture 5 metrics:
messages_counter
messages_process_rate{pid=""11119"",topic=""alarm-state-transitions""}
messages_process_rate{pid=""11118"",topic=""metrics""}
messages_processed{pid=""11119"",topic=""alarm-state-transitions""}
messages_processed{pid=""11118"",topic=""metrics""}

Change-Id: I7c7acf235a1b87cfef406f0ef5daeb035d09c90a
",git fetch https://review.opendev.org/openstack/monasca-persister refs/changes/89/410389/7 && git format-patch -1 --stdout FETCH_HEAD,"['monasca_persister/persister.py', 'requirements.txt', 'monasca_persister/repositories/persister.py']",3,6ae6ec76c4c40c726d513afa357b9c4532339d23,persister_with_prometheus_client,"# (C) Copyright 2016 Hewlett Packard Enterprise Development LPimport timefrom oslo_log import log from prometheus_client import Counter, Gauge, CollectorRegistry, multiprocess self._start_time = time.time() self._end_time = 0 self.registry = CollectorRegistry() multiprocess.MultiProcessCollector(self.registry) self.metric_counter = Counter('messages_counter', 'total count of messages') self.metric_gauge = Gauge('messages_processed', 'total number of messages processed from ' 'one topic', ['topic'], multiprocess_mode='all') self.process_rate_gauge = Gauge('messages_process_rate', 'message processed per second from ' 'one topic', ['topic'], multiprocess_mode='all') self._end_time = time.time() self.metric_counter.inc(len(self._data_points)) self.metric_gauge.labels(topic=self._kafka_topic).set( len(self._data_points)) self.process_rate_gauge.labels(topic=self._kafka_topic).set( len(self._data_points) / (self._end_time - self._start_time)) self._start_time = self._end_time",# (C) Copyright 2016 Hewlett Packard Enterprise Development Company LP from oslo_log import log,54,5
openstack%2Frally~master~I3a31ac8f16deb3bad680bb0d184ab064b22e9acb,openstack/rally,master,I3a31ac8f16deb3bad680bb0d184ab064b22e9acb,[WIP] Move validators from 'utils.py' to common file,NEW,2017-04-05 11:08:41.000000000,2017-12-18 03:30:24.000000000,,"[{'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 23094}]","[{'number': 1, 'created': '2017-04-05 11:08:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c3ee8066723403dbf3bcea0411b066a48cec87cd', 'message': ""[WIP] Move validators from 'utils.py' to common file\n\nChange-Id: I3a31ac8f16deb3bad680bb0d184ab064b22e9acb\n""}, {'number': 2, 'created': '2017-04-05 15:33:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/9b50a55518cee1017e02fbf991fcb194b5b3576e', 'message': ""[WIP] Move validators from 'utils.py' to common file\n\nChange-Id: I3a31ac8f16deb3bad680bb0d184ab064b22e9acb\n""}, {'number': 3, 'created': '2017-04-06 07:27:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1e18ffddf5cfe482328a3aa52f47dede85c64d48', 'message': ""[WIP] Move validators from 'utils.py' to common file\n\nChange-Id: I3a31ac8f16deb3bad680bb0d184ab064b22e9acb\n""}, {'number': 4, 'created': '2017-04-06 09:18:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b7d8838158af6cefa8ed5e350f9d5be30bbc3aa6', 'message': ""[WIP] Move validators from 'utils.py' to common file\n\nChange-Id: I3a31ac8f16deb3bad680bb0d184ab064b22e9acb\n""}, {'number': 5, 'created': '2017-04-06 09:21:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/2aa3ed74054b19d5c61e259f0cf970278318c0d7', 'message': ""[WIP] Move validators from 'utils.py' to common file\n\nChange-Id: I3a31ac8f16deb3bad680bb0d184ab064b22e9acb\n""}, {'number': 6, 'created': '2017-04-11 10:31:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/674b6ddb28fc18a8e0c39dd3551d26ebbebb8d01', 'message': ""[WIP] Move validators from 'utils.py' to common file\n\nChange-Id: I3a31ac8f16deb3bad680bb0d184ab064b22e9acb\n""}, {'number': 7, 'created': '2017-04-14 11:44:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1687a83717c599454504d6fa9463989fa7bf5a86', 'message': ""[WIP] Move validators from 'utils.py' to common file\n\nChange-Id: I3a31ac8f16deb3bad680bb0d184ab064b22e9acb\n""}, {'number': 8, 'created': '2017-04-14 13:54:47.000000000', 'files': ['rally/plugins/openstack/validators.py', 'tests/unit/doc/missed_docstrings.txt', 'rally/plugins/openstack/scenarios/cinder/volumes.py', 'rally/common/validation.py', 'tests/unit/plugins/openstack/test_validators.py', 'tests/unit/doc/test_docstrings.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/28aa73d68c7f32a3642171ac7df63cd9690fe66b', 'message': ""[WIP] Move validators from 'utils.py' to common file\n\nChange-Id: I3a31ac8f16deb3bad680bb0d184ab064b22e9acb\n""}]",10,453576,28aa73d68c7f32a3642171ac7df63cd9690fe66b,25,3,8,23094,,,0,"[WIP] Move validators from 'utils.py' to common file

Change-Id: I3a31ac8f16deb3bad680bb0d184ab064b22e9acb
",git fetch https://review.opendev.org/openstack/rally refs/changes/76/453576/8 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/openstack/validators.py', 'rally/plugins/openstack/scenarios/cinder/volumes.py']",2,c3ee8066723403dbf3bcea0411b066a48cec87cd,astarove-class-based-validators,"from rally.common import validation@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""image_exists"", param_name=""image"", nullable=True) @validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""restricted_parameters"", pram_names=[""name"", ""display_name""]) @validation.add(""image_exists"", param_name=""image"", nullable=True) @validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""required_services"", requested_services=[consts.Service.CINDER])@validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""], subdict=""create_volume_kwargs"") @validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""], subdict=""update_volume_kwargs"") @validation.add(""image_exists"", param_name=""image"", nullable=True) @validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""image_exists"", param_name=""image"", nullable=True) @validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""image_exists"", param_name=""image"", nullable=True) @validation.add(""required_services"", param_names=[consts.Service.CINDER])@validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""required_contexts"", context_names=[""volumes""])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""required_services"", required_services=[consts.Service.CINDER]) @validation.add(""required_contexts"", context_names=[""volumes""])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""required_services"", reqired_services=[consts.Service.CINDER]) @validation.add(""required_contexts"", context_names=[""volumes""])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""], subdict=""create_volume_params"") @validation.add(""image_valid_on_flavor"", flavor_name=""flavor"", image_name=""image"") @validation.add(""required_services"", required_services=[consts.Service.NOVA, consts.Service.CINDER])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""volume_type_exists"", param_name=""volume_type"") @validation.add(""required_services"", required_services=[consts.Service.NOVA, consts.Service.CINDER])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""], subdict=""create_volume_kwargs"") @validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""], subdict=""create_snapshot_kwargs"") @validation.add(""required_services"", required_services=[consts.Service.NOVA, consts.Service.CINDER])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""required_services"", required_services=[consts.Service.CINDER]) @validation.add(""required_contexts"", context_names=[""volumes""])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""required_services"", required_services=[consts.Service.CINDER, consts.Service.GLANCE])@validation.add(""required_parameters"", param_names=[""size""])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""], subdict=""create_volume_kwargs"") @validation.add(""restricted_parameters"", param_names=[""name""], subdict=""create_backup_kwargs"") @validation.add(""required_cinder_services"", service_name=""cinder-backup"") @validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""], subdict=""create_volume_kwargs"") @validation.add(""restricted_parameters"", param_names=[""name""], subdict=""create_backup_kwargs"") @validation.add(""required_cinder_services"", service_name=""cinder-backup"") @validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""], subdict=""create_volume_kwargs"") @validation.add(""restricted_parameters"", param_names=[""name""], subdict=""create_backup_kwargs"") @validation.add(""required_cinder_services"", service_name=""cinder-backup"") @validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""image_exists"", param_name=""image"", nullable=True) @validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""], subdict=""create_snapshot_kwargs"") @validation.add(""required_services"", required_services=[consts.Service.CINDER]) @validation.add(""required_contexts"", context_names=[""volumes""])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""image_exists"", param_name=""image"", nullable=True) @validation.add(""required_services"", required_services=[consts.Service.CINDER])@validation.add(""restricted_parameters"", param_names=[""name"", ""display_name""]) @validation.add(""image_exists"", param_name=""image"", nullable=True) @validation.add(""required_services"", required_services=[consts.Service.CINDER])","from rally.task import validation@validation.restricted_parameters([""name"", ""display_name""]) @validation.image_exists(""image"", nullable=True) @validation.required_services(consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""]) @validation.image_exists(""image"", nullable=True) @validation.required_services(consts.Service.CINDER)@validation.required_services(consts.Service.CINDER)@validation.required_services(consts.Service.CINDER)@validation.required_services(consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""], subdict=""create_volume_kwargs"") @validation.restricted_parameters([""name"", ""display_name""], subdict=""update_volume_kwargs"") @validation.image_exists(""image"", nullable=True) @validation.required_services(consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""]) @validation.image_exists(""image"", nullable=True) @validation.required_services(consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""]) @validation.image_exists(""image"", nullable=True) @validation.required_services(consts.Service.CINDER)@validation.required_services(consts.Service.CINDER)@validation.required_contexts(""volumes"")@validation.restricted_parameters([""name"", ""display_name""]) @validation.required_services(consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""]) @validation.required_services(consts.Service.CINDER) @validation.required_contexts(""volumes"")@validation.restricted_parameters([""name"", ""display_name""]) @validation.required_services(consts.Service.CINDER) @validation.required_contexts(""volumes"")@validation.restricted_parameters([""name"", ""display_name""], subdict=""create_volume_params"") @validation.image_valid_on_flavor(""flavor"", ""image"") @validation.required_services(consts.Service.NOVA, consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""]) @validation.volume_type_exists(""volume_type"") @validation.required_services(consts.Service.NOVA, consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""], subdict=""create_volume_kwargs"") @validation.restricted_parameters([""name"", ""display_name""], subdict=""create_snapshot_kwargs"") @validation.required_services(consts.Service.NOVA, consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""]) @validation.required_services(consts.Service.CINDER) @validation.required_contexts(""volumes"")@validation.restricted_parameters([""name"", ""display_name""]) @validation.required_services(consts.Service.CINDER, consts.Service.GLANCE)@validation.required_parameters(""size"")@validation.restricted_parameters([""name"", ""display_name""], subdict=""create_volume_kwargs"") @validation.restricted_parameters(""name"", subdict=""create_backup_kwargs"") @validation.required_cinder_services(""cinder-backup"") @validation.required_services(consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""], subdict=""create_volume_kwargs"") @validation.restricted_parameters(""name"", subdict=""create_backup_kwargs"") @validation.required_cinder_services(""cinder-backup"") @validation.required_services(consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""], subdict=""create_volume_kwargs"") @validation.restricted_parameters(""name"", subdict=""create_backup_kwargs"") @validation.required_cinder_services(""cinder-backup"") @validation.required_services(consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""]) @validation.image_exists(""image"", nullable=True) @validation.required_services(consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""]) @validation.restricted_parameters([""name"", ""display_name""], subdict=""create_snapshot_kwargs"") @validation.required_services(consts.Service.CINDER) @validation.required_contexts(""volumes"")@validation.restricted_parameters([""name"", ""display_name""]) @validation.image_exists(""image"", nullable=True) @validation.required_services(consts.Service.CINDER)@validation.restricted_parameters([""name"", ""display_name""]) @validation.image_exists(""image"", nullable=True) @validation.required_services(consts.Service.CINDER)",404,79
openstack%2Fswift~master~Ie8e2a89f3283930785dabd69c227c95204905eae,openstack/swift,master,Ie8e2a89f3283930785dabd69c227c95204905eae,don't wait on remote container updates with write affinity,NEW,2016-08-29 15:28:18.000000000,2017-12-18 03:29:25.000000000,,"[{'_account_id': 995}, {'_account_id': 6968}, {'_account_id': 13052}]","[{'number': 1, 'created': '2016-08-29 15:28:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1e78d0070026c66fbe9d7798265a578e18e824f9', 'message': ""don't wait on remote container updates with write affinity\n\nIf write affinity is turned on and the container is not in the\nsame region as the object server that is sending the update\nthen don't wait any time for asyncronous container update.\n\nChange-Id: Ie8e2a89f3283930785dabd69c227c95204905eae\n""}, {'number': 2, 'created': '2016-08-29 15:40:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/460f5b6052ffca64b90cb5e08ebc8803cc3410f0', 'message': ""don't wait on remote container updates with write affinity\n\nIf write affinity is turned on and the container is not in the\nsame region as the object server that is sending the update\nthen don't wait any time for asyncronous container update.\n\nChange-Id: Ie8e2a89f3283930785dabd69c227c95204905eae\n""}, {'number': 3, 'created': '2016-08-29 17:37:08.000000000', 'files': ['swift/obj/server.py', 'test/unit/proxy/test_server.py', 'swift/proxy/controllers/obj.py', 'swift/proxy/controllers/base.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/56e211798cf5c9419137b1d81e61364f50328b2a', 'message': ""don't wait on remote container updates with write affinity\n\nIf write affinity is turned on and the container is not in the\nsame region as the object server that is sending the update\nthen don't wait any time for asyncronous container update.\n\nChange-Id: Ie8e2a89f3283930785dabd69c227c95204905eae\n""}]",0,362200,56e211798cf5c9419137b1d81e61364f50328b2a,14,3,3,995,,,0,"don't wait on remote container updates with write affinity

If write affinity is turned on and the container is not in the
same region as the object server that is sending the update
then don't wait any time for asyncronous container update.

Change-Id: Ie8e2a89f3283930785dabd69c227c95204905eae
",git fetch https://review.opendev.org/openstack/swift refs/changes/00/362200/1 && git format-patch -1 --stdout FETCH_HEAD,"['swift/obj/server.py', 'test/unit/proxy/test_server.py', 'swift/proxy/controllers/obj.py', 'swift/proxy/controllers/base.py']",4,1e78d0070026c66fbe9d7798265a578e18e824f9,container-affinity-mst," self._set_container_affinity_header(node, headers) def _set_container_affinity_header(self, node, out_headers): if self.app.write_affinity_is_local_fn is None: return cont_region = out_headers.get(""X-Container-Region"") if cont_region and int(node['region']) != int(cont_region): out_headers[""X-Backend-Container-Update-No-Wait""] = ""yes"" ",,26,1
openstack%2Fswift~master~I118509a4ceec6e6f2743a415474ae261d25f39ec,openstack/swift,master,I118509a4ceec6e6f2743a415474ae261d25f39ec,WIP: Cleanup EC backend logging on disconnect,NEW,2016-03-25 19:14:08.000000000,2017-12-18 03:29:22.000000000,,"[{'_account_id': 1179}, {'_account_id': 13052}, {'_account_id': 15343}]","[{'number': 1, 'created': '2016-03-25 19:14:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e73da866c3ad5bf0ea3cd27d2641a62b0b478df4', 'message': 'WIP: Cleanup EC backend logging on disconnect\n\nAll ResummingGetters used on the backend or making logging noise when\na client disconnects.\n\nThis attempt has the down-side of mutating the request inplace which\ncould be bad in some cases.\n\nChange-Id: I118509a4ceec6e6f2743a415474ae261d25f39ec\n'}, {'number': 2, 'created': '2016-03-29 22:05:43.000000000', 'files': ['test/unit/proxy/test_server.py', 'swift/proxy/controllers/obj.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/352572d2a25d5f27b62317715b26146492b7303d', 'message': 'WIP: Cleanup EC backend logging on disconnect\n\nAll ResummingGetters used on the backend are making logging noise when\na client disconnects.\n\nThis attempt has the down-side of mutating the request inplace which\ncould be bad in some cases.\n\nChange-Id: I118509a4ceec6e6f2743a415474ae261d25f39ec\n'}]",3,297822,352572d2a25d5f27b62317715b26146492b7303d,12,3,2,1179,,,0,"WIP: Cleanup EC backend logging on disconnect

All ResummingGetters used on the backend are making logging noise when
a client disconnects.

This attempt has the down-side of mutating the request inplace which
could be bad in some cases.

Change-Id: I118509a4ceec6e6f2743a415474ae261d25f39ec
",git fetch https://review.opendev.org/openstack/swift refs/changes/22/297822/2 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/proxy/test_server.py', 'swift/proxy/controllers/obj.py']",2,e73da866c3ad5bf0ea3cd27d2641a62b0b478df4,get-part-iter-disconnect," def app_iter(): try: for chunk in self.stashed_iter: yield chunk except GeneratorExit: self.logger.warning(_('Client disconnected on read')) return app_iter() # these are backend connections, not part of the outer iter returned # to eventlet wsgi - if they are ever closed - it's because we closed # them in ECAppIter req.environ['swift.non_client_disconnect'] = True", return iter(self.stashed_iter),13,3
openstack%2Fpython-swiftclient~master~I1a67dd2d8b16faf17add85728c5408c237b4c300,openstack/python-swiftclient,master,I1a67dd2d8b16faf17add85728c5408c237b4c300,Don't use auth_uri and prefix,NEW,2017-04-14 17:54:55.000000000,2017-12-18 03:28:51.000000000,,"[{'_account_id': 1179}, {'_account_id': 15343}]","[{'number': 1, 'created': '2017-04-14 17:54:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/806d8a043a85cf9496fd15dc114e47b282edc11b', 'message': 'Fix example test configuration\n\nDocument the prefered way to configure the tests.\n\nChange-Id: I1a67dd2d8b16faf17add85728c5408c237b4c300\nDepends-On: Ie427f3b0b9eb834ff940fa5d52444a5a6cdcab15\n'}, {'number': 2, 'created': '2017-04-14 22:57:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/fe5cb63c05cda0b95743627085496704dc6bb6db', 'message': ""Don't use auth_uri and prefix\n\n... and document the preferred method in sample.conf\n\nFollow-up:\n\nparse auth_version from auth_uri\n\nDocument the prefered way to configure the tests.\n\nChange-Id: I1a67dd2d8b16faf17add85728c5408c237b4c300\nDepends-On: Ie427f3b0b9eb834ff940fa5d52444a5a6cdcab15\n""}, {'number': 3, 'created': '2017-04-14 23:05:54.000000000', 'files': ['tests/sample.conf', 'tests/functional/test_swiftclient.py'], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/9030e3a28a88310ce86fcc5fbf497ada1d283b90', 'message': ""Don't use auth_uri and prefix\n\n... and document the preferred method in sample.conf\n\nKnown-Issues:\n\n * auth_version could be parsed from from auth_uri\n\nChange-Id: I1a67dd2d8b16faf17add85728c5408c237b4c300\nDepends-On: Ie427f3b0b9eb834ff940fa5d52444a5a6cdcab15\n""}]",2,456940,9030e3a28a88310ce86fcc5fbf497ada1d283b90,9,2,3,1179,,,0,"Don't use auth_uri and prefix

... and document the preferred method in sample.conf

Known-Issues:

 * auth_version could be parsed from from auth_uri

Change-Id: I1a67dd2d8b16faf17add85728c5408c237b4c300
Depends-On: Ie427f3b0b9eb834ff940fa5d52444a5a6cdcab15
",git fetch https://review.opendev.org/openstack/python-swiftclient refs/changes/40/456940/3 && git format-patch -1 --stdout FETCH_HEAD,['tests/sample.conf'],1,806d8a043a85cf9496fd15dc114e47b282edc11b,auth_changes,auth_uri = http://127.0.0.1:8080/auth/v1.0# For keystone v2 change auth_version to 2 and auth_prefix to /v2.0/ #auth_version = 3 #auth_uri = http://localhost:5000/v3/,auth_host = 127.0.0.1 auth_port = 8080 auth_ssl = no auth_prefix = /auth/# For keystone v3 change auth_version to 3 and auth_prefix to /v3/ #auth_version = 2 #auth_host = localhost #auth_port = 5000 #auth_ssl = no #auth_prefix = /v2.0/,4,10
openstack%2Fpython-ironicclient~master~I0c160c159a50c96ca1c24052baea221baf27af09,openstack/python-ironicclient,master,I0c160c159a50c96ca1c24052baea221baf27af09,Trivial: Rename and reorder deploy baremetal test,NEW,2016-09-28 01:49:22.000000000,2017-12-18 03:28:37.000000000,,"[{'_account_id': 6618}, {'_account_id': 12356}, {'_account_id': 13719}, {'_account_id': 14614}, {'_account_id': 18893}, {'_account_id': 25316}]","[{'number': 1, 'created': '2016-09-28 01:49:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/4018e65f29f0cbe2eb0f3bd848fc763d6abfff86', 'message': 'Trivial: Rename and reorder deploy baremetal test\n\nTo keep the consistency with other privision state tests.\n\nChange-Id: I0c160c159a50c96ca1c24052baea221baf27af09\n'}, {'number': 2, 'created': '2016-10-11 10:22:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/c9244030de061af82bc961ef104c4c5bee60a5b1', 'message': 'Trivial: Rename and reorder deploy baremetal test\n\nTo keep the consistency with other privision state tests.\n\nChange-Id: I0c160c159a50c96ca1c24052baea221baf27af09\n'}, {'number': 3, 'created': '2016-10-17 02:19:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/1d02e14ffe09a98f4c0b1ca8da2e5119373f7ca3', 'message': 'Trivial: Rename and reorder deploy baremetal test\n\nTo keep the consistency with other privision state tests.\n\nChange-Id: I0c160c159a50c96ca1c24052baea221baf27af09\n'}, {'number': 4, 'created': '2016-10-17 10:14:09.000000000', 'files': ['ironicclient/tests/unit/osc/v1/test_baremetal_node.py'], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/666e34f5518788b95f634276b286edd8e37098b5', 'message': 'Trivial: Rename and reorder deploy baremetal test\n\nTo keep the consistency with other privision state tests.\n\nChange-Id: I0c160c159a50c96ca1c24052baea221baf27af09\n'}]",6,378140,666e34f5518788b95f634276b286edd8e37098b5,20,6,4,14937,,,0,"Trivial: Rename and reorder deploy baremetal test

To keep the consistency with other privision state tests.

Change-Id: I0c160c159a50c96ca1c24052baea221baf27af09
",git fetch https://review.opendev.org/openstack/python-ironicclient refs/changes/40/378140/3 && git format-patch -1 --stdout FETCH_HEAD,['ironicclient/tests/unit/osc/v1/test_baremetal_node.py'],1,4018e65f29f0cbe2eb0f3bd848fc763d6abfff86,state-test-20160927,"class TestBaremetalDeploy(TestBaremetal): def setUp(self): super(TestBaremetalDeploy, self).setUp() # Get the command object to test self.cmd = baremetal_node.DeployBaremetalNode(self.app, None) def test_deploy_active_and_configdrive(self): arglist = ['node_uuid', '--config-drive', 'path/to/drive'] verifylist = [ ('node', 'node_uuid'), ('provision_state', 'active'), ('config_drive', 'path/to/drive'), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) self.cmd.take_action(parsed_args) self.baremetal_mock.node.set_provision_state.assert_called_once_with( 'node_uuid', 'active', configdrive='path/to/drive') def test_deploy_mismatch(self): arglist = ['node_uuid', '--provision-state', 'abort'] verifylist = [ ('node', 'node_uuid'), ('provision_state', 'active'), ] self.assertRaises(oscutils.ParserException, self.check_parser, self.cmd, arglist, verifylist) ","class TestDeployBaremetalProvisionState(TestBaremetal): def setUp(self): super(TestDeployBaremetalProvisionState, self).setUp() # Get the command object to test self.cmd = baremetal_node.DeployBaremetalNode(self.app, None) def test_deploy_baremetal_provision_state_active_and_configdrive(self): arglist = ['node_uuid', '--config-drive', 'path/to/drive'] verifylist = [ ('node', 'node_uuid'), ('provision_state', 'active'), ('config_drive', 'path/to/drive'), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) self.cmd.take_action(parsed_args) self.baremetal_mock.node.set_provision_state.assert_called_once_with( 'node_uuid', 'active', configdrive='path/to/drive') def test_deploy_baremetal_provision_state_mismatch(self): arglist = ['node_uuid', '--provision-state', 'abort'] verifylist = [ ('node', 'node_uuid'), ('provision_state', 'active'), ] self.assertRaises(oscutils.ParserException, self.check_parser, self.cmd, arglist, verifylist) ",36,36
openstack%2Frally~master~I5d67d25ccfc75e88d18d0c7b154a09317e0b5044,openstack/rally,master,I5d67d25ccfc75e88d18d0c7b154a09317e0b5044,DO NOT MERGE,NEW,2017-04-21 21:08:04.000000000,2017-12-18 03:28:25.000000000,,[{'_account_id': 14817}],"[{'number': 1, 'created': '2017-04-21 21:08:04.000000000', 'files': ['rally/plugins/openstack/scenarios/neutron/security_groups.py', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'samples/tasks/scenarios/neutron/create-and-list-security-group-rules.yaml', 'rally-jobs/rally-neutron.yaml'], 'web_link': 'https://opendev.org/openstack/rally/commit/1e6d5f455f15e6164ea287990d86aa5cd7c9ed30', 'message': 'DO NOT MERGE\n\nAdd Security group rules scenario\n\nChange-Id: I5d67d25ccfc75e88d18d0c7b154a09317e0b5044\nSigned-off-by: Pramod Raghavendra Jayathirth <pramod.raghavendra.jayathirth@intel.com>\n'}]",0,458972,1e6d5f455f15e6164ea287990d86aa5cd7c9ed30,4,1,1,16935,,,0,"DO NOT MERGE

Add Security group rules scenario

Change-Id: I5d67d25ccfc75e88d18d0c7b154a09317e0b5044
Signed-off-by: Pramod Raghavendra Jayathirth <pramod.raghavendra.jayathirth@intel.com>
",git fetch https://review.opendev.org/openstack/rally refs/changes/72/458972/1 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/openstack/scenarios/neutron/security_groups.py', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'samples/tasks/scenarios/neutron/create-and-list-security-group-rules.yaml', 'rally-jobs/rally-neutron.yaml']",4,1e6d5f455f15e6164ea287990d86aa5cd7c9ed30,," NeutronSecurityGroupRule.create_and_list_security_group_rules: - args: security_group_count: 5 rules_per_security_group: 5 runner: type: ""constant"" times: 4 concurrency: 2 context: users: tenants: 2 users_per_tenant: 2 user_choice_method: ""round_robin"" quotas: neutron: security_group: -1 security_group_rule: -1 sla: failure_rate: max: 0 NeutronSecurityGroupRule.create_and_delete_security_group_rules: - args: security_group_count: 5 rules_per_security_group: 5 runner: type: ""constant"" times: 4 concurrency: 4 context: users: tenants: 3 users_per_tenant: 2 quotas: neutron: security_group: -1 security_group_rule: -1 sla: failure_rate: max: 0 NeutronSecurityGroupRule.create_and_update_security_group_rules: - args: security_group_count: 5 runner: type: ""constant"" times: 4 concurrency: 4 context: users: tenants: 3 users_per_tenant: 2 quotas: neutron: security_group: -1 sla: failure_rate: max: 0 ",,179,0
openstack%2Fzun~master~I2e9bfccac55ce5ca7a58f3c4b9a337dc52e6c362,openstack/zun,master,I2e9bfccac55ce5ca7a58f3c4b9a337dc52e6c362,"Support the command ""zun stats""",NEW,2017-02-18 03:33:53.000000000,2017-12-18 03:28:08.000000000,,"[{'_account_id': 8264}, {'_account_id': 11536}]","[{'number': 1, 'created': '2017-02-18 03:33:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/cc6b68ac9590491fe734d7a59baa9f951f47acc0', 'message': 'Support the command ""zun stats""\n\nThe BP adds ""zun stats"" which can display the stats of container\n\nChange-Id: I2e9bfccac55ce5ca7a58f3c4b9a337dc52e6c362\nImplements: blueprint support-zun-stats\n'}, {'number': 2, 'created': '2017-02-20 11:19:14.000000000', 'files': ['zun/common/utils.py', 'zun/compute/api.py', 'zun/compute/rpcapi.py', 'zun/container/docker/driver.py', 'zun/container/driver.py', 'zun/compute/manager.py', 'zun/tests/unit/api/controllers/v1/test_containers.py', 'etc/zun/policy.json', 'zun/api/controllers/v1/containers.py'], 'web_link': 'https://opendev.org/openstack/zun/commit/d341c1c7399977d6964c1c805353330bed22cb24', 'message': 'Support the command ""zun stats""\n\nThe BP adds ""zun stats"" which can display the stats of container\n\nChange-Id: I2e9bfccac55ce5ca7a58f3c4b9a337dc52e6c362\nImplements: blueprint support-zun-stats\n'}]",0,435649,d341c1c7399977d6964c1c805353330bed22cb24,8,2,2,24115,,,0,"Support the command ""zun stats""

The BP adds ""zun stats"" which can display the stats of container

Change-Id: I2e9bfccac55ce5ca7a58f3c4b9a337dc52e6c362
Implements: blueprint support-zun-stats
",git fetch https://review.opendev.org/openstack/zun refs/changes/49/435649/2 && git format-patch -1 --stdout FETCH_HEAD,"['zun/common/utils.py', 'zun/compute/api.py', 'zun/compute/rpcapi.py', 'zun/container/docker/driver.py', 'zun/compute/manager.py', 'zun/container/driver.py', 'zun/tests/unit/api/controllers/v1/test_containers.py', 'etc/zun/policy.json', 'zun/api/controllers/v1/containers.py']",9,cc6b68ac9590491fe734d7a59baa9f951f47acc0,bp/adds," 'stats': ['GET'], @pecan.expose('json') @exception.wrap_pecan_controller_exception def stats(self, container_id, decode=False, stream=True): container = _get_container(container_id) check_policy_on_container(container.as_dict(), ""container:stats"") utils.validate_container_state(container, 'stats') decode = strutils.bool_from_string(decode, strict=True) stream = strutils.bool_from_string(stream, strict=True) LOG.debug('Calling compute.container_stats with %s decode %s stream %s' % (container.uuid, decode, stream)) context = pecan.request.context compute_api = pecan.request.compute_api return compute_api.container_stats(context, container, decode, stream)",,88,0
openstack%2Frally~master~I25bcec43f71b561f66f5f139d82a227b20ca5380,openstack/rally,master,I25bcec43f71b561f66f5f139d82a227b20ca5380,[WIP][WIP][Core] Subtask context,NEW,2016-11-29 11:45:48.000000000,2017-12-18 03:27:16.000000000,,"[{'_account_id': 9545}, {'_account_id': 10475}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 19011}, {'_account_id': 22960}, {'_account_id': 24350}]","[{'number': 1, 'created': '2016-11-29 11:45:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/183a944b047be3ab4819cebad378b210cf59ba64', 'message': '[Core] Subtask context\n\n* Adds support for subtask context\n* Allows to specify more than one subtask\n  in v2 task configuration.\n\nChange-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380\n'}, {'number': 2, 'created': '2016-11-29 12:45:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/63726ee212bfda29e00e96e55839903d2fe3e95b', 'message': '[Core] Subtask context\n\n* Adds support for subtask context\n* Allows to specify more than one subtask\n  in v2 task configuration.\n\nChange-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380\n'}, {'number': 3, 'created': '2016-11-30 14:01:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/59540daeea379e23e5fdfba56c1b005a4608083a', 'message': '[Core] Subtask context\n\n* Adds support for subtask context\n* Allows to specify more than one subtask\n  in v2 task configuration.\n\nChange-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380\n'}, {'number': 4, 'created': '2016-12-22 13:44:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/53ec628484fd68c1feeb85f53287647696ec2422', 'message': '[WIP][Core] Subtask context\n\n* Adds support for subtask context\n* Allows to specify more than one subtask\n  in v2 task configuration.\n\nChange-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380\n'}, {'number': 5, 'created': '2016-12-23 09:17:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5c1585de2219680d86a245177d6e41125a4f122a', 'message': '[Core] Subtask context\n\n* Adds support for subtask context\n* Allows to specify more than one subtask\n  in v2 task configuration.\n\nChange-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380\n'}, {'number': 6, 'created': '2016-12-23 10:07:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/467df12ca17ee45e7785ff1615f484774cc18a09', 'message': '[Core] Subtask context\n\n* Adds support for subtask context\n* Allows to specify more than one subtask\n  in v2 task configuration.\n\nChange-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380\n'}, {'number': 7, 'created': '2016-12-26 18:57:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8dd4a42a23af99849b670a173146364ae22b07ba', 'message': '[Core] Subtask context\n\n* Adds support for subtask context\n* Allows to specify more than one subtask\n  in v2 task configuration.\n\nChange-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380\n'}, {'number': 8, 'created': '2016-12-27 18:17:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/48598bd22d8f4cc1e81971f5f4786edddfbd8fcd', 'message': '[Core] Subtask context\n\n* Adds support for subtask context\n* Allows to specify more than one subtask\n  in v2 task configuration.\n\nChange-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380\n'}, {'number': 9, 'created': '2017-04-14 09:27:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/2bcefee80cd0274d43dba6bf77a0489af212bc2d', 'message': '[WIP][Core] Subtask context\n\nChange-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380\n'}, {'number': 10, 'created': '2017-04-14 16:33:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/712b63770c122035b6f9cd96ccc0d766a2f731a4', 'message': '[WIP][Core] Subtask context\n\nChange-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380\n'}, {'number': 11, 'created': '2017-04-17 07:52:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5ae465ba1400cdf20dbbc1b6476ffa507f366257', 'message': '[Core] Subtask context\n\n* Added subtask context to v2 config.\n* Moved manila-multibackend job to v2 config.\n\nChange-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380\n'}, {'number': 12, 'created': '2017-04-28 15:46:42.000000000', 'files': ['rally/task/context.py', 'rally/plugins/openstack/scenario.py', 'rally/task/engine.py', 'rally/plugins/openstack/context/dataplane/heat.py', 'rally-jobs/rally-manila.yaml', 'rally/plugins/openstack/context/keystone/users.py', 'tests/unit/task/test_engine.py', 'tests/unit/task/test_context.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/b09e7b7c2c6db52620b6b413c17d82a868d6f850', 'message': '[WIP][WIP][Core] Subtask context\n\n* Added subtask context to v2 config.\n* Moved manila-multibackend job to v2 config.\n\nTODO: unit tests\n\nCo-Authored-By: Anton Studenov <astudenov@mirantis.com>\nChange-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380\n'}]",8,404168,b09e7b7c2c6db52620b6b413c17d82a868d6f850,45,7,12,19011,,,0,"[WIP][WIP][Core] Subtask context

* Added subtask context to v2 config.
* Moved manila-multibackend job to v2 config.

TODO: unit tests

Co-Authored-By: Anton Studenov <astudenov@mirantis.com>
Change-Id: I25bcec43f71b561f66f5f139d82a227b20ca5380
",git fetch https://review.opendev.org/openstack/rally refs/changes/68/404168/4 && git format-patch -1 --stdout FETCH_HEAD,"['rally/task/engine.py', 'rally/exceptions.py', 'tests/unit/task/test_engine.py']",3,183a944b047be3ab4819cebad378b210cf59ba64,subtask-context," mock_context_manager_cleanup.side_effect = Exception self.assertEqual(3, mock_log.exception.call_count) def test__prepare_context(self, mock_scenario_get): def test__prepare_subtask_context(self): context = {""b"": 3, ""c"": 4} credential = mock.MagicMock() config = { ""a.task"": [{""context"": {""context_a"": {""a"": 1}}}], } eng = engine.TaskEngine(config, task) result = eng._prepare_subtask_context(context, credential) expected_context = {""users"": {}} expected_context.update(context) expected_result = { ""task"": task, ""admin"": {""credential"": credential}, ""config"": expected_context } self.assertEqual(expected_result, result) def test__prepare_subtask_context_with_existing_users(self): task = mock.MagicMock() result = eng._prepare_subtask_context(context, credential)"," mock_context_manager_setup.side_effect = Exception self.assertEqual(2, mock_log.exception.call_count) @mock.patch(""rally.task.engine.TaskConfig"") def test__prepare_context(self, mock_scenario_get, mock_task_config): expected_context.setdefault(""users"", {}) @mock.patch(""rally.task.engine.TaskConfig"") @mock.patch(""rally.task.engine.scenario.Scenario.get"") def test__prepare_context_with_existing_users(self, mock_scenario_get, mock_task_config): mock_scenario_get.return_value._meta_get.return_value = {} name = ""a.task"" result = eng._prepare_context(context, name, credential) ""scenario_name"": name, mock_scenario_get.assert_called_once_with(name) mock_scenario_get.return_value._meta_get.assert_called_once_with( ""default_context"")",102,56
openstack%2Fdevstack-gate~master~Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e,openstack/devstack-gate,master,Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e,WIP: Test SSL wsgi,NEW,2016-12-26 13:54:04.000000000,2017-12-18 03:26:47.000000000,,[{'_account_id': 23330}],"[{'number': 1, 'created': '2016-12-26 13:54:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/55182859c729d402822195b9fbdccb1be3279357', 'message': 'WIP: test grenade ironic\n\nSome test gate job with ironic\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 2, 'created': '2016-12-27 07:12:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/409b349841fdbd27a457ebf0600947b5b14d2aa2', 'message': 'WIP: test grenade ironic\n\nSome test gate job with ironic\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 3, 'created': '2017-02-16 10:26:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/a48bd1f340e5bc2c5f2f72e89e72184ee2cddbdd', 'message': 'WIP: test grenade ironic\n\nSome test gate job with ironic\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 4, 'created': '2017-02-23 11:13:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/dfad11db10f7e4145e039f56e265eaef45c873f0', 'message': 'WIP: test grenade ironic\n\nSome test gate job with ironic\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 5, 'created': '2017-03-01 09:19:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/26d787d86b6d00cbeeb8520e2056a7b10ac797e8', 'message': 'WIP: Test SSL wsgi\n\nTest ironic to use SSL with wsgi\n\nChange-ironic:\nDepends-On: I9c5ad56e1acd292ff0f9cc9b460125fc420abda5\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 6, 'created': '2017-03-01 09:23:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/d108715740372ec64f68caa64b410375426aaa68', 'message': 'WIP: Test SSL wsgi\n\nTest ironic to use SSL with wsgi\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 7, 'created': '2017-03-01 09:42:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/75ca2dfc3cac31bd74875d012cdef9366901bea3', 'message': 'WIP: Test SSL wsgi\n\nTest ironic to use SSL with wsgi\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 8, 'created': '2017-03-01 09:50:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/155f5795db3a99925ebf8c67b5082f7bf4423616', 'message': 'WIP: Test SSL wsgi\n\nTest ironic to use SSL with wsgi\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 9, 'created': '2017-03-01 11:10:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/97a60a8e8ee0a07b72e9483a2f939486d04967cf', 'message': 'WIP: Test SSL wsgi\n\nTest ironic to use SSL with wsgi\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 10, 'created': '2017-03-01 11:30:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/f79ff3f476611cf58ab8f570bd60b75677633c3b', 'message': 'WIP: Test SSL wsgi\n\nTest ironic to use SSL with wsgi\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 11, 'created': '2017-03-03 12:22:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/af97b217445338fdeb38dad36c68641569528915', 'message': 'WIP: Test SSL wsgi\n\nTest ironic to use SSL with wsgi\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 12, 'created': '2017-03-28 11:06:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/bc210669b5a415b89d931af626e07ef87011daf3', 'message': 'WIP: Test SSL wsgi\n\nTest ironic to use SSL with wsgi\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 13, 'created': '2017-03-28 11:14:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/4c1a878174e6d0e91c2f2d4bba07a8bb6ef760a9', 'message': 'WIP: Test SSL wsgi\n\nTest ironic to use SSL with wsgi\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}, {'number': 14, 'created': '2017-05-03 07:18:50.000000000', 'files': ['devstack-vm-gate.sh'], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/682ccf4eed370483e04143b5310cbd8d277c8411', 'message': 'WIP: Test SSL wsgi\n\nTest ironic to use SSL with wsgi\n\nChange-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n'}]",0,414986,682ccf4eed370483e04143b5310cbd8d277c8411,33,1,14,23330,,,0,"WIP: Test SSL wsgi

Test ironic to use SSL with wsgi

Change-Id: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e
",git fetch https://review.opendev.org/openstack/devstack-gate refs/changes/86/414986/4 && git format-patch -1 --stdout FETCH_HEAD,['devstack-vm-gate-wrap.sh'],1,55182859c729d402822195b9fbdccb1be3279357,ssl,"PROJECTS=""openstack-dev/grenade $PROJECTS""export DEVSTACK_GATE_GRENADE=pullup export GRENADE_OLD_BRANCH=""stable/master""","export DEVSTACK_GATE_GRENADE=${DEVSTACK_GATE_GRENADE:-} export GRENADE_OLD_BRANCH=""stable/newton""",3,2
openstack%2Fironic~master~If54590af05aa7c1f620f0bbdd0ef316bc8aca4a0,openstack/ironic,master,If54590af05aa7c1f620f0bbdd0ef316bc8aca4a0,Install networking-baremetal ML2 plugin,NEW,2017-03-27 21:43:42.000000000,2017-12-18 03:25:35.000000000,,"[{'_account_id': 10379}, {'_account_id': 14525}, {'_account_id': 14629}, {'_account_id': 14826}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}]","[{'number': 1, 'created': '2017-03-27 21:43:42.000000000', 'files': ['install-guide/source/include/configure-neutron-networks.rst', 'devstack/settings'], 'web_link': 'https://opendev.org/openstack/ironic/commit/0a877fcabdb01a05cbd5467e76d0ac6b48b51293', 'message': 'Install networking-baremetal ML2 plugin\n\nThis patch install networking-baremetal plugin with all\nironic by default.\n\nUpdate install-guide.\n\nDepends-On: I605853a1decee6292ed9469be2f35b572049d919\nChange-Id: If54590af05aa7c1f620f0bbdd0ef316bc8aca4a0\n'}]",0,450467,0a877fcabdb01a05cbd5467e76d0ac6b48b51293,17,7,1,14525,,,0,"Install networking-baremetal ML2 plugin

This patch install networking-baremetal plugin with all
ironic by default.

Update install-guide.

Depends-On: I605853a1decee6292ed9469be2f35b572049d919
Change-Id: If54590af05aa7c1f620f0bbdd0ef316bc8aca4a0
",git fetch https://review.opendev.org/openstack/ironic refs/changes/67/450467/1 && git format-patch -1 --stdout FETCH_HEAD,"['install-guide/source/include/configure-neutron-networks.rst', 'devstack/settings']",2,0a877fcabdb01a05cbd5467e76d0ac6b48b51293,mstr,enable_plugin networking-baremetal http://github.com/openstack/networking-baremetal.git ,,7,1
openstack%2Ftacker~master~Id602eae86f841f540f52c150933371585e7d8ad6,openstack/tacker,master,Id602eae86f841f540f52c150933371585e7d8ad6,Refactor retry logic to make it reusable,NEW,2017-05-04 20:23:34.000000000,2017-12-18 03:25:10.000000000,,[{'_account_id': 10487}],"[{'number': 1, 'created': '2017-05-04 20:23:34.000000000', 'files': ['tacker/tests/unit/vnfm/infra_drivers/openstack/test_openstack_driver.py', 'tacker/vnfm/infra_drivers/openstack/openstack.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/ebd71fa804105b43288d4ec61813415f06e70408', 'message': 'Refactor retry logic to make it reusable\n\nIn openstack infra driver, refactor the the XX_wait() method\nto reuse the logic for retry and wait.\n\nCurrently, retry and wait logic is written in mix with the\nbusiness logic, so remove the logic for the retry and wait\noperation and resue in all XX_wait() methods.\n\nWe can use the behaviour pattern to refactor it.\ne.g.\n\ndef XX_wait(...):\n    ....\n    # business logic which will be reattempted\n    # based on stack state and reattempt count.\n    ...\n\nNew Implementation\ndef XX_wait(...):\n    ...\n    # Behavior of XX_wait\n    def handler(state, retry_count):\n        ...\n        # behaviour based on state and retry_count\n        ...\n\n    # retry method has logic for wait and retry\n    retry(handler,....)\n\nretry method will calculate the state and retry_count\nand feed to the handler.\n\nChange-Id: Id602eae86f841f540f52c150933371585e7d8ad6\nSigned-off-by: subhash kumar singh <subh.singh007@gmail.com>\n'}]",1,462730,ebd71fa804105b43288d4ec61813415f06e70408,4,1,1,19087,,,0,"Refactor retry logic to make it reusable

In openstack infra driver, refactor the the XX_wait() method
to reuse the logic for retry and wait.

Currently, retry and wait logic is written in mix with the
business logic, so remove the logic for the retry and wait
operation and resue in all XX_wait() methods.

We can use the behaviour pattern to refactor it.
e.g.

def XX_wait(...):
    ....
    # business logic which will be reattempted
    # based on stack state and reattempt count.
    ...

New Implementation
def XX_wait(...):
    ...
    # Behavior of XX_wait
    def handler(state, retry_count):
        ...
        # behaviour based on state and retry_count
        ...

    # retry method has logic for wait and retry
    retry(handler,....)

retry method will calculate the state and retry_count
and feed to the handler.

Change-Id: Id602eae86f841f540f52c150933371585e7d8ad6
Signed-off-by: subhash kumar singh <subh.singh007@gmail.com>
",git fetch https://review.opendev.org/openstack/tacker refs/changes/30/462730/1 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/tests/unit/vnfm/infra_drivers/openstack/test_openstack_driver.py', 'tacker/vnfm/infra_drivers/openstack/openstack.py']",2,ebd71fa804105b43288d4ec61813415f06e70408,," def retry_create_wait(status, stack_retries): if status == 'CREATE_IN_PROGRESS' and stack_retries <= self.STACK_RETRIES: ""happened because Heat API request failed "" ""while waiting for the stack %(stack)s to be "" ""created""), {'stack': vnf_id}) LOG.debug(_('status: %s'), status) return True LOG.debug(_('stack status: %(stack)s %(status)s'), {'stack': str(stack), 'status': status}) if status != 'CREATE_COMPLETE': error_reason = _(""Resource creation is not completed within"" "" {wait} seconds as creation of stack {stack}"" "" is not completed"").format( wait=(self.STACK_RETRIES * self.STACK_RETRY_WAIT), stack=vnf_id) # TODO: why wait is required ? LOG.warning(_(""VNF Creation failed: %(reason)s""), {'reason': error_reason}) raise vnfm.VNFCreateWaitFailed(reason=error_reason) elif status == 'CREATE_COMPLETE': def _find_mgmt_ips(outputs): LOG.debug(_('outputs %s'), outputs) mgmt_ips = dict((output['output_key'][len(OUTPUT_PREFIX):], output['output_value']) for output in outputs if output.get('output_key', '').startswith(OUTPUT_PREFIX)) return mgmt_ips # scaling enabled if vnf_dict['attributes'].get('scaling_group_names'): group_names = jsonutils.loads( vnf_dict['attributes'].get('scaling_group_names')).values() mgmt_ips = self._find_mgmt_ips_from_groups(heatclient, vnf_id, group_names) else: mgmt_ips = _find_mgmt_ips(stack.outputs) if mgmt_ips: vnf_dict['mgmt_url'] = jsonutils.dumps(mgmt_ips) self.retry(retry_create_wait, heatclient, self.STACK_RETRIES, self.STACK_RETRY_WAIT, vnf_id) @log.log def retry(self, retry_method, heatclient, retry_count, retry_wait, vnf_id=None): retry_flg = True temp = retry_count retry_infinite = False # if the retry_count is -1 then execute the retry method # infinite times if retry_count == -1: retry_infinite = True while (retry_count > 0 or retry_infinite) and retry_flg: time.sleep(retry_wait) status = heatclient.get(vnf_id).stack_status if vnf_id != None else None LOG.debug(_('stack status: %(status) retry_count: %(count)'), {'status': status, 'count': retry_count}) retry_flg = retry_method(status, temp - retry_count + 1) retry_count = retry_count - 1"," while status == 'CREATE_IN_PROGRESS' and stack_retries > 0: time.sleep(self.STACK_RETRY_WAIT) try: stack = heatclient.get(vnf_id) except Exception: ""happened because Heat API request failed "" ""while waiting for the stack %(stack)s to be "" ""created""), {'stack': vnf_id}) status = stack.stack_status LOG.debug(_('status: %s'), status) stack_retries = stack_retries - 1 LOG.debug(_('stack status: %(stack)s %(status)s'), {'stack': str(stack), 'status': status}) if stack_retries == 0 and status != 'CREATE_COMPLETE': error_reason = _(""Resource creation is not completed within"" "" {wait} seconds as creation of stack {stack}"" "" is not completed"").format( wait=(self.STACK_RETRIES * self.STACK_RETRY_WAIT), stack=vnf_id) LOG.warning(_(""VNF Creation failed: %(reason)s""), {'reason': error_reason}) raise vnfm.VNFCreateWaitFailed(reason=error_reason) elif stack_retries != 0 and status != 'CREATE_COMPLETE': error_reason = stack.stack_status_reason raise vnfm.VNFCreateWaitFailed(reason=error_reason) def _find_mgmt_ips(outputs): LOG.debug(_('outputs %s'), outputs) mgmt_ips = dict((output['output_key'][len(OUTPUT_PREFIX):], output['output_value']) for output in outputs if output.get('output_key', '').startswith(OUTPUT_PREFIX)) return mgmt_ips # scaling enabled if vnf_dict['attributes'].get('scaling_group_names'): group_names = jsonutils.loads( vnf_dict['attributes'].get('scaling_group_names')).values() mgmt_ips = self._find_mgmt_ips_from_groups(heatclient, vnf_id, group_names) else: mgmt_ips = _find_mgmt_ips(stack.outputs) if mgmt_ips: vnf_dict['mgmt_url'] = jsonutils.dumps(mgmt_ips)",73,45
openstack%2Fdiskimage-builder~master~I6b9adbb53357c37544769e747331f68513ac7871,openstack/diskimage-builder,master,I6b9adbb53357c37544769e747331f68513ac7871,Deprecate block-device commands incompatible with config file,NEW,2017-05-08 05:15:11.000000000,2017-12-18 03:24:56.000000000,,"[{'_account_id': 10118}, {'_account_id': 21741}]","[{'number': 1, 'created': '2017-05-08 05:15:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/b98334966368b2167ef888909a8cdfc541e33dde', 'message': 'Deprecate block-device commands incompatible with config file\n\nThere are a number of global block-device commands that don\'t make\nsense in a multiple-partition world -- in cases where you\'re building\nyour own block-device configuration or using the ""vm"" element (which\nuses it\'s own block-device configs).\n\nCurrently we look for the ""mkfs->mkfs_root"" entry in the configuration\nfile and merge these global values in -- but only for the fall-through\ncase where you don\'t specify a config-file and aren\'t using the vm\nelement.  Otherwise these are silently ignored.\n\nThis is done via entries the ""params"" file, so these values are passed\nto every instantiation of the block-device object unnecessarily;\nmaking it rather confusing as to when they are actually used.\n\nSince these are put in to maintain base backwards compatibility, I\nthink we are better off keep all logic about this out of\ndib-block-device and putting the values into the default configuration\nfile.  Additionally, make it clear via documentation and run-time\nchecks when these values are not going to work.\n\nChange-Id: I6b9adbb53357c37544769e747331f68513ac7871\n'}, {'number': 2, 'created': '2017-05-10 05:14:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/9ac3152abafb708bc791f263070b67aacd5bb61b', 'message': 'Deprecate block-device commands incompatible with config file\n\nThere are a number of global block-device commands that don\'t make\nsense in a multiple-partition world -- in cases where you\'re building\nyour own block-device configuration or using the ""vm"" element (which\nuses it\'s own block-device configs).\n\nCurrently we look for the ""mkfs->mkfs_root"" entry in the configuration\nfile and merge these global values in -- but only for the fall-through\ncase where you don\'t specify a config-file and aren\'t using the vm\nelement.  Otherwise these are silently ignored.\n\nThis is done via entries the ""params"" file, so these values are passed\nto every instantiation of the block-device object unnecessarily;\nmaking it rather confusing as to when they are actually used.\n\nSince these are put in to maintain base backwards compatibility, I\nthink we are better off keep all logic about this out of\ndib-block-device and putting the values into the default configuration\nfile.  Additionally, make it clear via documentation and run-time\nchecks when these values are not going to work.\n\nChange-Id: I6b9adbb53357c37544769e747331f68513ac7871\n'}, {'number': 3, 'created': '2017-05-10 10:12:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/245e36f54db982dee606d5b1c842c2e57fc6df9a', 'message': 'Deprecate block-device commands incompatible with config file\n\nThere are a number of global block-device commands that don\'t make\nsense in a multiple-partition world -- in cases where you\'re building\nyour own block-device configuration or using the ""vm"" element (which\nuses it\'s own block-device configs).\n\nCurrently we look for the ""mkfs->mkfs_root"" entry in the configuration\nfile and merge these global values in -- but only for the fall-through\ncase where you don\'t specify a config-file and aren\'t using the vm\nelement.  Otherwise these are silently ignored.\n\nThis is done via entries the ""params"" file, so these values are passed\nto every instantiation of the block-device object unnecessarily;\nmaking it rather confusing as to when they are actually used.\n\nSince these are put in to maintain base backwards compatibility, I\nthink we are better off keep all logic about this out of\ndib-block-device and putting the values into the default configuration\nfile.  Additionally, make it clear via documentation and run-time\nchecks when these values are not going to work.\n\nChange-Id: I6b9adbb53357c37544769e747331f68513ac7871\n'}, {'number': 4, 'created': '2017-05-10 10:38:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/367b7f89ca392c010ec4cb3bcf866e1704e5c9c7', 'message': 'Deprecate block-device commands incompatible with config file\n\nThere are a number of global block-device commands that don\'t make\nsense in a multiple-partition world -- in cases where you\'re building\nyour own block-device configuration or using the ""vm"" element (which\nuses it\'s own block-device configs).\n\nCurrently we look for the ""mkfs->mkfs_root"" entry in the configuration\nfile and merge these global values in -- but only for the fall-through\ncase where you don\'t specify a config-file and aren\'t using the vm\nelement.  Otherwise these are silently ignored.\n\nThis is done via entries the ""params"" file, so these values are passed\nto every instantiation of the block-device object unnecessarily;\nmaking it rather confusing as to when they are actually used.\n\nSince these are put in to maintain base backwards compatibility, I\nthink we are better off keep all logic about this out of\ndib-block-device and putting the values into the default configuration\nfile.  Additionally, make it clear via documentation and run-time\nchecks when these values are not going to work.\n\nChange-Id: I6b9adbb53357c37544769e747331f68513ac7871\n'}, {'number': 5, 'created': '2017-05-10 10:45:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/ccfdd74a22d8531f9acb158ea339bb58435677f1', 'message': 'Deprecate block-device commands incompatible with config file\n\nThere are a number of global block-device commands that don\'t make\nsense in a multiple-partition world -- in cases where you\'re building\nyour own block-device configuration or using the ""vm"" element (which\nuses it\'s own block-device configs).\n\nCurrently we look for the ""mkfs->mkfs_root"" entry in the configuration\nfile and merge these global values in -- but only for the fall-through\ncase where you don\'t specify a config-file and aren\'t using the vm\nelement.  Otherwise these are silently ignored.\n\nThis is done via entries the ""params"" file, so these values are passed\nto every instantiation of the block-device object unnecessarily;\nmaking it rather confusing as to when they are actually used.\n\nSince these are put in to maintain base backwards compatibility, I\nthink we are better off keep all logic about this out of\ndib-block-device and putting the values into the default configuration\nfile.  Additionally, make it clear via documentation and run-time\nchecks when these values are not going to work.\n\nChange-Id: I6b9adbb53357c37544769e747331f68513ac7871\n'}, {'number': 6, 'created': '2017-05-10 11:00:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/01bcd0b064dce956d6fbea959decdd69e8c9db50', 'message': 'Deprecate block-device commands incompatible with config file\n\nThere are a number of global block-device commands that don\'t make\nsense in a multiple-partition world -- in cases where you\'re building\nyour own block-device configuration or using the ""vm"" element (which\nuses it\'s own block-device configs).\n\nCurrently we look for the ""mkfs->mkfs_root"" entry in the configuration\nfile and merge these global values in -- but only for the fall-through\ncase where you don\'t specify a config-file and aren\'t using the vm\nelement.  Otherwise these are silently ignored.\n\nThis is done via entries the ""params"" file, so these values are passed\nto every instantiation of the block-device object unnecessarily;\nmaking it rather confusing as to when they are actually used.\n\nSince these are put in to maintain base backwards compatibility, I\nthink we are better off keep all logic about this out of\ndib-block-device and putting the values into the default configuration\nfile.  Additionally, make it clear via documentation and run-time\nchecks when these values are not going to work.\n\nChange-Id: I6b9adbb53357c37544769e747331f68513ac7871\n'}, {'number': 7, 'created': '2017-05-10 11:13:16.000000000', 'files': ['diskimage_builder/block_device/cmd.py', 'diskimage_builder/lib/common-functions', 'diskimage_builder/lib/img-defaults', 'doc/source/user_guide/building_an_image.rst', 'diskimage_builder/lib/disk-image-create', 'diskimage_builder/block_device/blockdevice.py'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/0ab5aed9df1beedffb6213912d9f4e4723cf3084', 'message': 'Deprecate block-device commands incompatible with config file\n\nThere are a number of global block-device commands that don\'t make\nsense in a multiple-partition world -- in cases where you\'re building\nyour own block-device configuration or using the ""vm"" element (which\nuses it\'s own block-device configs).\n\nCurrently we look for the ""mkfs->mkfs_root"" entry in the configuration\nfile and merge these global values in -- but only for the fall-through\ncase where you don\'t specify a config-file and aren\'t using the vm\nelement.  Otherwise these are silently ignored.\n\nThis is done via entries the ""params"" file, so these values are passed\nto every instantiation of the block-device object unnecessarily;\nmaking it rather confusing as to when they are actually used.\n\nSince these are put in to maintain base backwards compatibility, I\nthink we are better off keep all logic about this out of\ndib-block-device and putting the values into the default configuration\nfile.  Additionally, make it clear via documentation and run-time\nchecks when these values are not going to work.\n\nChange-Id: I6b9adbb53357c37544769e747331f68513ac7871\n'}]",0,463243,0ab5aed9df1beedffb6213912d9f4e4723cf3084,22,2,7,7118,,,0,"Deprecate block-device commands incompatible with config file

There are a number of global block-device commands that don't make
sense in a multiple-partition world -- in cases where you're building
your own block-device configuration or using the ""vm"" element (which
uses it's own block-device configs).

Currently we look for the ""mkfs->mkfs_root"" entry in the configuration
file and merge these global values in -- but only for the fall-through
case where you don't specify a config-file and aren't using the vm
element.  Otherwise these are silently ignored.

This is done via entries the ""params"" file, so these values are passed
to every instantiation of the block-device object unnecessarily;
making it rather confusing as to when they are actually used.

Since these are put in to maintain base backwards compatibility, I
think we are better off keep all logic about this out of
dib-block-device and putting the values into the default configuration
file.  Additionally, make it clear via documentation and run-time
checks when these values are not going to work.

Change-Id: I6b9adbb53357c37544769e747331f68513ac7871
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/43/463243/5 && git format-patch -1 --stdout FETCH_HEAD,"['diskimage_builder/block_device/cmd.py', 'diskimage_builder/lib/common-functions', 'diskimage_builder/lib/img-defaults', 'doc/source/user_guide/building_an_image.rst', 'diskimage_builder/lib/disk-image-create', 'diskimage_builder/block_device/blockdevice.py']",6,b98334966368b2167ef888909a8cdfc541e33dde,refactor/blockdevice/command_line_passing_001,," def _merge_into_config(self): """"""Merge old (default) config into new There is the need to be compatible using some old environment variables. This is done in the way, that if there is no explicit value given, these values are inserted into the current configuration. """""" for entry in self.config: for k, v in entry.items(): if k == 'mkfs': if 'name' not in v: continue if v['name'] != 'mkfs_root': continue if 'type' not in v \ and 'root-fs-type' in self.params: v['type'] = self.params['root-fs-type'] if 'opts' not in v \ and 'root-fs-opts' in self.params: v['opts'] = self.params['root-fs-opts'] if 'label' not in v \ and 'root-label' in self.params: if self.params['root-label'] is not None: v['label'] = self.params['root-label'] else: v['label'] = ""cloudimg-rootfs"" logger.debug(""Config before merge [%s]"" % config) self._merge_into_config()",58,49
openstack%2Fswift~master~Ibc8023a04ef3a14ff9c1bf5cc0f23844598a8f18,openstack/swift,master,Ibc8023a04ef3a14ff9c1bf5cc0f23844598a8f18,"Add object replicator progress, heartbeat to recon",NEW,2015-05-23 07:14:43.000000000,2017-12-18 03:24:51.000000000,,"[{'_account_id': 5600}, {'_account_id': 6968}, {'_account_id': 12193}, {'_account_id': 12279}, {'_account_id': 13052}]","[{'number': 1, 'created': '2015-05-23 07:14:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7f7c67bfd79cbeb40a7ee5bd610ae271fb889958', 'message': 'Add object replicator progress to recon stats\n\nToday the object replicator only emits very useful stats about its\nprogress into the replicator logs. In a large cluster, the only\nfeasible way to get near real-time replication status is to forward\nand index logs in real-ish time and query for it.\n\nThis exposes the same information through recon at the same interval\nthat it is emitted in the logs. The body returned by\n/recon/replication[/object] now includes an extra var named\n""object_replication_stats"" (a dict) with these three metrics:\n  - replicated: partitions replicated thus far\n  - total: total partitions to be replicated/checked\n  - time: total time elapsed for this replicator run\n\nThis also helps with automation where it is important to know whether\na replication cycle is complete.\n\nThe other numbers that are exposed in the replicator status lines can\neasily be calculated from the above three numbers.\n\nChange-Id: Ibc8023a04ef3a14ff9c1bf5cc0f23844598a8f18\nCloses-Bug: #1458132\n'}, {'number': 2, 'created': '2015-05-23 22:13:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/3e7c29329da50108fb87835a4f4c449f27069a23', 'message': 'Add object replicator progress to recon stats\n\nToday the object replicator only emits very useful stats about its\nprogress into the replicator logs. In a large cluster, the only\nfeasible way to get near real-time replication status is to forward\nand index logs in real-ish time and query for it.\n\nThis exposes the same information through recon at the same interval\nthat it is emitted in the logs. The body returned by\n/recon/replication[/object] now includes an extra var named\n""object_replication_stats"" (a dict) with these three metrics:\n  - replicated: partitions replicated thus far\n  - total: total partitions to be replicated/checked\n  - time: total time elapsed for this replicator run\n\nThis also helps with automation where it is important to know whether\na replication cycle is complete.\n\nThe other numbers that are exposed in the replicator status lines can\neasily be calculated from the above three numbers.\n\nChange-Id: Ibc8023a04ef3a14ff9c1bf5cc0f23844598a8f18\nCloses-Bug: #1458132\n'}, {'number': 3, 'created': '2017-05-10 16:07:37.000000000', 'files': ['swift/obj/replicator.py', 'swift/common/middleware/recon.py', 'swift/common/utils.py', 'test/unit/common/middleware/test_recon.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/a90da74eac790cb4ac03e10fbe9a10765573b8c5', 'message': ""Add object replicator progress, heartbeat to recon\n\nCurrently the only way to keep an eye on replication progress in\nclusters using rsync replication is by watching the replicator logs\nfor messages that expose a few stats. Monitoring this in an automated\nand consistent way that makes alerting possible then requires quite a\nlot of extra infrastructure, or gets hacky: either centralize and\nfilter your replicator logs using something like Logstash and\nElasticsearch, or worse, scrape logs locally; in either case, regexing\nand extracting the useful numbers, when they could just be exposed in\na more machine-friendly and easily accessible way.\n\nSo, this reworks a bit of the existing status reporting to use a dict\nthat contains all the existing log line stats, plus a bit more of what\nthe replicator is already tracking internally. It writes that same\ndict out to the object.recon file so that those raw numbers are made\navailable to any system that needs them via the object-server's\n/recon/replication/object endpoint. The freshness of these stats is\nstill controlled with the stats_interval config option.\n\nThis also adds a heartbeat timestamp and completed replication cycle\ncounter. The heartbeat is updated each time the replicator writes the\nstatus line and recon entries, and is useful to help locate stalled\nreplicators in the cluster (there is code that tries to prevent\nstalls, but it still occurs under specific types of disk failures and\ndoes not recover after the disk is replaced). This at least allows for\nan automated response to stalled replicators, rather than requiring a\nhuman to log in, kill it, and restart it.\n\nThe cycle counter is useful for Swift management planes that attempt\nto do automated ring rebalances/pushes, or otherwise determine whether\nthe cluster is healthy enough for someone to perform one. When an\noperator needs to perform a number of rebalances to gradually\nincrease weight on new capacity in a cluster, the minimum hours set\nin the ring is not the only thing to take into account; it's also\nnecessary to ensure, before beginning a successive rebalance, that\nevery replicator has completed at least the previous rebalance cycle\nto move relocated data. In addition, there may have been handoff data\nwritten during that initial rebalance cycle, so waiting for a second\ncycle to complete is usually a Good Idea™.\n\nIn other situations it's helpful for such a management plane to\ndetermine whether any nodes in the cluster are finishing normal,\nnon-rebalance cycles at a much lower rate than other nodes (happens\nwhen disk issues crop up that do not otherwise result in hard\nfailure during scans/reads). The cycle counter is a relatively simple\nway to meet both of these needs.\n\nChange-Id: Ibc8023a04ef3a14ff9c1bf5cc0f23844598a8f18\nCloses-Bug: #1458132\n""}]",1,185221,a90da74eac790cb4ac03e10fbe9a10765573b8c5,19,5,3,5600,,,0,"Add object replicator progress, heartbeat to recon

Currently the only way to keep an eye on replication progress in
clusters using rsync replication is by watching the replicator logs
for messages that expose a few stats. Monitoring this in an automated
and consistent way that makes alerting possible then requires quite a
lot of extra infrastructure, or gets hacky: either centralize and
filter your replicator logs using something like Logstash and
Elasticsearch, or worse, scrape logs locally; in either case, regexing
and extracting the useful numbers, when they could just be exposed in
a more machine-friendly and easily accessible way.

So, this reworks a bit of the existing status reporting to use a dict
that contains all the existing log line stats, plus a bit more of what
the replicator is already tracking internally. It writes that same
dict out to the object.recon file so that those raw numbers are made
available to any system that needs them via the object-server's
/recon/replication/object endpoint. The freshness of these stats is
still controlled with the stats_interval config option.

This also adds a heartbeat timestamp and completed replication cycle
counter. The heartbeat is updated each time the replicator writes the
status line and recon entries, and is useful to help locate stalled
replicators in the cluster (there is code that tries to prevent
stalls, but it still occurs under specific types of disk failures and
does not recover after the disk is replaced). This at least allows for
an automated response to stalled replicators, rather than requiring a
human to log in, kill it, and restart it.

The cycle counter is useful for Swift management planes that attempt
to do automated ring rebalances/pushes, or otherwise determine whether
the cluster is healthy enough for someone to perform one. When an
operator needs to perform a number of rebalances to gradually
increase weight on new capacity in a cluster, the minimum hours set
in the ring is not the only thing to take into account; it's also
necessary to ensure, before beginning a successive rebalance, that
every replicator has completed at least the previous rebalance cycle
to move relocated data. In addition, there may have been handoff data
written during that initial rebalance cycle, so waiting for a second
cycle to complete is usually a Good Idea™.

In other situations it's helpful for such a management plane to
determine whether any nodes in the cluster are finishing normal,
non-rebalance cycles at a much lower rate than other nodes (happens
when disk issues crop up that do not otherwise result in hard
failure during scans/reads). The cycle counter is a relatively simple
way to meet both of these needs.

Change-Id: Ibc8023a04ef3a14ff9c1bf5cc0f23844598a8f18
Closes-Bug: #1458132
",git fetch https://review.opendev.org/openstack/swift refs/changes/21/185221/3 && git format-patch -1 --stdout FETCH_HEAD,"['swift/obj/replicator.py', 'swift/common/middleware/recon.py', 'test/unit/common/middleware/test_recon.py']",3,7f7c67bfd79cbeb40a7ee5bd610ae271fb889958,bug/1458132," from_cache_response = { 'object_replication_time': 200.0, 'object_replication_last': 1357962809.15, 'object_replication_stats': { 'total': 528, 'replicated': 491, 'time': 0.528491}} 'object_replication_stats', 'object_replication_last': 1357962809.15, 'object_replication_stats': { 'total': 528, 'replicated': 491, 'time': 0.528491}})"," from_cache_response = {""object_replication_time"": 200.0, ""object_replication_last"": 1357962809.15} 'object_replication_last': 1357962809.15})",26,9
openstack%2Fswift~master~If60aeb13932a9c81493b9518f8b7e72b2c3d9965,openstack/swift,master,If60aeb13932a9c81493b9518f8b7e72b2c3d9965,wip: cleanup SO_REUSEPORT,NEW,2015-03-03 03:27:59.000000000,2017-12-18 03:24:42.000000000,,"[{'_account_id': 330}, {'_account_id': 1179}, {'_account_id': 13052}]","[{'number': 1, 'created': '2015-03-03 03:27:59.000000000', 'files': ['bin/swift-container-server', 'swift/common/utils.py', 'bin/swift-object-server', 'bin/swift-account-server', 'swift/common/manager.py', 'swift/common/wsgi.py', 'bin/swift-proxy-server'], 'web_link': 'https://opendev.org/openstack/swift/commit/01ea460bc3d6f48910ffcfa0006f3f94031bedda', 'message': 'wip: cleanup SO_REUSEPORT\n\nChange-Id: If60aeb13932a9c81493b9518f8b7e72b2c3d9965\n'}]",0,160621,01ea460bc3d6f48910ffcfa0006f3f94031bedda,6,3,1,1179,,,0,"wip: cleanup SO_REUSEPORT

Change-Id: If60aeb13932a9c81493b9518f8b7e72b2c3d9965
",git fetch https://review.opendev.org/openstack/swift refs/changes/21/160621/1 && git format-patch -1 --stdout FETCH_HEAD,"['bin/swift-container-server', 'swift/common/utils.py', 'bin/swift-object-server', 'bin/swift-account-server', 'swift/common/manager.py', 'swift/common/wsgi.py', 'bin/swift-proxy-server']",7,01ea460bc3d6f48910ffcfa0006f3f94031bedda,reuseport," conf_file, options = parse_options()"," conf_file, options = parse_options(server=True)",97,123
openstack%2Frally~master~Ic9f0ef1d2cda9eebfa4c09e911813dcecf7d2469,openstack/rally,master,Ic9f0ef1d2cda9eebfa4c09e911813dcecf7d2469,Add test that checks rational usage of rally.exceptions module,NEW,2015-03-15 22:50:13.000000000,2017-12-18 03:24:30.000000000,,"[{'_account_id': 14817}, {'_account_id': 22960}]","[{'number': 1, 'created': '2015-03-15 22:50:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/47c8473b84880db39fdbd76b86bb24fb4762c2a6', 'message': ""Add test that checks rational usage of rally.exceptions module\n\nIt doesn't allow you to create exceptions that are used less then 3 times.\nSo all exceptions that are used once (through and catch) won't pass this\ntest\n\nChange-Id: Ic9f0ef1d2cda9eebfa4c09e911813dcecf7d2469\n""}, {'number': 2, 'created': '2015-03-15 23:36:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a2e1396a1f71f67da11dd3272cd600b102b1c106', 'message': ""Add test that checks rational usage of rally.exceptions module\n\nIt doesn't allow you to create exceptions that are used less then 3 times.\nSo all exceptions that are used once (through and catch) won't pass this\ntest\n\nChange-Id: Ic9f0ef1d2cda9eebfa4c09e911813dcecf7d2469\n""}, {'number': 3, 'created': '2015-03-16 11:42:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/99e7f4ed3d1bf0c0ce1b83eb9bba2772009ae4b2', 'message': ""Add test that checks rational usage of rally.exceptions module\n\nIt doesn't allow you to create exceptions that are used less then 3 times.\nSo all exceptions that are used once (through and catch) won't pass this\ntest\n\nChange-Id: Ic9f0ef1d2cda9eebfa4c09e911813dcecf7d2469\n""}, {'number': 4, 'created': '2015-03-16 11:55:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/19888c953d23ac3b1b9d1b5b8e5a65a5eb525c2e', 'message': ""Add test that checks rational usage of rally.exceptions module\n\nIt doesn't allow you to create exceptions that are used less then 3 times.\nSo all exceptions that are used once (through and catch) won't pass this\ntest\n\nChange-Id: Ic9f0ef1d2cda9eebfa4c09e911813dcecf7d2469\n""}, {'number': 5, 'created': '2017-05-15 04:27:09.000000000', 'files': ['tests/unit/test_exceptions.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/190f6428b1550f49295ba11d82bdc1cbbbb1d571', 'message': ""Add test that checks rational usage of rally.exceptions module\n\nIt doesn't allow you to create exceptions that are used less then 3 times.\nSo all exceptions that are used once (through and catch) won't pass this\ntest\n\nChange-Id: Ic9f0ef1d2cda9eebfa4c09e911813dcecf7d2469\n""}]",1,164548,190f6428b1550f49295ba11d82bdc1cbbbb1d571,14,2,5,6172,,,0,"Add test that checks rational usage of rally.exceptions module

It doesn't allow you to create exceptions that are used less then 3 times.
So all exceptions that are used once (through and catch) won't pass this
test

Change-Id: Ic9f0ef1d2cda9eebfa4c09e911813dcecf7d2469
",git fetch https://review.opendev.org/openstack/rally refs/changes/48/164548/4 && git format-patch -1 --stdout FETCH_HEAD,['tests/unit/test_exceptions.py'],1,47c8473b84880db39fdbd76b86bb24fb4762c2a6,improve_hackings,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import os import re from rally.common import utils from rally import exceptions from tests.unit import test class ExceptionsTestCase(test.TestCase): rally_path = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir, os.pardir, ""rally"") def test_rational_usage_of_exceptions(self): list_of_modules = [] for (dirpath, dirnames, filenames) in os.walk(self.rally_path): for filename in filenames: if (filename.endswith("".py"") and not filename.endswith(""exceptions.py"")): list_of_modules.append(os.sep.join([dirpath, filename])) all_exc_repr = [] for e in utils.itersubclasses(exceptions.RallyException): all_exc_repr.append((re.compile(""\.%s\("" % e.__name__), e.__name__)) exc_usage = dict([(k[1], 0) for k in all_exc_repr]) all_code = """" for module in list_of_modules: with open(module) as f: all_code += f.read() for exc in all_exc_repr: exc_usage[exc[1]] += len(list(exc[0].finditer(all_code))) useless_exc = [] for k, v in exc_usage.items(): if v < 3: useless_exc.append(k) self.assertEqual([], sorted(useless_exc), ""You have useless exceptions remove them"") ",,59,0
openstack%2Fswift~master~Iccc38a5bd5c98162b21f4b2b316eb012053fe85d,openstack/swift,master,Iccc38a5bd5c98162b21f4b2b316eb012053fe85d,Add composite_metadata as an attribute of RingData,NEW,2017-04-20 16:33:48.000000000,2017-12-18 03:23:26.000000000,,"[{'_account_id': 4608}, {'_account_id': 7847}, {'_account_id': 13052}]","[{'number': 1, 'created': '2017-04-20 16:33:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7f65a697f84d52cb503d53510fc3ce2d1491e78c', 'message': ""Make composite_metadata an attribute of RingData\n\n**Intended to squash into parent patch**\n\nInstead of having a 'metadata' attribute for unstructured\nmetadata in the RingData, and setting 'composite_metadata'\nas a key in that metadata, make composite_metadata an\nattr of the class.\n\nIF we are going to persist composite metadata in a ring.gz then\nthe reason is to future-proof in case we ever want to read it\nback. Given that, have this attr set when constructing RingData\nfor a composite ring - otherwise there is a risk that the\ncomposite_metadata is not set later.\n\nChange-Id: Iccc38a5bd5c98162b21f4b2b316eb012053fe85d\n""}, {'number': 2, 'created': '2017-05-02 10:22:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/16dd8daf06c08ad7217823b112785f16443d275d', 'message': ""Make composite_metadata an attribute of RingData\n\n**Intended to squash into parent patch**\n\nInstead of having a 'metadata' attribute for unstructured\nmetadata in the RingData, and setting 'composite_metadata'\nas a key in that metadata, make composite_metadata an\nattr of the class.\n\nIF we are going to persist composite metadata in a ring.gz then\nthe reason is to future-proof in case we ever want to read it\nback. Given that, have this attr set when constructing RingData\nfor a composite ring - otherwise there is a risk that the\ncomposite_metadata is not set later.\n\nChange-Id: Iccc38a5bd5c98162b21f4b2b316eb012053fe85d\n""}, {'number': 3, 'created': '2017-05-10 16:17:13.000000000', 'files': ['swift/common/ring/composite_builder.py', 'test/unit/common/ring/test_ring.py', 'swift/common/ring/ring.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/0cd11bc470a80cc020644e2b8c5b82219e0e5e17', 'message': 'Add composite_metadata as an attribute of RingData\n\nEnable composite ring metadata to added to RingData and\npersisted in ring .gz files.\n\nRelated-Change: I0d8928b55020592f8e75321d1f7678688301d797\nChange-Id: Iccc38a5bd5c98162b21f4b2b316eb012053fe85d\n'}]",2,458585,0cd11bc470a80cc020644e2b8c5b82219e0e5e17,13,3,3,7847,,,0,"Add composite_metadata as an attribute of RingData

Enable composite ring metadata to added to RingData and
persisted in ring .gz files.

Related-Change: I0d8928b55020592f8e75321d1f7678688301d797
Change-Id: Iccc38a5bd5c98162b21f4b2b316eb012053fe85d
",git fetch https://review.opendev.org/openstack/swift refs/changes/85/458585/3 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/ring/composite_builder.py', 'test/unit/common/ring/test_ring.py', 'test/unit/common/ring/test_composite_builder.py', 'swift/common/ring/ring.py']",4,7f65a697f84d52cb503d53510fc3ce2d1491e78c,composite-ring-builder," def __init__(self, replica2part2dev_id, devs, part_shift, composite_metadata=None): self._composite_metadata = composite_metadata @property def composite_metadata(self): # use property here to cope with instances loaded from old-style # pickled classes that may not have a _composite_metadata attribute return getattr(self, '_composite_metadata', None) Deserialize a v1 ring file into a dictionary which will have `devs`, `part_shift`, and `replica2part2dev_id` keys and may have `byteorder` and `composite_metadata` keys. :param bool metadata_only: If True, do not load `replica2part2dev_id`. :returns: A dict containing `devs`, `part_shift`, `replica2part2dev_id` and optionally `byteorder` and `composite_metadata`. :param bool metadata_only: If True, do not load `replica2part2dev_id`. # older rings may not have composite_metadata in their dict composite_metadata = ring_data.get('composite_metadata') composite_metadata=composite_metadata) 'byteorder': sys.byteorder, 'composite_metadata': ring['composite_metadata']}) 'composite_metadata': self.composite_metadata}"," def __init__(self, replica2part2dev_id, devs, part_shift, metadata=None): self.metadata = {} if metadata is None else metadata Deserialize a v1 ring file into a dictionary with `devs`, `part_shift`, and `replica2part2dev_id` keys. :param bool metadata_only: If True, only load `devs` and `part_shift` :returns: A dict containing `devs`, `part_shift`, and `replica2part2dev_id` :param bool metadata_only: If True, only load `devs` and `part_shift`. # metadata is optional and old ring can be None ring_data.get('metadata')) 'byteorder': sys.byteorder, 'metadata': ring['metadata']}) 'metadata': self.metadata} self._metadata = ring_data.metadata",82,52
openstack%2Ftacker~master~Ie1bfe6c628108a223e7db52c4f803735660caaf8,openstack/tacker,master,Ie1bfe6c628108a223e7db52c4f803735660caaf8,Change VNF status to ERROR when update fails,NEW,2016-10-09 14:52:58.000000000,2017-12-18 03:23:22.000000000,,"[{'_account_id': 18546}, {'_account_id': 18955}]","[{'number': 1, 'created': '2016-10-09 14:52:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/8fc9259854024640fb1770343d3801f30930d935', 'message': 'Change VNF status to ERROR when update fails\n\nCurrently, VNF status will stuck in PENDING_UPDATE when update fails.\nThis patch will change the status to ERROR in this case.\n\nChange-Id: Ie1bfe6c628108a223e7db52c4f803735660caaf8\nCloses-bug: #1631752\n'}, {'number': 2, 'created': '2016-10-10 09:32:16.000000000', 'files': ['tacker/vnfm/plugin.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/b94d50d20aedaee59658c8d23d65f2ef8a4ded3a', 'message': 'Change VNF status to ERROR when update fails\n\nCurrently, VNF status will stuck in PENDING_UPDATE when update fails.\nThis patch will change the status to ERROR in this case.\n\nChange-Id: Ie1bfe6c628108a223e7db52c4f803735660caaf8\nCloses-bug: #1631752\n'}]",0,384217,b94d50d20aedaee59658c8d23d65f2ef8a4ded3a,8,2,2,18546,,,0,"Change VNF status to ERROR when update fails

Currently, VNF status will stuck in PENDING_UPDATE when update fails.
This patch will change the status to ERROR in this case.

Change-Id: Ie1bfe6c628108a223e7db52c4f803735660caaf8
Closes-bug: #1631752
",git fetch https://review.opendev.org/openstack/tacker refs/changes/17/384217/1 && git format-patch -1 --stdout FETCH_HEAD,['tacker/vnfm/plugin.py'],1,8fc9259854024640fb1770343d3801f30930d935,bug/1631752," self._update_vnf_post(context, vnf_id, constants.ERROR, vnf_dict)"," self._update_vnf_post(context, vnf_id, constants.ERROR)",1,1
openstack%2Ftacker~master~I7f5b830dea1598e307b5ae33ce54dda086a70afa,openstack/tacker,master,I7f5b830dea1598e307b5ae33ce54dda086a70afa,Fix post update event creation failure when VNF update fails,NEW,2016-10-10 16:40:55.000000000,2017-12-18 03:23:19.000000000,,[{'_account_id': 10487}],"[{'number': 1, 'created': '2016-10-10 16:40:55.000000000', 'files': ['tacker/db/vnfm/vnfm_db.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/bc530f253e3e336c282e1acb0369ded7f18ffd4c', 'message': 'Fix post update event creation failure when VNF update fails\n\nRefer to\nhttps://bugs.launchpad.net/tacker/+bug/1632018\nPost update event creation will fail if VNF update fails,\nthis patch will fix this.\n\nChange-Id: I7f5b830dea1598e307b5ae33ce54dda086a70afa\nCloses-Bug: #1632018\n'}]",1,384629,bc530f253e3e336c282e1acb0369ded7f18ffd4c,5,1,1,18546,,,0,"Fix post update event creation failure when VNF update fails

Refer to
https://bugs.launchpad.net/tacker/+bug/1632018
Post update event creation will fail if VNF update fails,
this patch will fix this.

Change-Id: I7f5b830dea1598e307b5ae33ce54dda086a70afa
Closes-Bug: #1632018
",git fetch https://review.opendev.org/openstack/tacker refs/changes/29/384629/1 && git format-patch -1 --stdout FETCH_HEAD,['tacker/db/vnfm/vnfm_db.py'],1,bc530f253e3e336c282e1acb0369ded7f18ffd4c,bug/1632018," timestamp = new_vnf_dict[constants.RES_EVT_UPDATED_FLD] if not timestamp: timestamp = timeutils.utcnow() tstamp=timestamp) self._update_vnf_post(context, vnf_id, constants.ACTIVE, vnf_dict)"," tstamp=new_vnf_dict[constants.RES_EVT_UPDATED_FLD]) self._update_vnf_post(context, vnf_id, constants.ACTIVE)",5,2
openstack%2Frally~master~Iea011f295e9f548099479f9d7f796356009d61c8,openstack/rally,master,Iea011f295e9f548099479f9d7f796356009d61c8,Add create_and_delete scenario to loadbalancer,NEW,2016-12-26 11:16:27.000000000,2017-12-18 03:22:55.000000000,,"[{'_account_id': 14817}, {'_account_id': 21528}, {'_account_id': 22960}]","[{'number': 1, 'created': '2016-12-26 11:16:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5eaa7c82315efb33e6f02b6bd6cec65189bfe7d0', 'message': '[WIP]Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 3, 'created': '2016-12-27 00:08:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ae2c11ce50f4948d5c6558165a22bf71bf4c0911', 'message': '[WIP]Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 4, 'created': '2016-12-27 01:59:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8a617fc08b00032658b73ea223e1a42dc8bce256', 'message': '[WIP]Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 5, 'created': '2016-12-27 05:39:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8ebe25bb3b6ad1d7fb9ddaa6f96bae10e81134ed', 'message': '[WIP]Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 6, 'created': '2016-12-27 07:06:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/be479aee597539da999cf814180f463c32771daa', 'message': '[WIP]Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 7, 'created': '2016-12-27 10:46:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6b41c7f546f2d5a6bacc7f19555176e0ecdd6bfb', 'message': '[WIP]Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 8, 'created': '2016-12-28 01:40:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6b0bdba786618b69074f8ea34d2f8f58b2957cc6', 'message': '[WIP]Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 9, 'created': '2016-12-28 03:18:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/19bbb2433af8740be5cbe55f001dd8b69ec19499', 'message': '[WIP]Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 10, 'created': '2016-12-28 05:22:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d525c87f8c7dc84185af9621f0fd0930d08f4571', 'message': '[WIP]Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 11, 'created': '2016-12-29 01:30:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/11db0050879bfb0b443966c6d092a7365686f683', 'message': '[WIP]Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 12, 'created': '2016-12-29 07:13:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5e7dd36b7a1f0d25f0fdea58438e99f5737d2a88', 'message': '[WIP]Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 13, 'created': '2017-01-23 01:32:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/cdb6d4767a9ffa87fff8b7f42343c9a194870798', 'message': '[WIP]Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 14, 'created': '2017-01-23 07:30:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4924080181dab2e20d7c9f76a3616da2692510a3', 'message': 'Add create_and_delete scenario to loadbalaner\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 15, 'created': '2017-02-15 00:57:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ebeb34789061356e84ca140af95ce4e9708d2cf1', 'message': 'Add create_and_delete scenario to loadbalancer\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}, {'number': 16, 'created': '2017-05-22 01:15:41.000000000', 'files': ['samples/tasks/scenarios/neutron/create-and-delete-loadbalancers.yaml', 'rally/plugins/openstack/cfg/neutron.py', 'tests/unit/plugins/openstack/scenarios/neutron/test_loadbalancer_v2.py', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'rally/plugins/openstack/scenarios/neutron/loadbalancer_v2.py', 'samples/tasks/scenarios/neutron/create-and-delete-loadbalancers.json', 'rally-jobs/rally-neutron-extensions.yaml', 'tests/unit/plugins/openstack/scenarios/neutron/test_utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/64c6ae3b3463b58001e306e875d0764047043df1', 'message': 'Add create_and_delete scenario to loadbalancer\n\nAdd NeutronLoadbalancerV2.create_and_delete_loadbalancers\nscenario to our lbaasv2.\n\nChange-Id: Iea011f295e9f548099479f9d7f796356009d61c8\n'}]",2,414934,64c6ae3b3463b58001e306e875d0764047043df1,88,3,15,21528,,,0,"Add create_and_delete scenario to loadbalancer

Add NeutronLoadbalancerV2.create_and_delete_loadbalancers
scenario to our lbaasv2.

Change-Id: Iea011f295e9f548099479f9d7f796356009d61c8
",git fetch https://review.opendev.org/openstack/rally refs/changes/34/414934/16 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/openstack/scenarios/neutron/utils.py', 'rally/plugins/openstack/scenarios/neutron/loadbalancer_v2.py', 'rally-jobs/rally-neutron-extensions.yaml']",3,5eaa7c82315efb33e6f02b6bd6cec65189bfe7d0,lbaas," NeutronLoadbalancerV2.create_and_delete_loadbalancers: - args: lb_create_args: {} runner: type: ""constant"" times: 5 concurrency: 2 context: users: tenants: 2 users_per_tenant: 2 network: {} sla: failure_rate: max: 0",,79,1
openstack%2Fsushy~master~I30e3899ecc4b5bcc543581d6d18a29a42dd18dfe,openstack/sushy,master,I30e3899ecc4b5bcc543581d6d18a29a42dd18dfe,Parse generic attributions for redfish resource,NEW,2017-03-23 00:49:17.000000000,2017-12-18 03:22:38.000000000,,"[{'_account_id': 6773}, {'_account_id': 10239}, {'_account_id': 12597}, {'_account_id': 16635}]","[{'number': 1, 'created': '2017-03-23 00:49:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy/commit/4cdd397fc03f380cc5ec87c888ed664e4587ed66', 'message': ""Parse generic attributions for redfish resource\n\nAll redfish resources shall include '@odata.id' and '@odata.type'\n[0]. This patch is intended to parse type name from '@odata.type'\nattribution, and ignore '@odata.id' because it is the same as\nresource path.\n\n[0] http://redfish.dmtf.org/schemas/DSP0266_1.1.html#resource-identifier-property\n\nChange-Id: I30e3899ecc4b5bcc543581d6d18a29a42dd18dfe\n""}, {'number': 2, 'created': '2017-03-24 00:04:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy/commit/44d8de1bd996a92275b6e23cece70ac90b663b74', 'message': ""Parse generic attributions for redfish resource\n\nAll redfish resources shall include '@odata.id' and '@odata.type'\n[0]. This patch is intended to parse type name from '@odata.type'\nattribution, and ignore '@odata.id' because it is the same as\nresource path.\n\n[0] http://redfish.dmtf.org/schemas/DSP0266_1.1.html#resource-identifier-property\n\nChange-Id: I30e3899ecc4b5bcc543581d6d18a29a42dd18dfe\n""}, {'number': 3, 'created': '2017-03-24 00:33:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy/commit/2448994add1fcd1048b78f4ef5c1bcfff1f65e82', 'message': ""Parse generic attributions for redfish resource\n\nAll redfish resources shall include '@odata.id' and '@odata.type'\n[0]. This patch is intended to parse type name from '@odata.type'\nattribution, and ignore '@odata.id' because it is the same as\nresource path.\n\n[0] http://redfish.dmtf.org/schemas/DSP0266_1.1.html#resource-identifier-property\n\nChange-Id: I30e3899ecc4b5bcc543581d6d18a29a42dd18dfe\n""}, {'number': 4, 'created': '2017-03-24 00:56:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy/commit/c0a0dcf5d259b5a0af25aa8b1462aca5819093f5', 'message': ""Parse generic attributions for redfish resource\n\nAll redfish resources shall include '@odata.id' and '@odata.type'\n[0]. This patch is intended to parse type name from '@odata.type'\nattribution, and ignore '@odata.id' because it is the same as\nresource path.\n\n[0] http://redfish.dmtf.org/schemas/DSP0266_1.1.html#resource-identifier-property\n\nChange-Id: I30e3899ecc4b5bcc543581d6d18a29a42dd18dfe\n""}, {'number': 5, 'created': '2017-03-27 14:46:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy/commit/bcf45b1fbf34ebc16d974a610ea175ea4914e4ea', 'message': ""Parse generic attributions for redfish resource\n\nAll redfish resources shall include '@odata.id' and '@odata.type'\n[0]. This patch is intended to parse type name from '@odata.type'\nattribution, and ignore '@odata.id' because it is the same as\nresource path.\n\n[0] http://redfish.dmtf.org/schemas/DSP0266_1.1.html#resource-identifier-property\n\nChange-Id: I30e3899ecc4b5bcc543581d6d18a29a42dd18dfe\n""}, {'number': 6, 'created': '2017-04-19 20:28:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy/commit/704eed6bce9b5e064bf9b07ef1b681c3701b37a7', 'message': ""Parse generic attributions for redfish resource\n\nAll redfish resources shall include '@odata.id' and '@odata.type'\n[0]. This patch is intended to parse type name from '@odata.type'\nattribution, and ignore '@odata.id' because it is the same as\nresource path.\n\n[0] http://redfish.dmtf.org/schemas/DSP0266_1.1.html#resource-identifier-property\n\nChange-Id: I30e3899ecc4b5bcc543581d6d18a29a42dd18dfe\n""}, {'number': 7, 'created': '2017-05-18 22:04:41.000000000', 'files': ['sushy/resources/system/system.py', 'sushy/tests/unit/resources/test_base.py', 'sushy/resources/base.py'], 'web_link': 'https://opendev.org/openstack/sushy/commit/dc4711c823b69cddc92092cb0fed1d749d55647b', 'message': ""Parse generic attributions for redfish resource\n\nAll redfish resources shall include '@odata.id' and '@odata.type'\n[0]. This patch is intended to parse type name from '@odata.type'\nattribution, and ignore '@odata.id' because it is the same as\nresource path.\n\n[0] http://redfish.dmtf.org/schemas/DSP0266_1.1.html#resource-identifier-property\n\nChange-Id: I30e3899ecc4b5bcc543581d6d18a29a42dd18dfe\n""}]",12,448834,dc4711c823b69cddc92092cb0fed1d749d55647b,26,4,7,12597,,,0,"Parse generic attributions for redfish resource

All redfish resources shall include '@odata.id' and '@odata.type'
[0]. This patch is intended to parse type name from '@odata.type'
attribution, and ignore '@odata.id' because it is the same as
resource path.

[0] http://redfish.dmtf.org/schemas/DSP0266_1.1.html#resource-identifier-property

Change-Id: I30e3899ecc4b5bcc543581d6d18a29a42dd18dfe
",git fetch https://review.opendev.org/openstack/sushy refs/changes/34/448834/5 && git format-patch -1 --stdout FETCH_HEAD,"['sushy/resources/system/system.py', 'sushy/tests/unit/test_main.py', 'sushy/tests/unit/resources/system/test_system.py', 'sushy/main.py', 'sushy/resources/base.py']",5,4cdd397fc03f380cc5ec87c888ed664e4587ed66,," def _parse_attributes(self): """"""Parse all generic attributes of a resource Parse below mandatory attributions for every redfish resource: '@odata.type' - the type property of resource '@odata.id' - the unique identifier property. Ignore it because it is same as 'path' attribution # Parse the type name. The '@odata.type' attribution shall follow # format #Namespace.TypeName resource_type = self.json.get('@odata.type') self.type_name = resource_type[resource_type.rfind('.') + 1:] super(ResourceCollectionBase, self)._parse_attributes() ","@six.add_metaclass(abc.ABCMeta) @abc.abstractmethod def _parse_attributes(self): """"""Parse the attributes of a resource This method should be overwritten and is responsible for parsing all the attributes of a resource.",19,5
openstack%2Fswift~master~I637c51b6ff22db8b5e7b02c9bc9f4eaec709912c,openstack/swift,master,I637c51b6ff22db8b5e7b02c9bc9f4eaec709912c,Replace assertRaisesRegexp with assertRaisesRegex,NEW,2017-05-19 06:49:40.000000000,2017-12-18 03:22:35.000000000,,"[{'_account_id': 13052}, {'_account_id': 15343}]","[{'number': 1, 'created': '2017-05-19 06:49:40.000000000', 'files': ['test/unit/common/middleware/crypto/test_keymaster.py', 'test/unit/account/test_backend.py', 'test/unit/common/test_daemon.py', 'test/unit/common/test_utils.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/8a2fa1fe96175c55cd0b1adf8cb6a680f6200429', 'message': 'Replace assertRaisesRegexp with assertRaisesRegex\n\nThis replaces the deprecated (in python 3.2) unittest.TestCase\nmethod assertRaisesRegexp() with assertRaisesRegex()\n\nChange-Id: I637c51b6ff22db8b5e7b02c9bc9f4eaec709912c\n'}]",0,466205,8a2fa1fe96175c55cd0b1adf8cb6a680f6200429,5,2,1,20401,,,0,"Replace assertRaisesRegexp with assertRaisesRegex

This replaces the deprecated (in python 3.2) unittest.TestCase
method assertRaisesRegexp() with assertRaisesRegex()

Change-Id: I637c51b6ff22db8b5e7b02c9bc9f4eaec709912c
",git fetch https://review.opendev.org/openstack/swift refs/changes/05/466205/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/common/middleware/crypto/test_keymaster.py', 'test/unit/account/test_backend.py', 'test/unit/common/test_daemon.py', 'test/unit/common/test_utils.py']",4,8a2fa1fe96175c55cd0b1adf8cb6a680f6200429,RaisesRegex, self.assertRaisesRegex(, self.assertRaisesRegexp(,8,8
openstack%2Frally~master~I971b25eb24009ff46bf71e4e9eb14fd58c28f541,openstack/rally,master,I971b25eb24009ff46bf71e4e9eb14fd58c28f541,specify ext_net when create router with external gateway,NEW,2017-05-25 10:03:13.000000000,2017-12-18 03:22:03.000000000,,"[{'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 25072}]","[{'number': 1, 'created': '2017-05-25 10:03:13.000000000', 'files': ['rally/plugins/openstack/scenarios/neutron/utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/dc58aa1382cfa106c165829dce1bbd527ef380a3', 'message': 'specify ext_net when create router with external gateway\n\nWhen there are several external networks in OpenStack, which external\nnetwork to use should be specified. This patch is to use to specify\nthe external network.\n\nChange-Id: I971b25eb24009ff46bf71e4e9eb14fd58c28f541\n'}]",1,467969,dc58aa1382cfa106c165829dce1bbd527ef380a3,6,3,1,25072,,,0,"specify ext_net when create router with external gateway

When there are several external networks in OpenStack, which external
network to use should be specified. This patch is to use to specify
the external network.

Change-Id: I971b25eb24009ff46bf71e4e9eb14fd58c28f541
",git fetch https://review.opendev.org/openstack/rally refs/changes/69/467969/1 && git format-patch -1 --stdout FETCH_HEAD,['rally/plugins/openstack/scenarios/neutron/utils.py'],1,dc58aa1382cfa106c165829dce1bbd527ef380a3,," def _create_router(self, router_create_args, external_gw=False, ext_net=None): :param router_create_args: POST /v2.0/routers request optionsi :param external_gw: bool, whether or not create router with external gateway :param ext_net: the id of external network when create router with external gateway if ext_net is not None: gw_info = {""network_id"": ext_net[""id""], ""enable_snat"": True} router_create_args.setdefault(""external_gateway_info"", gw_info) else: for network in self._list_networks(): if network.get(""router:external""): external_network = network gw_info = {""network_id"": external_network[""id""], ""enable_snat"": True} router_create_args.setdefault(""external_gateway_info"", gw_info)"," def _create_router(self, router_create_args, external_gw=False): :param router_create_args: POST /v2.0/routers request options for network in self._list_networks(): if network.get(""router:external""): external_network = network gw_info = {""network_id"": external_network[""id""], ""enable_snat"": True} router_create_args.setdefault(""external_gateway_info"", gw_info) ",20,10
openstack%2Fdiskimage-builder~master~If0f2435b01d5245f45520a984a0617e6c27fc07a,openstack/diskimage-builder,master,If0f2435b01d5245f45520a984a0617e6c27fc07a,Move config out of parameter YAML file,NEW,2017-05-08 05:15:11.000000000,2017-12-18 03:21:45.000000000,,"[{'_account_id': 6133}, {'_account_id': 10118}]","[{'number': 1, 'created': '2017-05-08 05:15:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/9b4fa08af685d3561077be8af63619cba1c944be', 'message': 'Move config out of parameter YAML file\n\nThe configuration file is only parsed once during the ""init"" phase.\nMove the configuration file out to an argument of the init call so it\nis clear that it is specific to that phase.\n\nChange-Id: If0f2435b01d5245f45520a984a0617e6c27fc07a\n'}, {'number': 2, 'created': '2017-05-10 05:14:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/164845bcec64836c706c85c3f4f8481c052e2703', 'message': 'Move config out of parameter YAML file\n\nThe configuration file is only parsed once during the ""init"" phase.\nMove the configuration file out to an argument of the init call so it\nis clear that it is specific to that phase.\n\nChange-Id: If0f2435b01d5245f45520a984a0617e6c27fc07a\n'}, {'number': 3, 'created': '2017-05-10 10:12:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/ea72bcaf3420049550eaeef33c0d90cd511d1d04', 'message': 'Move config out of parameter YAML file\n\nThe configuration file is only parsed once during the ""init"" phase.\nMove the configuration file out to an argument of the init call so it\nis clear that it is specific to that phase.\n\nChange-Id: If0f2435b01d5245f45520a984a0617e6c27fc07a\n'}, {'number': 4, 'created': '2017-05-10 10:38:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/1639a8aa9216b2fdca4b0284752d5c17e29e5a6c', 'message': 'Move config out of parameter YAML file\n\nThe configuration file is only parsed once during the ""init"" phase.\nMove the configuration file out to an argument of the init call so it\nis clear that it is specific to that phase.\n\nChange-Id: If0f2435b01d5245f45520a984a0617e6c27fc07a\n'}, {'number': 5, 'created': '2017-05-10 10:45:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/4c4a09d64e611d687b942fb6be7a6a17e9d863b8', 'message': 'Move config out of parameter YAML file\n\nThe configuration file is only parsed once during the ""init"" phase.\nMove the configuration file out to an argument of the init call so it\nis clear that it is specific to that phase.\n\nChange-Id: If0f2435b01d5245f45520a984a0617e6c27fc07a\n'}, {'number': 6, 'created': '2017-05-10 11:00:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/52770e9d833461b51cacf866e9870b33b7535a2a', 'message': 'Move config out of parameter YAML file\n\nThe configuration file is only parsed once during the ""init"" phase.\nMove the configuration file out to an argument of the init call so it\nis clear that it is specific to that phase.\n\nChange-Id: If0f2435b01d5245f45520a984a0617e6c27fc07a\n'}, {'number': 7, 'created': '2017-05-10 11:13:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/591ee9d2233aaf8e768e7ea7987ecc3b88880c03', 'message': 'Move config out of parameter YAML file\n\nThe configuration file is only parsed once during the ""init"" phase.\nMove the configuration file out to an argument of the init call so it\nis clear that it is specific to that phase.\n\nChange-Id: If0f2435b01d5245f45520a984a0617e6c27fc07a\n'}, {'number': 8, 'created': '2017-05-11 23:40:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/674fd171c658c87b31afd2d1abf8728099e258a9', 'message': 'Move config out of parameter YAML file\n\nThe configuration file is only parsed once during the ""init"" phase.\nMove the configuration file out to an argument of the init call so it\nis clear that it is specific to that phase.\n\nChange-Id: If0f2435b01d5245f45520a984a0617e6c27fc07a\n'}, {'number': 9, 'created': '2017-05-26 20:03:53.000000000', 'files': ['diskimage_builder/block_device/cmd.py', 'diskimage_builder/lib/disk-image-create', 'diskimage_builder/block_device/blockdevice.py'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/4e485999540c9faf8c314603b8c242e54dc1759c', 'message': 'Move config out of parameter YAML file\n\nThe configuration file is only parsed once during the ""init"" phase.\nMove the configuration file out to an argument of the init call so it\nis clear that it is specific to that phase.\n\nChange-Id: If0f2435b01d5245f45520a984a0617e6c27fc07a\n'}]",0,463242,4e485999540c9faf8c314603b8c242e54dc1759c,30,2,9,7118,,,0,"Move config out of parameter YAML file

The configuration file is only parsed once during the ""init"" phase.
Move the configuration file out to an argument of the init call so it
is clear that it is specific to that phase.

Change-Id: If0f2435b01d5245f45520a984a0617e6c27fc07a
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/42/463242/9 && git format-patch -1 --stdout FETCH_HEAD,"['diskimage_builder/block_device/cmd.py', 'diskimage_builder/lib/disk-image-create', 'diskimage_builder/block_device/blockdevice.py']",3,9b4fa08af685d3561077be8af63619cba1c944be,refactor/blockdevice/command_line_passing_001," def cmd_init(self, config): Arguments: :param config: configuration dictionary logger.debug(""Config before merge [%s]"" % config) self.config = self._config_tree_to_digraph(config, self.plugin_manager)"," def cmd_init(self): with open(self.params['config'], ""rt"") as config_fd: self.config = yaml.safe_load(config_fd) logger.debug(""Config before merge [%s]"" % self.config) self.config = self._config_tree_to_digraph(self.config, self.plugin_manager) logger.debug(""Config before merge [%s]"" % self.config)",15,10
openstack%2Fironic~master~Id11a44aaf69d4bbd04b501cd16615546b1b95f21,openstack/ironic,master,Id11a44aaf69d4bbd04b501cd16615546b1b95f21,ETAGs maintenance for all api node's requests,NEW,2016-11-01 16:00:12.000000000,2017-12-18 03:20:49.000000000,,"[{'_account_id': 6618}, {'_account_id': 10118}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 18893}, {'_account_id': 19003}, {'_account_id': 19072}, {'_account_id': 19213}, {'_account_id': 19339}, {'_account_id': 19604}, {'_account_id': 22724}]","[{'number': 1, 'created': '2016-11-01 16:00:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d7f9359fac0ed31a87f55ab1a57201ed2dc675c0', 'message': ""ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch should solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 2, 'created': '2016-11-02 10:08:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1454ed9818381c7dc7a7687d95648d290826a8cc', 'message': ""ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\n\nAdded etag in the data model for now.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 3, 'created': '2016-11-02 17:03:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0b217b130b57d3cad606ae7d5fb8adf1ba52b91c', 'message': ""ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nTrying to implement draft for cache just in case. Added etag\nin the data model for now. Put stubs to requests processing.\nAdded exception when user tries to update with out-of-date tag.\nAdded checking api micro versions.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 4, 'created': '2016-11-03 10:29:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/aab7b5d461f18150a355b74dc32e05cdf8184e79', 'message': ""ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nTrying to implement draft for cache just in case. Added etag\nin the data model for now. Put stubs to requests processing.\nAdded exception when user tries to update with out-of-date tag.\nAdded checking api micro versions.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 5, 'created': '2016-11-16 12:46:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ce1de95c82a020d0f14e6eece0e88989be2ecaaa', 'message': ""ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nTrying to implement draft for cache just in case. Added etag\nin the data model for now. Put stubs to requests processing.\nAdded exception when user tries to update with out-of-date tag.\nAdded checking api micro versions.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 6, 'created': '2016-11-25 12:46:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/36c3b1d2410efe7f55f37cd596a35068d90662ab', 'message': ""POC: ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nAdded etag in the data model for now.\nAdded exception when user tries to update with out-of-date tag.\nAdded checking api micro versions.\nAdded processsing on post, get, patch requests\n\nDone this for node and some workaround for chassis.\nTests for etags are to be written.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 7, 'created': '2016-11-25 13:07:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3143004bdd82fcd9afa7cdf8e735500741f0f5d9', 'message': ""POC: ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nAdded etag in the data model for now.\nAdded exception when user tries to update with out-of-date tag.\nAdded checking api micro versions.\nAdded processsing on post, get, patch requests\n\nDone this for node and some workaround for chassis.\nTests for etags are to be written.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 8, 'created': '2016-11-25 14:20:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/57bb3e24fd18c799f61bac9484d16b81ef613684', 'message': ""POC: ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nAdded etag in the data model for now.\nAdded exception when user tries to update with out-of-date tag.\nAdded checking api micro versions.\nAdded processsing on post, get, patch requests\n\nDone this for node and some workaround for chassis.\nTests for etags are to be written.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 9, 'created': '2016-11-30 10:19:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cbbeab58503bbef5bd05d9580bbd23c9009ed971', 'message': ""POC: ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nAdded etag in the data model for now.\nAdded exception when user tries to update with out-of-date tag.\nAdded checking api micro versions.\nAdded processsing on post, get, patch requests\n\nDone this for node and some workaround for chassis.\nTests for etags are to be written.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 10, 'created': '2016-12-02 13:24:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cdd399e2ffd2dbac17c7c0bf70487bcfe11c23a0', 'message': ""POC: ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nAdded etag in the data model for now.\nAdded exception when user tries to update with out-of-date tag.\nAdded checking api micro versions.\nAdded processsing on post, get, patch requests\n\nDone this for node and some workaround for chassis.\nTests for etags are to be written.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 11, 'created': '2016-12-05 13:03:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/af4fdb2674922467b8c1ad0eddd34bb02cbc09b4', 'message': ""[WIP] ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nAdded etag in the data model for now.\nAdded exception when user tries to update with out-of-date tag.\nAdded checking api micro versions.\nAdded processsing on post, get, patch requests\n\nDone this for node and some workaround for chassis.\nTests for etags are to be written.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 12, 'created': '2016-12-09 08:49:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/da4ced43c6f342cb33c20fe1f811f1edce29c5ce', 'message': ""[WIP] ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nRemainining:\n- etag for delete, patch\n- same work for port,portgroup, chassis\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 13, 'created': '2016-12-12 15:36:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cb24e8b211bfc1e04f6664d255019a3776e97786', 'message': ""[WIP] ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nRemainining:\n- etag for delete, patch\n- same work for port,portgroup, chassis\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 14, 'created': '2016-12-12 16:36:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/49e3aad2f77674ea7d17339bcf73d56baa28d935', 'message': ""[WIP] ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nRemainining:\n- etag for delete, patch\n- same work for port,portgroup, chassis\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 15, 'created': '2016-12-13 13:10:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/87c481a61274dffafd5397e5218ae6d3cf9f2195', 'message': ""[WIP] ETAG supporting to enhance API evolution\n\nIronic's REST API does not currently return an ETAG (*) ID\nin the headers of any responses. This patch would solve\nthe ``lost update problem``.\nFollowing spec https://review.openstack.org/#/c/381991.\nRemainining:\n- etag for delete, patch\n- same work for port,portgroup, chassis\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 16, 'created': '2016-12-14 11:59:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e7f32ac7629fa5f642fb5f0b85c7989d19d2bb96', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 17, 'created': '2016-12-14 14:03:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/48bea955fd16427c4c13bc72710a2462b5c86e5e', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 18, 'created': '2016-12-15 12:29:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8c94700acd9bd1819f150a25b5203425fc44d303', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 19, 'created': '2016-12-15 13:41:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/02b3994e978526d02660b6b60e1f78d8d51a160c', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 20, 'created': '2016-12-16 11:04:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/af8609dba0ecba4d7a30c79a243295ded5d11b9c', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 21, 'created': '2016-12-16 16:19:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/11d8b3d209cb21634c74c9da6329c07ac7acce78', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 22, 'created': '2016-12-19 15:47:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8485626578e458ebd07b4d7289cd10cb47143ada', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 23, 'created': '2016-12-19 18:04:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/61e676400b937e410b91f157c002efb8de705b56', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 24, 'created': '2016-12-20 08:40:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/49f568c374e1b19a1a0335611cff77ce3971e0cb', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 25, 'created': '2016-12-23 16:14:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f42d7b02f24ebe95ea02f80d2c0725f96fcb8a60', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 26, 'created': '2017-01-05 10:08:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7a1fbf624a84fcfad6f5833577918358c3fd9404', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 27, 'created': '2017-01-06 11:12:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/05f4ddd74d0acda6478aa0e916f39df9195f38e3', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 28, 'created': '2017-01-25 15:29:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d4a1038c34e741fd7e3da86652a0995f9dfd24e5', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 29, 'created': '2017-01-26 11:55:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6702398bed7ea6c29e2c58f16974d37885a04a73', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 30, 'created': '2017-01-27 10:56:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e85f636fc73816a99619e3b2473032374ef3aac4', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 31, 'created': '2017-01-27 12:33:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ff28ea7b5450e37a4fca88049ed37675b3fbf41d', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 32, 'created': '2017-01-27 14:01:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e0e1e248fbd490fba702bd4fdc71ee0362f55337', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 33, 'created': '2017-01-31 08:52:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/512231fcad70c464086f923b6d4e1cfc0c30c5fd', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 34, 'created': '2017-01-31 10:21:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/58265bb886a6d0244c5eb69131d4404e3144ef99', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 35, 'created': '2017-01-31 11:36:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1637e19bc066414aab0910c0e12691ea339f0aba', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 36, 'created': '2017-02-01 08:59:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/71fe1fa2559537b63b814a4d1e8f4a6a14e776cc', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 37, 'created': '2017-02-01 09:01:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/35d57bb8b177c51a6a55d568179f56af23cb14dc', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 38, 'created': '2017-02-01 09:08:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/15ed7a3a4042d4a44fb166310b49b9069a06ff42', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 39, 'created': '2017-02-01 09:52:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b8ba3fc293cdd3cabe9f8d7e88864d480b73520f', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 40, 'created': '2017-02-01 15:14:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6381366a7359a9eeabb736f1a3db1a8c8289c7ed', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 41, 'created': '2017-02-01 16:08:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ace490f98e9035ee9e358e034027e5449c220ace', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\n""}, {'number': 42, 'created': '2017-02-03 10:52:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a6e48eb1a6cef842b7584c9387ffcaa26807af35', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\nDepends-On: I0f47552c424973af3704903a52f98db819b18c3f\n""}, {'number': 43, 'created': '2017-02-09 12:22:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/14d49af9b2a158de68d347f0e18c3c3cb749e74c', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\nDepends-On: I0f47552c424973af3704903a52f98db819b18c3f\n""}, {'number': 44, 'created': '2017-02-15 10:10:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/988703fdcc26c934dc5409a292b92ae01949b1b2', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\nDepends-On: I0f47552c424973af3704903a52f98db819b18c3f\n""}, {'number': 45, 'created': '2017-02-15 10:56:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fcb1fd7c1e365598d8e0054fcb44f4225b104c77', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\nDepends-On: I0f47552c424973af3704903a52f98db819b18c3f\n""}, {'number': 46, 'created': '2017-02-16 10:35:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d54e8c7154ec650571c327962c003bb410269d61', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\nDepends-On: I0f47552c424973af3704903a52f98db819b18c3f\n""}, {'number': 47, 'created': '2017-03-16 10:23:26.000000000', 'files': ['ironic/objects/portgroup.py', 'ironic/db/sqlalchemy/alembic/versions/d29d2974a28_add_etags.py', 'ironic/conductor/rpcapi.py', 'ironic/objects/node.py', 'ironic/common/exception.py', 'ironic/api/controllers/v1/chassis.py', 'ironic/conductor/manager.py', 'ironic/api/controllers/v1/versions.py', 'ironic/db/sqlalchemy/api.py', 'ironic/objects/chassis.py', 'ironic/tests/unit/db/utils.py', 'ironic/api/controllers/base.py', 'ironic/api/controllers/v1/utils.py', 'ironic/objects/port.py', 'ironic/tests/unit/api/base.py', 'ironic/db/sqlalchemy/models.py', 'ironic/api/controllers/v1/node.py', 'ironic/objects/base.py', 'ironic/tests/unit/objects/test_node.py', 'ironic/tests/unit/objects/utils.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/e8aa8fa99f5f92fdd6c173ba7456bfb78c1c4ff7', 'message': ""ETAGs maintenance for all api node's requests\n\nSpec: https://review.openstack.org/#/c/381991.\nAdded etag maintenance in ironic project for\na node's API requests.\nETAG invalidation on conductor side under lock\nwhere it is possible.\nRaising exceptions of two types:\n  - NOT_ACCEPTABLE if the ``If-Match`` header supplied\n    with appropriate API version header,\n  - PRECONDITION_FAILED if the supplied etag in the headers\n    does not pass invalidation process.\n\nETAG generation on create and update operations for object.\nFor now fields that are excluded from valuable fields to be\nbased on during etag generation are fields with\nsuffix ``updated_at``, ``created_at`` and internal fields\nlike driver_internal_info.\n\nChange-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21\nCloses-Bug: 1605728\nDepends-On: I0f47552c424973af3704903a52f98db819b18c3f\n""}]",34,392213,e8aa8fa99f5f92fdd6c173ba7456bfb78c1c4ff7,291,11,47,22724,,,0,"ETAGs maintenance for all api node's requests

Spec: https://review.openstack.org/#/c/381991.
Added etag maintenance in ironic project for
a node's API requests.
ETAG invalidation on conductor side under lock
where it is possible.
Raising exceptions of two types:
  - NOT_ACCEPTABLE if the ``If-Match`` header supplied
    with appropriate API version header,
  - PRECONDITION_FAILED if the supplied etag in the headers
    does not pass invalidation process.

ETAG generation on create and update operations for object.
For now fields that are excluded from valuable fields to be
based on during etag generation are fields with
suffix ``updated_at``, ``created_at`` and internal fields
like driver_internal_info.

Change-Id: Id11a44aaf69d4bbd04b501cd16615546b1b95f21
Closes-Bug: 1605728
Depends-On: I0f47552c424973af3704903a52f98db819b18c3f
",git fetch https://review.opendev.org/openstack/ironic refs/changes/13/392213/38 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/conductor/manager.py', 'ironic/db/sqlalchemy/alembic/versions/d29d2974a28_add_etags.py']",2,d7f9359fac0ed31a87f55ab1a57201ed2dc675c0,bug/1605728,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """"""Add etags Revision ID: d29d2974a28 Revises: f6fdb920c182 Create Date: 2016-11-01 16:53:42.606574 """""" # revision identifiers, used by Alembic. revision = 'd29d2974a28' down_revision = 'f6fdb920c182' from alembic import op import sqlalchemy as sa def upgrade(): # Note(gzholtkevych): This is a question about chassis: should etag be # necessary for chassis case, as chassis update actions is not really # used for now op.add_column('chassis', sa.Column('etag', sa.Text(), nullable=True)) op.add_column('nodes', sa.Column('etag', sa.Text(), nullable=False)) op.add_column('ports', sa.Column('etag', sa.Text(), nullable=False)) op.add_column('portgroups', sa.Column('etag', sa.Text(), nullable=False)) ",,58,0
openstack%2Fsolum-specs~master~I97315ae076b36a73c75b1b381e505c04a830758e,openstack/solum-specs,master,I97315ae076b36a73c75b1b381e505c04a830758e,Replace os.popen with subprocess module,NEW,2016-01-18 12:11:57.000000000,2017-12-18 03:19:49.000000000,,"[{'_account_id': 11689}, {'_account_id': 19930}]","[{'number': 1, 'created': '2016-01-18 12:11:57.000000000', 'files': ['doc/source/conf.py'], 'web_link': 'https://opendev.org/openstack/solum-specs/commit/1f72285a5301df5fb283a87e2f9a07d124574168', 'message': 'Replace os.popen with subprocess module\n\nos.popen() is deprecated since version 2.6. Resolved with use of subprocess module.\n\nChange-Id: I97315ae076b36a73c75b1b381e505c04a830758e\nCloses-Bug: #1529836\n'}]",1,269014,1f72285a5301df5fb283a87e2f9a07d124574168,8,2,1,11689,,,0,"Replace os.popen with subprocess module

os.popen() is deprecated since version 2.6. Resolved with use of subprocess module.

Change-Id: I97315ae076b36a73c75b1b381e505c04a830758e
Closes-Bug: #1529836
",git fetch https://review.opendev.org/openstack/solum-specs refs/changes/14/269014/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/conf.py'],1,1f72285a5301df5fb283a87e2f9a07d124574168,,"import subprocesshtml_last_updated_fmt = subprocess.Popen( git_cmd, stdout=subprocess.PIPE).communicate()[0]",html_last_updated_fmt = os.popen(git_cmd).read(),3,1
openstack%2Fblazar~master~I88f2a961d770d6deebd9af567d6407e677c102ae,openstack/blazar,master,I88f2a961d770d6deebd9af567d6407e677c102ae,Refactored blazar tempest plugin,MERGED,2017-02-02 09:57:50.000000000,2017-12-18 03:19:37.000000000,2017-12-18 03:19:37.000000000,"[{'_account_id': 3}, {'_account_id': 8878}, {'_account_id': 12393}, {'_account_id': 13192}, {'_account_id': 22348}, {'_account_id': 23840}]","[{'number': 1, 'created': '2017-02-02 09:57:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/blazar/commit/02434ab344de7c92538bd0042d81be44334974de', 'message': ""Use tempest plugin for blazar scenario tests\n\nTempest has plugin models for another project that wants to use tempest\nframework for its tests.\n\nThis commit uses tempest plugin for Blazar's scenario tests.\n\nChange-Id: I88f2a961d770d6deebd9af567d6407e677c102ae\n""}, {'number': 2, 'created': '2017-02-02 10:04:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/blazar/commit/fb81cb1cc1338155e2a608aeaa29d4ca29a07787', 'message': ""Use tempest plugin for blazar scenario tests\n\nTempest has plugin models for another project that wants to use tempest\nframework for its tests.\n\nThis commit uses tempest plugin for Blazar's scenario tests.\n\nChange-Id: I88f2a961d770d6deebd9af567d6407e677c102ae\n""}, {'number': 3, 'created': '2017-02-02 10:31:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/blazar/commit/399e67abc52735280520cf30821bb3d60e0fe11c', 'message': ""Use tempest plugin for blazar scenario tests\n\nTempest has plugin models for another project that wants to use tempest\nframework for its tests.\n\nThis commit uses tempest plugin for Blazar's scenario tests.\n\nChange-Id: I88f2a961d770d6deebd9af567d6407e677c102ae\n""}, {'number': 4, 'created': '2017-02-02 10:44:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/blazar/commit/e231fed66e06d1040dc9800ca5855076394998c8', 'message': ""Use tempest plugin for blazar scenario tests\n\nTempest has plugin models for another project that wants to use tempest\nframework for its tests.\n\nThis commit uses tempest plugin for Blazar's scenario tests.\n\nChange-Id: I88f2a961d770d6deebd9af567d6407e677c102ae\n""}, {'number': 5, 'created': '2017-12-13 10:40:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/blazar/commit/e6c5b7cd754a04a427a8462f98bea83fea62c5bf', 'message': 'Refactored blazar tempest plugin\n\n* In order to complete the tempest plugin split goal, we need to\n  refactor the blazar tempest plugin so that we can easily consume.\n\nChange-Id: I88f2a961d770d6deebd9af567d6407e677c102ae\n'}, {'number': 6, 'created': '2017-12-13 10:53:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/blazar/commit/3db4bfaf432fabca1a63ea3c0051bc7eab48387e', 'message': 'Refactored blazar tempest plugin\n\n* In order to complete the tempest plugin split goal, we need to\n  refactor the blazar tempest plugin so that we can easily consume.\n\nChange-Id: I88f2a961d770d6deebd9af567d6407e677c102ae\n'}, {'number': 7, 'created': '2017-12-13 12:58:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/blazar/commit/c7e8c796e88d1fbf93b5052134862f06be7d356c', 'message': 'Refactored blazar tempest plugin\n\n* In order to complete the tempest plugin split goal, we need to\n  refactor the blazar tempest plugin so that we can easily consume.\n\n* Added post test hook to run blazar tempest tests\n\nChange-Id: I88f2a961d770d6deebd9af567d6407e677c102ae\n'}, {'number': 8, 'created': '2017-12-13 16:10:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/blazar/commit/681f55391f75554988acc6a7ee83b3b7e9d04854', 'message': 'Refactored blazar tempest plugin\n\n* In order to complete the tempest plugin split goal, we need to\n  refactor the blazar tempest plugin so that we can easily consume.\n\n* Added post test hook to run blazar tempest tests\n\nChange-Id: I88f2a961d770d6deebd9af567d6407e677c102ae\n'}, {'number': 9, 'created': '2017-12-14 05:44:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/blazar/commit/a3e041683801b3d7f30256a97a8b4f1a0301c1e3', 'message': 'Refactored blazar tempest plugin\n\n* In order to complete the tempest plugin split goal, we need to\n  refactor the blazar tempest plugin so that we can easily consume.\n\n* Added post test hook to run blazar tempest tests\n\nChange-Id: I88f2a961d770d6deebd9af567d6407e677c102ae\n'}, {'number': 10, 'created': '2017-12-14 06:23:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/blazar/commit/f83160a7d5cf4337f789dc51f9fd7b6ee7280cd6', 'message': 'Refactored blazar tempest plugin\n\n* In order to complete the tempest plugin split goal, we need to\n  refactor the blazar tempest plugin so that we can easily consume.\n\nChange-Id: I88f2a961d770d6deebd9af567d6407e677c102ae\n'}, {'number': 11, 'created': '2017-12-14 07:31:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/blazar/commit/4c5992d7560b58fce4e4555d376812c4e1e5a0d8', 'message': 'Refactored blazar tempest plugin\n\n* In order to complete the tempest plugin split goal, we need to\n  refactor the blazar tempest plugin so that we can easily consume.\n\n* use six.moves import range instead xrange to avoid flake8 error\n\nChange-Id: I88f2a961d770d6deebd9af567d6407e677c102ae\n'}, {'number': 12, 'created': '2017-12-15 06:38:31.000000000', 'files': ['blazar_tempest_plugin/plugin.py', 'blazar_tempest_plugin/tests/scenario/test_instance_reservation.py', 'blazar_tempest_plugin/tests/scenario/test_reservation_concurrency.py', 'contrib/tempest/tempest/cli/blazarclient.py', 'contrib/tempest/README.rst', 'contrib/tempest/tempest/cli/simple_read_only/test_resource_reservation.py', 'blazar_tempest_plugin/README.rst', 'blazar_tempest_plugin/tests/__init__.py', 'blazar_tempest_plugin/services/__init__.py', 'blazar_tempest_plugin/tests/scenario/manager_freeze.py', 'blazar_tempest_plugin/__init__.py', 'blazar_tempest_plugin/tests/scenario/test_host_reservation.py', 'blazar_tempest_plugin/services/reservation/reservation_client.py', 'blazar_tempest_plugin/tests/scenario/__init__.py', 'playbooks/legacy/blazar-devstack-dsvm/run.yaml', 'setup.cfg', 'blazar_tempest_plugin/config.py', 'blazar_tempest_plugin/services/reservation/__init__.py', 'blazar_tempest_plugin/tests/scenario/resource_reservation_scenario.py'], 'web_link': 'https://opendev.org/openstack/blazar/commit/1f2e722e0471b69d7adceeb373beef6c010a98c7', 'message': 'Refactored blazar tempest plugin\n\n* In order to complete the tempest plugin split goal, we need to\n  refactor the blazar tempest plugin so that we can easily consume.\n\n* use six.moves import range instead xrange to avoid flake8 error\n\nChange-Id: I88f2a961d770d6deebd9af567d6407e677c102ae\n'}]",1,428067,1f2e722e0471b69d7adceeb373beef6c010a98c7,33,6,12,8878,,,0,"Refactored blazar tempest plugin

* In order to complete the tempest plugin split goal, we need to
  refactor the blazar tempest plugin so that we can easily consume.

* use six.moves import range instead xrange to avoid flake8 error

Change-Id: I88f2a961d770d6deebd9af567d6407e677c102ae
",git fetch https://review.opendev.org/openstack/blazar refs/changes/67/428067/2 && git format-patch -1 --stdout FETCH_HEAD,"['blazar_tempest_tests/services/__init__.py', 'blazar_tempest_tests/plugin.py', 'blazar_tempest_tests/tests/api/__init__.py', 'blazar_tempest_tests/tests/__init__.py', 'blazar_tempest_tests/tests/scenario/__init__.py', 'blazar_tempest_tests/tests/scenario/resource_reservation_scenario.py', 'contrib/tempest/tempest/cli/climateclient.py', 'blazar_tempest_tests/__init__.py', 'contrib/tempest/README.rst', 'blazar_tempest_tests/config.py', 'blazar_tempest_tests/README.rst', 'blazar_tempest_tests/tests/scenario/test_host_reservation.py', 'blazar_tempest_tests/tests/scenario/test_instance_reservation.py', 'contrib/tempest/tempest/cli/simple_read_only/test_resource_reservation.py', 'blazar_tempest_tests/services/reservation/reservation_client.py', 'blazar_tempest_tests/services/reservation/__init__.py', 'setup.cfg']",17,02434ab344de7c92538bd0042d81be44334974de,goal-split-tempest-plugin,tempest.test_plugins = blazar_tests = blazar_tempests_test.plugin.BlazarTempestPlugin ,,65,132
openstack%2Fdevstack-gate~master~Icc7aa6986943f558dfb4f88c6fea2a3bad41a472,openstack/devstack-gate,master,Icc7aa6986943f558dfb4f88c6fea2a3bad41a472,Add ability to fallback to a specific ref,NEW,2017-06-05 14:58:39.000000000,2017-12-18 03:19:07.000000000,,[{'_account_id': 1}],"[{'number': 1, 'created': '2017-06-05 14:58:39.000000000', 'files': ['functions.sh'], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/f248177c256b079ab593687af502992670bcd422', 'message': ""Add ability to fallback to a specific ref\n\nThe per-project branch override doesn't work if you need to test against\na specific tag. Provide the ability for a job to specify a ref to use\nfrom a project. This allows people to re-use jobs that are designed\naround git source to test against specific releases of some of those\nthings.\n\nChange-Id: Icc7aa6986943f558dfb4f88c6fea2a3bad41a472\n""}]",0,470997,f248177c256b079ab593687af502992670bcd422,4,1,1,2,,,0,"Add ability to fallback to a specific ref

The per-project branch override doesn't work if you need to test against
a specific tag. Provide the ability for a job to specify a ref to use
from a project. This allows people to re-use jobs that are designed
around git source to test against specific releases of some of those
things.

Change-Id: Icc7aa6986943f558dfb4f88c6fea2a3bad41a472
",git fetch https://review.opendev.org/openstack/devstack-gate refs/changes/97/470997/1 && git format-patch -1 --stdout FETCH_HEAD,['functions.sh'],1,f248177c256b079ab593687af502992670bcd422,,"# compatibility, then supply that as the argument instead. # In some rare cases, you'll want to specify a specific ref instead of # a branch, such as testing client patches against a specific old tag. # That can be accomplished by setting OVERRIDE_$PROJECT_PROJECT_REF# This function will try to check out the following (in order): # # The specific ref for the project specific OVERRIDE_$PROJECT_PROJECT_REF if specified local project_ref_var=""\$OVERRIDE_${uc_project}_PROJECT_REF"" local OVERRIDE_PROJECT_REF=`eval echo ${project_ref_var}` if git_fetch_at_ref $project $OVERRIDE_PROJECT_REF || \ git_fetch_at_ref $project $OVERRIDE_ZUUL_REF || \","# compatibility, then supply that as the argument instead. This # function will try to check out the following (in order): if git_fetch_at_ref $project $OVERRIDE_ZUUL_REF || \",11,4
openstack%2Fironic~master~Idadc92d9e01deb0d02ea2480f142dc8c7cd08304,openstack/ironic,master,Idadc92d9e01deb0d02ea2480f142dc8c7cd08304,Ability to use SwiftAPI for temporary url in glance,NEW,2016-12-14 13:49:00.000000000,2017-12-18 03:18:52.000000000,,"[{'_account_id': 10118}, {'_account_id': 10239}, {'_account_id': 11655}, {'_account_id': 12356}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 19593}, {'_account_id': 19686}, {'_account_id': 22724}]","[{'number': 1, 'created': '2016-12-14 13:49:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/35256a33401c886265234c0786ce88cb59d7e45d', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `swift_image_store`.\n\nUsing keystoneauth swift session for authentication in swift, setup\nauth options for swift session.\nDeprecated appropriate options in glance and in swift_image_store\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\n'}, {'number': 2, 'created': '2016-12-14 14:02:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d20f4306db0524939bd17e736bb899bbc4c939ed', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `swift_image_store`.\n\nUsing keystoneauth swift session for authentication in swift, setup\nauth options for swift session.\nDeprecated appropriate options in glance and in swift_image_store\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\n'}, {'number': 3, 'created': '2016-12-15 10:53:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/083ae22623f58b5393e87343ee975ba5bc925d8e', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `swift_image_store`.\n\nUsing keystoneauth swift session for authentication in swift, setup\nauth options for swift session.\nDeprecated appropriate options in glance and in swift_image_store\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 4, 'created': '2016-12-16 16:58:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f433d827d1e938ef5033589c49daff5d036a4a27', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `swift_image_store`.\n\nUsing keystoneauth swift session for authentication in swift, setup\nauth options for swift session.\nDeprecated appropriate options in glance and in swift_image_store\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\n'}, {'number': 5, 'created': '2016-12-19 08:56:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a5e4dce54052b7efc48d166c316599906b28d913', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `swift_image_store`.\n\nUsing keystoneauth swift session for authentication in swift, setup\nauth options for swift session.\nDeprecated appropriate options in glance and in swift_image_store\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 6, 'created': '2016-12-19 11:23:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/79b8d69cc1b4b6b1e56d4a5dacd1087ff5309f63', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `swift_image_store`.\n\nUsing keystoneauth swift session for authentication in swift, setup\nauth options for swift session.\nDeprecated appropriate options in glance and in swift_image_store\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 7, 'created': '2017-01-10 09:41:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/60e3e1add3a319774ffd0aa8b60ee3db2dcddf3b', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `swift_image_store`.\n\nUsing keystoneauth swift session for authentication in swift, setup\nauth options for swift session.\nDeprecated appropriate options in glance and in swift_image_store\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\n'}, {'number': 8, 'created': '2017-01-10 10:07:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e183f6b1eba64c67d321adaf3837fc45d6462ee0', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `swift_image_store`.\n\nUsing keystoneauth swift session for authentication in swift, setup\nauth options for swift session.\nDeprecated appropriate options in glance and in swift_image_store\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 9, 'created': '2017-01-11 07:58:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/86d9c34355e68ff74796a15a1175d1e2d82df9fd', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `swift_image_store`.\n\nUsing keystoneauth swift session for authentication in swift,\nsetup appropriate auth options.\nDeprecated appropriate options in glance and in swift_image_store\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 10, 'created': '2017-01-19 14:52:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e339a21198a6ddd62434933c10f7d6dafdeb905d', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `glance_backend_swift`.\n\nUsing keystoneauth swift session for authentication in swift,\nsetup appropriate auth options.\nDeprecated appropriate options in glance and in glance_backend_swift\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 11, 'created': '2017-01-19 14:54:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/50ec5f8e8bfaab61dd2466edf9a88d3223030462', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `glance_backend_swift`.\n\nUsing keystoneauth swift session for authentication in swift,\nsetup appropriate auth options.\nDeprecated appropriate options in glance and in glance_backend_swift\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 12, 'created': '2017-02-07 11:11:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d86c040129373577320b77d0cda3bd6ef86829e1', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `glance_backend_swift`.\n\nUsing keystoneauth swift session for authentication in swift,\nsetup appropriate auth options.\nDeprecated appropriate options in glance and in glance_backend_swift\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\nDepends-On: I52f1386df45ebe0a43b11fe1583e012dfa3af532\n'}, {'number': 13, 'created': '2017-02-07 11:41:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5a17bb10a3c442974d09a0f1a3e3713af1fd7d41', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `glance_backend_swift`.\n\nUsing keystoneauth swift session for authentication in swift,\nsetup appropriate auth options.\nDeprecated appropriate options in glance and in glance_backend_swift\nsections also since the all needed temporary url information from\nswift connection is going to be used instead of config options like:\n``swift_endpoint_url``,``swift_api_version``, ``swift_account`` and\n``swift_tempurl_key``.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\nDepends-On: I52f1386df45ebe0a43b11fe1583e012dfa3af532\n'}, {'number': 14, 'created': '2017-02-08 16:42:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0bc1816751c6223e0c4bd1a672d5c2475feda624', 'message': 'Keystone session for swift-temp-url generation\n\nRefactoring code of swift temp url generation. Moved swift-store\nrelated config options out of glance session.\nCreated specific section for it called  `glance_backend_swift`.\n\nUsing keystoneauth swift session for authentication in swift,\nsetup appropriate auth options.\n\nAlso there is a proposal:\n\nIt would be nice to deprecate options related to swift temporary url\ngeneration in some of future releases:\n- [glance_backend_swift]account\n- [glance_backend_swift]endpoint_url\n- [glance_backend_swift]api_version\n- [glance_backend_swift]temp_url_key\nsince all needed temporary url information can be obtained from\nswift connection.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 15, 'created': '2017-02-09 12:12:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/56ea10ff2f9ad3d4f5c3c5bbfe42c796e8e3b4ca', 'message': 'Use SwiftAPI for temporary url generation in glance\n\nGeneral swift temp url generation in image service is still using\nhardcoded configuration values in glance session:\nswift_account, swift_api_version, swift_endpoint_url,\nswift_temp_url_key.\nBut since this information can be obtained from swift connection\nobject, there is no need to use these options.\nThis proposes to deprecate them and use SwiftAPI object to generate\ntemp url, like it is done for ILO drivers and image url downloading\nwhen deploying a node.\nProviding backward compatibility, still leaving appropriate functions.\n\nThis patch also refactors code of swift temp url generation moving out\nswift-related code to swift service.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 16, 'created': '2017-02-13 12:34:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e3748bd1eac0eec98fa12d625b798988dfc9078d', 'message': 'Use SwiftAPI for temporary url generation in glance\n\nGeneral swift temp url generation in image service is still using\nhardcoded configuration values in glance session:\nswift_account, swift_api_version, swift_endpoint_url,\nswift_temp_url_key.\nBut since this information can be obtained from swift connection\nobject, there is no need to use these options.\nTaking into account previous reviews (see\nhttps://review.openstack.org/#/c/352289, comments on 52nd patch).\nFor backward compatibility it is important to keep the ability\nto generate temporary url the old way, but if appropriate options\nare not specified SwiftAPI object is used like it is done for ILO\ndrivers and image url downloading.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 17, 'created': '2017-02-13 13:41:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/be3e7d6a923b9ab21c800ce617569a200bb66980', 'message': 'Use SwiftAPI for temporary url generation in glance\n\nGeneral swift temp url generation in image service is still using\nhardcoded configuration values in glance session:\nswift_account, swift_api_version, swift_endpoint_url,\nswift_temp_url_key.\nBut since this information can be obtained from swift connection\nobject, there is no need to use these options.\nTaking into account previous reviews (see\nhttps://review.openstack.org/#/c/352289, comments on 52nd patch).\nFor backward compatibility it is important to keep the ability\nto generate temporary url the old way, but if appropriate options\nare not specified SwiftAPI object is used like it is done for ILO\ndrivers and image url downloading.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 18, 'created': '2017-02-14 15:00:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5a67f5dbcd8ef3fb74b353543b14c4ea2ea2dcb4', 'message': 'Ability to use SwiftAPI for temporary url in glance\n\nGeneral swift temp url generation in image service is still using\nhardcoded configuration values in glance session:\nswift_account, swift_api_version, swift_endpoint_url,\nswift_temp_url_key.\nBut since this information can be obtained from swift connection\nobject, there is no need to use these options.\nTaking into account previous reviews (see\nhttps://review.openstack.org/#/c/352289, comments on 52nd patch).\nFor backward compatibility it is important to keep the ability\nto generate temporary url the old way, but if appropriate options\nare not specified, SwiftAPI object is used like it is done for ILO\ndrivers and image url downloading.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 19, 'created': '2017-02-24 12:26:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/38bc7213a40ee1ae80eeb6804dbbdd5150cb3709', 'message': 'Ability to use SwiftAPI for temporary url in glance\n\nGeneral swift temp url generation in image service is still using\nhardcoded configuration values in glance session:\nswift_account, swift_api_version, swift_endpoint_url,\nswift_temp_url_key.\nBut since this information can be obtained from swift connection\nobject, there is no need to use these options.\nTaking into account previous reviews (see\nhttps://review.openstack.org/#/c/352289, comments on 52nd patch).\nFor backward compatibility it is important to keep the ability\nto generate temporary url the old way, but if appropriate options\nare not specified, SwiftAPI object is used like it is done for ILO\ndrivers and image url downloading.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 20, 'created': '2017-04-18 10:51:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fa70f821efc161c52812e87a611e51e7237eb83e', 'message': 'Ability to use SwiftAPI for temporary url in glance\n\nGeneral swift temp url generation in image service is still using\nhardcoded configuration values in glance session:\nswift_account, swift_api_version, swift_endpoint_url,\nswift_temp_url_key.\nBut since this information can be obtained from swift connection\nobject, there is no need to use these options.\nTaking into account previous reviews (see\nhttps://review.openstack.org/#/c/352289, comments on 52nd patch).\nFor backward compatibility it is important to keep the ability\nto generate temporary url the old way, but if appropriate options\nare not specified, SwiftAPI object is used like it is done for ILO\ndrivers and image url downloading.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}, {'number': 21, 'created': '2017-04-18 10:57:40.000000000', 'files': ['releasenotes/notes/session-usage-for-swift-temp-url-ec024181f64ce775.yaml', 'ironic/common/glance_service/v2/image_service.py', 'ironic/tests/unit/common/test_glance_service.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/c4abe809276596ca3194d1dccc64f61397703ccb', 'message': 'Ability to use SwiftAPI for temporary url in glance\n\nGeneral swift temp url generation in image service is still using\nhardcoded configuration values in glance session:\nswift_account, swift_api_version, swift_endpoint_url,\nswift_temp_url_key.\nBut since this information can be obtained from swift connection\nobject, there is no need to use these options.\nTaking into account previous reviews (see\nhttps://review.openstack.org/#/c/352289, comments on 52nd patch).\nFor backward compatibility it is important to keep the ability\nto generate temporary url the old way, but if appropriate options\nare not specified, SwiftAPI object is used like it is done for ILO\ndrivers and image url downloading.\n\nChange-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304\nCloses-Bug: 1521197\nDepends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d\n'}]",68,410764,c4abe809276596ca3194d1dccc64f61397703ccb,131,11,21,22724,,,0,"Ability to use SwiftAPI for temporary url in glance

General swift temp url generation in image service is still using
hardcoded configuration values in glance session:
swift_account, swift_api_version, swift_endpoint_url,
swift_temp_url_key.
But since this information can be obtained from swift connection
object, there is no need to use these options.
Taking into account previous reviews (see
https://review.openstack.org/#/c/352289, comments on 52nd patch).
For backward compatibility it is important to keep the ability
to generate temporary url the old way, but if appropriate options
are not specified, SwiftAPI object is used like it is done for ILO
drivers and image url downloading.

Change-Id: Idadc92d9e01deb0d02ea2480f142dc8c7cd08304
Closes-Bug: 1521197
Depends-On: Ia7c8925c78cf27d3b24308541c4757bbddc4c48d
",git fetch https://review.opendev.org/openstack/ironic refs/changes/64/410764/1 && git format-patch -1 --stdout FETCH_HEAD,"['devstack/lib/ironic', 'ironic/conf/swift_image_store.py', 'etc/ironic/ironic.conf.sample', 'ironic/common/swift.py', 'ironic/conf/opts.py', 'ironic/conf/__init__.py', 'ironic/common/glance_service/v2/image_service.py', 'ironic/tests/unit/common/test_glance_service.py', 'ironic/conf/glance.py']",9,35256a33401c886265234c0786ce88cb59d7e45d,bug/1521197," deprecated_for_removal=True, deprecated_reason='This option was moved to swift_image_store ' 'section which is more appropriate for it.' 'Please use temp_url_key in new section ' 'instead', deprecated_for_removal=True, deprecated_reason='This option was moved to swift_image_store ' 'section which is more appropriate for it.' 'Please use temp_url_duration in new section ' 'instead', deprecated_for_removal=True, deprecated_reason='This option was moved to swift_image_store ' 'section which is more appropriate for it.' 'Please use temp_url_cache_enabled in new ' 'section instead', cfg.IntOpt( 'swift_temp_url_expected_download_start_delay', default=0, min=0, deprecated_for_removal=True, deprecated_reason='This option was moved to swift_image_store ' 'section which is more appropriate for it.' 'Please use temp_url_expected_download_start_delay ' 'in new section instead', help=_('This is the delay (in seconds) from the time of the ' 'deploy request (when the Swift temporary URL is ' 'generated) to when the IPA ramdisk starts up and URL ' 'is used for the image download. This value is used to ' 'check if the Swift temporary URL duration is large ' 'enough to let the image download begin. Also if ' 'temporary URL caching is enabled this will determine ' 'if a cached entry will still be valid when the ' 'download starts. swift_temp_url_duration value must be ' 'greater than or equal to this option\'s value. ' 'Defaults to 0.')), deprecated_for_removal=True, deprecated_reason='This option was moved to swift_image_store ' 'section which is more appropriate for it.' 'Please use endpoint_url in new section instead', deprecated_for_removal=True, deprecated_reason=_('This option was moved to ' 'swift_image_store section which is more ' 'appropriate for it. Please use api_version ' 'in new section instead'), deprecated_for_removal=True, deprecated_reason=_('This option was moved to ' 'swift_image_store section which is more ' 'appropriate for it' 'Please use api_version in new section instead'), deprecated_for_removal=True, deprecated_reason=_('This configuration option is moved to ' 'swift_image_store section which is more' 'appropriate for it. Please, use ' 'container option in new section instead'), cfg.IntOpt( 'swift_store_multiple_containers_seed', default=0, deprecated_for_removal=True, deprecated_reason=_('This configuration option is moved to ' 'swift_image_store section which is more ' 'appropriate for it. Please, use ' 'option multiple_containers_seed in section ' 'swift_image_store instead'), help=_('This should match a config by the same name in the ' 'Glance configuration file. When set to 0, a ' 'single-tenant store will only use one ' 'container to store all images. When set to an integer ' 'value between 1 and 32, a single-tenant store will use ' 'multiple containers to store images, and this value ' 'will determine how many containers are created.')), deprecated_for_removal=True, deprecated_reason=_('This configuration option is moved to ' 'swift_image_store section which is more ' 'appropriate for it. Please, use option' 'temp_url_endpoint_type in section ' 'swift_image_store instead'),"," cfg.IntOpt('swift_temp_url_expected_download_start_delay', default=0, min=0, help=_('This is the delay (in seconds) from the time of the ' 'deploy request (when the Swift temporary URL is ' 'generated) to when the IPA ramdisk starts up and URL ' 'is used for the image download. This value is used to ' 'check if the Swift temporary URL duration is large ' 'enough to let the image download begin. Also if ' 'temporary URL caching is enabled this will determine ' 'if a cached entry will still be valid when the ' 'download starts. swift_temp_url_duration value must be ' 'greater than or equal to this option\'s value. ' 'Defaults to 0.')), cfg.IntOpt('swift_store_multiple_containers_seed', default=0, help=_('This should match a config by the same name in the ' 'Glance configuration file. When set to 0, a ' 'single-tenant store will only use one ' 'container to store all images. When set to an integer ' 'value between 1 and 32, a single-tenant store will use ' 'multiple containers to store images, and this value ' 'will determine how many containers are created.')),",1006,337
openstack%2Fheat-templates~master~If0004a1d181107630ea481439c0515f44c244e3e,openstack/heat-templates,master,If0004a1d181107630ea481439c0515f44c244e3e,Update hot/autoscaling.yaml to use query property,NEW,2014-10-12 17:50:12.000000000,2017-12-18 03:18:47.000000000,,"[{'_account_id': 7193}, {'_account_id': 8328}]","[{'number': 1, 'created': '2014-10-12 17:50:12.000000000', 'files': ['hot/autoscaling.yaml'], 'web_link': 'https://opendev.org/openstack/heat-templates/commit/acf603d04da86d521769f2f1ec8a867141a357d4', 'message': 'Update hot/autoscaling.yaml to use query property\n\nFollowing https://review.openstack.org/#/c/127821/ .\nReplaced usage of matching_metadata with usage of\nthe query property on an OS::Ceilometer::Alarm.\n\nChange-Id: If0004a1d181107630ea481439c0515f44c244e3e\n'}]",4,127848,acf603d04da86d521769f2f1ec8a867141a357d4,6,2,1,8328,,,0,"Update hot/autoscaling.yaml to use query property

Following https://review.openstack.org/#/c/127821/ .
Replaced usage of matching_metadata with usage of
the query property on an OS::Ceilometer::Alarm.

Change-Id: If0004a1d181107630ea481439c0515f44c244e3e
",git fetch https://review.opendev.org/openstack/heat-templates refs/changes/48/127848/1 && git format-patch -1 --stdout FETCH_HEAD,['hot/autoscaling.yaml'],1,acf603d04da86d521769f2f1ec8a867141a357d4,add/alarmquery," comparison_operator: gt query: - field: metadata.user_metadata.stack op: eq value: {get_param: ""OS::stack_id""} comparison_operator: lt query: - field: metadata.user_metadata.stack op: eq value: {get_param: ""OS::stack_id""} description: template: description: ceilometer_alarm_show_high: value: str_replace: template: ceilometer alarm-show alarmid params: alarmid: {get_resource: cpu_alarm_high} description: This is a Ceilometer command that shows the state of the alarm for the high CPU utilization condition. Use this to investigate why your scaling group is or is not scaling up. ceilometer_alarm_show_low: value: str_replace: template: ceilometer alarm-show alarmid params: alarmid: {get_resource: cpu_alarm_low} description: This is a Ceilometer command that shows the state of the alarm for the low CPU utilization condition. Use this to investigate why your scaling group is or is not scaling down."," matching_metadata: {'metadata.user_metadata.stack': {get_param: ""OS::stack_id""}} comparison_operator: gt matching_metadata: {'metadata.user_metadata.stack': {get_param: ""OS::stack_id""}} comparison_operator: lt description: > template: > description: >",33,7
openstack%2Fironic~master~I838d7f14ea5b6e0ed7fdbd9e788564bf8e66d1d9,openstack/ironic,master,I838d7f14ea5b6e0ed7fdbd9e788564bf8e66d1d9,"Using assertFalse(A) instead of assertEqual(False, A)",ABANDONED,2017-06-15 05:58:38.000000000,2017-12-18 03:18:16.000000000,,"[{'_account_id': 7711}, {'_account_id': 10239}, {'_account_id': 11076}, {'_account_id': 17998}, {'_account_id': 19339}]","[{'number': 1, 'created': '2017-06-15 05:58:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7a7fc103462337ced543e30fa2fed00b81f3f3cc', 'message': 'Using assertFalse(A) instead of assertEqual(False, A)\n\nThis patch is to replace assertEqual(False, A) with assertFalse(A), which\nthe latter is more straightforward and easier to understand.\n\nChange-Id: I838d7f14ea5b6e0ed7fdbd9e788564bf8e66d1d9\n'}, {'number': 2, 'created': '2017-07-14 07:10:22.000000000', 'files': ['ironic/tests/unit/drivers/modules/drac/test_periodic_task.py', 'ironic/tests/unit/drivers/modules/test_deploy_utils.py', 'ironic/tests/unit/api/v1/test_portgroups.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/86a59e2219978bb62e30a2045419c74eca17b22c', 'message': 'Using assertFalse(A) instead of assertEqual(False, A)\n\nThis patch is to replace assertEqual(False, A) with assertFalse(A), which\nthe latter is more straightforward and easier to understand.\n\nChange-Id: I838d7f14ea5b6e0ed7fdbd9e788564bf8e66d1d9\n'}]",1,474453,86a59e2219978bb62e30a2045419c74eca17b22c,14,5,2,25571,,,0,"Using assertFalse(A) instead of assertEqual(False, A)

This patch is to replace assertEqual(False, A) with assertFalse(A), which
the latter is more straightforward and easier to understand.

Change-Id: I838d7f14ea5b6e0ed7fdbd9e788564bf8e66d1d9
",git fetch https://review.opendev.org/openstack/ironic refs/changes/53/474453/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/tests/unit/api/v1/test_notification_utils.py', 'ironic/tests/unit/drivers/modules/drac/test_periodic_task.py', 'ironic/tests/unit/api/v1/test_portgroups.py', 'ironic/tests/unit/drivers/modules/test_deploy_utils.py']",4,7a7fc103462337ced543e30fa2fed00b81f3f3cc,, self.assertFalse(task.node.driver_internal_info[ self.assertTrue(task.node.driver_internal_info[," self.assertEqual(False, task.node.driver_internal_info[ self.assertEqual(True, task.node.driver_internal_info[",5,5
openstack%2Fceilometer~master~Ia93e02a4b6667e7e9d689061df5d8331e884d794,openstack/ceilometer,master,Ia93e02a4b6667e7e9d689061df5d8331e884d794,Replace the usage of 'manager' with 'os_primary',ABANDONED,2017-06-29 05:00:12.000000000,2017-12-18 03:17:39.000000000,,"[{'_account_id': 6537}, {'_account_id': 11224}, {'_account_id': 15843}, {'_account_id': 25903}, {'_account_id': 26159}]","[{'number': 1, 'created': '2017-06-29 05:00:12.000000000', 'files': ['ceilometer/tests/tempest/scenario/manager.py'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/8e231882c0bdda49460d885808e31178aadf56f6', 'message': ""Replace the usage of 'manager' with 'os_primary'\n\nIn tempest, alias 'manager' has been moved to 'os_primary'\nin version Pike, and it will be removed in version Queens.\nThis patch is to replace the usage of 'manager' with 'os_primary'.\n\nFor other details, please check [1] and [2]\n[1] https://review.openstack.org/#/c/468036/\n[2] https://review.openstack.org/#/c/463484/\n\nChange-Id: Ia93e02a4b6667e7e9d689061df5d8331e884d794\n""}]",0,478732,8e231882c0bdda49460d885808e31178aadf56f6,9,5,1,25571,,,0,"Replace the usage of 'manager' with 'os_primary'

In tempest, alias 'manager' has been moved to 'os_primary'
in version Pike, and it will be removed in version Queens.
This patch is to replace the usage of 'manager' with 'os_primary'.

For other details, please check [1] and [2]
[1] https://review.openstack.org/#/c/468036/
[2] https://review.openstack.org/#/c/463484/

Change-Id: Ia93e02a4b6667e7e9d689061df5d8331e884d794
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/32/478732/1 && git format-patch -1 --stdout FETCH_HEAD,['ceilometer/tests/tempest/scenario/manager.py'],1,8e231882c0bdda49460d885808e31178aadf56f6,, cidr_in_use = self.os_admin.subnets_client.list_subnets( ports = self.os_admin.ports_client.list_ports( net = self.os_admin.networks_client.list_networks(, cidr_in_use = self.admin_manager.subnets_client.list_subnets( ports = self.admin_manager.ports_client.list_ports( net = self.admin_manager.networks_client.list_networks(,3,3
openstack%2Fdiskimage-builder~master~I23e293b957cd4f008611656cf9166391b1b537a2,openstack/diskimage-builder,master,I23e293b957cd4f008611656cf9166391b1b537a2,Check source-repository-* files for trailing newline,MERGED,2017-12-08 03:25:17.000000000,2017-12-18 03:17:04.000000000,2017-12-18 02:16:34.000000000,"[{'_account_id': 7118}, {'_account_id': 10118}, {'_account_id': 21741}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2017-12-08 03:25:17.000000000', 'files': ['bin/dib-lint'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/55537519e51de064e78795d260e1fed1af1a352a', 'message': ""Check source-repository-* files for trailing newline\n\nBecause we read this in via a bash loop, without a trailing newline we\ncan hit one of the oldest bash gotcha tricks and end up skipping the\nfinal line (or only line, if there's only one) when the description\nfiles don't have trailing newlines.  Add a check.\n\nChange-Id: I23e293b957cd4f008611656cf9166391b1b537a2\n""}]",0,526583,55537519e51de064e78795d260e1fed1af1a352a,23,5,1,7118,,,0,"Check source-repository-* files for trailing newline

Because we read this in via a bash loop, without a trailing newline we
can hit one of the oldest bash gotcha tricks and end up skipping the
final line (or only line, if there's only one) when the description
files don't have trailing newlines.  Add a check.

Change-Id: I23e293b957cd4f008611656cf9166391b1b537a2
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/83/526583/1 && git format-patch -1 --stdout FETCH_HEAD,['bin/dib-lint'],1,55537519e51de064e78795d260e1fed1af1a352a,newline-check," # source-repository does a read < $file and can miss the last line # (or only line, if there's only one) when not newline terminated. if [[ $(basename ""${i}"") =~ ""source-repository-"" ]]; then nl=$(tail -c 1 ${i}) if [[ ""${nl}"" != """" ]]; then error ""$i does not end with a newline"" fi fi ",,9,0
openstack%2Fswift~master~I5145c3fcad1535c6bae5eb258ebb09828cbc3b84,openstack/swift,master,I5145c3fcad1535c6bae5eb258ebb09828cbc3b84,WIP: Changing Policies,NEW,2015-08-05 03:15:24.000000000,2017-12-18 03:17:01.000000000,,"[{'_account_id': 860}, {'_account_id': 1179}, {'_account_id': 8859}, {'_account_id': 9625}, {'_account_id': 13052}, {'_account_id': 14766}, {'_account_id': 23308}]","[{'number': 1, 'created': '2015-08-05 03:15:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/723063c4b48787cd3a4fac5ae001f00c9be2e473', 'message': ""WIP: Changing Policies\n\nThis is an early version of changing policies functions.\nThis patch ignores failure cases, includes dirty codes and lacks\nsome test cases. However, I would like to show my update to Swift\ndev team in advance of Swift Hackathon held next week.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* Enhance container-server's GET API and SQL select statements\n  to get a list of objects to be moved\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 2, 'created': '2015-10-21 11:53:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c948423678bccb76abfa033ff53fc6b763288597', 'message': 'WIP: Changing Policies\n\nTHIS PATCH IS NOT READY FOR REVIEW\n--------------------------------------------------------------\nI do not test this patch and check pep8, but let me push this\npatch to notify ""this patch is active"" and to store working\ndata for backup purpose (actually, yesterday my pc went wrong\nand all my writing patches went away...)\nI will repush a patch soon after testing.\n--------------------------------------------------------------\n\nThis patch reflects the discussion results at Austin Hackathon.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta \'X-Container-Sysmeta-Prev-Index\' and\n  \'X-Container-Sysmeta-Objects-Queued\'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler\'s process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy\'s stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n'}, {'number': 3, 'created': '2015-10-23 09:12:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/08c3889ff1a747fa1186934538ce0fff81fa0d37', 'message': ""WIP: Changing Policies\n\nThe discussion results at Austin Hackathon is reflected to\nthis patch.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta 'X-Container-Sysmeta-Prev-Index' and\n  'X-Container-Sysmeta-Objects-Queued'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 4, 'created': '2015-10-26 06:56:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/33fa2bb6bc9ee6ac70ffb564f6a3b463fb284680', 'message': ""WIP: Changing Policies\n\nThe discussion results at Austin Hackathon is reflected to\nthis patch. Currently, tese cases do not cover all modifications.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta 'X-Container-Sysmeta-Prev-Index' and\n  'X-Container-Sysmeta-Objects-Queued'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 5, 'created': '2015-11-26 05:46:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c39fd8baaebf3791c2e7b3e49a8770e906a4fdc9', 'message': ""WIP: Changing Policies\n\n------------------------------------------------------------------\nTHIS PATCH IS STILL A WORK IN PROGRESS AND IS NOT READY FOR REVIEW\n------------------------------------------------------------------\n\nThe discussion results at Austin Hackathon and Tokyo Summit is\nreflected to this patch. Currently, tese cases do not cover all\nmodifications.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta 'X-Container-Sysmeta-Prev-Index' and\n  'X-Container-Sysmeta-Objects-Queued'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 6, 'created': '2016-02-22 22:48:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/9fd7e7af6f75dfa12bb6b6aa7bc323678b7a04b1', 'message': ""WIP: Changing Policies\n\nThe discussion results at Austin Hackathon and Tokyo Summit is\nreflected to this patch. Currently, tese cases do not cover all\nmodifications.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta 'X-Container-Sysmeta-Prev-Index' and\n  'X-Container-Sysmeta-Objects-Queued'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 7, 'created': '2016-02-23 22:19:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a858b422018feb9f1be62f43ad591860899e1824', 'message': ""WIP: Changing Policies\n\nThe discussion results at Austin Hackathon and Tokyo Summit is\nreflected to this patch. Currently, tese cases do not cover all\nmodifications.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta 'X-Container-Sysmeta-Prev-Index' and\n  'X-Container-Sysmeta-Objects-Queued'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 8, 'created': '2016-03-03 16:37:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/68bdfdb60e7383a3d213fcd630b445c877bda561', 'message': ""WIP: Changing Policies\n\nThe discussion results at Austin Hackathon and Tokyo Summit is\nreflected to this patch. Currently, tese cases do not cover all\nmodifications.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta 'X-Container-Sysmeta-Prev-Index' and\n  'X-Container-Sysmeta-Objects-Queued'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 9, 'created': '2016-04-13 23:51:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/241890cc45fa00911c2849e43fc31ae8376950f5', 'message': ""WIP: Changing Policies\n\nThe discussion results at Austin Hackathon and Tokyo Summit is\nreflected to this patch. Currently, tese cases do not cover all\nmodifications.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta 'X-Container-Sysmeta-Prev-Index' and\n  'X-Container-Sysmeta-Objects-Queued'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 10, 'created': '2016-04-21 02:28:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a2722680b6e930183558f1b33f1f68cb445ed8ff', 'message': ""WIP: Changing Policies\n\nThe discussion results at Austin Hackathon and Tokyo Summit is\nreflected to this patch. Currently, tese cases do not cover all\nmodifications.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta 'X-Container-Sysmeta-Prev-Index' and\n  'X-Container-Sysmeta-Objects-Queued'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 11, 'created': '2016-05-26 00:53:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/660e5367dd2037915dd38da47932fe18482cecf1', 'message': ""WIP: Changing Policies\n\nThe discussion results at Austin Hackathon and Tokyo Summit is\nreflected to this patch. Currently, tese cases do not cover all\nmodifications.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta 'X-Container-Sysmeta-Prev-Index' and\n  'X-Container-Sysmeta-Objects-Queued'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 12, 'created': '2016-06-15 18:50:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2ce57c0a2ad4a88479f9c609937b86c4acfd04c7', 'message': ""WIP: Changing Policies\n\nThe discussion results at Austin Hackathon and Tokyo Summit is\nreflected to this patch. Currently, tese cases do not cover all\nmodifications.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta 'X-Container-Sysmeta-Prev-Index' and\n  'X-Container-Sysmeta-Objects-Queued'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 13, 'created': '2016-07-12 21:42:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a67db3efcc8c3c29ed6599e3a3d541dd6ce3a32c', 'message': ""WIP: Changing Policies\n\nThe discussion results at Austin Hackathon and Tokyo Summit is\nreflected to this patch. Currently, tese cases do not cover all\nmodifications.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta 'X-Container-Sysmeta-Prev-Index' and\n  'X-Container-Sysmeta-Objects-Queued'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 14, 'created': '2016-07-28 00:59:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4cb00a0403b98b4fd4f0dd808fc30505c7f60ff2', 'message': ""WIP: Changing Policies\n\nThe discussion results at Austin Hackathon and Tokyo Summit is\nreflected to this patch. Currently, tese cases do not cover all\nmodifications.\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  two sysmeta 'X-Container-Sysmeta-Prev-Index' and\n  'X-Container-Sysmeta-Objects-Queued'. The former stores the\n  index of source policy and the latter indicates if objects\n  in the container are queued for policy changing.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 15, 'created': '2016-08-18 23:09:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2af6da9a26a5597e8c47292e9a5a2bb41e236e86', 'message': ""WIP: Changing Policies\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  sysmeta 'X-Container-Sysmeta-Prev-Index'. This stores the\n  previous index of storage policy.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 16, 'created': '2017-01-14 03:26:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/39e7ef01167639db78c16e0e5b3f61dc60823d0c', 'message': ""WIP: Changing Policies\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  sysmeta 'X-Container-Sysmeta-Prev-Index'. This stores the\n  previous index of storage policy.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 17, 'created': '2017-02-22 15:16:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/550d9e80d7deaac05ec91acbe187ccfdff3f03c8', 'message': ""WIP: Changing Policies\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  sysmeta 'X-Container-Sysmeta-Prev-Index'. This stores the\n  previous index of storage policy.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}, {'number': 18, 'created': '2017-03-15 23:43:56.000000000', 'files': ['swift/common/middleware/change_policy.py', 'test/unit/container/test_server.py', 'test/unit/container/test_backend.py', 'etc/object-transferrer.conf-sample', 'etc/object-server.conf-sample', 'swift/obj/transferrer.py', 'swift/container/server.py', 'test/probe/test_db_replicator.py', 'swift/common/manager.py', 'test/unit/container/test_reconciler.py', 'swift/container/reconciler.py', 'swift/common/internal_client.py', 'test/unit/common/middleware/test_change_policy.py', 'test/unit/container/test_replicator.py', 'test/unit/common/test_db_replicator.py', 'swift/container/replicator.py', 'doc/source/overview_policies.rst', 'bin/swift-object-transferrer', 'swift/common/utils.py', 'test/unit/common/test_internal_client.py', 'test/unit/common/middleware/test_versioned_writes.py', 'test/unit/obj/test_transferrer.py', 'etc/proxy-server.conf-sample', 'swift/common/db_replicator.py', 'setup.cfg', 'swift/container/backend.py', 'swift/proxy/controllers/container.py', 'test/unit/common/middleware/helpers.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/4437aa061b9ced732fcab48485c045f487409c8a', 'message': ""WIP: Changing Policies\n\nIn this patch, following parts are added or modified\n* Add API to force changing a policy of a container\n* Add object-transferrer daemon to send objects according to\n  the ring of the changed policy\n* A container to be planned to change its policy is tagged by\n  sysmeta 'X-Container-Sysmeta-Prev-Index'. This stores the\n  previous index of storage policy.\n* Outline of changing policy process for some container is as\n  follows (it is similar to reconciler's process)\n  1. User orders to change the storage policy of container by\n     Container POST method\n  2. Container-replicator checks if a container is orderred to\n     change the policy, and if so, replicator create a special\n     container and enqueue objects info to this container so that\n     object-transferrer can really migrate the policy of objects\n  3. Container-update notifies to a system account for policy\n     changing that a special container is created\n  4. Object-transferrer finds objects to be transferred\n* Add headers which show each policy's stats for Container\n  GET/HEAD to find the progress of policy changing\n\nAbove procedures can minimize the burden of container DB and\nkeep user-side API atomic\n\nImplements: blueprint changing-policies\nChange-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84\n""}]",33,209329,4437aa061b9ced732fcab48485c045f487409c8a,72,7,18,8859,,,0,"WIP: Changing Policies

In this patch, following parts are added or modified
* Add API to force changing a policy of a container
* Add object-transferrer daemon to send objects according to
  the ring of the changed policy
* A container to be planned to change its policy is tagged by
  sysmeta 'X-Container-Sysmeta-Prev-Index'. This stores the
  previous index of storage policy.
* Outline of changing policy process for some container is as
  follows (it is similar to reconciler's process)
  1. User orders to change the storage policy of container by
     Container POST method
  2. Container-replicator checks if a container is orderred to
     change the policy, and if so, replicator create a special
     container and enqueue objects info to this container so that
     object-transferrer can really migrate the policy of objects
  3. Container-update notifies to a system account for policy
     changing that a special container is created
  4. Object-transferrer finds objects to be transferred
* Add headers which show each policy's stats for Container
  GET/HEAD to find the progress of policy changing

Above procedures can minimize the burden of container DB and
keep user-side API atomic

Implements: blueprint changing-policies
Change-Id: I5145c3fcad1535c6bae5eb258ebb09828cbc3b84
",git fetch https://review.opendev.org/openstack/swift refs/changes/29/209329/18 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/common/test_direct_client.py', 'test/unit/container/test_server.py', 'test/unit/container/test_backend.py', 'swift/proxy/controllers/obj.py', 'etc/object-transferrer.conf-sample', 'etc/object-server.conf-sample', 'swift/obj/transferrer.py', 'bin/swift-object-transferrer', 'swift/container/server.py', 'swift/common/direct_client.py', 'swift/common/manager.py', 'setup.cfg', 'swift/container/backend.py', 'swift/proxy/controllers/container.py']",14,723063c4b48787cd3a4fac5ae001f00c9be2e473,bp/changing-policies,"from swift.common.utils import public, csv_append, Timestamp, hash_path HTTPNotFound, Request from swift.obj.transferrer import POLICY_CHANGE_ACCOUNT def _convert_policy_to_index(self, req, keyname='X-Storage-Policy'): :param keyname: header key name of storage policy policy_name = req.headers.get(keyname) % (keyname, policy_name))) policy_index = self._convert_policy_to_index( req, 'X-Forced-Change-Storage-Policy') if policy_index is not None: headers['X-Forced-Change-Storage-Policy-Index'] = policy_index if (policy_index is not None and resp.status.startswith('202') and 'x-container-sysmeta-prev-index' in resp.headers): # If changing policy request is accepted, a proxy will # register a special container for object-transferrer container_path = '/%s/%s' % (self.account_name, self.container_name) container_path_hash = hash_path(self.account_name, self.container_name) special_path = '/v1/%s/%s' % (POLICY_CHANGE_ACCOUNT, container_path_hash) special_headers = {'X-Container-Sysmeta-URI': container_path, 'X-Timestamp': resp.headers['x-timestamp'], 'X-Container-Sysmeta-Prev-Storage-Policy-Index': resp.headers['x-container-sysmeta-prev-index'] } special_environ = {'REQUEST_METHOD': 'PUT'} preq = Request.blank(special_path, headers=special_headers, environ=special_environ) p_acc_name = self.account_name p_cont_name = self.container_name self.account_name = POLICY_CHANGE_ACCOUNT self.container_name = container_path_hash # TODO: How to recover to put a special container in the case of # failure. Can other container related daemons re-put a special # container self.PUT(preq) self.account_name = p_acc_name self.container_name = p_cont_name","from swift.common.utils import public, csv_append, Timestamp HTTPNotFound def _convert_policy_to_index(self, req): policy_name = req.headers.get('X-Storage-Policy') % ('X-Storage-Policy', policy_name)))",711,19
openstack%2Fswift~master~I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8,openstack/swift,master,I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8,Add process level concurrency to container sync,NEW,2015-08-06 19:37:56.000000000,2017-12-18 03:16:58.000000000,,"[{'_account_id': 860}, {'_account_id': 1179}, {'_account_id': 3094}, {'_account_id': 5600}, {'_account_id': 7233}, {'_account_id': 7847}, {'_account_id': 9816}, {'_account_id': 11317}, {'_account_id': 12193}, {'_account_id': 12279}, {'_account_id': 13052}, {'_account_id': 15932}]","[{'number': 1, 'created': '2015-08-06 19:37:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f9f2190424dbe4c2e34ae14eb970c1d8cfe6fe93', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'concurrency' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'concurrency' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe saio config files were edited to facilitate the probe tests,\nand the default for concurrency is 1 to best maintain current\nrun time behavior.\\\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 2, 'created': '2015-08-08 15:08:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/bfa410477b37356a8487d07c566a06f56dee51ac', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'concurrency' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'concurrency' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe saio config files were edited to facilitate the probe tests,\nand the default for concurrency is 1 to best maintain current\nrun time behavior.\\\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 3, 'created': '2015-08-11 12:36:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ee486b4ac60fe8e5f54373031856f8f71330d2be', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'concurrency' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'concurrency' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe saio config files were edited to facilitate the probe tests,\nand the default for concurrency is 1 to best maintain current\nrun time behavior.\\\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 4, 'created': '2015-09-12 21:33:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/57772e3214a908faba7d01029bbb9ab23b48767b', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 5, 'created': '2015-09-18 21:33:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/63be430a71e40d119a8f7864b45d11e5454e3b9e', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 6, 'created': '2015-09-19 18:44:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/185c7c03544c35d7ece668a69ed4aad40ff0b228', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 7, 'created': '2015-09-19 19:56:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8d50317e2e5be966a861b086882d2393a19aac05', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 8, 'created': '2015-10-20 07:46:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/b604f738c26257cde4f3e9542a35b2d3b8cabbb9', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 9, 'created': '2015-10-20 19:46:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/005ed3173f4b3b4704e1dce6f842df75a2b01702', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 10, 'created': '2015-10-21 09:32:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ec6de0d6066618833842ad26d960973f3b5023d2', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 11, 'created': '2015-10-30 04:48:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ce397a8501b92bc6393428df66b1ac78f7b346c2', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 12, 'created': '2015-11-22 08:59:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/234775c860853ce69cc36242eded5337d3cd652c', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 13, 'created': '2016-03-17 15:08:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a735accf2422b325bc7d210278fa63810332b5bf', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 14, 'created': '2016-03-23 13:55:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/77d730237f6a1efa550a4dcb510763bff898e92f', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 15, 'created': '2016-03-24 11:19:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/09fa1de127aee7866bf2158b439ce71d99a9e9a8', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 16, 'created': '2016-03-24 18:25:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/d03f5d0795a3bcb4bf0783007909e78254753730', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 17, 'created': '2016-04-18 17:54:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/5168e1097629cea9b134458bbd1d8c57c109aa4e', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 18, 'created': '2016-04-26 11:43:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/43f102bf177766bea74009cef02cd2cd44b560b7', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nPerformance measurement results:\nhttps://ibm.app.box.com/files/0/f/7639223798/Container-sync_performance\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 19, 'created': '2016-05-01 14:27:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c244b2eb1db0510c5223c0af2935c37ccf782359', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nPerformance measurement results:\nhttps://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 20, 'created': '2016-05-02 07:45:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/b6c06ce002e1de3f43a4d5b4a4528fb461eb5b93', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 21, 'created': '2016-05-02 10:21:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/26e175e4177f18abb397bba3fb02a5c4d7202c07', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 22, 'created': '2016-07-27 11:46:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/74dfb8e72c38e8d6b4710d2824976fd5a2914f8b', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 23, 'created': '2016-08-01 13:29:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/538d3457fa7670ad2df5a18e9a2e911cf10a8193', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 24, 'created': '2016-08-01 13:41:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/b06a315c3ab3b2cdf96e4feacdcfaef21feabea5', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 25, 'created': '2016-08-08 15:31:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6c5a5b249dba1335c3e7813db890e6af95219dc8', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nCo-Authored-By: Oshrit Feder <oshritf@il.ibm.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hpe.com>\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 26, 'created': '2016-08-10 08:18:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6ae1be1a6b32d3710376496e9facaf7cf62d999a', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nCo-Authored-By: Oshrit Feder <oshritf@il.ibm.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hpe.com>\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 27, 'created': '2016-08-10 17:56:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/13aacd54fba4f186dabe5868dd124d6109d10ff0', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nCo-Authored-By: Oshrit Feder <oshritf@il.ibm.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hpe.com>\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 28, 'created': '2016-08-10 18:04:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/beffc5014c01ee62cecefe447122912129418155', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1 so as to maintain current\nrun time behavior.\nThe saio config files were edited to facilitate the probe tests\nso as to test the daemon with > 1 processes.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nCo-Authored-By: Oshrit Feder <oshritf@il.ibm.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hpe.com>\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 29, 'created': '2016-08-12 17:16:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2d27472dccd7e2935f80519a619653f0fa21f039', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1, so that by default a single\nchild process is spawned in addition to the daemon.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nCo-Authored-By: Oshrit Feder <oshritf@il.ibm.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hpe.com>\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 30, 'created': '2016-08-15 10:55:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/d67c35cdc76822b65f06cf75d8a3bca8576bdc71', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1, so that by default a single\nchild process is spawned in addition to the daemon.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nCo-Authored-By: Oshrit Feder <oshritf@il.ibm.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hpe.com>\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 31, 'created': '2016-08-18 17:52:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c9b644718e962655dfb827b181e6ee9a4205c373', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1, so that by default a single\nchild process is spawned in addition to the daemon.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nCo-Authored-By: Oshrit Feder <oshritf@il.ibm.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hpe.com>\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 32, 'created': '2016-09-07 18:13:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/b0361d796c0f7e122a35547e0494a57106a22554', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1, so that by default a single\nchild process is spawned in addition to the daemon.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nCo-Authored-By: Oshrit Feder <oshritf@il.ibm.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hpe.com>\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 33, 'created': '2016-09-19 07:59:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c83190f61344bdf2cb3dceb5b4b4900c79f27df5', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1, so that by default a single\nchild process is spawned in addition to the daemon.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nCo-Authored-By: Oshrit Feder <oshritf@il.ibm.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hpe.com>\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 34, 'created': '2016-09-19 20:21:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/5cfc29eec02043b02c44e6d385ee2d2e94795906', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1, so that by default a single\nchild process is spawned in addition to the daemon.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nCo-Authored-By: Oshrit Feder <oshritf@il.ibm.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hpe.com>\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}, {'number': 35, 'created': '2016-11-10 16:22:51.000000000', 'files': ['swift/container/sync.py', 'etc/container-server.conf-sample', 'test/unit/container/test_sync.py', 'test/probe/test_container_sync.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/e4616490c9b4fdf0320cdbb75e1afaac24bd03a1', 'message': ""Add process level concurrency to container sync\n\nThis change introduces a new config option 'processes' to the\ncontainer sync daemon. The daemon spawns a new process per each\ncontainer to sync, up to the 'processes' limit. When the limit\nis reached, the daemon waits for one of the processes to finish.\n\nThe 'spawning pattern' follows that of e.g. the container updater\nThe daemon process communicates with pipes with the child processes\nto keep track of sync statistics. This, again, follows the container\nupdater pid2filename usage only with pipes instead of files.\n\nThe default for processes is set to 1, so that by default a single\nchild process is spawned in addition to the daemon.\n\nThis patch is closely related to [1] which adds thread level\nconcurrency to the sync daemon. [2] gives full details on\nperformance measurements showing the gain in throughput when\napplying different levels of concurrency to the sync daemon.\n\n[1] https://review.openstack.org/#/c/225338/\n[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi\n\nCo-Authored-By: Oshrit Feder <oshritf@il.ibm.com>\nCo-Authored-By: Alistair Coles <alistair.coles@hpe.com>\n\nChange-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\nPartial-Bug: #1068426\n""}]",122,210099,e4616490c9b4fdf0320cdbb75e1afaac24bd03a1,134,12,35,11317,,,0,"Add process level concurrency to container sync

This change introduces a new config option 'processes' to the
container sync daemon. The daemon spawns a new process per each
container to sync, up to the 'processes' limit. When the limit
is reached, the daemon waits for one of the processes to finish.

The 'spawning pattern' follows that of e.g. the container updater
The daemon process communicates with pipes with the child processes
to keep track of sync statistics. This, again, follows the container
updater pid2filename usage only with pipes instead of files.

The default for processes is set to 1, so that by default a single
child process is spawned in addition to the daemon.

This patch is closely related to [1] which adds thread level
concurrency to the sync daemon. [2] gives full details on
performance measurements showing the gain in throughput when
applying different levels of concurrency to the sync daemon.

[1] https://review.openstack.org/#/c/225338/
[2] https://ibm.box.com/s/oerppuv1fj907l2oysw0p39u3q1quvxi

Co-Authored-By: Oshrit Feder <oshritf@il.ibm.com>
Co-Authored-By: Alistair Coles <alistair.coles@hpe.com>

Change-Id: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8
Partial-Bug: #1068426
",git fetch https://review.opendev.org/openstack/swift refs/changes/99/210099/32 && git format-patch -1 --stdout FETCH_HEAD,"['doc/saio/swift/container-server/3.conf', 'swift/container/sync.py', 'doc/saio/swift/container-server/1.conf', 'etc/container-server.conf-sample', 'test/unit/container/test_sync.py', 'doc/saio/swift/container-server/2.conf', 'doc/saio/swift/container-server/4.conf', 'test/probe/test_container_sync.py']",8,f9f2190424dbe4c2e34ae14eb970c1d8cfe6fe93,bug/1068426,"import timefrom swift.common.utils import readconf def parse_conf(self): conf_files = [] for server in self.sync.servers: conf_files.extend(server.conf_files()) conf_file = conf_files[0] return readconf(conf_file, 'container-sync') self.sync = Manager(['container-sync']) self.sync_conf = self.parse_conf() self.concurrency = int(self.sync_conf['concurrency']) self.interval = int(self.sync_conf['interval']) def create_source_and_dest_containers(self, container_source_prefix, container_dest_prefix, num): container_suffixes = list() for i in range(num): container_suffix = uuid.uuid4() container_suffixes.append(container_suffix) # setup dest container dest_container = '%s-container-%s' % (container_dest_prefix, container_suffix) dest_headers = base_headers.copy() dest_policy = None if len(ENABLED_POLICIES) > 1: dest_policy = random.choice(ENABLED_POLICIES) dest_headers['X-Storage-Policy'] = dest_policy.name client.put_container(self.url, self.token, dest_container, headers=dest_headers) # setup source container source_container = '%s-container-%s' % (container_source_prefix, container_suffix) source_headers = base_headers.copy() sync_to = '//%s/%s/%s/%s' % (self.realm, self.cluster, self.account, dest_container) source_headers['X-Container-Sync-To'] = sync_to if dest_policy: source_policy = random.choice([p for p in ENABLED_POLICIES if p is not dest_policy]) source_headers['X-Storage-Policy'] = source_policy.name client.put_container(self.url, self.token, source_container, headers=source_headers) return container_suffixes def populate_source_containers(self, container_source_prefix, container_suffixes, num): obj_names = [] for i in range(num): obj_names.append(uuid.uuid4()) for container_suffix in container_suffixes: source_container = '%s-container-%s' % (container_source_prefix, container_suffix) for obj_name in obj_names: client.put_object(self.url, self.token, source_container, obj_name, '%s\n%s' % (container_suffix, obj_name)) return obj_names def validate_container_sync(self, container_dest_prefix, container_suffixes, obj_names): for container_suffix in container_suffixes: dest_container = '%s-container-%s' % (container_dest_prefix, container_suffix) for obj_name in obj_names: headers, body = client.get_object(self.url, self.token, dest_container, obj_name) obj_body = '%s\n%s' % (container_suffix, obj_name) self.assertEqual(body, obj_body) def sync_containers(self, once): if once is True: Manager(['container-sync']).once() else: self.sync.start() time.sleep(self.interval) def test_sync_once(self): container_suffixes = self.create_source_and_dest_containers('source', 'dest', 1) obj_names = self.populate_source_containers('source', container_suffixes, 1) self.sync_containers(True) self.validate_container_sync('dest', container_suffixes, obj_names) def test_sync_forever_single_container(self): # Test with number of containers == 1 container_suffixes = self.create_source_and_dest_containers('source', 'dest', 1) obj_names = self.populate_source_containers('source', container_suffixes, 1) self.sync_containers(False) self.validate_container_sync('dest', container_suffixes, obj_names) def test_sync_forever_multiple_containers(self): # Test with number of containers == concurrency container_suffixes = self.create_source_and_dest_containers( 'source', 'dest', self.concurrency) obj_names = self.populate_source_containers('source', container_suffixes, 1) self.sync_containers(False) self.validate_container_sync('dest', container_suffixes, obj_names) def test_sync_forever_multiple_containers_and_objects(self): containers = 2 * self.concurrency container_suffixes = self.create_source_and_dest_containers( 'source', 'dest', containers) obj_names = self.populate_source_containers('source', container_suffixes, 10) self.sync_containers(False) self.validate_container_sync('dest', container_suffixes, obj_names)"," def test_sync(self): # setup dest container dest_container = 'dest-container-%s' % uuid.uuid4() dest_headers = base_headers.copy() dest_policy = None if len(ENABLED_POLICIES) > 1: dest_policy = random.choice(ENABLED_POLICIES) dest_headers['X-Storage-Policy'] = dest_policy.name client.put_container(self.url, self.token, dest_container, headers=dest_headers) # setup source container source_container = 'source-container-%s' % uuid.uuid4() source_headers = base_headers.copy() sync_to = '//%s/%s/%s/%s' % (self.realm, self.cluster, self.account, dest_container) source_headers['X-Container-Sync-To'] = sync_to if dest_policy: source_policy = random.choice([p for p in ENABLED_POLICIES if p is not dest_policy]) source_headers['X-Storage-Policy'] = source_policy.name client.put_container(self.url, self.token, source_container, headers=source_headers) # upload to source object_name = 'object-%s' % uuid.uuid4() client.put_object(self.url, self.token, source_container, object_name, 'test-body') Manager(['container-sync']).once() # retrieve from sync'd container headers, body = client.get_object(self.url, self.token, dest_container, object_name) self.assertEqual(body, 'test-body') ",306,61
openstack%2Fswift~master~Ia305c53140fd2c2ffe18f60bfee5c166c7a3531a,openstack/swift,master,Ia305c53140fd2c2ffe18f60bfee5c166c7a3531a,Refactor container sync stats reporting,NEW,2016-11-08 17:36:49.000000000,2017-12-18 03:16:48.000000000,,"[{'_account_id': 3094}, {'_account_id': 12279}, {'_account_id': 13052}]","[{'number': 1, 'created': '2016-11-08 17:36:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2678d232d48d849ea562c3cafe8707149b7f2918', 'message': 'Refactor container sync stats reporting\n\nPreviously the container sync daemon stats would be updated directly\nfrom the container_sync and container_row_sync methods. This makes it\nharder to split a container_sync operation out to a separate process\nor thread as proposed in [1]. For example, special care must be taken\nto correctly reset any daemon stats in a forked process before\naccumulating stats in that process.\n\nThis patch refactors the stats handling so that the container_sync and\ncontainer_sync_row methods now return a stats dict containing only\nincremental updates for stats, and the daemon then applies returned\ncontainer stats to its stats dict.\n\nThis required a good deal of test refactoring, and in the process it\nwas discovered that some unit tests were obscuring errors. In\nparticular test_container_sync_first_loop and\ntest_container_sync_second_loop in test/unit/container/test_sync.py\nseem to have been erroneous since [2]. These tests have been replaced\nwith new tests that mock the container_sync_row method in order to\nbetter isolate and verify the behavior of the container_sync\nmethod. The container_sync_row method is covered by other tests.\n\n[1] Related-Change: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\n[2] commit 8b140033f01333fbd6d41e2946db949ab6f92599\n\nChange-Id: Ia305c53140fd2c2ffe18f60bfee5c166c7a3531a\n'}, {'number': 2, 'created': '2016-11-10 14:12:05.000000000', 'files': ['swift/container/sync.py', 'test/unit/container/test_sync.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/43c0115e71cbd4242d7dadaff60081e792909878', 'message': 'Refactor container sync stats reporting\n\nPreviously the container sync daemon stats would be updated directly\nfrom the container_sync and container_row_sync methods. This makes it\nharder to split a container_sync operation out to a separate process\nor thread as proposed in [1]. For example, special care must be taken\nto correctly reset any daemon stats in a forked process before\naccumulating stats in that process.\n\nThis patch refactors the stats handling so that the container_sync and\ncontainer_sync_row methods now return a stats dict containing only\nincremental updates for stats, and the daemon then applies returned\ncontainer stats to its stats dict.\n\nThis required a good deal of test refactoring, and in the process it\nwas discovered that some unit tests were obscuring errors. In\nparticular test_container_sync_first_loop and\ntest_container_sync_second_loop in test/unit/container/test_sync.py\nseem to have been erroneous since [2]. These tests have been replaced\nwith new tests that mock the container_sync_row method in order to\nbetter isolate and verify the behavior of the container_sync\nmethod. The container_sync_row method is covered by other tests.\n\n[1] Related-Change: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8\n[2] commit 8b140033f01333fbd6d41e2946db949ab6f92599\n\nChange-Id: Ia305c53140fd2c2ffe18f60bfee5c166c7a3531a\n'}]",3,395094,43c0115e71cbd4242d7dadaff60081e792909878,10,3,2,7847,,,0,"Refactor container sync stats reporting

Previously the container sync daemon stats would be updated directly
from the container_sync and container_row_sync methods. This makes it
harder to split a container_sync operation out to a separate process
or thread as proposed in [1]. For example, special care must be taken
to correctly reset any daemon stats in a forked process before
accumulating stats in that process.

This patch refactors the stats handling so that the container_sync and
container_sync_row methods now return a stats dict containing only
incremental updates for stats, and the daemon then applies returned
container stats to its stats dict.

This required a good deal of test refactoring, and in the process it
was discovered that some unit tests were obscuring errors. In
particular test_container_sync_first_loop and
test_container_sync_second_loop in test/unit/container/test_sync.py
seem to have been erroneous since [2]. These tests have been replaced
with new tests that mock the container_sync_row method in order to
better isolate and verify the behavior of the container_sync
method. The container_sync_row method is covered by other tests.

[1] Related-Change: I8566b8b0d5511e93f8f5ea2eb2a25d0d3631bae8
[2] commit 8b140033f01333fbd6d41e2946db949ab6f92599

Change-Id: Ia305c53140fd2c2ffe18f60bfee5c166c7a3531a
",git fetch https://review.opendev.org/openstack/swift refs/changes/94/395094/1 && git format-patch -1 --stdout FETCH_HEAD,"['swift/container/sync.py', 'test/unit/container/test_sync.py']",2,2678d232d48d849ea562c3cafe8707149b7f2918,p-sync-container-stats,"import contextlibimport timefrom test.unit import FakeLogger @property def sync_point1(self): # convenience for test assertions return self.info.get('x_container_sync_point1') @property def sync_point2(self): # convenience for test assertions return self.info.get('x_container_sync_point2') def __str__(self): return self.db_file self.info.update({'x_container_sync_point1': sync_point1, 'x_container_sync_point2': sync_point2}) @contextlib.contextmanager def mocked_container_broker(sync_point_1, sync_point_2, items_since): fcb = FakeContainerBroker( 'path', info={'account': 'a', 'container': 'c', 'storage_policy_index': 0, 'x_container_sync_point1': sync_point_1, 'x_container_sync_point2': sync_point_2}, metadata={'x-container-sync-to': ('http://127.0.0.1/a/c', 1), 'x-container-sync-key': ('key', 1)}, items_since=items_since) with mock.patch('swift.container.sync.ContainerBroker', lambda p: fcb): yield fcb def fake_sync_row(row_stats_list): # Make a function that simulates container_sync_row side effects def do_add_stats(row, sync_to, user_key, broker, info, realm, realm_key): return row_stats_list.pop(0) return do_add_stats cring = FakeRing() with mock.patch('swift.container.sync.InternalClient'): self.cs = sync.ContainerSync( {}, container_ring=cring, logger=debug_logger()) self.cs._myips = ['10.0.0.0'] # Match self.cs._myport = 1000 # Match self.cs.allowed_sync_hosts = ['127.0.0.1'] self.assertEqual(cs.stats['failures'], 0) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 1) self.assertEqual(stats['skips'], 0) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 1) self.assertEqual(stats['skips'], 0) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 1) self.assertEqual(stats['skips'], 0) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 1) self.assertEqual(stats['skips'], 0) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 0) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 0) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 0) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 1) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 1) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 0) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 0) self.assertEqual(stats['skips'], 1) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 0) self.assertEqual(stats['skips'], 1) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 0) self.assertEqual(stats['skips'], 1) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 1) self.assertEqual(stats['skips'], 0) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 0) self.assertEqual(stats['skips'], 0) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 1) self.assertEqual(stats['skips'], 0) stats = cs.container_sync('isa.db') self.assertEqual(stats['failures'], 0) self.assertEqual(stats['skips'], 0) def test_container_sync_no_rows(self): # Ensure first loop runs by having unequal sync points with mocked_container_broker(2, -1, []) as fcb: with mock.patch.object( self.cs, 'container_sync_row') as mock_sync_row: stats = self.cs.container_sync('isa.db') self.assertEqual(0, mock_sync_row.call_count) self.assertDictEqual(dict(syncs=1, puts=0, deletes=0, bytes=0), stats) self.assertEqual(fcb.sync_point1, 2) self.assertEqual(fcb.sync_point2, -1) log_lines = self.cs.logger.get_lines_for_level('info') self.assertIn('Container sync report: a/c', log_lines[0]) self.assertIn('puts: 0', log_lines[0]) self.assertIn('posts: 0', log_lines[0]) self.assertIn('deletes: 0', log_lines[0]) self.assertIn('sync_point1: 2', log_lines[0]) self.assertIn('sync_point2: -1', log_lines[0]) self.assertIn('total_rows: 1', log_lines[0]) self.assertFalse(log_lines[1:]) self.assertFalse(self.cs.logger.get_lines_for_level('error')) def test_container_sync_first_loop_one_row_put_succeeds(self): # Ensure first loop runs by having unequal sync points row = {'ROWID': 1, 'name': 'o', 'created_at': '1.2', 'deleted': False} row_stats = {'puts': 1, 'bytes': 42} with mocked_container_broker(2, -1, [row]) as fcb: with mock.patch.object( self.cs, 'container_sync_row', side_effect=fake_sync_row([row_stats]) ) as mock_sync_row: stats = self.cs.container_sync('isa.db') mock_sync_row.assert_called_once_with( row, 'http://127.0.0.1/a/c', 'key', fcb, fcb.info, None, None) self.assertDictEqual(dict(syncs=1, puts=1, deletes=0, bytes=42), stats) self.assertEqual(fcb.sync_point1, None) self.assertEqual(fcb.sync_point2, 1) log_lines = self.cs.logger.get_lines_for_level('info') self.assertIn('Container sync report: a/c', log_lines[0]) self.assertIn('puts: 1', log_lines[0]) self.assertIn('posts: 0', log_lines[0]) self.assertIn('deletes: 0', log_lines[0]) self.assertIn('sync_point1: 2', log_lines[0]) self.assertIn('sync_point2: 1', log_lines[0]) self.assertIn('total_rows: 1', log_lines[0]) self.assertFalse(log_lines[1:]) self.assertFalse(self.cs.logger.get_lines_for_level('error')) def test_container_sync_first_loop_one_row_delete_succeeds(self): # Ensure first loop runs by having unequal sync points row = {'ROWID': 1, 'name': 'o', 'created_at': '1.2', 'deleted': True} row_stats = {'deletes': 1} with mocked_container_broker(2, -1, [row]) as fcb: with mock.patch.object( self.cs, 'container_sync_row', side_effect=fake_sync_row([row_stats]) ) as mock_sync_row: stats = self.cs.container_sync('isa.db') mock_sync_row.assert_called_once_with( row, 'http://127.0.0.1/a/c', 'key', fcb, fcb.info, None, None) self.assertDictEqual(dict(syncs=1, puts=0, deletes=1, bytes=0), stats) self.assertEqual(fcb.sync_point1, None) self.assertEqual(fcb.sync_point2, 1) log_lines = self.cs.logger.get_lines_for_level('info') self.assertIn('Container sync report: a/c', log_lines[0]) self.assertIn('puts: 0', log_lines[0]) self.assertIn('posts: 0', log_lines[0]) self.assertIn('deletes: 1', log_lines[0]) self.assertIn('sync_point1: 2', log_lines[0]) self.assertIn('sync_point2: 1', log_lines[0]) self.assertIn('total_rows: 1', log_lines[0]) self.assertFalse(log_lines[1:]) self.assertFalse(self.cs.logger.get_lines_for_level('error')) def test_container_sync_first_loop_one_row_errors(self): # Ensure first loop runs by having unequal sync points row = {'ROWID': 1, 'name': 'o', 'created_at': '1.2', 'deleted': False} with mocked_container_broker(2, -1, [row]) as fcb: with mock.patch.object( self.cs, 'container_sync_row', side_effect=Exception('whoops') ) as mock_sync_row: stats = self.cs.container_sync('isa.db') mock_sync_row.assert_called_once_with( row, 'http://127.0.0.1/a/c', 'key', fcb, fcb.info, None, None) self.assertDictEqual( dict(failures=1, puts=0, deletes=0, bytes=0), stats) self.assertEqual(fcb.sync_point1, 2) self.assertEqual(fcb.sync_point2, -1) log_lines = self.cs.logger.get_lines_for_level('info') self.assertIn('Container sync report: a/c', log_lines[0]) self.assertIn('puts: 0', log_lines[0]) self.assertIn('posts: 0', log_lines[0]) self.assertIn('deletes: 0', log_lines[0]) self.assertIn('sync_point1: 2', log_lines[0]) self.assertIn('sync_point2: None', log_lines[0]) self.assertIn('total_rows: 1', log_lines[0]) self.assertFalse(log_lines[1:]) log_lines = self.cs.logger.get_lines_for_level('error') self.assertIn('ERROR Syncing path', log_lines[0]) self.assertFalse(log_lines[1:]) def test_container_sync_first_loop_one_row_fails_one_succeeds(self): # Ensure first loop runs by having unequal sync points rows = [ {'ROWID': 1, 'name': 'o', 'created_at': '1.2', 'deleted': False}, {'ROWID': 2, 'name': 'p', 'created_at': '1.3', 'deleted': False} ] row_stats_1 = {'failures': 1} row_stats_2 = {'puts': 1, 'bytes': 42} with mocked_container_broker(2, -1, rows) as fcb: with mock.patch.object( self.cs, 'container_sync_row', side_effect=fake_sync_row([row_stats_1, row_stats_2]) ) as mock_sync_row: stats = self.cs.container_sync('isa.db') mock_sync_row.assert_has_calls([ mock.call(rows[0], 'http://127.0.0.1/a/c', 'key', fcb, fcb.info, None, None), mock.call(rows[1], 'http://127.0.0.1/a/c', 'key', fcb, fcb.info, None, None)]) self.assertDictEqual( dict(failures=1, syncs=1, puts=1, deletes=0, bytes=42), stats) self.assertEqual(fcb.sync_point1, None) self.assertEqual(fcb.sync_point2, -1) log_lines = self.cs.logger.get_lines_for_level('info') self.assertIn('Container sync report: a/c', log_lines[0]) self.assertIn('puts: 1', log_lines[0]) self.assertIn('posts: 0', log_lines[0]) self.assertIn('deletes: 0', log_lines[0]) self.assertIn('sync_point1: 2', log_lines[0]) self.assertIn('sync_point2: -1', log_lines[0]) self.assertIn('total_rows: 1', log_lines[0]) self.assertFalse(log_lines[1:]) self.assertFalse(self.cs.logger.get_lines_for_level('error')) def test_container_sync_second_loop_one_row_no_match(self): # Ensure second loop runs by having equal sync points row = {'ROWID': 1, 'name': 'o', 'created_at': '1.2', 'deleted': False} with mocked_container_broker(-1, -1, [row]) as fcb: with mock.patch.object( self.cs, 'container_sync_row') as mock_sync_row: # Ensures row does not match for second loop with mock.patch.object(sync, 'hash_path', return_value='\x01' * 16): stats = self.cs.container_sync('isa.db') self.assertEqual(0, mock_sync_row.call_count) self.assertDictEqual(dict(syncs=1, puts=0, deletes=0, bytes=0), stats) self.assertEqual(fcb.sync_point1, 1) self.assertEqual(fcb.sync_point2, None) log_lines = self.cs.logger.get_lines_for_level('info') self.assertIn('Container sync report: a/c', log_lines[0]) self.assertIn('puts: 0', log_lines[0]) self.assertIn('posts: 0', log_lines[0]) self.assertIn('deletes: 0', log_lines[0]) self.assertIn('sync_point1: 1', log_lines[0]) self.assertIn('sync_point2: -1', log_lines[0]) self.assertIn('total_rows: 1', log_lines[0]) self.assertFalse(log_lines[1:]) self.assertFalse(self.cs.logger.get_lines_for_level('error')) def test_container_sync_second_loop_one_row_put_succeeds(self): # Ensure second loop runs by having equal sync points row = {'ROWID': 1, 'name': 'o', 'created_at': '1.2', 'deleted': False} row_stats = {'puts': 1, 'bytes': 42} with mocked_container_broker(-1, -1, [row]) as fcb: with mock.patch.object( self.cs, 'container_sync_row', side_effect=fake_sync_row([row_stats]) ) as mock_sync_row: # Ensures row matches for second loop with mock.patch.object(sync, 'hash_path', return_value='\x00' * 16): stats = self.cs.container_sync('isa.db') mock_sync_row.assert_called_once_with( row, 'http://127.0.0.1/a/c', 'key', fcb, fcb.info, None, None) self.assertDictEqual(dict(syncs=1, puts=1, deletes=0, bytes=42), stats) self.assertEqual(fcb.sync_point1, 1) self.assertEqual(fcb.sync_point2, None) log_lines = self.cs.logger.get_lines_for_level('info') self.assertIn('Container sync report: a/c', log_lines[0]) self.assertIn('puts: 1', log_lines[0]) self.assertIn('posts: 0', log_lines[0]) self.assertIn('deletes: 0', log_lines[0]) self.assertIn('sync_point1: 1', log_lines[0]) self.assertIn('sync_point2: -1', log_lines[0]) self.assertIn('total_rows: 1', log_lines[0]) self.assertFalse(log_lines[1:]) self.assertFalse(self.cs.logger.get_lines_for_level('error')) def test_container_sync_second_loop_one_row_delete_succeeds(self): # Ensure second loop runs by having equal sync points row = {'ROWID': 1, 'name': 'o', 'created_at': '1.2', 'deleted': True} row_stats = {'deletes': 1} with mocked_container_broker(-1, -1, [row]) as fcb: with mock.patch.object( self.cs, 'container_sync_row', side_effect=fake_sync_row([row_stats]) ) as mock_sync_row: # Ensures row matches for second loop with mock.patch.object(sync, 'hash_path', return_value='\x00' * 16): stats = self.cs.container_sync('isa.db') mock_sync_row.assert_called_once_with( row, 'http://127.0.0.1/a/c', 'key', fcb, fcb.info, None, None) self.assertDictEqual(dict(syncs=1, puts=0, deletes=1, bytes=0), stats) self.assertEqual(fcb.sync_point1, 1) self.assertEqual(fcb.sync_point2, None) log_lines = self.cs.logger.get_lines_for_level('info') self.assertIn('Container sync report: a/c', log_lines[0]) self.assertIn('puts: 0', log_lines[0]) self.assertIn('posts: 0', log_lines[0]) self.assertIn('deletes: 1', log_lines[0]) self.assertIn('sync_point1: 1', log_lines[0]) self.assertIn('sync_point2: -1', log_lines[0]) self.assertIn('total_rows: 1', log_lines[0]) self.assertFalse(log_lines[1:]) self.assertFalse(self.cs.logger.get_lines_for_level('error')) def test_container_sync_second_loop_one_row_errors(self): # Ensure second loop runs by having equal sync points row = {'ROWID': 1, 'name': 'o', 'created_at': '1.2', 'deleted': False} with mocked_container_broker(-1, -1, [row]) as fcb: with mock.patch.object( self.cs, 'container_sync_row', side_effect=Exception('Whoops') ) as mock_sync_row: # Ensures row matches for second loop with mock.patch.object(sync, 'hash_path', return_value='\x00' * 16): stats = self.cs.container_sync('isa.db') mock_sync_row.assert_called_once_with( row, 'http://127.0.0.1/a/c', 'key', fcb, fcb.info, None, None) self.assertDictEqual( dict(failures=1, puts=0, deletes=0, bytes=0), stats) self.assertEqual(fcb.sync_point1, -1) self.assertEqual(fcb.sync_point2, -1) log_lines = self.cs.logger.get_lines_for_level('info') self.assertIn('Container sync report: a/c', log_lines[0]) self.assertIn('puts: 0', log_lines[0]) self.assertIn('posts: 0', log_lines[0]) self.assertIn('deletes: 0', log_lines[0]) self.assertIn('sync_point1: -1', log_lines[0]) self.assertIn('sync_point2: -1', log_lines[0]) self.assertIn('total_rows: 1', log_lines[0]) self.assertFalse(log_lines[1:]) log_lines = self.cs.logger.get_lines_for_level('error') self.assertIn('ERROR Syncing path', log_lines[0]) self.assertFalse(log_lines[1:]) def test_container_sync_second_loop_one_row_fails_one_succeeds(self): # Ensure second loop runs by having equal sync points rows = [ {'ROWID': 1, 'name': 'o', 'created_at': '1.2', 'deleted': False}, {'ROWID': 2, 'name': 'p', 'created_at': '1.3', 'deleted': False} ] row_stats_1 = {'failures': 1} row_stats_2 = {'puts': 1, 'bytes': 42} with mocked_container_broker(-1, -1, rows) as fcb: with mock.patch.object( self.cs, 'container_sync_row', side_effect=fake_sync_row([row_stats_1, row_stats_2]) ) as mock_sync_row: # Ensures row matches for second loop with mock.patch.object(sync, 'hash_path', return_value='\x00' * 16): stats = self.cs.container_sync('isa.db') mock_sync_row.assert_has_calls([ mock.call(rows[0], 'http://127.0.0.1/a/c', 'key', fcb, fcb.info, None, None), mock.call(rows[1], 'http://127.0.0.1/a/c', 'key', fcb, fcb.info, None, None)]) self.assertDictEqual( dict(failures=1, syncs=1, puts=1, deletes=0, bytes=42), stats) self.assertEqual(fcb.sync_point1, 2) self.assertEqual(fcb.sync_point2, None) log_lines = self.cs.logger.get_lines_for_level('info') self.assertIn('Container sync report: a/c', log_lines[0]) self.assertIn('puts: 1', log_lines[0]) self.assertIn('posts: 0', log_lines[0]) self.assertIn('deletes: 0', log_lines[0]) self.assertIn('sync_point1: 2', log_lines[0]) self.assertIn('sync_point2: -1', log_lines[0]) self.assertIn('total_rows: 1', log_lines[0]) self.assertFalse(log_lines[1:]) self.assertFalse(self.cs.logger.get_lines_for_level('error')) self.assertEqual(cs.container_sync_row( realm, realm_key), {'deletes': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {'failures': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {'failures': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {'deletes': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {'bytes': 50, 'puts': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {'bytes': 60, 'puts': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {'bytes': 60, 'puts': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {'bytes': 60, 'puts': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {'failures': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {'failures': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {'failures': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {'failures': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {'failures': 1}) self.assertEqual(cs.container_sync_row( realm, realm_key), {}) # No additional errors self.assertEqual(cs.container_sync_row( realm, realm_key), {}) # No additional errors self.assertEqual(cs.container_sync_row( test_info, realm, realm_key), {'bytes': 10, 'puts': 1}) self.assertEqual(cs.container_sync_row( test_info, realm, realm_key), {'failures': 1}) self.assertEqual(cs.container_sync_row( test_row, 'http://sync/to/path', 'key', FakeContainerBroker('broker'), test_info, realm, realm_key), {'failures': 1}) self.assertEqual(cs.container_sync_row( test_info, realm, realm_key), {'bytes': 10, 'puts': 1}) def test_update_stats(self): self.cs._update_stats({'deletes': 1, 'syncs': 1}) exp = {'deletes': 1, 'puts': 0, 'skips': 0, 'syncs': 1, 'failures': 0} self.assertDictEqual(exp, self.cs.stats) self.cs._update_stats({'puts': 1, 'syncs': 1}) exp = {'deletes': 1, 'puts': 1, 'skips': 0, 'syncs': 2, 'failures': 0} self.assertDictEqual(exp, self.cs.stats) self.cs._update_stats({'failures': 1, 'syncs': 1, 'skips': 3}) exp = {'deletes': 1, 'puts': 1, 'skips': 3, 'syncs': 3, 'failures': 1} self.assertDictEqual(exp, self.cs.stats) self.cs._update_stats( {'deletes': 5, 'puts': 6, 'skips': 4, 'syncs': 3, 'failures': 8}) exp = {'deletes': 6, 'puts': 7, 'skips': 7, 'syncs': 6, 'failures': 9} self.assertDictEqual(exp, self.cs.stats) self.cs._update_stats({'failures': 90}) exp = {'deletes': 6, 'puts': 7, 'skips': 7, 'syncs': 6, 'failures': 99} self.assertDictEqual(exp, self.cs.stats) def test_report(self): fake_times = (0, 99, 1999) with mock.patch('swift.container.sync.InternalClient'), \ mock.patch('swift.container.sync.time', side_effect=fake_times): cs = sync.ContainerSync( {}, container_ring=FakeRing(), logger=FakeLogger()) self.assertEqual(fake_times[0], cs.reported) cs.stats = dict(puts=0, deletes=0, skips=0, syncs=0, failures=0) cs.report() self.assertFalse(cs.stats) self.assertEqual(fake_times[1], cs.reported) info_lines = cs.logger.get_lines_for_level('info') self.assertEqual( 'Since time: %s, synced: 0, deletes: 0, puts: 0, ' 'skipped: 0, failed: 0' % time.ctime(fake_times[0]), info_lines[0]) self.assertFalse(info_lines[1:]) cs.stats = dict(puts=6, deletes=7, skips=3, syncs=4, failures=1) cs.report() self.assertFalse(cs.stats) self.assertEqual(fake_times[2], cs.reported) info_lines = cs.logger.get_lines_for_level('info') self.assertEqual( 'Since time: %s, synced: 4, deletes: 7, puts: 6, ' 'skipped: 3, failed: 1' % time.ctime(fake_times[1]), info_lines[1]) self.assertFalse(info_lines[2:]) "," self.sync_point1 = -1 self.sync_point2 = -1 self.sync_point1 = sync_point1 self.sync_point2 = sync_point2 self.assertEqual(cs.container_failures, 0) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 1) self.assertEqual(cs.container_skips, 0) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 2) self.assertEqual(cs.container_skips, 0) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 3) self.assertEqual(cs.container_skips, 0) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 4) self.assertEqual(cs.container_skips, 0) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 0) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 0) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 0) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 1) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 1) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 1) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 0) self.assertEqual(cs.container_skips, 1) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 0) self.assertEqual(cs.container_skips, 2) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 0) self.assertEqual(cs.container_skips, 3) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 1) self.assertEqual(cs.container_skips, 3) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 1) self.assertEqual(cs.container_skips, 3) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 1) self.assertEqual(cs.container_skips, 0) cs.container_sync('isa.db') self.assertEqual(cs.container_failures, 1) self.assertEqual(cs.container_skips, 0) def test_container_first_loop(self): cring = FakeRing() with mock.patch('swift.container.sync.InternalClient'): cs = sync.ContainerSync({}, container_ring=cring) def fake_hash_path(account, container, obj, raw_digest=False): # Ensures that no rows match for full syncing, ordinal is 0 and # all hashes are 0 return '\x00' * 16 fcb = FakeContainerBroker( 'path', info={'account': 'a', 'container': 'c', 'storage_policy_index': 0, 'x_container_sync_point1': 2, 'x_container_sync_point2': -1}, metadata={'x-container-sync-to': ('http://127.0.0.1/a/c', 1), 'x-container-sync-key': ('key', 1)}, items_since=[{'ROWID': 1, 'name': 'o'}]) with mock.patch('swift.container.sync.ContainerBroker', lambda p: fcb), \ mock.patch('swift.container.sync.hash_path', fake_hash_path): cs._myips = ['10.0.0.0'] # Match cs._myport = 1000 # Match cs.allowed_sync_hosts = ['127.0.0.1'] cs.container_sync('isa.db') # Succeeds because no rows match self.assertEqual(cs.container_failures, 1) self.assertEqual(cs.container_skips, 0) self.assertEqual(fcb.sync_point1, None) self.assertEqual(fcb.sync_point2, -1) def fake_hash_path(account, container, obj, raw_digest=False): # Ensures that all rows match for full syncing, ordinal is 0 # and all hashes are 1 return '\x01' * 16 fcb = FakeContainerBroker('path', info={'account': 'a', 'container': 'c', 'storage_policy_index': 0, 'x_container_sync_point1': 1, 'x_container_sync_point2': 1}, metadata={'x-container-sync-to': ('http://127.0.0.1/a/c', 1), 'x-container-sync-key': ('key', 1)}, items_since=[{'ROWID': 1, 'name': 'o'}]) with mock.patch('swift.container.sync.ContainerBroker', lambda p: fcb), \ mock.patch('swift.container.sync.hash_path', fake_hash_path): cs._myips = ['10.0.0.0'] # Match cs._myport = 1000 # Match cs.allowed_sync_hosts = ['127.0.0.1'] cs.container_sync('isa.db') # Succeeds because the two sync points haven't deviated yet self.assertEqual(cs.container_failures, 1) self.assertEqual(cs.container_skips, 0) self.assertEqual(fcb.sync_point1, -1) self.assertEqual(fcb.sync_point2, -1) fcb = FakeContainerBroker( 'path', info={'account': 'a', 'container': 'c', 'storage_policy_index': 0, 'x_container_sync_point1': 2, 'x_container_sync_point2': -1}, metadata={'x-container-sync-to': ('http://127.0.0.1/a/c', 1), 'x-container-sync-key': ('key', 1)}, items_since=[{'ROWID': 1, 'name': 'o'}]) with mock.patch('swift.container.sync.ContainerBroker', lambda p: fcb): cs._myips = ['10.0.0.0'] # Match cs._myport = 1000 # Match cs.allowed_sync_hosts = ['127.0.0.1'] cs.container_sync('isa.db') # Fails because container_sync_row will fail since the row has no # 'deleted' key self.assertEqual(cs.container_failures, 2) self.assertEqual(cs.container_skips, 0) self.assertEqual(fcb.sync_point1, None) self.assertEqual(fcb.sync_point2, -1) def fake_delete_object(*args, **kwargs): raise ClientException fcb = FakeContainerBroker( 'path', info={'account': 'a', 'container': 'c', 'storage_policy_index': 0, 'x_container_sync_point1': 2, 'x_container_sync_point2': -1}, metadata={'x-container-sync-to': ('http://127.0.0.1/a/c', 1), 'x-container-sync-key': ('key', 1)}, items_since=[{'ROWID': 1, 'name': 'o', 'created_at': '1.2', 'deleted': True}]) with mock.patch('swift.container.sync.ContainerBroker', lambda p: fcb), \ mock.patch('swift.container.sync.delete_object', fake_delete_object): cs._myips = ['10.0.0.0'] # Match cs._myport = 1000 # Match cs.allowed_sync_hosts = ['127.0.0.1'] cs.container_sync('isa.db') # Fails because delete_object fails self.assertEqual(cs.container_failures, 3) self.assertEqual(cs.container_skips, 0) self.assertEqual(fcb.sync_point1, None) self.assertEqual(fcb.sync_point2, -1) fcb = FakeContainerBroker( 'path', info={'account': 'a', 'container': 'c', 'storage_policy_index': 0, 'x_container_sync_point1': 2, 'x_container_sync_point2': -1}, metadata={'x-container-sync-to': ('http://127.0.0.1/a/c', 1), 'x-container-sync-key': ('key', 1)}, items_since=[{'ROWID': 1, 'name': 'o', 'created_at': '1.2', 'deleted': True}]) with mock.patch('swift.container.sync.ContainerBroker', lambda p: fcb), \ mock.patch('swift.container.sync.delete_object', lambda *x, **y: None): cs._myips = ['10.0.0.0'] # Match cs._myport = 1000 # Match cs.allowed_sync_hosts = ['127.0.0.1'] cs.container_sync('isa.db') # Succeeds because delete_object succeeds self.assertEqual(cs.container_failures, 3) self.assertEqual(cs.container_skips, 0) self.assertEqual(fcb.sync_point1, None) self.assertEqual(fcb.sync_point2, 1) def test_container_second_loop(self): cring = FakeRing() with mock.patch('swift.container.sync.InternalClient'): cs = sync.ContainerSync({}, container_ring=cring, logger=self.logger) orig_ContainerBroker = sync.ContainerBroker orig_hash_path = sync.hash_path orig_delete_object = sync.delete_object try: # We'll ensure the first loop is always skipped by keeping the two # sync points equal def fake_hash_path(account, container, obj, raw_digest=False): # Ensures that no rows match for second loop, ordinal is 0 and # all hashes are 1 return '\x01' * 16 sync.hash_path = fake_hash_path fcb = FakeContainerBroker( 'path', info={'account': 'a', 'container': 'c', 'storage_policy_index': 0, 'x_container_sync_point1': -1, 'x_container_sync_point2': -1}, metadata={'x-container-sync-to': ('http://127.0.0.1/a/c', 1), 'x-container-sync-key': ('key', 1)}, items_since=[{'ROWID': 1, 'name': 'o'}]) sync.ContainerBroker = lambda p: fcb cs._myips = ['10.0.0.0'] # Match cs._myport = 1000 # Match cs.allowed_sync_hosts = ['127.0.0.1'] cs.container_sync('isa.db') # Succeeds because no rows match self.assertEqual(cs.container_failures, 0) self.assertEqual(cs.container_skips, 0) self.assertEqual(fcb.sync_point1, 1) self.assertEqual(fcb.sync_point2, None) def fake_hash_path(account, container, obj, raw_digest=False): # Ensures that all rows match for second loop, ordinal is 0 and # all hashes are 0 return '\x00' * 16 def fake_delete_object(*args, **kwargs): pass sync.hash_path = fake_hash_path sync.delete_object = fake_delete_object fcb = FakeContainerBroker( 'path', info={'account': 'a', 'container': 'c', 'storage_policy_index': 0, 'x_container_sync_point1': -1, 'x_container_sync_point2': -1}, metadata={'x-container-sync-to': ('http://127.0.0.1/a/c', 1), 'x-container-sync-key': ('key', 1)}, items_since=[{'ROWID': 1, 'name': 'o'}]) sync.ContainerBroker = lambda p: fcb cs._myips = ['10.0.0.0'] # Match cs._myport = 1000 # Match cs.allowed_sync_hosts = ['127.0.0.1'] cs.container_sync('isa.db') # Fails because row is missing 'deleted' key # Nevertheless the fault is skipped self.assertEqual(cs.container_failures, 1) self.assertEqual(cs.container_skips, 0) self.assertEqual(fcb.sync_point1, 1) self.assertEqual(fcb.sync_point2, None) fcb = FakeContainerBroker( 'path', info={'account': 'a', 'container': 'c', 'storage_policy_index': 0, 'x_container_sync_point1': -1, 'x_container_sync_point2': -1}, metadata={'x-container-sync-to': ('http://127.0.0.1/a/c', 1), 'x-container-sync-key': ('key', 1)}, items_since=[{'ROWID': 1, 'name': 'o', 'created_at': '1.2', 'deleted': True}]) sync.ContainerBroker = lambda p: fcb cs._myips = ['10.0.0.0'] # Match cs._myport = 1000 # Match cs.allowed_sync_hosts = ['127.0.0.1'] cs.container_sync('isa.db') # Succeeds because row now has 'deleted' key and delete_object # succeeds self.assertEqual(cs.container_failures, 1) self.assertEqual(cs.container_skips, 0) self.assertEqual(fcb.sync_point1, 1) self.assertEqual(fcb.sync_point2, None) finally: sync.ContainerBroker = orig_ContainerBroker sync.hash_path = orig_hash_path sync.delete_object = orig_delete_object self.assertTrue(cs.container_sync_row( realm, realm_key)) self.assertEqual(cs.container_deletes, 1) self.assertFalse(cs.container_sync_row( realm, realm_key)) self.assertEqual(cs.container_deletes, 1) self.assertFalse(cs.container_sync_row( realm, realm_key)) self.assertEqual(cs.container_deletes, 1) self.assertTrue(cs.container_sync_row( realm, realm_key)) self.assertEqual(cs.container_deletes, 2) expected_put_count = 0 excepted_failure_count = 0 self.assertTrue(cs.container_sync_row( realm, realm_key)) expected_put_count += 1 self.assertEqual(cs.container_puts, expected_put_count) self.assertTrue(cs.container_sync_row( realm, realm_key)) expected_put_count += 1 self.assertEqual(cs.container_puts, expected_put_count) self.assertTrue(cs.container_sync_row( realm, realm_key)) expected_put_count += 1 self.assertEqual(cs.container_puts, expected_put_count) self.assertTrue(cs.container_sync_row( realm, realm_key)) expected_put_count += 1 self.assertEqual(cs.container_puts, expected_put_count) self.assertFalse(cs.container_sync_row( realm, realm_key)) self.assertEqual(cs.container_puts, expected_put_count) excepted_failure_count += 1 self.assertFalse(cs.container_sync_row( realm, realm_key)) self.assertEqual(cs.container_puts, expected_put_count) excepted_failure_count += 1 self.assertFalse(cs.container_sync_row( realm, realm_key)) self.assertEqual(cs.container_puts, expected_put_count) excepted_failure_count += 1 self.assertEqual(cs.container_failures, excepted_failure_count) self.assertFalse(cs.container_sync_row( realm, realm_key)) self.assertEqual(cs.container_puts, expected_put_count) excepted_failure_count += 1 self.assertEqual(cs.container_failures, excepted_failure_count) self.assertFalse(cs.container_sync_row( realm, realm_key)) self.assertEqual(cs.container_puts, expected_put_count) excepted_failure_count += 1 self.assertEqual(cs.container_failures, excepted_failure_count) self.assertTrue(cs.container_sync_row( realm, realm_key)) # No additional errors self.assertEqual(cs.container_failures, excepted_failure_count) self.assertTrue(cs.container_sync_row( realm, realm_key)) # No additional errors self.assertEqual(cs.container_failures, excepted_failure_count) self.assertTrue(cs.container_sync_row( test_info, realm, realm_key)) # No additional errors self.assertEqual(cs.container_failures, excepted_failure_count) self.assertFalse(cs.container_sync_row( test_info, realm, realm_key)) excepted_failure_count += 1 self.assertEqual(cs.container_failures, excepted_failure_count) self.assertFalse(cs.container_sync_row( test_row, 'http://sync/to/path', 'key', FakeContainerBroker('broker'), test_info, realm, realm_key)) excepted_failure_count += 1 self.assertEqual(cs.container_failures, excepted_failure_count) self.assertTrue(cs.container_sync_row( test_info, realm, realm_key)) self.assertEqual(cs.container_failures, excepted_failure_count)",552,405
openstack%2Frally~master~I40c1562900b155bd167d4bfc24d31613249a79f3,openstack/rally,master,I40c1562900b155bd167d4bfc24d31613249a79f3,Move api.task.render_template to task.utils,NEW,2017-05-05 08:36:03.000000000,2017-12-18 03:16:32.000000000,,"[{'_account_id': 13340}, {'_account_id': 14817}, {'_account_id': 21528}, {'_account_id': 22960}]","[{'number': 1, 'created': '2017-05-05 08:36:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c4e8032e87ef6585e6a6cf632c406f99cec2b557', 'message': 'Move api.task.render_template to task.utils\n\n1. move api.task.render_template to task.utils\n2. make template sample path relative to rally directory\n\nChange-Id: I40c1562900b155bd167d4bfc24d31613249a79f3\n'}, {'number': 2, 'created': '2017-05-05 08:47:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/19547ca3ab4bb8c2ba57d7c769b8f67decc89e37', 'message': 'Move api.task.render_template to task.utils\n\n1. move api.task.render_template to task.utils\n2. make template sample path relative to rally directory\n\nChange-Id: I40c1562900b155bd167d4bfc24d31613249a79f3\n'}, {'number': 3, 'created': '2017-05-06 02:56:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c6afae92a5565e049854e79a2f6639c8f17f1b58', 'message': 'Move api.task.render_template to task.utils\n\n1. move api.task.render_template to task.utils\n2. make template sample path relative to rally directory\n\nChange-Id: I40c1562900b155bd167d4bfc24d31613249a79f3\n'}, {'number': 4, 'created': '2017-05-06 06:02:03.000000000', 'files': ['rally/cli/commands/task.py', 'rally/api.py', 'tests/unit/cli/commands/test_task.py', 'tests/unit/test_api.py', 'tests/unit/task/test_utils.py', 'tests/unit/rally_jobs/test_jobs.py', 'tests/unit/doc/test_task_samples.py', 'tests/functional/test_task_samples.py', 'rally/task/utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/f0c0740f66f702ac2676a9cacf7468756e06c31c', 'message': 'Move api.task.render_template to task.utils\n\n1. move api.task.render_template to task.utils\n2. make template sample path relative to rally directory\n\nChange-Id: I40c1562900b155bd167d4bfc24d31613249a79f3\n'}]",1,462869,f0c0740f66f702ac2676a9cacf7468756e06c31c,14,4,4,21528,,,0,"Move api.task.render_template to task.utils

1. move api.task.render_template to task.utils
2. make template sample path relative to rally directory

Change-Id: I40c1562900b155bd167d4bfc24d31613249a79f3
",git fetch https://review.opendev.org/openstack/rally refs/changes/69/462869/4 && git format-patch -1 --stdout FETCH_HEAD,"['rally/cli/commands/task.py', 'rally/api.py', 'tests/unit/cli/commands/test_task.py', 'tests/unit/test_api.py', 'tests/unit/task/test_utils.py', 'rally/task/utils.py']",6,c4e8032e87ef6585e6a6cf632c406f99cec2b557,move.rendertemplate,"import reimport jinja2 import jinja2.meta def render_template(task_template, template_dir=""./"", **kwargs): """"""Render jinja2 task template to Rally input task. :param task_template: String that contains template :param template_dir: The path of directory contain template files :param kwargs: Dict with template arguments :returns: rendered template str """""" def is_really_missing(mis, task_template): # NOTE(boris-42): Removing variables that have default values from # missing. Construction that won't be properly # checked is {% set x = x or 1} if re.search(mis.join([""{%\s*set\s+"", ""\s*=\s*"", ""[^\w]+""]), task_template): return False # NOTE(jlk): Also check for a default filter which can show up as # a missing variable if re.search(mis + ""\s*\|\s*default\("", task_template): return False return True def create_template_functions(): def template_min(int1, int2): return min(int1, int2) def template_max(int1, int2): return max(int1, int2) def template_round(float1): return int(round(float1)) def template_ceil(float1): import math return int(math.ceil(float1)) return {""min"": template_min, ""max"": template_max, ""ceil"": template_ceil, ""round"": template_round} # NOTE(boris-42): We have to import builtins to get the full list of # builtin functions (e.g. range()). Unfortunately, # __builtins__ doesn't return them (when it is not # main module) from six.moves import builtins env = jinja2.Environment( loader=jinja2.FileSystemLoader(template_dir, encoding=""utf8"")) env.globals.update(create_template_functions()) ast = env.parse(task_template) # NOTE(Julia Varigina): # Bug in jinja2.meta.find_undeclared_variables # # The method shows inconsistent behavior: # it does not return undeclared variables that appear # in included templates only (via {%- include ""some_template.yaml""-%}) # and in the same time is declared in jinja2.Environment.globals. # # This is different for undeclared variables that appear directly # in task_template. The method jinja2.meta.find_undeclared_variables # returns an undeclared variable that is used in task_template # and is set in jinja2.Environment.globals. # # Despite this bug, jinja resolves values # declared in jinja2.Environment.globals for both types of undeclared # variables and successfully renders templates in both cases. required_kwargs = jinja2.meta.find_undeclared_variables(ast) missing = (set(required_kwargs) - set(kwargs) - set(dir(builtins)) - set(env.globals)) real_missing = [mis for mis in missing if is_really_missing(mis, task_template)] if real_missing: multi_msg = _(""Please specify next template task arguments: %s"") single_msg = _(""Please specify template task argument: %s"") raise TypeError((len(real_missing) > 1 and multi_msg or single_msg) % "", "".join(real_missing)) render_template = env.from_string(task_template).render(**kwargs) return render_template",,153,146
openstack%2Ftacker~master~I2a7ea70c84beab61391cb4c70ee76b4b52522150,openstack/tacker,master,I2a7ea70c84beab61391cb4c70ee76b4b52522150,Remove unused logging import,NEW,2017-02-20 06:55:19.000000000,2017-12-18 03:16:03.000000000,,"[{'_account_id': 18955}, {'_account_id': 19999}]","[{'number': 1, 'created': '2017-02-20 06:55:19.000000000', 'files': ['tacker/nfvo/drivers/workflow/mistral.py', 'tacker/tests/functional/keystone.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/3eb6bafdb5ca674a621db640c4f1273ef7d78c15', 'message': 'Remove unused logging import\n\nChange-Id: I2a7ea70c84beab61391cb4c70ee76b4b52522150\n'}]",1,435866,3eb6bafdb5ca674a621db640c4f1273ef7d78c15,6,2,1,19935,,,0,"Remove unused logging import

Change-Id: I2a7ea70c84beab61391cb4c70ee76b4b52522150
",git fetch https://review.opendev.org/openstack/tacker refs/changes/66/435866/1 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/nfvo/drivers/workflow/mistral.py', 'tacker/tests/functional/keystone.py']",2,3eb6bafdb5ca674a621db640c4f1273ef7d78c15,, ,from oslo_log import log as logging LOG = logging.getLogger(__name__),0,5
openstack%2Ftacker~master~Ic134fa1d0c6e4ae010f2bf20e8978f9cb6c023eb,openstack/tacker,master,Ic134fa1d0c6e4ae010f2bf20e8978f9cb6c023eb,Correct reduplicative/non-standard log lib ref,NEW,2017-03-02 02:51:31.000000000,2017-12-18 03:15:58.000000000,,"[{'_account_id': 8580}, {'_account_id': 12455}, {'_account_id': 18955}, {'_account_id': 19999}, {'_account_id': 21511}]","[{'number': 1, 'created': '2017-03-02 02:51:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/a30e76d6cef2c59c0d8e56ef58f765f114f773fe', 'message': 'Correct reduplicative/non-standard log lib ref\n\nCorrect reduplicative/non-standard log lib ref to make\nthe code to be more concise.\n\nChange-Id: Ic134fa1d0c6e4ae010f2bf20e8978f9cb6c023eb\n'}, {'number': 2, 'created': '2017-03-15 08:40:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/0c6703f8a52be4b285fe177430a0d8715d5f25a2', 'message': 'Correct reduplicative/non-standard log lib ref\n\nCorrect reduplicative/non-standard log lib ref to make\nthe code to be more concise.\n\nChange-Id: Ic134fa1d0c6e4ae010f2bf20e8978f9cb6c023eb\n'}, {'number': 3, 'created': '2017-03-16 04:53:01.000000000', 'files': ['tacker/common/utils.py', 'tacker/cmd/__init__.py', 'tacker/service.py', 'tacker/tests/base.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/ca16eeffd08b32491b577568b994d1598e318d83', 'message': 'Correct reduplicative/non-standard log lib ref\n\nCorrect reduplicative/non-standard log lib ref to make\nthe code to be more concise.\n\nChange-Id: Ic134fa1d0c6e4ae010f2bf20e8978f9cb6c023eb\n'}]",5,439959,ca16eeffd08b32491b577568b994d1598e318d83,19,5,3,21511,,,0,"Correct reduplicative/non-standard log lib ref

Correct reduplicative/non-standard log lib ref to make
the code to be more concise.

Change-Id: Ic134fa1d0c6e4ae010f2bf20e8978f9cb6c023eb
",git fetch https://review.opendev.org/openstack/tacker refs/changes/59/439959/1 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/common/utils.py', 'tacker/service.py', 'tacker/tests/base.py']",3,a30e76d6cef2c59c0d8e56ef58f765f114f773fe,tacker2017,from oslo_log import log as logging,import logging,4,6
openstack%2Fironic~master~I33d3362d72c1e5880d947f64754ce90d6b0e9fc1,openstack/ironic,master,I33d3362d72c1e5880d947f64754ce90d6b0e9fc1,Refactor API policy and microversion checks,NEW,2017-03-29 18:13:27.000000000,2017-12-18 03:15:44.000000000,,"[{'_account_id': 6637}, {'_account_id': 10118}, {'_account_id': 12356}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}]","[{'number': 1, 'created': '2017-03-29 18:13:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/369ea3b9119a9340671dd945cd78b45185557819', 'message': 'WIP: Refactor API microversion checks\n\nChange-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1\n'}, {'number': 2, 'created': '2017-03-29 18:15:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3f9e72caa2108f3e46f6999554a09b2a21498dc8', 'message': 'WIP: Refactor API microversion checks\n\nChange-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1\n'}, {'number': 3, 'created': '2017-03-30 11:24:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ffa4639c19d3e34bdc1445877b8f9c11ddefc1d2', 'message': 'WIP: Refactor API microversion checks\n\nChange-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1\n'}, {'number': 4, 'created': '2017-03-30 16:37:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/06e5f7ca680df2cfb5348a9ff2f913dc7f0ba1b4', 'message': 'WIP: Refactor API policy and microversion checks\n\nThis change introduces the following decorators:\n\n1. check_policy - checks the policy of the method using the policy\n   string passed as a parameter;\n\n2. <resource>_is_subcontroller - checks if the endpoint is being\n   accessed through subcontroller, meaning some operations are not\n   allowed;\n\n3. api_version_checker - checks that the endpoint itself, the request\n   body and the arguments passed in request correspond to what our\n   API expects at the given microversion.\n\nThese decorators need to be used in this exact order, to preserve the\nprevious order of checks in the API.\n\nChange-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1\n'}, {'number': 5, 'created': '2017-03-30 17:07:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8db5cfceb490fb11f9d1b75208e5af9ebc55d62c', 'message': 'Refactor API policy and microversion checks\n\nThis change introduces the following decorators:\n\n1. check_policy - checks the policy of the method using the policy\n   string passed as a parameter;\n\n2. <resource>_is_subcontroller - checks if the endpoint is being\n   accessed through subcontroller, meaning some operations are not\n   allowed;\n\n3. api_version_checker - checks that the endpoint itself, the request\n   body and the arguments passed in request correspond to what our\n   API expects at the given microversion.\n\nThese decorators need to be used in this exact order, to preserve the\nprevious order of checks in the API.\n\nChange-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1\n'}, {'number': 6, 'created': '2017-03-31 12:50:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/37e4a41b7d0668d2bcb335ddb07e01dcfdb818a8', 'message': 'Refactor API policy and microversion checks\n\nThis change introduces the following decorators:\n\n1. check_policy - checks the policy of the method using the policy\n   string passed as a parameter;\n\n2. <resource>_is_subcontroller - checks if the endpoint is being\n   accessed through subcontroller, meaning some operations are not\n   allowed;\n\n3. api_version_checker - checks that the endpoint itself, the request\n   body and the arguments passed in request correspond to what our\n   API expects at the given microversion.\n\nThese decorators need to be used in this exact order, to preserve the\nprevious order of checks in the API.\n\nChange-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1\n'}, {'number': 7, 'created': '2017-03-31 15:38:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/601416be703cc7e0da3b1d2af627b3018c816fa5', 'message': ""Refactor API policy and microversion checks\n\nThis change introduces the following decorators:\n\n1. policy_checker - checks the policy of the method using the policy\n   string passed as a parameter;\n\n2. <resource>_is_subcontroller - checks if the endpoint is being\n   accessed through subcontroller, meaning some operations are not\n   allowed;\n\n3. api_version_checker - checks that the endpoint itself, the request\n   body and the arguments passed in request correspond to what our\n   API expects at the given microversion.\n\nThese decorators need to be used in this exact order, to preserve the\nprevious order of checks in the API.\n\nThe unittest to check that the exposed methods contain the\npolicy.authorize call is removed. Instead, we assert this during the\nAPI startup.\n\nThe decorator module is added as a dependency, as it is the only one\nthat is able to preserve the funtion signature in python 2. We need\nthis so that wsme's expose continues to work properly.\n\nChange-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1\n""}, {'number': 8, 'created': '2017-04-11 17:13:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/346f83f4669061eda0c05476e0c50ec4c52ee6af', 'message': ""Refactor API policy and microversion checks\n\nThis change introduces the following decorators:\n\n1. policy_checker - checks the policy of the method using the policy\n   string passed as a parameter;\n\n2. <resource>_is_subcontroller - checks if the endpoint is being\n   accessed through subcontroller, meaning some operations are not\n   allowed;\n\n3. api_version_checker - checks that the endpoint itself, the request\n   body and the arguments passed in request correspond to what our\n   API expects at the given microversion.\n\nThese decorators need to be used in this exact order, to preserve the\nprevious order of checks in the API.\n\nThe unittest to check that the exposed methods contain the\npolicy.authorize call is removed. Instead, we assert this during the\nAPI startup.\n\nThe decorator module is added as a dependency, as it is the only one\nthat is able to preserve the funtion signature in python 2. We need\nthis so that wsme's expose continues to work properly.\n\nChange-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1\n""}, {'number': 9, 'created': '2017-04-12 12:39:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a54201684b44b68d1d5932554ef10d02f296d93d', 'message': ""Refactor API policy and microversion checks\n\nThis change introduces the following decorators:\n\n1. policy_checker - checks the policy of the method using the policy\n   string passed as a parameter;\n\n2. <resource>_is_subcontroller - checks if the endpoint is being\n   accessed through subcontroller, meaning some operations are not\n   allowed;\n\n3. api_version_checker - checks that the endpoint itself, the request\n   body and the arguments passed in request correspond to what our\n   API expects at the given microversion.\n\nThese decorators need to be used in this exact order, to preserve the\nprevious order of checks in the API.\n\nThe unittest to check that the exposed methods contain the\npolicy.authorize call is removed. Instead, we assert this during the\nAPI startup.\n\nThe decorator module is added as a dependency, as it is the only one\nthat is able to preserve the funtion signature in python 2. We need\nthis so that wsme's expose continues to work properly.\n\nChange-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1\n""}, {'number': 10, 'created': '2017-04-12 12:48:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5ed51d4b07c975091317b4fca65760089ada84c4', 'message': ""Refactor API policy and microversion checks\n\nThis change introduces the following decorators:\n\n1. policy_checker - checks the policy of the method using the policy\n   string passed as a parameter;\n\n2. <resource>_is_subcontroller - checks if the endpoint is being\n   accessed through subcontroller, meaning some operations are not\n   allowed;\n\n3. api_version_checker - checks that the endpoint itself, the request\n   body and the arguments passed in request correspond to what our\n   API expects at the given microversion.\n\nThese decorators need to be used in this exact order, to preserve the\nprevious order of checks in the API.\n\nThe unittest to check that the exposed methods contain the\npolicy.authorize call is removed. Instead, we assert this during the\nAPI startup.\n\nThe decorator module is added as a dependency, as it is the only one\nthat is able to preserve the funtion signature in python 2. We need\nthis so that wsme's expose continues to work properly.\n\nChange-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1\n""}, {'number': 11, 'created': '2017-05-22 14:27:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cb35e23df1d66dbb6a1356bc05e67fd2e8f3d107', 'message': ""Refactor API policy and microversion checks\n\nThis change introduces the following decorators:\n\n1. policy_checker - checks the policy of the method using the policy\n   string passed as a parameter;\n\n2. <resource>_is_subcontroller - checks if the endpoint is being\n   accessed through subcontroller, meaning some operations are not\n   allowed;\n\n3. api_version_checker - checks that the endpoint itself, the request\n   body and the arguments passed in request correspond to what our\n   API expects at the given microversion.\n\nThese decorators need to be used in this exact order, to preserve the\nprevious order of checks in the API.\n\nThe unittest to check that the exposed methods contain the\npolicy.authorize call is removed. Instead, we assert this during the\nAPI startup.\n\nThe decorator module is added as a dependency, as it is the only one\nthat is able to preserve the funtion signature in python 2. We need\nthis so that wsme's expose continues to work properly.\n\nChange-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1\n""}, {'number': 12, 'created': '2017-05-22 14:38:09.000000000', 'files': ['ironic/api/controllers/v1/port.py', 'ironic/api/controllers/v1/driver.py', 'ironic/api/controllers/v1/ramdisk.py', 'ironic/api/controllers/v1/portgroup.py', 'ironic/tests/unit/api/v1/test_utils.py', 'ironic/api/expose.py', 'requirements.txt', 'ironic/api/controllers/v1/chassis.py', 'ironic/tests/unit/api/v1/test_ports.py', 'ironic/tests/unit/api/test_expose.py', 'ironic/api/controllers/base.py', 'ironic/api/controllers/v1/utils.py', 'ironic/api/controllers/v1/node.py', 'ironic/tests/unit/api/v1/test_expose.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/d1661f2cfbc48c5e5e6750c439f0a0f459f78159', 'message': ""Refactor API policy and microversion checks\n\nThis change introduces the following decorators:\n\n1. policy_checker - checks the policy of the method using the policy\n   string passed as a parameter;\n\n2. <resource>_is_subcontroller - checks if the endpoint is being\n   accessed through subcontroller, meaning some operations are not\n   allowed;\n\n3. api_version_checker - checks that the endpoint itself, the request\n   body and the arguments passed in request correspond to what our\n   API expects at the given microversion.\n\nThese decorators need to be used in this exact order, to preserve the\nprevious order of checks in the API.\n\nThe unittest to check that the exposed methods contain the\npolicy.authorize call is removed. Instead, we assert this during the\nAPI startup.\n\nThe decorator module is added as a dependency, as it is the only one\nthat is able to preserve the funtion signature in python 2. We need\nthis so that wsme's expose continues to work properly.\n\nChange-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1\n""}]",4,451506,d1661f2cfbc48c5e5e6750c439f0a0f459f78159,45,7,12,12356,,,0,"Refactor API policy and microversion checks

This change introduces the following decorators:

1. policy_checker - checks the policy of the method using the policy
   string passed as a parameter;

2. <resource>_is_subcontroller - checks if the endpoint is being
   accessed through subcontroller, meaning some operations are not
   allowed;

3. api_version_checker - checks that the endpoint itself, the request
   body and the arguments passed in request correspond to what our
   API expects at the given microversion.

These decorators need to be used in this exact order, to preserve the
previous order of checks in the API.

The unittest to check that the exposed methods contain the
policy.authorize call is removed. Instead, we assert this during the
API startup.

The decorator module is added as a dependency, as it is the only one
that is able to preserve the funtion signature in python 2. We need
this so that wsme's expose continues to work properly.

Change-Id: I33d3362d72c1e5880d947f64754ce90d6b0e9fc1
",git fetch https://review.opendev.org/openstack/ironic refs/changes/06/451506/9 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/tests/unit/api/utils.py', 'ironic/api/controllers/v1/portgroup.py', 'ironic/api/controllers/v1/utils.py', 'ironic/api/controllers/v1/node.py', 'ironic/tests/unit/api/v1/test_expose.py']",5,369ea3b9119a9340671dd945cd78b45185557819,api-refactor,"class _TestExposedAPIMethodsCheckPolicy(test_base.TestCase): super(_TestExposedAPIMethodsCheckPolicy, self).setUp() #def test_chasis_api_policy(self): # self._test('ironic.api.controllers.v1.chassis') #def test_driver_api_policy(self): # self._test('ironic.api.controllers.v1.driver') #def test_node_api_policy(self): # self._test('ironic.api.controllers.v1.node') #def test_port_api_policy(self): # self._test('ironic.api.controllers.v1.port') #def test_portgroup_api_policy(self): # self._test('ironic.api.controllers.v1.portgroup') #def test_ramdisk_api_policy(self): # self._test('ironic.api.controllers.v1.ramdisk')","class TestExposedAPIMethodsCheckPolicy(test_base.TestCase): super(TestExposedAPIMethodsCheckPolicy, self).setUp() def test_chasis_api_policy(self): self._test('ironic.api.controllers.v1.chassis') def test_driver_api_policy(self): self._test('ironic.api.controllers.v1.driver') def test_node_api_policy(self): self._test('ironic.api.controllers.v1.node') def test_port_api_policy(self): self._test('ironic.api.controllers.v1.port') def test_portgroup_api_policy(self): self._test('ironic.api.controllers.v1.portgroup') def test_ramdisk_api_policy(self): self._test('ironic.api.controllers.v1.ramdisk')",160,103
openstack%2Fheat-translator~master~Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34,openstack/heat-translator,master,Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34,Implement Heat Translator Server,NEW,2016-08-25 14:40:27.000000000,2017-12-18 03:15:36.000000000,,"[{'_account_id': 91}, {'_account_id': 6456}, {'_account_id': 10487}, {'_account_id': 12455}, {'_account_id': 13380}, {'_account_id': 16059}, {'_account_id': 16511}]","[{'number': 1, 'created': '2016-08-25 14:40:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/cc8e466f34905493c8593d1d9112f33541ff09e8', 'message': 'Implement Heat Translator Server\n\nimplements blueprint: heat-translator-as-a-service\n\nChange-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34\n'}, {'number': 2, 'created': '2016-08-25 14:41:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/c1b4d162d5275c60d73408dfb7d1308534537fe7', 'message': '[WIP]Implement Heat Translator Server\n\nimplements blueprint: heat-translator-as-a-service\n\nChange-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34\n'}, {'number': 3, 'created': '2016-09-11 17:38:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/4df5b3f76d509ebecd9f185f176ff521b6ddd1a1', 'message': '[WIP]Implement Heat Translator Server\n\nimplements blueprint: heat-translator-as-a-service\n\nChange-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34\n'}, {'number': 4, 'created': '2016-09-13 15:18:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/1b4bfaa71d48510d44e3f78fc253c1c24e9e2a2d', 'message': 'Implement Heat Translator Server\n\nimplements blueprint: heat-translator-as-a-service\n\nChange-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34\n'}, {'number': 5, 'created': '2016-11-20 17:01:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/3c5d961131172dcf64afdb04487df8a4788de890', 'message': 'Implement Heat Translator Server\n\nimplements blueprint: heat-translator-as-a-service\n\nChange-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34\n'}, {'number': 6, 'created': '2016-11-20 17:14:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/10cc7220f743ecd8cbef2eef980fa2c542d00332', 'message': 'Implement Heat Translator Server\n\nimplements blueprint: heat-translator-as-a-service\n\nChange-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34\n'}, {'number': 7, 'created': '2016-11-22 20:54:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/c09a3a6131450e66e056f667cd725de7317d7c13', 'message': 'Implement Heat Translator Server\n\nimplements blueprint: heat-translator-as-a-service\n\nChange-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34\n'}, {'number': 8, 'created': '2016-11-23 09:57:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/1265d1f24dda318f72210d39535f8c922514515b', 'message': 'Implement Heat Translator Server\n\nimplements blueprint: heat-translator-as-a-service\n\nChange-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34\n'}, {'number': 9, 'created': '2016-11-27 09:27:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/bba72221924d5c7dc0bf10e85c01c9224519a1a5', 'message': 'Implement Heat Translator Server\n\nimplements blueprint: heat-translator-as-a-service\n\nChange-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34\n'}, {'number': 10, 'created': '2016-12-20 15:09:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/a4647ae5e0e62728dd68c42d92962183614ff759', 'message': 'Implement Heat Translator Server\n\nimplements blueprint: heat-translator-as-a-service\n\nChange-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34\n'}, {'number': 11, 'created': '2016-12-23 17:57:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/a1f083d3827891cf200449fa36287a74a1b86e3e', 'message': 'Implement Heat Translator Server\n\nimplements blueprint: heat-translator-as-a-service\n\nChange-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34\n'}, {'number': 12, 'created': '2017-06-12 16:24:23.000000000', 'files': ['translator/tests/api/__init__.py', 'translator/conf/heat_translator_logging.conf', 'translator/service.py', 'doc/source/usage.rst', 'translator/shell.py', 'translator/tests/api/controllers/__init__.py', 'translator/tests/api/base.py', 'translator/tests/api/controllers/test_root.py', 'translator/middleware/parsable_error.py', 'requirements.txt', 'etc/translator/api-paste.ini', 'translator/controllers.py', 'translator/middleware/__init__.py', 'setup.cfg', 'translator/config.py', 'translator/app.py'], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/44c611e392ebbe7fdceeda2dea093f4f7fd7133f', 'message': 'Implement Heat Translator Server\n\nImplements: blueprint heat-translator-as-a-service\n\nCo-Authored-By: Bob Haddleton <bob.haddleton@nokia.com>\nChange-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34\n'}]",9,360603,44c611e392ebbe7fdceeda2dea093f4f7fd7133f,33,7,12,12455,,,0,"Implement Heat Translator Server

Implements: blueprint heat-translator-as-a-service

Co-Authored-By: Bob Haddleton <bob.haddleton@nokia.com>
Change-Id: Ia4b0feec9600977b6f7fe7fce3c760c75cd4ee34
",git fetch https://review.opendev.org/openstack/heat-translator refs/changes/03/360603/12 && git format-patch -1 --stdout FETCH_HEAD,"['translator/middleware/parsable_error.py', 'translator/controllers.py', 'translator/wsgi.py', 'translator/middleware/__init__.py', 'translator/shell.py', 'translator/config.py']",6,cc8e466f34905493c8593d1d9112f33541ff09e8,bp/heat-translator-as-a-service,"# Copyright 2013 - Noorul Islam K M # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # Pecan Application Configurations app = { 'root': 'translator.controllers.RootController', 'modules': ['translator'], 'debug': True, } ",,263,3
openstack%2Fswift~master~I961a5c150d5707a0fe330903b5c5eb2e5b892d3f,openstack/swift,master,I961a5c150d5707a0fe330903b5c5eb2e5b892d3f,Add a space to pass the check of pep8,NEW,2016-12-21 01:28:59.000000000,2017-12-18 03:15:05.000000000,,"[{'_account_id': 13052}, {'_account_id': 18602}, {'_account_id': 25254}]","[{'number': 1, 'created': '2016-12-21 01:28:59.000000000', 'files': ['api-ref/source/conf.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/f9acfefc8fc5db4e6ecbd2b24608ba5aa7c2ca4d', 'message': 'Add a space to pass the check of pep8\n\nAdd a space to pass the check of pep8\n\nChange-Id: I961a5c150d5707a0fe330903b5c5eb2e5b892d3f\n'}]",0,413322,f9acfefc8fc5db4e6ecbd2b24608ba5aa7c2ca4d,6,3,1,24061,,,0,"Add a space to pass the check of pep8

Add a space to pass the check of pep8

Change-Id: I961a5c150d5707a0fe330903b5c5eb2e5b892d3f
",git fetch https://review.opendev.org/openstack/swift refs/changes/22/413322/1 && git format-patch -1 --stdout FETCH_HEAD,['api-ref/source/conf.py'],1,f9acfefc8fc5db4e6ecbd2b24608ba5aa7c2ca4d,pep8_check,# html_static_path = ['_static'],#html_static_path = ['_static'],1,1
openstack%2Fironic~master~I9c97baca8e24d766fb1da2b47ee965858b4ff7ea,openstack/ironic,master,I9c97baca8e24d766fb1da2b47ee965858b4ff7ea,Adds TLS-proxy protect,NEW,2017-03-07 11:37:47.000000000,2017-12-18 03:14:04.000000000,,"[{'_account_id': 10118}, {'_account_id': 11655}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 19686}, {'_account_id': 23330}]","[{'number': 1, 'created': '2017-03-07 11:37:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e37748ee9cb3968b7eede30f4ef6d98f0dc9bf45', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 2, 'created': '2017-03-07 11:45:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5bab4bfdf448df0043dab22bb44c0e09c766eba8', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 3, 'created': '2017-03-07 15:11:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fcd4034ab4be3aacfe24d0ef89f391e974714b51', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 4, 'created': '2017-03-10 13:01:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ddb6d9b525c5280a232d3e4a07144fac705fb96e', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 5, 'created': '2017-03-10 13:41:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/139e6d13652b6a3b944028548ced05ae8b03812b', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 6, 'created': '2017-03-10 16:14:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a104be1fbdcc6f97b619d10f39a1b0cccb338e07', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 7, 'created': '2017-03-27 11:02:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/20151ec1f3aba31118de4963a9840a588de3fadd', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 8, 'created': '2017-03-27 14:27:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/01de646de7761552d1e251f103a79116ab221ca1', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 9, 'created': '2017-03-28 10:24:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/4d352a19400cc0a388857469718ce3b55391bd9d', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 10, 'created': '2017-03-28 11:16:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/36d2a3ce8265b8ed35ff53952508dbc6c7b580e1', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 11, 'created': '2017-03-29 10:20:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7decb6d696822f60962b9722a091622a35d77adb', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 12, 'created': '2017-03-29 10:46:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e2500faa7d90e9fdfecb316f7213f1f7a3fc4c99', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 13, 'created': '2017-03-30 11:28:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/25265d3459dd51c7bd23a73cdf37b5fbeb735f9b', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 14, 'created': '2017-03-30 11:33:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/23a90c9dd80be20e9fb7a1a552ea474d681378db', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 15, 'created': '2017-03-30 14:04:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c3562f2bbafc9e8c87e6da1bdd04c8f7dd1256bb', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 16, 'created': '2017-03-31 09:20:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/764fd61699f5d091077f1b1354c2165632bb8699', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nGate-change\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nDevstack-changes\nDepends-On: Ieb405e2fae1d9fd69192694d6654966a9cab2631\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 17, 'created': '2017-04-03 09:58:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/dca950ab33d6d44248676d44038c1bee4c68c737', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nGate-change\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nDevstack-changes\nDepends-On: Ieb405e2fae1d9fd69192694d6654966a9cab2631\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 18, 'created': '2017-04-03 10:59:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b8e555e9f1a4745a7b0717110429e3d9505f3fac', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nGate-change\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nDevstack-changes\nDepends-On: Ieb405e2fae1d9fd69192694d6654966a9cab2631\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 19, 'created': '2017-04-03 15:33:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c3ec1f8c6ce8216561f88830a870f880c02b7139', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nGate-change\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nDevstack-changes\nDepends-On: Ieb405e2fae1d9fd69192694d6654966a9cab2631\n\nIPA-changes:\nDepends-On: Iba1fc89fc23733572cef50905db3d83ad05f688f\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 20, 'created': '2017-04-04 14:26:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9aace0fab86be8c51952af20a6de1a9dcb8a1315', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nGate-change\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nDevstack-changes\nDepends-On: Ieb405e2fae1d9fd69192694d6654966a9cab2631\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 21, 'created': '2017-04-04 15:44:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d3977a39da39c8825670c225a4c07eb278700ea1', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nGate-change\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nDevstack-changes\nDepends-On: Ieb405e2fae1d9fd69192694d6654966a9cab2631\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 22, 'created': '2017-04-04 15:52:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ca1fb1c02691762e260bc418922ee7aca8b4872d', 'message': ""WIP: SSL TEST\n\nPlease don't reviev it's test\n\nGate-change\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nDevstack-changes\nDepends-On: Ieb405e2fae1d9fd69192694d6654966a9cab2631\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n""}, {'number': 23, 'created': '2017-05-02 20:52:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a10dc82dbc2d637185104322687902bffb07b5d5', 'message': 'Ads TLS-proxy protect\n\nThis patch uses tls-proxy protect in devstack\n\nGate-change\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nDevstack-changes\nDepends-On: Ieb405e2fae1d9fd69192694d6654966a9cab2631\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n'}, {'number': 24, 'created': '2017-05-02 21:17:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2530117ff71bd3d61840d8beb0ee16399db8d614', 'message': 'Ads TLS-proxy protect\n\nThis patch uses tls-proxy protect in devstack\n\nGate-change\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nDevstack-changes\nDepends-On: Ieb405e2fae1d9fd69192694d6654966a9cab2631\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n'}, {'number': 25, 'created': '2017-05-02 21:23:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fd40560b2bf5fbe438eecc16778be82c6606097e', 'message': 'Ads TLS-proxy protect\n\nThis patch uses tls-proxy protect in devstack\n\nGate-change\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n'}, {'number': 26, 'created': '2017-05-03 13:47:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e8af7f1a1466daaf86ccf31815d6978415d4d009', 'message': 'Adds TLS-proxy protect\n\nThis patch uses tls-proxy protect in devstack\n\nGate-change\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n'}, {'number': 27, 'created': '2017-05-26 14:13:13.000000000', 'files': ['devstack/lib/ironic'], 'web_link': 'https://opendev.org/openstack/ironic/commit/d0957d7241d7d2330ad4ce0d5a9e680df8ac35ba', 'message': 'Adds TLS-proxy protect\n\nThis patch uses tls-proxy protect in devstack\n\nGate-change\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nChange-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea\n'}]",1,442402,d0957d7241d7d2330ad4ce0d5a9e680df8ac35ba,118,8,27,23330,,,0,"Adds TLS-proxy protect

This patch uses tls-proxy protect in devstack

Gate-change
Depends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e

Change-Id: I9c97baca8e24d766fb1da2b47ee965858b4ff7ea
",git fetch https://review.opendev.org/openstack/ironic refs/changes/02/442402/13 && git format-patch -1 --stdout FETCH_HEAD,"['devstack/files/apache-ipxe-ironic.template', 'devstack/lib/ironic', 'requirements.txt', 'devstack/files/apache-ironic.template']",4,e37748ee9cb3968b7eede30f4ef6d98f0dc9bf45,ssl_dev,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # This is an example Apache2 configuration file for using the # Ironic API through mod_wsgi. This version assumes you are # running devstack to configure the software. Listen %IRONIC_SERVICE_PORT% <VirtualHost *:%IRONIC_SERVICE_PORT%> WSGIDaemonProcess ironic user=%USER% processes=%APIWORKERS% threads=%APIWORKERS% display-name=%{GROUP} WSGIScriptAlias / %IRONIC_WSGI_DIR%/app.wsgi WSGIApplicationGroup %{GLOBAL} WSGIProcessGroup ironic %SSLENGINE% %SSLCERTFILE% %SSLKEYFILE% ErrorLog /var/log/apache2/ironic_error.log LogLevel info CustomLog /var/log/apache2/ironic_access.log combined <Directory %IRONIC_WSGI_DIR%> WSGIProcessGroup ironic WSGIApplicationGroup %{GLOBAL} <IfVersion >= 2.4> Require all granted </IfVersion> <IfVersion < 2.4> Order allow,deny Allow from all </IfVersion>","Listen %PUBLICPORT% <VirtualHost *:%PUBLICPORT%> DocumentRoot ""%HTTPROOT%"" <Directory ""%HTTPROOT%""> Options Indexes FollowSymLinks AllowOverride None Order allow,deny Allow from all Require all granted",85,13
openstack%2Fblazar~master~I77003768a793031433ef13b6d5f7237cb83b1915,openstack/blazar,master,I77003768a793031433ef13b6d5f7237cb83b1915,Updated from global requirements,MERGED,2017-12-15 21:22:21.000000000,2017-12-18 03:13:25.000000000,2017-12-18 03:13:25.000000000,"[{'_account_id': 15197}, {'_account_id': 22348}, {'_account_id': 23840}]","[{'number': 1, 'created': '2017-12-15 21:22:21.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/blazar/commit/b93cc9ba4b2aa599529a3a2989ad756b03e6f5cc', 'message': 'Updated from global requirements\n\nChange-Id: I77003768a793031433ef13b6d5f7237cb83b1915\n'}]",0,528394,b93cc9ba4b2aa599529a3a2989ad756b03e6f5cc,7,3,1,11131,,,0,"Updated from global requirements

Change-Id: I77003768a793031433ef13b6d5f7237cb83b1915
",git fetch https://review.opendev.org/openstack/blazar refs/changes/94/528394/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,b93cc9ba4b2aa599529a3a2989ad756b03e6f5cc,openstack/requirements,"oslo.service!=1.28.1,>=1.24.0 # Apache-2.0",oslo.service>=1.24.0 # Apache-2.0,1,1
openstack%2Fironic~master~I7efcc8aef3767c482eb8f4774a7744cb1bdccaa9,openstack/ironic,master,I7efcc8aef3767c482eb8f4774a7744cb1bdccaa9,[WIP] Make storage calls agnostic to deploy method.,NEW,2017-06-22 15:28:50.000000000,2017-12-18 03:12:18.000000000,,"[{'_account_id': 10118}, {'_account_id': 11655}, {'_account_id': 11929}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19339}]","[{'number': 1, 'created': '2017-06-22 15:28:50.000000000', 'files': ['ironic/drivers/modules/agent.py', 'ironic/conductor/manager.py', 'ironic/drivers/modules/iscsi_deploy.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/tests/unit/drivers/modules/test_iscsi_deploy.py', 'ironic/tests/unit/drivers/modules/test_agent.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/be28aaf93cdfb327536382ed2ea39919781986cf', 'message': ""[WIP] Make storage calls agnostic to deploy method.\n\nCurrently we are handling some storage operations in the deploy\ndriver. This patch moves storage calls out of the drivers\nand into the manager.\n\nWIP as I haven't moved all the tests over.\n\nChange-Id: I7efcc8aef3767c482eb8f4774a7744cb1bdccaa9\nPartial-Bug: #1559691\nRelated-Bug: #1679834\n""}]",2,476601,be28aaf93cdfb327536382ed2ea39919781986cf,10,6,1,11929,,,0,"[WIP] Make storage calls agnostic to deploy method.

Currently we are handling some storage operations in the deploy
driver. This patch moves storage calls out of the drivers
and into the manager.

WIP as I haven't moved all the tests over.

Change-Id: I7efcc8aef3767c482eb8f4774a7744cb1bdccaa9
Partial-Bug: #1559691
Related-Bug: #1679834
",git fetch https://review.opendev.org/openstack/ironic refs/changes/01/476601/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/drivers/modules/agent.py', 'ironic/conductor/manager.py', 'ironic/drivers/modules/iscsi_deploy.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/tests/unit/drivers/modules/test_iscsi_deploy.py', 'ironic/tests/unit/drivers/modules/test_agent.py']",6,be28aaf93cdfb327536382ed2ea39919781986cf,bug/1559691, unconfigure_tenant_nets_mock): pxe_prepare_ramdisk_mock): add_provisioning_net_mock):," @mock.patch.object(noop_storage.NoopStorage, 'detach_volumes', autospec=True) unconfigure_tenant_nets_mock, storage_detach_volumes_mock): storage_detach_volumes_mock.assert_called_once_with( task.driver.storage, task) # Verify no volumes exist for new task instances. with task_manager.acquire( self.context, self.node['uuid'], shared=False) as task: self.assertEqual(0, len(task.volume_targets)) @mock.patch.object(noop_storage.NoopStorage, 'attach_volumes', autospec=True) @mock.patch.object(deploy_utils, 'populate_storage_driver_internal_info') pxe_prepare_ramdisk_mock, storage_driver_info_mock, storage_attach_volumes_mock): storage_driver_info_mock.assert_called_once_with(task) storage_attach_volumes_mock.assert_called_once_with( task.driver.storage, task) @mock.patch.object(noop_storage.NoopStorage, 'attach_volumes', autospec=True) @mock.patch.object(deploy_utils, 'populate_storage_driver_internal_info') add_provisioning_net_mock, storage_driver_info_mock, storage_attach_volumes_mock): self.assertTrue(storage_driver_info_mock.called) self.assertFalse(storage_attach_volumes_mock.called)",21,63
openstack%2Fmolteniron~master~If68d335493f37e33d72a0ded1d3c05efa9795ca1,openstack/molteniron,master,If68d335493f37e33d72a0ded1d3c05efa9795ca1,Add a culling thread to molteniron,ABANDONED,2017-02-28 14:39:43.000000000,2017-12-18 03:08:46.000000000,,"[{'_account_id': 5660}, {'_account_id': 11929}, {'_account_id': 18242}]","[{'number': 1, 'created': '2017-02-28 14:39:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/molteniron/commit/f03e3483b8d7f5975434c50450653192d8a9171a', 'message': 'Add a culling thread to molteniron\n\nThe culling thread will sleep for --polling-seconds and deallocate\nnodes older than maxTime defined in conf.yaml.\n\nChange-Id: If68d335493f37e33d72a0ded1d3c05efa9795ca1\nCloses-Bug: 1639960\n'}, {'number': 2, 'created': '2017-02-28 16:32:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/molteniron/commit/896bffc50ba5cdf29dd55692325ff7248bbc2d28', 'message': 'Add a culling thread to molteniron\n\nThe culling thread will sleep for --polling-seconds and deallocate\nnodes older than maxTime defined in conf.yaml.\n\nChange-Id: If68d335493f37e33d72a0ded1d3c05efa9795ca1\nCloses-Bug: 1639960\n'}, {'number': 3, 'created': '2017-03-03 22:26:04.000000000', 'files': ['molteniron/molteniron.py', 'molteniron/moltenirond-culler', 'molteniron/moltenirond.py', 'README.rst', 'setup.py'], 'web_link': 'https://opendev.org/openstack/molteniron/commit/130a89c58387c701ffb207374d008e7c7a766de4', 'message': 'Add a culling thread to molteniron\n\nThe culling thread will sleep for --polling-seconds and deallocate\nnodes older than maxTime defined in conf.yaml.\n\nChange-Id: If68d335493f37e33d72a0ded1d3c05efa9795ca1\nCloses-Bug: 1639960\n'}]",7,438980,130a89c58387c701ffb207374d008e7c7a766de4,14,3,3,18242,,,0,"Add a culling thread to molteniron

The culling thread will sleep for --polling-seconds and deallocate
nodes older than maxTime defined in conf.yaml.

Change-Id: If68d335493f37e33d72a0ded1d3c05efa9795ca1
Closes-Bug: 1639960
",git fetch https://review.opendev.org/openstack/molteniron refs/changes/80/438980/3 && git format-patch -1 --stdout FETCH_HEAD,"['molteniron/molteniron.py', 'molteniron/moltenirond.py', 'molteniron/culler.py']",3,f03e3483b8d7f5975434c50450653192d8a9171a,bug/1639960,"#!/usr/bin/env python """""" This is the MoltenIron culling thread. """""" # Copyright (c) 2017 IBM Corporation. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. # flake8 disabling E242 # https://pep8.readthedocs.io/en/latest/intro.html # https://gitlab.com/pycqa/flake8/issues/63 # Gah! # pylint: disable-msg=C0103 # pylint: disable=redefined-outer-name from __future__ import print_function import argparse from daemonize import Daemonize from molteniron import molteniron import os import signal import sys import threading import time import yaml DEBUG = False DAEMONIZE = True POLLING_SECONDS = 60 YAML_CONF = ""/usr/local/etc/molteniron/conf.yaml"" PID = ""/var/run/culler.pid"" STOP_FLAG = threading.Event() class CullingThread(threading.Thread): """"""Thread responsible for periodically running a cull operation"""""" def __init__(self, event, yaml_conf, polling_seconds): """"""Handles thread initialization"""""" threading.Thread.__init__(self) self.stopped = event self.mi = molteniron.MoltenIron() conf_parts = yaml_conf.split(""/"")[:-1] self.conf_dir = ""/"".join(conf_parts) with open(yaml_conf, ""r"") as fobj: conf = yaml.load(fobj) self.maxSeconds = conf[""maxTime""] self.mi.setup_conf(conf) self.polling_seconds = polling_seconds def run(self): """"""Called when it is time to run"""""" while not self.stopped.wait(self.polling_seconds): if DEBUG: print(""CullingThread:run:loop(%d)"" % (self.polling_seconds, )) # For example: # args_map = {""output"": ""json"", # ""type"": ""human"", # ""func"": getattr(self.mi, ""status""), # ""conf_dir"": ""testenv/etc/molteniron/""} args_map = { ""maxSeconds"": self.maxSeconds, ""func"": getattr(self.mi, ""cull""), ""conf_dir"": self.conf_dir } # Call the function self.mi.call_function(args_map) # Get the result response_map = self.mi.get_response_map() try: rc = response_map[""status""] except KeyError: msg = (""Error: Server returned: %s and we expected a status "" + ""somewhere"") % (response_map, ) print(msg, file=sys.stderr) exit(444) if rc != 200: msg = ""Error: Status was not 200 %s"" % ( response_map[""message""], ) print(msg, file=sys.stderr) exit(rc) if DEBUG: print(""CullingThread:run:exit"") def signal_handler(signal, frame): """"""When a SIGINT or SIGUSR1 is caught, trip our stop event"""""" if DEBUG: print(""signal_handler (%s)"" % (signal, )) STOP_FLAG.set() def molteniron_culling(): """"""This is the main routine for the MoltenIron culling thread."""""" # This is meant to be started from another program but support running # from main as well as a daemon t = CullingThread(STOP_FLAG, YAML_CONF, POLLING_SECONDS) t.start() while not STOP_FLAG.is_set(): time.sleep(60) if DEBUG: print(""molteniron_culling:exit"") if __name__ == ""__main__"": parser = argparse.ArgumentParser(description=""Molteniron culler"") parser.add_argument(""-c"", ""--conf-dir"", action=""store"", type=str, dest=""conf_dir"", help=""The directory where configuration is stored"") parser.add_argument(""-s"", ""--polling-seconds"", action=""store"", type=str, dest=""polling_seconds"", help=""The amount of seconds to sleep before a poll"") parser.add_argument(""-p"", ""--pid-dir"", action=""store"", type=str, dest=""pid_dir"", help=""The directory where PID information is stored"") args = parser.parse_args() if args.polling_seconds: POLLING_SECONDS = int(args.polling_seconds) if args.conf_dir: if not os.path.isdir(args.conf_dir): msg = ""Error: %s is not a valid directory"" % (args.conf_dir, ) print(msg, file=sys.stderr) sys.exit(1) YAML_CONF = os.path.realpath(""%s/conf.yaml"" % (args.conf_dir, )) if args.pid_dir: if not os.path.isdir(args.pid_dir): msg = ""Error: %s is not a valid directory"" % (args.pid_dir, ) print(msg, file=sys.stderr) sys.exit(1) PID = os.path.realpath(""%s/culler.pid"" % (args.pid_dir, )) signal.signal(signal.SIGINT, signal_handler) signal.signal(signal.SIGUSR1, signal_handler) if DAEMONIZE: daemon = Daemonize(app=""molteniron_culling"", pid=PID, action=molteniron_culling) daemon.start() else: molteniron_culling() ",,223,9
openstack%2Fmagnum~master~I746848578982e523c89c659a356eca6752d6bb37,openstack/magnum,master,I746848578982e523c89c659a356eca6752d6bb37,add a ft test for cinder volume,ABANDONED,2016-10-25 07:48:25.000000000,2017-12-18 03:05:30.000000000,,"[{'_account_id': 668}, {'_account_id': 10206}, {'_account_id': 13861}, {'_account_id': 23365}, {'_account_id': 23419}]","[{'number': 1, 'created': '2016-10-25 07:48:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/6b4d1efa725bd9ab57217d251db594de20bd6c12', 'message': 'Blueprints/k8s use cinder volume function test\n\nChange-Id: I746848578982e523c89c659a356eca6752d6bb37\n'}, {'number': 2, 'created': '2016-10-25 09:12:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/0157695db12a1342c26f35426442c58024f019a3', 'message': 'Blueprints/k8s use cinder volume function test\n\nChange-Id: I746848578982e523c89c659a356eca6752d6bb37\n'}, {'number': 3, 'created': '2016-12-03 01:54:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/e9892936452c15a8121ba932dd82cd4daa09c9d8', 'message': 'add a ft test\n\nA ft test for the function of using cinder volume\n\nImplements: blueprint k8s use cinder volume function test\n\nChange-Id: I746848578982e523c89c659a356eca6752d6bb37'}, {'number': 4, 'created': '2016-12-08 07:50:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/b1f0afed5e5572b40a59839a22db804d7b215a09', 'message': 'add a ft test for cinder volume\n\nA ft test for the function of using cinder volume\n\nImplements: blueprint k8s use cinder volume function test\n\nChange-Id: I746848578982e523c89c659a356eca6752d6bb37'}, {'number': 5, 'created': '2016-12-16 06:47:26.000000000', 'files': ['magnum/tests/functional/python_client_base.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/4bab9cef063d1c391b40358f9e2b6e3e567e86f0', 'message': 'add a ft test for cinder volume\n\nA ft test for the function of using cinder volume and coe is k8s\n\nImplements: blueprint k8s-use-cinder-volume-function-test\n\nChange-Id: I746848578982e523c89c659a356eca6752d6bb37'}]",7,390464,4bab9cef063d1c391b40358f9e2b6e3e567e86f0,25,5,5,23419,,,0,"add a ft test for cinder volume

A ft test for the function of using cinder volume and coe is k8s

Implements: blueprint k8s-use-cinder-volume-function-test

Change-Id: I746848578982e523c89c659a356eca6752d6bb37",git fetch https://review.opendev.org/openstack/magnum refs/changes/64/390464/1 && git format-patch -1 --stdout FETCH_HEAD,['magnum/tests/functional/python_client_base.py'],1,6b4d1efa725bd9ab57217d251db594de20bd6c12,bp/s," def test_cinder_pod(self): os.popen("". /root/keystonerc_admin && \ cinder create --display-name=cinder-volume 1"") pod_manifest = { 'apiVersion': 'v1', 'kind': 'Pod', 'metadata': {'name': 'testcinder'}, 'spec': {'containers': [{'image': 'dockerfile/redis', 'name': 'redis'}], 'volumes': [{'name': 'html-volume', 'cinder': {'volumeID': 'cinder-volume', 'fsType': 'ext4'}}]}} resp = self.k8s_api.create_namespaced_pod(body=pod_manifest, namespace='default') time.sleep(60) resp = self.k8s_api.read_namespaced_pod(name='testcinder', namespace='default') self.assertEqual('Running', resp.status.phase) resp = self.k8s_api.delete_namespaced_pod(name='testcinder', body={}, namespace='default') time.sleep(30) os.system("". /root/keystonerc_admin && cinder delete cinder-volume"")",,23,0
openstack%2Fproject-config~master~Ib65ecf5c56c131717431eb76c8f9724a1a1c4853,openstack/project-config,master,Ib65ecf5c56c131717431eb76c8f9724a1a1c4853,Add myself to accessbot,MERGED,2017-12-17 23:16:47.000000000,2017-12-18 03:01:05.000000000,2017-12-18 03:01:04.000000000,"[{'_account_id': 5263}, {'_account_id': 9061}, {'_account_id': 22348}, {'_account_id': 25632}]","[{'number': 1, 'created': '2017-12-17 23:16:47.000000000', 'files': ['accessbot/channels.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/f2411a209078ecd614269b7143343dde40deb648', 'message': ""Add myself to accessbot\n\nGiven tz's, sometimes it might be handy for me to be able to kick for\noccasional spam etc in irc.  Also sort alphabetically just because\nthat seems like the way it was.\n\nChange-Id: Ib65ecf5c56c131717431eb76c8f9724a1a1c4853\n""}]",0,528589,f2411a209078ecd614269b7143343dde40deb648,8,4,1,7118,,,0,"Add myself to accessbot

Given tz's, sometimes it might be handy for me to be able to kick for
occasional spam etc in irc.  Also sort alphabetically just because
that seems like the way it was.

Change-Id: Ib65ecf5c56c131717431eb76c8f9724a1a1c4853
",git fetch https://review.opendev.org/openstack/project-config refs/changes/89/528589/1 && git format-patch -1 --stdout FETCH_HEAD,['accessbot/channels.yaml'],1,f2411a209078ecd614269b7143343dde40deb648,ianw-irc, - dmsimard - frickler - ianw, - frickler - dmsimard,3,2
openstack%2Fpython-aodhclient~master~If324de0b444ae04f3798e2eb72c3ae8b50e6bc91,openstack/python-aodhclient,master,If324de0b444ae04f3798e2eb72c3ae8b50e6bc91,Pass environment variables of proxy to tox,NEW,2017-06-27 09:55:27.000000000,2017-12-18 02:58:37.000000000,,[{'_account_id': 1669}],"[{'number': 1, 'created': '2017-06-27 09:55:27.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/python-aodhclient/commit/278210403e4b7059b37839cd619bc795914a3864', 'message': 'Pass environment variables of proxy to tox\n\nWhen development environment is under proxy,tox is failed even if\nenvironment variables of the proxy are set.\n\nChange-Id: If324de0b444ae04f3798e2eb72c3ae8b50e6bc91\n'}]",1,477859,278210403e4b7059b37839cd619bc795914a3864,4,1,1,25005,,,0,"Pass environment variables of proxy to tox

When development environment is under proxy,tox is failed even if
environment variables of the proxy are set.

Change-Id: If324de0b444ae04f3798e2eb72c3ae8b50e6bc91
",git fetch https://review.opendev.org/openstack/python-aodhclient refs/changes/59/477859/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,278210403e4b7059b37839cd619bc795914a3864,tox, http_proxy HTTP_PROXY https_proxy HTTPS_PROXY no_proxy NO_PROXY,,1,0
openstack%2Fswift~master~I8f26118d83aaa7de98e966cd14d9ae554b4c4819,openstack/swift,master,I8f26118d83aaa7de98e966cd14d9ae554b4c4819,Add abort flag to replicator/reconstructor,NEW,2017-02-03 19:51:47.000000000,2017-12-18 02:58:09.000000000,,"[{'_account_id': 1179}, {'_account_id': 6968}, {'_account_id': 13052}, {'_account_id': 15343}]","[{'number': 1, 'created': '2017-02-03 19:51:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/246410513b7a94c11d00011633931401b04cc7eb', 'message': 'Add Timeouts to spawn()s in replicator/reconstructor\n\nWe do it for the waitall() at the end, but those spawn()s can block, too.\n\nChange-Id: I8f26118d83aaa7de98e966cd14d9ae554b4c4819\n'}, {'number': 2, 'created': '2017-02-03 22:08:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/99a645b4ea22ab2da1f5467e33410b52644de5e0', 'message': 'Add Timeouts to spawn()s in replicator/reconstructor\n\nWe do it for the waitall() at the end, but those spawn()s can block, too.\n\nChange-Id: I8f26118d83aaa7de98e966cd14d9ae554b4c4819\n'}, {'number': 3, 'created': '2017-02-06 21:31:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/78457a27ec5dbd067503772d9dda5c30150f7334', 'message': 'Add abort flag to replicator/reconstructor\n\nCurrently, when the lockup_detector goes and kills all the co-routines,\nthe replicator/reconstructor keeps spawning jobs. This seems counter to\nthe idea behind the lockup_timeout; we want to just abort the run and\ntry again later.\n\nChange-Id: I8f26118d83aaa7de98e966cd14d9ae554b4c4819\n'}, {'number': 4, 'created': '2017-02-06 22:41:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4c013f6d495e489d938016dfbc3ec3869163d917', 'message': 'Add abort flag to replicator/reconstructor\n\nCurrently, when the lockup_detector goes and kills all the co-routines,\nthe replicator/reconstructor keeps spawning jobs. This seems counter to\nthe idea behind the lockup_timeout; we want to just abort the run and\ntry again later.\n\nChange-Id: I8f26118d83aaa7de98e966cd14d9ae554b4c4819\n'}, {'number': 5, 'created': '2017-02-21 19:15:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ffa66ab45d28f613544f579762a663300bf6bc7c', 'message': 'Add abort flag to replicator/reconstructor\n\nCurrently, when the lockup_detector goes and kills all the co-routines,\nthe replicator/reconstructor keeps spawning jobs. This seems counter to\nthe idea behind the lockup_timeout; we want to just abort the run and\ntry again later.\n\nChange-Id: I8f26118d83aaa7de98e966cd14d9ae554b4c4819\n'}, {'number': 6, 'created': '2017-02-23 01:57:22.000000000', 'files': ['swift/obj/replicator.py', 'swift/obj/reconstructor.py', 'test/unit/obj/test_replicator.py', 'test/unit/obj/test_reconstructor.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/6f85d635b1ba7ecacf07df472140d303dfa0cb37', 'message': 'Add abort flag to replicator/reconstructor\n\nCurrently, when the lockup_detector goes and kills all the co-routines,\nthe replicator/reconstructor keeps spawning jobs. This seems counter to\nthe idea behind the lockup_timeout; we want to just abort the run and\ntry again later.\n\nChange-Id: I8f26118d83aaa7de98e966cd14d9ae554b4c4819\n'}]",3,429015,6f85d635b1ba7ecacf07df472140d303dfa0cb37,26,4,6,15343,,,0,"Add abort flag to replicator/reconstructor

Currently, when the lockup_detector goes and kills all the co-routines,
the replicator/reconstructor keeps spawning jobs. This seems counter to
the idea behind the lockup_timeout; we want to just abort the run and
try again later.

Change-Id: I8f26118d83aaa7de98e966cd14d9ae554b4c4819
",git fetch https://review.opendev.org/openstack/swift refs/changes/15/429015/1 && git format-patch -1 --stdout FETCH_HEAD,"['swift/obj/replicator.py', 'swift/obj/reconstructor.py', 'test/unit/obj/test_replicator.py', 'test/unit/obj/test_reconstructor.py']",4,246410513b7a94c11d00011633931401b04cc7eb,429015," def test_lockup_timeout_in_reconstruct(self): def fake_job(job): fake_job.jobs_started += 1 if fake_job.jobs_started > 2: # Two jobs per part; give the first part a pass. # All the rest lockup though sleep(1) fake_job.jobs_started = 0 reconstructor = object_reconstructor.ObjectReconstructor( self.conf, logger=self.logger) # Actually it's an int, but no reason for this to take a full second reconstructor.lockup_timeout = 0.01 reconstructor.concurrency = 2 _real_collect_parts = reconstructor.collect_parts def fake_collect_parts_builder(num_parts): def fake_collect_parts(**kwargs): parts = _real_collect_parts(**kwargs) return parts[:num_parts] return fake_collect_parts # Jobs get stuck on the waitall() with mock.patch.object(reconstructor, 'collect_parts', fake_collect_parts_builder(2)), \ mock.patch.object(reconstructor, 'process_job', fake_job), \ mock.patch.object(reconstructor, 'delete_partition', fake_job): reconstructor.run_once() log_lines = reconstructor.logger.get_lines_for_level('error') self.assertEqual(log_lines, [ 'Exception in top-level reconstruction loop: Timeout (0.01s)']) self.assertEqual(fake_job.jobs_started, 4) reconstructor.logger.clear() fake_job.jobs_started = 0 # Now we'll get stuck on the spawn() with mock.patch.object(reconstructor, 'collect_parts', fake_collect_parts_builder(3)), \ mock.patch.object(reconstructor, 'process_job', fake_job), \ mock.patch.object(reconstructor, 'delete_partition', fake_job): reconstructor.run_once() log_lines = reconstructor.logger.get_lines_for_level('error') self.assertEqual(log_lines, [ 'Exception in top-level reconstruction loop: Timeout (0.01s)']) self.assertEqual(fake_job.jobs_started, 4) ",,105,9
openstack%2Fswift~master~Ie88ee9047d8e699135dab3fdc891e2f425466f61,openstack/swift,master,Ie88ee9047d8e699135dab3fdc891e2f425466f61,Cleaned unwanted contents in output of swift-init,NEW,2016-03-30 05:16:55.000000000,2017-12-18 02:57:26.000000000,,"[{'_account_id': 7847}, {'_account_id': 10068}, {'_account_id': 13052}, {'_account_id': 21021}]","[{'number': 1, 'created': '2016-03-30 05:16:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/15f3db4bb366c64ea08e6e0d28b146ccb105bb9f', 'message': 'Wrong output in \'swift-init main restart\'\n\nIf user runs ""swift-init main restart"", it gives\nextra output on screen.\n\nThis fix removes extra traceback messages and displays\nproper message to user.\n\nChange-Id: Ie88ee9047d8e699135dab3fdc891e2f425466f61\nCloses-Bug:#1557337\n'}, {'number': 2, 'created': '2016-03-31 07:55:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/5b41b9771c7c4d0bb7acb81af87f455eaa97de20', 'message': 'Cleaned unwanted contents in output of swift-init\n\nIf user runs ""swift-init main restart"", it gives\nextra output on screen.\n\nThis fix removes extra traceback messages and displays\nproper message to user.\n\nChange-Id: Ie88ee9047d8e699135dab3fdc891e2f425466f61\nCloses-Bug:#1564245\n'}, {'number': 3, 'created': '2016-04-11 09:36:11.000000000', 'files': ['swift/common/wsgi.py', 'test/unit/common/test_wsgi.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/ea1dcc878a8a58eff742459375db7b3dc5b5e9ad', 'message': 'Cleaned unwanted contents in output of swift-init\n\nIf user runs ""swift-init main restart"", it gives\nextra output on screen.\n\nThis fix removes extra traceback messages and displays\nproper message to user.\n\nChange-Id: Ie88ee9047d8e699135dab3fdc891e2f425466f61\nCloses-Bug:#1564245\n'}]",2,299123,ea1dcc878a8a58eff742459375db7b3dc5b5e9ad,17,4,3,21021,,,0,"Cleaned unwanted contents in output of swift-init

If user runs ""swift-init main restart"", it gives
extra output on screen.

This fix removes extra traceback messages and displays
proper message to user.

Change-Id: Ie88ee9047d8e699135dab3fdc891e2f425466f61
Closes-Bug:#1564245
",git fetch https://review.opendev.org/openstack/swift refs/changes/23/299123/1 && git format-patch -1 --stdout FETCH_HEAD,['swift/common/wsgi.py'],1,15f3db4bb366c64ea08e6e0d28b146ccb105bb9f,bug/1564245," try: error_msg = strategy.bind_ports() if error_msg: logger.error(error_msg) print(error_msg) return 1 except Exception as exp: message = app_section + "" : "" + str(exp) logger.error(message) print(message)", error_msg = strategy.bind_ports() if error_msg: logger.error(error_msg) print(error_msg),10,4
openstack%2Fzun~master~I10dfd6ba8c0c02a749c956c1d7247629aabad3ac,openstack/zun,master,I10dfd6ba8c0c02a749c956c1d7247629aabad3ac,Start sandbox before starting user's container,NEW,2017-03-16 08:45:17.000000000,2017-12-18 02:57:21.000000000,,[{'_account_id': 16277}],"[{'number': 1, 'created': '2017-03-16 08:45:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/7dd06c5c2bfefeef017653c1692b0ce7060c5819', 'message': ""Start sandbox before starting user's container\n\nWhen docker deamon restarts all the containers goes into\nstopped state, if user tries to start his container, his container\nwill not be started since sandbox container is not running.\nSo this patch tries to fix that by starting sandbox container first.\n\nClose-Bug: #1671713\nChange-Id: I10dfd6ba8c0c02a749c956c1d7247629aabad3ac\n""}, {'number': 2, 'created': '2017-03-16 10:18:48.000000000', 'files': ['zun/common/nova.py', 'zun/tests/unit/container/fake_driver.py', 'zun/container/docker/driver.py', 'zun/container/driver.py', 'zun/compute/manager.py', 'zun/tests/unit/container/docker/test_docker_driver.py'], 'web_link': 'https://opendev.org/openstack/zun/commit/3e9e53c17c07df45ab80e65a735589f871caa91a', 'message': ""Start sandbox before starting user's container\n\nWhen docker deamon restarts all the containers goes into\nstopped state, if user tries to start his container, his container\nwill not be started since sandbox container is not running.\nSo this patch tries to fix that by starting sandbox container first.\n\nClose-Bug: #1671713\nChange-Id: I10dfd6ba8c0c02a749c956c1d7247629aabad3ac\n""}]",0,446358,3e9e53c17c07df45ab80e65a735589f871caa91a,9,1,2,16277,,,0,"Start sandbox before starting user's container

When docker deamon restarts all the containers goes into
stopped state, if user tries to start his container, his container
will not be started since sandbox container is not running.
So this patch tries to fix that by starting sandbox container first.

Close-Bug: #1671713
Change-Id: I10dfd6ba8c0c02a749c956c1d7247629aabad3ac
",git fetch https://review.opendev.org/openstack/zun refs/changes/58/446358/1 && git format-patch -1 --stdout FETCH_HEAD,"['zun/common/nova.py', 'zun/container/docker/driver.py', 'zun/tests/unit/container/fake_driver.py', 'zun/compute/manager.py', 'zun/container/driver.py', 'zun/tests/unit/container/docker/test_docker_driver.py']",6,7dd06c5c2bfefeef017653c1692b0ce7060c5819,bug/1671713," def test_start_sandbox(self): self.mock_docker.start = mock.Mock() self.driver.start_sandbox(context=self.context, self.mock_docker.start.assert_called_once_with('test_sandbox_id') def test_start_sandbox(self, mock_find_server_by_container_id, nova_client_instance.start_server.return_value = 'stop_server_id' self.driver.start_sandbox(self.context, sandbox_id='test_sandbox_id') nova_client_instance.start_server.assert_called_once_with("," def test_stop_sandbox(self): self.mock_docker.stop = mock.Mock() self.driver.stop_sandbox(context=self.context, self.mock_docker.stop.assert_called_once_with('test_sandbox_id') def test_stop_sandbox(self, mock_find_server_by_container_id, nova_client_instance.stop_server.return_value = 'stop_server_id' self.driver.stop_sandbox(self.context, sandbox_id='test_sandbox_id') nova_client_instance.stop_server.assert_called_once_with(",20,17
openstack%2Fmonasca-agent~master~I98d9faa1e31e99b6860eb3869d4dd4fd47bafeb3,openstack/monasca-agent,master,I98d9faa1e31e99b6860eb3869d4dd4fd47bafeb3,Add feature to report persistent volume capacity,NEW,2017-04-26 21:14:11.000000000,2017-12-18 02:57:19.000000000,,[{'_account_id': 16168}],"[{'number': 1, 'created': '2017-04-26 21:14:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/898030aff8391f40483f631fa7faeedd15745cb6', 'message': 'Add feature to report persistent volume capacity\n\nAdds the ability to Kubernetes API plugin to grab persistent\nvolume capacity per a storage class by namespace and cluster\nwide. This review only works for ceph/rbd type of persistent\nvolume storage.\n\nChange-Id: I98d9faa1e31e99b6860eb3869d4dd4fd47bafeb3\n'}, {'number': 2, 'created': '2017-04-26 21:52:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/95d97e0f7aa754f68b456fbd05867fdac98c8054', 'message': 'Add feature to report persistent volume capacity\n\nAdds the ability to Kubernetes API plugin to grab persistent\nvolume capacity per a storage class by namespace and cluster\nwide. This review only works for ceph/rbd type of persistent\nvolume storage.\n\nChange-Id: I98d9faa1e31e99b6860eb3869d4dd4fd47bafeb3\n'}, {'number': 3, 'created': '2017-04-26 22:23:31.000000000', 'files': ['monasca_agent/collector/checks_d/kubernetes_api.py', 'conf.d/kubernetes.yaml.example', 'monasca_agent/collector/checks_d/kubernetes.py', 'conf.d/kubernetes_api.yaml.example'], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/dfedf0c622092ef546e8c2c4528f36a19c9134d1', 'message': 'Add feature to report persistent volume capacity\n\nAdds the ability to Kubernetes API plugin to grab persistent\nvolume capacity per a storage class by namespace and cluster\nwide. This review only works for ceph/rbd type of persistent\nvolume storage.\n\nThis review is split from:\nhttps://review.openstack.org/#/c/448895/\n\nChange-Id: I98d9faa1e31e99b6860eb3869d4dd4fd47bafeb3\n'}]",0,460317,dfedf0c622092ef546e8c2c4528f36a19c9134d1,6,1,3,18179,,,0,"Add feature to report persistent volume capacity

Adds the ability to Kubernetes API plugin to grab persistent
volume capacity per a storage class by namespace and cluster
wide. This review only works for ceph/rbd type of persistent
volume storage.

This review is split from:
https://review.openstack.org/#/c/448895/

Change-Id: I98d9faa1e31e99b6860eb3869d4dd4fd47bafeb3
",git fetch https://review.opendev.org/openstack/monasca-agent refs/changes/17/460317/2 && git format-patch -1 --stdout FETCH_HEAD,"['conf.d/kubernetes.yaml.example', 'conf.d/kubernetes_api.yaml.example']",2,898030aff8391f40483f631fa7faeedd15745cb6,feature/persistent_volume_capacity, # Report persistent storage capacity per a storage class per a namespace and cluster. Default is false # report_persistent_storage: True # Set of storage class parameters to set as dimensions when reporting persistent storage capacity # storage_parameters_dimensions: ['type'],,8,0
openstack%2Fdiskimage-builder~master~I281679cff41e6205be6fa09cbc7bca50b7c0cb5e,openstack/diskimage-builder,master,I281679cff41e6205be6fa09cbc7bca50b7c0cb5e,download image Name suffix is error,NEW,2017-03-09 10:47:51.000000000,2017-12-18 02:57:11.000000000,,[{'_account_id': 21741}],"[{'number': 1, 'created': '2017-03-09 10:47:51.000000000', 'files': ['diskimage_builder/elements/centos/root.d/10-centos6-cloud-image'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/f50b47842fe3ce081baa392d2661c7c289b3f9a0', 'message': 'download image Name suffix is error\n\nChange-Id: I281679cff41e6205be6fa09cbc7bca50b7c0cb5e\nCloses-Bug: #1671414\n'}]",0,443559,f50b47842fe3ce081baa392d2661c7c289b3f9a0,6,1,1,22403,,,0,"download image Name suffix is error

Change-Id: I281679cff41e6205be6fa09cbc7bca50b7c0cb5e
Closes-Bug: #1671414
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/59/443559/1 && git format-patch -1 --stdout FETCH_HEAD,['diskimage_builder/elements/centos/root.d/10-centos6-cloud-image'],1,f50b47842fe3ce081baa392d2661c7c289b3f9a0,bug/1671414, BASE_IMAGE_TAR=$BASE_IMAGE_FILE.gz BASE_IMAGE_TAR=$BASE_IMAGE_FILE.gz, BASE_IMAGE_TAR=$BASE_IMAGE_FILE.tgz BASE_IMAGE_TAR=$BASE_IMAGE_FILE.tgz,2,2
openstack%2Fswift~master~I2248b73fc57e3b01b1e02e984509182142eec168,openstack/swift,master,I2248b73fc57e3b01b1e02e984509182142eec168,Update example output of remakerings,NEW,2017-06-21 20:52:38.000000000,2017-12-18 02:56:57.000000000,,[{'_account_id': 13052}],"[{'number': 1, 'created': '2017-06-21 20:52:38.000000000', 'files': ['doc/source/development_saio.rst'], 'web_link': 'https://opendev.org/openstack/swift/commit/4adc6016b3f8a425094ff50998ecb21b39321565', 'message': 'Update example output of remakerings\n\nChange-Id: I2248b73fc57e3b01b1e02e984509182142eec168\n'}]",0,476269,4adc6016b3f8a425094ff50998ecb21b39321565,5,1,1,1179,,,0,"Update example output of remakerings

Change-Id: I2248b73fc57e3b01b1e02e984509182142eec168
",git fetch https://review.opendev.org/openstack/swift refs/changes/69/476269/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/development_saio.rst'],1,4adc6016b3f8a425094ff50998ecb21b39321565,475202," Device d1r1z2-127.0.0.2:6020R127.0.0.2:6020/sdb2_"""" with 1.0 weight got id 1 Device d2r1z3-127.0.0.3:6030R127.0.0.3:6030/sdb3_"""" with 1.0 weight got id 2 Device d3r1z4-127.0.0.4:6040R127.0.0.4:6040/sdb4_"""" with 1.0 weight got id 3 Reassigned 3072 (300.00%) partitions. Balance is now 0.00. Dispersion is now 0.00 Device d1r1z2-127.0.0.2:6020R127.0.0.2:6020/sdb2_"""" with 1.0 weight got id 1 Device d2r1z3-127.0.0.3:6030R127.0.0.3:6030/sdb3_"""" with 1.0 weight got id 2 Device d3r1z4-127.0.0.4:6040R127.0.0.4:6040/sdb4_"""" with 1.0 weight got id 3 Reassigned 2048 (200.00%) partitions. Balance is now 0.00. Dispersion is now 0.00 Device d2r1z2-127.0.0.2:6020R127.0.0.2:6020/sdb2_"""" with 1.0 weight got id 2 Device d3r1z2-127.0.0.2:6020R127.0.0.2:6020/sdb6_"""" with 1.0 weight got id 3 Device d4r1z3-127.0.0.3:6030R127.0.0.3:6030/sdb3_"""" with 1.0 weight got id 4 Device d5r1z3-127.0.0.3:6030R127.0.0.3:6030/sdb7_"""" with 1.0 weight got id 5 Device d6r1z4-127.0.0.4:6040R127.0.0.4:6040/sdb4_"""" with 1.0 weight got id 6 Device d7r1z4-127.0.0.4:6040R127.0.0.4:6040/sdb8_"""" with 1.0 weight got id 7 Reassigned 6144 (600.00%) partitions. Balance is now 0.00. Dispersion is now 0.00 Device d1r1z2-127.0.0.2:6021R127.0.0.2:6021/sdb2_"""" with 1.0 weight got id 1 Device d2r1z3-127.0.0.3:6031R127.0.0.3:6031/sdb3_"""" with 1.0 weight got id 2 Device d3r1z4-127.0.0.4:6041R127.0.0.4:6041/sdb4_"""" with 1.0 weight got id 3 Reassigned 3072 (300.00%) partitions. Balance is now 0.00. Dispersion is now 0.00 Device d1r1z2-127.0.0.2:6022R127.0.0.2:6022/sdb2_"""" with 1.0 weight got id 1 Device d2r1z3-127.0.0.3:6032R127.0.0.3:6032/sdb3_"""" with 1.0 weight got id 2 Device d3r1z4-127.0.0.4:6042R127.0.0.4:6042/sdb4_"""" with 1.0 weight got id 3 Reassigned 3072 (300.00%) partitions. Balance is now 0.00. Dispersion is now 0.00"," Device d1r1z2-127.0.0.1:6020R127.0.0.1:6020/sdb2_"""" with 1.0 weight got id 1 Device d2r1z3-127.0.0.1:6030R127.0.0.1:6030/sdb3_"""" with 1.0 weight got id 2 Device d3r1z4-127.0.0.1:6040R127.0.0.1:6040/sdb4_"""" with 1.0 weight got id 3 Reassigned 1024 (100.00%) partitions. Balance is now 0.00. Dispersion is now 0.00 Device d1r1z2-127.0.0.1:6020R127.0.0.1:6020/sdb2_"""" with 1.0 weight got id 1 Device d2r1z3-127.0.0.1:6030R127.0.0.1:6030/sdb3_"""" with 1.0 weight got id 2 Device d3r1z4-127.0.0.1:6040R127.0.0.1:6040/sdb4_"""" with 1.0 weight got id 3 Reassigned 1024 (100.00%) partitions. Balance is now 0.00. Dispersion is now 0.00 Device d2r1z2-127.0.0.1:6020R127.0.0.1:6020/sdb2_"""" with 1.0 weight got id 2 Device d3r1z2-127.0.0.1:6020R127.0.0.1:6020/sdb6_"""" with 1.0 weight got id 3 Device d4r1z3-127.0.0.1:6030R127.0.0.1:6030/sdb3_"""" with 1.0 weight got id 4 Device d5r1z3-127.0.0.1:6030R127.0.0.1:6030/sdb7_"""" with 1.0 weight got id 5 Device d6r1z4-127.0.0.1:6040R127.0.0.1:6040/sdb4_"""" with 1.0 weight got id 6 Device d7r1z4-127.0.0.1:6040R127.0.0.1:6040/sdb8_"""" with 1.0 weight got id 7 Reassigned 1024 (100.00%) partitions. Balance is now 0.00. Dispersion is now 0.00 Device d1r1z2-127.0.0.1:6021R127.0.0.1:6021/sdb2_"""" with 1.0 weight got id 1 Device d2r1z3-127.0.0.1:6031R127.0.0.1:6031/sdb3_"""" with 1.0 weight got id 2 Device d3r1z4-127.0.0.1:6041R127.0.0.1:6041/sdb4_"""" with 1.0 weight got id 3 Reassigned 1024 (100.00%) partitions. Balance is now 0.00. Dispersion is now 0.00 Device d1r1z2-127.0.0.1:6022R127.0.0.1:6022/sdb2_"""" with 1.0 weight got id 1 Device d2r1z3-127.0.0.1:6032R127.0.0.1:6032/sdb3_"""" with 1.0 weight got id 2 Device d3r1z4-127.0.0.1:6042R127.0.0.1:6042/sdb4_"""" with 1.0 weight got id 3 Reassigned 1024 (100.00%) partitions. Balance is now 0.00. Dispersion is now 0.00",23,23
openstack%2Ftacker~master~I695609b7ef0eae06c29b8c093a68e21ea4f0e196,openstack/tacker,master,I695609b7ef0eae06c29b8c093a68e21ea4f0e196,Fix oslo.i18n problems in tacker,NEW,2016-08-15 06:25:31.000000000,2017-12-18 02:56:50.000000000,,"[{'_account_id': 2874}, {'_account_id': 16237}, {'_account_id': 18955}, {'_account_id': 19999}, {'_account_id': 22588}, {'_account_id': 23304}]","[{'number': 1, 'created': '2016-08-15 06:25:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/c38e0e9ab52a311fb7fd4c540f619ae12e3d867b', 'message': 'Fix olso.i18n problems in tacker\n\nHelp msg in tacker project should support olso.i18n.\n\nChange-Id: I695609b7ef0eae06c29b8c093a68e21ea4f0e196\n'}, {'number': 2, 'created': '2016-09-12 02:12:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/404dd0f32a1d9a0c2fd8d5bbea9473830e6c3a2f', 'message': ""Fix oslo.i18n problems in tacker\n\nHelp msg in tacker project should support oslo.i18n.\nBut tests directory faces developer, So it needn't\nsupport oslo.i18n.\n\nChange-Id: I695609b7ef0eae06c29b8c093a68e21ea4f0e196\n""}, {'number': 3, 'created': '2016-09-12 02:56:25.000000000', 'files': ['tacker/nfvo/drivers/vim/openstack_driver.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/2b6edaf4ee1ce73172fa3c522c4aae3f02d5434b', 'message': ""Fix oslo.i18n problems in tacker\n\nHelp msg in tacker project should support oslo.i18n.\nBut tests directory faces developer, So it needn't\nsupport oslo.i18n.\n\nChange-Id: I695609b7ef0eae06c29b8c093a68e21ea4f0e196\n""}]",0,355319,2b6edaf4ee1ce73172fa3c522c4aae3f02d5434b,19,6,3,22132,,,0,"Fix oslo.i18n problems in tacker

Help msg in tacker project should support oslo.i18n.
But tests directory faces developer, So it needn't
support oslo.i18n.

Change-Id: I695609b7ef0eae06c29b8c093a68e21ea4f0e196
",git fetch https://review.opendev.org/openstack/tacker refs/changes/19/355319/1 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/nfvo/drivers/vim/openstack_driver.py', 'tacker/tests/unit/vm/nfvo/drivers/vim/test_openstack_driver.py']",2,c38e0e9ab52a311fb7fd4c540f619ae12e3d867b,fix_olso.i18n,"from tacker._i18n import _OPTS = [cfg.StrOpt('user_domain_id', default='default', help=_('User Domain Id')), cfg.StrOpt('project_domain_id', default='default', help=_('Project Domain Id'))] ","OPTS = [cfg.StrOpt('user_domain_id', default='default', help='User Domain Id'), cfg.StrOpt('project_domain_id', default='default', help='Project ' 'Domain Id')]",10,4
openstack%2Fswift~master~I03b6d6371b788c84006ef7f560d5a20fa9a8e0c2,openstack/swift,master,I03b6d6371b788c84006ef7f560d5a20fa9a8e0c2,Buffer DiskfileWriter writes,NEW,2014-04-29 18:59:02.000000000,2017-12-18 02:55:48.000000000,,"[{'_account_id': 330}, {'_account_id': 1179}, {'_account_id': 2622}, {'_account_id': 7479}, {'_account_id': 8871}, {'_account_id': 9625}, {'_account_id': 10068}, {'_account_id': 11281}, {'_account_id': 13052}, {'_account_id': 15343}]","[{'number': 1, 'created': '2014-04-29 18:59:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/34707618f8c397a36c149f24b21f39cd70165666', 'message': ""Buffer DiskfileWriter writes\n\nAdds a buffer between the network layer and the disk layer for object-server so disk_chunk_size affects both read and write size now. This change was inspired by a problem with chunked transfers and eventlet's wsgi readchunked having performance issues with large chunked read requests. The change also allows network_chunk_size to be optimized independently from the size of writes to the disk.\n\nChange-Id: I03b6d6371b788c84006ef7f560d5a20fa9a8e0c2\n""}, {'number': 2, 'created': '2014-04-29 20:07:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4be299e7f3934ffa4ed16bd78abd3f812c5cddbf', 'message': ""Buffer DiskfileWriter writes\n\nAdds a buffer between the network layer and the disk layer for\nobject-server so disk_chunk_size affects both read and write size now.\nThis change was inspired by a problem with chunked transfers and \neventlet's wsgi readchunked having performance issues with large chunked \nread requests. The change also allows network_chunk_size to be optimized \nindependently from the size of writes to the disk.\n\nChange-Id: I03b6d6371b788c84006ef7f560d5a20fa9a8e0c2\n""}, {'number': 3, 'created': '2014-04-29 23:43:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a5db7bb0d7a8beb29768121089d6fc2a68c7e01d', 'message': ""Buffer DiskfileWriter writes\n\nFull write-up and description here:\nhttps://wiki.openstack.org/wiki/User:Physcx\n\nAdds a buffer between the network layer and the disk layer for object-server so\ndisk_chunk_size affects both read and write size now. This change was inspired\nby a problem with chunked transfers and eventlet's wsgi readchunked having\nperformance issues with large chunked read requests. The change also allows\nnetwork_chunk_size to be optimized independently from the size of writes to the\ndisk.\n\nChange-Id: I03b6d6371b788c84006ef7f560d5a20fa9a8e0c2\n""}, {'number': 4, 'created': '2014-06-30 18:16:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c455043d10f0432f12bfc9630180299dd395e2ce', 'message': ""Buffer DiskfileWriter writes\n\nFull write-up and description here:\nhttps://wiki.openstack.org/wiki/User:Physcx\n\nAdds a buffer between the network layer and the disk layer for object-server so\ndisk_chunk_size affects both read and write size now. This change was inspired\nby a problem with chunked transfers and eventlet's wsgi readchunked having\nperformance issues with large chunked read requests. The change also allows\nnetwork_chunk_size to be optimized independently from the size of writes to the\ndisk.\n\nChange-Id: I03b6d6371b788c84006ef7f560d5a20fa9a8e0c2\n""}, {'number': 5, 'created': '2016-05-27 01:09:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/78d5af5182bbcdee554dcc98bdb966217972248f', 'message': ""Buffer DiskfileWriter writes\n\nFull write-up and description here:\nhttps://wiki.openstack.org/wiki/User:Physcx\n\nAdds a buffer between the network layer and the disk layer for object-server so\ndisk_chunk_size affects both read and write size now. This change was inspired\nby a problem with chunked transfers and eventlet's wsgi readchunked having\nperformance issues with large chunked read requests. The change also allows\nnetwork_chunk_size to be optimized independently from the size of writes to the\ndisk.\n\nChange-Id: I03b6d6371b788c84006ef7f560d5a20fa9a8e0c2\n""}, {'number': 6, 'created': '2017-02-20 07:32:19.000000000', 'files': ['test/unit/obj/test_auditor.py', 'test/unit/obj/test_diskfile.py', 'swift/obj/diskfile.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/0554eb05a444c1e84f531a75aaf4fc904236a93d', 'message': ""Buffer DiskfileWriter writes\n\nFull write-up and description here:\nhttps://wiki.openstack.org/wiki/User:Physcx\n\nAdds a buffer between the network layer and the disk layer for object-server so\ndisk_chunk_size affects both read and write size now. This change was inspired\nby a problem with chunked transfers and eventlet's wsgi readchunked having\nperformance issues with large chunked read requests. The change also allows\nnetwork_chunk_size to be optimized independently from the size of writes to the\ndisk.\n\nChange-Id: I03b6d6371b788c84006ef7f560d5a20fa9a8e0c2\n""}]",3,91133,0554eb05a444c1e84f531a75aaf4fc904236a93d,55,10,6,11281,,,0,"Buffer DiskfileWriter writes

Full write-up and description here:
https://wiki.openstack.org/wiki/User:Physcx

Adds a buffer between the network layer and the disk layer for object-server so
disk_chunk_size affects both read and write size now. This change was inspired
by a problem with chunked transfers and eventlet's wsgi readchunked having
performance issues with large chunked read requests. The change also allows
network_chunk_size to be optimized independently from the size of writes to the
disk.

Change-Id: I03b6d6371b788c84006ef7f560d5a20fa9a8e0c2
",git fetch https://review.opendev.org/openstack/swift refs/changes/33/91133/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/obj/test_auditor.py', 'swift/obj/diskfile.py']",2,34707618f8c397a36c149f24b21f39cd70165666,BufferedWrites," def __init__(self, name, datadir, fd, tmppath, bytes_per_sync, disk_chunk_size, threadpool): self._disk_chunk_size = disk_chunk_size self._written_size = 0 self._write_buffer_size = 0 self._write_buffer = [] def _do_write(self, chunk): """""" Write a chunk of data to disk. :param chunk: the chunk of data to write as a string object """""" def _write_entire_chunk(chunk): while chunk: written = os.write(self._fd, chunk) self._written_size += written chunk = chunk[written:] self._threadpool.run_in_thread(_write_entire_chunk, chunk) # For large files sync every 512MB (by default) written diff = self._written_size - self._last_sync if diff >= self._bytes_per_sync: self._threadpool.force_run_in_thread(fdatasync, self._fd) drop_buffer_cache(self._fd, self._last_sync, diff) self._last_sync = self._written_size def flush_write_buffer(self): # Write any data in the write_buffer to disk immediately if self._write_buffer: self._do_write(''.join(self._write_buffer)) self._write_buffer = [] self._write_buffer_size = 0 new_chunk_len = len(chunk) self._upload_size += new_chunk_len avail_data = self._write_buffer_size + new_chunk_len # If data already in write_buffer and new data enough to trigger # a write, flush write buffer if self._write_buffer and avail_data >= self._disk_chunk_size: split_loc = self._disk_chunk_size - self._write_buffer_size self._write_buffer.append(chunk[:split_loc]) chunk = chunk[split_loc:] self.flush_write_buffer() avail_data -= self._disk_chunk_size # While data ready to be written is greater than disk_chunk_size while avail_data >= self._disk_chunk_size: self._do_write(chunk[:self._disk_chunk_size]) chunk = chunk[self._disk_chunk_size:] avail_data -= self._disk_chunk_size # Buffer any leftover data if chunk: self._write_buffer.append(chunk) self._write_buffer_size += avail_data # Flush the write buffer self.flush_write_buffer() self._bytes_per_sync, self._disk_chunk_size, self._threadpool)"," def __init__(self, name, datadir, fd, tmppath, bytes_per_sync, threadpool): def _write_entire_chunk(chunk): while chunk: written = os.write(self._fd, chunk) self._upload_size += written chunk = chunk[written:] self._threadpool.run_in_thread(_write_entire_chunk, chunk) # For large files sync every 512MB (by default) written diff = self._upload_size - self._last_sync if diff >= self._bytes_per_sync: self._threadpool.force_run_in_thread(fdatasync, self._fd) drop_buffer_cache(self._fd, self._last_sync, diff) self._last_sync = self._upload_size self._bytes_per_sync, self._threadpool)",82,35
openstack%2Fswift~master~I9ba5e8b896f592f5d110636861a1ee07580861a5,openstack/swift,master,I9ba5e8b896f592f5d110636861a1ee07580861a5,WIP: Add notification policy and transport middleware,NEW,2016-10-19 05:12:11.000000000,2017-12-18 02:55:43.000000000,,"[{'_account_id': 9622}, {'_account_id': 12279}, {'_account_id': 13052}, {'_account_id': 18186}, {'_account_id': 20553}, {'_account_id': 21231}]","[{'number': 1, 'created': '2016-10-19 05:12:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/773543be6136c3c1552c0812099e24e17dc224d7', 'message': 'WIP: Add notification policy and transport middleware\n\nAdds two new pieces of middleware which provide a notification\nframework that allows for backend messaging deployment flexibility.\n\nStill TODO:\n- Lots of tests\n- The transport implementation currently implemented is Kafka. There\n  are plans to upload a Zaqar middleware as well.\n- Add /info support as discussed in docs\n\nChange-Id: I9ba5e8b896f592f5d110636861a1ee07580861a5\n'}, {'number': 2, 'created': '2016-11-08 22:36:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2dc1d0f0dbe4c6ada746bb0f2d044faa71cd03f7', 'message': 'WIP: Add notification policy and transport middleware\n\nAdds two new pieces of middleware which provide a notification\nframework that allows for backend messaging deployment flexibility.\n\nStill TODO:\n- Lots of tests\n- The transport implementation currently implemented is Kafka. There\n  had been plans to upload a Zaqar middleware as well.\n  However, this patch may change to only include the policy\n  middleware as a result of discussions at the summit.\n- Add /info support as discussed in docs\n\nChange-Id: I9ba5e8b896f592f5d110636861a1ee07580861a5\n'}, {'number': 3, 'created': '2016-12-03 00:33:57.000000000', 'files': ['swift/common/middleware/notifications/policy.py', 'doc/source/overview_notifications.rst', 'doc/source/index.rst', 'requirements.txt', 'swift/common/middleware/notifications/__init__.py', 'swift/common/middleware/notifications/transport_kafka.py', 'swift/common/middleware/notifications/transport_dummy.py', 'etc/proxy-server.conf-sample', 'doc/source/middleware.rst', 'setup.cfg', 'test/unit/common/middleware/notifications/test_policy.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/e90caff542fc110d19b7ddd69d1686992bbb247d', 'message': 'WIP: Add notification policy and transport middleware\n\nAdds two new pieces of middleware which provide a notification\nframework that allows for backend messaging deployment flexibility.\n\nStill TODO:\n- Lots of tests\n- The transport implementation currently implemented is Kafka. There\n  had been plans to upload a Zaqar middleware as well.\n  However, this patch may change to only include the policy\n  middleware as a result of discussions at the summit.\n- Add /info support as discussed in docs\n\nChange-Id: I9ba5e8b896f592f5d110636861a1ee07580861a5\n'}]",18,388393,e90caff542fc110d19b7ddd69d1686992bbb247d,24,6,3,21231,,,0,"WIP: Add notification policy and transport middleware

Adds two new pieces of middleware which provide a notification
framework that allows for backend messaging deployment flexibility.

Still TODO:
- Lots of tests
- The transport implementation currently implemented is Kafka. There
  had been plans to upload a Zaqar middleware as well.
  However, this patch may change to only include the policy
  middleware as a result of discussions at the summit.
- Add /info support as discussed in docs

Change-Id: I9ba5e8b896f592f5d110636861a1ee07580861a5
",git fetch https://review.opendev.org/openstack/swift refs/changes/93/388393/3 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/middleware/notifications/policy.py', 'doc/source/index.rst', 'doc/source/overview_notifications.rst', 'requirements.txt', 'swift/common/middleware/notifications/__init__.py', 'swift/common/middleware/notifications/transport_dummy.py', 'swift/common/middleware/notifications/transport_kafka.py', 'etc/proxy-server.conf-sample', 'doc/source/middleware.rst', 'setup.cfg']",10,773543be6136c3c1552c0812099e24e17dc224d7,notifications, notification-policy = swift.common.middleware.notifications.policy:filter_factory notification-kafka = swift.common.middleware.notifications.transport_kafka:filter_factory notification-dummy = swift.common.middleware.notifications.transport_dummy:filter_factory,,488,1
openstack%2Fironic~master~Id5604678622e3fbc700da94e7161b79cfb1e8575,openstack/ironic,master,Id5604678622e3fbc700da94e7161b79cfb1e8575,Add mention about desirec config_drive_format on nova-compute,NEW,2017-06-12 10:59:17.000000000,2017-12-18 02:55:36.000000000,,"[{'_account_id': 6618}, {'_account_id': 10239}, {'_account_id': 19339}]","[{'number': 1, 'created': '2017-06-12 10:59:17.000000000', 'files': ['install-guide/source/configure-compute.rst'], 'web_link': 'https://opendev.org/openstack/ironic/commit/d39d79de90f6ece455a3f1f5f30b02ef48e933c0', 'message': 'Add mention about desirec config_drive_format on nova-compute\n\nTo avoid instance_info lenght overflow, it is highly recommended to use\niso9660 instead of vfat as config_drive_format.\nThis patch updates install-guide for nova-compute.\n\nChange-Id: Id5604678622e3fbc700da94e7161b79cfb1e8575\n'}]",5,473370,d39d79de90f6ece455a3f1f5f30b02ef48e933c0,7,3,1,14525,,,0,"Add mention about desirec config_drive_format on nova-compute

To avoid instance_info lenght overflow, it is highly recommended to use
iso9660 instead of vfat as config_drive_format.
This patch updates install-guide for nova-compute.

Change-Id: Id5604678622e3fbc700da94e7161b79cfb1e8575
",git fetch https://review.opendev.org/openstack/ironic refs/changes/70/473370/1 && git format-patch -1 --stdout FETCH_HEAD,['install-guide/source/configure-compute.rst'],1,d39d79de90f6ece455a3f1f5f30b02ef48e933c0,update_doc, # Configuration drive format that will contain metadata attached to the # instance when it boots. # Ironic stores config_drive in the instance_info database field. # To avoid instance_info legth overflow use iso9660 instead of vfat. config_drive_format=iso9660 ,,7,0
openstack%2Fswift~master~Ib5f8856cbc313894762f0f1230d4eeb8ac959dfe,openstack/swift,master,Ib5f8856cbc313894762f0f1230d4eeb8ac959dfe,Prepare iter_nodes_local_fist able to take parimary nodes in args,NEW,2017-06-15 07:43:46.000000000,2017-12-18 02:55:18.000000000,,"[{'_account_id': 1179}, {'_account_id': 7847}, {'_account_id': 13052}]","[{'number': 1, 'created': '2017-06-15 07:43:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/61a33c25ef3a3152cec52bab3abcd07882e44e21', 'message': 'Prepare iter_nodes_local_fist able to take parimary nodes in args\n\nChange-Id: Ib5f8856cbc313894762f0f1230d4eeb8ac959dfe\n'}, {'number': 2, 'created': '2017-06-21 21:13:20.000000000', 'files': ['test/unit/proxy/controllers/test_obj.py', 'swift/proxy/controllers/obj.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/dfa361261cb87d8863a69b2f1fb88ed6cd503410', 'message': 'Prepare iter_nodes_local_fist able to take parimary nodes in args\n\nChange-Id: Ib5f8856cbc313894762f0f1230d4eeb8ac959dfe\n'}]",4,474472,dfa361261cb87d8863a69b2f1fb88ed6cd503410,10,3,2,4608,,,0,"Prepare iter_nodes_local_fist able to take parimary nodes in args

Change-Id: Ib5f8856cbc313894762f0f1230d4eeb8ac959dfe
",git fetch https://review.opendev.org/openstack/swift refs/changes/72/474472/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/proxy/controllers/test_obj.py', 'swift/proxy/controllers/obj.py']",2,61a33c25ef3a3152cec52bab3abcd07882e44e21,global-ec-cluster," def iter_nodes_local_first( self, primary_nodes, ring, partition, policy=None): :param primary_nodes: primary node list self.iter_nodes_local_first( nodes, obj_ring, partition, policy=policy))"," def iter_nodes_local_first(self, ring, partition, policy=None): primary_nodes = ring.get_part_nodes(partition) self.iter_nodes_local_first(obj_ring, partition, policy=policy))",13,7
openstack%2Fswift~master~I00c766541fe22f77d97efbc34af31edf9b209b96,openstack/swift,master,I00c766541fe22f77d97efbc34af31edf9b209b96,Automatic refresh of memcache config settings,NEW,2015-08-28 22:00:22.000000000,2017-12-18 02:55:16.000000000,,"[{'_account_id': 330}, {'_account_id': 2622}, {'_account_id': 7233}, {'_account_id': 9816}, {'_account_id': 12279}, {'_account_id': 13052}, {'_account_id': 15343}, {'_account_id': 16896}, {'_account_id': 16964}, {'_account_id': 17156}, {'_account_id': 18820}, {'_account_id': 21231}]","[{'number': 1, 'created': '2015-08-28 22:00:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2497b7f0b96fac0ca7dc1743a0ad609127bbaf6c', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. The value can be set in\nproxy-server.conf, and the default min time between reloads is five\nminutes. Every time the memcache middleware gets called, it checks\nto see if the specified amount of time has elapsed. If so, it then\ndetermines whether or not memcache.conf has been modified since the\nlast load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 2, 'created': '2015-08-28 22:17:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/bc2af47c49157ed103d42848b1dde6fdcb102927', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. The value can be set in\nproxy-server.conf, and the default min time between reloads is five\nminutes. Every time the memcache middleware gets called, it checks\nto see if the specified amount of time has elapsed. If so, it then\ndetermines whether or not memcache.conf has been modified since the\nlast load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 3, 'created': '2015-09-02 19:52:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/07cdca7573b6d7b1fed63396c2a320eaf6b26e04', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. The value can be set in\nproxy-server.conf, and the default min time between reloads is five\nminutes. Every time the memcache middleware gets called, it checks\nto see if the specified amount of time has elapsed. If so, it then\ndetermines whether or not memcache.conf has been modified since the\nlast load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 4, 'created': '2015-10-16 16:35:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4f8c6d29d9aca010e6323448a48c3c0947754092', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. The value can be set in\nproxy-server.conf, and the default min time between reloads is five\nminutes. Every time the memcache middleware gets called, it checks\nto see if the specified amount of time has elapsed. If so, it then\ndetermines whether or not memcache.conf has been modified since the\nlast load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 5, 'created': '2015-10-16 16:56:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f05f193d9b35b412113e91be8d19fce73cd1249b', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. The value can be set in\nproxy-server.conf, and the default min time between reloads is five\nminutes. Every time the memcache middleware gets called, it checks\nto see if the specified amount of time has elapsed. If so, it then\ndetermines whether or not memcache.conf has been modified since the\nlast load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 6, 'created': '2015-10-16 18:57:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a05f0bb5fe2634a6c439d18b19cabbb7d4166d23', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. The value can be set in\nproxy-server.conf, and the default min time between reloads is five\nminutes. Every time the memcache middleware gets called, it checks\nto see if the specified amount of time has elapsed. If so, it then\ndetermines whether or not memcache.conf has been modified since the\nlast load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 7, 'created': '2015-11-16 19:18:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/367f8dbfa7ba2a8d5d6cb4873fd6363854f10e1f', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. The value can be set in\nproxy-server.conf, and the default min time between reloads is five\nminutes. Every time the memcache middleware gets called, it checks\nto see if the specified amount of time has elapsed. If so, it then\ndetermines whether or not memcache.conf has been modified since the\nlast load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 8, 'created': '2015-11-16 20:50:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8d3d2f08cf67226a235dda74e31e211dc121a060', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. The value can be set in\nproxy-server.conf, and the default min time between reloads is five\nminutes. Every time the memcache middleware gets called, it checks\nto see if the specified amount of time has elapsed. If so, it then\ndetermines whether or not memcache.conf has been modified since the\nlast load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 9, 'created': '2016-01-12 00:07:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4072c26be9cf5d2908218782d85b9f1ee5f477eb', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. This value can be set in\nproxy-server.conf. Every time the memcache middleware gets called,\nit checks to see if the specified amount of time has elapsed. If so,\nit then determines whether or not memcache.conf has been modified\nsince the last load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 10, 'created': '2016-01-12 16:14:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c9ed0f8389eb1916939817b776a1d1c10950e44e', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. This value can be set in\nproxy-server.conf. Every time the memcache middleware gets called,\nit checks to see if the specified amount of time has elapsed. If so,\nit then determines whether or not memcache.conf has been modified\nsince the last load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 11, 'created': '2016-01-25 23:03:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c7e6d848159e15d126aa406519c319bc4eccec7d', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. This value can be set in\nproxy-server.conf. Every time the memcache middleware gets called,\nit checks to see if the specified amount of time has elapsed. If so,\nit then determines whether or not memcache.conf has been modified\nsince the last load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 12, 'created': '2016-01-27 21:43:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ce5b050f8fcd4d709184cb1420f1898dad98b2a6', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. This value can be set in\nproxy-server.conf. Every time the memcache middleware gets called,\nit checks to see if the specified amount of time has elapsed. If so,\nit then determines whether or not memcache.conf has been modified\nsince the last load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 13, 'created': '2016-02-01 23:25:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/cd7b1c026c36be75ed1d028d2fde12831f18f1df', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. This value can be set in\nproxy-server.conf. Every time the memcache middleware gets called,\nit checks to see if the specified amount of time has elapsed. If so,\nit then determines whether or not memcache.conf has been modified\nsince the last load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 14, 'created': '2016-08-04 02:19:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/359645eb0c562a7c1fda6d4d6a038c79c44ac6ba', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. This value can be set in\nproxy-server.conf. Every time the memcache middleware gets called,\nit checks to see if the specified amount of time has elapsed. If so,\nit then determines whether or not memcache.conf has been modified\nsince the last load. If it has, we then reload the config settings.\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 15, 'created': '2016-08-05 05:33:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e02975eef906bec25b258b92574ded954631aca6', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. This value can be set in\nproxy-server.conf or in memcache.conf. Every time the memcache\nmiddleware gets called, it checks to see if the specified amount of\ntime has elapsed. If so, it then determines whether or not\nmemcache.conf has been modified since the last load.\nIf it has, we then reload the config settings.\n\nCo-Authored-By: Janie Richling <jrichli@us.ibm.com>\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 16, 'created': '2016-08-29 04:18:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/be1374317d4c21a6760e342f2f670905a4af8d5c', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. This value can be set in\nproxy-server.conf or in memcache.conf. Every time the memcache\nmiddleware gets called, it checks to see if the specified amount of\ntime has elapsed. If so, it then determines whether or not\nmemcache.conf has been modified since the last load.\nIf it has, we then reload the config settings.\n\nCo-Authored-By: Janie Richling <jrichli@us.ibm.com>\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 17, 'created': '2016-10-01 05:53:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/9e469e0ecd51d1f09b9f9c19e386f8ba9e3061b3', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. This value can be set in\nproxy-server.conf or in memcache.conf. Every time the memcache\nmiddleware gets called, it checks to see if the specified amount of\ntime has elapsed. If so, it then determines whether or not\nmemcache.conf has been modified since the last load.\nIf it has, we then reload the config settings.\n\nCo-Authored-By: Janie Richling <jrichli@us.ibm.com>\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 18, 'created': '2016-10-01 23:58:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8331abffaef666adadb7be3453da871d3c1887e2', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. This value can be set in\nproxy-server.conf or in memcache.conf. Every time the memcache\nmiddleware gets called, it checks to see if the specified amount of\ntime has elapsed. If so, it then determines whether or not\nmemcache.conf has been modified since the last load.\nIf it has, we then reload the config settings.\n\nCo-Authored-By: Janie Richling <jrichli@us.ibm.com>\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}, {'number': 19, 'created': '2016-11-11 15:48:44.000000000', 'files': ['test/unit/common/middleware/test_memcache.py', 'doc/source/deployment_guide.rst', 'etc/proxy-server.conf-sample', 'doc/manpages/proxy-server.conf.5', 'swift/common/middleware/memcache.py', 'etc/memcache.conf-sample'], 'web_link': 'https://opendev.org/openstack/swift/commit/fc630c58b5cd34f717bdf5edc80526bc1ce3db26', 'message': 'Automatic refresh of memcache config settings\n\nCurrently, we load memcache config settings once, when the memcache\nmiddleware is instantiated. This patch, based on ring.py, allows\nmemcache.conf to be reloaded at regular intervals. When the memcache\nmiddleware is instantiated, it sets the minimum amount of time that\nshould pass between reloads. This value can be set in\nproxy-server.conf or in memcache.conf. Every time the memcache\nmiddleware gets called, it checks to see if the specified amount of\ntime has elapsed. If so, it then determines whether or not\nmemcache.conf has been modified since the last load.\nIf it has, we then reload the config settings.\n\nCo-Authored-By: Janie Richling <jrichli@us.ibm.com>\n\nChange-Id: I00c766541fe22f77d97efbc34af31edf9b209b96\n'}]",50,218490,fc630c58b5cd34f717bdf5edc80526bc1ce3db26,87,12,19,16964,,,0,"Automatic refresh of memcache config settings

Currently, we load memcache config settings once, when the memcache
middleware is instantiated. This patch, based on ring.py, allows
memcache.conf to be reloaded at regular intervals. When the memcache
middleware is instantiated, it sets the minimum amount of time that
should pass between reloads. This value can be set in
proxy-server.conf or in memcache.conf. Every time the memcache
middleware gets called, it checks to see if the specified amount of
time has elapsed. If so, it then determines whether or not
memcache.conf has been modified since the last load.
If it has, we then reload the config settings.

Co-Authored-By: Janie Richling <jrichli@us.ibm.com>

Change-Id: I00c766541fe22f77d97efbc34af31edf9b209b96
",git fetch https://review.opendev.org/openstack/swift refs/changes/90/218490/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/common/middleware/test_memcache.py', 'swift/common/middleware/memcache.py']",2,2497b7f0b96fac0ca7dc1743a0ad609127bbaf6c,auto_reload_memcache,"from time import time from os.path import getmtime, isfile self.conf = conf self.conf_path = os.path.join(conf.get('swift_dir', '/etc/swift'), 'memcache.conf') self.conf_file_exists = isfile(self.conf_path) self.reload_time = int(conf.get('reload_time', 300)) self._reload(True) def __call__(self, env, start_response): if time() > self._rtime: self._reload() env['swift.cache'] = self.memcache return self.app(env, start_response) def _update_rtime(self): self._rtime = time() + self.reload_time def _update_mtime(self): if isfile(self.conf_path): self._mtime = getmtime(self.conf_path) def _reload(self, force=False): self._update_rtime() if force or self.has_changed(): self._load() self._update_mtime() def _load(self): memcache_conf = ConfigParser() self.memcache_servers = self.conf.get('memcache_servers') serialization_format = self.conf.get('memcache_serialization_support') max_conns = int(self.conf.get('memcache_max_connections', self.conf.get('max_connections', 0))) if memcache_conf.read(self.conf_path): memcache_options.update(self.conf) def has_changed(self): if not isfile(self.conf_path): if self.conf_file_exists: self.conf_file_exists = False return True else: return False else: if not self.conf_file_exists: self.conf_file_exists = True return True else: return getmtime(self.conf_path) != self._mtime"," self.memcache_servers = conf.get('memcache_servers') serialization_format = conf.get('memcache_serialization_support') max_conns = int(conf.get('memcache_max_connections', conf.get('max_connections', 0))) path = os.path.join(conf.get('swift_dir', '/etc/swift'), 'memcache.conf') memcache_conf = ConfigParser() if memcache_conf.read(path): memcache_options.update(conf) def __call__(self, env, start_response): env['swift.cache'] = self.memcache return self.app(env, start_response)",323,12
openstack%2Frally~master~Idd368408ac5b0f522535fe821039e7283b7b41c8,openstack/rally,master,Idd368408ac5b0f522535fe821039e7283b7b41c8,Fix attempt to delete heat stack during restoring,NEW,2017-06-26 12:42:27.000000000,2017-12-18 02:54:17.000000000,,"[{'_account_id': 6172}, {'_account_id': 8871}, {'_account_id': 21528}, {'_account_id': 23435}]","[{'number': 1, 'created': '2017-06-26 12:42:27.000000000', 'files': ['tests/unit/plugins/openstack/scenarios/heat/test_stacks.py', 'rally/plugins/openstack/scenarios/heat/stacks.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/3ac7c83ea618095be5a9b86e31a748e947d4f507', 'message': ""Fix attempt to delete heat stack during restoring\n\nrestoring of stack not yet finished but delete already started.\nso wait for or sleep should be need, but 'wait for' already exists.\n\nChange-Id: Idd368408ac5b0f522535fe821039e7283b7b41c8\nCloses-Bug: #1558525\n""}]",2,477520,3ac7c83ea618095be5a9b86e31a748e947d4f507,8,4,1,23435,,,0,"Fix attempt to delete heat stack during restoring

restoring of stack not yet finished but delete already started.
so wait for or sleep should be need, but 'wait for' already exists.

Change-Id: Idd368408ac5b0f522535fe821039e7283b7b41c8
Closes-Bug: #1558525
",git fetch https://review.opendev.org/openstack/rally refs/changes/20/477520/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/plugins/openstack/scenarios/heat/test_stacks.py', 'rally/plugins/openstack/scenarios/heat/stacks.py']",2,3ac7c83ea618095be5a9b86e31a748e947d4f507,bug/1558525," files=None, environment=None, min_sleep=0, max_sleep=0): :param min_sleep: Minimum sleep time in seconds (non-negative) :param max_sleep: Maximum sleep time in seconds (non-negative) self.sleep_between(min_sleep, max_sleep)"," files=None, environment=None):",11,3
openstack%2Frally~master~I1001dc9c372a48e01d8245ce84b1ad238e7f1470,openstack/rally,master,I1001dc9c372a48e01d8245ce84b1ad238e7f1470,WIP: fix murano cleanup,NEW,2017-04-25 23:17:45.000000000,2017-12-18 02:54:15.000000000,,"[{'_account_id': 9545}, {'_account_id': 14817}]","[{'number': 1, 'created': '2017-04-25 23:17:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b0df85c63a2a91dd97bce59cc14334e2085371c1', 'message': 'WIP: change resource name template for murano\n\nChange-Id: I1001dc9c372a48e01d8245ce84b1ad238e7f1470\n'}, {'number': 2, 'created': '2017-04-25 23:20:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/77f4184e0f04c2188370fc3936156648967195be', 'message': 'WIP: change resource name template for murano\n\nChange-Id: I1001dc9c372a48e01d8245ce84b1ad238e7f1470\n'}, {'number': 3, 'created': '2017-04-26 15:37:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/fd18539567c7dca6408e2f7c662e8942714d1c99', 'message': 'WIP: fix murano cleanup\n\nChange-Id: I1001dc9c372a48e01d8245ce84b1ad238e7f1470\n'}, {'number': 4, 'created': '2017-04-27 09:37:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/10cd67386c22bea232f486da0b2ed30053cf16a2', 'message': 'WIP: fix murano cleanup\n\nChange-Id: I1001dc9c372a48e01d8245ce84b1ad238e7f1470\n'}, {'number': 5, 'created': '2017-04-27 09:39:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/eceb675915289983f17dea188cb6dd1882ee4eae', 'message': 'WIP: fix murano cleanup\n\nChange-Id: I1001dc9c372a48e01d8245ce84b1ad238e7f1470\n'}, {'number': 6, 'created': '2017-04-27 10:26:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/12a7aebcda30801ded5325bf8d6d2fd2dd6bb845', 'message': 'WIP: fix murano cleanup\n\nChange-Id: I1001dc9c372a48e01d8245ce84b1ad238e7f1470\n'}, {'number': 7, 'created': '2017-04-27 13:11:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e5a9e29a11fd8970258433d8bd3296def55d7cfe', 'message': 'WIP: fix murano cleanup\n\nChange-Id: I1001dc9c372a48e01d8245ce84b1ad238e7f1470\n'}, {'number': 8, 'created': '2017-04-27 13:57:43.000000000', 'files': ['rally/plugins/openstack/context/murano/murano_packages.py', 'rally/plugins/openstack/scenarios/murano/packages.py', 'rally/plugins/openstack/cleanup/resources.py', 'rally/plugins/openstack/scenarios/murano/utils.py', 'rally/common/utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/f16bc8282e67e3ffbf34f3d1f16dc9037b07d9e6', 'message': 'WIP: fix murano cleanup\n\nChange-Id: I1001dc9c372a48e01d8245ce84b1ad238e7f1470\n'}]",0,459890,f16bc8282e67e3ffbf34f3d1f16dc9037b07d9e6,23,2,8,9545,,,0,"WIP: fix murano cleanup

Change-Id: I1001dc9c372a48e01d8245ce84b1ad238e7f1470
",git fetch https://review.opendev.org/openstack/rally refs/changes/90/459890/8 && git format-patch -1 --stdout FETCH_HEAD,['rally/plugins/openstack/scenarios/murano/packages.py'],1,b0df85c63a2a91dd97bce59cc14334e2085371c1,murano, RESOURCE_NAME_FORMAT = utils.MuranoPackageManager.RESOURCE_NAME_FORMAT RESOURCE_NAME_FORMAT = utils.MuranoPackageManager.RESOURCE_NAME_FORMAT RESOURCE_NAME_FORMAT = utils.MuranoPackageManager.RESOURCE_NAME_FORMAT RESOURCE_NAME_FORMAT = utils.MuranoPackageManager.RESOURCE_NAME_FORMAT ,,8,0
openstack%2Frally~master~I696ef7c656bdb84d941ee54414b2b33c61c910a4,openstack/rally,master,I696ef7c656bdb84d941ee54414b2b33c61c910a4,Add CinderVolumes.create_unmanage_and_manage_volume,NEW,2017-01-17 06:29:29.000000000,2017-12-18 02:54:12.000000000,,"[{'_account_id': 1736}, {'_account_id': 8871}, {'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 18404}, {'_account_id': 21528}, {'_account_id': 23435}]","[{'number': 1, 'created': '2017-01-17 06:29:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c6f9f0908602b63b0ce699699a548d20b99104fc', 'message': 'Add CinderVolumes.create_unmanage_and_manage_volume\n\nThis scenario first creates a volume, then unmanages it, finally\nmanages and deletes it.\n\nChange-Id: I696ef7c656bdb84d941ee54414b2b33c61c910a4\n'}, {'number': 2, 'created': '2017-02-06 03:23:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/219bce1e34e0bbf869d482a9b90237719beccb5a', 'message': 'Add CinderVolumes.create_unmanage_and_manage_volume\n\nThis scenario first creates a volume, then unmanages it, finally\nmanages and deletes it.\n\nChange-Id: I696ef7c656bdb84d941ee54414b2b33c61c910a4\n'}, {'number': 3, 'created': '2017-05-31 08:22:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/75bc8efa81201f37b51a8a2d2f7cdfd3abd3307b', 'message': 'Add CinderVolumes.create_unmanage_and_manage_volume\n\nThis scenario first creates a volume, then unmanages it, finally\nmanages and deletes it.\n\nChange-Id: I696ef7c656bdb84d941ee54414b2b33c61c910a4\n'}, {'number': 4, 'created': '2017-06-06 06:33:31.000000000', 'files': ['samples/tasks/scenarios/cinder/create-unmanage-and-manage-volume.yaml', 'rally/plugins/openstack/services/storage/cinder_v2.py', 'tests/unit/plugins/openstack/scenarios/cinder/test_volumes.py', 'tests/unit/plugins/openstack/services/storage/test_cinder_v2.py', 'rally-jobs/cinder.yaml', 'rally/plugins/openstack/scenarios/cinder/volumes.py', 'samples/tasks/scenarios/cinder/create-unmanage-and-manage-volume.json'], 'web_link': 'https://opendev.org/openstack/rally/commit/f8a2d357a542be2c50f2f98561c9099895ae28f2', 'message': 'Add CinderVolumes.create_unmanage_and_manage_volume\n\nThis scenario first creates a volume, then unmanages it, finally\nmanages and deletes it.\n\nChange-Id: I696ef7c656bdb84d941ee54414b2b33c61c910a4\n'}]",10,421088,f8a2d357a542be2c50f2f98561c9099895ae28f2,25,7,4,18404,,,0,"Add CinderVolumes.create_unmanage_and_manage_volume

This scenario first creates a volume, then unmanages it, finally
manages and deletes it.

Change-Id: I696ef7c656bdb84d941ee54414b2b33c61c910a4
",git fetch https://review.opendev.org/openstack/rally refs/changes/88/421088/1 && git format-patch -1 --stdout FETCH_HEAD,"['samples/tasks/scenarios/cinder/create-unmanage-and-manage-volume.yaml', 'tests/unit/plugins/openstack/scenarios/cinder/test_volumes.py', 'rally/plugins/openstack/scenarios/cinder/utils.py', 'rally-jobs/cinder.yaml', 'tests/unit/plugins/openstack/scenarios/cinder/test_utils.py', 'rally/plugins/openstack/scenarios/cinder/volumes.py', 'samples/tasks/scenarios/cinder/create-unmanage-and-manage-volume.json']",7,c6f9f0908602b63b0ce699699a548d20b99104fc,CinderVolumes.create_unmanage_and_manage_volume,"{ ""CinderVolumes.create_unmanage_and_manage_volume"": [ { ""args"": { ""size"": 1 }, ""runner"": { ""type"": ""constant"", ""times"": 4, ""concurrency"": 2 }, ""context"": { ""users"": { ""tenants"": 2, ""users_per_tenant"": 2 }, ""roles"": [""admin""] }, ""sla"": { ""failure_rate"": { ""max"": 0 } } } ] } ",,197,0
openstack%2Ftacker~master~Ifcd1e0eeab9ce8e866258659f04b4e3b4e72cfe4,openstack/tacker,master,Ifcd1e0eeab9ce8e866258659f04b4e3b4e72cfe4,Implement: VIM Name and VIM ID argument in REST calls for VNF create operation,NEW,2016-10-11 20:43:33.000000000,2017-12-18 02:54:10.000000000,,"[{'_account_id': 2874}, {'_account_id': 10068}, {'_account_id': 10487}, {'_account_id': 12455}, {'_account_id': 18955}]","[{'number': 1, 'created': '2016-10-11 20:43:33.000000000', 'files': ['doc/source/devref/mano_api.rst'], 'web_link': 'https://opendev.org/openstack/tacker/commit/23d5e622e3637069bbe2b7faff520db7964bd7a8', 'message': 'Implement: VIM Name and VIM ID argument in REST calls for VNF create operation\n\nChange-Id: Ifcd1e0eeab9ce8e866258659f04b4e3b4e72cfe4\n'}]",2,385145,23d5e622e3637069bbe2b7faff520db7964bd7a8,7,5,1,23480,,,0,"Implement: VIM Name and VIM ID argument in REST calls for VNF create operation

Change-Id: Ifcd1e0eeab9ce8e866258659f04b4e3b4e72cfe4
",git fetch https://review.opendev.org/openstack/tacker refs/changes/45/385145/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/devref/mano_api.rst'],1,23d5e622e3637069bbe2b7faff520db7964bd7a8,bug/1598257," ""vim_name"": """",",,1,0
openstack%2Ftacker~master~I7df404ffcc1cffc05b7ef12c52cd625d61131d65,openstack/tacker,master,I7df404ffcc1cffc05b7ef12c52cd625d61131d65,Add blueprint only tacker development process,NEW,2016-08-09 01:45:05.000000000,2017-12-18 02:54:07.000000000,,"[{'_account_id': 2874}, {'_account_id': 12525}, {'_account_id': 20866}]","[{'number': 1, 'created': '2016-08-09 01:45:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/1b21cb8df5451f47e7e99af236adc174e8860958', 'message': ""Add blueprint only tacker development process\n\nWe have used Request for Enhancements (RFE) bugs bring in smaller\nenhancements. However in practise this process has created a proliferation\nof bugs marked as RFEs sometime even trivial enhancements. On the other\nhand some significant RFEs doesn't get the visibilty because it is buried\nin the huge list of RFE bugs.\n\nThis patchset proposes a middle ground where launchpad blueprint will be\nused request for enhancement and still doesn't need an elaborate spec write\nup to bring the new enhancement. Instead we can use the weekly meeting, ML\nand implementation gerrit to have the discussion, approve and merge.\n\nChange-Id: I7df404ffcc1cffc05b7ef12c52cd625d61131d65\n""}, {'number': 2, 'created': '2016-08-09 06:32:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/3165e79e206eeadaf7c80aa34b06ed9083b14258', 'message': ""Add blueprint only tacker development process\n\nWe have used Request for Enhancements (RFE) bugs bring in smaller\nenhancements. However in practise this process has created a proliferation\nof bugs marked as RFEs sometime even trivial enhancements. On the other\nhand some significant RFEs doesn't get the visibilty because it is buried\nin the huge list of RFE bugs.\n\nThis patchset proposes a middle ground where launchpad blueprint will be\nused request for enhancement and still doesn't need an elaborate spec write\nup to bring the new enhancement. Instead we can use the weekly meeting, ML\nand implementation gerrit to have the discussion, approve and merge.\n\nChange-Id: I7df404ffcc1cffc05b7ef12c52cd625d61131d65\n""}, {'number': 3, 'created': '2016-08-09 06:34:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/4b10a77ac92a58f825067ba3ef2e478b04ce98f5', 'message': ""Add blueprint only tacker development process\n\nWe have used Request for Enhancements (RFE) bugs bring in smaller\nenhancements. However in practise this process has created a\nproliferation of bugs marked as RFEs sometime even trivial\nenhancements. On the other hand some significant RFEs doesn't get\nthe visibilty because it is buried in the huge list of RFE bugs.\n\nThis patchset proposes a middle ground where launchpad blueprint\nwill be used request for enhancement and still doesn't need an\nelaborate spec write up to bring the new enhancement. Instead\nwe can use the weekly meeting, ML and implementation gerrit to\nhave the discussion, approve and merge.\n\nChange-Id: I7df404ffcc1cffc05b7ef12c52cd625d61131d65\n""}, {'number': 4, 'created': '2016-08-10 01:50:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/99540734f4f4dbc1864a57b8e8bbbf59370834a5', 'message': ""Add blueprint only tacker development process\n\nWe have used Request for Enhancements (RFE) bugs bring in smaller\nenhancements. However in practise this process has created a\nproliferation of bugs marked as RFEs sometime even trivial\nenhancements. On the other hand some significant RFEs doesn't get\nthe visibilty because it is buried in the huge list of RFE bugs.\n\nThis patchset proposes a middle ground where launchpad blueprint\nwill be used request for enhancement and still doesn't need an\nelaborate spec write up to bring the new enhancement. Instead\nwe can use the weekly meeting, ML and implementation gerrit to\nhave the discussion, approve and merge.\n\nCo-Authored-By: Sridhar Ramaswamy <srics.r@gmail.com>\nCo-Authored-By: gong yong sheng <gong.yongsheng@99cloud.net>\n\nChange-Id: I7df404ffcc1cffc05b7ef12c52cd625d61131d65\n""}, {'number': 5, 'created': '2016-08-11 00:19:14.000000000', 'files': ['doc/source/policies/dev-process.rst'], 'web_link': 'https://opendev.org/openstack/tacker/commit/c81010194cff435c5d4e497d08e0654334f899ac', 'message': ""Add blueprint only tacker development process\n\nWe have used Request for Enhancements (RFE) bugs bring in smaller\nenhancements. However in practise this process has created a\nproliferation of bugs marked as RFEs sometime even trivial\nenhancements. On the other hand some significant RFEs doesn't get\nthe visibilty because it is buried in the huge list of RFE bugs.\n\nThis patchset proposes a middle ground where launchpad blueprint\nwill be used request for enhancement and still doesn't need an\nelaborate spec write up to bring the new enhancement. Instead\nwe can use the weekly meeting, ML and implementation gerrit to\nhave the discussion, approve and merge.\n\nCo-Authored-By: Sridhar Ramaswamy <srics.r@gmail.com>\nCo-Authored-By: gong yong sheng <gong.yongsheng@99cloud.net>\n\nChange-Id: I7df404ffcc1cffc05b7ef12c52cd625d61131d65\n""}]",6,352663,c81010194cff435c5d4e497d08e0654334f899ac,15,3,5,13380,,,0,"Add blueprint only tacker development process

We have used Request for Enhancements (RFE) bugs bring in smaller
enhancements. However in practise this process has created a
proliferation of bugs marked as RFEs sometime even trivial
enhancements. On the other hand some significant RFEs doesn't get
the visibilty because it is buried in the huge list of RFE bugs.

This patchset proposes a middle ground where launchpad blueprint
will be used request for enhancement and still doesn't need an
elaborate spec write up to bring the new enhancement. Instead
we can use the weekly meeting, ML and implementation gerrit to
have the discussion, approve and merge.

Co-Authored-By: Sridhar Ramaswamy <srics.r@gmail.com>
Co-Authored-By: gong yong sheng <gong.yongsheng@99cloud.net>

Change-Id: I7df404ffcc1cffc05b7ef12c52cd625d61131d65
",git fetch https://review.opendev.org/openstack/tacker refs/changes/63/352663/5 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/policies/dev-process.rst'],1,1b21cb8df5451f47e7e99af236adc174e8860958,bp/only,"Blueprint only process ====================== The developer, or an operator, can write up the requested enhancement by registering a blueprint in the launchpad [3]. * The requester and the team will have a discussion on the blueprint using weekly team meeting and/or openstack-dev ML. * Once consensus has reached to proceed the blueprint's Definition field will be marked as Approved. * Developer submits one or more patchsets to implement the enhancement. The commit message should use ""Implements: blueprint <blueprint-name>"" using the same name as the blueprint name. * Once all the patchsets are merged the blueprint will be as ""Implemented"" by the tacker core team. * The developer is expected to add a release note and a devref describing the usage of the feature and other related topics in tacker/doc/source/devref directory. This process is recommended for medium to small enhancements that can be described easily and it is relatively easy to implement in a short period of time. Request for Enhancement (RFE) Process ===================================== The developer, or an operator, can write up the requested enhancement in a Tacker launchpad [#]_ bug. * The requester need to mark the bug with ""RFE"" tag. * The bug will be in the initial ""New"" state. * The requester and team will have a discussion on the enhancement in the launchpad bug. * Once the discussion is over a tacker-core team member will acknowledge the validity of this feature enhancement by moving it to the ""Confirmed"" state. * Developer will submit patchsets to implement the enhancement using the bug-id. Note, if there are multiple patchsets Partial-Bug header should be used instead of Closes-Bug in the commit message. * Once all the patchsets are merged the bug will be moved to the ""Completed"" state. * The developer is expected to add a devref describing the usage of the feature and other related topics in tacker/doc/source/devref directory. This process is recommended for smaller enhancements that can be described easily and it is relatively easy to implement in a short period of time. ","Request for Enhancement (RFE) Process ===================================== The developer, or an operator, can write up the requested enhancement in a Tacker launchpad [#]_ bug. * The requester need to mark the bug with ""RFE"" tag. * The bug will be in the initial ""New"" state. * The requester and team will have a discussion on the enhancement in the launchpad bug. * Once the discussion is over a tacker-core team member will acknowledge the validity of this feature enhancement by moving it to the ""Confirmed"" state. * Developer will submit patchsets to implement the enhancement using the bug-id. Note, if there are multiple patchsets Partial-Bug header should be used instead of Closes-Bug in the commit message. * Once all the patchsets are merged the bug will be moved to the ""Completed"" state. * The developer is expected to add a devref describing the usage of the feature and other related topics in tacker/doc/source/devref directory. This process is recommended for smaller enhancements that can be described easily and it is relatively easy to implement in a short period of time. ",47,23
openstack%2Fironic-specs~master~I343c00bd92c63b073ee7c1308479a2a965f34ca8,openstack/ironic-specs,master,I343c00bd92c63b073ee7c1308479a2a965f34ca8,Remove pbr warnerrors in favor of sphinx check,NEW,2017-06-29 11:25:25.000000000,2017-12-18 02:54:00.000000000,,"[{'_account_id': 10239}, {'_account_id': 24925}, {'_account_id': 25254}]","[{'number': 1, 'created': '2017-06-29 11:25:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-specs/commit/13c25361617e4d0d7247748b5e0887349627fd29', 'message': 'Remove pbr warnerrors in favor of sphinx check\n\nThis change removes the now unused ""warnerrors"" setting,\nwhich is replaced by ""warning-is-error"" in sphinx\nreleases >= 1.5[0]. This also fixes any warnings\nthat came up when testing with the latest version\nof sphinx:\n\n- Redundant loading of todo extension\n- Empty man_pages config value\n\nAlso updated the requirements for pbr and sphinx to the latest\nversion(s) in requirements.txt\n\nWith this change, any doc warnings will cause the build to fail\n[0] http://lists.openstack.org/pipermail/openstack-dev/2017-March/113085.html\n\nChange-Id: I343c00bd92c63b073ee7c1308479a2a965f34ca8\n'}, {'number': 2, 'created': '2017-07-04 11:18:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-specs/commit/ed2fe867cea1646fa582ee90eb61f777b0cb9949', 'message': 'Remove pbr warnerrors in favor of sphinx check\n\nThis change removes the now unused ""warnerrors"" setting,\nwhich is replaced by ""warning-is-error"" in sphinx\nreleases >= 1.5[0]. This also fixes any warnings\nthat came up when testing with the latest version\nof sphinx:\n\n- Redundant loading of todo extension\n- Empty man_pages config value\n\nAlso updated the requirements for pbr and sphinx to the latest\nversion(s) in requirements.txt\n\nWith this change, any doc warnings will cause the build to fail\n[0] http://lists.openstack.org/pipermail/openstack-dev/2017-March/113085.html\n\nChange-Id: I343c00bd92c63b073ee7c1308479a2a965f34ca8\n'}, {'number': 3, 'created': '2017-07-10 05:04:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-specs/commit/6206e33eed6a49990091a58b0520b706fd03b3e1', 'message': 'Remove pbr warnerrors in favor of sphinx check\n\nThis change removes the now unused ""warnerrors"" setting,\nwhich is replaced by ""warning-is-error"" in sphinx\nreleases >= 1.5[0]. This also fixes any warnings\nthat came up when testing with the latest version\nof sphinx:\n\n- Redundant loading of todo extension\n- Empty man_pages config value\n\nAlso updated the requirements for pbr and sphinx to the latest\nversion(s) in requirements.txt\n\nWith this change, any doc warnings will cause the build to fail\n[0] http://lists.openstack.org/pipermail/openstack-dev/2017-March/113085.html\n\nChange-Id: I343c00bd92c63b073ee7c1308479a2a965f34ca8\n'}, {'number': 4, 'created': '2017-07-10 05:20:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-specs/commit/8fe8fc0341cc7f30c1235552f7f805d8b27f4de8', 'message': 'Remove pbr warnerrors in favor of sphinx check\n\nThis change removes the now unused ""warnerrors"" setting,\nwhich is replaced by ""warning-is-error"" in sphinx\nreleases >= 1.5[0]. This also fixes any warnings\nthat came up when testing with the latest version\nof sphinx:\n\n- Redundant loading of todo extension\n- Empty man_pages config value\n\nAlso updated the requirements for pbr and sphinx to the latest\nversion(s) in requirements.txt\n\nWith this change, any doc warnings will cause the build to fail\n[0] http://lists.openstack.org/pipermail/openstack-dev/2017-March/113085.html\n\nChange-Id: I343c00bd92c63b073ee7c1308479a2a965f34ca8\n'}, {'number': 5, 'created': '2017-07-10 05:29:35.000000000', 'files': ['requirements.txt', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/ironic-specs/commit/bbed13be53c3e5bd1037e85eadba2bd80c91ad21', 'message': 'Remove pbr warnerrors in favor of sphinx check\n\nThis change removes the now unused ""warnerrors"" setting,\nwhich is replaced by ""warning-is-error"" in sphinx\nreleases >= 1.5[0]. This also fixes any warnings\nthat came up when testing with the latest version\nof sphinx:\n\n- Redundant loading of todo extension\n- Empty man_pages config value\n\nAlso updated the requirements for pbr and sphinx to the latest\nversion(s) in requirements.txt\n\nWith this change, any doc warnings will cause the build to fail\n[0] http://lists.openstack.org/pipermail/openstack-dev/2017-March/113085.html\n\nChange-Id: I343c00bd92c63b073ee7c1308479a2a965f34ca8\n'}]",3,478879,bbed13be53c3e5bd1037e85eadba2bd80c91ad21,16,3,5,24925,,,0,"Remove pbr warnerrors in favor of sphinx check

This change removes the now unused ""warnerrors"" setting,
which is replaced by ""warning-is-error"" in sphinx
releases >= 1.5[0]. This also fixes any warnings
that came up when testing with the latest version
of sphinx:

- Redundant loading of todo extension
- Empty man_pages config value

Also updated the requirements for pbr and sphinx to the latest
version(s) in requirements.txt

With this change, any doc warnings will cause the build to fail
[0] http://lists.openstack.org/pipermail/openstack-dev/2017-March/113085.html

Change-Id: I343c00bd92c63b073ee7c1308479a2a965f34ca8
",git fetch https://review.opendev.org/openstack/ironic-specs refs/changes/79/478879/2 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'doc/source/conf.py', 'setup.cfg']",3,13c25361617e4d0d7247748b5e0887349627fd29,sphinx,builders = html warning-is-error = 1,,4,6
openstack%2Fpython-swiftclient~master~Ieb8b52134211cfa1f375efc5f382efc99f7234c9,openstack/python-swiftclient,master,Ieb8b52134211cfa1f375efc5f382efc99f7234c9,Allow arbitrary content-types for directory markers,NEW,2015-12-22 19:35:44.000000000,2017-12-18 02:53:55.000000000,,"[{'_account_id': 12050}, {'_account_id': 15343}, {'_account_id': 20395}]","[{'number': 1, 'created': '2015-12-22 19:35:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/df89306e8c939c225d4e0b87bd9b5b565da5bb5a', 'message': 'Allow arbitrary content-types for directory markers\n\nThis allows, for example, Horizon users to correctly download\npseudo-folders with something like:\n\n    swift download container --dir-marker application/pseudo-folder\n\nMultiple content-types may be specified; if an object matches any of\nthem, a directory will be made. The set of directory marker\ncontent-types will always include application/directory and\ntext/directory.\n\nThis option may also be used when uploading. The first dir-marker\nlisted will be used for new markers. Additional content-types will be\nused to recognize existing directory markers when using the --changed\noption.\n\nCloses-Bug: 1257848\nChange-Id: Ieb8b52134211cfa1f375efc5f382efc99f7234c9\n'}, {'number': 2, 'created': '2016-02-10 21:28:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/64b34f238f266944344d3862ef1d31ea157bac48', 'message': 'Allow arbitrary content-types for directory markers\n\nThis allows, for example, Horizon users to correctly download\npseudo-folders with something like:\n\n    swift download container --dir-marker application/pseudo-folder\n\nMultiple content-types may be specified; if an object matches any of\nthem, a directory will be made. The set of directory marker\ncontent-types will always include application/directory and\ntext/directory.\n\nThis option may also be used when uploading. The first dir-marker\nlisted will be used for new markers. Additional content-types will be\nused to recognize existing directory markers when using the --changed\noption.\n\nCloses-Bug: 1257848\nChange-Id: Ieb8b52134211cfa1f375efc5f382efc99f7234c9\n'}, {'number': 3, 'created': '2016-02-29 23:20:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/de380fd098fc1da270e2bac136715d1f271767f1', 'message': 'Allow arbitrary content-types for directory markers\n\nThis allows, for example, Horizon users to correctly download\npseudo-folders with something like:\n\n    swift download container --dir-marker application/pseudo-folder\n\nMultiple content-types may be specified; if an object matches any of\nthem, a directory will be made. The set of directory marker\ncontent-types will always include application/directory and\ntext/directory.\n\nThis option may also be used when uploading. The first dir-marker\nlisted will be used for new markers. Additional content-types will be\nused to recognize existing directory markers when using the --changed\noption.\n\nCloses-Bug: 1257848\nChange-Id: Ieb8b52134211cfa1f375efc5f382efc99f7234c9\n'}, {'number': 4, 'created': '2016-04-09 00:10:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/98e2341301590fad04d584743115ac11bb049275', 'message': 'Allow arbitrary content-types for directory markers\n\nThis allows, for example, Horizon users to correctly download\npseudo-folders with something like:\n\n    swift download container --dir-marker application/pseudo-folder\n\nMultiple content-types may be specified; if an object matches any of\nthem, a directory will be made. The set of directory marker\ncontent-types will always include application/directory and\ntext/directory.\n\nThis option may also be used when uploading. The first dir-marker\nlisted will be used for new markers. Additional content-types will be\nused to recognize existing directory markers when using the --changed\noption.\n\nCloses-Bug: 1257848\nChange-Id: Ieb8b52134211cfa1f375efc5f382efc99f7234c9\n'}, {'number': 5, 'created': '2016-11-18 19:50:47.000000000', 'files': ['swiftclient/shell.py', 'tests/unit/test_service.py', 'swiftclient/service.py'], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/375119cce195a4511e945c8d808a9f46572aebef', 'message': 'Allow arbitrary content-types for directory markers\n\nThis allows, for example, Horizon users to correctly download\npseudo-folders with something like:\n\n    swift download container --dir-marker application/pseudo-folder\n\nMultiple content-types may be specified; if an object matches any of\nthem, a directory will be made. The set of directory marker\ncontent-types will always include application/directory and\ntext/directory.\n\nThis option may also be used when uploading. The first dir-marker\nlisted will be used for new markers. Additional content-types will be\nused to recognize existing directory markers when using the --changed\noption.\n\nCloses-Bug: 1257848\nChange-Id: Ieb8b52134211cfa1f375efc5f382efc99f7234c9\n'}]",0,260689,375119cce195a4511e945c8d808a9f46572aebef,20,3,5,15343,,,0,"Allow arbitrary content-types for directory markers

This allows, for example, Horizon users to correctly download
pseudo-folders with something like:

    swift download container --dir-marker application/pseudo-folder

Multiple content-types may be specified; if an object matches any of
them, a directory will be made. The set of directory marker
content-types will always include application/directory and
text/directory.

This option may also be used when uploading. The first dir-marker
listed will be used for new markers. Additional content-types will be
used to recognize existing directory markers when using the --changed
option.

Closes-Bug: 1257848
Change-Id: Ieb8b52134211cfa1f375efc5f382efc99f7234c9
",git fetch https://review.opendev.org/openstack/python-swiftclient refs/changes/89/260689/1 && git format-patch -1 --stdout FETCH_HEAD,"['swiftclient/shell.py', 'tests/unit/test_service.py', 'swiftclient/service.py']",3,df89306e8c939c225d4e0b87bd9b5b565da5bb5a,254921," if 'dir_markers' in options: # Copy dir_markers so we can extend it with default markers options['dir_markers'] = tuple(options['dir_markers']) + \ KNOWN_DIR_MARKERS 'dir_markers': [], 'shuffle' : False, 'dir_markers': [] if content_type in options['dir_markers']: 'dir_marker': False, # Only for None sources 'dir_markers': [] if (ct in options['dir_markers'] and content_type=options['dir_markers'][0],"," 'shuffle' : False if content_type in KNOWN_DIR_MARKERS: 'dir_marker': False # Only for None sources if (ct in KNOWN_DIR_MARKERS and content_type=KNOWN_DIR_MARKERS[0],",132,30
openstack%2Fpython-zunclient~master~I9431bca24d758380a577b31b681a32996c3867fe,openstack/python-zunclient,master,I9431bca24d758380a577b31b681a32996c3867fe,Use 'project' instead of 'tenant' when switching to openstackclient command,NEW,2017-06-30 05:47:59.000000000,2017-12-18 02:53:45.000000000,,[{'_account_id': 22406}],"[{'number': 1, 'created': '2017-06-30 05:47:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-zunclient/commit/d63158574d516c15ef876d4b4aca4f8f7d30a266', 'message': ""Use 'project' instead of 'tenant' when switching to openstackclient command\n\nWe tend to use 'project' rather than 'tenant' when switching to openstackclient\ncommand, such as:\n\n  openstack role add --user <user> --project <project> <role>\n\nChange-Id: I9431bca24d758380a577b31b681a32996c3867fe\n""}, {'number': 2, 'created': '2017-07-10 09:53:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-zunclient/commit/4e87202d87b4000f6981b05b127a92a1a121ae1b', 'message': ""Use 'project' instead of 'tenant' when switching to openstackclient command\n\nWe tend to use 'project' rather than 'tenant' when switching to openstackclient\ncommand, such as:\n\n  openstack role add --user <user> --project <project> <role>\n\nChange-Id: I9431bca24d758380a577b31b681a32996c3867fe\n""}, {'number': 3, 'created': '2017-07-10 10:24:55.000000000', 'files': ['zunclient/tests/functional/base.py', 'zunclient/common/apiclient/base.py', 'zunclient/common/base.py', 'zunclient/common/utils.py', 'zunclient/shell.py', 'zunclient/v1/containers_shell.py', 'zunclient/common/apiclient/auth.py', 'zunclient/tests/unit/test_shell.py', 'zunclient/osc/v1/containers.py', 'zunclient/v1/containers.py'], 'web_link': 'https://opendev.org/openstack/python-zunclient/commit/139d410fcbfea63584847dc37b843a797da9acb0', 'message': ""Use 'project' instead of 'tenant' when switching to openstackclient command\n\nWe tend to use 'project' rather than 'tenant' when switching to openstackclient\ncommand, such as:\n\n  openstack role add --user <user> --project <project> <role>\n\nChange-Id: I9431bca24d758380a577b31b681a32996c3867fe\n""}]",0,479158,139d410fcbfea63584847dc37b843a797da9acb0,8,1,3,24924,,,0,"Use 'project' instead of 'tenant' when switching to openstackclient command

We tend to use 'project' rather than 'tenant' when switching to openstackclient
command, such as:

  openstack role add --user <user> --project <project> <role>

Change-Id: I9431bca24d758380a577b31b681a32996c3867fe
",git fetch https://review.opendev.org/openstack/python-zunclient refs/changes/58/479158/2 && git format-patch -1 --stdout FETCH_HEAD,"['zunclient/tests/functional/base.py', 'zunclient/common/apiclient/base.py', 'zunclient/common/base.py', 'zunclient/common/utils.py', 'zunclient/shell.py', 'zunclient/common/apiclient/auth.py', 'zunclient/v1/containers_shell.py', 'zunclient/osc/v1/containers.py', 'zunclient/v1/containers.py']",9,d63158574d516c15ef876d4b4aca4f8f7d30a266,tenant," sort_dir=None, detail=False, all_projects=False): :param all_projects: Optional, list containers in all projects sort_dir, all_projects)"," sort_dir=None, detail=False, all_tenants=False): :param all_tenants: Optional, list containers in all tenants sort_dir, all_tenants)",26,26
openstack%2Fironic-inspector~master~I508e86c0ffd17ee903cd428bdd83bb53e976da39,openstack/ironic-inspector,master,I508e86c0ffd17ee903cd428bdd83bb53e976da39,Update .gitignore,NEW,2017-07-10 09:42:26.000000000,2017-12-18 02:53:43.000000000,,[],"[{'number': 1, 'created': '2017-07-10 09:42:26.000000000', 'files': ['.gitignore'], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/2528c5c26ca0548efc29acbbbabc057e327f1023', 'message': 'Update .gitignore\n\nBecause egg* already ignores egg-info.\n\nChange-Id: I508e86c0ffd17ee903cd428bdd83bb53e976da39\n'}]",0,482077,2528c5c26ca0548efc29acbbbabc057e327f1023,3,0,1,26295,,,0,"Update .gitignore

Because egg* already ignores egg-info.

Change-Id: I508e86c0ffd17ee903cd428bdd83bb53e976da39
",git fetch https://review.opendev.org/openstack/ironic-inspector refs/changes/77/482077/1 && git format-patch -1 --stdout FETCH_HEAD,['.gitignore'],1,2528c5c26ca0548efc29acbbbabc057e327f1023,ignore_eggs,*.egg*,*.egg *.egg-info,1,2
openstack%2Frally~master~I76a6009a2b114397af32e054abe39c7d316d1f2a,openstack/rally,master,I76a6009a2b114397af32e054abe39c7d316d1f2a,Add Neutron.CreateAndList FirewallGroup scenrio,NEW,2017-07-04 06:12:45.000000000,2017-12-18 02:53:41.000000000,,"[{'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 18404}, {'_account_id': 19853}, {'_account_id': 22960}, {'_account_id': 26137}]","[{'number': 1, 'created': '2017-07-04 06:12:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/bc719e79d4cdcb7041eca3952480ce197adcac2e', 'message': 'Add Neutron.CreateAndList FirewallGroup scenrio\n\nChange-Id: I76a6009a2b114397af32e054abe39c7d316d1f2a\n'}, {'number': 2, 'created': '2017-07-04 07:38:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a9fe3bfaf591361f6febf0704c2f90c85ae32bfe', 'message': 'Add Neutron.CreateAndList FirewallGroup scenrio\n\nChange-Id: I76a6009a2b114397af32e054abe39c7d316d1f2a\n'}, {'number': 3, 'created': '2017-07-04 09:07:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ba4f447c6dbd9417937d8af9eb3029f2d5e0ef9a', 'message': 'Add Neutron.CreateAndList FirewallGroup scenrio\n\nChange-Id: I76a6009a2b114397af32e054abe39c7d316d1f2a\n'}, {'number': 4, 'created': '2017-07-04 12:13:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/fe6f4fb9c2a8a848c4c013a9638c68e745ee3b9d', 'message': 'Add Neutron.CreateAndList FirewallGroup scenrio\n\nChange-Id: I76a6009a2b114397af32e054abe39c7d316d1f2a\n'}, {'number': 5, 'created': '2017-07-05 02:15:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/cf4494203804631da507615cc554aac533de024e', 'message': 'Add Neutron.CreateAndList FirewallGroup scenrio\n\nChange-Id: I76a6009a2b114397af32e054abe39c7d316d1f2a\n'}, {'number': 6, 'created': '2017-07-06 01:18:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4286b49d1534c2ad51cca984810b96d2e48bec77', 'message': 'Add Neutron.CreateAndList FirewallGroup scenrio\n\nChange-Id: I76a6009a2b114397af32e054abe39c7d316d1f2a\n'}, {'number': 7, 'created': '2017-07-06 04:52:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/38a35eeeddbb63e3710fd316318b9f8c8d002f9e', 'message': 'Add Neutron.CreateAndList FirewallGroup scenrio\n\nChange-Id: I76a6009a2b114397af32e054abe39c7d316d1f2a\n'}, {'number': 8, 'created': '2017-07-06 10:59:12.000000000', 'files': ['samples/tasks/scenarios/neutron/create-and-list-firewall-group.yaml', 'samples/tasks/scenarios/neutron/create-and-list-firewall-group.json', 'rally/plugins/openstack/scenarios/neutron/firewall_group.py', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'tests/unit/plugins/openstack/scenarios/neutron/test_firewall_group.py', 'rally-jobs/rally-neutron.yaml', 'tests/unit/plugins/openstack/scenarios/neutron/test_utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/f2785bc27b60f9fa9fbc0da7ef5352037e24dcb0', 'message': 'Add Neutron.CreateAndList FirewallGroup scenrio\n\nChange-Id: I76a6009a2b114397af32e054abe39c7d316d1f2a\n'}]",11,480029,f2785bc27b60f9fa9fbc0da7ef5352037e24dcb0,48,6,8,26137,,,0,"Add Neutron.CreateAndList FirewallGroup scenrio

Change-Id: I76a6009a2b114397af32e054abe39c7d316d1f2a
",git fetch https://review.opendev.org/openstack/rally refs/changes/29/480029/8 && git format-patch -1 --stdout FETCH_HEAD,"['samples/tasks/scenarios/neutron/create-and-list-firewall-group.yaml', 'samples/tasks/scenarios/neutron/create-and-list-firewall-group.json', 'rally/plugins/openstack/scenarios/neutron/firewall_group.py', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'tests/unit/plugins/openstack/scenarios/neutron/test_firewall_group.py', 'tests/unit/plugins/openstack/scenarios/neutron/test_utils.py']",6,bc719e79d4cdcb7041eca3952480ce197adcac2e,," def test_create_firewall_group(self): create_firewall_group = {""firewall_group"": { ""name"": ""firewall-test""}} value = {""firewall_group"": {""name"": ""firewall-test""}} self.clients( ""neutron"").create_fwaas_firewall_group.return_value = value return_value = self.scenario._create_firewall_group( **create_firewall_group) self.assertEqual(return_value, value) self._test_atomic_action_timer(self.scenario.atomic_actions(), ""neutron.create_firewall_group"") def test_list_firewall_group(self): firewall_group_list = [] value = {""firewall_groups"": firewall_group_list} self.clients( ""neutron"").list_fwaas_firewall_groups.return_value = value return_value = self.scenario._list_firewall_group() self.assertEqual(return_value, value[""firewall_groups""]) self._test_atomic_action_timer(self.scenario.atomic_actions(), ""neutron.list_firewall_group"") ",,162,0
openstack%2Frally~master~I51d16c91195b320b1ca2db9199bbda26e1e80612,openstack/rally,master,I51d16c91195b320b1ca2db9199bbda26e1e80612,WIP: Add server console log to scenario output data,NEW,2017-07-13 01:32:50.000000000,2017-12-18 02:51:23.000000000,,[],"[{'number': 1, 'created': '2017-07-13 01:32:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/826900297d842ba60211879ea2b63c75b83faff8', 'message': 'WIP: Add server console log to scenario output data\n\nThis is very useful in case when one uses Rally for monitoring Nova.\nBasically if VM fails to start for any reason one can use this for\ndebugging purpouse.\n\nChange-Id: I51d16c91195b320b1ca2db9199bbda26e1e80612\n'}, {'number': 2, 'created': '2017-07-13 01:37:34.000000000', 'files': ['rally/plugins/openstack/scenarios/nova/utils.py', 'rally/plugins/openstack/scenarios/nova/servers.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/52618fade6616b745cc3ad22db21930e92726f49', 'message': 'WIP: Add server console log to scenario output data\n\nThis is very useful in case when one uses Rally for monitoring Nova.\nBasically if VM fails to start for any reason one can use this for\ndebugging purpouse.\n\nChange-Id: I51d16c91195b320b1ca2db9199bbda26e1e80612\n'}]",0,483150,52618fade6616b745cc3ad22db21930e92726f49,5,0,2,6172,,,0,"WIP: Add server console log to scenario output data

This is very useful in case when one uses Rally for monitoring Nova.
Basically if VM fails to start for any reason one can use this for
debugging purpouse.

Change-Id: I51d16c91195b320b1ca2db9199bbda26e1e80612
",git fetch https://review.opendev.org/openstack/rally refs/changes/50/483150/1 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/openstack/scenarios/nova/utils.py', 'rally/plugins/openstack/scenarios/nova/servers.py']",2,826900297d842ba60211879ea2b63c75b83faff8,add_console_log_to_output," def run(self, image, flavor, detailed=True, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) force_delete=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) max_sleep=0, force_delete=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) min_sleep=0, max_sleep=0, force_delete=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, force_delete=False, actions=None, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, min_sleep=0, max_sleep=0, force_delete=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, force_delete=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, auto_assign_nic=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) volume_type=None, auto_assign_nic=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, to_flavor, force_delete=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) force_delete=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) boot_server_kwargs=None, create_volume_kwargs=None, get_console_output=False): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) boot_server_kwargs=None, create_volume_kwargs=None, get_console_output=False): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, force_delete=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, force_delete=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, force_delete=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) min_sleep=0, max_sleep=0, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) min_sleep=0, max_sleep=0, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) create_volume_kwargs=None, min_sleep=0, max_sleep=0, get_console_output=False): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, from_image, to_image, flavor, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) boot_server_args=None, get_console_output=False): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, description=None, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console logs if get_console_output: self._store_server_console_output(server) auto_assign_nic=False, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server) def run(self, image, flavor, get_console_output=False, **kwargs): :param get_console_output: Add to raw output Servers console log if get_console_output: self._store_server_console_output(server)"," def run(self, image, flavor, detailed=True, **kwargs): force_delete=False, **kwargs): max_sleep=0, force_delete=False, **kwargs): min_sleep=0, max_sleep=0, force_delete=False, **kwargs): def run(self, image, flavor, force_delete=False, actions=None, **kwargs): def run(self, image, flavor, min_sleep=0, max_sleep=0, force_delete=False, **kwargs): def run(self, image, flavor, force_delete=False, **kwargs): def run(self, image, flavor, auto_assign_nic=False, **kwargs): volume_type=None, auto_assign_nic=False, **kwargs): def run(self, image, flavor, to_flavor, force_delete=False, **kwargs): force_delete=False, **kwargs): boot_server_kwargs=None, create_volume_kwargs=None): boot_server_kwargs=None, create_volume_kwargs=None): def run(self, image, flavor, force_delete=False, **kwargs): def run(self, image, flavor, force_delete=False, **kwargs): def run(self, image, flavor, force_delete=False, **kwargs): min_sleep=0, max_sleep=0, **kwargs): min_sleep=0, max_sleep=0, **kwargs): create_volume_kwargs=None, min_sleep=0, max_sleep=0): def run(self, image, flavor, **kwargs): def run(self, from_image, to_image, flavor, **kwargs): def run(self, image, flavor, **kwargs): boot_server_args=None): def run(self, image, flavor, **kwargs): def run(self, image, flavor, description=None, **kwargs): auto_assign_nic=False, **kwargs): def run(self, image, flavor, **kwargs): def run(self, image, flavor, **kwargs):",147,33
openstack%2Fpython-tripleoclient~master~I01837a9daf6f119292b5a2ffc361506925423f11,openstack/python-tripleoclient,master,I01837a9daf6f119292b5a2ffc361506925423f11,pm_user is optional for pxe_ipmitool nodes,ABANDONED,2017-09-08 03:27:44.000000000,2017-12-18 02:50:53.000000000,,"[{'_account_id': 3}, {'_account_id': 10239}, {'_account_id': 12898}]","[{'number': 1, 'created': '2017-09-08 03:27:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/dc691b1d77525ffd50d7bd12ccfe8f5bc77a9184', 'message': 'pm_user is optional for pxe_ipmitool nodes\n\nRelated-Bug: 1715787\nChange-Id: I01837a9daf6f119292b5a2ffc361506925423f11\n'}, {'number': 2, 'created': '2017-09-08 03:31:31.000000000', 'files': ['tripleoclient/tests/v1/baremetal/test_baremetal.py', 'tripleoclient/v1/baremetal.py'], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/362873966a1f8758612deacbef0cbddc937f11c1', 'message': 'pm_user is optional for pxe_ipmitool nodes\n\nBlueprint: multiarch-support\nRelated-Bug: 1715787\nChange-Id: I01837a9daf6f119292b5a2ffc361506925423f11\n'}]",5,501935,362873966a1f8758612deacbef0cbddc937f11c1,11,3,2,12898,,,0,"pm_user is optional for pxe_ipmitool nodes

Blueprint: multiarch-support
Related-Bug: 1715787
Change-Id: I01837a9daf6f119292b5a2ffc361506925423f11
",git fetch https://review.opendev.org/openstack/python-tripleoclient refs/changes/35/501935/2 && git format-patch -1 --stdout FETCH_HEAD,"['tripleoclient/tests/v1/baremetal/test_baremetal.py', 'tripleoclient/v1/baremetal.py']",2,dc691b1d77525ffd50d7bd12ccfe8f5bc77a9184,bug/1715787," # NOTE(tonyb): pm_user is optional for IPMI so don't attempt to # validate it if we're using IPMI if node['pm_type'] != ""pxe_ipmitool"": try: if len(node['pm_user']) == 0: self.log.error('ERROR: User 0 length.') self.error_count += 1 except Exception as e: self.log.error('ERROR: User does not exist: %s', e) user_flag = '' if node.get('pm_user'): user_flag = '-U %s' % node.get('pm_user') cmd = ('ipmitool -R 1 -I lanplus -H %s %s -P %s chassis ' 'status' % (node['pm_addr'], user_flag,"," try: if len(node['pm_user']) == 0: self.log.error('ERROR: User 0 length.') except Exception as e: self.log.error('ERROR: User does not exist: %s', e) self.error_count += 1 cmd = ('ipmitool -R 1 -I lanplus -H %s -U %s -P %s chassis ' 'status' % (node['pm_addr'], node['pm_user'],",41,10
openstack%2Fpython-zunclient~master~I96fcb3eb498d873e6aa3f62bbd830b7d2f7b4b90,openstack/python-zunclient,master,I96fcb3eb498d873e6aa3f62bbd830b7d2f7b4b90,delete bash_completion in subcommand,NEW,2017-05-01 02:36:30.000000000,2017-12-18 02:50:02.000000000,,[{'_account_id': 23365}],"[{'number': 1, 'created': '2017-05-01 02:36:30.000000000', 'files': ['zunclient/shell.py'], 'web_link': 'https://opendev.org/openstack/python-zunclient/commit/c0c7da882c4ece59d74f2f4321442a49d8bea6c4', 'message': 'delete bash_completion in subcommand\n\nThere are two ""completion"" in the subcommand table: bash-completion\nand bash_completion. but ""bash_completion"" is not in help information\nand it is repeated with ""bash-completion"", so delete it.\n\nChange-Id: I96fcb3eb498d873e6aa3f62bbd830b7d2f7b4b90\nCloses-Bug: #1670123\n'}]",0,461334,c0c7da882c4ece59d74f2f4321442a49d8bea6c4,4,1,1,21515,,,0,"delete bash_completion in subcommand

There are two ""completion"" in the subcommand table: bash-completion
and bash_completion. but ""bash_completion"" is not in help information
and it is repeated with ""bash-completion"", so delete it.

Change-Id: I96fcb3eb498d873e6aa3f62bbd830b7d2f7b4b90
Closes-Bug: #1670123
",git fetch https://review.opendev.org/openstack/python-zunclient refs/changes/34/461334/1 && git format-patch -1 --stdout FETCH_HEAD,['zunclient/shell.py'],1,c0c7da882c4ece59d74f2f4321442a49d8bea6c4,bug/1670123,," self._add_bash_completion_subparser(subparsers) def _add_bash_completion_subparser(self, subparsers): subparser = ( subparsers.add_parser('bash_completion', add_help=False, formatter_class=OpenStackHelpFormatter) ) self.subcommands['bash_completion'] = subparser subparser.set_defaults(func=self.do_bash_completion) commands.remove('bash_completion')",0,12
openstack%2Frally~master~I52e5b5ef044ce3b3fc2dab32764ecf7cc32c754e,openstack/rally,master,I52e5b5ef044ce3b3fc2dab32764ecf7cc32c754e,[db] Get rid of redundatn method,NEW,2017-07-17 17:16:02.000000000,2017-12-18 02:49:43.000000000,,[{'_account_id': 14817}],"[{'number': 1, 'created': '2017-07-17 17:16:02.000000000', 'files': ['rally/common/db/sqlalchemy/api.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/2c6b52953f470a2dcb3b5857eac7f4e5401f1009', 'message': '[db] Get rid of redundatn method\n\nMethod _task_workload_data_get_all is used only in one place and it is\nquite small, so there is no need to dedicate a single method to it.\n\nChange-Id: I52e5b5ef044ce3b3fc2dab32764ecf7cc32c754e\n'}]",0,484432,2c6b52953f470a2dcb3b5857eac7f4e5401f1009,4,1,1,9545,,,0,"[db] Get rid of redundatn method

Method _task_workload_data_get_all is used only in one place and it is
quite small, so there is no need to dedicate a single method to it.

Change-Id: I52e5b5ef044ce3b3fc2dab32764ecf7cc32c754e
",git fetch https://review.opendev.org/openstack/rally refs/changes/32/484432/1 && git format-patch -1 --stdout FETCH_HEAD,['rally/common/db/sqlalchemy/api.py'],1,2c6b52953f470a2dcb3b5857eac7f4e5401f1009,db_optimization," data = ( self.model_query(models.WorkloadData, session=session). filter_by(workload_uuid=workload_uuid). order_by(models.WorkloadData.chunk_order.asc())) workload_results = sorted( [raw for workload_data in data for raw in workload_data.chunk_data[""raw""]], key=lambda x: x[""timestamp""])"," def _task_workload_data_get_all(self, workload_uuid): session = get_session() with session.begin(): results = (self.model_query(models.WorkloadData, session=session). filter_by(workload_uuid=workload_uuid). order_by(models.WorkloadData.chunk_order.asc())) return sorted([raw for workload_data in results for raw in workload_data.chunk_data[""raw""]], key=lambda x: x[""timestamp""]) workload_results = self._task_workload_data_get_all(workload_uuid)",9,12
openstack%2Fironic~master~Icd12224a08e2acbc6ee86e06e45fc34c8ad0b912,openstack/ironic,master,Icd12224a08e2acbc6ee86e06e45fc34c8ad0b912,[PoC] Kubernetes for long running external tasks,NEW,2017-02-20 15:51:42.000000000,2017-12-18 02:49:16.000000000,,"[{'_account_id': 7711}, {'_account_id': 10118}, {'_account_id': 14629}, {'_account_id': 19003}, {'_account_id': 19339}]","[{'number': 1, 'created': '2017-02-20 15:51:42.000000000', 'files': ['contrib/docker/shellinabox-ipmi/Dockerfile', 'ironic/drivers/ipmi.py', 'requirements.txt', 'ironic/drivers/modules/ipmitool.py', 'ironic/tests/unit/drivers/test_ipmi.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/15a437e542130112ddfaaf90b48b0ffc7ba29301', 'message': '[PoC] Kubernetes for long running external tasks\n\nShellinabox ipmi console used in PoC.\n\nChange-Id: Icd12224a08e2acbc6ee86e06e45fc34c8ad0b912\n'}]",0,436083,15a437e542130112ddfaaf90b48b0ffc7ba29301,10,5,1,7711,,,0,"[PoC] Kubernetes for long running external tasks

Shellinabox ipmi console used in PoC.

Change-Id: Icd12224a08e2acbc6ee86e06e45fc34c8ad0b912
",git fetch https://review.opendev.org/openstack/ironic refs/changes/83/436083/1 && git format-patch -1 --stdout FETCH_HEAD,"['contrib/docker/shellinabox-ipmi/Dockerfile', 'ironic/drivers/ipmi.py', 'requirements.txt', 'ironic/drivers/modules/ipmitool.py', 'ironic/tests/unit/drivers/test_ipmi.py']",5,15a437e542130112ddfaaf90b48b0ffc7ba29301,kube-poc,," self.assertIsInstance(driver.console, ipmitool.IPMIShellinaboxConsole)",149,2
openstack%2Fironic~master~I80a11277b6c3c2d26f69aa4e0f7b4574b7b7ecf9,openstack/ironic,master,I80a11277b6c3c2d26f69aa4e0f7b4574b7b7ecf9,Fail if remote boot volume defined but not ipxe,NEW,2017-06-09 22:44:17.000000000,2017-12-18 02:49:08.000000000,,"[{'_account_id': 10118}, {'_account_id': 10206}, {'_account_id': 11076}, {'_account_id': 11655}, {'_account_id': 11929}, {'_account_id': 13689}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19339}, {'_account_id': 19686}]","[{'number': 1, 'created': '2017-06-09 22:44:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6b7943bfe8e297673999e9b86f8bf305674c071b', 'message': 'Fail if boot_from_volume enabled but not ipxe\n\nWe should fail on building pxe options if boot from volume is\nenabled but ipxe is not. This patch is a followup to 413324 and\naddresses some nits that were in the patch.\n\nChange-Id: I80a11277b6c3c2d26f69aa4e0f7b4574b7b7ecf9\n'}, {'number': 2, 'created': '2017-06-15 14:18:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/27cb260550373727cc9f9455c0998c102852511e', 'message': 'Fail if boot_from_volume enabled but not ipxe\n\nWe should fail on building pxe options if boot from volume is\nenabled but ipxe is not. This patch is a followup to the iPXE template\npatch [0] (I75869262dbfd1caa779fa21e93cdb31f193cb829) and addresses some\nnits that were in the patch.\n\n[0] https://review.openstack.org/#/c/413324/\n\nPartial-Bug: #1559691\nChange-Id: I80a11277b6c3c2d26f69aa4e0f7b4574b7b7ecf9\n'}, {'number': 3, 'created': '2017-06-15 19:20:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e13af8385358dcdbdb9821ec8265f27d62dc1265', 'message': 'Fail if boot_from_volume enabled but not ipxe\n\nWe should fail on building pxe options if boot from volume is\nenabled but ipxe is not. This patch is a followup to the iPXE template\npatch [0] (I75869262dbfd1caa779fa21e93cdb31f193cb829) and addresses some\nnits that were in the patch.\n\n[0] https://review.openstack.org/#/c/413324/\n\nPartial-Bug: #1559691\nChange-Id: I80a11277b6c3c2d26f69aa4e0f7b4574b7b7ecf9\n'}, {'number': 4, 'created': '2017-06-22 15:55:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c70d8f3e7f9ae7f9a63feca44fc1a52a27d13c34', 'message': 'Fail if boot_from_volume enabled but not ipxe\n\nWe should fail on building pxe options if boot from volume is\nenabled but ipxe is not. This patch is a followup to the iPXE template\npatch [0] and addresses some nits that were in the patch.\n\n[0] I75869262dbfd1caa779fa21e93cdb31f193cb829\n\nPartial-Bug: #1559691\nChange-Id: I80a11277b6c3c2d26f69aa4e0f7b4574b7b7ecf9\n'}, {'number': 5, 'created': '2017-06-26 17:10:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d8c5bad25aad2242c7222207199c21d8d0a04e59', 'message': 'Fail if boot_from_volume enabled but not ipxe\n\nWe should fail on building pxe options if boot from volume is\nenabled but ipxe is not. This patch is a followup to the iPXE template\npatch [0] and addresses some nits that were in the patch.\n\n[0] I75869262dbfd1caa779fa21e93cdb31f193cb829\n\nPartial-Bug: #1559691\nChange-Id: I80a11277b6c3c2d26f69aa4e0f7b4574b7b7ecf9\n'}, {'number': 6, 'created': '2017-06-26 18:03:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5fa986dab61d674283f2f96f0f63f70f62faec9b', 'message': 'Fail if boot_from_volume enabled but not ipxe\n\nWe should fail on building pxe options if boot from volume is\nenabled but ipxe is not. This patch is a followup to the iPXE template\npatch [0] and addresses some nits that were in the patch.\n\n[0] I75869262dbfd1caa779fa21e93cdb31f193cb829\n\nPartial-Bug: #1559691\nChange-Id: I80a11277b6c3c2d26f69aa4e0f7b4574b7b7ecf9\n'}, {'number': 7, 'created': '2017-07-17 15:58:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1fd424d17382312293c1b2a40fd554543ec7341a', 'message': 'Fail if boot_from_volume enabled but not ipxe\n\nWe should fail on building pxe options if boot from volume is\nenabled but ipxe is not. This patch is a followup to the iPXE template\npatch [0] and addresses some nits that were in the patch.\n\n[0] I75869262dbfd1caa779fa21e93cdb31f193cb829\n\nPartial-Bug: #1559691\nChange-Id: I80a11277b6c3c2d26f69aa4e0f7b4574b7b7ecf9\n'}, {'number': 8, 'created': '2017-07-17 16:01:35.000000000', 'files': ['ironic/drivers/modules/deploy_utils.py', 'ironic/drivers/modules/pxe.py', 'ironic/tests/unit/drivers/modules/test_pxe.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/de35f75a1cc9405b2da41795680c82aff33050ab', 'message': 'Fail if remote boot volume defined but not ipxe\n\nWe should fail on building pxe options if boot from volume is\nenabled but ipxe is not. This patch is a followup to the iPXE template\npatch [0] and addresses some nits that were in the patch.\n\n[0] I75869262dbfd1caa779fa21e93cdb31f193cb829\n\nPartial-Bug: #1559691\nChange-Id: I80a11277b6c3c2d26f69aa4e0f7b4574b7b7ecf9\n'}]",24,472856,de35f75a1cc9405b2da41795680c82aff33050ab,63,10,8,11929,,,0,"Fail if remote boot volume defined but not ipxe

We should fail on building pxe options if boot from volume is
enabled but ipxe is not. This patch is a followup to the iPXE template
patch [0] and addresses some nits that were in the patch.

[0] I75869262dbfd1caa779fa21e93cdb31f193cb829

Partial-Bug: #1559691
Change-Id: I80a11277b6c3c2d26f69aa4e0f7b4574b7b7ecf9
",git fetch https://review.opendev.org/openstack/ironic refs/changes/56/472856/7 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/drivers/modules/deploy_utils.py', 'ironic/drivers/modules/pxe.py', 'ironic/tests/unit/drivers/modules/test_pxe.py']",3,6b7943bfe8e297673999e9b86f8bf305674c071b,bug/1559691," def test__build_pxe_config_options_fails_no_ipxe_and_iscsi_boot(self): with task_manager.acquire(self.context, self.node.uuid, shared=True) as task: self.config(ipxe_enabled=False, group='pxe') task.node.driver_internal_info['boot_from_volume'] = 'fake_vol_id' self.assertRaises(exception.InvalidParameterValue, pxe._build_pxe_config_options, task, {'mock': 'pxe_info'}) ",,20,5
openstack%2Fglance_store~master~Ie85f9d6ca3a3951b0278b9cfd538705cf5c8e3c5,openstack/glance_store,master,Ie85f9d6ca3a3951b0278b9cfd538705cf5c8e3c5,WIP: Remove deprecated swift store auth opts,NEW,2017-03-22 00:26:44.000000000,2017-12-18 02:49:01.000000000,,"[{'_account_id': 5314}, {'_account_id': 11904}, {'_account_id': 14070}]","[{'number': 1, 'created': '2017-03-22 00:26:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/fcb90a1074048060319a08c5673cff39b04714c5', 'message': 'Release note for the removal of deprecated swift store auth opts\n\nThe following opts can be removed this cycle as per deprecation\npolicy:\n* swift_store_auth_version\n* swift_store_auth_address\n* swift_store_user\n* swift_store_key\nThe params from the swift conf file will be used instead. This\nis a pre-release note that talks about the steps users will have\nto take if using swift store as the backend.\n\nChange-Id: Ie85f9d6ca3a3951b0278b9cfd538705cf5c8e3c5\n'}, {'number': 2, 'created': '2017-07-14 14:15:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/ef65546b0beb36649f9a043fa9f792cf98aa5e5a', 'message': 'Remove deprecated swift store auth opts\n\nThe following opts can be removed this cycle as per deprecation\npolicy:\n* swift_store_auth_version\n* swift_store_auth_address\n* swift_store_user\n* swift_store_key\nThe params from the swift conf file will be used instead. This\npatch includes a detailed release note that talks about the steps\nusers will have to take if using swift store as the backend.\n\nCo-authored-by: Dharini Chandrasekar <dharini.chandrasekar@intel.com>\nCo-authored-by: Brian Rosmaita <rosmaita.fossdev@gmail.com>\nChange-Id: Ie85f9d6ca3a3951b0278b9cfd538705cf5c8e3c5\n'}, {'number': 3, 'created': '2017-07-14 14:19:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/95ed9f1cac8c1c582d21ef2b426ad493e2005760', 'message': 'Remove deprecated swift store auth opts\n\nThe following opts can be removed this cycle as per deprecation\npolicy:\n* swift_store_auth_version\n* swift_store_auth_address\n* swift_store_user\n* swift_store_key\nThe params from the swift conf file will be used instead. This\npatch includes a detailed release note that talks about the steps\nusers will have to take if using swift store as the backend.\n\nCo-authored-by: Dharini Chandrasekar <dharini.chandrasekar@intel.com>\nCo-authored-by: Brian Rosmaita <rosmaita.fossdev@gmail.com>\nChange-Id: Ie85f9d6ca3a3951b0278b9cfd538705cf5c8e3c5\n'}, {'number': 4, 'created': '2017-07-17 16:50:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/a0f02a45373fd52349fc3d8d1417f61e7d2ef935', 'message': 'Remove deprecated swift store auth opts\n\nThe following opts can be removed this cycle as per deprecation\npolicy:\n* swift_store_auth_version\n* swift_store_auth_address\n* swift_store_user\n* swift_store_key\nThe params from the swift conf file will be used instead. This\npatch includes a detailed release note that talks about the steps\nusers will have to take if using swift store as the backend.\n\nEliminating these options entails that the swift conf file MUST\nnow be used whenever the swift glance_store driver is used, and\nthis in turn has entailed some changes in driver logic (where the\npresence/absence of the swift conf file was used to decide the course\nof execution at some branch points).\n\nCo-authored-by: Dharini Chandrasekar <dharini.chandrasekar@intel.com>\nCo-authored-by: Brian Rosmaita <rosmaita.fossdev@gmail.com>\nChange-Id: Ie85f9d6ca3a3951b0278b9cfd538705cf5c8e3c5\n'}, {'number': 5, 'created': '2017-07-18 15:04:36.000000000', 'files': ['releasenotes/notes/use-swift-conf-file-a71d741a9c8fb1a8.yaml', 'glance_store/_drivers/swift/utils.py', 'glance_store/_drivers/swift/store.py'], 'web_link': 'https://opendev.org/openstack/glance_store/commit/1e54fb1814c7057260093a4e6952b831210b6ef5', 'message': 'WIP: Remove deprecated swift store auth opts\n\nThe following opts can be removed this cycle as per deprecation\npolicy:\n* swift_store_auth_version\n* swift_store_auth_address\n* swift_store_user\n* swift_store_key\nThe params from the swift conf file will be used instead. This\npatch includes a detailed release note that talks about the steps\nusers will have to take if using swift store as the backend.\n\nEliminating these options entails that the swift conf file MUST\nnow be used whenever the swift glance_store driver is used, and\nthis in turn has entailed some changes in driver logic (where the\npresence/absence of the swift conf file was used to decide the course\nof execution at some branch points).\n\nCo-authored-by: Dharini Chandrasekar <dharini.chandrasekar@intel.com>\nCo-authored-by: Brian Rosmaita <rosmaita.fossdev@gmail.com>\nChange-Id: Ie85f9d6ca3a3951b0278b9cfd538705cf5c8e3c5\n'}]",1,448316,1e54fb1814c7057260093a4e6952b831210b6ef5,19,3,5,21722,,,0,"WIP: Remove deprecated swift store auth opts

The following opts can be removed this cycle as per deprecation
policy:
* swift_store_auth_version
* swift_store_auth_address
* swift_store_user
* swift_store_key
The params from the swift conf file will be used instead. This
patch includes a detailed release note that talks about the steps
users will have to take if using swift store as the backend.

Eliminating these options entails that the swift conf file MUST
now be used whenever the swift glance_store driver is used, and
this in turn has entailed some changes in driver logic (where the
presence/absence of the swift conf file was used to decide the course
of execution at some branch points).

Co-authored-by: Dharini Chandrasekar <dharini.chandrasekar@intel.com>
Co-authored-by: Brian Rosmaita <rosmaita.fossdev@gmail.com>
Change-Id: Ie85f9d6ca3a3951b0278b9cfd538705cf5c8e3c5
",git fetch https://review.opendev.org/openstack/glance_store refs/changes/16/448316/5 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/notes/use-swift-conf-file-a71d741a9c8fb1a8.yaml'],1,fcb90a1074048060319a08c5673cff39b04714c5,use-swift-conf-file-remove-auth-opts,"--- deprecations: - | The swift store opts ``swift_store_auth_version``, ``swift_store_auth_address``, ``swift_store_user`` and ``swift_store_key`` have been removed this cycle in favor of specifying the same in the Swift back-end configuration file given with ``swift_store_config_file``. Please declare the params in this file and ensure you specify the reference you would like to be used with the ``default_swift_reference`` option. If using swift store in multi-tenant mode, please ensure that your ``default_swift_reference`` appropriately points to your desired set of user params to use. With this deprecation, it is necessary that your reference in the swift conf file include: * auth_address * auth_version * user * key ",,17,0
openstack%2Fswift~master~Ie467f069863bda3984cc948e3cb2cc8494491f5d,openstack/swift,master,Ie467f069863bda3984cc948e3cb2cc8494491f5d,added expirer service to list,NEW,2016-10-18 19:30:01.000000000,2017-12-18 02:47:21.000000000,,"[{'_account_id': 597}, {'_account_id': 9625}, {'_account_id': 13052}, {'_account_id': 16896}]","[{'number': 1, 'created': '2016-10-18 19:30:01.000000000', 'files': ['install-guide/source/finalize-installation-obs.rst', 'install-guide/source/finalize-installation-rdo.rst'], 'web_link': 'https://opendev.org/openstack/swift/commit/41fe66da30637887dbf6daf8b217f7387951b420', 'message': 'added expirer service to list\n\nAdded expirer service to list of services to\nrun on the storage nodes.\n\nChange-Id: Ie467f069863bda3984cc948e3cb2cc8494491f5d\nSigned-off-by: Thiago da Silva <thiago@redhat.com>\n'}]",0,388185,41fe66da30637887dbf6daf8b217f7387951b420,8,4,1,9625,,,0,"added expirer service to list

Added expirer service to list of services to
run on the storage nodes.

Change-Id: Ie467f069863bda3984cc948e3cb2cc8494491f5d
Signed-off-by: Thiago da Silva <thiago@redhat.com>
",git fetch https://review.opendev.org/openstack/swift refs/changes/85/388185/1 && git format-patch -1 --stdout FETCH_HEAD,"['install-guide/source/finalize-installation-obs.rst', 'install-guide/source/finalize-installation-rdo.rst']",2,41fe66da30637887dbf6daf8b217f7387951b420,add_expirer_service, openstack-swift-object-replicator.service openstack-swift-object-updater.service \ openstack-swift-object-expirer.service openstack-swift-object-replicator.service openstack-swift-object-updater.service \ openstack-swift-object-expirer.service, openstack-swift-object-replicator.service openstack-swift-object-updater.service openstack-swift-object-replicator.service openstack-swift-object-updater.service,8,4
openstack%2Ftacker~master~I83d3a72605753f14184181af437990ccb7caffe2,openstack/tacker,master,I83d3a72605753f14184181af437990ccb7caffe2,Part 1: Add policies to VNFs in network service,NEW,2017-02-14 07:49:40.000000000,2017-12-18 02:47:19.000000000,,"[{'_account_id': 2874}, {'_account_id': 12455}, {'_account_id': 13380}, {'_account_id': 18955}, {'_account_id': 19644}, {'_account_id': 20560}]","[{'number': 1, 'created': '2017-02-14 07:49:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/e405a48f7efbee33639baae9202e74c397707560', 'message': '[WIP] Add policies to VNFs in network service\n\nThis patch will provide templates for autoscaling and\nrespawning in network service.\n\nChange-Id: I83d3a72605753f14184181af437990ccb7caffe2\n'}, {'number': 2, 'created': '2017-02-14 09:48:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/2628a0152a5ff973720852ee3f42e6173a9a364d', 'message': '[WIP] Add policies to VNFs in network service\n\nThis patch will provide templates for autoscaling and\nrespawning in network service.\n\nChange-Id: I83d3a72605753f14184181af437990ccb7caffe2\n'}, {'number': 3, 'created': '2017-03-03 14:18:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/fa145da07749e7595af636acad3bd84ecfa91013', 'message': 'Part 1: Add policies to VNFs in network service\n\nThis patch will provide autoscaling and\nauto-healing for VNFs in network service.\n\nChange-Id: I83d3a72605753f14184181af437990ccb7caffe2\n'}, {'number': 4, 'created': '2017-03-08 08:14:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/026f60a143d51632676b79afcb4b16327ff8da00', 'message': 'Part 1: Add policies to VNFs in network service\n\nThis patch will provide autoscaling and\nauto-healing for VNFs in network service.\n\nImplement bp: #service-assurance-engine\n\nChange-Id: I83d3a72605753f14184181af437990ccb7caffe2\n'}, {'number': 5, 'created': '2017-04-07 18:29:18.000000000', 'files': ['tacker/tests/etc/samples/test-ns-vnfd2-rs.yaml', 'tacker/tests/etc/samples/test-ns-nsd-rs.yaml', 'samples/tosca-templates/nsd/sample-tosca-autoscaling-vnfd1.yaml', 'tacker/tests/etc/samples/test-nsd-vnfd1-as.yaml', 'samples/tosca-templates/nsd/sample-tosca-respawning-vnfd1.yaml', 'tacker/tests/etc/samples/test-nsd-vnfd2-rs.yaml', 'tacker/tests/etc/samples/test-ns-vnfd1-as.yaml', 'tacker/tests/etc/samples/test-nsd-vnfd1-rs.yaml', 'tacker/tests/etc/samples/test-nsd-rs.yaml', 'tacker/tests/etc/samples/test-nsd-vnfd2-as.yaml', 'tacker/tests/etc/samples/test-ns-vnfd1-rs.yaml', 'tacker/tests/functional/nfvo/test_nfvo.py', 'tacker/tests/etc/samples/test-nsd-as.yaml', 'tacker/tests/etc/samples/test-ns-vnfd2-as.yaml', 'samples/tosca-templates/nsd/sample-tosca-autoscaling-vnfd2.yaml', 'tacker/tests/etc/samples/test-ns-nsd-as.yaml', 'samples/tosca-templates/nsd/sample-tosca-respawning-vnfd2.yaml'], 'web_link': 'https://opendev.org/openstack/tacker/commit/e6798de8bea92936c414952f64c170c2364f409a', 'message': 'Part 1: Add policies to VNFs in network service\n\nThis patch will provide autoscaling and\nauto-healing for VNFs in network service.\n\nImplement bp: #service-assurance-engine\n\nChange-Id: I83d3a72605753f14184181af437990ccb7caffe2\n'}]",8,433474,e6798de8bea92936c414952f64c170c2364f409a,34,6,5,20560,,,0,"Part 1: Add policies to VNFs in network service

This patch will provide autoscaling and
auto-healing for VNFs in network service.

Implement bp: #service-assurance-engine

Change-Id: I83d3a72605753f14184181af437990ccb7caffe2
",git fetch https://review.opendev.org/openstack/tacker refs/changes/74/433474/2 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/tests/etc/samples/test-ns-vnfd2-rs.yaml', 'tacker/tests/etc/samples/test-ns-nsd-rs.yaml', 'samples/tosca-templates/nsd/sample-tosca-autoscaling-vnfd1.yaml', 'tacker/tests/etc/samples/test-nsd-vnfd1-as.yaml', 'samples/tosca-templates/nsd/sample-tosca-respawning-vnfd1.yaml', 'tacker/tests/etc/samples/test-nsd-vnfd2-rs.yaml', 'tacker/tests/etc/samples/test-ns-vnfd1-as.yaml', 'tacker/tests/etc/samples/test-nsd-vnfd1-rs.yaml', 'tacker/tests/etc/samples/test-nsd-rs.yaml', 'tacker/tests/etc/samples/test-nsd-vnfd2-as.yaml', 'tacker/tests/etc/samples/test-ns-vnfd1-rs.yaml', 'tacker/tests/functional/nfvo/test_nfvo.py', 'tacker/tests/etc/samples/test-nsd-as.yaml', 'tacker/tests/etc/samples/test-ns-vnfd2-as.yaml', 'samples/tosca-templates/nsd/sample-tosca-autoscaling-vnfd2.yaml', 'tacker/tests/etc/samples/test-ns-nsd-as.yaml', 'samples/tosca-templates/nsd/sample-tosca-respawning-vnfd2.yaml']",17,e405a48f7efbee33639baae9202e74c397707560,tacker/ns-policies,"tosca_definitions_version: tosca_simple_profile_for_nfv_1_0_0 description: Demo example node_types: tosca.nodes.nfv.VNF2: capabilities: forwarder1: type: tosca.capabilities.nfv.Forwarder topology_template: substitution_mappings: node_type: tosca.nodes.nfv.VNF2 capabilities: forwarder1: [CP21, forwarder] node_templates: VDU1: type: tosca.nodes.nfv.VDU.Tacker properties: image: cirros-0.3.4-x86_64-uec flavor: m1.tiny availability_zone: nova metadata: {metering.vnf: RG2} mgmt_driver: noop config: | param0: key1 param1: key2 CP21: type: tosca.nodes.nfv.CP.Tacker properties: management: true anti_spoofing_protection: false requirements: - virtualLink: node: VL1 - virtualBinding: node: VDU1 VDU2: type: tosca.nodes.nfv.VDU.Tacker properties: image: cirros-0.3.4-x86_64-uec flavor: m1.medium availability_zone: nova metadata: {metering.vnf: RG2} mgmt_driver: noop config: | param0: key1 param1: key2 CP22: type: tosca.nodes.nfv.CP.Tacker requirements: - virtualLink: node: VL2 - virtualBinding: node: VDU2 VL1: type: tosca.nodes.nfv.VL properties: network_name: net_mgmt vendor: Tacker VL2: type: tosca.nodes.nfv.VL properties: network_name: net0 vendor: Tacker policies: - vdu1_cpu_usage_monitoring_policy: type: tosca.policies.tacker.Alarming triggers: vdu_hcpu_usage_respawning: event_type: type: tosca.events.resource.utilization implementation: ceilometer metrics: cpu_util condition: threshold: 50 constraint: utilization greater_than 50% period: 600 evaluations: 1 method: avg comparison_operator: gt metadata: RG2 actions: [respawn] ",,1607,22
openstack%2Fzun~master~I846c20f2d0bc4c77a7fc46a8d8756cb7d5db496b,openstack/zun,master,I846c20f2d0bc4c77a7fc46a8d8756cb7d5db496b,Add resource_providers API endpoint,NEW,2017-02-15 08:11:04.000000000,2017-12-18 02:47:14.000000000,,"[{'_account_id': 11536}, {'_account_id': 12175}, {'_account_id': 16277}]","[{'number': 1, 'created': '2017-02-15 08:11:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/d04eacde7a05e0fb4c9633762a362da7848e819d', 'message': '[WIP] Add resource_providers API endpoint\n\nThis patch add API endpoint for resource_providers resource.\nIt only implements create and get functions.\n\nPartially-Implements: blueprint expose-host-capabilities\nChange-Id: I846c20f2d0bc4c77a7fc46a8d8756cb7d5db496b\n'}, {'number': 2, 'created': '2017-02-15 09:42:13.000000000', 'files': ['zun/api/controllers/v1/views/resource_providers_view.py', 'zun/api/controllers/v1/resource_providers.py', 'zun/api/controllers/v1/__init__.py', 'zun/api/controllers/v1/views/images_view.py'], 'web_link': 'https://opendev.org/openstack/zun/commit/ad5095ee987d044cf9f2385d86b2a27dd3f5fc3b', 'message': 'Add resource_providers API endpoint\n\nThis patch add API endpoint for resource_providers resource.\nIt only implements create and get functions.\n\nTODOs:\n1.Add Validation once it is clear about the request schema\n2.Add UTs\n3.Add FTs\n4.Add more API methods\n\nPartially-Implements: blueprint expose-host-capabilities\nChange-Id: I846c20f2d0bc4c77a7fc46a8d8756cb7d5db496b\n'}]",0,434126,ad5095ee987d044cf9f2385d86b2a27dd3f5fc3b,9,3,2,16277,,,0,"Add resource_providers API endpoint

This patch add API endpoint for resource_providers resource.
It only implements create and get functions.

TODOs:
1.Add Validation once it is clear about the request schema
2.Add UTs
3.Add FTs
4.Add more API methods

Partially-Implements: blueprint expose-host-capabilities
Change-Id: I846c20f2d0bc4c77a7fc46a8d8756cb7d5db496b
",git fetch https://review.opendev.org/openstack/zun refs/changes/26/434126/2 && git format-patch -1 --stdout FETCH_HEAD,"['zun/api/controllers/v1/views/resource_providers_view.py', 'zun/api/controllers/v1/__init__.py', 'zun/api/controllers/v1/resource_providers.py', 'zun/api/controllers/v1/views/images_view.py']",4,d04eacde7a05e0fb4c9633762a362da7848e819d,bp/expose-host-capabilities," 'tag',", 'tag',115,1
openstack%2Fpython-qinlingclient~master~I075141f0a2323f7863f1bf40be937d9f5f6dc46a,openstack/python-qinlingclient,master,I075141f0a2323f7863f1bf40be937d9f5f6dc46a,Delete all executions for function,MERGED,2017-12-18 01:49:44.000000000,2017-12-18 02:46:48.000000000,2017-12-18 02:46:48.000000000,"[{'_account_id': 6732}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-18 01:49:44.000000000', 'files': ['qinlingclient/osc/v1/function_execution.py'], 'web_link': 'https://opendev.org/openstack/python-qinlingclient/commit/1b8e6407aee12dca1985bb2b05cf90ae51ba213f', 'message': 'Delete all executions for function\n\nSupport to delete executions of one or more functions in CLI.\n\nChange-Id: I075141f0a2323f7863f1bf40be937d9f5f6dc46a\n'}]",0,528596,1b8e6407aee12dca1985bb2b05cf90ae51ba213f,6,2,1,6732,,,0,"Delete all executions for function

Support to delete executions of one or more functions in CLI.

Change-Id: I075141f0a2323f7863f1bf40be937d9f5f6dc46a
",git fetch https://review.opendev.org/openstack/python-qinlingclient refs/changes/96/528596/1 && git format-patch -1 --stdout FETCH_HEAD,['qinlingclient/osc/v1/function_execution.py'],1,1b8e6407aee12dca1985bb2b05cf90ae51ba213f,delete-executions-for-function," group = parser.add_mutually_exclusive_group() group.add_argument( ""--execution"", help=""ID of function execution(s)."" ) group.add_argument( ""--function"", nargs='+', help=""ID of function(s)."" if parsed_args.execution: self.delete_resources(parsed_args.execution) elif parsed_args.function: for f in parsed_args.function: execs = client.function_executions.list(function_id=f) ids = [e.id for e in execs] self.delete_resources(ids)"," parser.add_argument( 'execution', metavar='EXECUTION', help='ID of function execution(s).' self.delete_resources(parsed_args.execution)",16,5
openstack%2Fkolla-ansible~master~If84927d6df31391ebd53fe40126ee6d6faaeb10d,openstack/kolla-ansible,master,If84927d6df31391ebd53fe40126ee6d6faaeb10d,Use openvswitch firewall instead of iptables hyper driver,ABANDONED,2017-07-01 06:12:23.000000000,2017-12-18 02:46:41.000000000,,"[{'_account_id': 7488}, {'_account_id': 8157}, {'_account_id': 19316}, {'_account_id': 25945}]","[{'number': 1, 'created': '2017-07-01 06:12:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/7005272fadd444ebd605f7da0b1098916632f2c0', 'message': 'Use openvswitch firewall instead of iptables hyper driver\n\n* use openvswitch base firewall when using openvswitch driver\n* use short name for firewall_driver option\n\nChange-Id: If84927d6df31391ebd53fe40126ee6d6faaeb10d\nCloses-Bug: #1701779\n'}, {'number': 2, 'created': '2017-07-12 09:32:22.000000000', 'files': ['ansible/roles/neutron/templates/ml2_conf.ini.j2', 'releasenotes/notes/move-to-openvswitch-firewall-f9d6fdf9e37e0c16.yaml'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/9ecfe5ab4c88c31af721998a36e7e5af300dd51f', 'message': 'Use openvswitch firewall instead of iptables hyper driver\n\n* use openvswitch base firewall when using openvswitch driver\n* use short name for firewall_driver option\n\nChange-Id: If84927d6df31391ebd53fe40126ee6d6faaeb10d\nCloses-Bug: #1701779\n'}]",1,479465,9ecfe5ab4c88c31af721998a36e7e5af300dd51f,19,4,2,7488,,,0,"Use openvswitch firewall instead of iptables hyper driver

* use openvswitch base firewall when using openvswitch driver
* use short name for firewall_driver option

Change-Id: If84927d6df31391ebd53fe40126ee6d6faaeb10d
Closes-Bug: #1701779
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/65/479465/2 && git format-patch -1 --stdout FETCH_HEAD,['ansible/roles/neutron/templates/ml2_conf.ini.j2'],1,7005272fadd444ebd605f7da0b1098916632f2c0,bug/1701779,firewall_driver = openvswitchfirewall_driver = iptables,firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriverfirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver,2,2
openstack%2Fmonasca-agent~master~I0a033897d3ea6d73331088e55765d3d7bcfc8294,openstack/monasca-agent,master,I0a033897d3ea6d73331088e55765d3d7bcfc8294,[INV] POC of rootwrap in monasca-agent,NEW,2017-06-02 07:19:04.000000000,2017-12-18 02:46:05.000000000,,[],"[{'number': 1, 'created': '2017-06-02 07:19:04.000000000', 'files': ['requirements.txt', 'etc/monasca/agent/rootwrap.conf', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/e6de5f71b08963fc78d609b2c092e99920c0a8ae', 'message': '[INV] POC of rootwrap in monasca-agent\n\nChange-Id: I0a033897d3ea6d73331088e55765d3d7bcfc8294\n'}]",0,470176,e6de5f71b08963fc78d609b2c092e99920c0a8ae,4,0,1,16168,,,0,"[INV] POC of rootwrap in monasca-agent

Change-Id: I0a033897d3ea6d73331088e55765d3d7bcfc8294
",git fetch https://review.opendev.org/openstack/monasca-agent refs/changes/76/470176/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'etc/monasca/agent/rootwrap.conf', 'setup.cfg']",3,e6de5f71b08963fc78d609b2c092e99920c0a8ae,rootwrap, monasca-agent-rootwrap = oslo_rootwrap.cmd:main monasca-agent-rootwrap-daemon = oslo_rootwrap.cmd:daemon,,31,0
openstack%2Frally~master~I05a6ee7a82a42690f93e8ad6824bf2e1bec97e78,openstack/rally,master,I05a6ee7a82a42690f93e8ad6824bf2e1bec97e78,Add Distributed runner,NEW,2015-12-25 16:25:30.000000000,2017-12-18 02:45:52.000000000,,"[{'_account_id': 8491}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 22960}]","[{'number': 1, 'created': '2015-12-25 16:25:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4003b43a0430dca30e2108ba75e3c8d00c03f4db', 'message': 'Add Distributed runner\n\nChange-Id: I05a6ee7a82a42690f93e8ad6824bf2e1bec97e78\n'}, {'number': 2, 'created': '2015-12-29 13:06:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/44fdb3546e53df9d5ea65800819b5df78e642465', 'message': 'Add Distributed runner\n\nChange-Id: I05a6ee7a82a42690f93e8ad6824bf2e1bec97e78\n'}, {'number': 3, 'created': '2015-12-29 14:18:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/172c1c0dfe7a817f66783423242cac60bd302490', 'message': 'Add Distributed runner\n\nChange-Id: I05a6ee7a82a42690f93e8ad6824bf2e1bec97e78\n'}, {'number': 4, 'created': '2015-12-29 16:43:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e95fa954b2d8283ae545665639aa7b42a13b0768', 'message': 'Add Distributed runner\n\nChange-Id: I05a6ee7a82a42690f93e8ad6824bf2e1bec97e78\n'}, {'number': 5, 'created': '2016-01-11 09:48:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b59d3f106a047955e938771090a19135fdd7be62', 'message': 'Add Distributed runner\n\nChange-Id: I05a6ee7a82a42690f93e8ad6824bf2e1bec97e78\n'}, {'number': 6, 'created': '2017-04-26 23:32:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/18251741dee5657c7571b7bd2b0bb3e24cbe1630', 'message': 'Add Distributed runner\n\nChange-Id: I05a6ee7a82a42690f93e8ad6824bf2e1bec97e78\n'}, {'number': 7, 'created': '2017-06-03 08:28:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8fa98568da957a528d1cff4649817472fa0a33a0', 'message': 'Add Distributed runner\n\nChange-Id: I05a6ee7a82a42690f93e8ad6824bf2e1bec97e78\n'}, {'number': 8, 'created': '2017-07-26 03:17:51.000000000', 'files': ['rally/plugins/common/runners/distributed.py', 'tests/unit/plugins/common/runners/test_distributed.py', 'rally/common/utils.py', 'rally/task/runner.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/a007ad0637f675a2e9743c82543f3cbd626a5a9f', 'message': 'Add Distributed runner\n\nChange-Id: I05a6ee7a82a42690f93e8ad6824bf2e1bec97e78\n'}]",0,261559,a007ad0637f675a2e9743c82543f3cbd626a5a9f,27,4,8,8491,,,0,"Add Distributed runner

Change-Id: I05a6ee7a82a42690f93e8ad6824bf2e1bec97e78
",git fetch https://review.opendev.org/openstack/rally refs/changes/59/261559/8 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/common/runners/distributed.py', 'requirements.txt', 'tests/unit/plugins/common/runners/test_distributed.py', 'rally/common/utils.py', 'rally/task/runner.py']",5,4003b43a0430dca30e2108ba75e3c8d00c03f4db,runner,"class ScenarioRunnerResult(dict, rutils.ComparableMixin): def __lt__(self, other): return self[""timestamp""] < other[""timestamp""] ",class ScenarioRunnerResult(dict):,475,1
openstack%2Frally~master~Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af,openstack/rally,master,Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af,"Prepare for distribured runner, part 2",NEW,2015-12-08 14:29:05.000000000,2017-12-18 02:45:50.000000000,,"[{'_account_id': 6172}, {'_account_id': 7369}, {'_account_id': 8491}, {'_account_id': 8871}, {'_account_id': 10475}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 22960}]","[{'number': 1, 'created': '2015-12-08 14:29:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8233d31a5218a458408db3d4957579941242a592', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 2, 'created': '2015-12-08 14:51:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d89d33d2ec34d78be6e9206cd0e8e7023608d1a7', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 3, 'created': '2015-12-09 13:52:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ef1d279e417292493330eda6d08993fee79ded2a', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 4, 'created': '2015-12-10 15:17:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6cb0e4eac9f158563b2174149bc1abdb2b8384ae', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 5, 'created': '2015-12-11 10:44:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/2c2229c96fde2e3a0743750dbf797bcde71ed34a', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 6, 'created': '2015-12-14 12:08:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d960f9b5d66a1242a555d832013a70e2b85e0c41', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 7, 'created': '2015-12-14 14:54:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4f8f4b73fdf348b7cf4704ebfd4269999dac8860', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 8, 'created': '2015-12-16 15:24:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/643be98e638c75486c7d1408f11465379cc8adf3', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 9, 'created': '2015-12-17 10:56:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a5d0593b19ca03425a17a6dadf5bd387405428fd', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 10, 'created': '2015-12-17 14:09:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a43133d6ded8ca17edf83b04610aa4672d18e753', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 11, 'created': '2015-12-17 14:11:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/00aac5ee9619a636f7c77f790dfea03d9eeedca1', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 12, 'created': '2015-12-17 21:16:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/7527e3fac6e9fb7d418aedaf4a6d13e70e2ad355', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 13, 'created': '2015-12-17 21:17:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/81e6d0ad8f520cc353b6ecdd50d06d7718963357', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 14, 'created': '2015-12-18 11:00:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/09fe77a1e96ffb7819355939dc00e52e8be36b12', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 15, 'created': '2015-12-21 13:19:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/326b9a6d6d4e841661d9825575265eed12b724c8', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 16, 'created': '2015-12-25 16:24:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a06ef8e97d0c33641cf0ccd4500b8dfab7bc9b61', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 17, 'created': '2015-12-29 14:14:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6c0d6abce25b04a2d16cffe2be4c34b9d5c657cb', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 18, 'created': '2016-01-11 08:59:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/463d10879a015a4c57d8d15c07652935124aebea', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 19, 'created': '2017-04-26 23:32:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/69d9a121dc3f5dc49d848fd82096dc75ac41f257', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 20, 'created': '2017-06-03 08:28:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8844ef70250d3bc4a9bfa6a4c6e1af9744acf669', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 21, 'created': '2017-07-26 02:50:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/2f8f1a89c269d3af3c134dc4cb4db2e6e77f77fa', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}, {'number': 22, 'created': '2017-07-26 08:34:08.000000000', 'files': ['tests/unit/task/test_sla.py', 'rally/task/engine.py', 'tests/unit/plugins/common/runners/test_serial.py', 'tests/unit/plugins/common/runners/test_rps.py', 'tests/unit/plugins/common/runners/test_constant.py', 'tests/unit/task/test_engine.py', 'tests/unit/fakes.py', 'tests/unit/task/test_runner.py', 'rally/task/sla.py', 'rally/task/runner.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/c501f17f4d6ec0f29c0444ee8b1d3212ffbad58e', 'message': 'Prepare for distribured runner, part 2\n\nRunner now aggregates SLA data.\nTaskEngine retrieves SLAChecker instances\nfrom Runner, merges and analises them.\n\nChange-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af\n'}]",33,254780,c501f17f4d6ec0f29c0444ee8b1d3212ffbad58e,93,8,22,8491,,,0,"Prepare for distribured runner, part 2

Runner now aggregates SLA data.
TaskEngine retrieves SLAChecker instances
from Runner, merges and analises them.

Change-Id: Ia0520892bce7f9d4a04cdb8842cac9ffecbad2af
",git fetch https://review.opendev.org/openstack/rally refs/changes/80/254780/9 && git format-patch -1 --stdout FETCH_HEAD,"['rally/task/engine.py', 'rally/task/runner.py']",2,8233d31a5218a458408db3d4957579941242a592,runner," def __init__(self, task, config, sla, batch_size=0): self.sla_config = sla self.sla_checker = sla.SLAChecker(self.sla_config) self.sla_checker.add_iteration(r) "," def __init__(self, task, config, batch_size=0):",9,3
openstack%2Fpython-solumclient~master~I1c44395a0cd9fe9a41cf5e9eb3f6aea7be2da439,openstack/python-solumclient,master,I1c44395a0cd9fe9a41cf5e9eb3f6aea7be2da439,Stop using deprecated 'message' attribute in Exception,NEW,2017-07-25 06:01:51.000000000,2017-12-18 02:44:58.000000000,,[{'_account_id': 14107}],"[{'number': 1, 'created': '2017-07-25 06:01:51.000000000', 'files': ['solumclient/tests/common/test_exc.py'], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/9e0467efa0ef4e27c55a6e6ac81b6c2f5a086d79', 'message': ""Stop using deprecated 'message' attribute in Exception\n\nThe 'message' attribute has been deprecated and removed\nfrom Python3.\nFor more details, please check:\nhttps://www.python.org/dev/peps/pep-0352/\n\nChange-Id: I1c44395a0cd9fe9a41cf5e9eb3f6aea7be2da439\n""}]",0,486883,9e0467efa0ef4e27c55a6e6ac81b6c2f5a086d79,4,1,1,24924,,,0,"Stop using deprecated 'message' attribute in Exception

The 'message' attribute has been deprecated and removed
from Python3.
For more details, please check:
https://www.python.org/dev/peps/pep-0352/

Change-Id: I1c44395a0cd9fe9a41cf5e9eb3f6aea7be2da439
",git fetch https://review.opendev.org/openstack/python-solumclient refs/changes/83/486883/1 && git format-patch -1 --stdout FETCH_HEAD,['solumclient/tests/common/test_exc.py'],1,9e0467efa0ef4e27c55a6e6ac81b6c2f5a086d79,message,"import six self.assertEqual(json_data[""faultstring""], six.text_type(ex))"," self.assertEqual(json_data[""faultstring""], ex.message)",3,1
openstack%2Fironic~master~Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d,openstack/ironic,master,Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d,DNM: TEST ironic with wsgi,NEW,2017-02-23 09:27:28.000000000,2017-12-18 02:44:26.000000000,,"[{'_account_id': 9542}, {'_account_id': 10118}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 23330}]","[{'number': 1, 'created': '2017-02-23 09:27:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ccc49ed936df6ce655291adcbc0a740dfec5ffc2', 'message': 'WIP: TEST GRENADE DEV\n\nTest for multinode devstack grenade resolve\n\nDevstack-gate changes:\n\nGrenade changes:\nDepends-On: I2a19cc8e5cc60ef076cfea335c7704cceaa561b0\n\nChange -Id: Ic967ac9c3b9dd7c512134588b5660fda89b287e4\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 2, 'created': '2017-02-23 11:31:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/721ef34d9cb6c0dd470c503af96bee58752ec17e', 'message': 'WIP: TEST ironic\n\nTest for multinode devstack grenade resolve\n\nDevstack-gate changes:\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 3, 'created': '2017-03-01 09:37:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/73b6c95578c34f1617593dd08cd7347a4db2aeef', 'message': 'WIP: TEST ironic with SSL\n\nTest SSL in devstack\n\nDevstack-gate changes:\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\nIronic-change:\nDepends-On: I9c5ad56e1acd292ff0f9cc9b460125fc420abda5\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 4, 'created': '2017-03-01 09:42:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2152568615e503c25c599d4731064158985b9ea4', 'message': 'WIP: TEST ironic with SSL\n\nTest SSL in devstack\n\nDevstack-gate changes:\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\nIronic-change:\nDepends-On: I9c5ad56e1acd292ff0f9cc9b460125fc420abda5\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 5, 'created': '2017-03-01 09:54:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fe0ec95508d9364768e8f7ce16a9250023811d2a', 'message': 'WIP: TEST ironic with SSL\n\nTest SSL in devstack\n\nDevstack-gate changes:\nDepends-On: Ide3bcc2e58c90a5e6e2380e5ebc02096b447627e\n\nIronic-change:\nDepends-On: I9c5ad56e1acd292ff0f9cc9b460125fc420abda5\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 6, 'created': '2017-03-06 16:50:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f8b986c8466415307a67142fea43ed016d828de6', 'message': ""DNM: TEST ironic with wsgi\n\nTest Ironic with WSGi\nIt's enable $IRONIC_USE_MOD_WSGI\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d""}, {'number': 7, 'created': '2017-03-06 17:12:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bd7448dc5a558046ff43025be43ca63bd16b1e46', 'message': ""DNM: TEST ironic with wsgi\n\nTest Ironic with WSGi\nIt's enable $IRONIC_USE_MOD_WSGI\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d""}, {'number': 8, 'created': '2017-03-07 05:12:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/82b7a3a93713ecd61515cd2fe4cde41562343fbc', 'message': ""DNM: TEST ironic with wsgi\n\nTest Ironic with WSGi\nIt's enable $IRONIC_USE_MOD_WSGI\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d""}, {'number': 9, 'created': '2017-03-07 10:31:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/90c791480b8a74e95ac396b437f8f08a0d28e347', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\nRelated-Bug: #1513005\n'}, {'number': 10, 'created': '2017-03-09 08:29:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cc169cb60e27aee69c2fa97b0b1f01b43552ed6c', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\nRelated-Bug: #1513005\n'}, {'number': 11, 'created': '2017-03-09 08:51:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6083ed1945bc840b1ec659eb9f0e105cb776ca55', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\nRelated-Bug: #1513005\n'}, {'number': 12, 'created': '2017-03-09 10:57:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1e4ed3be97b61bcfd8d28226f07222da11ce5997', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\nRelated-Bug: #1513005\n'}, {'number': 13, 'created': '2017-03-09 11:11:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/01c1c47c6dc39e8f31103ca24d85d8e0a6befe4e', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\nRelated-Bug: #1513005\n'}, {'number': 14, 'created': '2017-03-09 14:56:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a2b6d9ab671956580b00dbb8f31aa922c27bcfcd', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\nRelated-Bug: #1513005\n'}, {'number': 15, 'created': '2017-03-10 15:58:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a9120181400a4f5b678e094cd7db3a3c32aec160', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\nRelated-Bug: #1513005\n'}, {'number': 16, 'created': '2017-03-13 08:08:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1b4bf7cd43a1f03e2afc753f57b9f08190dffac2', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\nRelated-Bug: #1513005\n'}, {'number': 17, 'created': '2017-03-13 10:18:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1da2f2a8e9d5ce3af790b8db4fbe9647c852a933', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\nRelated-Bug: #1513005\n'}, {'number': 18, 'created': '2017-03-13 10:25:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/16254c8eac034f2f9278435f7edc7551495173d7', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 19, 'created': '2017-03-13 10:55:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/df6be004ba4a187e888b347f045346f51adb622b', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 20, 'created': '2017-03-13 11:37:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/be2f2d2bc5743238952624df38c4896a65871cbb', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 21, 'created': '2017-03-13 12:38:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b81a85cf689007dda017791aa1c324ed58f37db7', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 22, 'created': '2017-03-13 16:27:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9b769c7184955cd8dec290af1d07136bfe0e35b5', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 23, 'created': '2017-03-13 19:06:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ff73dce4c4c2428cd53d83683f25238461b84d38', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 24, 'created': '2017-03-13 21:08:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5acfc5291e8f6f88ee2af7d48c9c622df1073537', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 25, 'created': '2017-03-13 22:12:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bff7ea49b8b803fea22d82cdb92a6f60455f6920', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 26, 'created': '2017-03-13 22:30:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d2b84f788e07f75afa852d88235acce072a225a8', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 27, 'created': '2017-03-14 12:07:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/451f79c3b45be9130fb3074547b95b45d5d36dc3', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 28, 'created': '2017-03-14 14:43:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/78f0b01ca0008bdf33fdb930065c067f98c0c0a7', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 29, 'created': '2017-03-17 08:01:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/39fcee8b1130ddf8134ffd061a3fb7a809ffb146', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 30, 'created': '2017-03-27 07:45:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1ac646e9d1775e52d6aba99a7314c7e469788e26', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 31, 'created': '2017-03-27 09:43:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/985ee45eb0e9ed84a13809944908c0b74086d33e', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 32, 'created': '2017-03-27 11:39:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ff3818f31950b48c6ee6e29906295fc3dd0a14f1', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 33, 'created': '2017-04-04 10:15:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d179585cba4d464adaf1898d8b34305190919618', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 34, 'created': '2017-04-04 14:43:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/271f4b7e0cbaff76df0e63850be4835a36768258', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 35, 'created': '2017-04-05 11:11:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/27f794a766f47f273065e82c16c1d160c850dd8c', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 36, 'created': '2017-04-09 21:05:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/73b67941378b0dc8ffa6347a66822d7a3a5239c4', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 37, 'created': '2017-04-11 10:22:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/21de5b57fe6045d998ef3cc1f1eaf1120e1d6cc6', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 38, 'created': '2017-04-18 08:04:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/345f49737f423ec7b4d740baff15087bfe5ac6c6', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}, {'number': 39, 'created': '2017-07-27 16:37:05.000000000', 'files': ['devstack/lib/ironic'], 'web_link': 'https://opendev.org/openstack/ironic/commit/e0d645cf32b0ca58ae99fcc497f19a13a7c4275c', 'message': 'DNM: TEST ironic with wsgi\n\nTest Ironic with WSGI, force running ironic-api under Apache.\n\nChange-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d\n'}]",0,437299,e0d645cf32b0ca58ae99fcc497f19a13a7c4275c,187,7,39,23330,,,0,"DNM: TEST ironic with wsgi

Test Ironic with WSGI, force running ironic-api under Apache.

Change-Id: Ifb448e96d07e7bb2cb0c41604a80631a7e0e132d
",git fetch https://review.opendev.org/openstack/ironic refs/changes/99/437299/32 && git format-patch -1 --stdout FETCH_HEAD,['3'],1,ccc49ed936df6ce655291adcbc0a740dfec5ffc2,rebase_wsgi,,,0,0
openstack%2Fpython-swiftclient~master~Ia1015f31c76c25ec7c3b500c3e58deec93a4795d,openstack/python-swiftclient,master,Ia1015f31c76c25ec7c3b500c3e58deec93a4795d,Support uploading to an object in swift from stdin,NEW,2015-12-18 12:22:19.000000000,2017-12-18 02:44:02.000000000,,"[{'_account_id': 455}, {'_account_id': 9216}, {'_account_id': 11373}, {'_account_id': 12279}, {'_account_id': 12965}, {'_account_id': 15343}, {'_account_id': 17363}, {'_account_id': 18738}, {'_account_id': 20403}]","[{'number': 1, 'created': '2015-12-18 12:22:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/63d9ab653ff9377fee24f548f3aff38232abd1b6', 'message': 'Support uploading to an object in swift from stdin\nCloses-bug: #1521342\n\nChange-Id: Ia1015f31c76c25ec7c3b500c3e58deec93a4795d\n'}, {'number': 2, 'created': '2015-12-18 12:38:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/34ea0c31ed843d4e170eebceab31ee088e303b05', 'message': 'Support uploading to an object in swift from stdin\nCloses-bug: #1521342\n\nChange-Id: Ia1015f31c76c25ec7c3b500c3e58deec93a4795d\n'}, {'number': 3, 'created': '2015-12-30 05:26:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/c3b2c59be981f6c6b103ce7f6f638996b035af99', 'message': 'Support uploading to an object in swift from stdin\nupdate code and add unittest\nCloses-bug: #1521342\n\nChange-Id: Ia1015f31c76c25ec7c3b500c3e58deec93a4795d\n'}, {'number': 4, 'created': '2015-12-31 06:03:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/bd391b3f6204b50102e40e3d850440937c946245', 'message': 'Support uploading to an object in swift from stdin\nupdate code and add unittest\nuse mkstemp instead of NamedTemporaryFile, upadte unittest\nCloses-bug: #1521342\n\nChange-Id: Ia1015f31c76c25ec7c3b500c3e58deec93a4795d\n'}, {'number': 5, 'created': '2016-01-08 02:16:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/125b01f2ffc689f97cbfce5007d40a48139dcd9b', 'message': 'Support uploading to an object in swift from stdin\nupdate code and add unittest\nuse mkstemp instead of NamedTemporaryFile, upadte unittest\nCloses-bug: #1521342\n\nChange-Id: Ia1015f31c76c25ec7c3b500c3e58deec93a4795d\n'}, {'number': 6, 'created': '2016-01-14 06:44:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/7fcc2f0a56535741d5c3460d0736dc359175f986', 'message': 'Support uploading to an object in swift from stdin\nupdate code and add unittest\nuse mkstemp instead of NamedTemporaryFile, upadte unittest\nCloses-bug: #1521342\n\nChange-Id: Ia1015f31c76c25ec7c3b500c3e58deec93a4795d\n'}, {'number': 7, 'created': '2016-01-19 06:40:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/35db0d04f2f70df533ef8c8277e24cd0d97a5a52', 'message': 'Support uploading to an object in swift from stdin\nupdate code and add unittest\nuse mkstemp instead of NamedTemporaryFile, upadte unittest\nCloses-bug: #1521342\n\nChange-Id: Ia1015f31c76c25ec7c3b500c3e58deec93a4795d\n'}, {'number': 8, 'created': '2016-01-25 05:08:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/a0ee76e4a31dc3c8204e6554818df5c46fd69724', 'message': 'Support uploading to an object in swift from stdin\nupdate code and add unittest\nuse mkstemp instead of NamedTemporaryFile, upadte unittest\nCloses-bug: #1521342\n\nChange-Id: Ia1015f31c76c25ec7c3b500c3e58deec93a4795d\n'}, {'number': 9, 'created': '2016-02-24 16:58:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/70bc210ff6e5d28d67286a2a7a0a506b8e093806', 'message': 'Support uploading to an object in swift from stdin\n\nthis patch is to support uploading an object inswift from stdin\nexample:\ncat foo | swift upload cont1 --object-name foo\n\n--object-name must be used\nif not,\nswift@swift:~$ cat foo |swift upload cont1\nobject-name must be used when uploading object from stdin\n\nCloses-bug: #1521342\n\nChange-Id: Ia1015f31c76c25ec7c3b500c3e58deec93a4795d\n'}, {'number': 10, 'created': '2016-02-25 00:16:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/27277d0983398eb970071d18eb8384380210ea8c', 'message': 'Support uploading to an object in swift from stdin\n\nthis patch is to support uploading an object inswift from stdin\nexample:\ncat foo | swift upload cont1 --object-name foo\n\n--object-name must be used\nif not,\nswift@swift:~$ cat foo |swift upload cont1\nobject-name must be used when uploading object from stdin\n\nCloses-bug: #1521342\n\nChange-Id: Ia1015f31c76c25ec7c3b500c3e58deec93a4795d\n'}, {'number': 11, 'created': '2016-04-09 09:37:41.000000000', 'files': ['swiftclient/shell.py', 'tests/unit/test_shell.py'], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/1a5b03124f7e79a95d1f73e4aeb761bf5fc85a56', 'message': 'Support uploading to an object in swift from stdin\n\nthis patch is to support uploading an object inswift from stdin\nexample:\ncat foo | swift upload cont1 --object-name foo\n\n--object-name must be used\nif not,\nswift@swift:~$ cat foo |swift upload cont1\nobject-name must be used when uploading object from stdin\n\nCloses-bug: #1521342\n\nChange-Id: Ia1015f31c76c25ec7c3b500c3e58deec93a4795d\n'}]",6,259410,1a5b03124f7e79a95d1f73e4aeb761bf5fc85a56,53,9,11,11373,,,0,"Support uploading to an object in swift from stdin

this patch is to support uploading an object inswift from stdin
example:
cat foo | swift upload cont1 --object-name foo

--object-name must be used
if not,
swift@swift:~$ cat foo |swift upload cont1
object-name must be used when uploading object from stdin

Closes-bug: #1521342

Change-Id: Ia1015f31c76c25ec7c3b500c3e58deec93a4795d
",git fetch https://review.opendev.org/openstack/python-swiftclient refs/changes/10/259410/1 && git format-patch -1 --stdout FETCH_HEAD,['swiftclient/shell.py'],1,63d9ab653ff9377fee24f548f3aff38232abd1b6,bug/1521342,"#!/usr/bin/python -u # Copyright (c) 2010-2012 OpenStack, LLC. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. from __future__ import print_function, unicode_literals import logging import signal import socket import sys import time import os from optparse import OptionParser, OptionGroup, SUPPRESS_HELP from os import environ, walk, _exit as os_exit from os.path import isfile, isdir, join from six import text_type from sys import argv as sys_argv, exit, stderr from time import gmtime, strftime from swiftclient import RequestException from swiftclient.utils import config_true_value, generate_temp_url, prt_bytes from swiftclient.multithreading import OutputManager from swiftclient.exceptions import ClientException from swiftclient import __version__ as client_version from swiftclient.service import SwiftService, SwiftError, \ SwiftUploadObject, get_conn from swiftclient.command_helpers import print_account_stats, \ print_container_stats, print_object_stats try: from shlex import quote as sh_quote except ImportError: from pipes import quote as sh_quote BASENAME = 'swift' commands = ('delete', 'download', 'list', 'post', 'stat', 'upload', 'capabilities', 'info', 'tempurl', 'auth') def immediate_exit(signum, frame): stderr.write("" Aborted\n"") os_exit(2) st_delete_options = '''[--all] [--leave-segments] [--object-threads <threads>] [--container-threads <threads>] [<container>] [<object>] [...] ''' st_delete_help = ''' Delete a container or objects within a container. Positional arguments: [<container>] Name of container to delete from. [<object>] Name of object to delete. Specify multiple times for multiple objects. Optional arguments: -a, --all Delete all containers and objects. --leave-segments Do not delete segments of manifest objects. --object-threads <threads> Number of threads to use for deleting objects. Default is 10. --container-threads <threads> Number of threads to use for deleting containers. Default is 10. '''.strip(""\n"") def st_delete(parser, args, output_manager): parser.add_option( '-a', '--all', action='store_true', dest='yes_all', default=False, help='Delete all containers and objects.') parser.add_option( '', '--leave-segments', action='store_true', dest='leave_segments', default=False, help='Do not delete segments of manifest objects.') parser.add_option( '', '--object-threads', type=int, default=10, help='Number of threads to use for deleting objects. ' 'Default is 10.') parser.add_option( '', '--container-threads', type=int, default=10, help='Number of threads to use for deleting containers. ' 'Default is 10.') (options, args) = parse_args(parser, args) args = args[1:] if (not args and not options.yes_all) or (args and options.yes_all): output_manager.error('Usage: %s delete %s\n%s', BASENAME, st_delete_options, st_delete_help) return _opts = vars(options) _opts['object_dd_threads'] = options.object_threads with SwiftService(options=_opts) as swift: try: if not args: del_iter = swift.delete() else: container = args[0] if '/' in container: output_manager.error( 'WARNING: / in container name; you ' ""might have meant '%s' instead of '%s'."" % (container.replace('/', ' ', 1), container) ) return objects = args[1:] if objects: del_iter = swift.delete(container=container, objects=objects) else: del_iter = swift.delete(container=container) for r in del_iter: c = r.get('container', '') o = r.get('object', '') a = r.get('attempts') if r['success']: if options.verbose: a = ' [after {0} attempts]'.format(a) if a > 1 else '' if r['action'] == 'delete_object': if options.yes_all: p = '{0}/{1}'.format(c, o) else: p = o elif r['action'] == 'delete_segment': p = '{0}/{1}'.format(c, o) elif r['action'] == 'delete_container': p = c output_manager.print_msg('{0}{1}'.format(p, a)) else: p = '{0}/{1}'.format(c, o) if o else c output_manager.error('Error Deleting: {0}: {1}' .format(p, r['error'])) except SwiftError as err: output_manager.error(err.value) st_download_options = '''[--all] [--marker] [--prefix <prefix>] [--output <out_file>] [--output-dir <out_directory>] [--object-threads <threads>] [--container-threads <threads>] [--no-download] [--skip-identical] [--remove-prefix] [--header <header:value>] [--no-shuffle] <container> <object> ''' st_download_help = ''' Download objects from containers. Positional arguments: <container> Name of container to download from. To download a whole account, omit this and specify --all. <object> Name of object to download. Specify multiple times for multiple objects. Omit this to download all objects from the container. Optional arguments: -a, --all Indicates that you really want to download everything in the account. -m, --marker Marker to use when starting a container or account download. -p, --prefix <prefix> Only download items beginning with <prefix> -r, --remove-prefix An optional flag for --prefix <prefix>, use this option to download items without <prefix> -o, --output <out_file> For a single file download, stream the output to <out_file>. Specifying ""-"" as <out_file> will redirect to stdout. -D, --output-dir <out_directory> An optional directory to which to store objects. By default, all objects are recreated in the current directory. --object-threads <threads> Number of threads to use for downloading objects. Default is 10. --container-threads <threads> Number of threads to use for downloading containers. Default is 10. --no-download Perform download(s), but don't actually write anything to disk. -H, --header <header:value> Adds a customized request header to the query, like ""Range"" or ""If-Match"". This option may be repeated. Example --header ""content-type:text/plain"" --skip-identical Skip downloading files that are identical on both sides. --no-shuffle By default, when downloading a complete account or container, download order is randomised in order to to reduce the load on individual drives when multiple clients are executed simultaneously to download the same set of objects (e.g. a nightly automated download script to multiple servers). Enable this option to submit download jobs to the thread pool in the order they are listed in the object store. '''.strip(""\n"") def st_download(parser, args, output_manager): parser.add_option( '-a', '--all', action='store_true', dest='yes_all', default=False, help='Indicates that you really want to download ' 'everything in the account.') parser.add_option( '-m', '--marker', dest='marker', default='', help='Marker to use when starting a container or ' 'account download.') parser.add_option( '-p', '--prefix', dest='prefix', help='Only download items beginning with the <prefix>.') parser.add_option( '-o', '--output', dest='out_file', help='For a single ' 'download, stream the output to <out_file>. ' 'Specifying ""-"" as <out_file> will redirect to stdout.') parser.add_option( '-D', '--output-dir', dest='out_directory', help='An optional directory to which to store objects. ' 'By default, all objects are recreated in the current directory.') parser.add_option( '-r', '--remove-prefix', action='store_true', dest='remove_prefix', default=False, help='An optional flag for --prefix <prefix>, ' 'use this option to download items without <prefix>.') parser.add_option( '', '--object-threads', type=int, default=10, help='Number of threads to use for downloading objects. ' 'Default is 10.') parser.add_option( '', '--container-threads', type=int, default=10, help='Number of threads to use for downloading containers. ' 'Default is 10.') parser.add_option( '', '--no-download', action='store_true', default=False, help=""Perform download(s), but don't actually write anything to disk."") parser.add_option( '-H', '--header', action='append', dest='header', default=[], help='Adds a customized request header to the query, like ""Range"" or ' '""If-Match"". This option may be repeated. ' 'Example: --header ""content-type:text/plain""') parser.add_option( '--skip-identical', action='store_true', dest='skip_identical', default=False, help='Skip downloading files that are identical on ' 'both sides.') parser.add_option( '--no-shuffle', action='store_false', dest='shuffle', default=True, help='By default, download order is randomised in order ' 'to reduce the load on individual drives when multiple clients are ' 'executed simultaneously to download the same set of objects (e.g. a ' 'nightly automated download script to multiple servers). Enable this ' 'option to submit download jobs to the thread pool in the order they ' 'are listed in the object store.') (options, args) = parse_args(parser, args) args = args[1:] if options.out_file == '-': options.verbose = 0 if options.out_file and len(args) != 2: exit('-o option only allowed for single file downloads') if not options.prefix: options.remove_prefix = False if options.out_directory and len(args) == 2: exit('Please use -o option for single file downloads and renames') if (not args and not options.yes_all) or (args and options.yes_all): output_manager.error('Usage: %s download %s\n%s', BASENAME, st_download_options, st_download_help) return _opts = vars(options) _opts['object_dd_threads'] = options.object_threads with SwiftService(options=_opts) as swift: try: if not args: down_iter = swift.download() else: container = args[0] if '/' in container: output_manager.error( 'WARNING: / in container name; you ' ""might have meant '%s' instead of '%s'."" % (container.replace('/', ' ', 1), container) ) return objects = args[1:] if not objects: down_iter = swift.download(container) else: down_iter = swift.download(container, objects) for down in down_iter: if options.out_file == '-' and 'contents' in down: contents = down['contents'] for chunk in contents: output_manager.print_raw(chunk) else: if down['success']: if options.verbose: start_time = down['start_time'] headers_receipt = \ down['headers_receipt'] - start_time auth_time = down['auth_end_time'] - start_time finish_time = down['finish_time'] read_length = down['read_length'] attempts = down['attempts'] total_time = finish_time - start_time down_time = total_time - auth_time _mega = 1000000 if down['pseudodir']: time_str = ( 'auth %.3fs, headers %.3fs, total %.3fs, ' 'pseudo' % ( auth_time, headers_receipt, total_time ) ) else: speed = float(read_length) / down_time / _mega time_str = ( 'auth %.3fs, headers %.3fs, total %.3fs, ' '%.3f MB/s' % ( auth_time, headers_receipt, total_time, speed ) ) path = down['path'] if attempts > 1: output_manager.print_msg( '%s [%s after %d attempts]', path, time_str, attempts ) else: output_manager.print_msg( '%s [%s]', path, time_str ) else: error = down['error'] path = down['path'] container = down['container'] obj = down['object'] if isinstance(error, ClientException): if error.http_status == 304 and \ options.skip_identical: output_manager.print_msg( ""Skipped identical file '%s'"", path) continue if error.http_status == 404: output_manager.error( ""Object '%s/%s' not found"", container, obj) continue output_manager.error( ""Error downloading object '%s/%s': %s"", container, obj, error) except SwiftError as e: output_manager.error(e.value) except Exception as e: output_manager.error(e) st_list_options = '''[--long] [--lh] [--totals] [--prefix <prefix>] [--delimiter <delimiter>] [container] ''' st_list_help = ''' Lists the containers for the account or the objects for a container. Positional arguments: [container] Name of container to list object in. Optional arguments: -l, --long Long listing format, similar to ls -l. --lh Report sizes in human readable format similar to ls -lh. -t, --totals Used with -l or --lh, only report totals. -p <prefix>, --prefix <prefix> Only list items beginning with the prefix. -d <delim>, --delimiter <delim> Roll up items with the given delimiter. For containers only. See OpenStack Swift API documentation for what this means. '''.strip('\n') def st_list(parser, args, output_manager): def _print_stats(options, stats): total_count = total_bytes = 0 container = stats.get(""container"", None) for item in stats[""listing""]: item_name = item.get('name') if not options.long and not options.human: output_manager.print_msg(item.get('name', item.get('subdir'))) else: if not container: # listing containers item_bytes = item.get('bytes') byte_str = prt_bytes(item_bytes, options.human) count = item.get('count') total_count += count try: meta = item.get('meta') utc = gmtime(float(meta.get('x-timestamp'))) datestamp = strftime('%Y-%m-%d %H:%M:%S', utc) except TypeError: datestamp = '????-??-?? ??:??:??' if not options.totals: output_manager.print_msg( ""%5s %s %s %s"", count, byte_str, datestamp, item_name) else: # list container contents subdir = item.get('subdir') content_type = item.get('content_type') if subdir is None: item_bytes = item.get('bytes') byte_str = prt_bytes(item_bytes, options.human) date, xtime = item.get('last_modified').split('T') xtime = xtime.split('.')[0] else: item_bytes = 0 byte_str = prt_bytes(item_bytes, options.human) date = xtime = '' item_name = subdir if not options.totals: output_manager.print_msg( ""%s %10s %8s %24s %s"", byte_str, date, xtime, content_type, item_name) total_bytes += item_bytes # report totals if options.long or options.human: if not container: output_manager.print_msg( ""%5s %s"", prt_bytes(total_count, True), prt_bytes(total_bytes, options.human)) else: output_manager.print_msg( prt_bytes(total_bytes, options.human)) parser.add_option( '-l', '--long', dest='long', action='store_true', default=False, help='Long listing format, similar to ls -l.') parser.add_option( '--lh', dest='human', action='store_true', default=False, help='Report sizes in human readable format, ' ""similar to ls -lh."") parser.add_option( '-t', '--totals', dest='totals', help='used with -l or --lh, only report totals.', action='store_true', default=False) parser.add_option( '-p', '--prefix', dest='prefix', help='Only list items beginning with the prefix.') parser.add_option( '-d', '--delimiter', dest='delimiter', help='Roll up items with the given delimiter. For containers ' 'only. See OpenStack Swift API documentation for ' 'what this means.') (options, args) = parse_args(parser, args) args = args[1:] if options.delimiter and not args: exit('-d option only allowed for container listings') _opts = vars(options).copy() if _opts['human']: _opts.pop('human') _opts['long'] = True if options.totals and not options.long and not options.human: output_manager.error( ""Listing totals only works with -l or --lh."") return with SwiftService(options=_opts) as swift: try: if not args: stats_parts_gen = swift.list() else: container = args[0] args = args[1:] if ""/"" in container or args: output_manager.error( 'Usage: %s list %s\n%s', BASENAME, st_list_options, st_list_help) return else: stats_parts_gen = swift.list(container=container) for stats in stats_parts_gen: if stats[""success""]: _print_stats(options, stats) else: raise stats[""error""] except SwiftError as e: output_manager.error(e.value) st_stat_options = '''[--lh] [container] [object] ''' st_stat_help = ''' Displays information for the account, container, or object. Positional arguments: [container] Name of container to stat from. [object] Name of object to stat. Optional arguments: --lh Report sizes in human readable format similar to ls -lh. '''.strip('\n') def st_stat(parser, args, output_manager): parser.add_option( '--lh', dest='human', action='store_true', default=False, help='Report sizes in human readable format similar to ls -lh.') (options, args) = parse_args(parser, args) args = args[1:] _opts = vars(options) with SwiftService(options=_opts) as swift: try: if not args: stat_result = swift.stat() if not stat_result['success']: raise stat_result['error'] items = stat_result['items'] headers = stat_result['headers'] print_account_stats(items, headers, output_manager) else: container = args[0] if '/' in container: output_manager.error( 'WARNING: / in container name; you might have ' ""meant '%s' instead of '%s'."" % (container.replace('/', ' ', 1), container)) return args = args[1:] if not args: stat_result = swift.stat(container=container) if not stat_result['success']: raise stat_result['error'] items = stat_result['items'] headers = stat_result['headers'] print_container_stats(items, headers, output_manager) else: if len(args) == 1: objects = [args[0]] stat_results = swift.stat( container=container, objects=objects) for stat_result in stat_results: # only 1 result if stat_result[""success""]: items = stat_result['items'] headers = stat_result['headers'] print_object_stats( items, headers, output_manager ) else: raise(stat_result[""error""]) else: output_manager.error( 'Usage: %s stat %s\n%s', BASENAME, st_stat_options, st_stat_help) except SwiftError as e: output_manager.error(e.value) st_post_options = '''[--read-acl <acl>] [--write-acl <acl>] [--sync-to] [--sync-key <sync-key>] [--meta <name:value>] [--header <header>] [container] [object] ''' st_post_help = ''' Updates meta information for the account, container, or object. If the container is not found, it will be created automatically. Positional arguments: [container] Name of container to post to. [object] Name of object to post. Optional arguments: -r, --read-acl <acl> Read ACL for containers. Quick summary of ACL syntax: .r:*, .r:-.example.com, .r:www.example.com, account1, account2:user2 -w, --write-acl <acl> Write ACL for containers. Quick summary of ACL syntax: account1 account2:user2 -t, --sync-to <sync-to> Sync To for containers, for multi-cluster replication. -k, --sync-key <sync-key> Sync Key for containers, for multi-cluster replication. -m, --meta <name:value> Sets a meta data item. This option may be repeated. Example: -m Color:Blue -m Size:Large -H, --header <header:value> Adds a customized request header. This option may be repeated. Example -H ""content-type:text/plain"" -H ""Content-Length: 4000"" '''.strip('\n') def st_post(parser, args, output_manager): parser.add_option( '-r', '--read-acl', dest='read_acl', help='Read ACL for containers. ' 'Quick summary of ACL syntax: .r:*, .r:-.example.com, ' '.r:www.example.com, account1, account2:user2') parser.add_option( '-w', '--write-acl', dest='write_acl', help='Write ACL for ' 'containers. Quick summary of ACL syntax: account1, ' 'account2:user2') parser.add_option( '-t', '--sync-to', dest='sync_to', help='Sets the ' 'Sync To for containers, for multi-cluster replication.') parser.add_option( '-k', '--sync-key', dest='sync_key', help='Sets the ' 'Sync Key for containers, for multi-cluster replication.') parser.add_option( '-m', '--meta', action='append', dest='meta', default=[], help='Sets a meta data item. This option may be repeated. ' 'Example: -m Color:Blue -m Size:Large') parser.add_option( '-H', '--header', action='append', dest='header', default=[], help='Adds a customized request header. ' 'This option may be repeated. ' 'Example: -H ""content-type:text/plain"" ' '-H ""Content-Length: 4000""') (options, args) = parse_args(parser, args) args = args[1:] if (options.read_acl or options.write_acl or options.sync_to or options.sync_key) and not args: exit('-r, -w, -t, and -k options only allowed for containers') _opts = vars(options) with SwiftService(options=_opts) as swift: try: if not args: result = swift.post() else: container = args[0] if '/' in container: output_manager.error( 'WARNING: / in container name; you might have ' ""meant '%s' instead of '%s'."" % (args[0].replace('/', ' ', 1), args[0])) return args = args[1:] if args: if len(args) == 1: objects = [args[0]] results_iterator = swift.post( container=container, objects=objects ) result = next(results_iterator) else: output_manager.error( 'Usage: %s post %s\n%s', BASENAME, st_post_options, st_post_help) return else: result = swift.post(container=container) if not result[""success""]: raise(result[""error""]) except SwiftError as e: output_manager.error(e.value) st_upload_options = '''[--changed] [--skip-identical] [--segment-size <size>] [--segment-container <container>] [--leave-segments] [--object-threads <thread>] [--segment-threads <threads>] [--header <header>] [--use-slo] [--ignore-checksum] [--object-name <object-name>] <container> <file_or_directory> [<file_or_directory>] [...] ''' st_upload_help = ''' Uploads specified files and directories to the given container. Or upload stdin content to the given container(--object-name must be used, and <file_or_directory> can not be used) Positional arguments: <container> Name of container to upload to. <file_or_directory> Name of file or directory to upload. Specify multiple times for multiple uploads. Optional arguments: -c, --changed Only upload files that have changed since the last upload. --skip-identical Skip uploading files that are identical on both sides. -S, --segment-size <size> Upload files in segments no larger than <size> (in Bytes) and then create a ""manifest"" file that will download all the segments as if it were the original file. --segment-container <container> Upload the segments into the specified container. If not specified, the segments will be uploaded to a <container>_segments container to not pollute the main <container> listings. --leave-segments Indicates that you want the older segments of manifest objects left alone (in the case of overwrites). --object-threads <threads> Number of threads to use for uploading full objects. Default is 10. --segment-threads <threads> Number of threads to use for uploading object segments. Default is 10. -H, --header <header:value> Adds a customized request header. This option may be repeated. Example -H ""content-type:text/plain"" -H ""Content-Length: 4000"". --use-slo When used in conjunction with --segment-size it will create a Static Large Object instead of the default Dynamic Large Object. --object-name <object-name> Upload file and name object to <object-name> or upload dir and use <object-name> as object prefix instead of folder name. --ignore-checksum Turn off checksum validation for uploads. '''.strip('\n') def st_upload(parser, args, output_manager): parser.add_option( '-c', '--changed', action='store_true', dest='changed', default=False, help='Only upload files that have changed since ' 'the last upload.') parser.add_option( '--skip-identical', action='store_true', dest='skip_identical', default=False, help='Skip uploading files that are identical on ' 'both sides.') parser.add_option( '-S', '--segment-size', dest='segment_size', help='Upload files ' 'in segments no larger than <size> (in Bytes) and then create a ' '""manifest"" file that will download all the segments as if it were ' 'the original file. Sizes may also be expressed as bytes with the ' 'B suffix, kilobytes with the K suffix, megabytes with the M suffix ' 'or gigabytes with the G suffix.') parser.add_option( '-C', '--segment-container', dest='segment_container', help='Upload the segments into the specified container. ' 'If not specified, the segments will be uploaded to a ' '<container>_segments container to not pollute the main ' '<container> listings.') parser.add_option( '', '--leave-segments', action='store_true', dest='leave_segments', default=False, help='Indicates that you want ' 'the older segments of manifest objects left alone (in the case of ' 'overwrites).') parser.add_option( '', '--object-threads', type=int, default=10, help='Number of threads to use for uploading full objects. ' 'Default is 10.') parser.add_option( '', '--segment-threads', type=int, default=10, help='Number of threads to use for uploading object segments. ' 'Default is 10.') parser.add_option( '-H', '--header', action='append', dest='header', default=[], help='Set request headers with the syntax header:value. ' ' This option may be repeated. Example -H ""content-type:text/plain"" ' '-H ""Content-Length: 4000""') parser.add_option( '', '--use-slo', action='store_true', default=False, help='When used in conjunction with --segment-size, it will ' 'create a Static Large Object instead of the default ' 'Dynamic Large Object.') parser.add_option( '', '--object-name', dest='object_name', help='Upload file and name object to <object-name> or upload dir and ' 'use <object-name> as object prefix instead of folder name.') parser.add_option( '', '--ignore-checksum', dest='checksum', default=True, action='store_false', help='Turn off checksum validation for uploads.') (options, args) = parse_args(parser, args) args = args[1:] stdin_file = None if len(args) < 2: if len(args) == 1 and not sys.stdin.isatty(): if options.object_name is None: output_manager.error('object-name must be used ' 'when upload object from stdin') return container = args[0] stdin_file = ""temp"" + str(time.time()) files = [stdin_file] else: output_manager.error( 'Usage: %s upload %s\n%s', BASENAME, st_upload_options, st_upload_help) return else: container = args[0] files = args[1:] if options.object_name is not None: if len(files) > 1: output_manager.error('object-name only be used with 1 file or dir') return else: orig_path = files[0] if options.segment_size: try: # If segment size only has digits assume it is bytes int(options.segment_size) except ValueError: try: size_mod = ""BKMG"".index(options.segment_size[-1].upper()) multiplier = int(options.segment_size[:-1]) except ValueError: output_manager.error(""Invalid segment size"") return options.segment_size = str((1024 ** size_mod) * multiplier) if int(options.segment_size) <= 0: output_manager.error(""segment-size should be positive"") return _opts = vars(options) _opts['object_uu_threads'] = options.object_threads if stdin_file: with open(stdin_file, ""w"") as fd: fd.write(sys.stdin.read()) with SwiftService(options=_opts) as swift: try: objs = [] dir_markers = [] for f in files: if isfile(f): objs.append(f) elif isdir(f): for (_dir, _ds, _fs) in walk(f): if not (_ds + _fs): dir_markers.append(_dir) else: objs.extend([join(_dir, _f) for _f in _fs]) else: output_manager.error(""Local file '%s' not found"" % f) # Now that we've collected all the required files and dir markers # build the tuples for the call to upload if options.object_name is not None: objs = [ SwiftUploadObject( o, object_name=o.replace( orig_path, options.object_name, 1 ) ) for o in objs ] dir_markers = [ SwiftUploadObject( None, object_name=d.replace( orig_path, options.object_name, 1 ), options={'dir_marker': True} ) for d in dir_markers ] for r in swift.upload(container, objs + dir_markers): if r['success']: if options.verbose: if 'attempts' in r and r['attempts'] > 1: if 'object' in r: output_manager.print_msg( '%s [after %d attempts]' % (r['object'], r['attempts']) ) else: if 'object' in r: output_manager.print_msg(r['object']) elif 'for_object' in r: output_manager.print_msg( '%s segment %s' % (r['for_object'], r['segment_index']) ) else: error = r['error'] if 'action' in r and r['action'] == ""create_container"": # it is not an error to be unable to create the # container so print a warning and carry on if isinstance(error, ClientException): if (r['headers'] and 'X-Storage-Policy' in r['headers']): msg = ' with Storage Policy %s' % \ r['headers']['X-Storage-Policy'].strip() else: msg = ' '.join(str(x) for x in ( error.http_status, error.http_reason) ) if error.http_response_content: if msg: msg += ': ' msg += error.http_response_content[:60] msg = ': %s' % msg else: msg = ': %s' % error output_manager.warning( 'Warning: failed to create container ' ""'%s'%s"", container, msg ) else: output_manager.error(""%s"" % error) too_large = (isinstance(error, ClientException) and error.http_status == 413) if too_large and options.verbose > 0: output_manager.error( ""Consider using the --segment-size option "" ""to chunk the object"") except SwiftError as e: output_manager.error(e.value) finally: if stdin_file: os.remove(stdin_file) st_capabilities_options = ""[<proxy_url>]"" st_info_options = st_capabilities_options st_capabilities_help = ''' Retrieve capability of the proxy. Optional positional arguments: <proxy_url> Proxy URL of the cluster to retrieve capabilities. '''.strip('\n') st_info_help = st_capabilities_help def st_capabilities(parser, args, output_manager): def _print_compo_cap(name, capabilities): for feature, options in sorted(capabilities.items(), key=lambda x: x[0]): output_manager.print_msg(""%s: %s"" % (name, feature)) if options: output_manager.print_msg("" Options:"") for key, value in sorted(options.items(), key=lambda x: x[0]): output_manager.print_msg("" %s: %s"" % (key, value)) (options, args) = parse_args(parser, args) if args and len(args) > 2: output_manager.error('Usage: %s capabilities %s\n%s', BASENAME, st_capabilities_options, st_capabilities_help) return _opts = vars(options) with SwiftService(options=_opts) as swift: try: if len(args) == 2: url = args[1] capabilities_result = swift.capabilities(url) capabilities = capabilities_result['capabilities'] else: capabilities_result = swift.capabilities() capabilities = capabilities_result['capabilities'] _print_compo_cap('Core', {'swift': capabilities['swift']}) del capabilities['swift'] _print_compo_cap('Additional middleware', capabilities) except SwiftError as e: output_manager.error(e.value) st_info = st_capabilities st_auth_help = ''' Display auth related authentication variables in shell friendly format. Commands to run to export storage url and auth token into OS_STORAGE_URL and OS_AUTH_TOKEN: swift auth Commands to append to a runcom file (e.g. ~/.bashrc, /etc/profile) for automatic authentication: swift auth -v -U test:tester -K testing \ -A http://localhost:8080/auth/v1.0 '''.strip('\n') def st_auth(parser, args, thread_manager): (options, args) = parse_args(parser, args) _opts = vars(options) if options.verbose > 1: if options.auth_version in ('1', '1.0'): print('export ST_AUTH=%s' % sh_quote(options.auth)) print('export ST_USER=%s' % sh_quote(options.user)) print('export ST_KEY=%s' % sh_quote(options.key)) else: print('export OS_IDENTITY_API_VERSION=%s' % sh_quote( options.auth_version)) print('export OS_AUTH_VERSION=%s' % sh_quote(options.auth_version)) print('export OS_AUTH_URL=%s' % sh_quote(options.auth)) for k, v in sorted(_opts.items()): if v and k.startswith('os_') and \ k not in ('os_auth_url', 'os_options'): print('export %s=%s' % (k.upper(), sh_quote(v))) else: conn = get_conn(_opts) url, token = conn.get_auth() print('export OS_STORAGE_URL=%s' % sh_quote(url)) print('export OS_AUTH_TOKEN=%s' % sh_quote(token)) st_tempurl_options = '''[--absolute] <method> <seconds> <path> <key>''' st_tempurl_help = ''' Generates a temporary URL for a Swift object. Positional arguments: <method> An HTTP method to allow for this temporary URL. Usually 'GET' or 'PUT'. <seconds> The amount of time in seconds the temporary URL will be valid for; or, if --absolute is passed, the Unix timestamp when the temporary URL will expire. <path> The full path to the Swift object. Example: /v1/AUTH_account/c/o. <key> The secret temporary URL key set on the Swift cluster. To set a key, run \'swift post -m ""Temp-URL-Key:b3968d0207b54ece87cccc06515a89d4""\' Optional arguments: --absolute Interpet the <seconds> positional argument as a Unix timestamp rather than a number of seconds in the future. '''.strip('\n') def st_tempurl(parser, args, thread_manager): parser.add_option( '--absolute', action='store_true', dest='absolute_expiry', default=False, help=(""If present, seconds argument will be interpreted as a Unix "" ""timestamp representing when the tempURL should expire, rather "" ""than an offset from the current time"") ) (options, args) = parse_args(parser, args) args = args[1:] if len(args) < 4: thread_manager.error('Usage: %s tempurl %s\n%s', BASENAME, st_tempurl_options, st_tempurl_help) return method, seconds, path, key = args[:4] try: seconds = int(seconds) except ValueError: thread_manager.error('Seconds must be an integer') return if method.upper() not in ['GET', 'PUT', 'HEAD', 'POST', 'DELETE']: thread_manager.print_msg('WARNING: Non default HTTP method %s for ' 'tempurl specified, possibly an error' % method.upper()) url = generate_temp_url(path, seconds, key, method, absolute=options.absolute_expiry) thread_manager.print_msg(url) def parse_args(parser, args, enforce_requires=True): if not args: args = ['-h'] (options, args) = parser.parse_args(args) if len(args) > 1 and args[1] == '--help': _help = globals().get('st_%s_help' % args[0], ""no help for %s"" % args[0]) print(_help) exit() # Short circuit for tempurl, which doesn't need auth if len(args) > 0 and args[0] == 'tempurl': return options, args if options.auth_version == '3.0': # tolerate sloppy auth_version options.auth_version = '3' if (not (options.auth and options.user and options.key) and options.auth_version != '3'): # Use keystone auth if any of the old-style args are missing options.auth_version = '2.0' # Use new-style args if old ones not present if not options.auth and options.os_auth_url: options.auth = options.os_auth_url if not options.user and options.os_username: options.user = options.os_username if not options.key and options.os_password: options.key = options.os_password # Specific OpenStack options options.os_options = { 'user_id': options.os_user_id, 'user_domain_id': options.os_user_domain_id, 'user_domain_name': options.os_user_domain_name, 'tenant_id': options.os_tenant_id, 'tenant_name': options.os_tenant_name, 'project_id': options.os_project_id, 'project_name': options.os_project_name, 'project_domain_id': options.os_project_domain_id, 'project_domain_name': options.os_project_domain_name, 'service_type': options.os_service_type, 'endpoint_type': options.os_endpoint_type, 'auth_token': options.os_auth_token, 'object_storage_url': options.os_storage_url, 'region_name': options.os_region_name, } if len(args) > 1 and args[0] == ""capabilities"": return options, args if (options.os_options.get('object_storage_url') and options.os_options.get('auth_token') and (options.auth_version == '2.0' or options.auth_version == '3')): return options, args if enforce_requires: if options.auth_version == '3': if not options.auth: exit('Auth version 3 requires OS_AUTH_URL to be set or ' + 'overridden with --os-auth-url') if not (options.user or options.os_user_id): exit('Auth version 3 requires either OS_USERNAME or ' + 'OS_USER_ID to be set or overridden with ' + '--os-username or --os-user-id respectively.') if not options.key: exit('Auth version 3 requires OS_PASSWORD to be set or ' + 'overridden with --os-password') elif not (options.auth and options.user and options.key): exit(''' Auth version 1.0 requires ST_AUTH, ST_USER, and ST_KEY environment variables to be set or overridden with -A, -U, or -K. Auth version 2.0 requires OS_AUTH_URL, OS_USERNAME, OS_PASSWORD, and OS_TENANT_NAME OS_TENANT_ID to be set or overridden with --os-auth-url, --os-username, --os-password, --os-tenant-name or os-tenant-id. Note: adding ""-V 2"" is necessary for this.'''.strip('\n')) return options, args def main(arguments=None): if arguments: argv = arguments else: argv = sys_argv argv = [a if isinstance(a, text_type) else a.decode('utf-8') for a in argv] version = client_version parser = OptionParser(version='python-swiftclient %s' % version, usage=''' usage: %prog [--version] [--help] [--os-help] [--snet] [--verbose] [--debug] [--info] [--quiet] [--auth <auth_url>] [--auth-version <auth_version>] [--user <username>] [--key <api_key>] [--retries <num_retries>] [--os-username <auth-user-name>] [--os-password <auth-password>] [--os-user-id <auth-user-id>] [--os-user-domain-id <auth-user-domain-id>] [--os-user-domain-name <auth-user-domain-name>] [--os-tenant-id <auth-tenant-id>] [--os-tenant-name <auth-tenant-name>] [--os-project-id <auth-project-id>] [--os-project-name <auth-project-name>] [--os-project-domain-id <auth-project-domain-id>] [--os-project-domain-name <auth-project-domain-name>] [--os-auth-url <auth-url>] [--os-auth-token <auth-token>] [--os-storage-url <storage-url>] [--os-region-name <region-name>] [--os-service-type <service-type>] [--os-endpoint-type <endpoint-type>] [--os-cacert <ca-certificate>] [--insecure] [--no-ssl-compression] <subcommand> [--help] [<subcommand options>] Command-line interface to the OpenStack Swift API. Positional arguments: <subcommand> delete Delete a container or objects within a container. download Download objects from containers. list Lists the containers for the account or the objects for a container. post Updates meta information for the account, container, or object; creates containers if not present. stat Displays information for the account, container, or object. upload Uploads files or directories to the given container. capabilities List cluster capabilities. tempurl Create a temporary URL. auth Display auth related environment variables. Examples: %prog download --help %prog -A https://auth.api.rackspacecloud.com/v1.0 -U user -K api_key stat -v %prog --os-auth-url https://api.example.com/v2.0 --os-tenant-name tenant \\ --os-username user --os-password password list %prog --os-auth-url https://api.example.com/v3 --auth-version 3\\ --os-project-name project1 --os-project-domain-name domain1 \\ --os-username user --os-user-domain-name domain1 \\ --os-password password list %prog --os-auth-url https://api.example.com/v3 --auth-version 3\\ --os-project-id 0123456789abcdef0123456789abcdef \\ --os-user-id abcdef0123456789abcdef0123456789 \\ --os-password password list %prog --os-auth-token 6ee5eb33efad4e45ab46806eac010566 \\ --os-storage-url https://10.1.5.2:8080/v1/AUTH_ced809b6a4baea7aeab61a \\ list %prog list --lh '''.strip('\n')) parser.add_option('--os-help', action='store_true', dest='os_help', help='Show OpenStack authentication options.') parser.add_option('--os_help', action='store_true', help=SUPPRESS_HELP) parser.add_option('-s', '--snet', action='store_true', dest='snet', default=False, help='Use SERVICENET internal network.') parser.add_option('-v', '--verbose', action='count', dest='verbose', default=1, help='Print more info.') parser.add_option('--debug', action='store_true', dest='debug', default=False, help='Show the curl commands and results ' 'of all http queries regardless of result status.') parser.add_option('--info', action='store_true', dest='info', default=False, help='Show the curl commands and results ' 'of all http queries which return an error.') parser.add_option('-q', '--quiet', action='store_const', dest='verbose', const=0, default=1, help='Suppress status output.') parser.add_option('-A', '--auth', dest='auth', default=environ.get('ST_AUTH'), help='URL for obtaining an auth token.') parser.add_option('-V', '--auth-version', dest='auth_version', default=environ.get('ST_AUTH_VERSION', (environ.get('OS_AUTH_VERSION', '1.0'))), type=str, help='Specify a version for authentication. ' 'Defaults to 1.0.') parser.add_option('-U', '--user', dest='user', default=environ.get('ST_USER'), help='User name for obtaining an auth token.') parser.add_option('-K', '--key', dest='key', default=environ.get('ST_KEY'), help='Key for obtaining an auth token.') parser.add_option('-R', '--retries', type=int, default=5, dest='retries', help='The number of times to retry a failed connection.') default_val = config_true_value(environ.get('SWIFTCLIENT_INSECURE')) parser.add_option('--insecure', action=""store_true"", dest=""insecure"", default=default_val, help='Allow swiftclient to access servers without ' 'having to verify the SSL certificate. ' 'Defaults to env[SWIFTCLIENT_INSECURE] ' '(set to \'true\' to enable).') parser.add_option('--no-ssl-compression', action='store_false', dest='ssl_compression', default=True, help='This option is deprecated and not used anymore. ' 'SSL compression should be disabled by default ' 'by the system SSL library.') os_grp = OptionGroup(parser, ""OpenStack authentication options"") os_grp.add_option('--os-username', metavar='<auth-user-name>', default=environ.get('OS_USERNAME'), help='OpenStack username. Defaults to env[OS_USERNAME].') os_grp.add_option('--os_username', help=SUPPRESS_HELP) os_grp.add_option('--os-user-id', metavar='<auth-user-id>', default=environ.get('OS_USER_ID'), help='OpenStack user ID. ' 'Defaults to env[OS_USER_ID].') os_grp.add_option('--os_user_id', help=SUPPRESS_HELP) os_grp.add_option('--os-user-domain-id', metavar='<auth-user-domain-id>', default=environ.get('OS_USER_DOMAIN_ID'), help='OpenStack user domain ID. ' 'Defaults to env[OS_USER_DOMAIN_ID].') os_grp.add_option('--os_user_domain_id', help=SUPPRESS_HELP) os_grp.add_option('--os-user-domain-name', metavar='<auth-user-domain-name>', default=environ.get('OS_USER_DOMAIN_NAME'), help='OpenStack user domain name. ' 'Defaults to env[OS_USER_DOMAIN_NAME].') os_grp.add_option('--os_user_domain_name', help=SUPPRESS_HELP) os_grp.add_option('--os-password', metavar='<auth-password>', default=environ.get('OS_PASSWORD'), help='OpenStack password. Defaults to env[OS_PASSWORD].') os_grp.add_option('--os_password', help=SUPPRESS_HELP) os_grp.add_option('--os-tenant-id', metavar='<auth-tenant-id>', default=environ.get('OS_TENANT_ID'), help='OpenStack tenant ID. ' 'Defaults to env[OS_TENANT_ID].') os_grp.add_option('--os_tenant_id', help=SUPPRESS_HELP) os_grp.add_option('--os-tenant-name', metavar='<auth-tenant-name>', default=environ.get('OS_TENANT_NAME'), help='OpenStack tenant name. ' 'Defaults to env[OS_TENANT_NAME].') os_grp.add_option('--os_tenant_name', help=SUPPRESS_HELP) os_grp.add_option('--os-project-id', metavar='<auth-project-id>', default=environ.get('OS_PROJECT_ID'), help='OpenStack project ID. ' 'Defaults to env[OS_PROJECT_ID].') os_grp.add_option('--os_project_id', help=SUPPRESS_HELP) os_grp.add_option('--os-project-name', metavar='<auth-project-name>', default=environ.get('OS_PROJECT_NAME'), help='OpenStack project name. ' 'Defaults to env[OS_PROJECT_NAME].') os_grp.add_option('--os_project_name', help=SUPPRESS_HELP) os_grp.add_option('--os-project-domain-id', metavar='<auth-project-domain-id>', default=environ.get('OS_PROJECT_DOMAIN_ID'), help='OpenStack project domain ID. ' 'Defaults to env[OS_PROJECT_DOMAIN_ID].') os_grp.add_option('--os_project_domain_id', help=SUPPRESS_HELP) os_grp.add_option('--os-project-domain-name', metavar='<auth-project-domain-name>', default=environ.get('OS_PROJECT_DOMAIN_NAME'), help='OpenStack project domain name. ' 'Defaults to env[OS_PROJECT_DOMAIN_NAME].') os_grp.add_option('--os_project_domain_name', help=SUPPRESS_HELP) os_grp.add_option('--os-auth-url', metavar='<auth-url>', default=environ.get('OS_AUTH_URL'), help='OpenStack auth URL. Defaults to env[OS_AUTH_URL].') os_grp.add_option('--os_auth_url', help=SUPPRESS_HELP) os_grp.add_option('--os-auth-token', metavar='<auth-token>', default=environ.get('OS_AUTH_TOKEN'), help='OpenStack token. Defaults to env[OS_AUTH_TOKEN]. ' 'Used with --os-storage-url to bypass the ' 'usual username/password authentication.') os_grp.add_option('--os_auth_token', help=SUPPRESS_HELP) os_grp.add_option('--os-storage-url', metavar='<storage-url>', default=environ.get('OS_STORAGE_URL'), help='OpenStack storage URL. ' 'Defaults to env[OS_STORAGE_URL]. ' 'Overrides the storage url returned during auth. ' 'Will bypass authentication when used with ' '--os-auth-token.') os_grp.add_option('--os_storage_url', help=SUPPRESS_HELP) os_grp.add_option('--os-region-name', metavar='<region-name>', default=environ.get('OS_REGION_NAME'), help='OpenStack region name. ' 'Defaults to env[OS_REGION_NAME].') os_grp.add_option('--os_region_name', help=SUPPRESS_HELP) os_grp.add_option('--os-service-type', metavar='<service-type>', default=environ.get('OS_SERVICE_TYPE'), help='OpenStack Service type. ' 'Defaults to env[OS_SERVICE_TYPE].') os_grp.add_option('--os_service_type', help=SUPPRESS_HELP) os_grp.add_option('--os-endpoint-type', metavar='<endpoint-type>', default=environ.get('OS_ENDPOINT_TYPE'), help='OpenStack Endpoint type. ' 'Defaults to env[OS_ENDPOINT_TYPE].') os_grp.add_option('--os_endpoint_type', help=SUPPRESS_HELP) os_grp.add_option('--os-cacert', metavar='<ca-certificate>', default=environ.get('OS_CACERT'), help='Specify a CA bundle file to use in verifying a ' 'TLS (https) server certificate. ' 'Defaults to env[OS_CACERT].') parser.disable_interspersed_args() # call parse_args before adding os options group so that -h, --help will # print a condensed help message without the os options (options, args) = parse_args(parser, argv[1:], enforce_requires=False) parser.add_option_group(os_grp) if options.os_help: # if openstack option help has been explicitly requested then force # help message, now that os_options group has been added to parser argv = ['-h'] (options, args) = parse_args(parser, argv[1:], enforce_requires=False) parser.enable_interspersed_args() if not args or args[0] not in commands: parser.print_usage() if args: exit('no such command: %s' % args[0]) exit() signal.signal(signal.SIGINT, immediate_exit) if options.debug or options.info: logging.getLogger(""swiftclient"") if options.debug: logging.basicConfig(level=logging.DEBUG) logging.getLogger('iso8601').setLevel(logging.WARNING) elif options.info: logging.basicConfig(level=logging.INFO) with OutputManager() as output: parser.usage = globals()['st_%s_help' % args[0]] try: globals()['st_%s' % args[0]](parser, argv[1:], output) except (ClientException, RequestException, socket.error) as err: output.error(str(err)) if output.get_error_count() > 0: exit(1) if __name__ == '__main__': main() ","#!/usr/bin/python -u # Copyright (c) 2010-2012 OpenStack, LLC. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. from __future__ import print_function, unicode_literals import logging import signal import socket from optparse import OptionParser, OptionGroup, SUPPRESS_HELP from os import environ, walk, _exit as os_exit from os.path import isfile, isdir, join from six import text_type from sys import argv as sys_argv, exit, stderr from time import gmtime, strftime from swiftclient import RequestException from swiftclient.utils import config_true_value, generate_temp_url, prt_bytes from swiftclient.multithreading import OutputManager from swiftclient.exceptions import ClientException from swiftclient import __version__ as client_version from swiftclient.service import SwiftService, SwiftError, \ SwiftUploadObject, get_conn from swiftclient.command_helpers import print_account_stats, \ print_container_stats, print_object_stats try: from shlex import quote as sh_quote except ImportError: from pipes import quote as sh_quote BASENAME = 'swift' commands = ('delete', 'download', 'list', 'post', 'stat', 'upload', 'capabilities', 'info', 'tempurl', 'auth') def immediate_exit(signum, frame): stderr.write("" Aborted\n"") os_exit(2) st_delete_options = '''[--all] [--leave-segments] [--object-threads <threads>] [--container-threads <threads>] [<container>] [<object>] [...] ''' st_delete_help = ''' Delete a container or objects within a container. Positional arguments: [<container>] Name of container to delete from. [<object>] Name of object to delete. Specify multiple times for multiple objects. Optional arguments: -a, --all Delete all containers and objects. --leave-segments Do not delete segments of manifest objects. --object-threads <threads> Number of threads to use for deleting objects. Default is 10. --container-threads <threads> Number of threads to use for deleting containers. Default is 10. '''.strip(""\n"") def st_delete(parser, args, output_manager): parser.add_option( '-a', '--all', action='store_true', dest='yes_all', default=False, help='Delete all containers and objects.') parser.add_option( '', '--leave-segments', action='store_true', dest='leave_segments', default=False, help='Do not delete segments of manifest objects.') parser.add_option( '', '--object-threads', type=int, default=10, help='Number of threads to use for deleting objects. ' 'Default is 10.') parser.add_option( '', '--container-threads', type=int, default=10, help='Number of threads to use for deleting containers. ' 'Default is 10.') (options, args) = parse_args(parser, args) args = args[1:] if (not args and not options.yes_all) or (args and options.yes_all): output_manager.error('Usage: %s delete %s\n%s', BASENAME, st_delete_options, st_delete_help) return _opts = vars(options) _opts['object_dd_threads'] = options.object_threads with SwiftService(options=_opts) as swift: try: if not args: del_iter = swift.delete() else: container = args[0] if '/' in container: output_manager.error( 'WARNING: / in container name; you ' ""might have meant '%s' instead of '%s'."" % (container.replace('/', ' ', 1), container) ) return objects = args[1:] if objects: del_iter = swift.delete(container=container, objects=objects) else: del_iter = swift.delete(container=container) for r in del_iter: c = r.get('container', '') o = r.get('object', '') a = r.get('attempts') if r['success']: if options.verbose: a = ' [after {0} attempts]'.format(a) if a > 1 else '' if r['action'] == 'delete_object': if options.yes_all: p = '{0}/{1}'.format(c, o) else: p = o elif r['action'] == 'delete_segment': p = '{0}/{1}'.format(c, o) elif r['action'] == 'delete_container': p = c output_manager.print_msg('{0}{1}'.format(p, a)) else: p = '{0}/{1}'.format(c, o) if o else c output_manager.error('Error Deleting: {0}: {1}' .format(p, r['error'])) except SwiftError as err: output_manager.error(err.value) st_download_options = '''[--all] [--marker] [--prefix <prefix>] [--output <out_file>] [--output-dir <out_directory>] [--object-threads <threads>] [--container-threads <threads>] [--no-download] [--skip-identical] [--remove-prefix] [--header <header:value>] [--no-shuffle] <container> <object> ''' st_download_help = ''' Download objects from containers. Positional arguments: <container> Name of container to download from. To download a whole account, omit this and specify --all. <object> Name of object to download. Specify multiple times for multiple objects. Omit this to download all objects from the container. Optional arguments: -a, --all Indicates that you really want to download everything in the account. -m, --marker Marker to use when starting a container or account download. -p, --prefix <prefix> Only download items beginning with <prefix> -r, --remove-prefix An optional flag for --prefix <prefix>, use this option to download items without <prefix> -o, --output <out_file> For a single file download, stream the output to <out_file>. Specifying ""-"" as <out_file> will redirect to stdout. -D, --output-dir <out_directory> An optional directory to which to store objects. By default, all objects are recreated in the current directory. --object-threads <threads> Number of threads to use for downloading objects. Default is 10. --container-threads <threads> Number of threads to use for downloading containers. Default is 10. --no-download Perform download(s), but don't actually write anything to disk. -H, --header <header:value> Adds a customized request header to the query, like ""Range"" or ""If-Match"". This option may be repeated. Example --header ""content-type:text/plain"" --skip-identical Skip downloading files that are identical on both sides. --no-shuffle By default, when downloading a complete account or container, download order is randomised in order to to reduce the load on individual drives when multiple clients are executed simultaneously to download the same set of objects (e.g. a nightly automated download script to multiple servers). Enable this option to submit download jobs to the thread pool in the order they are listed in the object store. '''.strip(""\n"") def st_download(parser, args, output_manager): parser.add_option( '-a', '--all', action='store_true', dest='yes_all', default=False, help='Indicates that you really want to download ' 'everything in the account.') parser.add_option( '-m', '--marker', dest='marker', default='', help='Marker to use when starting a container or ' 'account download.') parser.add_option( '-p', '--prefix', dest='prefix', help='Only download items beginning with the <prefix>.') parser.add_option( '-o', '--output', dest='out_file', help='For a single ' 'download, stream the output to <out_file>. ' 'Specifying ""-"" as <out_file> will redirect to stdout.') parser.add_option( '-D', '--output-dir', dest='out_directory', help='An optional directory to which to store objects. ' 'By default, all objects are recreated in the current directory.') parser.add_option( '-r', '--remove-prefix', action='store_true', dest='remove_prefix', default=False, help='An optional flag for --prefix <prefix>, ' 'use this option to download items without <prefix>.') parser.add_option( '', '--object-threads', type=int, default=10, help='Number of threads to use for downloading objects. ' 'Default is 10.') parser.add_option( '', '--container-threads', type=int, default=10, help='Number of threads to use for downloading containers. ' 'Default is 10.') parser.add_option( '', '--no-download', action='store_true', default=False, help=""Perform download(s), but don't actually write anything to disk."") parser.add_option( '-H', '--header', action='append', dest='header', default=[], help='Adds a customized request header to the query, like ""Range"" or ' '""If-Match"". This option may be repeated. ' 'Example: --header ""content-type:text/plain""') parser.add_option( '--skip-identical', action='store_true', dest='skip_identical', default=False, help='Skip downloading files that are identical on ' 'both sides.') parser.add_option( '--no-shuffle', action='store_false', dest='shuffle', default=True, help='By default, download order is randomised in order ' 'to reduce the load on individual drives when multiple clients are ' 'executed simultaneously to download the same set of objects (e.g. a ' 'nightly automated download script to multiple servers). Enable this ' 'option to submit download jobs to the thread pool in the order they ' 'are listed in the object store.') (options, args) = parse_args(parser, args) args = args[1:] if options.out_file == '-': options.verbose = 0 if options.out_file and len(args) != 2: exit('-o option only allowed for single file downloads') if not options.prefix: options.remove_prefix = False if options.out_directory and len(args) == 2: exit('Please use -o option for single file downloads and renames') if (not args and not options.yes_all) or (args and options.yes_all): output_manager.error('Usage: %s download %s\n%s', BASENAME, st_download_options, st_download_help) return _opts = vars(options) _opts['object_dd_threads'] = options.object_threads with SwiftService(options=_opts) as swift: try: if not args: down_iter = swift.download() else: container = args[0] if '/' in container: output_manager.error( 'WARNING: / in container name; you ' ""might have meant '%s' instead of '%s'."" % (container.replace('/', ' ', 1), container) ) return objects = args[1:] if not objects: down_iter = swift.download(container) else: down_iter = swift.download(container, objects) for down in down_iter: if options.out_file == '-' and 'contents' in down: contents = down['contents'] for chunk in contents: output_manager.print_raw(chunk) else: if down['success']: if options.verbose: start_time = down['start_time'] headers_receipt = \ down['headers_receipt'] - start_time auth_time = down['auth_end_time'] - start_time finish_time = down['finish_time'] read_length = down['read_length'] attempts = down['attempts'] total_time = finish_time - start_time down_time = total_time - auth_time _mega = 1000000 if down['pseudodir']: time_str = ( 'auth %.3fs, headers %.3fs, total %.3fs, ' 'pseudo' % ( auth_time, headers_receipt, total_time ) ) else: speed = float(read_length) / down_time / _mega time_str = ( 'auth %.3fs, headers %.3fs, total %.3fs, ' '%.3f MB/s' % ( auth_time, headers_receipt, total_time, speed ) ) path = down['path'] if attempts > 1: output_manager.print_msg( '%s [%s after %d attempts]', path, time_str, attempts ) else: output_manager.print_msg( '%s [%s]', path, time_str ) else: error = down['error'] path = down['path'] container = down['container'] obj = down['object'] if isinstance(error, ClientException): if error.http_status == 304 and \ options.skip_identical: output_manager.print_msg( ""Skipped identical file '%s'"", path) continue if error.http_status == 404: output_manager.error( ""Object '%s/%s' not found"", container, obj) continue output_manager.error( ""Error downloading object '%s/%s': %s"", container, obj, error) except SwiftError as e: output_manager.error(e.value) except Exception as e: output_manager.error(e) st_list_options = '''[--long] [--lh] [--totals] [--prefix <prefix>] [--delimiter <delimiter>] [container] ''' st_list_help = ''' Lists the containers for the account or the objects for a container. Positional arguments: [container] Name of container to list object in. Optional arguments: -l, --long Long listing format, similar to ls -l. --lh Report sizes in human readable format similar to ls -lh. -t, --totals Used with -l or --lh, only report totals. -p <prefix>, --prefix <prefix> Only list items beginning with the prefix. -d <delim>, --delimiter <delim> Roll up items with the given delimiter. For containers only. See OpenStack Swift API documentation for what this means. '''.strip('\n') def st_list(parser, args, output_manager): def _print_stats(options, stats): total_count = total_bytes = 0 container = stats.get(""container"", None) for item in stats[""listing""]: item_name = item.get('name') if not options.long and not options.human: output_manager.print_msg(item.get('name', item.get('subdir'))) else: if not container: # listing containers item_bytes = item.get('bytes') byte_str = prt_bytes(item_bytes, options.human) count = item.get('count') total_count += count try: meta = item.get('meta') utc = gmtime(float(meta.get('x-timestamp'))) datestamp = strftime('%Y-%m-%d %H:%M:%S', utc) except TypeError: datestamp = '????-??-?? ??:??:??' if not options.totals: output_manager.print_msg( ""%5s %s %s %s"", count, byte_str, datestamp, item_name) else: # list container contents subdir = item.get('subdir') content_type = item.get('content_type') if subdir is None: item_bytes = item.get('bytes') byte_str = prt_bytes(item_bytes, options.human) date, xtime = item.get('last_modified').split('T') xtime = xtime.split('.')[0] else: item_bytes = 0 byte_str = prt_bytes(item_bytes, options.human) date = xtime = '' item_name = subdir if not options.totals: output_manager.print_msg( ""%s %10s %8s %24s %s"", byte_str, date, xtime, content_type, item_name) total_bytes += item_bytes # report totals if options.long or options.human: if not container: output_manager.print_msg( ""%5s %s"", prt_bytes(total_count, True), prt_bytes(total_bytes, options.human)) else: output_manager.print_msg( prt_bytes(total_bytes, options.human)) parser.add_option( '-l', '--long', dest='long', action='store_true', default=False, help='Long listing format, similar to ls -l.') parser.add_option( '--lh', dest='human', action='store_true', default=False, help='Report sizes in human readable format, ' ""similar to ls -lh."") parser.add_option( '-t', '--totals', dest='totals', help='used with -l or --lh, only report totals.', action='store_true', default=False) parser.add_option( '-p', '--prefix', dest='prefix', help='Only list items beginning with the prefix.') parser.add_option( '-d', '--delimiter', dest='delimiter', help='Roll up items with the given delimiter. For containers ' 'only. See OpenStack Swift API documentation for ' 'what this means.') (options, args) = parse_args(parser, args) args = args[1:] if options.delimiter and not args: exit('-d option only allowed for container listings') _opts = vars(options).copy() if _opts['human']: _opts.pop('human') _opts['long'] = True if options.totals and not options.long and not options.human: output_manager.error( ""Listing totals only works with -l or --lh."") return with SwiftService(options=_opts) as swift: try: if not args: stats_parts_gen = swift.list() else: container = args[0] args = args[1:] if ""/"" in container or args: output_manager.error( 'Usage: %s list %s\n%s', BASENAME, st_list_options, st_list_help) return else: stats_parts_gen = swift.list(container=container) for stats in stats_parts_gen: if stats[""success""]: _print_stats(options, stats) else: raise stats[""error""] except SwiftError as e: output_manager.error(e.value) st_stat_options = '''[--lh] [container] [object] ''' st_stat_help = ''' Displays information for the account, container, or object. Positional arguments: [container] Name of container to stat from. [object] Name of object to stat. Optional arguments: --lh Report sizes in human readable format similar to ls -lh. '''.strip('\n') def st_stat(parser, args, output_manager): parser.add_option( '--lh', dest='human', action='store_true', default=False, help='Report sizes in human readable format similar to ls -lh.') (options, args) = parse_args(parser, args) args = args[1:] _opts = vars(options) with SwiftService(options=_opts) as swift: try: if not args: stat_result = swift.stat() if not stat_result['success']: raise stat_result['error'] items = stat_result['items'] headers = stat_result['headers'] print_account_stats(items, headers, output_manager) else: container = args[0] if '/' in container: output_manager.error( 'WARNING: / in container name; you might have ' ""meant '%s' instead of '%s'."" % (container.replace('/', ' ', 1), container)) return args = args[1:] if not args: stat_result = swift.stat(container=container) if not stat_result['success']: raise stat_result['error'] items = stat_result['items'] headers = stat_result['headers'] print_container_stats(items, headers, output_manager) else: if len(args) == 1: objects = [args[0]] stat_results = swift.stat( container=container, objects=objects) for stat_result in stat_results: # only 1 result if stat_result[""success""]: items = stat_result['items'] headers = stat_result['headers'] print_object_stats( items, headers, output_manager ) else: raise(stat_result[""error""]) else: output_manager.error( 'Usage: %s stat %s\n%s', BASENAME, st_stat_options, st_stat_help) except SwiftError as e: output_manager.error(e.value) st_post_options = '''[--read-acl <acl>] [--write-acl <acl>] [--sync-to] [--sync-key <sync-key>] [--meta <name:value>] [--header <header>] [container] [object] ''' st_post_help = ''' Updates meta information for the account, container, or object. If the container is not found, it will be created automatically. Positional arguments: [container] Name of container to post to. [object] Name of object to post. Optional arguments: -r, --read-acl <acl> Read ACL for containers. Quick summary of ACL syntax: .r:*, .r:-.example.com, .r:www.example.com, account1, account2:user2 -w, --write-acl <acl> Write ACL for containers. Quick summary of ACL syntax: account1 account2:user2 -t, --sync-to <sync-to> Sync To for containers, for multi-cluster replication. -k, --sync-key <sync-key> Sync Key for containers, for multi-cluster replication. -m, --meta <name:value> Sets a meta data item. This option may be repeated. Example: -m Color:Blue -m Size:Large -H, --header <header:value> Adds a customized request header. This option may be repeated. Example -H ""content-type:text/plain"" -H ""Content-Length: 4000"" '''.strip('\n') def st_post(parser, args, output_manager): parser.add_option( '-r', '--read-acl', dest='read_acl', help='Read ACL for containers. ' 'Quick summary of ACL syntax: .r:*, .r:-.example.com, ' '.r:www.example.com, account1, account2:user2') parser.add_option( '-w', '--write-acl', dest='write_acl', help='Write ACL for ' 'containers. Quick summary of ACL syntax: account1, ' 'account2:user2') parser.add_option( '-t', '--sync-to', dest='sync_to', help='Sets the ' 'Sync To for containers, for multi-cluster replication.') parser.add_option( '-k', '--sync-key', dest='sync_key', help='Sets the ' 'Sync Key for containers, for multi-cluster replication.') parser.add_option( '-m', '--meta', action='append', dest='meta', default=[], help='Sets a meta data item. This option may be repeated. ' 'Example: -m Color:Blue -m Size:Large') parser.add_option( '-H', '--header', action='append', dest='header', default=[], help='Adds a customized request header. ' 'This option may be repeated. ' 'Example: -H ""content-type:text/plain"" ' '-H ""Content-Length: 4000""') (options, args) = parse_args(parser, args) args = args[1:] if (options.read_acl or options.write_acl or options.sync_to or options.sync_key) and not args: exit('-r, -w, -t, and -k options only allowed for containers') _opts = vars(options) with SwiftService(options=_opts) as swift: try: if not args: result = swift.post() else: container = args[0] if '/' in container: output_manager.error( 'WARNING: / in container name; you might have ' ""meant '%s' instead of '%s'."" % (args[0].replace('/', ' ', 1), args[0])) return args = args[1:] if args: if len(args) == 1: objects = [args[0]] results_iterator = swift.post( container=container, objects=objects ) result = next(results_iterator) else: output_manager.error( 'Usage: %s post %s\n%s', BASENAME, st_post_options, st_post_help) return else: result = swift.post(container=container) if not result[""success""]: raise(result[""error""]) except SwiftError as e: output_manager.error(e.value) st_upload_options = '''[--changed] [--skip-identical] [--segment-size <size>] [--segment-container <container>] [--leave-segments] [--object-threads <thread>] [--segment-threads <threads>] [--header <header>] [--use-slo] [--ignore-checksum] [--object-name <object-name>] <container> <file_or_directory> [<file_or_directory>] [...] ''' st_upload_help = ''' Uploads specified files and directories to the given container. Positional arguments: <container> Name of container to upload to. <file_or_directory> Name of file or directory to upload. Specify multiple times for multiple uploads. Optional arguments: -c, --changed Only upload files that have changed since the last upload. --skip-identical Skip uploading files that are identical on both sides. -S, --segment-size <size> Upload files in segments no larger than <size> (in Bytes) and then create a ""manifest"" file that will download all the segments as if it were the original file. --segment-container <container> Upload the segments into the specified container. If not specified, the segments will be uploaded to a <container>_segments container to not pollute the main <container> listings. --leave-segments Indicates that you want the older segments of manifest objects left alone (in the case of overwrites). --object-threads <threads> Number of threads to use for uploading full objects. Default is 10. --segment-threads <threads> Number of threads to use for uploading object segments. Default is 10. -H, --header <header:value> Adds a customized request header. This option may be repeated. Example -H ""content-type:text/plain"" -H ""Content-Length: 4000"". --use-slo When used in conjunction with --segment-size it will create a Static Large Object instead of the default Dynamic Large Object. --object-name <object-name> Upload file and name object to <object-name> or upload dir and use <object-name> as object prefix instead of folder name. --ignore-checksum Turn off checksum validation for uploads. '''.strip('\n') def st_upload(parser, args, output_manager): parser.add_option( '-c', '--changed', action='store_true', dest='changed', default=False, help='Only upload files that have changed since ' 'the last upload.') parser.add_option( '--skip-identical', action='store_true', dest='skip_identical', default=False, help='Skip uploading files that are identical on ' 'both sides.') parser.add_option( '-S', '--segment-size', dest='segment_size', help='Upload files ' 'in segments no larger than <size> (in Bytes) and then create a ' '""manifest"" file that will download all the segments as if it were ' 'the original file. Sizes may also be expressed as bytes with the ' 'B suffix, kilobytes with the K suffix, megabytes with the M suffix ' 'or gigabytes with the G suffix.') parser.add_option( '-C', '--segment-container', dest='segment_container', help='Upload the segments into the specified container. ' 'If not specified, the segments will be uploaded to a ' '<container>_segments container to not pollute the main ' '<container> listings.') parser.add_option( '', '--leave-segments', action='store_true', dest='leave_segments', default=False, help='Indicates that you want ' 'the older segments of manifest objects left alone (in the case of ' 'overwrites).') parser.add_option( '', '--object-threads', type=int, default=10, help='Number of threads to use for uploading full objects. ' 'Default is 10.') parser.add_option( '', '--segment-threads', type=int, default=10, help='Number of threads to use for uploading object segments. ' 'Default is 10.') parser.add_option( '-H', '--header', action='append', dest='header', default=[], help='Set request headers with the syntax header:value. ' ' This option may be repeated. Example -H ""content-type:text/plain"" ' '-H ""Content-Length: 4000""') parser.add_option( '', '--use-slo', action='store_true', default=False, help='When used in conjunction with --segment-size, it will ' 'create a Static Large Object instead of the default ' 'Dynamic Large Object.') parser.add_option( '', '--object-name', dest='object_name', help='Upload file and name object to <object-name> or upload dir and ' 'use <object-name> as object prefix instead of folder name.') parser.add_option( '', '--ignore-checksum', dest='checksum', default=True, action='store_false', help='Turn off checksum validation for uploads.') (options, args) = parse_args(parser, args) args = args[1:] if len(args) < 2: output_manager.error( 'Usage: %s upload %s\n%s', BASENAME, st_upload_options, st_upload_help) return else: container = args[0] files = args[1:] if options.object_name is not None: if len(files) > 1: output_manager.error('object-name only be used with 1 file or dir') return else: orig_path = files[0] if options.segment_size: try: # If segment size only has digits assume it is bytes int(options.segment_size) except ValueError: try: size_mod = ""BKMG"".index(options.segment_size[-1].upper()) multiplier = int(options.segment_size[:-1]) except ValueError: output_manager.error(""Invalid segment size"") return options.segment_size = str((1024 ** size_mod) * multiplier) if int(options.segment_size) <= 0: output_manager.error(""segment-size should be positive"") return _opts = vars(options) _opts['object_uu_threads'] = options.object_threads with SwiftService(options=_opts) as swift: try: objs = [] dir_markers = [] for f in files: if isfile(f): objs.append(f) elif isdir(f): for (_dir, _ds, _fs) in walk(f): if not (_ds + _fs): dir_markers.append(_dir) else: objs.extend([join(_dir, _f) for _f in _fs]) else: output_manager.error(""Local file '%s' not found"" % f) # Now that we've collected all the required files and dir markers # build the tuples for the call to upload if options.object_name is not None: objs = [ SwiftUploadObject( o, object_name=o.replace( orig_path, options.object_name, 1 ) ) for o in objs ] dir_markers = [ SwiftUploadObject( None, object_name=d.replace( orig_path, options.object_name, 1 ), options={'dir_marker': True} ) for d in dir_markers ] for r in swift.upload(container, objs + dir_markers): if r['success']: if options.verbose: if 'attempts' in r and r['attempts'] > 1: if 'object' in r: output_manager.print_msg( '%s [after %d attempts]' % (r['object'], r['attempts']) ) else: if 'object' in r: output_manager.print_msg(r['object']) elif 'for_object' in r: output_manager.print_msg( '%s segment %s' % (r['for_object'], r['segment_index']) ) else: error = r['error'] if 'action' in r and r['action'] == ""create_container"": # it is not an error to be unable to create the # container so print a warning and carry on if isinstance(error, ClientException): if (r['headers'] and 'X-Storage-Policy' in r['headers']): msg = ' with Storage Policy %s' % \ r['headers']['X-Storage-Policy'].strip() else: msg = ' '.join(str(x) for x in ( error.http_status, error.http_reason) ) if error.http_response_content: if msg: msg += ': ' msg += error.http_response_content[:60] msg = ': %s' % msg else: msg = ': %s' % error output_manager.warning( 'Warning: failed to create container ' ""'%s'%s"", container, msg ) else: output_manager.error(""%s"" % error) too_large = (isinstance(error, ClientException) and error.http_status == 413) if too_large and options.verbose > 0: output_manager.error( ""Consider using the --segment-size option "" ""to chunk the object"") except SwiftError as e: output_manager.error(e.value) st_capabilities_options = ""[<proxy_url>]"" st_info_options = st_capabilities_options st_capabilities_help = ''' Retrieve capability of the proxy. Optional positional arguments: <proxy_url> Proxy URL of the cluster to retrieve capabilities. '''.strip('\n') st_info_help = st_capabilities_help def st_capabilities(parser, args, output_manager): def _print_compo_cap(name, capabilities): for feature, options in sorted(capabilities.items(), key=lambda x: x[0]): output_manager.print_msg(""%s: %s"" % (name, feature)) if options: output_manager.print_msg("" Options:"") for key, value in sorted(options.items(), key=lambda x: x[0]): output_manager.print_msg("" %s: %s"" % (key, value)) (options, args) = parse_args(parser, args) if args and len(args) > 2: output_manager.error('Usage: %s capabilities %s\n%s', BASENAME, st_capabilities_options, st_capabilities_help) return _opts = vars(options) with SwiftService(options=_opts) as swift: try: if len(args) == 2: url = args[1] capabilities_result = swift.capabilities(url) capabilities = capabilities_result['capabilities'] else: capabilities_result = swift.capabilities() capabilities = capabilities_result['capabilities'] _print_compo_cap('Core', {'swift': capabilities['swift']}) del capabilities['swift'] _print_compo_cap('Additional middleware', capabilities) except SwiftError as e: output_manager.error(e.value) st_info = st_capabilities st_auth_help = ''' Display auth related authentication variables in shell friendly format. Commands to run to export storage url and auth token into OS_STORAGE_URL and OS_AUTH_TOKEN: swift auth Commands to append to a runcom file (e.g. ~/.bashrc, /etc/profile) for automatic authentication: swift auth -v -U test:tester -K testing \ -A http://localhost:8080/auth/v1.0 '''.strip('\n') def st_auth(parser, args, thread_manager): (options, args) = parse_args(parser, args) _opts = vars(options) if options.verbose > 1: if options.auth_version in ('1', '1.0'): print('export ST_AUTH=%s' % sh_quote(options.auth)) print('export ST_USER=%s' % sh_quote(options.user)) print('export ST_KEY=%s' % sh_quote(options.key)) else: print('export OS_IDENTITY_API_VERSION=%s' % sh_quote( options.auth_version)) print('export OS_AUTH_VERSION=%s' % sh_quote(options.auth_version)) print('export OS_AUTH_URL=%s' % sh_quote(options.auth)) for k, v in sorted(_opts.items()): if v and k.startswith('os_') and \ k not in ('os_auth_url', 'os_options'): print('export %s=%s' % (k.upper(), sh_quote(v))) else: conn = get_conn(_opts) url, token = conn.get_auth() print('export OS_STORAGE_URL=%s' % sh_quote(url)) print('export OS_AUTH_TOKEN=%s' % sh_quote(token)) st_tempurl_options = '''[--absolute] <method> <seconds> <path> <key>''' st_tempurl_help = ''' Generates a temporary URL for a Swift object. Positional arguments: <method> An HTTP method to allow for this temporary URL. Usually 'GET' or 'PUT'. <seconds> The amount of time in seconds the temporary URL will be valid for; or, if --absolute is passed, the Unix timestamp when the temporary URL will expire. <path> The full path to the Swift object. Example: /v1/AUTH_account/c/o. <key> The secret temporary URL key set on the Swift cluster. To set a key, run \'swift post -m ""Temp-URL-Key:b3968d0207b54ece87cccc06515a89d4""\' Optional arguments: --absolute Interpet the <seconds> positional argument as a Unix timestamp rather than a number of seconds in the future. '''.strip('\n') def st_tempurl(parser, args, thread_manager): parser.add_option( '--absolute', action='store_true', dest='absolute_expiry', default=False, help=(""If present, seconds argument will be interpreted as a Unix "" ""timestamp representing when the tempURL should expire, rather "" ""than an offset from the current time"") ) (options, args) = parse_args(parser, args) args = args[1:] if len(args) < 4: thread_manager.error('Usage: %s tempurl %s\n%s', BASENAME, st_tempurl_options, st_tempurl_help) return method, seconds, path, key = args[:4] try: seconds = int(seconds) except ValueError: thread_manager.error('Seconds must be an integer') return if method.upper() not in ['GET', 'PUT', 'HEAD', 'POST', 'DELETE']: thread_manager.print_msg('WARNING: Non default HTTP method %s for ' 'tempurl specified, possibly an error' % method.upper()) url = generate_temp_url(path, seconds, key, method, absolute=options.absolute_expiry) thread_manager.print_msg(url) def parse_args(parser, args, enforce_requires=True): if not args: args = ['-h'] (options, args) = parser.parse_args(args) if len(args) > 1 and args[1] == '--help': _help = globals().get('st_%s_help' % args[0], ""no help for %s"" % args[0]) print(_help) exit() # Short circuit for tempurl, which doesn't need auth if len(args) > 0 and args[0] == 'tempurl': return options, args if options.auth_version == '3.0': # tolerate sloppy auth_version options.auth_version = '3' if (not (options.auth and options.user and options.key) and options.auth_version != '3'): # Use keystone auth if any of the old-style args are missing options.auth_version = '2.0' # Use new-style args if old ones not present if not options.auth and options.os_auth_url: options.auth = options.os_auth_url if not options.user and options.os_username: options.user = options.os_username if not options.key and options.os_password: options.key = options.os_password # Specific OpenStack options options.os_options = { 'user_id': options.os_user_id, 'user_domain_id': options.os_user_domain_id, 'user_domain_name': options.os_user_domain_name, 'tenant_id': options.os_tenant_id, 'tenant_name': options.os_tenant_name, 'project_id': options.os_project_id, 'project_name': options.os_project_name, 'project_domain_id': options.os_project_domain_id, 'project_domain_name': options.os_project_domain_name, 'service_type': options.os_service_type, 'endpoint_type': options.os_endpoint_type, 'auth_token': options.os_auth_token, 'object_storage_url': options.os_storage_url, 'region_name': options.os_region_name, } if len(args) > 1 and args[0] == ""capabilities"": return options, args if (options.os_options.get('object_storage_url') and options.os_options.get('auth_token') and (options.auth_version == '2.0' or options.auth_version == '3')): return options, args if enforce_requires: if options.auth_version == '3': if not options.auth: exit('Auth version 3 requires OS_AUTH_URL to be set or ' + 'overridden with --os-auth-url') if not (options.user or options.os_user_id): exit('Auth version 3 requires either OS_USERNAME or ' + 'OS_USER_ID to be set or overridden with ' + '--os-username or --os-user-id respectively.') if not options.key: exit('Auth version 3 requires OS_PASSWORD to be set or ' + 'overridden with --os-password') elif not (options.auth and options.user and options.key): exit(''' Auth version 1.0 requires ST_AUTH, ST_USER, and ST_KEY environment variables to be set or overridden with -A, -U, or -K. Auth version 2.0 requires OS_AUTH_URL, OS_USERNAME, OS_PASSWORD, and OS_TENANT_NAME OS_TENANT_ID to be set or overridden with --os-auth-url, --os-username, --os-password, --os-tenant-name or os-tenant-id. Note: adding ""-V 2"" is necessary for this.'''.strip('\n')) return options, args def main(arguments=None): if arguments: argv = arguments else: argv = sys_argv argv = [a if isinstance(a, text_type) else a.decode('utf-8') for a in argv] version = client_version parser = OptionParser(version='python-swiftclient %s' % version, usage=''' usage: %prog [--version] [--help] [--os-help] [--snet] [--verbose] [--debug] [--info] [--quiet] [--auth <auth_url>] [--auth-version <auth_version>] [--user <username>] [--key <api_key>] [--retries <num_retries>] [--os-username <auth-user-name>] [--os-password <auth-password>] [--os-user-id <auth-user-id>] [--os-user-domain-id <auth-user-domain-id>] [--os-user-domain-name <auth-user-domain-name>] [--os-tenant-id <auth-tenant-id>] [--os-tenant-name <auth-tenant-name>] [--os-project-id <auth-project-id>] [--os-project-name <auth-project-name>] [--os-project-domain-id <auth-project-domain-id>] [--os-project-domain-name <auth-project-domain-name>] [--os-auth-url <auth-url>] [--os-auth-token <auth-token>] [--os-storage-url <storage-url>] [--os-region-name <region-name>] [--os-service-type <service-type>] [--os-endpoint-type <endpoint-type>] [--os-cacert <ca-certificate>] [--insecure] [--no-ssl-compression] <subcommand> [--help] [<subcommand options>] Command-line interface to the OpenStack Swift API. Positional arguments: <subcommand> delete Delete a container or objects within a container. download Download objects from containers. list Lists the containers for the account or the objects for a container. post Updates meta information for the account, container, or object; creates containers if not present. stat Displays information for the account, container, or object. upload Uploads files or directories to the given container. capabilities List cluster capabilities. tempurl Create a temporary URL. auth Display auth related environment variables. Examples: %prog download --help %prog -A https://auth.api.rackspacecloud.com/v1.0 -U user -K api_key stat -v %prog --os-auth-url https://api.example.com/v2.0 --os-tenant-name tenant \\ --os-username user --os-password password list %prog --os-auth-url https://api.example.com/v3 --auth-version 3\\ --os-project-name project1 --os-project-domain-name domain1 \\ --os-username user --os-user-domain-name domain1 \\ --os-password password list %prog --os-auth-url https://api.example.com/v3 --auth-version 3\\ --os-project-id 0123456789abcdef0123456789abcdef \\ --os-user-id abcdef0123456789abcdef0123456789 \\ --os-password password list %prog --os-auth-token 6ee5eb33efad4e45ab46806eac010566 \\ --os-storage-url https://10.1.5.2:8080/v1/AUTH_ced809b6a4baea7aeab61a \\ list %prog list --lh '''.strip('\n')) parser.add_option('--os-help', action='store_true', dest='os_help', help='Show OpenStack authentication options.') parser.add_option('--os_help', action='store_true', help=SUPPRESS_HELP) parser.add_option('-s', '--snet', action='store_true', dest='snet', default=False, help='Use SERVICENET internal network.') parser.add_option('-v', '--verbose', action='count', dest='verbose', default=1, help='Print more info.') parser.add_option('--debug', action='store_true', dest='debug', default=False, help='Show the curl commands and results ' 'of all http queries regardless of result status.') parser.add_option('--info', action='store_true', dest='info', default=False, help='Show the curl commands and results ' 'of all http queries which return an error.') parser.add_option('-q', '--quiet', action='store_const', dest='verbose', const=0, default=1, help='Suppress status output.') parser.add_option('-A', '--auth', dest='auth', default=environ.get('ST_AUTH'), help='URL for obtaining an auth token.') parser.add_option('-V', '--auth-version', dest='auth_version', default=environ.get('ST_AUTH_VERSION', (environ.get('OS_AUTH_VERSION', '1.0'))), type=str, help='Specify a version for authentication. ' 'Defaults to 1.0.') parser.add_option('-U', '--user', dest='user', default=environ.get('ST_USER'), help='User name for obtaining an auth token.') parser.add_option('-K', '--key', dest='key', default=environ.get('ST_KEY'), help='Key for obtaining an auth token.') parser.add_option('-R', '--retries', type=int, default=5, dest='retries', help='The number of times to retry a failed connection.') default_val = config_true_value(environ.get('SWIFTCLIENT_INSECURE')) parser.add_option('--insecure', action=""store_true"", dest=""insecure"", default=default_val, help='Allow swiftclient to access servers without ' 'having to verify the SSL certificate. ' 'Defaults to env[SWIFTCLIENT_INSECURE] ' '(set to \'true\' to enable).') parser.add_option('--no-ssl-compression', action='store_false', dest='ssl_compression', default=True, help='This option is deprecated and not used anymore. ' 'SSL compression should be disabled by default ' 'by the system SSL library.') os_grp = OptionGroup(parser, ""OpenStack authentication options"") os_grp.add_option('--os-username', metavar='<auth-user-name>', default=environ.get('OS_USERNAME'), help='OpenStack username. Defaults to env[OS_USERNAME].') os_grp.add_option('--os_username', help=SUPPRESS_HELP) os_grp.add_option('--os-user-id', metavar='<auth-user-id>', default=environ.get('OS_USER_ID'), help='OpenStack user ID. ' 'Defaults to env[OS_USER_ID].') os_grp.add_option('--os_user_id', help=SUPPRESS_HELP) os_grp.add_option('--os-user-domain-id', metavar='<auth-user-domain-id>', default=environ.get('OS_USER_DOMAIN_ID'), help='OpenStack user domain ID. ' 'Defaults to env[OS_USER_DOMAIN_ID].') os_grp.add_option('--os_user_domain_id', help=SUPPRESS_HELP) os_grp.add_option('--os-user-domain-name', metavar='<auth-user-domain-name>', default=environ.get('OS_USER_DOMAIN_NAME'), help='OpenStack user domain name. ' 'Defaults to env[OS_USER_DOMAIN_NAME].') os_grp.add_option('--os_user_domain_name', help=SUPPRESS_HELP) os_grp.add_option('--os-password', metavar='<auth-password>', default=environ.get('OS_PASSWORD'), help='OpenStack password. Defaults to env[OS_PASSWORD].') os_grp.add_option('--os_password', help=SUPPRESS_HELP) os_grp.add_option('--os-tenant-id', metavar='<auth-tenant-id>', default=environ.get('OS_TENANT_ID'), help='OpenStack tenant ID. ' 'Defaults to env[OS_TENANT_ID].') os_grp.add_option('--os_tenant_id', help=SUPPRESS_HELP) os_grp.add_option('--os-tenant-name', metavar='<auth-tenant-name>', default=environ.get('OS_TENANT_NAME'), help='OpenStack tenant name. ' 'Defaults to env[OS_TENANT_NAME].') os_grp.add_option('--os_tenant_name', help=SUPPRESS_HELP) os_grp.add_option('--os-project-id', metavar='<auth-project-id>', default=environ.get('OS_PROJECT_ID'), help='OpenStack project ID. ' 'Defaults to env[OS_PROJECT_ID].') os_grp.add_option('--os_project_id', help=SUPPRESS_HELP) os_grp.add_option('--os-project-name', metavar='<auth-project-name>', default=environ.get('OS_PROJECT_NAME'), help='OpenStack project name. ' 'Defaults to env[OS_PROJECT_NAME].') os_grp.add_option('--os_project_name', help=SUPPRESS_HELP) os_grp.add_option('--os-project-domain-id', metavar='<auth-project-domain-id>', default=environ.get('OS_PROJECT_DOMAIN_ID'), help='OpenStack project domain ID. ' 'Defaults to env[OS_PROJECT_DOMAIN_ID].') os_grp.add_option('--os_project_domain_id', help=SUPPRESS_HELP) os_grp.add_option('--os-project-domain-name', metavar='<auth-project-domain-name>', default=environ.get('OS_PROJECT_DOMAIN_NAME'), help='OpenStack project domain name. ' 'Defaults to env[OS_PROJECT_DOMAIN_NAME].') os_grp.add_option('--os_project_domain_name', help=SUPPRESS_HELP) os_grp.add_option('--os-auth-url', metavar='<auth-url>', default=environ.get('OS_AUTH_URL'), help='OpenStack auth URL. Defaults to env[OS_AUTH_URL].') os_grp.add_option('--os_auth_url', help=SUPPRESS_HELP) os_grp.add_option('--os-auth-token', metavar='<auth-token>', default=environ.get('OS_AUTH_TOKEN'), help='OpenStack token. Defaults to env[OS_AUTH_TOKEN]. ' 'Used with --os-storage-url to bypass the ' 'usual username/password authentication.') os_grp.add_option('--os_auth_token', help=SUPPRESS_HELP) os_grp.add_option('--os-storage-url', metavar='<storage-url>', default=environ.get('OS_STORAGE_URL'), help='OpenStack storage URL. ' 'Defaults to env[OS_STORAGE_URL]. ' 'Overrides the storage url returned during auth. ' 'Will bypass authentication when used with ' '--os-auth-token.') os_grp.add_option('--os_storage_url', help=SUPPRESS_HELP) os_grp.add_option('--os-region-name', metavar='<region-name>', default=environ.get('OS_REGION_NAME'), help='OpenStack region name. ' 'Defaults to env[OS_REGION_NAME].') os_grp.add_option('--os_region_name', help=SUPPRESS_HELP) os_grp.add_option('--os-service-type', metavar='<service-type>', default=environ.get('OS_SERVICE_TYPE'), help='OpenStack Service type. ' 'Defaults to env[OS_SERVICE_TYPE].') os_grp.add_option('--os_service_type', help=SUPPRESS_HELP) os_grp.add_option('--os-endpoint-type', metavar='<endpoint-type>', default=environ.get('OS_ENDPOINT_TYPE'), help='OpenStack Endpoint type. ' 'Defaults to env[OS_ENDPOINT_TYPE].') os_grp.add_option('--os_endpoint_type', help=SUPPRESS_HELP) os_grp.add_option('--os-cacert', metavar='<ca-certificate>', default=environ.get('OS_CACERT'), help='Specify a CA bundle file to use in verifying a ' 'TLS (https) server certificate. ' 'Defaults to env[OS_CACERT].') parser.disable_interspersed_args() # call parse_args before adding os options group so that -h, --help will # print a condensed help message without the os options (options, args) = parse_args(parser, argv[1:], enforce_requires=False) parser.add_option_group(os_grp) if options.os_help: # if openstack option help has been explicitly requested then force # help message, now that os_options group has been added to parser argv = ['-h'] (options, args) = parse_args(parser, argv[1:], enforce_requires=False) parser.enable_interspersed_args() if not args or args[0] not in commands: parser.print_usage() if args: exit('no such command: %s' % args[0]) exit() signal.signal(signal.SIGINT, immediate_exit) if options.debug or options.info: logging.getLogger(""swiftclient"") if options.debug: logging.basicConfig(level=logging.DEBUG) logging.getLogger('iso8601').setLevel(logging.WARNING) elif options.info: logging.basicConfig(level=logging.INFO) with OutputManager() as output: parser.usage = globals()['st_%s_help' % args[0]] try: globals()['st_%s' % args[0]](parser, argv[1:], output) except (ClientException, RequestException, socket.error) as err: output.error(str(err)) if output.get_error_count() > 0: exit(1) if __name__ == '__main__': main()",1460,1439
openstack%2Frally~master~Ic001e4175a4a358b5e5b9d3b103b60f4ca29948d,openstack/rally,master,Ic001e4175a4a358b5e5b9d3b103b60f4ca29948d,[WIP] Add distributed agent,NEW,2017-07-31 12:37:07.000000000,2017-12-18 02:43:27.000000000,,"[{'_account_id': 14817}, {'_account_id': 22960}]","[{'number': 1, 'created': '2017-07-31 12:37:07.000000000', 'files': ['rally/task/agent.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/deb5b2297bb14f4773e5084533684269e328cfbe', 'message': '[WIP] Add distributed agent\n\nChange-Id: Ic001e4175a4a358b5e5b9d3b103b60f4ca29948d\n'}]",0,489202,deb5b2297bb14f4773e5084533684269e328cfbe,5,2,1,22960,,,0,"[WIP] Add distributed agent

Change-Id: Ic001e4175a4a358b5e5b9d3b103b60f4ca29948d
",git fetch https://review.opendev.org/openstack/rally refs/changes/02/489202/1 && git format-patch -1 --stdout FETCH_HEAD,['rally/task/agent.py'],1,deb5b2297bb14f4773e5084533684269e328cfbe,agent,"# All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from rally.common import logging LOG = logging.getLogger(__name__) class Agent: def __init__(self, engine): pass def receive_chunck_data(self): pass def send_result(self): pass def start(self): pass ",,32,0
openstack%2Fironic~master~If33314bfa8df79335fb8e1df02b2241e90fbc638,openstack/ironic,master,If33314bfa8df79335fb8e1df02b2241e90fbc638,Support oslo.messaging.zmq in ironic,NEW,2017-03-14 16:03:55.000000000,2017-12-18 02:43:02.000000000,,"[{'_account_id': 7711}, {'_account_id': 12356}, {'_account_id': 13290}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 22724}]","[{'number': 1, 'created': '2017-03-14 16:03:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a5cc52bd809ac542c8936a77c6d29bc747826603', 'message': '[WIP] Support oslo.messaging.zmq in ironic\n\nWhen ironic is deployed with zmq driver instead of default\nrabbit one, any actions that related to rpc call\ncause 500 Internal Error. This is because redis matchmaker\nshould fails to parse rpc topic correct and does not recognize\nthe host where to send RPC call.\nAs the result when ironic is deployed with zmq, ``server`` parameter,\ndestination host, should be explicitly passed to rpc context and\nhostname should be erased from the topic.\n\nChange-Id: If33314bfa8df79335fb8e1df02b2241e90fbc638\nCloses-Bug: #1669243\n'}, {'number': 2, 'created': '2017-03-15 08:15:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/039847ecb745d9eaa66c997ffde6b45af6c6ed36', 'message': '[WIP] Support oslo.messaging.zmq in ironic\n\nWhen ironic is deployed with zmq driver instead of default\nrabbit one, any actions that related to rpc call\ncause 500 Internal Error. This is because redis matchmaker\nfails to parse rpc topic correct and does not recognize\nthe host where to send RPC call.\nAs the result when ironic is deployed with zmq, ``server`` parameter,\ndestination host, should be explicitly passed to rpc context and\nhostname should be erased from the topic.\n\nChange-Id: If33314bfa8df79335fb8e1df02b2241e90fbc638\nCloses-Bug: #1669243\n'}, {'number': 3, 'created': '2017-04-11 15:22:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8e4ed4925aa085cb52882f7f501a1fe1165977c7', 'message': 'Support oslo.messaging.zmq in ironic\n\nExperimental\n\nWhen ironic is deployed with zmq driver instead of default\nrabbit one, any actions that related to rpc call\ncause 500 Internal Error. This is because redis matchmaker\nfails to parse rpc topic correct and does not recognize\nthe host where to send RPC call.\nAs the result when ironic is deployed with zmq, ``server`` parameter,\ndestination host, should be explicitly passed to rpc context and\nhostname should be erased from the topic.\n\nOther possible solution is to upload a related change for ironic\nspecifically to oslo.messaging\n\nChange-Id: If33314bfa8df79335fb8e1df02b2241e90fbc638\nCloses-Bug: #1669243\n'}, {'number': 4, 'created': '2017-04-12 08:30:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/57c3b5074fa4744115b5f182586982b8080daaf8', 'message': 'Support oslo.messaging.zmq in ironic\n\nExperimental\n\nWhen ironic is deployed with zmq driver instead of default\nrabbit one, any actions that related to rpc call\ncause 500 Internal Error. This is because redis matchmaker\nfails to parse rpc topic correct and does not recognize\nthe host where to send RPC call.\nAs the result when ironic is deployed with zmq, ``server`` parameter,\ndestination host, should be explicitly passed to rpc context and\nhostname should be erased from the topic.\n\nOther possible solution is to upload a related change for ironic\nspecifically to oslo.messaging\n\nChange-Id: If33314bfa8df79335fb8e1df02b2241e90fbc638\nCloses-Bug: #1669243\n'}, {'number': 5, 'created': '2017-04-12 09:04:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/748ad289f962a4a9c5af0743aa750529cba34ca3', 'message': 'Support oslo.messaging.zmq in ironic\n\nExperimental\n\nWhen ironic is deployed with zmq driver instead of default\nrabbit one, any actions that related to rpc call\ncause 500 Internal Error. This is because redis matchmaker\nfails to parse rpc topic correct and does not recognize\nthe host where to send RPC call.\nAs the result when ironic is deployed with zmq, ``server`` parameter,\ndestination host, should be explicitly passed to rpc context and\nhostname should be erased from the topic.\n\nOther possible solution is to upload a related change for ironic\nspecifically to oslo.messaging\n\nChange-Id: If33314bfa8df79335fb8e1df02b2241e90fbc638\nCloses-Bug: #1669243\n'}, {'number': 6, 'created': '2017-04-13 12:07:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/445ce9906ba93a94f323cc78df6199aff535df24', 'message': 'Support zmq for Baremetal Service\n\nWhen ironic is deployed with zmq driver instead of default\nrabbit one, any actions that related to rpc call\ncause 500 Internal Error. This is because redis matchmaker\nfails to parse rpc topic correct and does not recognize\nthe host where to send RPC call.\nMoreover, if ``server`` parameter is not passed to rpc context\nzmq driver will use default Round-Robin algorithm instead of\ndesirable HashRing.\nAs the result when ironic is deployed with zmq, destination host,\nshould be explicitly passed to rpc context and hostname should be\nerased from the topic.\n\nChange-Id: If33314bfa8df79335fb8e1df02b2241e90fbc638\nCloses-Bug: #1669243\n'}, {'number': 7, 'created': '2017-04-13 12:12:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bd4496d3cdc2aa01ce52f98821dec65d52180f90', 'message': 'Support zmq for Baremetal Service\n\nWhen ironic is deployed with zmq driver instead of default\nrabbit one, any actions that related to rpc call\ncause 500 Internal Error. This is because redis matchmaker\nfails to parse rpc topic correct and does not recognize\nthe host where to send RPC call.\nMoreover, if ``server`` parameter is not passed to rpc context\nzmq driver will use default Round-Robin algorithm instead of\ndesirable HashRing.\nAs the result when ironic is deployed with zmq, destination host,\nshould be explicitly passed to rpc context and hostname should be\nerased from the topic.\n\nChange-Id: If33314bfa8df79335fb8e1df02b2241e90fbc638\nCloses-Bug: #1669243\n'}, {'number': 8, 'created': '2017-04-14 08:49:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0cfc9e03df6c8b5d83c63a3703f29426ff04d980', 'message': 'Support zmq for Baremetal Service\n\nWhen ironic is deployed with zmq driver instead of default\nrabbit one, any actions that related to rpc call\ncause 500 Internal Error. This is because redis matchmaker\nfails to parse rpc topic correct and does not recognize\nthe host where to send RPC call.\nMoreover, if ``server`` parameter is not passed to rpc context\nzmq driver will use default Round-Robin algorithm instead of\ndesirable HashRing.\nAs the result when ironic is deployed with zmq, destination host,\nshould be explicitly passed to rpc context and hostname should be\nerased from the topic.\n\nChange-Id: If33314bfa8df79335fb8e1df02b2241e90fbc638\nCloses-Bug: #1669243\n'}, {'number': 9, 'created': '2017-04-18 12:41:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/dfac4bd2c696911f54af7bbb012d1862e4836b36', 'message': '[WIP] Support oslo.messaging.zmq in ironic\n\nWhen ironic is deployed with zmq driver instead of default\nrabbit one, any actions that related to rpc call\ncause 500 Internal Error. This is because redis matchmaker\nfails to set rpc topic correct and the host is not recognized\nas the RPC call destination.\nAs the result when ironic is deployed with zmq, ``server`` parameter,\ndestination host, should be explicitly passed to rpc context and\nhostname should be erased from the topic.\nThis relieves from the necessity to correct rpc topics in redis\nmatchmaker and force zmq driver to use destination defined by HashRing\nironic implementation (instead of RoundRobin).\n\nChange-Id: If33314bfa8df79335fb8e1df02b2241e90fbc638\nCloses-Bug: #1669243\n'}, {'number': 10, 'created': '2017-04-18 13:00:24.000000000', 'files': ['ironic/tests/unit/conductor/test_rpcapi.py', 'releasenotes/notes/zmq-support-2bc5948c1d735d98.yaml', 'ironic/conductor/rpcapi.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/a9a4dc58155758f991073ee45db8810358ffecfd', 'message': 'Support oslo.messaging.zmq in ironic\n\nWhen ironic is deployed with zmq driver instead of default\nrabbit one, any actions that related to rpc call\ncause 500 Internal Error. This is because redis matchmaker\nfails to set rpc topic correct and the host is not recognized\nas the RPC call destination.\nAs the result when ironic is deployed with zmq, ``server`` parameter,\ndestination host, should be explicitly passed to rpc context and\nhostname should be erased from the topic.\nThis relieves from the necessity to correct rpc topics in redis\nmatchmaker and force zmq driver to use destination defined by HashRing\nironic implementation (instead of RoundRobin).\n\nChange-Id: If33314bfa8df79335fb8e1df02b2241e90fbc638\nCloses-Bug: #1669243\n'}]",16,445550,a9a4dc58155758f991073ee45db8810358ffecfd,54,8,10,22724,,,0,"Support oslo.messaging.zmq in ironic

When ironic is deployed with zmq driver instead of default
rabbit one, any actions that related to rpc call
cause 500 Internal Error. This is because redis matchmaker
fails to set rpc topic correct and the host is not recognized
as the RPC call destination.
As the result when ironic is deployed with zmq, ``server`` parameter,
destination host, should be explicitly passed to rpc context and
hostname should be erased from the topic.
This relieves from the necessity to correct rpc topics in redis
matchmaker and force zmq driver to use destination defined by HashRing
ironic implementation (instead of RoundRobin).

Change-Id: If33314bfa8df79335fb8e1df02b2241e90fbc638
Closes-Bug: #1669243
",git fetch https://review.opendev.org/openstack/ironic refs/changes/50/445550/7 && git format-patch -1 --stdout FETCH_HEAD,['ironic/conductor/rpcapi.py'],1,a5cc52bd809ac542c8936a77c6d29bc747826603,bug/1669243,"from oslo_log import logLOG = log.getLogger(__name__) def get_server_for(self, node): self.ring_manager.reset() try: ring = self.ring_manager[node.driver] dest = ring.get_nodes(node.uuid.encode('utf-8'), replicas=CONF.hash_distribution_replicas) return dest.pop() except exception.DriverNotFound: reason = (_('No conductor service registered which supports ' 'driver %s.') % node.driver) raise exception.NoValidHost(reason=reason) if CONF.rpc_backend == 'zmq': return self.topic server = None if CONF.api.rpc_backend == 'zmq': server = self.get_server_for(node_obj) cctxt = self.client.prepare(topic=topic or self.topic, server=server, version='1.36')"," cctxt = self.client.prepare(topic=topic or self.topic, version='1.36')",23,2
openstack%2Fironic~master~Ibae2bbdf803456dd880cae0eb6892726fc575737,openstack/ironic,master,Ibae2bbdf803456dd880cae0eb6892726fc575737,[POC|DNM|TEST] Add node.configdrive,NEW,2017-04-11 19:28:34.000000000,2017-12-18 02:41:19.000000000,,"[{'_account_id': 6618}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 19554}, {'_account_id': 22255}]","[{'number': 1, 'created': '2017-04-11 19:28:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/601cf0c68382d6c35efcb88fa18572dd99f39acb', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.23 of Node\nand API microversion 1.32.\n\nThe RPC version is changed to 1.41 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 2, 'created': '2017-04-11 21:16:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/36696aac04b5aa0e85207d0c83d1ae6b26c3776e', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.23 of Node\nand API microversion 1.32.\n\nThe RPC version is changed to 1.41 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 3, 'created': '2017-04-14 03:36:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fe7fd79c20f503f07f490cf541febe49da0ee5b5', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.23 of Node\nand API microversion 1.32.\n\nThe RPC version is changed to 1.41 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 4, 'created': '2017-04-14 04:04:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/981c9d03c31c1f4135eef7098b2949b7a06b8948', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.23 of Node\nand API microversion 1.32.\n\nThe RPC version is changed to 1.41 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 5, 'created': '2017-05-15 22:14:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c03d4d74152e378d58d815d052bafab9f4ebaeb9', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.23 of Node\nand API microversion 1.32.\n\nThe RPC version is changed to 1.41 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 6, 'created': '2017-05-31 23:54:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a195d9a4bc195b5b204760e55a98fcf798e383db', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.22 of Node\nand API microversion 1.32.\n\nThe RPC version is changed to 1.41 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 7, 'created': '2017-06-06 19:38:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/395d689bea5ff201619676d624c8c17bf515129d', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.22 of Node\nand API microversion 1.32.\n\nThe RPC version is changed to 1.41 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 8, 'created': '2017-06-27 01:24:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0be94ac1b446193bbc3e285c45fde71900cbb99e', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.22 of Node\nand API microversion 1.32.\n\nThe RPC version is changed to 1.42 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 9, 'created': '2017-06-27 03:22:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c43790b06b0f42eb2884dadb20c94a7ca9c93cb3', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.22 of Node\nand API microversion 1.32.\n\nThe RPC version is changed to 1.42 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 10, 'created': '2017-06-28 18:13:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9fc145cfb3a6480ae7d2dd9ca2adb1b07d869842', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.22 of Node\nand API microversion 1.32.\n\nThe RPC version is changed to 1.42 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 11, 'created': '2017-07-12 18:15:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/186e3bfa054793f1cd7b782fb6e246ffa4d9f6ae', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.22 of Node\nand API microversion 1.34.\n\nThe RPC version is changed to 1.42 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 12, 'created': '2017-07-19 20:57:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cdbda009f346b15f7a7c928aedb9685462aaec0d', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.22 of Node\nand API microversion 1.34.\n\nThe RPC version is changed to 1.42 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 13, 'created': '2017-07-20 14:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bb82409370fe9ba948a6134c13d0c6b733ee01c6', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.22 of Node\nand API microversion 1.35.\n\nThe RPC version is changed to 1.42 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 14, 'created': '2017-07-25 22:07:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/29470c80b5ba392064ef290e38a2d5c7177e50cc', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.22 of Node\nand API microversion 1.35.\n\nThe RPC version is changed to 1.42 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}, {'number': 15, 'created': '2017-07-31 18:42:38.000000000', 'files': ['ironic/tests/unit/api/utils.py', 'ironic/tests/unit/conductor/test_rpcapi.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/conductor/rpcapi.py', 'ironic/objects/node.py', 'ironic/drivers/modules/agent_client.py', 'ironic/drivers/modules/deploy_utils.py', 'ironic/drivers/modules/iscsi_deploy.py', 'ironic/db/sqlalchemy/api.py', 'ironic/api/controllers/v1/utils.py', 'ironic/common/release_mappings.py', 'ironic/drivers/modules/agent.py', 'ironic/db/sqlalchemy/alembic/versions/5146e0b12deb_add_node_configdrive.py', 'ironic/cmd/dbsync.py', 'ironic/tests/unit/drivers/modules/test_iscsi_deploy.py', 'ironic/tests/unit/drivers/modules/test_deploy_utils.py', 'doc/source/contributor/webapi-version-history.rst', 'ironic/conductor/manager.py', 'ironic/tests/unit/api/v1/test_nodes.py', 'ironic/api/controllers/v1/versions.py', 'ironic/tests/unit/db/utils.py', 'ironic/tests/unit/drivers/modules/test_agent.py', 'ironic/tests/unit/drivers/modules/test_agent_client.py', 'ironic/tests/unit/objects/test_objects.py', 'ironic/db/sqlalchemy/models.py', 'ironic/api/controllers/v1/node.py', 'ironic/db/api.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/605bd5ed8681bad4a45543f7d8a67ca3bae93ea4', 'message': ""[POC|DNM|TEST] Add node.configdrive\n\nThis is a test for rolling upgrades. It shows what a future patch might look\nlike that modifies a versioned object or modifies the RPC version.\n\nTHIS SHOULD NOT BE MERGED!!!\n\nThis patch replaces (and deprecates) node.instance_info['configdrive']\nwith a new node.configdrive field. This is available in version 1.22 of Node\nand API microversion 1.35.\n\nThe RPC version is changed to 1.42 because do_node_deploy() can take a new\nargument 'use_node_configdrive', a Boolean that indicates whether to use\nnode.configdrive as the configdrive.\n\nChange-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737\nRelated-Bug: #1526283\nCo-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>\nCo-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>\nCo-Authored-By: Lin Tan <lin.tan@intel.com>\n""}]",4,455811,605bd5ed8681bad4a45543f7d8a67ca3bae93ea4,114,9,15,6618,,,0,"[POC|DNM|TEST] Add node.configdrive

This is a test for rolling upgrades. It shows what a future patch might look
like that modifies a versioned object or modifies the RPC version.

THIS SHOULD NOT BE MERGED!!!

This patch replaces (and deprecates) node.instance_info['configdrive']
with a new node.configdrive field. This is available in version 1.22 of Node
and API microversion 1.35.

The RPC version is changed to 1.42 because do_node_deploy() can take a new
argument 'use_node_configdrive', a Boolean that indicates whether to use
node.configdrive as the configdrive.

Change-Id: Ibae2bbdf803456dd880cae0eb6892726fc575737
Related-Bug: #1526283
Co-Authored-By: Grzegorz Grasza <grzegorz.grasza@intel.com>
Co-Authored-By: Szymon Borkowski <szymon.borkowski@intel.com>
Co-Authored-By: Lin Tan <lin.tan@intel.com>
",git fetch https://review.opendev.org/openstack/ironic refs/changes/11/455811/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/tests/unit/api/utils.py', 'ironic/tests/unit/conductor/test_rpcapi.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/conductor/rpcapi.py', 'ironic/objects/node.py', 'doc/source/dev/webapi-version-history.rst', 'ironic/drivers/modules/agent_client.py', 'ironic/drivers/modules/deploy_utils.py', 'ironic/drivers/modules/iscsi_deploy.py', 'ironic/db/sqlalchemy/api.py', 'ironic/api/controllers/v1/utils.py', 'ironic/common/release_mappings.py', 'ironic/drivers/modules/agent.py', 'ironic/tests/unit/drivers/modules/test_iscsi_deploy.py', 'ironic/tests/unit/drivers/modules/test_deploy_utils.py', 'ironic/conductor/manager.py', 'ironic/tests/unit/api/v1/test_nodes.py', 'ironic/api/controllers/v1/versions.py', 'ironic/tests/unit/db/utils.py', 'ironic/tests/unit/drivers/modules/test_agent.py', 'ironic/tests/unit/drivers/modules/test_agent_client.py', 'ironic/tests/unit/objects/test_objects.py', 'ironic/db/sqlalchemy/models.py', 'ironic/api/controllers/v1/node.py', 'ironic/db/api.py']",25,601cf0c68382d6c35efcb88fa18572dd99f39acb,bug/1526283," def get_lt_version(self, model, version): """"""Returns objects with versions less than (<) the specified version. :param model: The model(type) of desired objects :param version: The version of interest :returns: The DB objects """""" @abc.abstractmethod",,276,67
openstack%2Frally~master~I165d044cb03d254234775dad33f14608c9fb111c,openstack/rally,master,I165d044cb03d254234775dad33f14608c9fb111c,[TESTING][senlin] try to fix ci,NEW,2017-04-19 10:28:24.000000000,2017-12-18 02:41:04.000000000,,"[{'_account_id': 6172}, {'_account_id': 8871}, {'_account_id': 11034}, {'_account_id': 14817}, {'_account_id': 21528}, {'_account_id': 22960}]","[{'number': 1, 'created': '2017-04-19 10:28:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/3f728b22de8674be883b7db68cf32f70ca360894', 'message': '[senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 2, 'created': '2017-04-25 23:54:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d64195992c037edda45994b16e73c2df868fb623', 'message': '[senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 3, 'created': '2017-04-26 02:01:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/95de03bf1d382752e530d6c7a0258c2452a5cbee', 'message': '[senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 4, 'created': '2017-04-26 05:02:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c2dc435a0ebba53b84b594e656fa51cfddde8be1', 'message': '[senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 5, 'created': '2017-04-26 07:34:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/9789942c8e5853ad99007e9240c3bfa72700e854', 'message': '[senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 6, 'created': '2017-04-26 15:32:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e7a4265e00115751008eee82edf6c4854a1a98c7', 'message': '[senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 7, 'created': '2017-04-27 01:49:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/dc9c4e8731ba4482a690aa7b016f65af861f3771', 'message': '[senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 8, 'created': '2017-04-27 06:21:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/552958643783c9f5907b54aa6a62ba09a68917e9', 'message': '[senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 9, 'created': '2017-07-23 21:29:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4e0e88a7cdf0fd28e01507f1e5fe2287fd4d017e', 'message': '[senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 10, 'created': '2017-07-31 09:00:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/80b0c169af2e289c9b0725fce3e680847d8748f0', 'message': '[TESTING][senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 11, 'created': '2017-08-01 01:27:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1a851223a4883c4f56f542dc29b2e656e5dc1cc5', 'message': '[TESTING][senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 12, 'created': '2017-08-01 02:34:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/565b607284fdf3b0d7e288658908c76da8092d77', 'message': '[TESTING][senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 13, 'created': '2017-08-01 03:43:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5f9e03071018e06a8be9eab663f41d183e2de02c', 'message': '[TESTING][senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 14, 'created': '2017-08-01 06:00:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b21839fdd29e738660ea34bebdf23685f29e8d3d', 'message': '[TESTING][senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 15, 'created': '2017-08-01 06:08:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/842733f3964d4d0948aca9e65ba6230fd2ecf662', 'message': '[TESTING][senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 16, 'created': '2017-08-01 07:05:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/fbf815a9456a6ed621be1c2de8d8305ef74c9b7f', 'message': '[TESTING][senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 17, 'created': '2017-08-01 11:21:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6a1470f60ca299a836c5e44ef316e9bbb1f7021b', 'message': '[TESTING][senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 18, 'created': '2017-08-02 01:43:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/cdcebd90a49e43b696916d7ed119d16e29422b95', 'message': '[TESTING][senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 19, 'created': '2017-08-02 07:07:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6c3f94b17220a8a7486984cfe3bfc9b60cdeb189', 'message': '[TESTING][senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 20, 'created': '2017-08-03 02:48:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/604c22954615e6146fad4d059ce88543eda0650d', 'message': '[TESTING][senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}, {'number': 21, 'created': '2017-08-03 05:29:43.000000000', 'files': ['rally/plugins/openstack/scenarios/senlin/utils.py', 'rally/plugins/openstack/context/senlin/profiles.py', 'tests/unit/plugins/openstack/context/senlin/test_profiles.py', 'tests/unit/plugins/openstack/cleanup/test_resources.py', 'rally-jobs/rally-senlin.yaml', 'rally/plugins/openstack/cleanup/resources.py', 'tests/unit/plugins/openstack/scenarios/senlin/test_utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/68af563b968f5c2ec572359ab3cfc72050da4cff', 'message': '[TESTING][senlin] try to fix ci\n\nChange-Id: I165d044cb03d254234775dad33f14608c9fb111c\n'}]",0,458010,68af563b968f5c2ec572359ab3cfc72050da4cff,77,6,21,9545,,,0,"[TESTING][senlin] try to fix ci

Change-Id: I165d044cb03d254234775dad33f14608c9fb111c
",git fetch https://review.opendev.org/openstack/rally refs/changes/10/458010/20 && git format-patch -1 --stdout FETCH_HEAD,['rally-jobs/rally-senlin.yaml'],1,3f728b22de8674be883b7db68cf32f70ca360894,senlin, - network: public, - network: private,1,1
openstack%2Ftacker~master~I6c12ffce9bbec7a4e40f2f50a3d00098a91625c6,openstack/tacker,master,I6c12ffce9bbec7a4e40f2f50a3d00098a91625c6,[WIP] Enable auto-healing for VNFFG,NEW,2017-07-15 01:50:09.000000000,2017-12-18 02:41:02.000000000,,[{'_account_id': 20560}],"[{'number': 1, 'created': '2017-07-15 01:50:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/1f069d46df7caeba03f40fdd0d972c1b839b7626', 'message': '[WIP] Enable auto-healing for VNFFG\n\nCurrently, VNFFG is orchestrated by the combination of VNFs in Tacker.\nIn lower layer, Tacker used Neutron SFC to chain VNFs via their connection\npoints. In case the VNFs are broken, it leads to the failure of the chains.\nIn this blueprint, tacker conductor is used to emit failure events from VNFM\nto NFVO so that VNFFG can recognize and auto-heal.\n\nImplement blueprint: #vnffg-healing\n\nChange-Id: I6c12ffce9bbec7a4e40f2f50a3d00098a91625c6\n'}, {'number': 2, 'created': '2017-07-15 01:56:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/7358a7f1c9fed80797fb85eed1afa60935d8b893', 'message': '[WIP] Enable auto-healing for VNFFG\n\nCurrently, VNFFG is orchestrated by the combination of VNFs in Tacker.\nIn lower layer, Tacker used Neutron SFC to chain VNFs via their connection\npoints. In case the VNFs are broken, it leads to the failure of the chains.\nIn this blueprint, tacker conductor is used to emit failure events from VNFM\nto NFVO so that VNFFG can recognize and auto-heal.\n\nImplement blueprint: #vnffg-healing\n\nChange-Id: I6c12ffce9bbec7a4e40f2f50a3d00098a91625c6\n'}, {'number': 3, 'created': '2017-07-24 01:03:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/f39ec841a4ddb0045aeb97e01978f80ff91eafaa', 'message': '[WIP] Enable auto-healing for VNFFG\n\nCurrently, VNFFG is orchestrated by the combination of VNFs in Tacker.\nIn lower layer, Tacker used Neutron SFC to chain VNFs via their connection\npoints. In case the VNFs are broken, it leads to the failure of the chains.\nIn this blueprint, tacker conductor is used to emit failure events from VNFM\nto NFVO so that VNFFG can recognize and auto-heal.\n\nImplement blueprint: #vnffg-healing\n\nChange-Id: I6c12ffce9bbec7a4e40f2f50a3d00098a91625c6\n'}, {'number': 4, 'created': '2017-07-25 02:16:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/8119af266049811784a2fa35a049a4673e382472', 'message': '[WIP] Enable auto-healing for VNFFG\n\nCurrently, VNFFG is orchestrated by the combination of VNFs in Tacker.\nIn lower layer, Tacker used Neutron SFC to chain VNFs via their connection\npoints. In case the VNFs are broken, it leads to the failure of the chains.\nIn this blueprint, tacker conductor is used to emit failure events from VNFM\nto NFVO so that VNFFG can recognize and auto-heal.\n\nImplement blueprint: #vnffg-healing\n\nChange-Id: I6c12ffce9bbec7a4e40f2f50a3d00098a91625c6\n'}, {'number': 5, 'created': '2017-08-03 05:46:01.000000000', 'files': ['samples/tosca-templates/vnffgd/tosca-vnffg-vnfd2-alarm-respawn.yaml', 'samples/tosca-templates/vnffgd/tosca-vnffg-vnfd1-alarm-respawn.yaml', 'tacker/nfvo/drivers/vnffg/vnffg_ha/__init__.py', 'tacker/nfvo/drivers/vnffg/vnffg_ha/vnffg_scaling.py', 'tacker/vnfm/policy_actions/notify/notify.py', 'tacker/vnfm/plugin.py', 'tacker/nfvo/drivers/vim/openstack_driver.py', 'tacker/nfvo/nfvo_plugin.py', 'tacker/vnfm/policy_actions/notify/__init__.py', 'tacker/conductor/conductorrpc/AutoScalingRPC.py', 'tacker/nfvo/drivers/vnffg/absttract_vnffg_ha_driver.py', 'tacker/conductor/conductorrpc/AutoHealingRPC.py', 'setup.cfg', 'tacker/conductor/conductor_server.py', 'tacker/nfvo/drivers/vnffg/abstract_vnffg_driver.py', 'tacker/nfvo/drivers/vnffg/vnffg_ha/vnffg_healing.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/da4364db2ef3989f9585b3a5638f7aedc0013cd6', 'message': '[WIP] Enable auto-healing for VNFFG\n\nCurrently, VNFFG is orchestrated by the combination of VNFs in Tacker.\nIn lower layer, Tacker used Neutron SFC to chain VNFs via their connection\npoints. In case the VNFs are broken, it leads to the failure of the chains.\nIn this blueprint, tacker conductor is used to emit failure events from VNFM\nto NFVO so that VNFFG can recognize and auto-heal.\n\nImplement blueprint: #vnffg-healing\n\nChange-Id: I6c12ffce9bbec7a4e40f2f50a3d00098a91625c6\n'}]",0,484088,da4364db2ef3989f9585b3a5638f7aedc0013cd6,14,1,5,20560,,,0,"[WIP] Enable auto-healing for VNFFG

Currently, VNFFG is orchestrated by the combination of VNFs in Tacker.
In lower layer, Tacker used Neutron SFC to chain VNFs via their connection
points. In case the VNFs are broken, it leads to the failure of the chains.
In this blueprint, tacker conductor is used to emit failure events from VNFM
to NFVO so that VNFFG can recognize and auto-heal.

Implement blueprint: #vnffg-healing

Change-Id: I6c12ffce9bbec7a4e40f2f50a3d00098a91625c6
",git fetch https://review.opendev.org/openstack/tacker refs/changes/88/484088/2 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/conductor/conductorrpc/VNFFGScalingRPC.py', 'tacker/conductor/conductorrpc/VNFFGHealingRPC.py', 'tacker/nfvo/drivers/vnffg/vnffg_ha/vnffg_scaling.py', 'tacker/nfvo/drivers/vim/openstack_driver.py', 'tacker/nfvo/drivers/vnffg/vnffg_ha/__init__.py', 'tacker/nfvo/nfvo_plugin.py', 'tacker/nfvo/drivers/vnffg/absttract_vnffg_ha_driver.py', 'tacker/conductor/conductor_server.py', 'tacker/nfvo/drivers/vnffg/abstract_vnffg_driver.py', 'tacker/nfvo/drivers/vnffg/vnffg_ha/vnffg_healing.py']",10,1f069d46df7caeba03f40fdd0d972c1b839b7626,,"# Copyright 2016 Red Hat Inc # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import uuid from oslo_utils import timeutils from tacker.db.common_services import common_services_db_plugin from tacker.plugins.common import constants from oslo_log import log as logging from tacker.common import log from tacker.nfvo.drivers.vnffg import absttract_vnffg_ha_driver LOG = logging.getLogger(__name__) def _log_vnffg_events(context, vnffg, evt_details): _cos_db_plg = common_services_db_plugin.CommonServicesPluginDb() _cos_db_plg.create_event(context, res_id=vnffg['id'], res_type=constants.RES_TYPE_VNF, res_state=vnffg['status'], evt_type=constants.RES_EVT_MONITOR, tstamp=timeutils.utcnow(), details=evt_details) class VNFFGHealing(absttract_vnffg_ha_driver.VnffgHaAbstractDriver): """"""Auto-healing policy for VNFFG"""""" def __init__(self): super(VNFFGHealing, self).__init__() self._instances = set() def get_type(self): return 'vnffg_healing' def get_name(self): return 'vnffg_healing' def get_description(self): return 'VNFFG Healing policy' @log.log def execute_policy(self, vnffg_id, vnfs, auth_attr=None): # add event-audit functions instance_id = str(uuid.uuid4()) self._instances.add(instance_id) return instance_id",,311,4
openstack%2Frally~master~I3750befca7f79fd578b420347ceabcb08dfc4a40,openstack/rally,master,I3750befca7f79fd578b420347ceabcb08dfc4a40,[db] Optimize sla_check and task status commands,NEW,2017-07-17 16:04:54.000000000,2017-12-18 02:40:32.000000000,,[{'_account_id': 14817}],"[{'number': 1, 'created': '2017-07-17 16:04:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/40244178c2273ba66a2d1261012646d24c37a456', 'message': '[db][wip] Optimize sla_check command\n\nChange-Id: I3750befca7f79fd578b420347ceabcb08dfc4a40\n'}, {'number': 2, 'created': '2017-07-17 17:07:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e27bf36c9c802be6bbdb1a003b7ac59eab52d96c', 'message': '[db] Optimize sla_check and task status commands\n\nFor sla_check and status commands, it is redundant to load all fields of\na task, subtasks and workloads. This patch extends the api to fix the\nnumber of loaded fields.\n\nChange-Id: I3750befca7f79fd578b420347ceabcb08dfc4a40\n'}, {'number': 3, 'created': '2017-07-17 17:16:02.000000000', 'files': ['rally/common/db/api.py', 'rally/cli/commands/task.py', 'tests/unit/common/db/test_api.py', 'rally/api.py', 'tests/unit/cli/commands/test_task.py', 'rally/common/db/sqlalchemy/api.py', 'rally/common/objects/task.py', 'tests/unit/cli/test_cliutils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/67fad1594d286fc6022b9591fef9092f6ac57c13', 'message': '[db] Optimize sla_check and task status commands\n\nFor sla_check and status commands, it is redundant to load all fields of\na task, subtasks and workloads. This patch extends the api to fix the\nnumber of loaded fields.\n\nChange-Id: I3750befca7f79fd578b420347ceabcb08dfc4a40\n'}]",0,484412,67fad1594d286fc6022b9591fef9092f6ac57c13,10,1,3,9545,,,0,"[db] Optimize sla_check and task status commands

For sla_check and status commands, it is redundant to load all fields of
a task, subtasks and workloads. This patch extends the api to fix the
number of loaded fields.

Change-Id: I3750befca7f79fd578b420347ceabcb08dfc4a40
",git fetch https://review.opendev.org/openstack/rally refs/changes/12/484412/2 && git format-patch -1 --stdout FETCH_HEAD,"['rally/common/db/api.py', 'tests/unit/common/db/test_api.py', 'rally/common/db/sqlalchemy/api.py']",3,40244178c2273ba66a2d1261012646d24c37a456,db_optimization,"from sqlalchemy.orm import Load session = get_session() with session.begin(): pre_query = self.model_query(models.Task, session=session) if detailed: pre_query = pre_query.options( sa_joinedload(models.Task.subtasks).joinedload( models.Subtask.workloads).joinedload( models.Workload.workload_data)) task = pre_query.filter_by(uuid=uuid).first() if not task: raise exceptions.TaskNotFound(uuid=uuid) task.tags = sorted( self._tags_get(uuid, consts.TagType.TASK, session)) if detailed: for subtask in task.subtasks: for workload in subtask.workloads: data = sorted([raw for data in workload.workload_data for raw in data.chunk_data[""raw""]], key=lambda x: x[""timestamp""]) del workload.workload_data workload.data = data return task def task_get_status(self, uuid): task = self.model_query(models.Task).options( sa_loadonly(""status"")).filter_by(uuid=uuid).first() if not task: raise exceptions.TaskNotFound(uuid=uuid) return task.status def task_get_sla_results(self, uuid): session = get_session() with session.begin(): query = self.model_query(models.Task, session=session) query = query.options( sa_loadonly(""pass_sla"", ""status"", ""uuid""). joinedload(models.Task.subtasks). load_only(""sla"", ""pass_sla"", ""status"", ""uuid""). joinedload(models.Subtask.workloads). load_only(""sla"", ""sla_results"", ""pass_sla"", ""uuid"")) task = query.filter_by(uuid=uuid).first() if not task: raise exceptions.TaskNotFound(uuid=uuid) return task"," def _task_get(self, uuid, load_only=None, detailed=False, session=None): pre_query = self.model_query(models.Task, session=session) if load_only and detailed: raise ValueError(""Setting load_only and detailed arguments are "" ""restricted."") options = None if load_only: options = sa_loadonly(load_only) if detailed: options = sa_joinedload(models.Task.subtasks).joinedload( models.Subtask.workloads).joinedload( models.Workload.workload_data) if options: pre_query = pre_query.options(options) task = pre_query.filter_by(uuid=uuid).first() if not task: raise exceptions.TaskNotFound(uuid=uuid) task.tags = sorted(self._tags_get(uuid, consts.TagType.TASK, session)) if detailed: for subtask in task.subtasks: for workload in subtask.workloads: data = sorted([raw for data in workload.workload_data for raw in data.chunk_data[""raw""]], key=lambda x: x[""timestamp""]) del workload.workload_data workload.data = data return task return self._task_get(uuid, detailed=detailed) def task_get_status(self, uuid): return self._task_get(uuid, load_only=""status"").status",74,39
openstack%2Frally~master~I3cc0a5ee68e01481f3c3993503a0495a094250ea,openstack/rally,master,I3cc0a5ee68e01481f3c3993503a0495a094250ea,[cleanup] Skip cleanup deleted resources,NEW,2017-03-16 09:35:17.000000000,2017-12-18 02:40:29.000000000,,"[{'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 22960}]","[{'number': 1, 'created': '2017-03-16 09:35:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b90d6bbc2d67e2e1d0e18ac65f41cb67d6e8f97d', 'message': '[cleanup] Skip cleanup deleted resources\n\nChange-Id: I3cc0a5ee68e01481f3c3993503a0495a094250ea\nCloses-Bug: #1411687\n'}, {'number': 2, 'created': '2017-03-16 13:06:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/2ddaf62fdfa85b64a434c3ddc387918bfb90fc25', 'message': '[cleanup] Skip cleanup deleted resources\n\nChange-Id: I3cc0a5ee68e01481f3c3993503a0495a094250ea\nCloses-Bug: #1411687\n'}, {'number': 3, 'created': '2017-03-17 02:03:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/85951ac705220275311849b645397d069c77199b', 'message': '[cleanup] Skip cleanup deleted resources\n\nChange-Id: I3cc0a5ee68e01481f3c3993503a0495a094250ea\nCloses-Bug: #1411687\n'}, {'number': 4, 'created': '2017-03-17 17:26:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/eaac4271e999dc940b38e60c1179a802c213ed11', 'message': '[cleanup] Skip cleanup deleted resources\n\nChange-Id: I3cc0a5ee68e01481f3c3993503a0495a094250ea\nCloses-Bug: #1411687\n'}, {'number': 5, 'created': '2017-03-18 05:41:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b9634db5b7f7b0be186873e1d37526769e40518f', 'message': '[cleanup] Skip cleanup deleted resources\n\nChange-Id: I3cc0a5ee68e01481f3c3993503a0495a094250ea\nCloses-Bug: #1411687\n'}, {'number': 6, 'created': '2017-03-20 02:33:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4696ee08d1ca6bf0195fd52cc891d5a78da57430', 'message': '[cleanup] Skip cleanup deleted resources\n\nChange-Id: I3cc0a5ee68e01481f3c3993503a0495a094250ea\nCloses-Bug: #1411687\n'}, {'number': 7, 'created': '2017-03-20 06:55:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/7abd24b49d5ba69c9055f5474a0b6baae82ff690', 'message': '[cleanup] Skip cleanup deleted resources\n\nChange-Id: I3cc0a5ee68e01481f3c3993503a0495a094250ea\nCloses-Bug: #1411687\n'}, {'number': 8, 'created': '2017-03-20 08:50:02.000000000', 'files': ['tests/unit/plugins/openstack/cleanup/test_manager.py', 'rally/plugins/openstack/cleanup/manager.py', 'rally/plugins/openstack/cleanup/resources.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/85cd255fcd49ac925367abea01d43fed3bcfdb6d', 'message': '[cleanup] Skip cleanup deleted resources\n\nChange-Id: I3cc0a5ee68e01481f3c3993503a0495a094250ea\nCloses-Bug: #1411687\n'}]",2,446394,85cd255fcd49ac925367abea01d43fed3bcfdb6d,28,3,8,22960,,,0,"[cleanup] Skip cleanup deleted resources

Change-Id: I3cc0a5ee68e01481f3c3993503a0495a094250ea
Closes-Bug: #1411687
",git fetch https://review.opendev.org/openstack/rally refs/changes/94/446394/2 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/plugins/openstack/cleanup/test_manager.py', 'rally/plugins/openstack/cleanup/manager.py']",2,b90d6bbc2d67e2e1d0e18ac65f41cb67d6e8f97d,bug/1411687, if manager.is_deleted(): return ,,6,0
openstack%2Fopenstack-health~master~I0a76d83146648e41258d73477da88fdb25dbd20f,openstack/openstack-health,master,I0a76d83146648e41258d73477da88fdb25dbd20f,Update requirements,NEW,2017-04-27 09:04:32.000000000,2017-12-18 02:40:08.000000000,,"[{'_account_id': 5689}, {'_account_id': 8556}, {'_account_id': 9725}]","[{'number': 1, 'created': '2017-04-27 09:04:32.000000000', 'files': ['requirements.txt', 'test-requirements.txt', 'setup.py'], 'web_link': 'https://opendev.org/openstack/openstack-health/commit/d43c1cb438799c25cd2ad1476140879ea1b3d058', 'message': 'Update requirements\n\nThis commit updates requirements which come from global-requirements. We\nshould keep them up-to-date as possible.\n\nChange-Id: I0a76d83146648e41258d73477da88fdb25dbd20f\n'}]",0,460458,d43c1cb438799c25cd2ad1476140879ea1b3d058,9,3,1,5689,,,0,"Update requirements

This commit updates requirements which come from global-requirements. We
should keep them up-to-date as possible.

Change-Id: I0a76d83146648e41258d73477da88fdb25dbd20f
",git fetch https://review.opendev.org/openstack/openstack-health refs/changes/58/460458/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'test-requirements.txt', 'setup.py']",3,d43c1cb438799c25cd2ad1476140879ea1b3d058,update-requirements," setup_requires=['pbr>=2.0.0'],"," setup_requires=['pbr>=1.8'],",14,14
openstack%2Fproject-config~master~Iff62952e89b8142102e323ddd9cd0040091c157a,openstack/project-config,master,Iff62952e89b8142102e323ddd9cd0040091c157a,Grafana: Use smartSummarize where appropriate,NEW,2016-08-26 06:09:51.000000000,2017-12-18 02:39:51.000000000,,[{'_account_id': 4162}],"[{'number': 1, 'created': '2016-08-26 06:09:51.000000000', 'files': ['grafana/nodepool-osic.yaml', 'grafana/nodepool.yaml', 'grafana/nodepool-internap.yaml', 'grafana/zuul-status.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/69e18ef16e0d9e39381090d496b416fde142b900', 'message': ""Grafana: Use smartSummarize where appropriate\n\nWhich is pretty much everywhere we are using it.  Most similar\ngraphs are already using this; some just haven't been updated.\n\nChange-Id: Iff62952e89b8142102e323ddd9cd0040091c157a\n""}]",2,360912,69e18ef16e0d9e39381090d496b416fde142b900,5,1,1,1,,,0,"Grafana: Use smartSummarize where appropriate

Which is pretty much everywhere we are using it.  Most similar
graphs are already using this; some just haven't been updated.

Change-Id: Iff62952e89b8142102e323ddd9cd0040091c157a
",git fetch https://review.opendev.org/openstack/project-config refs/changes/12/360912/1 && git format-patch -1 --stdout FETCH_HEAD,"['grafana/nodepool-osic.yaml', 'grafana/nodepool.yaml', 'grafana/nodepool-internap.yaml', 'grafana/zuul-status.yaml']",4,69e18ef16e0d9e39381090d496b416fde142b900,," - target: alias(smartSummarize(sumSeries(stats_counts.zuul.pipeline.*.all_jobs), '1h'), 'All Jobs') - target: alias(smartSummarize(stats_counts.gerrit.event.comment-added, '1h'), 'Comment added') - target: alias(smartSummarize(stats_counts.gerrit.event.patchset-created, '1h'), 'Patchset created') - target: alias(smartSummarize(stats_counts.gerrit.event.change-merged, '1h'), 'Change merged')"," - target: alias(summarize(sumSeries(stats_counts.zuul.pipeline.*.all_jobs), '1h'), 'All Jobs') - target: alias(summarize(stats_counts.gerrit.event.comment-added, '1h'), 'Comment added') - target: alias(summarize(stats_counts.gerrit.event.patchset-created, '1h'), 'Patchset created') - target: alias(summarize(stats_counts.gerrit.event.change-merged, '1h'), 'Change merged')",7,7
openstack%2Fdiskimage-builder~master~Ibf583802dda9eda1397e2622f9973e558b2e91ae,openstack/diskimage-builder,master,Ibf583802dda9eda1397e2622f9973e558b2e91ae,Fix ubuntu minimal build failure,NEW,2017-08-08 02:55:48.000000000,2017-12-18 02:37:44.000000000,,"[{'_account_id': 7118}, {'_account_id': 10118}, {'_account_id': 24013}]","[{'number': 1, 'created': '2017-08-08 02:55:48.000000000', 'files': ['diskimage_builder/elements/ubuntu-minimal/root.d/75-ubuntu-minimal-baseinstall'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/e47acafd4231e12ee8e6fdb0ee410dbf203adc4a', 'message': 'Fix ubuntu minimal build failure\n\nEnvironment variable PATH need to be set before running apt-get commands\n\nChange-Id: Ibf583802dda9eda1397e2622f9973e558b2e91ae\nCloses-Bug: #1709227\n'}]",0,491653,e47acafd4231e12ee8e6fdb0ee410dbf203adc4a,8,3,1,24013,,,0,"Fix ubuntu minimal build failure

Environment variable PATH need to be set before running apt-get commands

Change-Id: Ibf583802dda9eda1397e2622f9973e558b2e91ae
Closes-Bug: #1709227
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/53/491653/1 && git format-patch -1 --stdout FETCH_HEAD,['diskimage_builder/elements/ubuntu-minimal/root.d/75-ubuntu-minimal-baseinstall'],1,e47acafd4231e12ee8e6fdb0ee410dbf203adc4a,bug/1709227,"sudo bash -c ""cat > $TARGET_ROOT/tmp.sh << _EOF_ export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin apt-get update apt-get clean apt-get dist-upgrade -y apt-get install -y busybox sudo _EOF_"" sudo bash -c ""echo apt-get install -y python >> $TARGET_ROOT/tmp.sh"" sudo bash -c ""echo apt-get install -y python3 >> $TARGET_ROOT/tmp.sh"" sudo chroot $TARGET_ROOT /bin/bash /tmp.sh sudo rm -f $TARGET_ROOT/tmp.sh","apt_get=""sudo chroot $TARGET_ROOT /usr/bin/apt-get"" # dib-lint: safe_sudo $apt_get update $apt_get clean $apt_get dist-upgrade -y $apt_get install -y busybox sudo $apt_get install -y python $apt_get install -y python3",12,10
openstack%2Fswift~master~Ia5b89962482c030e59a8864657eb08064dbfe48e,openstack/swift,master,Ia5b89962482c030e59a8864657eb08064dbfe48e,EC: return 404 when durable quorum not found,NEW,2017-03-16 18:45:08.000000000,2017-12-18 02:30:58.000000000,,"[{'_account_id': 597}, {'_account_id': 7847}, {'_account_id': 13052}]","[{'number': 1, 'created': '2017-03-16 18:45:08.000000000', 'files': ['test/unit/proxy/test_server.py', 'test/unit/proxy/controllers/test_obj.py', 'swift/proxy/controllers/obj.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/dcf0dff34976cbe5bdb8e3d047c471de9a5c6d36', 'message': 'EC: return 404 when durable quorum not found\n\nPreviously, when getting an EC object, if all backend\nnodes returned 200, but a durable quorum of fragments\nwas not achieved (i.e. a mix of objects\' fragments),\nthen the proxy would always return 503.\n\nIf all backend nodes returned either 404 or 200, but a\ndurable quorum of fragments was not achieved, the proxy\nwould sometimes return 503 and sometimes return 404. The\noutcome depended on whether, on getting a 404, the\nResumingGetter could proceed to get a 200 response or not.\nThe proxy would only return 404 if one of the getters had\na *last* status of 404.\n\nIf there are insufficient fragments to achieve a quorum,\nbut otherwise no errors, the proxy should return a 404.\nThis patch modifies the ECObjectController analysis of\n""bad"" backend responses to include the history of all\ngetters\' responses, such that previously ignored 404\nbackend responses are now considered when forming the\nproxy response.\n\nChange-Id: Ia5b89962482c030e59a8864657eb08064dbfe48e\n'}]",3,446692,dcf0dff34976cbe5bdb8e3d047c471de9a5c6d36,8,3,1,7847,,,0,"EC: return 404 when durable quorum not found

Previously, when getting an EC object, if all backend
nodes returned 200, but a durable quorum of fragments
was not achieved (i.e. a mix of objects' fragments),
then the proxy would always return 503.

If all backend nodes returned either 404 or 200, but a
durable quorum of fragments was not achieved, the proxy
would sometimes return 503 and sometimes return 404. The
outcome depended on whether, on getting a 404, the
ResumingGetter could proceed to get a 200 response or not.
The proxy would only return 404 if one of the getters had
a *last* status of 404.

If there are insufficient fragments to achieve a quorum,
but otherwise no errors, the proxy should return a 404.
This patch modifies the ECObjectController analysis of
""bad"" backend responses to include the history of all
getters' responses, such that previously ignored 404
backend responses are now considered when forming the
proxy response.

Change-Id: Ia5b89962482c030e59a8864657eb08064dbfe48e
",git fetch https://review.opendev.org/openstack/swift refs/changes/92/446692/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/proxy/test_server.py', 'test/unit/proxy/controllers/test_obj.py', 'swift/proxy/controllers/obj.py']",3,dcf0dff34976cbe5bdb8e3d047c471de9a5c6d36,p-ec-return-404," def get_responses(self, include_duplicates=False): for frag_index, sources in self.gets.items(): if include_duplicates or frag_index is None: # TODO: we can get here with less than ec_ndata 416's and may then # return a 416 which is questionable because a non-range get for # same object would return 404 or 503. def extract_response_data(responses, limit=None): for getter, _parts_iter in responses: statuses.extend(getter.statuses[:limit]) reasons.extend(getter.reasons[:limit]) bodies.extend(getter.bodies[:limit]) headers.extend(getter.source_headers[:limit]) extract_response_data(bad_bucket.get_responses()) for bucket in buckets.buckets.values(): # When choosing the best non-success response, include the # non-success responses that successful getters may have # ignored on the way to getting a success response. extract_response_data( bucket.get_responses(include_duplicates=True), limit=-1) if buckets.buckets and not statuses: # this combination of some successful responses, but no bad # responses in any getter, indicates no quorum - force a 404 # response. resp = HTTPNotFound(request=req) else: resp = self.best_response( req, statuses, reasons, bodies, 'Object', headers=headers)"," def get_responses(self): for frag_index, sources in self.gets.items(): if frag_index is None: # TODO: we can get here if all buckets are successful but none # have ec_ndata getters, so bad_bucket may have no gets and we will # return a 503 when a 404 may be more appropriate. We can also get # here with less than ec_ndata 416's and may then return a 416 # which is also questionable because a non-range get for same # object would return 404 or 503. for getter, _parts_iter in bad_bucket.get_responses(): statuses.extend(getter.statuses) reasons.extend(getter.reasons) bodies.extend(getter.bodies) headers.extend(getter.source_headers) resp = self.best_response( req, statuses, reasons, bodies, 'Object', headers=headers)",177,149
openstack%2Ftacker~master~I1b637d0411a9c32f8b4c1d3326cd09d5be928540,openstack/tacker,master,I1b637d0411a9c32f8b4c1d3326cd09d5be928540,Integrate senlin to Tacker for autoscaling management,NEW,2017-03-24 08:06:37.000000000,2017-12-18 02:30:51.000000000,,"[{'_account_id': 2874}, {'_account_id': 6348}, {'_account_id': 8246}, {'_account_id': 15857}, {'_account_id': 18955}, {'_account_id': 19999}, {'_account_id': 20157}, {'_account_id': 20560}, {'_account_id': 22998}]","[{'number': 1, 'created': '2017-03-24 08:06:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/a077ba0caffcb2a69043af5cdbbb22450515f26f', 'message': 'Integrate senlin to Tacker for autoscaling management\n\nThis patch is an initial patch which invites senlin clustering service\nto manage VNF autoscaling feature in Tacker.\n\nblueprint autoscaling-management-with-senlin-resource\n\nChange-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540\n'}, {'number': 2, 'created': '2017-03-29 01:12:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/635a5b9997b6366ac7d75ab0bf8a89f072ba7d76', 'message': 'Integrate senlin to Tacker for autoscaling management\n\nThis patch is an initial patch which invites senlin clustering service\nto manage VNF autoscaling feature in Tacker.\n\nblueprint autoscaling-management-with-senlin-resource\n\nChange-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540\n'}, {'number': 3, 'created': '2017-03-29 08:19:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/0a1bd9837aefd473363363f90e496bba6714bdd3', 'message': 'Integrate senlin to Tacker for autoscaling management\n\nThis patch is an initial patch which invites senlin clustering service\nto manage VNF autoscaling feature in Tacker.\n\nblueprint autoscaling-management-with-senlin-resource\n\nChange-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540\n'}, {'number': 4, 'created': '2017-03-30 08:01:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/941ea12312ce73e61c9c31724d938a5c2ecf7d97', 'message': 'Integrate senlin to Tacker for autoscaling management\n\nThis patch is an initial patch which invites senlin clustering service\nto manage VNF autoscaling feature in Tacker.\n\nblueprint autoscaling-management-with-senlin-resource\n\nChange-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540\n'}, {'number': 5, 'created': '2017-04-06 10:38:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/a674a4ab813cb74e820b70b5f0c100134286cfaa', 'message': 'Integrate senlin to Tacker for autoscaling management\n\nThis patch is an initial patch which invites senlin clustering service\nto manage VNF autoscaling feature in Tacker.\n\nblueprint autoscaling-management-with-senlin-resource\n\nChange-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540\n'}, {'number': 6, 'created': '2017-04-07 08:14:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/fda10ab6568237c7ad10c1e054681297e781bcbf', 'message': 'Integrate senlin to Tacker for autoscaling management\n\nThis patch is an initial patch which invites senlin clustering service\nto manage VNF autoscaling feature in Tacker.\n\nblueprint autoscaling-management-with-senlin-resource\n\nChange-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540\n'}, {'number': 7, 'created': '2017-04-13 08:07:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/80687d4440b86c39c949d03e06f028b2cb4abf6a', 'message': 'Integrate senlin to Tacker for autoscaling management\n\nThis patch is an initial patch which invites senlin clustering service\nto manage VNF autoscaling feature in Tacker.\n\nblueprint autoscaling-management-with-senlin-resource\n\nChange-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540\n'}, {'number': 8, 'created': '2017-05-07 06:04:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/48ae285abe0162be86672b1ce62931941badcfb6', 'message': 'Integrate senlin to Tacker for autoscaling management\n\nThis patch is an initial patch which invites senlin clustering service\nto manage VNF autoscaling feature in Tacker.\n\nImplements: blueprint autoscaling-management-with-senlin-resource\n\nCo-Authored-By: XueFeng Liu <liu.xuefeng1@zte.com.cn>\nChange-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540\n'}, {'number': 9, 'created': '2017-05-22 06:30:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/a5b4d63c805656306752614558abda169b05ff9e', 'message': 'Integrate senlin to Tacker for autoscaling management\n\nThis patch is an initial patch which invites senlin clustering service\nto manage VNF autoscaling feature in Tacker.\n\nImplements: blueprint autoscaling-management-with-senlin-resource\n\nCo-Authored-By: XueFeng Liu <liu.xuefeng1@zte.com.cn>\nChange-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540\n'}, {'number': 10, 'created': '2017-05-22 06:32:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/4c5e8a2371c973e6c6552f924bdde4c721347a95', 'message': 'Integrate senlin to Tacker for autoscaling management\n\nThis patch is an initial patch which invites senlin clustering service\nto manage VNF autoscaling feature in Tacker.\n\nImplements: blueprint autoscaling-management-with-senlin-resource\n\nCo-Authored-By: XueFeng Liu <liu.xuefeng1@zte.com.cn>\nChange-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540\n'}, {'number': 11, 'created': '2017-05-22 06:46:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/a00bf6b5b73d8260faa3ca8623d413cdf008cca6', 'message': 'Integrate senlin to Tacker for autoscaling management\n\nThis patch is an initial patch which invites senlin clustering service\nto manage VNF autoscaling feature in Tacker.\n\nImplements: blueprint autoscaling-management-with-senlin-resource\n\nCo-Authored-By: XueFeng Liu <liu.xuefeng1@zte.com.cn>\nChange-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540\n'}, {'number': 12, 'created': '2017-07-12 05:52:35.000000000', 'files': ['samples/tosca-templates/vnfd/tosca-vnfd-senlin-scale.yaml', 'tacker/extensions/vnfm.py', 'tacker/vnfm/infra_drivers/openstack/translate_template.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/e7fc9914654f7c84afee8e3fb97adfa32b4292d1', 'message': 'Integrate senlin to Tacker for autoscaling management\n\nThis patch is an initial patch which invites senlin clustering service\nto manage VNF autoscaling feature in Tacker.\n\nImplements: blueprint autoscaling-management-with-senlin-resource\n\nCo-Authored-By: XueFeng Liu <liu.xuefeng1@zte.com.cn>\nChange-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540\n'}]",15,449471,e7fc9914654f7c84afee8e3fb97adfa32b4292d1,39,9,12,6348,,,0,"Integrate senlin to Tacker for autoscaling management

This patch is an initial patch which invites senlin clustering service
to manage VNF autoscaling feature in Tacker.

Implements: blueprint autoscaling-management-with-senlin-resource

Co-Authored-By: XueFeng Liu <liu.xuefeng1@zte.com.cn>
Change-Id: I1b637d0411a9c32f8b4c1d3326cd09d5be928540
",git fetch https://review.opendev.org/openstack/tacker refs/changes/71/449471/12 && git format-patch -1 --stdout FETCH_HEAD,"['samples/tosca-templates/vnfd/tosca-vnfd-senlin-scale.yaml', 'tacker/vnfm/infra_drivers/openstack/translate_template.py']",2,a077ba0caffcb2a69043af5cdbbb22450515f26f,bp/autoscaling-management-with-senlin-resource," if policy['driver'] == 'heat': resources, scaling_group_names =\ self._convert_to_heat_scaling_policy( policy['properties'], scale_resource_type, name) elif policy['driver'] == 'senlin': resources = self._handle_senlin_scaling_policy( vnfd_dict, policy['properties'], name) def _handle_senlin_scaling_policy(self, vnfd_dict, properties, name): profile_tpl = self._create_profile_tpl(name) cluster_tpl = self._create_cluster_tpl(properties, name) policy_tpl = self._create_policy_tpl(properties, name) receiver_tpl = self._create_receiver_tpl(properties, name) alarm_tpl = self._create_alarm_tpl(properties, name) resources_tpl = dict(profile_tpl.items() + cluster_tpl.items() + policy_tpl.items() + receiver_tpl.items()) return resources_tpl @log.log def _create_profile_tpl(self, name): profile_name = name + '_profile' profile_tpl = {} profile = {} profile['type'] = 'OS::Senlin::Profile' properties = {} properties['type'] = 'os.heat.stack-1.0' properties['template'] = self.heat_template_yaml profile['properties'] = properties profile_tpl[profile_name] = profile return profile_tpl @log.log def _create_cluster_tpl(self, properties, name): cluster_name = name + '_cluster' profile_name = name + '_profile' cluster_tpl = {} cluster = {} cluster['type'] = 'OS::Senlin::Cluster' props = {} props['desired_capacity'] = properties.get('default_instances') props['min_size'] = properties.get('min_instance') props['max_size'] = properties.get('max_instance') props['profile'] = {'get_resource': profile_name} cluster['properties'] = props cluster_tpl[cluster_name] = cluster return cluster_tpl @log.log def _create_policy_tpl(self, properties, name): policy_name = name + '_policy' cluster_name = name + '_cluster' policy_tpl = {} policy = {} policy['type'] = 'OS::Senlin::Policy' props = {} props['type'] = 'senlin.policy.scaling-1.0' props['bindings'] = [] props['bindings'].append({'get_resource': cluster_name}) props['properties'] = { 'event': 'CLUSTER_SCALE_OUT', 'adjustment': { 'type': 'CHANGE_IN_CAPACITY', 'number': properties['increment'] } } policy['properties'] = props policy_tpl[policy_name] = policy return policy_tpl @log.log def _create_receiver_tpl(self, properties, name): receiver_name = name + '_receiver' cluster_name = name + '_cluster' receiver_tpl = {} receiver = {} receiver['type'] = 'OS::Senlin::Receiver' props = { 'cluster': {'get_resource': cluster_name}, 'action': 'CLUSTER_SCALE_OUT', 'type': 'webhook' } receiver['properties'] = props receiver_tpl[receiver_name] = receiver return receiver_tpl @log.log def _create_alarm_tpl(self, properties, name): return @log.log"," resources, scaling_group_names =\ self._convert_to_heat_scaling_policy( policy['properties'], scale_resource_type, name)",166,3
openstack%2Frally~master~I7270d657ea80e52f2c3f05c6d8ca42889e2871b9,openstack/rally,master,I7270d657ea80e52f2c3f05c6d8ca42889e2871b9,Add NeutronNetworks.create_and_list_qos_policies,NEW,2017-06-08 07:56:21.000000000,2017-12-18 02:30:41.000000000,,"[{'_account_id': 8871}, {'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 22960}, {'_account_id': 25072}, {'_account_id': 26135}]","[{'number': 1, 'created': '2017-06-08 07:56:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b8dbac3d947cdeb34e6ba21bf95f4de063690c24', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 2, 'created': '2017-06-09 06:50:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/af929bcad4bf68a8eed5f2646700a72f716d542d', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 3, 'created': '2017-06-09 13:24:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a75f4c71bd51937df2f04316822d954d5b379aa6', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 4, 'created': '2017-06-10 15:32:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/3c4ad838df29d5624b1656a9b82d1b80cbe041cd', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 5, 'created': '2017-06-10 15:51:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/da1ec06a7c42a993465c086a929b3a7f9b22ccfe', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 6, 'created': '2017-06-27 15:51:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b9a779622e1765c1098b4a6f8a766f1103a2ddae', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 7, 'created': '2017-06-28 00:54:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/dced0a0e3d480061315a52001d73f3c2986a001b', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 8, 'created': '2017-06-28 02:18:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6e09fb5b3eb74893c60b54b7b1e113cb27b0ace5', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 9, 'created': '2017-06-28 02:20:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/7085b642e4b7509bce7dabc11a6d14feab3e4a1a', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 10, 'created': '2017-06-28 02:32:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/3d52f3b44ea6c59125d70bde0a9c0594aa0a9ab7', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 11, 'created': '2017-06-28 03:58:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b9aceac7926911cd2dd925f27c2be1b37428a7b2', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 12, 'created': '2017-06-28 08:17:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/fc1de8e8c0ff7ef88f50c8a60e8a3f70453eaee4', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 13, 'created': '2017-06-28 13:50:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c8c39b49cf00a43026fbd0414db6a5c574222e67', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 14, 'created': '2017-06-28 14:09:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/284649f9443982939ef36a4460ad0413fc382e5b', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 15, 'created': '2017-07-05 01:17:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/2ce26429eee5ac5c20dc1815299e97590dbd9183', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 16, 'created': '2017-07-05 15:50:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/f70dba161094ed0ffc18e4cc77fe45509838e2d4', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 17, 'created': '2017-07-06 03:29:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/3a27285bb8f54db3fc0f83b3be8ec13ff32b1bfa', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}, {'number': 18, 'created': '2017-07-10 05:09:28.000000000', 'files': ['samples/tasks/scenarios/neutron/create-and-list-qos-policies.yaml', 'tests/unit/plugins/openstack/scenarios/neutron/test_network.py', 'samples/tasks/scenarios/neutron/create-and-list-qos-policies.json', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'rally/plugins/openstack/cleanup/resources.py', 'rally-jobs/rally-neutron.yaml', 'tests/unit/plugins/openstack/scenarios/neutron/test_utils.py', 'rally/plugins/openstack/scenarios/neutron/network.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/f0df3cdac1274b89dcdea8736c6f34db6598007c', 'message': 'Add NeutronNetworks.create_and_list_qos_policies\n\nThis scenario first creates a network qos policy, then list existing\nnetwork qos policies.\n\nChange-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9\n'}]",8,472129,f0df3cdac1274b89dcdea8736c6f34db6598007c,79,6,18,26135,,,0,"Add NeutronNetworks.create_and_list_qos_policies

This scenario first creates a network qos policy, then list existing
network qos policies.

Change-Id: I7270d657ea80e52f2c3f05c6d8ca42889e2871b9
",git fetch https://review.opendev.org/openstack/rally refs/changes/29/472129/9 && git format-patch -1 --stdout FETCH_HEAD,"['samples/tasks/scenarios/neutron/create-and-list-qos-policies.json', 'samples/tasks/scenarios/neutron/create-and-list-qos-policy.yaml', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'rally-jobs/rally-neutron.yaml', 'rally/plugins/openstack/scenarios/neutron/network.py']",5,b8dbac3d947cdeb34e6ba21bf95f4de063690c24,," @validation.add(""required_services"", services=[consts.Service.NEUTRON]) @validation.add(""required_platform"", platform=""openstack"", admin=True) @scenario.configure(context={""cleanup"": [""neutron""]}, name=""NeutronNetworks.create_and_list_qos_policies"") class CreateAndListQosPolicies(utils.NeutronScenario): def run(self, policies_args=None): """"""Create a network qos policy and list the policies Measure the ""neutron create_qos_policy"" and ""neutron list_qos_policy"" command performance :param policies_args: dict, POST /v2.0/qos/policies/ request options """""" policies_args = policies_args or {} self._create_qos_policy(**policies_args) self._list_qos_policies()",,92,0
openstack%2Frally~master~Ia350639b68e00f28b02ee75d5560471f3b345f91,openstack/rally,master,Ia350639b68e00f28b02ee75d5560471f3b345f91,Task Hangs during create_volume_from_snapshot,NEW,2017-01-06 10:02:20.000000000,2017-12-18 02:29:35.000000000,,"[{'_account_id': 1523}, {'_account_id': 1736}, {'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 21528}, {'_account_id': 22960}, {'_account_id': 23435}]","[{'number': 1, 'created': '2017-01-06 10:02:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/92ac399d27e820746521b4617ef2b6d3df03f29b', 'message': 'Task Hangs during create_volume_from_snapshot\n\nIf we progress CinderVolumes.create_volume_from_snapshot\nwith do_delete arguments, the task will hang forever.\nThe reason is that the source code tries to delete snapshot\nfirst before cloned volume from snapshot.\nHowever the snapshot cannot deleted because of cloned volume.\nWe need to change the step of deleting snapshots and volumes.\n\nCloses-Bug: #1654519\nChange-Id: Ia350639b68e00f28b02ee75d5560471f3b345f91\n'}, {'number': 2, 'created': '2017-07-19 09:48:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/69e23cab92eb1599d350fefa1806881d806d8dc0', 'message': 'Task Hangs during create_volume_from_snapshot\n\nIf we progress CinderVolumes.create_volume_from_snapshot\nwith do_delete arguments, the task will hang forever.\nThe reason is that the source code tries to delete snapshot\nfirst before cloned volume from snapshot.\nHowever the snapshot cannot deleted because of cloned volume.\nWe need to change the step of deleting snapshots and volumes.\n\nCloses-Bug: #1654519\nChange-Id: Ia350639b68e00f28b02ee75d5560471f3b345f91\n'}, {'number': 3, 'created': '2017-07-29 07:37:59.000000000', 'files': ['rally/plugins/openstack/scenarios/cinder/volumes.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/61c4d7d7735fc37c89354fe56a907693c1d726cf', 'message': 'Task Hangs during create_volume_from_snapshot\n\nIf we progress CinderVolumes.create_volume_from_snapshot\nwith do_delete arguments, the task will hang forever.\nThe reason is that the source code tries to delete snapshot\nfirst before cloned volume from snapshot.\nHowever the snapshot cannot deleted because of cloned volume.\nWe need to change the step of deleting snapshots and volumes.\n\nCloses-Bug: #1654519\nChange-Id: Ia350639b68e00f28b02ee75d5560471f3b345f91\n'}]",0,417334,61c4d7d7735fc37c89354fe56a907693c1d726cf,31,7,3,1523,,,0,"Task Hangs during create_volume_from_snapshot

If we progress CinderVolumes.create_volume_from_snapshot
with do_delete arguments, the task will hang forever.
The reason is that the source code tries to delete snapshot
first before cloned volume from snapshot.
However the snapshot cannot deleted because of cloned volume.
We need to change the step of deleting snapshots and volumes.

Closes-Bug: #1654519
Change-Id: Ia350639b68e00f28b02ee75d5560471f3b345f91
",git fetch https://review.opendev.org/openstack/rally refs/changes/34/417334/1 && git format-patch -1 --stdout FETCH_HEAD,['rally/plugins/openstack/scenarios/cinder/volumes.py'],1,92ac399d27e820746521b4617ef2b6d3df03f29b,bug/1654519, self._delete_snapshot(snapshot), self._delete_snapshot(snapshot),1,1
openstack%2Fironic~master~Icc0ca13c06f3a5e30ee464b6fa331a62e366afeb,openstack/ironic,master,Icc0ca13c06f3a5e30ee464b6fa331a62e366afeb,WIP: wait for resources after destroy,NEW,2017-08-15 07:03:39.000000000,2017-12-18 02:29:15.000000000,,"[{'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 14525}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19339}]","[{'number': 1, 'created': '2017-08-15 07:03:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f1909b4878628e41071aa85173a67475abadb9b0', 'message': 'WIP: wait for resources after destroy\n\nChange-Id: Icc0ca13c06f3a5e30ee464b6fa331a62e366afeb\nDepends-On: I0bd3a65dddc59857d411f803cc874e7a564a62f5\n'}, {'number': 2, 'created': '2017-08-15 07:25:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2a635dad2a729a75f3c4122abc67801a55dfc1cf', 'message': 'WIP: wait for resources after destroy\n\nChange-Id: Icc0ca13c06f3a5e30ee464b6fa331a62e366afeb\nDepends-On: I0bd3a65dddc59857d411f803cc874e7a564a62f5\n'}, {'number': 3, 'created': '2017-08-15 08:47:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/207ed04d1ea8fe29b27c9d824667fd01870c52e0', 'message': 'WIP: wait for resources after destroy\n\nChange-Id: Icc0ca13c06f3a5e30ee464b6fa331a62e366afeb\nDepends-On: I0bd3a65dddc59857d411f803cc874e7a564a62f5\n'}, {'number': 4, 'created': '2017-08-15 19:08:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c90c19c3093812f8b7c66291cac5fadb67c929ff', 'message': 'WIP: wait for resources after destroy\n\nChange-Id: Icc0ca13c06f3a5e30ee464b6fa331a62e366afeb\nDepends-On: I0bd3a65dddc59857d411f803cc874e7a564a62f5\nDepends-On: I0874fe3b3628cb3e662ee01f24c4599247fdc82d\n'}, {'number': 5, 'created': '2017-08-16 08:41:41.000000000', 'files': ['devstack/upgrade/resources.sh'], 'web_link': 'https://opendev.org/openstack/ironic/commit/a71db59a516c19db645f6d631a3b455052aa3117', 'message': 'WIP: wait for resources after destroy\n\nChange-Id: Icc0ca13c06f3a5e30ee464b6fa331a62e366afeb\nDepends-On: I0bd3a65dddc59857d411f803cc874e7a564a62f5\nDepends-On: I0874fe3b3628cb3e662ee01f24c4599247fdc82d\nDepends-On: I84ca621f0200838c04e267101848dc7ca16a5cfe\n'}]",0,493767,a71db59a516c19db645f6d631a3b455052aa3117,37,6,5,14525,,,0,"WIP: wait for resources after destroy

Change-Id: Icc0ca13c06f3a5e30ee464b6fa331a62e366afeb
Depends-On: I0bd3a65dddc59857d411f803cc874e7a564a62f5
Depends-On: I0874fe3b3628cb3e662ee01f24c4599247fdc82d
Depends-On: I84ca621f0200838c04e267101848dc7ca16a5cfe
",git fetch https://review.opendev.org/openstack/ironic refs/changes/67/493767/1 && git format-patch -1 --stdout FETCH_HEAD,['devstack/upgrade/resources.sh'],1,f1909b4878628e41071aa85173a67475abadb9b0,(HEAD," ""force_destroy"") wait_for_ironic_resources wait_for_nova_resources ""vcpus"" $total_cpus ;;",,4,0
openstack%2Fdiskimage-builder~master~I884ccc668b0e8a5e4990290e47fd495931a35f4c,openstack/diskimage-builder,master,I884ccc668b0e8a5e4990290e47fd495931a35f4c,Add networkd-dhcp element,NEW,2017-04-24 18:01:10.000000000,2017-12-18 02:28:48.000000000,,"[{'_account_id': 10035}, {'_account_id': 10118}, {'_account_id': 21741}]","[{'number': 1, 'created': '2017-04-24 18:01:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/c24cffb87d3152802c276d13f7cb679a7df425a9', 'message': 'Add networkd-dhcp element\n\nThis is a much much simpler replacement for dhcp-all interfaces using\nnetworkd, which supports this functionality out of the box.\n\nChange-Id: I884ccc668b0e8a5e4990290e47fd495931a35f4c\n'}, {'number': 2, 'created': '2017-04-24 18:15:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/471b7fd2492323dad8d13cf2a158ea7797115c1d', 'message': 'Add networkd-dhcp element\n\nThis is a much much simpler replacement for dhcp-all interfaces using\nnetworkd, which supports this functionality out of the box.\n\nChange-Id: I884ccc668b0e8a5e4990290e47fd495931a35f4c\n'}, {'number': 3, 'created': '2017-04-24 18:29:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/27709d2d7a731267d223034fde48efb6ac1ac2d6', 'message': 'Add networkd-dhcp element\n\nThis is a much much simpler replacement for dhcp-all interfaces using\nnetworkd, which supports this functionality out of the box.\n\nChange-Id: I884ccc668b0e8a5e4990290e47fd495931a35f4c\n'}, {'number': 4, 'created': '2017-04-24 19:08:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/7cb673b6a843994b1fa4fe055b3fd1ee430459b8', 'message': 'Add networkd-dhcp element\n\nThis is a much much simpler replacement for dhcp-all interfaces using\nnetworkd, which supports this functionality out of the box.\n\nChange-Id: I884ccc668b0e8a5e4990290e47fd495931a35f4c\n'}, {'number': 5, 'created': '2017-08-18 18:53:36.000000000', 'files': ['diskimage_builder/elements/networkd-dhcp/README.rst', 'diskimage_builder/elements/networkd-dhcp/finalise.d/90-resolved-resolvconf', 'diskimage_builder/elements/networkd-dhcp/element-deps', 'diskimage_builder/elements/networkd-dhcp/package-installs.yaml', 'diskimage_builder/elements/networkd-dhcp/install.d/50-networkd-dhcp'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/0831d93616649f3ef5cf280698b7907397bbc3d2', 'message': 'Add networkd-dhcp element\n\nThis is a much much simpler replacement for dhcp-all interfaces using\nnetworkd, which supports this functionality out of the box.\n\nChange-Id: I884ccc668b0e8a5e4990290e47fd495931a35f4c\n'}]",3,459419,0831d93616649f3ef5cf280698b7907397bbc3d2,17,3,5,10035,,,0,"Add networkd-dhcp element

This is a much much simpler replacement for dhcp-all interfaces using
networkd, which supports this functionality out of the box.

Change-Id: I884ccc668b0e8a5e4990290e47fd495931a35f4c
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/19/459419/5 && git format-patch -1 --stdout FETCH_HEAD,"['diskimage_builder/elements/networkd-dhcp/README.rst', 'diskimage_builder/elements/networkd-dhcp/install.d/50-networkd-dhcp']",2,c24cffb87d3152802c276d13f7cb679a7df425a9,,#!/bin/bash if [ ${DIB_DEBUG_TRACE:-0} -gt 0 ]; then set -x fi set -eu set -o pipefail DIB_NETWORKD_DHCP_MATCH=${DIB_NETWORKD_DHCP_MATCH:-'en*'} cat << EOF > /etc/systemd/network/dib-dhcp.network [Match] Name=en* [Network] DHCP=ipv4 EOF ,,22,0
openstack%2Fswift~master~I24bcc9ffc8a92d67ffb7460c191ecbf64349d818,openstack/swift,master,I24bcc9ffc8a92d67ffb7460c191ecbf64349d818,Expose encryption status to client,NEW,2016-07-29 01:30:29.000000000,2017-12-18 02:27:54.000000000,,"[{'_account_id': 6968}, {'_account_id': 12279}, {'_account_id': 13052}, {'_account_id': 15343}]","[{'number': 1, 'created': '2016-07-29 01:30:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a84a73b3974ace5d0d714ad0fe245c23ce67e811', 'message': 'Expose encryption status to client\n\n... if the operator turns on expose_encryption_status in the keymaster\nfilter config.\n\nChange-Id: I24bcc9ffc8a92d67ffb7460c191ecbf64349d818\nCloses-Bug: #1607116\n'}, {'number': 2, 'created': '2016-08-19 01:05:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0c2dbc54aa599fe80c7ea2ef0061ad534032ad08', 'message': 'Expose encryption status to client\n\n... if the operator turns on expose_encryption_status in the keymaster\nfilter config.\n\nChange-Id: I24bcc9ffc8a92d67ffb7460c191ecbf64349d818\nCloses-Bug: #1607116\n'}, {'number': 3, 'created': '2016-08-19 17:27:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/64db7929ab179ad83a2582e555c4ce3409a308ea', 'message': 'Expose encryption status to client\n\n... if the operator turns on expose_encryption_status in the keymaster\nfilter config.\n\nChange-Id: I24bcc9ffc8a92d67ffb7460c191ecbf64349d818\nCloses-Bug: #1607116\n'}, {'number': 4, 'created': '2016-08-26 00:47:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c3a740af16ea1e37d3d14152f8ee4c64415c15e6', 'message': 'Expose encryption status to client\n\n... if the operator turns on expose_encryption_status in the keymaster\nfilter config.\n\nChange-Id: I24bcc9ffc8a92d67ffb7460c191ecbf64349d818\nCloses-Bug: #1607116\n'}, {'number': 5, 'created': '2016-08-26 18:57:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6132e6d6ed7b3b5779701accf07a870d3e13e2a9', 'message': 'Expose encryption status to client\n\n... if the operator turns on expose_encryption_status in the keymaster\nfilter config.\n\nChange-Id: I24bcc9ffc8a92d67ffb7460c191ecbf64349d818\nCloses-Bug: #1607116\n'}, {'number': 6, 'created': '2016-09-19 22:00:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4d5ab497d381a8f3fe178effe9c6c5c57656b0d7', 'message': ""Expose encryption status to client\n\n... if the operator turns on expose_encryption_status in the keymaster\nfilter config. The headers returned are:\n\n * X-Encryption-Body-Is-Encrypted\n   - 'true' if the stored object was encrypted; not present otherwise.\n     Note that empty objects are considered not-encrypted.\n\n * X-Encryption-Meta-Is-Encrypted\n   - 'true' if any of the stored object user-metadata was encrypted; not\n     present otherwise. If no user-metadata was stored, it will be\n     considered not-encrypted. In theory, there is the potential for\n     unencrypted user-metadata to also exist (if there was some\n     third-party middleware after encryption that would insert it), but\n     in practice the presence of *any* encrypted user-metadata should\n     imply that *all* user-metadata was encrypted.\n\nChange-Id: I24bcc9ffc8a92d67ffb7460c191ecbf64349d818\nCloses-Bug: #1607116\n""}, {'number': 7, 'created': '2016-09-19 22:56:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4b78cdeafd9950024d51cff83ef0194469feaf78', 'message': ""Expose encryption status to client\n\n... if the operator turns on expose_encryption_status in the keymaster\nfilter config. The headers returned are:\n\n * X-Encryption-Body-Is-Encrypted\n   - 'true' if the stored object was encrypted; not present otherwise.\n     Note that empty objects are considered not-encrypted.\n\n * X-Encryption-Meta-Is-Encrypted\n   - 'true' if any of the stored object user-metadata was encrypted; not\n     present otherwise. If no user-metadata was stored, it will be\n     considered not-encrypted. In theory, there is the potential for\n     unencrypted user-metadata to also exist (if there was some\n     third-party middleware after encryption that would insert it), but\n     in practice the presence of *any* encrypted user-metadata should\n     imply that *all* user-metadata was encrypted.\n\nChange-Id: I24bcc9ffc8a92d67ffb7460c191ecbf64349d818\nCloses-Bug: #1607116\n""}, {'number': 8, 'created': '2017-01-26 22:58:32.000000000', 'files': ['api-ref/source/parameters.yaml', 'api-ref/source/storage-object-services.inc', 'swift/common/middleware/crypto/keymaster.py', 'doc/source/overview_encryption.rst', 'etc/proxy-server.conf-sample', 'test/unit/common/middleware/crypto/test_encryption.py', 'test/unit/common/middleware/crypto/crypto_helpers.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/5a8e91947cfe8e00b70f54a244ac601490171ba3', 'message': ""Expose encryption status to client\n\n... if the operator turns on expose_encryption_status in the keymaster\nfilter config. The headers returned are:\n\n * X-Encryption-Body-Is-Encrypted\n   - 'true' if the stored object was encrypted; not present otherwise.\n     Note that empty objects are considered not-encrypted.\n\n * X-Encryption-Meta-Is-Encrypted\n   - 'true' if any of the stored object user-metadata was encrypted; not\n     present otherwise. If no user-metadata was stored, it will be\n     considered not-encrypted. In theory, there is the potential for\n     unencrypted user-metadata to also exist (if there was some\n     third-party middleware after encryption that would insert it), but\n     in practice the presence of *any* encrypted user-metadata should\n     imply that *all* user-metadata was encrypted.\n\nChange-Id: I24bcc9ffc8a92d67ffb7460c191ecbf64349d818\nCloses-Bug: #1607116\n""}]",0,348604,5a8e91947cfe8e00b70f54a244ac601490171ba3,28,4,8,15343,,,0,"Expose encryption status to client

... if the operator turns on expose_encryption_status in the keymaster
filter config. The headers returned are:

 * X-Encryption-Body-Is-Encrypted
   - 'true' if the stored object was encrypted; not present otherwise.
     Note that empty objects are considered not-encrypted.

 * X-Encryption-Meta-Is-Encrypted
   - 'true' if any of the stored object user-metadata was encrypted; not
     present otherwise. If no user-metadata was stored, it will be
     considered not-encrypted. In theory, there is the potential for
     unencrypted user-metadata to also exist (if there was some
     third-party middleware after encryption that would insert it), but
     in practice the presence of *any* encrypted user-metadata should
     imply that *all* user-metadata was encrypted.

Change-Id: I24bcc9ffc8a92d67ffb7460c191ecbf64349d818
Closes-Bug: #1607116
",git fetch https://review.opendev.org/openstack/swift refs/changes/04/348604/8 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/middleware/crypto/keymaster.py', 'etc/proxy-server.conf-sample']",2,a84a73b3974ace5d0d714ad0fe245c23ce67e811,bug/1607116,"# Sets whether to reveal the encryption status of body and metadata to clients # during object requests. This allows end-users to verify whether an object is # encrypted or not, and may be useful for certain audit requirements. On the # other hand, an operator may not wish to reveal any encryption-related # configuration to clients, including whether encryption is even enabled. # expose_encryption_status = False ",,26,2
openstack%2Fironic~master~I0c62b504600cebcc154be504efe937d58e465ab6,openstack/ironic,master,I0c62b504600cebcc154be504efe937d58e465ab6,Add pluggable credentials storage,NEW,2015-05-22 17:06:56.000000000,2017-12-18 02:27:51.000000000,,"[{'_account_id': 6618}, {'_account_id': 7711}, {'_account_id': 9751}, {'_account_id': 12081}, {'_account_id': 12356}]","[{'number': 1, 'created': '2015-05-22 17:06:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ecd8d622d8ec852c69d412b7447e6956fda98752', 'message': 'Add pluggable credentials storage\n\nThis change adds ability to store SSH credentials in Keystone.\n\nImplements blueprint: pluggable-credential-storage\nImplements blueprint: credential-secure-storage\n\nChange-Id: I0c62b504600cebcc154be504efe937d58e465ab6\n'}, {'number': 2, 'created': '2015-05-26 13:44:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9545a07a7cb8a5f67af0157d626328ee4648cfe2', 'message': 'Add pluggable credentials storage\n\nThis change adds ability to store SSH credentials in Keystone.\n\nImplements blueprint: pluggable-credential-storage\nImplements blueprint: credential-secure-storage\n\nChange-Id: I0c62b504600cebcc154be504efe937d58e465ab6\n'}, {'number': 3, 'created': '2015-05-27 18:49:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/41f2fe409d23b661322046deb4fb219cc82ba35a', 'message': 'Add pluggable credentials storage\n\nThis change adds ability to store SSH credentials in Keystone.\n\nDepends-On: Icb5d0eb36491b5eff9137173206654b002e3bd55\n\nImplements blueprint: pluggable-credential-storage\nImplements blueprint: credential-secure-storage\n\nChange-Id: I0c62b504600cebcc154be504efe937d58e465ab6\n'}, {'number': 4, 'created': '2015-05-29 16:27:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/228f0de5d7d953e6800fa38bcb4b69bb0388b409', 'message': 'Add pluggable credentials storage\n\nThis change adds ability to store SSH credentials in Keystone.\n\nDepends-On: Icb5d0eb36491b5eff9137173206654b002e3bd55\n\nImplements blueprint: pluggable-credential-storage\nImplements blueprint: credential-secure-storage\n\nChange-Id: I0c62b504600cebcc154be504efe937d58e465ab6\n'}, {'number': 5, 'created': '2015-06-18 17:04:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/daab15039f603ac2502f2fb811c265b7f0be50fc', 'message': 'Add pluggable credentials storage\n\nThis change adds ability to store SSH credentials in Keystone.\n\nDepends-On: Icb5d0eb36491b5eff9137173206654b002e3bd55\n\nImplements blueprint: pluggable-credential-storage\nImplements blueprint: credential-secure-storage\n\nChange-Id: I0c62b504600cebcc154be504efe937d58e465ab6\n'}, {'number': 6, 'created': '2015-08-25 14:38:27.000000000', 'files': ['ironic/api/controllers/v1/driver.py', 'ironic/credentials/none.py', 'ironic/tests/api/v1/test_nodes.py', 'ironic/common/keystone.py', 'ironic/common/credentials_factory.py', 'ironic/common/exception.py', 'ironic/tests/conductor/test_manager.py', 'requirements.txt', 'etc/ironic/ironic.conf.sample', 'ironic/credentials/base.py', 'ironic/credentials/encrypted_db.py', 'ironic/drivers/modules/ssh.py', 'ironic/credentials/__init__.py', 'ironic/api/controllers/v1/utils.py', 'setup.cfg', 'ironic/credentials/keystone.py', 'ironic/api/controllers/v1/node.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/a4dd5585f8497d630a3d440408d06a8438ab7855', 'message': 'Add pluggable credentials storage\n\nThis change adds ability to store SSH credentials in Keystone.\n\nDepends-On: Icb5d0eb36491b5eff9137173206654b002e3bd55\n\nImplements blueprint: pluggable-credential-storage\nImplements blueprint: credential-secure-storage\n\nChange-Id: I0c62b504600cebcc154be504efe937d58e465ab6\n'}]",6,185074,a4dd5585f8497d630a3d440408d06a8438ab7855,36,5,6,12356,,,0,"Add pluggable credentials storage

This change adds ability to store SSH credentials in Keystone.

Depends-On: Icb5d0eb36491b5eff9137173206654b002e3bd55

Implements blueprint: pluggable-credential-storage
Implements blueprint: credential-secure-storage

Change-Id: I0c62b504600cebcc154be504efe937d58e465ab6
",git fetch https://review.opendev.org/openstack/ironic refs/changes/74/185074/3 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/common/utils.py', 'ironic/common/keystone.py', 'ironic/common/credentials_factory.py', 'ironic/common/exception.py', 'etc/ironic/ironic.conf.sample', 'ironic/credentials/base.py', 'ironic/drivers/modules/ssh.py', 'ironic/credentials/__init__.py', 'ironic/api/controllers/v1/utils.py', 'ironic/credentials/drivers_credentials_fields.py', 'setup.cfg', 'ironic/credentials/keystone.py', 'ironic/api/controllers/v1/node.py']",13,ecd8d622d8ec852c69d412b7447e6956fda98752,bp/pluggable-credential-storage," **api_utils.upload_credentials(node.as_dict())) node = Node(**api_utils.update_credentials(node_dict, patch)) credentials_id = rpc_node.driver_info.get('credentials_id') api_utils.delete_credentials(credentials_id)"," **node.as_dict()) node = Node(**api_utils.apply_jsonpatch(node_dict, patch))",313,17
openstack%2Fnose-html-output~master~I170d65a73bc285db9f47cc77c0913b364265e9ca,openstack/nose-html-output,master,I170d65a73bc285db9f47cc77c0913b364265e9ca,Removed usage of relative imports,NEW,2014-05-13 14:51:29.000000000,2017-12-18 02:27:39.000000000,,"[{'_account_id': 4146}, {'_account_id': 4162}, {'_account_id': 6316}, {'_account_id': 7069}, {'_account_id': 7233}, {'_account_id': 16272}]","[{'number': 1, 'created': '2014-05-13 14:51:29.000000000', 'files': ['htmloutput/htmloutput.py'], 'web_link': 'https://opendev.org/openstack/nose-html-output/commit/98d41325b3cb24ddacd22ed0287a1217bf467403', 'message': 'Removed usage of relative imports\n\nThis is needed for Python3.\n\nChange-Id: I170d65a73bc285db9f47cc77c0913b364265e9ca\n'}]",0,93458,98d41325b3cb24ddacd22ed0287a1217bf467403,15,6,1,7680,,,0,"Removed usage of relative imports

This is needed for Python3.

Change-Id: I170d65a73bc285db9f47cc77c0913b364265e9ca
",git fetch https://review.opendev.org/openstack/nose-html-output refs/changes/58/93458/1 && git format-patch -1 --stdout FETCH_HEAD,['htmloutput/htmloutput.py'],1,98d41325b3cb24ddacd22ed0287a1217bf467403,relative-import,from . import version,import version,1,1
openstack%2Fswift~master~I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7,openstack/swift,master,I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7,EC: Avoid conflicts when ssync'ing fragments,NEW,2015-05-08 14:30:05.000000000,2017-12-18 02:27:04.000000000,,"[{'_account_id': 7479}, {'_account_id': 7847}, {'_account_id': 13052}, {'_account_id': 15343}]","[{'number': 1, 'created': '2015-05-08 14:30:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/09790a7d4572ded72f20a2c2a8041f6529170aae', 'message': ""Don't ssync fragments that conflict with another fragment\n\nWhen using ssync to revert EC fragments, the ssync receiver will\nindicate that the reverted fragment is missing even if the receiver\nhas another fragment of same object at same time. When the ssync\nsender PUTs the fragment the receiver then rightly gets a 409 conflict\ndue to the other existing fragment.\n\nThis is undesirable because (a) it wastes bytes sent over network\nthat will not be written at the receiver, and (b) it causes receiver\n'failure' events that may cause an early disconnect and always result\nin the entire ssync run being considered failed, and then none of the\nsuccessfully sent objects are reported as sync'd, so none get deleted\non the sender side.\n\nThis patch changes the receiver missing_check behavior to check if any\nconflicting fragment exists and if so indicate to the sender that it's\nfragment is not in sync but should not be sent. The sender does not\ninclude that object in the in_sync_objs that it returns to its caller.\n\nChange-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7\n""}, {'number': 2, 'created': '2015-05-12 15:43:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/9206dcdac12c9fb097663cb61d9cac5674456ef0', 'message': ""Don't ssync fragments that conflict with another fragment\n\nWhen using ssync to revert EC fragments, the ssync receiver will\nindicate that the reverted fragment is missing even if the receiver\nhas another fragment of same object at same time. When the ssync\nsender PUTs the fragment the receiver then rightly gets a 409 conflict\ndue to the other existing fragment.\n\nThis is undesirable because (a) it wastes bytes sent over network\nthat will not be written at the receiver, and (b) it causes receiver\n'failure' events that may cause an early disconnect and always result\nin the entire ssync run being considered failed, and then none of the\nsuccessfully sent objects are reported as sync'd, so none get deleted\non the sender side.\n\nThis patch changes the receiver missing_check behavior to check if any\nconflicting fragment exists and if so indicate to the sender that it's\nfragment is not in sync but should not be sent. The sender does not\ninclude that object in the in_sync_objs that it returns to its caller.\n\nChange-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7\n""}, {'number': 3, 'created': '2015-10-02 16:56:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/eb2ca27736269aaea107be815802231f55ef586f', 'message': ""Don't ssync fragments to handoffs if they conflict\n\nWhen using ssync to revert EC fragments to a\n*handoff* node, the ssync receiver will indicate\nthat the reverted fragment is missing even if the\nreceiver has another fragment of same object at\nsame time. When the ssync sender PUTs the fragment\nthe receiver then rightly gets a 409 conflict due\nto the other existing fragment.\n\n(Note: primary nodes behave differently and allow\nthe reverted fragment to be PUT alongside any\nexisting fragment).\n\nThis is undesirable because (a) it wastes bytes\nsent over network that will not be written at the\nreceiver, and (b) it causes receiver 'failure'\nevents that may cause an early disconnect and\nalways result in the entire ssync run being\nconsidered failed.  Consequently none of the\nsuccessfully sent objects are reported as sync'd,\nso none get deleted on the sender side.\n\nThis patch changes the receiver missing_check\nbehavior on handoff nodes to check if any\nconflicting fragment exists and if so indicate to\nthe sender that it's fragment is not in sync but\nshould not be sent. The sender does not include\nthat object in the in_sync_objs that it returns to\nits caller (i.e. the reconstructor).\n\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\n\nChange-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7\n""}, {'number': 4, 'created': '2015-10-05 17:53:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2976dd21f244b7838a24bc86cf7cdbc785178023', 'message': ""EC: Avoid conflicts when ssync'ing to handoffs\n\nWhen using ssync to revert EC fragments to a\n*handoff* node, the ssync receiver will indicate\nthat the reverted fragment is missing even if the\nreceiver has another fragment of same object at\nsame time. When the ssync sender PUTs the fragment\nthe receiver then rightly gets a 409 conflict due\nto the other existing fragment.\n\n(Note: ssync to a primary node behaves differently\nand allows the reverted fragment to be PUT alongside\nany existing fragments with different indexes).\n\nThis is undesirable because (a) it wastes bytes\nsent over network that will not be written at the\nreceiver, and (b) it causes receiver 'failure'\nevents that may cause an early disconnect and\nalways result in the entire ssync run being\nconsidered failed.  Consequently none of the\nsuccessfully sent objects are reported as sync'd,\nso none get deleted on the sender side.\n\nThis patch changes the receiver missing_check\nbehavior on handoff nodes to check if any\nconflicting fragment exists and if so indicate to\nthe sender that it's fragment is not in sync but\nshould not be sent. The sender does not include\nthat object in the in_sync_objs that it returns to\nits caller (i.e. the reconstructor).\n\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\n\nChange-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7\n""}, {'number': 5, 'created': '2015-10-08 17:36:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6c3c44f79d7ddec9a8dbe16ae5a8840d9e56adab', 'message': ""EC: Avoid conflicts when ssync'ing to handoffs\n\nWhen using ssync to revert EC fragments to a\n*handoff* node, the ssync receiver will indicate\nthat the reverted fragment is missing even if the\nreceiver has another fragment of same object at\nsame time. When the ssync sender PUTs the fragment\nthe receiver then rightly gets a 409 conflict due\nto the other existing fragment.\n\n(Note: ssync to a primary node behaves differently\nand allows the reverted fragment to be PUT alongside\nany existing fragments with different indexes).\n\nThis is undesirable because (a) it wastes bytes\nsent over network that will not be written at the\nreceiver, and (b) it causes receiver 'failure'\nevents that may cause an early disconnect and\nalways result in the entire ssync run being\nconsidered failed.  Consequently none of the\nsuccessfully sent objects are reported as sync'd,\nso none get deleted on the sender side.\n\nThis patch changes the receiver missing_check\nbehavior on handoff nodes to check if any\nconflicting fragment exists and if so indicate to\nthe sender that it's fragment is not in sync but\nshould not be sent. The sender does not include\nthat object in the in_sync_objs that it returns to\nits caller (i.e. the reconstructor).\n\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\n\nChange-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7\n""}, {'number': 6, 'created': '2015-10-13 12:59:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2f3715bd1206f0572a9eb0996abdceeb9136ba0f', 'message': ""EC: Avoid conflicts when ssync'ing fragments\n\nWhen using ssync to revert EC fragments to a\n*handoff* node, the ssync receiver will indicate\nthat the reverted fragment is missing even if the\nreceiver has another fragment of same object at\nsame time or newer time. When the ssync sender PUTs\nthe fragment the receiver then rightly gets a 409\nconflict due to the other existing fragment.\n\nSsync to a *primary* node behaves differently and\nallows the reverted fragment to be PUT alongside\nany existing fragments with different indexes at\nthe same time. However, the ssync receiver will\ncurrently indicate that the fragment to revert is\nmissing even if it finds another fragment of the\nsame object *at a newer time*. This will then\ncause a 409 conflict when the sender attempts to\nrevert its older fragment.\n\nThis is undesirable because (a) it wastes bytes\nsent over network that will not be written at the\nreceiver, and (b) it causes receiver 'failure'\nevents that may cause an early disconnect and\nalways result in the entire ssync run being\nconsidered failed.  Consequently none of the\nsuccessfully sent objects are reported as sync'd,\nso none get deleted on the sender side.\n\nThis patch changes the receiver missing_check\nbehavior on handoff nodes to check if any\nconflicting fragment exists and if so indicate to\nthe sender that it's fragment is not in sync but\nshould not be sent. The sender does not include\nthat object in the in_sync_objs that it returns to\nits caller (i.e. the reconstructor).\n\nThe patch also results in newer fragments on the\nreceiver always resulting in the sender not attempting\nto PUT data.\n\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\n\nChange-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7\n""}, {'number': 7, 'created': '2015-11-26 17:41:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a15464ab2df2d336b09176ad016ec1233bbb1c9d', 'message': ""EC: Avoid conflicts when ssync'ing fragments\n\nWhen using ssync to revert EC fragments to a\n*handoff* node, the ssync receiver will indicate\nthat the reverted fragment is missing even if the\nreceiver has another fragment of same object at\nsame time or newer time. When the ssync sender PUTs\nthe fragment the receiver then rightly gets a 409\nconflict due to the other existing fragment.\n\nSsync to a *primary* node behaves differently and\nallows the reverted fragment to be PUT alongside\nany existing fragments with different indexes at\nthe same time. However, the ssync receiver will\ncurrently indicate that the fragment to revert is\nmissing even if it finds another fragment of the\nsame object *at a newer time*. This will then\ncause a 409 conflict when the sender attempts to\nrevert its older fragment.\n\nThis is undesirable because (a) it wastes bytes\nsent over network that will not be written at the\nreceiver, and (b) it causes receiver 'failure'\nevents that may cause an early disconnect and\nalways result in the entire ssync run being\nconsidered failed.  Consequently none of the\nsuccessfully sent objects are reported as sync'd,\nso none get deleted on the sender side.\n\nThis patch changes the receiver missing_check\nbehavior on handoff nodes to check if any\nconflicting fragment exists and if so indicate to\nthe sender that it's fragment is not in sync but\nshould not be sent. The sender does not include\nthat object in the in_sync_objs that it returns to\nits caller (i.e. the reconstructor).\n\nThe patch also results in newer fragments on the\nreceiver always resulting in the sender not attempting\nto PUT data.\n\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\n\nChange-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7\n""}, {'number': 8, 'created': '2015-12-03 15:57:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/422c53a91a27a89e44e80f1691d7882fc6624985', 'message': ""EC: Avoid conflicts when ssync'ing fragments\n\nWhen using ssync to revert EC fragments to a\n*handoff* node, the ssync receiver will indicate\nthat the reverted fragment is missing even if the\nreceiver has another fragment of same object at\nsame time or newer time. When the ssync sender PUTs\nthe fragment the receiver then rightly gets a 409\nconflict due to the other existing fragment.\n\nSsync to a *primary* node behaves differently and\nallows the reverted fragment to be PUT alongside\nany existing fragments with different indexes at\nthe same time. However, the ssync receiver will\ncurrently indicate that the fragment to revert is\nmissing even if it finds another fragment of the\nsame object *at a newer time*. This will then\ncause a 409 conflict when the sender attempts to\nrevert its older fragment.\n\nThis is undesirable because (a) it wastes bytes\nsent over network that will not be written at the\nreceiver, and (b) it causes receiver 'failure'\nevents that may cause an early disconnect and\nalways result in the entire ssync run being\nconsidered failed.  Consequently none of the\nsuccessfully sent objects are reported as sync'd,\nso none get deleted on the sender side.\n\nThis patch changes the receiver missing_check\nbehavior on handoff nodes to check if any\nconflicting fragment exists and if so indicate to\nthe sender that it's fragment is not in sync but\nshould not be sent. The sender does not include\nthat object in the in_sync_objs that it returns to\nits caller (i.e. the reconstructor).\n\nThe patch also results in newer fragments on the\nreceiver always resulting in the sender not attempting\nto PUT data.\n\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\n\nChange-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7\n""}, {'number': 9, 'created': '2015-12-21 17:14:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2fe9dc8fc32fe1aabb8365fc19c9004d72153c7f', 'message': ""EC: Avoid conflicts when ssync'ing fragments\n\nWhen using ssync to revert EC fragments to a\n*handoff* node, the ssync receiver will indicate\nthat the reverted fragment is missing even if the\nreceiver has another fragment of same object at\nsame time or newer time. When the ssync sender PUTs\nthe fragment the receiver then rightly gets a 409\nconflict due to the other existing fragment.\n\nSsync to a *primary* node behaves differently and\nallows the reverted fragment to be PUT alongside\nany existing fragments with different indexes at\nthe same time. However, the ssync receiver will\ncurrently indicate that the fragment to revert is\nmissing even if it finds another fragment of the\nsame object *at a newer time*. This will then\ncause a 409 conflict when the sender attempts to\nrevert its older fragment.\n\nThis is undesirable because (a) it wastes bytes\nsent over network that will not be written at the\nreceiver, and (b) it causes receiver 'failure'\nevents that may cause an early disconnect and\nalways result in the entire ssync run being\nconsidered failed.  Consequently none of the\nsuccessfully sent objects are reported as sync'd,\nso none get deleted on the sender side.\n\nThis patch changes the receiver missing_check\nbehavior on handoff nodes to check if any\nconflicting fragment exists and if so indicate to\nthe sender that it's fragment is not in sync but\nshould not be sent. The sender does not include\nthat object in the in_sync_objs that it returns to\nits caller (i.e. the reconstructor).\n\nThe patch also results in newer fragments on the\nreceiver always resulting in the sender not attempting\nto PUT data.\n\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\n\nChange-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7\n""}, {'number': 10, 'created': '2016-01-19 12:35:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/13b144b8ad019f62a5ec1a15c023c21da5383201', 'message': ""EC: Avoid conflicts when ssync'ing fragments\n\nWhen using ssync to revert EC fragments to a\n*handoff* node, the ssync receiver will indicate\nthat the reverted fragment is missing even if the\nreceiver has another fragment of same object at\nsame time or newer time. When the ssync sender PUTs\nthe fragment the receiver then rightly gets a 409\nconflict due to the other existing fragment.\n\nSsync to a *primary* node behaves differently and\nallows the reverted fragment to be PUT alongside\nany existing fragments with different indexes at\nthe same time. However, the ssync receiver will\ncurrently indicate that the fragment to revert is\nmissing even if it finds another fragment of the\nsame object *at a newer time*. This will then\ncause a 409 conflict when the sender attempts to\nrevert its older fragment.\n\nThis is undesirable because (a) it wastes bytes\nsent over network that will not be written at the\nreceiver, and (b) it causes receiver 'failure'\nevents that may cause an early disconnect and\nalways result in the entire ssync run being\nconsidered failed.  Consequently none of the\nsuccessfully sent objects are reported as sync'd,\nso none get deleted on the sender side.\n\nThis patch changes the receiver missing_check\nbehavior on handoff nodes to check if any\nconflicting fragment exists and if so indicate to\nthe sender that it's fragment is not in sync but\nshould not be sent. The sender does not include\nthat object in the in_sync_objs that it returns to\nits caller (i.e. the reconstructor).\n\nThe patch also results in newer fragments on the\nreceiver always resulting in the sender not attempting\nto PUT data.\n\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\n\nChange-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7\n""}, {'number': 11, 'created': '2016-03-04 15:58:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/54328d499b72117977ce7c6e240acf293bbc4069', 'message': ""EC: Avoid conflicts when ssync'ing fragments\n\nWhen using ssync to revert EC fragments to a\n*handoff* node, the ssync receiver will indicate\nthat the reverted fragment is missing even if the\nreceiver has another fragment of same object at\nsame time or newer time. When the ssync sender PUTs\nthe fragment the receiver then rightly gets a 409\nconflict due to the other existing fragment.\n\nSsync to a *primary* node behaves differently and\nallows the reverted fragment to be PUT alongside\nany existing fragments with different indexes at\nthe same time. However, the ssync receiver will\ncurrently indicate that the fragment to revert is\nmissing even if it finds another fragment of the\nsame object *at a newer time*. This will then\ncause a 409 conflict when the sender attempts to\nrevert its older fragment.\n\nThis is undesirable because (a) it wastes bytes\nsent over network that will not be written at the\nreceiver, and (b) it causes receiver 'failure'\nevents that may cause an early disconnect and\nalways result in the entire ssync run being\nconsidered failed.  Consequently none of the\nsuccessfully sent objects are reported as sync'd,\nso none get deleted on the sender side.\n\nThis patch changes the receiver missing_check\nbehavior on handoff nodes to check if any\nconflicting fragment exists and if so indicate to\nthe sender that it's fragment is not in sync but\nshould not be sent. The sender does not include\nthat object in the in_sync_objs that it returns to\nits caller (i.e. the reconstructor).\n\nThe patch also results in newer fragments on the\nreceiver always resulting in the sender not attempting\nto PUT data.\n\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\n\nChange-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7\n""}, {'number': 12, 'created': '2016-03-22 15:55:24.000000000', 'files': ['test/probe/common.py', 'test/probe/test_reconstructor_revert.py', 'swift/obj/ssync_sender.py', 'test/unit/obj/test_ssync.py', 'swift/obj/ssync_receiver.py', 'test/unit/obj/test_ssync_receiver.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/05ddd6f0f8581d7176f33dfeb0f0553ce2832467', 'message': ""EC: Avoid conflicts when ssync'ing fragments\n\nWhen using ssync to revert EC fragments to a\n*handoff* node, the ssync receiver will indicate\nthat the reverted fragment is missing even if the\nreceiver has another fragment of same object at\nsame time or newer time. When the ssync sender PUTs\nthe fragment the receiver then rightly gets a 409\nconflict due to the other existing fragment.\n\nSsync to a *primary* node behaves differently and\nallows the reverted fragment to be PUT alongside\nany existing fragments with different indexes at\nthe same time. However, the ssync receiver will\ncurrently indicate that the fragment to revert is\nmissing even if it finds another fragment of the\nsame object *at a newer time*. This will then\ncause a 409 conflict when the sender attempts to\nrevert its older fragment.\n\nThis is undesirable because (a) it wastes bytes\nsent over network that will not be written at the\nreceiver, and (b) it causes receiver 'failure'\nevents that may cause an early disconnect and\nalways result in the entire ssync run being\nconsidered failed.  Consequently none of the\nsuccessfully sent objects are reported as sync'd,\nso none get deleted on the sender side.\n\nThis patch changes the receiver missing_check\nbehavior on handoff nodes to check if any\nconflicting fragment exists and if so indicate to\nthe sender that it's fragment is not in sync but\nshould not be sent. The sender does not include\nthat object in the in_sync_objs that it returns to\nits caller (i.e. the reconstructor).\n\nThe patch also results in newer fragments on the\nreceiver always resulting in the sender not attempting\nto PUT data.\n\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\n\nChange-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7\n""}]",4,181407,05ddd6f0f8581d7176f33dfeb0f0553ce2832467,56,4,12,7847,,,0,"EC: Avoid conflicts when ssync'ing fragments

When using ssync to revert EC fragments to a
*handoff* node, the ssync receiver will indicate
that the reverted fragment is missing even if the
receiver has another fragment of same object at
same time or newer time. When the ssync sender PUTs
the fragment the receiver then rightly gets a 409
conflict due to the other existing fragment.

Ssync to a *primary* node behaves differently and
allows the reverted fragment to be PUT alongside
any existing fragments with different indexes at
the same time. However, the ssync receiver will
currently indicate that the fragment to revert is
missing even if it finds another fragment of the
same object *at a newer time*. This will then
cause a 409 conflict when the sender attempts to
revert its older fragment.

This is undesirable because (a) it wastes bytes
sent over network that will not be written at the
receiver, and (b) it causes receiver 'failure'
events that may cause an early disconnect and
always result in the entire ssync run being
considered failed.  Consequently none of the
successfully sent objects are reported as sync'd,
so none get deleted on the sender side.

This patch changes the receiver missing_check
behavior on handoff nodes to check if any
conflicting fragment exists and if so indicate to
the sender that it's fragment is not in sync but
should not be sent. The sender does not include
that object in the in_sync_objs that it returns to
its caller (i.e. the reconstructor).

The patch also results in newer fragments on the
receiver always resulting in the sender not attempting
to PUT data.

Co-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>

Change-Id: I4115482a3d5a0c8c21123ec19e0ab693fc2ef5f7
",git fetch https://review.opendev.org/openstack/swift refs/changes/07/181407/6 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/obj/test_ssync_sender.py', 'swift/obj/ssync_sender.py', 'test/unit/obj/test_diskfile.py', 'swift/obj/diskfile.py', 'swift/obj/ssync_receiver.py']",5,09790a7d4572ded72f20a2c2a8041f6529170aae,p-ssync-avoid-conflicts," self.device, self.partition, object_hash, self.policy) if (df.timestamp == t_remote_data and hasattr(df, 'fragment_index') and df.fragment_index != self.frag_index): # fragment not in sync but would conflict want = ['later']"," self.device, self.partition, object_hash, self.policy, frag_index=self.frag_index)",83,19
openstack%2Fwatcher~master~I1c7067e2ffcf48a5556d6648eccb2abc2c199756,openstack/watcher,master,I1c7067e2ffcf48a5556d6648eccb2abc2c199756,Adding tempest test that executes outlet_temp_control strategy.,NEW,2017-02-28 22:04:19.000000000,2017-12-18 02:26:51.000000000,,"[{'_account_id': 18971}, {'_account_id': 19055}, {'_account_id': 21361}, {'_account_id': 24872}, {'_account_id': 25898}]","[{'number': 1, 'created': '2017-02-28 22:04:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/watcher/commit/09d794b63718c41891a1573531eb26d8f390b08d', 'message': 'Adding tempest test that executes outlet_temp_control strategy.\n\nChange-Id: I1c7067e2ffcf48a5556d6648eccb2abc2c199756\n'}, {'number': 2, 'created': '2017-03-20 18:28:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/watcher/commit/3c5f6da292c77f9c76742504cd67571903ce8086', 'message': 'Adding tempest test that executes outlet_temp_control strategy.\n\nChange-Id: I1c7067e2ffcf48a5556d6648eccb2abc2c199756\n'}, {'number': 3, 'created': '2017-03-20 20:29:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/watcher/commit/ff37821e8764477d13bc32a97056fd480387282a', 'message': 'Adding tempest test that executes outlet_temp_control strategy.\n\nChange-Id: I1c7067e2ffcf48a5556d6648eccb2abc2c199756\n'}, {'number': 4, 'created': '2017-03-20 22:14:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/watcher/commit/a6dd3515e6cc665595f411bc0b7ca7aae59a3353', 'message': 'Adding tempest test that executes outlet_temp_control strategy.\n\nChange-Id: I1c7067e2ffcf48a5556d6648eccb2abc2c199756\n'}, {'number': 5, 'created': '2017-04-07 14:01:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/watcher/commit/9c57017bd4bd1ea18ee6ba70cb01a3f21fb7b825', 'message': 'Adding tempest test that executes outlet_temp_control strategy.\n\nChange-Id: I1c7067e2ffcf48a5556d6648eccb2abc2c199756\n'}, {'number': 6, 'created': '2017-04-07 15:50:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/watcher/commit/b7cd5e48130db960d5bb5266a37a26d4730642af', 'message': 'Adding tempest test that executes outlet_temp_control strategy.\n\nChange-Id: I1c7067e2ffcf48a5556d6648eccb2abc2c199756\n'}, {'number': 7, 'created': '2017-08-08 18:41:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/watcher/commit/54c9aa73e869b144dc477f8203ce5d25f0bd0ea2', 'message': 'Adding tempest test that executes outlet_temp_control strategy.\n\nChange-Id: I1c7067e2ffcf48a5556d6648eccb2abc2c199756\n'}, {'number': 8, 'created': '2017-08-09 07:49:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/watcher/commit/3f9b069dbb07778199a307936166565be96e829e', 'message': 'Adding tempest test that executes outlet_temp_control strategy.\n\nChange-Id: I1c7067e2ffcf48a5556d6648eccb2abc2c199756\n'}, {'number': 9, 'created': '2017-08-09 08:55:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/watcher/commit/73f7927e3a50a6b1d5ee295da730ce9b0b4cc6bf', 'message': 'Adding tempest test that executes outlet_temp_control strategy.\n\nChange-Id: I1c7067e2ffcf48a5556d6648eccb2abc2c199756\n'}, {'number': 10, 'created': '2017-08-10 11:37:47.000000000', 'files': ['watcher_tempest_plugin/tests/scenario/base.py', 'watcher_tempest_plugin/tests/scenario/test_execute_outlet_temp_control.py', 'watcher_tempest_plugin/gnocchi_client.py', 'watcher_tempest_plugin/infra_optim_clients.py'], 'web_link': 'https://opendev.org/openstack/watcher/commit/e170ffae7e6e1e73c617ee4d255ea32bc8810560', 'message': 'Adding tempest test that executes outlet_temp_control strategy.\n\nChange-Id: I1c7067e2ffcf48a5556d6648eccb2abc2c199756\n'}]",6,439184,e170ffae7e6e1e73c617ee4d255ea32bc8810560,32,5,10,21361,,,0,"Adding tempest test that executes outlet_temp_control strategy.

Change-Id: I1c7067e2ffcf48a5556d6648eccb2abc2c199756
",git fetch https://review.opendev.org/openstack/watcher refs/changes/84/439184/8 && git format-patch -1 --stdout FETCH_HEAD,['watcher_tempest_plugin/tests/scenario/test_execute_outlet_temp_control.py'],1,09d794b63718c41891a1573531eb26d8f390b08d,detached/temp_control_tempest,"# -*- encoding: utf-8 -*- # Copyright (c) Intel Corporation 2017 # # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. from __future__ import unicode_literals import functools from oslo_log import log from tempest import config from tempest import test from watcher_tempest_plugin.tests.scenario import test_execute_basic_optim as b CONF = config.CONF LOG = log.getLogger(__name__) class TestExecOutletTempControl(b.TestExecuteBasicStrategy): """"""Tests for action plans"""""" METER_NAME = ""hardware.ipmi.node.outlet_temperature"" GOAL = ""thermal_optimization"" @classmethod def skip_checks(cls): super(TestExecOutletTempControl, cls).skip_checks() @classmethod def resource_setup(cls): super(TestExecOutletTempControl, cls).resource_setup() @classmethod def get_compute_nodes_setup(cls): super(TestExecOutletTempControl, cls).get_compute_nodes_setup() @classmethod def wait_for_compute_nodes_setup(cls): super(TestExecOutletTempControl, cls).wait_for_compute_nodes_setup() @classmethod def rollback_compute_nodes_status(cls): super(TestExecOutletTempControl, cls).rollback_compute_nodes_status() def _pack_all_created_instances_on_one_host(self, instances): hypervisors = [ hyp['hypervisor_hostname'] for hyp in self.get_hypervisors_setup() if hyp['state'] == 'up'] node = hypervisors[0] for instance in instances: if instance.get('OS-EXT-SRV-ATTR:hypervisor_hostname') != node: self._migrate_server_to(instance['id'], node) def test_execute_outlet_temp_control(self): """"""Execute an action plan using the outlet temp control strategy"""""" self.addCleanup(self.rollback_compute_nodes_status) instances = self._create_one_instance_per_host() self._pack_all_created_instances_on_one_host(instances) audit_parameters = {""threshold"": 35.0} _, goal = self.client.show_goal(self.GOAL) _, strategy = self.client.show_strategy(""outlet_temperature"") _, audit_template = self.create_audit_template( goal['uuid'], strategy=strategy['uuid'] ) _, audit = self.create_audit( audit_template['uuid'], parameters=audit_parameters) try: self.assertTrue(test.call_until_true( func=functools.partial(self.has_audit_finished, audit['uuid']), duration=600, sleep_for=2 )) except ValueError: self.fail(""The audit has failed!"") _, finished_audit = self.client.show_audit(audit['uuid']) if finished_audit.get('state') in ('FAILED', 'CANCELLED'): self.fail(""The audit ended in unexpected state: %s!"", finished_audit.get('state')) _, action_plans = self.client.list_action_plans( audit_uuid=audit['uuid']) action_plan = action_plans['action_plans'][0] _, action_plan = self.client.show_action_plan(action_plan['uuid']) _, action_list = self.client.list_actions( action_plan_uuid=action_plan['uuid']) ",,102,0
openstack%2Fswift~master~I466d3739f632db85f05c303c3d6e496967634955,openstack/swift,master,I466d3739f632db85f05c303c3d6e496967634955,Small updates to saio doc,NEW,2017-07-14 18:57:37.000000000,2017-12-18 02:26:23.000000000,,"[{'_account_id': 597}, {'_account_id': 7233}, {'_account_id': 13052}, {'_account_id': 15343}]","[{'number': 1, 'created': '2017-07-14 18:57:37.000000000', 'files': ['doc/source/development_saio.rst'], 'web_link': 'https://opendev.org/openstack/swift/commit/b7e3dee9dd61ed4bc361bc875a160855a27d4c6f', 'message': 'Small updates to saio doc\n\nUpdating some of the instructions for setting up\nsaio on Fedora\n\nRelated-Bug #1539369\n\nChange-Id: I466d3739f632db85f05c303c3d6e496967634955\n'}]",0,484016,b7e3dee9dd61ed4bc361bc875a160855a27d4c6f,8,4,1,9625,,,0,"Small updates to saio doc

Updating some of the instructions for setting up
saio on Fedora

Related-Bug #1539369

Change-Id: I466d3739f632db85f05c303c3d6e496967634955
",git fetch https://review.opendev.org/openstack/swift refs/changes/16/484016/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/development_saio.rst'],1,b7e3dee9dd61ed4bc361bc875a160855a27d4c6f,bug/1539369,"* Get an Ubuntu 14.04 LTS server image or try the latest version of python-mock redhat-rpm-config rsyslog * On Fedora you might need to install package ``rsync-daemon``, then run::","* Get an Ubuntu 14.04 LTS server image or try something python-mock * On Fedora, run::",3,3
openstack%2Frally~master~I35457d5923a3e1bc57a75ca968630be90cf1e2b4,openstack/rally,master,I35457d5923a3e1bc57a75ca968630be90cf1e2b4,add DictWithDeprecation class,NEW,2015-11-13 11:21:01.000000000,2017-12-18 02:26:12.000000000,,"[{'_account_id': 4428}, {'_account_id': 6172}, {'_account_id': 6835}, {'_account_id': 7369}, {'_account_id': 7428}, {'_account_id': 9545}, {'_account_id': 10475}, {'_account_id': 13609}, {'_account_id': 13919}, {'_account_id': 14817}, {'_account_id': 18785}]","[{'number': 1, 'created': '2015-11-13 11:21:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a31f44094534173d7716240d156a28ed0384788b', 'message': 'WIP: add DictWithDeprecation class\n\nChange-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4\n'}, {'number': 2, 'created': '2015-11-13 11:28:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/492d17270d84d26f8e383a908af306c2c43e1fd8', 'message': 'WIP: add DictWithDeprecation class\n\nThis class allows to deprecate keys in dict.\n\nChange-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4\n'}, {'number': 3, 'created': '2015-11-13 13:55:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/af55257d0d077deafd382eba03306cd901410557', 'message': 'add DictWithDeprecation class\n\nThis class allows to deprecate keys in dict.\n\nChange-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4\n'}, {'number': 4, 'created': '2015-11-13 14:01:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5fd7b81d525a53a856b968b265cced242c121a0e', 'message': 'add DictWithDeprecation class\n\nThis class allows to deprecate keys in dict.\n\nChange-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4\n'}, {'number': 5, 'created': '2015-11-13 15:12:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1ee8d1ffb490dda5a38fea7b4a59032fb45d771b', 'message': 'add DictWithDeprecation class\n\nThis class allows to deprecate keys in dict.\n\nChange-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4\n'}, {'number': 6, 'created': '2015-11-13 15:34:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e73e699913fef93c5efb10b768324a3a0587b93c', 'message': 'add DictWithDeprecation class\n\nThis class allows to deprecate keys in dict.\n\nChange-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4\n'}, {'number': 7, 'created': '2015-11-13 16:02:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c51f122b46f5bd7c5a8e8dee1faafb1642cb7524', 'message': 'add DictWithDeprecation class\n\nThis class allows to deprecate keys in dict.\n\nChange-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4\n'}, {'number': 8, 'created': '2015-11-20 10:20:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/9c1f1d24216e86ef6676cc9c6e9a65f70167fd87', 'message': 'add DictWithDeprecation class\n\nThis class allows to deprecate keys in dict.\n\nChange-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4\n'}, {'number': 9, 'created': '2015-11-20 14:21:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6f5db165fc6770b108ab75f0e969ce1b56bfe69c', 'message': 'add DictWithDeprecation class\n\nThis class allows to deprecate keys in dict.\n\nChange-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4\n'}, {'number': 10, 'created': '2015-11-24 14:24:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/82d8867730b0c8d807abf72a4cfb95851b2caa07', 'message': 'add DictWithDeprecation class\n\nThis class allows to deprecate keys in dict.\n\nChange-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4\n'}, {'number': 11, 'created': '2015-11-25 10:32:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b4c34db1262298ff0dd794bb6fa1c2869b0a99b3', 'message': 'add DictWithDeprecation class\n\nThis class allows to deprecate keys in dict.\n\nChange-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4\n'}, {'number': 12, 'created': '2015-11-25 11:39:25.000000000', 'files': ['tests/unit/common/test_utils.py', 'rally/common/utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/45a423b084b41b0f728728cbfb57335df131b8f1', 'message': 'add DictWithDeprecation class\n\nThis class allows to deprecate keys in dict.\n\nChange-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4\n'}]",37,245094,45a423b084b41b0f728728cbfb57335df131b8f1,62,11,12,9545,,,0,"add DictWithDeprecation class

This class allows to deprecate keys in dict.

Change-Id: I35457d5923a3e1bc57a75ca968630be90cf1e2b4
",git fetch https://review.opendev.org/openstack/rally refs/changes/94/245094/10 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/common/test_utils.py', 'rally/common/utils.py']",2,a31f44094534173d7716240d156a28ed0384788b,deprecated,"import warnings import jsonschemafrom rally import consts class DictWithDeprecation(dict): """"""Dict object with helper for deprecated keys Usage example: # deprecate key without alternative some_dict = DictWithDeprecated In [1]: some_dict = DictWithDeprecation( ...: {""k1"": ""v1"", ""k2"": ""v2""}, ...: deprecated_keys={""k2"": {""release"": ""0.1.2""}}) In [2]: some_dict[""k2""] UserWarning: Key 'k2' is deprecated since Rally 0.1.2. warnings.warn(msg) Out[2]: 'v2' In [3]: another_dict = DictWithDeprecation( ...: {""k1"": ""v1""}, ...: deprecated_keys={""k2"": {""release"": ""0.1.2"", ""alternative"": ""k1""}}) In [4]: another_dict[""k2""] UserWarning: Key 'k2' is deprecated since Rally 0.1.2. Use k1 instead. ""alternative"": self.__deprecate_keys[key][""alternative""]}) Out[4]: 'v1' """""" __DEPRECATED_KEYS_SCHEME = { ""type"": ""object"", ""$schema"": consts.JSON_SCHEMA, ""patternProperties"": { ""^.+$"": { ""type"": ""object"", ""properties"": { ""release"": { ""type"": ""string"" }, ""alternative"": { ""type"": ""string"" } }, ""additionalProperties"": False, ""required"": [""release""] } }, ""additionalProperties"": False } def __init__(self, iterable, deprecated_keys=None): """"""Init dict :param iterable: Iterable object :param deprecated_keys: Dictionary with information for deprecated keys. Supported format: {""deprecated_key_name"": { ""release"": ""Rally release from which key was deprecated"", ""alternative"": ""optional key_name, which will be used as "" ""alternative""}} """""" super(DictWithDeprecation, self).__init__(iterable) jsonschema.validate(deprecated_keys, self.__DEPRECATED_KEYS_SCHEME) self.__deprecate_keys = deprecated_keys def __check_deprecated(self, key): if key in self.__deprecate_keys: msg = ""Key '%(key)s' is deprecated since Rally %(release)s."" % { ""key"": key, ""release"": self.__deprecate_keys[key][""release""]} if self.__deprecate_keys[key].get(""alternative""): warnings.warn(""%(msg)s Use %(alternative)s instead."" % { ""msg"": msg, ""alternative"": self.__deprecate_keys[key][""alternative""]}) key = self.__deprecate_keys[key][""alternative""] else: warnings.warn(msg) return key def __getitem__(self, key): return super(DictWithDeprecation, self).__getitem__( self.__check_deprecated(key)) def get(self, key, default=None): return super(DictWithDeprecation, self).get( self.__check_deprecated(key), default) def has_key(self, key): return super(DictWithDeprecation, self).__contains__( self.__check_deprecated(key)) def __contains__(self, key): return self.has_key(key)",,101,0
openstack%2Fironic~master~Ib3160abb2296d30c707704f1852746dd9205fc37,openstack/ironic,master,Ib3160abb2296d30c707704f1852746dd9205fc37,Add tests for the node update API,NEW,2016-11-22 14:26:21.000000000,2017-12-18 02:25:27.000000000,,"[{'_account_id': 14525}, {'_account_id': 19339}]","[{'number': 1, 'created': '2016-11-22 14:26:21.000000000', 'files': ['ironic/tests/unit/api/v1/test_nodes.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/9b129f389b2254a04608b2d321ed9fbcd1c5a830', 'message': 'Add tests for the node update API\n\nAdd tests for the _update_changed_fields() method, which is used when\nadding, updating, and removing a node’s fields.\nThis is a follow-up for patch https://review.openstack.org/#/c/390558/\n\nChange-Id: Ib3160abb2296d30c707704f1852746dd9205fc37\nRelated-Bug: #1563263\n'}]",0,400792,9b129f389b2254a04608b2d321ed9fbcd1c5a830,6,2,1,20522,,,0,"Add tests for the node update API

Add tests for the _update_changed_fields() method, which is used when
adding, updating, and removing a node’s fields.
This is a follow-up for patch https://review.openstack.org/#/c/390558/

Change-Id: Ib3160abb2296d30c707704f1852746dd9205fc37
Related-Bug: #1563263
",git fetch https://review.opendev.org/openstack/ironic refs/changes/92/400792/1 && git format-patch -1 --stdout FETCH_HEAD,['ironic/tests/unit/api/v1/test_nodes.py'],1,9b129f389b2254a04608b2d321ed9fbcd1c5a830,bug/1563263," def test__update_changed_fields_add_name(self, mock_pecan_req): mock_pecan_req.version.minor = versions.MINOR_MAX_VERSION controller = api_node.NodesController() node_dict = self.node_no_name.as_dict() node_dict['name'] = 'new_name' updated_node = api_node.Node(**node_dict) controller._update_changed_fields(updated_node, self.node_no_name) self.assertEqual('new_name', self.node_no_name.name) @mock.patch(""pecan.request"") def test__update_changed_fields_update_name(self, mock_pecan_req): mock_pecan_req.version.minor = versions.MINOR_MAX_VERSION controller = api_node.NodesController() node_dict = self.node.as_dict() node_dict['name'] = 'new_name' updated_node = api_node.Node(**node_dict) controller._update_changed_fields(updated_node, self.node) self.assertEqual('new_name', self.node.name) @mock.patch(""pecan.request"") def test__update_changed_fields_remove_name(self, mock_pecan_req): mock_pecan_req.version.minor = versions.MINOR_MAX_VERSION controller = api_node.NodesController() node_dict = self.node.as_dict() del node_dict['name'] updated_node = api_node.Node(**node_dict) controller._update_changed_fields(updated_node, self.node) self.assertIsNone(self.node.name) @mock.patch(""pecan.request"") def test__update_changed_fields_add_chassis_uuid(self, mock_pecan_req): mock_pecan_req.version.minor = versions.MINOR_MAX_VERSION controller = api_node.NodesController() node_no_chassis = obj_utils.create_test_node( self.context, uuid=uuidutils.generate_uuid(), chassis_id=None) node_dict = node_no_chassis.as_dict() node_dict['chassis_id'] = self.chassis.id updated_node = api_node.Node(**node_dict) controller._update_changed_fields(updated_node, node_no_chassis) self.assertEqual(self.chassis.id, node_no_chassis.chassis_id) @mock.patch(""pecan.request"") def test__update_changed_fields_update_chassis_uuid(self, mock_pecan_req): mock_pecan_req.version.minor = versions.MINOR_MAX_VERSION controller = api_node.NodesController() new_chassis = obj_utils.create_test_chassis( self.context, uuid=uuidutils.generate_uuid()) node_dict = self.node.as_dict() node_dict['chassis_id'] = new_chassis.id updated_node = api_node.Node(**node_dict) controller._update_changed_fields(updated_node, self.node) self.assertEqual(new_chassis.id, self.node.chassis_id) @mock.patch(""pecan.request"") @mock.patch(""pecan.request"") def test__update_changed_fields_remove_chassis_uuid_invalid_api( self, mock_pecan_req): mock_pecan_req.version.minor = 24 controller = api_node.NodesController() node_dict = self.node.as_dict() del node_dict['chassis_id'] node_no_chassis = api_node.Node(**node_dict) self.assertRaises(exception.NotAcceptable, controller._update_changed_fields, node_no_chassis, self.node) ",,83,0
openstack%2Fdevstack-gate~master~I110fb5f151683fcc393720face29b2284997eeea,openstack/devstack-gate,master,I110fb5f151683fcc393720face29b2284997eeea,Update README and move ~/workspace-cache after project setup,NEW,2015-12-15 21:05:20.000000000,2017-12-18 02:25:22.000000000,,"[{'_account_id': 12400}, {'_account_id': 15942}]","[{'number': 1, 'created': '2015-12-15 21:05:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/b45c7297437b9092f5055fb168f80f6c1242b213', 'message': 'Update README and move ~/workspace-cache after project setup\n\nUpdated README to the more recent devstack-gate changes:\n\n  - updated copy from id_rsa.pub to authorized_keys\n  - updated to point to <project>.yaml and not just devstack-gate\n  - updated with a concrete example\n\nMoved ~/workspace-cache later in workspace_setup function.  It\ncurrently is what ZUUL_URL points to workspace-cache.  We clone\nour project to that directory and then promptly delete it.  Then,\nwhen we try to fetch in function.sh in git_fetch_at_ref function,\nit fails with fatal error.\n\nChange-Id: I110fb5f151683fcc393720face29b2284997eeea\n'}, {'number': 2, 'created': '2015-12-21 15:30:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/f35c8bb923b05a97ffeb3781772c38e182f4622e', 'message': 'Update README and move ~/workspace-cache after project setup\n\nUpdated README to the more recent devstack-gate changes:\n\n  - updated copy from id_rsa.pub to authorized_keys\n  - updated to point to <project>.yaml and not just devstack-gate\n  - updated with a concrete example\n\nMoved ~/workspace-cache later in workspace_setup function.  It\ncurrently is what ZUUL_URL points to workspace-cache.  We clone\nour project to that directory and then promptly delete it.  Then,\nwhen we try to fetch in function.sh in git_fetch_at_ref function,\nit fails with fatal error.\n\nChange-Id: I110fb5f151683fcc393720face29b2284997eeea\n'}, {'number': 3, 'created': '2016-01-15 22:27:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/735aebae31a6d4e6db2c3c23a46920158f65ca85', 'message': 'Update README and move ~/workspace-cache after project setup\n\nUpdated README to the more recent devstack-gate changes:\n\n  - updated copy from id_rsa.pub to authorized_keys\n  - updated to point to <project>.yaml and not just devstack-gate\n  - updated with a concrete example\n\nMoved ~/workspace-cache later in workspace_setup function.  It\ncurrently is what ZUUL_URL points to workspace-cache.  We clone\nour project to that directory and then promptly delete it.  Then,\nwhen we try to fetch in function.sh in git_fetch_at_ref function,\nit fails with fatal error.\n\nChange-Id: I110fb5f151683fcc393720face29b2284997eeea\n'}, {'number': 4, 'created': '2016-01-27 19:15:08.000000000', 'files': ['README.rst', 'functions.sh'], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/a02a1c641e10f67b241de0671ce92d2ba88dc2fb', 'message': 'Update README and move ~/workspace-cache after project setup\n\nUpdated README to the more recent devstack-gate changes:\n\n  - updated copy from id_rsa.pub to authorized_keys\n  - updated to point to <project>.yaml and not just devstack-gate\n  - updated with a concrete example\n\nMoved ~/workspace-cache later in workspace_setup function.  It\ncurrently is what ZUUL_URL points to workspace-cache.  We clone\nour project to that directory and then promptly delete it.  Then,\nwhen we try to fetch in function.sh in git_fetch_at_ref function,\nit fails with fatal error.\n\nChange-Id: I110fb5f151683fcc393720face29b2284997eeea\n'}]",2,258148,a02a1c641e10f67b241de0671ce92d2ba88dc2fb,18,2,4,12400,,,0,"Update README and move ~/workspace-cache after project setup

Updated README to the more recent devstack-gate changes:

  - updated copy from id_rsa.pub to authorized_keys
  - updated to point to <project>.yaml and not just devstack-gate
  - updated with a concrete example

Moved ~/workspace-cache later in workspace_setup function.  It
currently is what ZUUL_URL points to workspace-cache.  We clone
our project to that directory and then promptly delete it.  Then,
when we try to fetch in function.sh in git_fetch_at_ref function,
it fails with fatal error.

Change-Id: I110fb5f151683fcc393720face29b2284997eeea
",git fetch https://review.opendev.org/openstack/devstack-gate refs/changes/48/258148/1 && git format-patch -1 --stdout FETCH_HEAD,"['README.rst', 'functions.sh']",2,b45c7297437b9092f5055fb168f80f6c1242b213,258148, # (dimtruck): this is going to remove workspace-cache after repo is # cloned into it but before it's copied to $BASE/new/<project> # moving this later on # rm -fr ~/workspace-cache/ # (dimtruck) remove workspace-cache here since we're done copying rm -fr ~/workspace-cache/ , rm -fr ~/workspace-cache/,113,4
openstack%2Fironic~master~I0981892f478a494abccc0d55fedb88f265777dde,openstack/ironic,master,I0981892f478a494abccc0d55fedb88f265777dde,Add RPC API to get the boot config,NEW,2016-11-07 12:16:22.000000000,2017-12-18 02:25:12.000000000,,"[{'_account_id': 8871}, {'_account_id': 9542}, {'_account_id': 10118}, {'_account_id': 10206}, {'_account_id': 11076}, {'_account_id': 11878}, {'_account_id': 13719}, {'_account_id': 14250}, {'_account_id': 14525}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 23883}, {'_account_id': 24597}]","[{'number': 1, 'created': '2016-11-07 12:16:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a670fcc50fc336c740877b6745b06a617d5a5828', 'message': ""Add RPC API to get iPXE template and options\n\nThis patch adds a new `get_ipxe_template` RPC method.\n\nDepending on node state, it collects iPXE boot config template name and\noptions to render it with from boot (and deploy) node's driver\ninterfaces and returns them in the form of a dictionary.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n""}, {'number': 2, 'created': '2016-11-07 13:56:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e420b56e7966dc23fc53a197d961b96ea3d4c2b8', 'message': ""Add RPC API to get iPXE template and options\n\nThis patch adds a new `get_ipxe_template` RPC method.\n\nDepending on node state, it collects iPXE boot config template name and\noptions to render it with from boot (and deploy) node's driver\ninterfaces and returns them in the form of a dictionary.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n""}, {'number': 3, 'created': '2016-11-08 14:35:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/32eb2f07562ace9567401b3373ee81fb3289bb38', 'message': ""Add RPC API to get iPXE template and options\n\nThis patch adds a new `get_ipxe_template` RPC method.\n\nDepending on node state, it collects iPXE boot config template name and\noptions to render it with from boot (and deploy) node's driver\ninterfaces and returns them in the form of a dictionary.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n""}, {'number': 4, 'created': '2016-11-08 17:35:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f1b3adafc462ce7f57e84fe488f3102a16fb0e4a', 'message': ""Add RPC API to get iPXE template and options\n\nThis patch adds a new `get_ipxe_template` RPC method.\n\nDepending on node state, it collects iPXE boot config template name and\noptions to render it with from boot (and deploy) node's driver\ninterfaces and returns them in the form of a dictionary.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n""}, {'number': 5, 'created': '2016-11-16 20:01:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f668e49f5afe1b238bec2c27d28776964ea69e00', 'message': ""Add RPC API to get iPXE template and options\n\nThis patch adds a new `get_ipxe_template` RPC method.\n\nDepending on node state, it collects iPXE boot config template name and\noptions to render it with from boot (and deploy) node's driver\ninterfaces and returns them in the form of a dictionary.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n""}, {'number': 6, 'created': '2016-11-17 13:24:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/60cfb8c494111d4f12ae0f53017223f4b97124b2', 'message': 'Add RPC API to get the iPXE boot config\n\nThis patch adds a new `get_ipxe_config` RPC method.\n\nIt generates iPXE boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 7, 'created': '2016-11-21 17:00:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/04035eac30a5b97e5cf89b34d5902a8ad3a2c509', 'message': 'Add RPC API to get the iPXE boot config\n\nThis patch adds a new `get_ipxe_config` RPC method.\n\nIt generates iPXE boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 8, 'created': '2016-11-24 18:36:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bf51fa4736b7eaeb823042c76c45006ef390db08', 'message': 'Add RPC API to get the iPXE boot config\n\nThis patch adds a new `get_ipxe_config` RPC method.\n\nIt generates iPXE boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 9, 'created': '2016-11-28 19:18:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f7db77d3175c9ab68713194d0c20c9293d0aa811', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 10, 'created': '2016-12-02 15:20:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/24249027ef96fa6846bacd811d2ef426459f65eb', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 11, 'created': '2016-12-02 18:04:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5dd64d83b519ca9506d3be265f2967daa4d5460c', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 12, 'created': '2016-12-02 18:48:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ca57b7ca835f8da5fa6647de1046c25cf0a7e9da', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 13, 'created': '2016-12-05 09:41:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9953ecc93f3ce6b453ef9248f6cbbe0e0c4ae29d', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 14, 'created': '2016-12-06 09:20:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f64bc6668f747b4f8e4a25b6eceb9d2d2b80fc20', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 15, 'created': '2016-12-19 19:44:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8cf16031a7db42d3d73e8d5b4d6dadf3e6d32541', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 16, 'created': '2017-01-10 17:36:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/290f4b153e8caa058604d3404c1f64f7727b89d5', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 17, 'created': '2017-01-13 10:13:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8a0ebbc08fd57abdd6eecbf5f6161a319fb4715a', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 18, 'created': '2017-01-23 08:44:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c594b27d348c6f646b52afcb1e9adfb98fb99601', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 19, 'created': '2017-02-07 17:14:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c2175d1d5ff18b34e7d8af67107959ee3b13bc2d', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 20, 'created': '2017-02-14 11:25:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a7b746102c19f12d2d26d5bf2e784a2244adb7e0', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 21, 'created': '2017-03-06 18:05:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8b5bef730a3a3784e3eee2340f4a513e9f0a0c40', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 22, 'created': '2017-03-07 14:05:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/55d7ab8161043401a20a65c962e93a4d88bd24c7', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 23, 'created': '2017-03-09 08:03:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e464890085bfd476c4bfc0fe7d4009be97163071', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 24, 'created': '2017-03-16 07:29:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7146480f587981a491c8368ac4612284c44850ee', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 25, 'created': '2017-04-06 07:34:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/167b3610021db7685684e29cad9a3f3a8e25e639', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 26, 'created': '2017-04-07 07:05:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/262e9853f1f385945604dc380e15c2cc6a4c5b10', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 27, 'created': '2017-04-07 10:11:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/31ad912ec8116c53fb814396a17a3f2410273080', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 28, 'created': '2017-04-12 09:52:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/783fd85abbb9dfd6420f673dcfcb4767e5cdcb74', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 29, 'created': '2017-05-16 05:52:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c779638342e17bf6751ca78899b4a9dae64b03a8', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 30, 'created': '2017-05-16 15:56:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d4d269bca941cdac32e1c84232cf31a50f875c5b', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 31, 'created': '2017-05-17 09:40:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f4213a8a77a9b10d5e608382253234c3a03951d9', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 32, 'created': '2017-05-22 07:59:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6afaccf293700109a446409262fdf8b1e5055591', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 33, 'created': '2017-06-09 07:56:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/abd1eeedee86604e11fd5b92896704e9879d1c89', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 34, 'created': '2017-06-29 10:17:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7e67ac533ce0aff2c07d7146fe6b4a5cdce18484', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 35, 'created': '2017-06-30 13:55:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c16a4ed3bc93eebb2e5dc4372af1e9a191584c3d', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 36, 'created': '2017-07-04 07:50:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5502a5c3cd77103dd2736ac556921d5b3818a44f', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 37, 'created': '2017-07-04 11:43:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/edc5166ed8276bf9ae98f5c4551df314e8e9d96f', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 38, 'created': '2017-07-05 09:03:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5e4a50d7004815005546ad82944356363e343a7d', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 39, 'created': '2017-07-10 11:49:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c1e1875784097e03339ffbac749d5a2b0a35437f', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 40, 'created': '2017-08-01 15:38:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1308cf20fd93737903b9f4279b8a559a8dd23d22', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 41, 'created': '2017-08-03 13:23:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/99dafcaf6f6073723863da590cd4b6281489d609', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 42, 'created': '2017-08-21 13:38:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/db0db49a1bf2e704b2b05677874140ce0bdf5b8f', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}, {'number': 43, 'created': '2017-08-29 10:31:43.000000000', 'files': ['ironic/conductor/manager.py', 'ironic/tests/unit/conductor/test_rpcapi.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/drivers/modules/fake.py', 'ironic/common/release_mappings.py', 'ironic/conductor/rpcapi.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/ea37ce8683a69b2ed54552f9545c7cdfb26b3d01', 'message': 'Add RPC API to get the boot config\n\nThis patch adds a new `get_boot_config` RPC method.\n\nIt generates boot config with the help of boot (and for non-active\nnodes, deploy) driver interfaces and returns it over RPC to the caller.\n\nChange-Id: I0981892f478a494abccc0d55fedb88f265777dde\nPartial-Bug: #1526275\n'}]",47,394399,ea37ce8683a69b2ed54552f9545c7cdfb26b3d01,295,15,43,9542,,,0,"Add RPC API to get the boot config

This patch adds a new `get_boot_config` RPC method.

It generates boot config with the help of boot (and for non-active
nodes, deploy) driver interfaces and returns it over RPC to the caller.

Change-Id: I0981892f478a494abccc0d55fedb88f265777dde
Partial-Bug: #1526275
",git fetch https://review.opendev.org/openstack/ironic refs/changes/99/394399/8 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/conductor/manager.py', 'ironic/conductor/rpcapi.py']",2,a670fcc50fc336c740877b6745b06a617d5a5828,bug/1526275," | 1.35 - Added get_ipxe_template RPC_API_VERSION = '1.35' def get_ipxe_template(self, context, node_id, topic=None): """"""Collect iPXE boot config options to render template. :param context: request context. :param node_id: node ID or UUID. :param topic: RPC topic. Defaults to self.topic. """""" cctxt = self.client.prepare(topic=topic or self.topic, version='1.35') return cctxt.call(context, 'get_ipxe_template', node_id=node_id)", RPC_API_VERSION = '1.34',44,2
openstack%2Fironic~master~I386663d3c73996f9693c80beb5cc37f31ae3efbc,openstack/ironic,master,I386663d3c73996f9693c80beb5cc37f31ae3efbc,DNM test enabled dynamic iPXE,NEW,2016-10-31 18:44:20.000000000,2017-12-18 02:25:10.000000000,,"[{'_account_id': 8871}, {'_account_id': 9542}, {'_account_id': 10118}, {'_account_id': 11878}, {'_account_id': 14250}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 22255}, {'_account_id': 22724}, {'_account_id': 23314}]","[{'number': 1, 'created': '2016-10-31 18:44:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3907ca638d3c87598c72fb64e66e50c59f38ff2a', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\n'}, {'number': 2, 'created': '2016-11-03 19:32:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0cade501df57f38fb22a3a8939b66aea6d505ff7', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\n'}, {'number': 3, 'created': '2016-11-07 12:17:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8b6cef8b36233b1b8f3468e290b4868481da24da', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 4, 'created': '2016-11-07 13:56:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f4b21bb45c1fd5859b6141af3f6dbcfe0a177cde', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 5, 'created': '2016-11-08 14:35:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/4c253f1424db38d0482b58146c1522daa20c1c31', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 6, 'created': '2016-11-08 17:35:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/41c7a60b4b3b3cdd58cb0856e67734e676947b1f', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 7, 'created': '2016-11-16 20:01:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/057bda65fbe2273002c96b66beada79687c41ee0', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 8, 'created': '2016-11-17 13:24:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e86f787685a2548c473bc9b904edf23291197abd', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 9, 'created': '2016-11-21 17:00:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/942bece9fff40db757fba4dc17d9ba0c766daa4f', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 10, 'created': '2016-11-24 18:37:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5eeb85b6350c2575cb36e9abe5b3ca285c4a0e42', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 11, 'created': '2016-11-24 20:13:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9db17771b23f0caf66a0a75afdc6e4251be11e58', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 12, 'created': '2016-11-25 09:20:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0bda4b697ba4bdc9e443c9a07c6292b2b49413d6', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 13, 'created': '2016-11-28 19:20:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f3b987529d28b781e292f92d1b7739c54383b2b7', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 14, 'created': '2016-12-02 15:20:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c924e7a858101fa75aec35104e58a497abb37751', 'message': 'DNM test enabled dynamic iPXE\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 15, 'created': '2016-12-02 18:49:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ab48affddf59f76e15612aede012cd075eb5593c', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 16, 'created': '2016-12-05 09:42:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/87b775b186286f4fcc642387fe3705884466fe23', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 17, 'created': '2016-12-06 09:20:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3314db75e09afa5d3af32bc6b59213168cc7abc0', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 18, 'created': '2016-12-19 19:45:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9ffd4b63d70d75b410401614e09c77359bf4808c', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 19, 'created': '2017-01-11 12:47:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/623dbdfd0bb08e1665f6be5b1bf46d5937f5efd2', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 20, 'created': '2017-01-13 10:13:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b662ec194a0eb76b692ab07de6c5c9301664aae4', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 21, 'created': '2017-01-23 08:45:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8dddf4476d590ef8b561a5ce6fae6b8a8704e5eb', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 22, 'created': '2017-02-07 17:14:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1d2103e7042203e8b3beb55803088c905262f535', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 23, 'created': '2017-02-14 11:26:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/52e89403b9c9d893f9cee6d34224dc9505352b9f', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 24, 'created': '2017-03-06 18:05:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cbbc5b3dc2b11e46ec76befa8e81c2b4c1092a75', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 25, 'created': '2017-03-07 14:06:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ec91b47bb36eb45916b1243ed84f7c424126dc6a', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 26, 'created': '2017-03-09 08:03:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cf7335afa838e57f2d07893b3d0ea505755fd8df', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 27, 'created': '2017-03-09 14:17:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a49b25ae7ca09f58f2dee8fc28cb32fb26926277', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 28, 'created': '2017-03-16 07:31:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/42fcc6a359917eb9e544e36bc36745327cf2d7a8', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 29, 'created': '2017-04-06 07:34:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/713251246da442868391af1fda9849291b2aa7c9', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 30, 'created': '2017-04-07 07:06:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b010e7fdebc467b05fb028870ef68f61b8865d12', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 31, 'created': '2017-04-07 10:12:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/dd20fe1552f0a831e522f4dcba3c5813ac653dd1', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 32, 'created': '2017-04-12 09:52:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9e9e733fefa6322e6f9e6a7e0549b784775e58d8', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 33, 'created': '2017-05-16 05:52:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9a239ae49ff2ca07d9661cee9c7e1c030ef6a130', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 34, 'created': '2017-05-16 15:58:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d561a2d99dbcaca272065b0b232def50626a56b0', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 35, 'created': '2017-05-17 09:41:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/dc844f40a1369e226e502270a7b22cb879ef0f00', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 36, 'created': '2017-05-22 08:00:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9d69e873fdf8d8bad68309cb1628512d8c891a25', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 37, 'created': '2017-06-09 07:56:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/aeb453f558e06ef3949d580c4fa7c0070e63e1fa', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 38, 'created': '2017-06-29 10:17:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/52e24147f06604825f38b3f9db3a55a0b273e2ff', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 39, 'created': '2017-06-30 13:55:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5572aab7b41827e73341ec45c37ac017f8d8741a', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 40, 'created': '2017-07-04 07:50:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/abe8a1479fad4c2bd97f9644bf245b0a340519b0', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 41, 'created': '2017-07-04 11:43:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/438627b3d7cf3746e6c46220133e4412ee2e7e1b', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 42, 'created': '2017-07-05 09:03:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e397136264c54b847982bf933b1dc8ceb7282e20', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 43, 'created': '2017-07-10 11:49:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/beb4195349e6227a9ee0415740192a73b8e0f9da', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 44, 'created': '2017-08-01 15:38:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ac167876db7c4cb88302b2b2e6dabff53f20497b', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 45, 'created': '2017-08-03 13:23:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ffa5dd9096dcbf527809062880d7fe438656d729', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 46, 'created': '2017-08-21 13:38:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5a1f8a0f474b3296eb495399a758170e2463d46d', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}, {'number': 47, 'created': '2017-08-29 10:31:43.000000000', 'files': ['devstack/lib/ironic'], 'web_link': 'https://opendev.org/openstack/ironic/commit/708cb67f6ff485fab76577595ccaf358fb15b26d', 'message': 'DNM test enabled dynamic iPXE\n\nAlso enable boot from swift temp urls when those are available\n\nChange-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc\nRelated-Bug: #1526275\n'}]",0,391943,708cb67f6ff485fab76577595ccaf358fb15b26d,279,12,47,9542,,,0,"DNM test enabled dynamic iPXE

Also enable boot from swift temp urls when those are available

Change-Id: I386663d3c73996f9693c80beb5cc37f31ae3efbc
Related-Bug: #1526275
",git fetch https://review.opendev.org/openstack/ironic refs/changes/43/391943/36 && git format-patch -1 --stdout FETCH_HEAD,['devstack/lib/ironic'],1,3907ca638d3c87598c72fb64e66e50c59f38ff2a,bug/1526275,"IRONIC_IPXE_SERVER_ENABLED=$(trueorfalse True IRONIC_IPXE_SERVER_ENABLED) if [[ ""$IRONIC_IPXE_SERVER_ENABLED"" == ""True"" ]]; then iniset $IRONIC_CONF_FILE pxe ipxe_server_enabled True fi",,4,0
openstack%2Frally~master~Id0db3088edccf76c325160b7fb60144b5d540e8c,openstack/rally,master,Id0db3088edccf76c325160b7fb60144b5d540e8c,Add heat dataplane scenario,NEW,2016-04-25 09:15:00.000000000,2017-12-18 02:25:04.000000000,,"[{'_account_id': 7369}, {'_account_id': 10475}, {'_account_id': 14817}]","[{'number': 1, 'created': '2016-04-25 09:15:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/7e7c2014a43c730d3abcddfd2aa67092bb21c25d', 'message': 'Add sample workload ""Wordpress""\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n'}, {'number': 2, 'created': '2016-04-25 09:42:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/432d70acf7da76916a4cca5c36960b66937de2be', 'message': ""Add heat workload scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 3, 'created': '2016-04-25 11:43:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d2eb9203c8a9a1428191e2899d413be7a592e595', 'message': ""Add heat workload scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 4, 'created': '2016-04-26 09:11:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4c85d82e3bdc5636c43c293a79a9d218ed82b8c7', 'message': ""Add heat workload scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 5, 'created': '2016-04-26 12:32:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/24a0e175f260054db9b2f3f5bbb0ad412ba5172f', 'message': ""Add heat workload scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 6, 'created': '2016-04-27 10:13:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/01ce58c263305e11efbd3a474544cdd0e137041e', 'message': ""Add heat workload scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 7, 'created': '2016-05-10 11:03:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b9e64697d7497b84f10064b016c9bd6ce324e250', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 8, 'created': '2016-05-13 09:00:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/af4a609aebe7ceb9e042515021f0697fb4406cb9', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 9, 'created': '2016-05-13 10:18:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e17b0072c080864c720e22e01eed9bce43f960ea', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 10, 'created': '2016-05-13 13:59:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a9c2170f0a9c39eca6ab3038df5b1d0d3670d31f', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 11, 'created': '2016-05-16 11:47:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/79f4202c7f3d03dc6d64f3c89625116428343487', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 12, 'created': '2016-05-17 10:05:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/afddddeb92b7dc39f202eff3bd07db2ca3ffb3cf', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 13, 'created': '2016-05-17 12:42:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/727749d8d19cbad2214334353c0752edc8e4d9b5', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 14, 'created': '2016-05-17 15:55:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6dbdcd1deb2ce281bf01977b3b39ec2559862bf0', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 15, 'created': '2016-05-18 10:18:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/879231be52011411fe7f1909bd0781e247d12481', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 16, 'created': '2016-05-26 16:31:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/129b0093a23c0d8486e1158d3eb7f690c2a29d74', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 17, 'created': '2016-05-31 10:21:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/fb1165af4effb88918770dcc85156545fc9c4e7a', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 18, 'created': '2016-06-02 09:18:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c3acaec8f5e6917e15178c6e6a8fa4a2f0422ace', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 19, 'created': '2016-06-02 11:28:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0abfa11440bbf3dd9515e0cd0829560e026f1645', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 20, 'created': '2016-06-07 12:15:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6361d107bf3eb7e98868b506569aebde8fc60f98', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 21, 'created': '2016-06-14 11:26:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/9c0e9287e715c5d2825b10621d8f3b85e3a16784', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 22, 'created': '2016-06-17 13:47:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/f1b2adfca4ae291769898a10b7052d2b46156a46', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}, {'number': 23, 'created': '2016-09-12 11:50:29.000000000', 'files': ['rally/plugins/dataplane/__init__.py', 'tests/unit/plugins/dataplane/wordpress/__init__.py', 'rally-jobs/extra/workload/wordpress_heat_template.yaml', 'rally/plugins/dataplane/wordpress/siege.py', 'rally-jobs/rally-mos.yaml', 'rally/plugins/dataplane/wordpress/wp-instances.yaml', 'samples/tasks/scenarios/dataplane/heat.yaml', 'samples/tasks/scenarios/dataplane/heat.json', 'rally/plugins/openstack/context/dataplane/heat.py', 'tests/unit/plugins/dataplane/__init__.py', 'tests/unit/plugins/openstack/scenarios/dataplane/test_heat.py', 'rally/plugins/dataplane/wordpress/__init__.py', 'tests/unit/plugins/dataplane/wordpress/test_siege.py', 'rally/plugins/dataplane/README.rst', 'rally/plugins/dataplane/wordpress/README.rst', 'tests/unit/plugins/openstack/scenarios/dataplane/__init__.py', 'rally/plugins/openstack/scenarios/dataplane/heat.py', 'rally/plugins/openstack/scenarios/dataplane/__init__.py', 'rally/plugins/dataplane/wordpress/wordpress_heat_template.yaml', 'rally-jobs/extra/workload/siege.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/b82636bc54a27d75e2231767825e93f77fa2f9a9', 'message': ""Add heat dataplane scenario\n\nHeat context is deploying stack by given template.\nScenario is running any workload in given stack.\n\nStack should have a resource named 'gate_node'. This should be a VM\naccessible by ssh. Rally will connect to this VM and run workload\nscript. Heat output will be sent to this script's stdin. Script should\nproduce valid json to stdout.\n\nRun Wordpress on given number of nodes and run http bechmarking tool.\n\nChange-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c\n""}]",15,309897,b82636bc54a27d75e2231767825e93f77fa2f9a9,95,3,23,7369,,,0,"Add heat dataplane scenario

Heat context is deploying stack by given template.
Scenario is running any workload in given stack.

Stack should have a resource named 'gate_node'. This should be a VM
accessible by ssh. Rally will connect to this VM and run workload
script. Heat output will be sent to this script's stdin. Script should
produce valid json to stdout.

Run Wordpress on given number of nodes and run http bechmarking tool.

Change-Id: Id0db3088edccf76c325160b7fb60144b5d540e8c
",git fetch https://review.opendev.org/openstack/rally refs/changes/97/309897/1 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/workload/README.rst', 'rally/plugins/workload/wordpress/siege.py', 'rally/plugins/workload/wordpress/test_siege.py', 'samples/tasks/scenarios/workload/heat.json', 'rally-jobs/extra/workload/wordpress_heat_template.yaml', 'rally/plugins/workload/wordpress/README.rst', 'rally/plugins/workload/wordpress/wp-instances.yaml', 'samples/tasks/scenarios/workload/heat.yaml', 'rally/plugins/workload/wordpress/__init__.py', 'rally/plugins/workload/wordpress/wordpress_heat_template.yaml', 'rally-jobs/extra/workload/siege.py']",11,7e7c2014a43c730d3abcddfd2aa67092bb21c25d,dataplane,"#!/usr/bin/env python # # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """"""Sample workload for Workload.heat scenario."""""" import json import re import subprocess import sys import tempfile SIEGE_RE = re.compile(r""^([a-zA-Z\ ]+):\s+(\d+(\.\d+)?).*"") def get_instances(): outputs = json.load(sys.stdin) for output in outputs: if output[""output_key""] == ""wp_nodes"": for node in output[""output_value""].values(): yield node[""wordpress-network""][0] def generate_urls_list(): urls = tempfile.NamedTemporaryFile(delete=False) with urls: for inst in get_instances(): for i in range(1, 1000): urls.write(""http://%s/wordpress/index.php/%d/\n"" % (inst, i)) return urls.name def run(): urls = generate_urls_list() cmd = [""siege"", ""-q"", ""-t"", ""60S"", ""-b"", ""-f"", urls] out = subprocess.check_output(cmd, stderr=subprocess.STDOUT) data = {} for line in out.splitlines(): m = SIEGE_RE.match(line) if m: data[m.group(1)] = float(m.group(2)) sys.stdout.write(json.dumps(data)) if __name__ == ""__main__"": sys.exit(run()) ",,843,0
openstack%2Fproject-config~master~Id0729cae51aa5b99eb0544075d9cb6d54562d7d2,openstack/project-config,master,Id0729cae51aa5b99eb0544075d9cb6d54562d7d2,WIP Split secret and non-secret data for proposals,NEW,2017-08-30 21:00:28.000000000,2017-12-18 02:24:26.000000000,,"[{'_account_id': 2}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-08-30 21:00:28.000000000', 'files': ['zuul.yaml', 'playbooks/proposal/pre.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/cc76d45b6812af95ad30c46b5fad1b84cec6b862', 'message': 'WIP Split secret and non-secret data for proposals\n\nMake the proposal job use a secret that only defines the secret data and\nputs the host information into a variable instead.\n\nChange-Id: Id0729cae51aa5b99eb0544075d9cb6d54562d7d2\nDepends-On: I47ca57fd43eb73ff6782da5f27dca3a3132f13de\n'}]",0,499339,cc76d45b6812af95ad30c46b5fad1b84cec6b862,6,2,1,2,,,0,"WIP Split secret and non-secret data for proposals

Make the proposal job use a secret that only defines the secret data and
puts the host information into a variable instead.

Change-Id: Id0729cae51aa5b99eb0544075d9cb6d54562d7d2
Depends-On: I47ca57fd43eb73ff6782da5f27dca3a3132f13de
",git fetch https://review.opendev.org/openstack/project-config refs/changes/39/499339/1 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.yaml', 'playbooks/proposal/pre.yaml']",2,cc76d45b6812af95ad30c46b5fad1b84cec6b862,498634, - add-hosts-to-known-hosts,,6,3
openstack%2Frally~master~I3b476f87daef743cfb5d2877ac0d3fe32c81c31a,openstack/rally,master,I3b476f87daef743cfb5d2877ac0d3fe32c81c31a,Output dot while upgrading database,NEW,2017-07-17 05:53:57.000000000,2017-12-18 02:23:31.000000000,,"[{'_account_id': 14817}, {'_account_id': 21528}]","[{'number': 1, 'created': '2017-07-17 05:53:57.000000000', 'files': ['rally/cli/manage.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/5e1e160675c58d347806bc3c09ede416f9ab8826', 'message': 'Output dot while upgrading database\n\nIt takes a lot of time while upgrading database,\nlooks like rally was dead. Let us print one dot per 3\nseconds to tell users rally is working.\n\nChange-Id: I3b476f87daef743cfb5d2877ac0d3fe32c81c31a\n'}]",0,484252,5e1e160675c58d347806bc3c09ede416f9ab8826,8,2,1,21528,,,0,"Output dot while upgrading database

It takes a lot of time while upgrading database,
looks like rally was dead. Let us print one dot per 3
seconds to tell users rally is working.

Change-Id: I3b476f87daef743cfb5d2877ac0d3fe32c81c31a
",git fetch https://review.opendev.org/openstack/rally refs/changes/52/484252/1 && git format-patch -1 --stdout FETCH_HEAD,['rally/cli/manage.py'],1,5e1e160675c58d347806bc3c09ede416f9ab8826,upgrade.database,"import threadingfrom rally.common import utils as tutilsdef output_migration_result(method_name, method): upgrade_thread = threading.Thread(target=method) upgrade_thread.start() while upgrade_thread.isAlive(): sys.stdout.write("". "") sys.stdout.flush() tutils.interruptable_sleep(3) output_migration_result(""upgrade"", db.schema_upgrade)","def output_migration_result(method_name): yield with output_migration_result(""upgrade""): db.schema_upgrade()",11,4
openstack%2Fpython-zunclient~master~I2b80bce3ab28dbb458ca00103187c2b5d57eb5eb,openstack/python-zunclient,master,I2b80bce3ab28dbb458ca00103187c2b5d57eb5eb,Add image_tag support - zun create,NEW,2017-08-21 11:16:40.000000000,2017-12-18 02:23:06.000000000,,[{'_account_id': 16190}],"[{'number': 1, 'created': '2017-08-21 11:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-zunclient/commit/a4cf194d9b40f2274a62fdac9bbd6a7cb134c9a8', 'message': 'Add image_tag support - zun create\n\nChange-Id: I2b80bce3ab28dbb458ca00103187c2b5d57eb5eb\nRelated-Bug: #1709238\n'}, {'number': 2, 'created': '2017-09-04 07:31:06.000000000', 'files': ['zunclient/common/utils.py', 'zunclient/v1/containers_shell.py', 'zunclient/osc/v1/containers.py', 'zunclient/v1/containers.py'], 'web_link': 'https://opendev.org/openstack/python-zunclient/commit/37595433ac0ce7ca9c2fbc2dc56d0044ddc59a0a', 'message': 'Add image_tag support - zun create\n\nChange-Id: I2b80bce3ab28dbb458ca00103187c2b5d57eb5eb\nRelated-Bug: #1709238\n'}]",0,495821,37595433ac0ce7ca9c2fbc2dc56d0044ddc59a0a,7,1,2,16190,,,0,"Add image_tag support - zun create

Change-Id: I2b80bce3ab28dbb458ca00103187c2b5d57eb5eb
Related-Bug: #1709238
",git fetch https://review.opendev.org/openstack/python-zunclient refs/changes/21/495821/1 && git format-patch -1 --stdout FETCH_HEAD,"['zunclient/v1/containers_shell.py', 'zunclient/v1/containers.py']",2,a4cf194d9b40f2274a62fdac9bbd6a7cb134c9a8,bug/1709238," 'image_tag', 'security_groups', 'hints', 'nets', 'auto_remove']"," 'security_groups', 'hints', 'nets', 'auto_remove']",9,1
openstack%2Fproject-config~master~I15c438d17a518cefb03bbdbac3399a7b1f6e324d,openstack/project-config,master,I15c438d17a518cefb03bbdbac3399a7b1f6e324d,Use rollup base roles in base-test job,NEW,2017-09-04 18:25:47.000000000,2017-12-18 02:22:40.000000000,,"[{'_account_id': 2}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-09-04 18:25:47.000000000', 'files': ['playbooks/base-test/post-ssh.yaml', 'playbooks/base-test/pre.yaml', 'playbooks/base-test/post-logs.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/0d0870a58d7e45212e5b14894824b3fad6f17f6f', 'message': 'Use rollup base roles in base-test job\n\nWe should be able to validate that they work before shifting everything\nto them.\n\nDepends-On: I2784b58bca4747b6417733b4394194dc20c35c04\nChange-Id: I15c438d17a518cefb03bbdbac3399a7b1f6e324d\n'}]",0,500614,0d0870a58d7e45212e5b14894824b3fad6f17f6f,6,2,1,2,,,0,"Use rollup base roles in base-test job

We should be able to validate that they work before shifting everything
to them.

Depends-On: I2784b58bca4747b6417733b4394194dc20c35c04
Change-Id: I15c438d17a518cefb03bbdbac3399a7b1f6e324d
",git fetch https://review.opendev.org/openstack/project-config refs/changes/14/500614/1 && git format-patch -1 --stdout FETCH_HEAD,"['playbooks/base-test/post-ssh.yaml', 'playbooks/base-test/pre.yaml', 'playbooks/base-test/post-logs.yaml']",3,0d0870a58d7e45212e5b14894824b3fad6f17f6f,zuulv3-base-rollup, - zuul-base-logs," - role: add-fileserver fileserver: ""{{ site_logs }}"" - emit-ara-html - hosts: ""{{ site_logs.fqdn }}"" gather_facts: False roles: - role: upload-logs zuul_log_url: ""http://logs.openstack.org""",3,16
openstack%2Fproject-config~master~I1541c73435acc1d919b11fcf48f27ef26ab7f27a,openstack/project-config,master,I1541c73435acc1d919b11fcf48f27ef26ab7f27a,Use zuul base roll-up roles in base jobs,NEW,2017-09-04 18:25:47.000000000,2017-12-18 02:22:37.000000000,,"[{'_account_id': 2}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-09-04 18:25:47.000000000', 'files': ['playbooks/base/pre.yaml', 'playbooks/base/post-logs.yaml', 'playbooks/base/post-ssh.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/378a5371e5355371a6582c52fc9010142cf53d53', 'message': ""Use zuul base roll-up roles in base jobs\n\nSimilar to puppet-openstackci, if we use the roll-up role ourselves,\nwe'll ensure it continues to work.\n\nChange-Id: I1541c73435acc1d919b11fcf48f27ef26ab7f27a\n""}]",0,500615,378a5371e5355371a6582c52fc9010142cf53d53,6,2,1,2,,,0,"Use zuul base roll-up roles in base jobs

Similar to puppet-openstackci, if we use the roll-up role ourselves,
we'll ensure it continues to work.

Change-Id: I1541c73435acc1d919b11fcf48f27ef26ab7f27a
",git fetch https://review.opendev.org/openstack/project-config refs/changes/15/500615/1 && git format-patch -1 --stdout FETCH_HEAD,"['playbooks/base/pre.yaml', 'playbooks/base/post-logs.yaml', 'playbooks/base/post-ssh.yaml']",3,378a5371e5355371a6582c52fc9010142cf53d53,zuulv3-base-rollup, - zuul-base-post, - remove-build-sshkey,3,15
openstack%2Ftosca-parser~master~I21b7a1098856f77d06930a55601b356b8f83a17e,openstack/tosca-parser,master,I21b7a1098856f77d06930a55601b356b8f83a17e,reverting the wrong-patch Bug Id: https://bugs.launchpad.net/tosca-parser/+bug/1698089 To replace the input parameters for the interface and relation templates. Taken care of the line indention etc.handle the scenario when no input is defined,NEW,2017-08-30 06:15:08.000000000,2017-12-18 02:22:30.000000000,,"[{'_account_id': 6456}, {'_account_id': 16511}]","[{'number': 1, 'created': '2017-08-30 06:15:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tosca-parser/commit/6d4608c9d871aa60944095fc54c602135349e951', 'message': 'Bug Id: https://bugs.launchpad.net/tosca-parser/+bug/1698089\nTo replace the input parameters for the interface and relation templates.\n\nChange-Id: I21b7a1098856f77d06930a55601b356b8f83a17e\nSigned-off-by: Manas Mal <manaskrmal@yahoo.com>\n'}, {'number': 2, 'created': '2017-08-30 07:09:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tosca-parser/commit/ccbd19ccc4448f018b0f233c787fd3f8edfdfdb4', 'message': 'Bug Id: https://bugs.launchpad.net/tosca-parser/+bug/1698089\nTo replace the input parameters for the interface and relation templates.\nTaken care of the line indention.\n\nChange-Id: I21b7a1098856f77d06930a55601b356b8f83a17e\nSigned-off-by: Manas Mal <manaskrmal@yahoo.com>\n'}, {'number': 3, 'created': '2017-08-30 07:25:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tosca-parser/commit/33bfb7fcc0258cfb0a5ff352765189eeb3649b37', 'message': 'Bug Id: https://bugs.launchpad.net/tosca-parser/+bug/1698089\nTo replace the input parameters for the interface and relation templates.\nTaken care of the line indention etc.\n\nChange-Id: I21b7a1098856f77d06930a55601b356b8f83a17e\nSigned-off-by: Manas Mal <manaskrmal@yahoo.com>\n'}, {'number': 4, 'created': '2017-08-30 09:26:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tosca-parser/commit/0a37f65675901987a8a2c561a777e82ad1d650ef', 'message': 'Bug Id: https://bugs.launchpad.net/tosca-parser/+bug/1698089\nTo replace the input parameters for the interface and relation templates.\nTaken care of the line indention etc.handle the scenario when no input is defined\n\nChange-Id: I21b7a1098856f77d06930a55601b356b8f83a17e\nSigned-off-by: Manas Mal <manaskrmal@yahoo.com>\n'}, {'number': 5, 'created': '2017-09-06 14:04:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tosca-parser/commit/585918ba16264275d565f840d6136f3daa4682b2', 'message': 'Adding the UT\nBug Id: https://bugs.launchpad.net/tosca-parser/+bug/1698089\nTo replace the input parameters for the interface and relation templates.\nTaken care of the line indention etc.handle the scenario when no input is defined\n\nChange-Id: I21b7a1098856f77d06930a55601b356b8f83a17e\nSigned-off-by: Manas Mal <manaskrmal@yahoo.com>\n'}, {'number': 6, 'created': '2017-09-06 14:20:02.000000000', 'files': ['toscaparser/topology_template.py', 'toscaparser/functions.py'], 'web_link': 'https://opendev.org/openstack/tosca-parser/commit/89215d26236c3f7ca0331d2e6339f952f80f8bb2', 'message': 'reverting the wrong-patch\nBug Id: https://bugs.launchpad.net/tosca-parser/+bug/1698089\nTo replace the input parameters for the interface and relation templates.\nTaken care of the line indention etc.handle the scenario when no input is defined\n\nChange-Id: I21b7a1098856f77d06930a55601b356b8f83a17e\nSigned-off-by: Manas Mal <manaskrmal@yahoo.com>\n'}]",0,499035,89215d26236c3f7ca0331d2e6339f952f80f8bb2,12,2,6,25998,,,0,"reverting the wrong-patch
Bug Id: https://bugs.launchpad.net/tosca-parser/+bug/1698089
To replace the input parameters for the interface and relation templates.
Taken care of the line indention etc.handle the scenario when no input is defined

Change-Id: I21b7a1098856f77d06930a55601b356b8f83a17e
Signed-off-by: Manas Mal <manaskrmal@yahoo.com>
",git fetch https://review.opendev.org/openstack/tosca-parser refs/changes/35/499035/5 && git format-patch -1 --stdout FETCH_HEAD,['toscaparser/topology_template.py'],1,6d4608c9d871aa60944095fc54c602135349e951,," interfacevalue = functions.get_function( if isinstance(interfacevalue, functions.GetInput): interface.inputs[name] = \ interfacevalue.result() interfacevalue = \ if isinstance(interfacevalue, \ functions.GetInput): interface.inputs[name] = \ interfacevalue.result() ", interface.inputs[name] = functions.get_function( interface.inputs[name] = \,11,2
openstack%2Fpython-swiftclient~master~Ifefc1826332ca63d0fb22d12465d2ab539e58e30,openstack/python-swiftclient,master,Ifefc1826332ca63d0fb22d12465d2ab539e58e30,Fixed capability discovery endpoint hardcode,NEW,2017-09-05 15:17:31.000000000,2017-12-18 02:22:13.000000000,,[],"[{'number': 1, 'created': '2017-09-05 15:17:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/2dbe13f1b24d2636e52cda4e535899c8998ce75f', 'message': 'Fixed capability discovery endpoint hardcode\n\nChange-Id: Ifefc1826332ca63d0fb22d12465d2ab539e58e30\nCloses-bug: #1712358\n'}, {'number': 2, 'created': '2017-09-07 06:06:57.000000000', 'files': ['swiftclient/client.py'], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/5f11455cd130a7b87e29e35c67ed96c35c0d491f', 'message': 'Fixed capability discovery endpoint hardcode\n\nChange-Id: Ifefc1826332ca63d0fb22d12465d2ab539e58e30\nCloses-bug: #1712358\n'}]",0,500883,5f11455cd130a7b87e29e35c67ed96c35c0d491f,5,0,2,26831,,,0,"Fixed capability discovery endpoint hardcode

Change-Id: Ifefc1826332ca63d0fb22d12465d2ab539e58e30
Closes-bug: #1712358
",git fetch https://review.opendev.org/openstack/python-swiftclient refs/changes/83/500883/2 && git format-patch -1 --stdout FETCH_HEAD,['swiftclient/client.py'],1,2dbe13f1b24d2636e52cda4e535899c8998ce75f,," #scheme = urlparse(url).scheme #netloc = urlparse(url).netloc #url = scheme + '://' + netloc + '/info' parts = url.split(""/"") parts[-1] = ""info"" url = ""/"".join(parts)", scheme = urlparse(url).scheme netloc = urlparse(url).netloc url = scheme + '://' + netloc + '/info',6,3
openstack%2Fironic~master~Ic908b276a0719a2e23bb8634011203932a33349d,openstack/ironic,master,Ic908b276a0719a2e23bb8634011203932a33349d,Serve boot configs from Ironic API,NEW,2016-09-13 13:06:22.000000000,2017-12-18 02:21:43.000000000,,"[{'_account_id': 6773}, {'_account_id': 7900}, {'_account_id': 8871}, {'_account_id': 9542}, {'_account_id': 10118}, {'_account_id': 11878}, {'_account_id': 12356}, {'_account_id': 13636}, {'_account_id': 14250}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 19593}, {'_account_id': 20311}, {'_account_id': 22255}, {'_account_id': 22724}, {'_account_id': 26340}]","[{'number': 1, 'created': '2016-09-13 13:06:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/15a23ed5862ac3200fe0976675f385fe3237ea03', 'message': ""WIP iPXE config endpoint\n\nAdds new endpoints 'v1/ipxe_config' to serve iPXE boot scripts and\nconfigs directly from Ironic API.\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped to 1.23, this endpoint does not\ncheck the requested API version and relies solely on config option\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nThis new API also honors 'CONF.api.restrict_lookup' config option\nand when enabled returns configs only if the node is in one of\nappropriate states.\n\nBoot script API:\nGET v1/ipxe_config?mac=<mac>&node_uuid=<node-uuid>\n(quite similar to lookup)\n(not there yet) RPC calls into conductor to get Ironic API\nand renders Jinja template for iPXE boot script directly in API.\nBoth mac and node_uuid are optional\n(but at least one of them must be provided).\nUsing the former is mostly suitable for standalone Ironic\nwhere external DHCP server can be pre-configured to generate the PXE opts\nfor DHCP request using the MAC of requester.\n\nBoot config:\nGET v1/ipxe_config/<node-uuid>\n(not there yet) gets params for boot config over RPC from\nrespective conductor and then renders the Jinja template in API.\n\nNew config option to this regard CONF.api.serve_ipxe_config\n(default is False).\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 2, 'created': '2016-09-13 16:12:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cc176bc478061878f1d48c30c9f9ca66d715ce08', 'message': ""WIP iPXE config endpoint\n\nAdds new endpoints `v1/ipxe_config` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped to 1.23, this endpoint does not\ncheck the requested API version and relies solely on config option\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option to this regard `CONF.api.serve_ipxe_config`\n(default is False).\n\nThis new API also honors 'CONF.api.restrict_lookup' config option\nand when that is enabled returns configs only if the node is in one of\nappropriate states.\n\nBoot script API:\n\nRequest: GET v1/ipxe_config?mac=<mac>&node_uuid=<node-uuid>\n\nDescription: resolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\nBoth `mac` and `node_uuid` are optional but at least one of them must be\nprovided, with node_uuid taking precedence.\nUsing the `?mac=` form is mostly suitable for standalone Ironic\nwhere external DHCP server can be pre-configured to generate the PXE opts\nfor DHCP request using the MAC of requester.\n\nBoot config:\n\nRequest: GET v1/ipxe_config/<node-uuid>\n\nDescription: (not there yet, returns fake config)\ngets params for boot config over RPC from respective conductor\nand then renders the Jinja template in API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 3, 'created': '2016-09-19 15:22:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/4bc448c269a5dcff31eebbe3cc0b10cd0676e049', 'message': ""Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped to 1.23, this endpoint does not\ncheck the requested API version and relies solely on config option\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option to this regard `CONF.pxe.ipxe_server_enabled`\n(default is False).\n\nThis new API also honors 'CONF.api.restrict_lookup' config option\nand when that is enabled returns configs only if the node is in one of\nappropriate states.\n\nRPC version is bumped due to new `get_ipxe_config` method.\n\nTo support this feature drivers are expected to implement\n`get_dynamic_ipxe_options` method in deploy interface and\n`get_pxe_options` method in boot interface.\nThis patch adds implementations to AgentDeploy and PXEBoot respectively.\n\nNew iPXE boot script `dynamic_boot.ipxe` is added and needs to be specified\nin ironic.conf as `[pxe]ipxe_boot_script` for this feature to work.\n\nBoot script API:\n\nRequest: GET v1/ipxe?mac=<mac>&node_uuid=<node-uuid>\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\nBoth `mac` and `node_uuid` are optional but at least one of them must be\nprovided, with node_uuid taking precedence.\nUsing the `?mac=` form is mostly suitable for standalone Ironic\nwhere external DHCP server can be pre-configured to generate\nappropriate PXE opts for DHCP response using the MAC of requester.\n\nBoot config:\n\nRequest: GET v1/ipxe/<node-uuid>\n\nDescription:\ngets boot config over RPC from conductor which manages given node\nand then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 4, 'created': '2016-09-20 17:38:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ce0a3e6af892090dea8a30fa5d5aa5496bbc1dd5', 'message': ""Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped to 1.23, this endpoint does not\ncheck the requested API version and relies solely on config option\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option to this regard `CONF.pxe.ipxe_server_enabled`\n(default is False).\n\nThis new API also honors 'CONF.api.restrict_lookup' config option\nand when that is enabled returns configs only if the node is in one of\nappropriate states.\n\nRPC version is bumped due to new `get_ipxe_config` method.\n\nTo support this feature drivers are expected to implement\n`get_dynamic_ipxe_options` method in deploy interface and\n`get_pxe_options` method in boot interface.\nThis patch adds implementations to AgentDeploy and PXEBoot respectively.\n\nNew iPXE boot script `dynamic_boot.ipxe` is added and needs to be specified\nin ironic.conf as `[pxe]ipxe_boot_script` for this feature to work.\n\nBoot script API:\n\nRequest: GET v1/ipxe?mac=<mac>&node_uuid=<node-uuid>\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\nBoth `mac` and `node_uuid` are optional but at least one of them must be\nprovided, with node_uuid taking precedence.\nUsing the `?mac=` form is mostly suitable for standalone Ironic\nwhere external DHCP server can be pre-configured to generate\nappropriate PXE opts for DHCP response using the MAC of requester.\n\nBoot config:\n\nRequest: GET v1/ipxe/<node-uuid>\n\nDescription:\ngets boot config over RPC from conductor which manages given node\nand then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 5, 'created': '2016-10-17 19:30:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/de42299a51c32eaff9d913147653fb727bcbce92', 'message': 'WiP Dynamic iPXE with core+vendor API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts\ndirectly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped to 1.23, this endpoint does not\ncheck the requested API version and relies solely on config option\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config options to this regard:\n\n- `CONF.pxe.enable_ipxe_server` to enable this new API (default is False)\n- `CONF.api.restrict_ipxe` to restrict iPXE config fetch to only nodes\n  in appropriate states (default is True), similar to\n  CONF.api.restrict_lookup\n- `CONF.api.ipxe_script_template` points to Jinja2 template of iPXE\n  boot script, default is $pybasedir/drivers/modules/ipxe.j2\n\nTo support this feature deploy driver interfaces are expected\nto inherit from new `IPXEDeployMixin` class and implement\n`get_ipxe_options` method.\nThis patch adds implementation based on AgentDeploy as IPXEAgentDeploy\ninterface.\n\nBoot script API:\n\nRequest: GET v1/ipxe?mac=<mac>&node_uuid=<node-uuid>\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\nBoth `mac` and `node_uuid` are optional but at least one of them must be\nprovided, with node_uuid taking precedence.\nUsing the `?mac=` form is mostly suitable for standalone Ironic\nwhere external DHCP server can be pre-configured to generate\nappropriate PXE opts for DHCP response using the MAC of requester.\n\nBoot config:\n\nRequest: GET v1/nodes/<node-uuid>/vendor_passthru/ipxe_config\n\nDescription:\nimplemented as vendor passthru extension, this call fetches boot config\nand then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n'}, {'number': 6, 'created': '2016-10-18 10:33:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/962278f10f118794ddf9deba9ac3770f99b63447', 'message': ""PoC Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped to 1.23, this endpoint does not\ncheck the requested API version and relies solely on config option\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option to this regard `CONF.pxe.ipxe_server_enabled`\n(default is False).\n\nThis new API also honors 'CONF.api.restrict_lookup' config option\nand when that is enabled returns configs only if the node is in one of\nappropriate states.\n\nRPC version is bumped due to new `get_ipxe_config` method.\n\nTo support this feature drivers are expected to implement\n`get_dynamic_ipxe_options` method in deploy interface and\n`get_pxe_options` method in boot interface.\nThis patch adds implementations to AgentDeploy and PXEBoot respectively.\n\nNew iPXE boot script `dynamic_boot.ipxe` is added and needs to be specified\nin ironic.conf as `[pxe]ipxe_boot_script` for this feature to work.\n\nBoot script API:\n\nRequest: GET v1/ipxe?mac=<mac>&node_uuid=<node-uuid>\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\nBoth `mac` and `node_uuid` are optional but at least one of them must be\nprovided, with node_uuid taking precedence.\nUsing the `?mac=` form is mostly suitable for standalone Ironic\nwhere external DHCP server can be pre-configured to generate\nappropriate PXE opts for DHCP response using the MAC of requester.\n\nBoot config:\n\nRequest: GET v1/ipxe/<node-uuid>\n\nDescription:\ngets boot config over RPC from conductor which manages given node\nand then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 7, 'created': '2016-10-21 15:57:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e922f30088bf240bff106fca38f57c43a39e3561', 'message': ""PoC Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped to 1.23, this endpoint does not\ncheck the requested API version and relies solely on config option\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option to this regard `CONF.pxe.ipxe_server_enabled`\n(default is False).\n\nThis new API also honors 'CONF.api.restrict_lookup' config option\nand when that is enabled returns configs only if the node is in one of\nappropriate states.\n\nRPC version is bumped due to new `get_ipxe_config` method.\n\nTo support this feature drivers are expected to implement\n`get_ipxe_options` methods in deploy and boot interfaces.\nThis patch adds implementations to AgentDeploy and PXEBoot respectively.\n\nNew iPXE boot script `dynamic_boot.ipxe` is added and needs to be specified\nin ironic.conf as `[pxe]ipxe_boot_script` for this feature to work.\n\nBoot script API:\n\nRequest: GET v1/ipxe?mac=<mac>&node_uuid=<node-uuid>\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\nBoth `mac` and `node_uuid` are optional but at least one of them must be\nprovided, with node_uuid taking precedence.\nUsing the `?mac=` form is mostly suitable for standalone Ironic\nwhere external DHCP server can be pre-configured to generate\nappropriate PXE opts for DHCP response using the MAC of requester.\n\nBoot config:\n\nRequest: GET v1/ipxe/<node-uuid>\n\nDescription:\ngets template file name and options to render it with over RPC\nfrom conductor which manages given node, renders the template on API side\nand then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 8, 'created': '2016-10-31 17:17:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e66b6f53a3b747c32599b55ef087fa5ba2a3e2ec', 'message': ""PoC Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped to 1.23, this endpoint does not\ncheck the requested API version and relies solely on config option\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option to this regard `CONF.pxe.ipxe_server_enabled`\n(default is False). Ideally this option should be deprecated from the\nstart as it is meant only for transition period, and dynamic iPXE should\nbe the default and only behavior when CONF.pxe.ipxe_enabled is True.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\nNote that currently switching pxe config is not suported yet (thus\nnetboot is neither).\n\nRPC version is bumped due to new `get_ipxe_config` method.\n\nTo support this feature drivers are expected to implement\n`get_ipxe_options` methods in deploy and boot interfaces.\nThis patch adds implementations to AgentDeploy and PXEBoot respectively.\n\nThe default iPXE boot script is converted to Jinja2 template.\nThis should not affect those using custom boot scripts (like bifrost)\nunless they enable set CONF.pxe.ipxe_server_enabled to True.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets template file name and options to render it with over RPC\nfrom conductor which manages this node,\nrenders the template on API side and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 9, 'created': '2016-10-31 18:44:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cb8896308e772748c2dfb4336ea47a0c6c8e9f4e', 'message': ""PoC Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option to this regard `CONF.pxe.ipxe_server_enabled`\n(default is False).\nThis option is set to deprecated from the start as it is meant only\nfor transition period, and ideally dynamic iPXE should be the default\nand only behavior when CONF.pxe.ipxe_enabled is True.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\nNote that currently switching pxe config is not implemented yet\n(thus netboot is currently actually not supported).\n\nRPC version is bumped due to new `get_ipxe_config` method.\n\nTo support this feature drivers are expected to implement\n`get_ipxe_options` methods in deploy and boot interfaces.\nThis patch adds implementations to AgentDeploy and PXEBoot respectively.\n\nThe default iPXE boot script is converted to Jinja2 template.\nThis should not affect those using custom boot scripts (like bifrost)\nunless they actually set CONF.pxe.ipxe_server_enabled to True.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets template file name and options to render it with over RPC\nfrom conductor which manages this node,\nrenders the template on API side and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 10, 'created': '2016-11-03 19:32:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5ca9fcc168155de1333dd4977c20f4752107aa4a', 'message': ""WiP Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option to this regard `CONF.pxe.ipxe_server_enabled`\n(default is False).\nThis option is set to deprecated from the start as it is meant only\nfor transition period, and ideally dynamic iPXE should be the default\nand only behavior when CONF.pxe.ipxe_enabled is True.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\nNote that currently switching pxe config is not implemented yet\n(thus netboot is currently actually not supported).\n\nRPC version is bumped due to new `get_ipxe_config` method.\n\nTo support this feature drivers are expected to implement\n`get_ipxe_options` methods, which are defined in new ABC classes\nDeployIPXEMixin for deploy interface and\nBootIPXEMixin for boot interface.\nThis patch makes AgentDeployMixin and PXEBoot depend on those\nrespectively.\n\nThe default iPXE boot script is converted to Jinja2 template.\nThis should not affect those using custom boot scripts (like bifrost)\nunless they actually set CONF.pxe.ipxe_server_enabled to True.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets template file name and options to render it with over RPC\nfrom conductor which manages this node,\nrenders the template on API side and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 11, 'created': '2016-11-07 12:16:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3e77a401dca37bbcbe2776fa52cb39d2992f2fbc', 'message': ""Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets template file name and options to render it with over RPC\nfrom conductor which manages this node,\nrenders the template on API side and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 12, 'created': '2016-11-07 13:56:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/66a7fad0c7f14026a50d09d02f3edf0af80a17d9', 'message': ""Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets template file name and options to render it with over RPC\nfrom conductor which manages this node,\nrenders the template on API side and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 13, 'created': '2016-11-08 14:35:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/08ad36e0fe7743546d7f991c33f6043010ede17e', 'message': ""Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets template file name and options to render it with over RPC\nfrom conductor which manages this node,\nrenders the template on API side and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 14, 'created': '2016-11-08 17:35:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/aa71060bd2abfed71402f13884828bbcd5928b40', 'message': ""Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets template file name and options to render it with over RPC\nfrom conductor which manages this node,\nrenders the template on API side and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 15, 'created': '2016-11-16 20:01:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1d6b8ed67ec7c2c76f229a9f135eac69f020b436', 'message': ""Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config with over RPC from the conductor\nwhich manages this node and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 16, 'created': '2016-11-17 13:24:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/48a5bc2e6d7733836381c7cb78df75f57c51751d', 'message': ""Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config with over RPC from the conductor\nwhich manages this node and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 17, 'created': '2016-11-21 17:00:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/60ee52c598de5afbd9d99e028f57b51b0b467a64', 'message': ""Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config with over RPC from the conductor\nwhich manages this node and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 18, 'created': '2016-11-24 18:36:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/635477f63e1cab8a68160ed829ae9af588a18614', 'message': ""Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 19, 'created': '2016-11-24 20:13:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d1f0212b7d1675f53c66450cfbe607e73f48f4f1', 'message': ""Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 20, 'created': '2016-11-25 09:20:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/da9bce50c1f29e324e176a81e627819fd4e62147', 'message': ""Serve iPXE configs from Ironic API\n\nAdds new endpoint `v1/ipxe` to serve iPXE boot scripts and\nconfigs directly from Ironic API.\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_ipxe` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/ipxe\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor iPXE boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/ipxe?mac=<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 21, 'created': '2016-11-28 19:18:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/99714c0581a9850fbe4b76f22975abac6aa906c1', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 22, 'created': '2016-12-02 15:20:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b567ce983b0afc5e863194015af19d18157bd6df', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 23, 'created': '2016-12-02 18:04:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/25f0c769a7495ecd8ad3f6e945d0733b789f3be2', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 24, 'created': '2016-12-02 18:48:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/46c43e30a89f6d5e02fe63165c780c65c248c04c', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 25, 'created': '2016-12-05 09:41:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/52dea0c0dcd5e1ad64cfc0d2aca7fba289c3979c', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 26, 'created': '2016-12-06 09:20:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9031c8f0d59227f1245c5c4972ba21d0a7ae6fac', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 27, 'created': '2016-12-19 19:44:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/992fe1fd2aa9cf89a0283c4de616380bd4532472', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 28, 'created': '2017-01-10 17:36:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1ae47210266728f62b6ccf8f63a1180cb5118408', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 29, 'created': '2017-01-13 10:13:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/db25ccdf2f0a036b425f288100db6e2813a00c9d', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 30, 'created': '2017-01-23 08:44:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/dc19b85d8e71bded5d64625ee2ca081f1fb3fd4b', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 31, 'created': '2017-02-07 17:14:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/627d6ec77b5251a091ccae481ed3ef2d8e978533', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 32, 'created': '2017-02-14 11:25:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/07c582fedfcbdeef09c6c12b9f1dce7cb705d120', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 33, 'created': '2017-03-06 18:05:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6fbfcb77954098f3328f994d26cf2984972a73a6', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 34, 'created': '2017-03-07 14:05:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/80f71ea5a78eb54c1bc52e522bf0cfe08384b9fa', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 35, 'created': '2017-03-09 08:03:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/17ccd51c1ac4b1c5853c88dcfa6402dd72ebcc33', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 36, 'created': '2017-03-09 14:15:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/60f0a351bd7787dc38e67d1f0aa21a1cc262a970', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 37, 'created': '2017-03-16 07:29:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f101e1771bbd7e6d0d4f7d07cf81efbf09f391d4', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 38, 'created': '2017-04-06 07:34:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f113c0e6bf4e44349f91bf6802b6eae4bc128139', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 39, 'created': '2017-04-07 07:05:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2ef81a46dab5158f25d08269a4e46fb00c30fc29', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 40, 'created': '2017-04-07 10:11:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cc2c254aaa4e6c683be4191b02a5abaadf6f2674', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 41, 'created': '2017-04-12 09:52:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/77b7b49047bac21530f6bae864429814b255cb64', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 42, 'created': '2017-05-16 05:52:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a5fa344bca9a173fe3bd7dd7c4877d496728abc7', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 43, 'created': '2017-05-16 15:56:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/404eff492deb8433cf8bdfc589f270d88ea6e09c', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 44, 'created': '2017-05-17 09:40:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2e99bc799417dfda44a0ab250859f6ae0ac903e2', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 45, 'created': '2017-05-22 07:59:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/526781104c40b73c4d70de2eebec4ff21817890d', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 46, 'created': '2017-06-09 07:56:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/db738c0d00c6c50649fb76b2b8907cb9e4f13481', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 47, 'created': '2017-06-29 10:17:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7f68ac1313c4797ac5e125b96cd25d5f4bb2362e', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 48, 'created': '2017-06-30 13:55:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/11698dabbff9d06cb1b9c8f66ac61ebd8dbb9997', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 49, 'created': '2017-07-04 07:50:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/52f5919cfaf540f633cf3d950777f461a251eb20', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 50, 'created': '2017-07-04 11:43:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0917b5548379df4ec612b7abcf92945d7efbf42a', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 51, 'created': '2017-07-05 09:03:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2c8660376a5bde59e5c5b7dbc9539d73454ed080', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 52, 'created': '2017-07-10 11:49:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0677fabcd49bc05b8a8ee4e39643b2278c6afb0f', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 53, 'created': '2017-08-01 15:38:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/62ee8a859eb81d6d4683fee0de59ea03bba0138b', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 54, 'created': '2017-08-03 13:23:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5852e5a65581cf5fd8415ef4d9d689102fa90677', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 55, 'created': '2017-08-21 13:38:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a5ea721f4c8c24f75d2da5e27ec8a6e095bf05c0', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}, {'number': 56, 'created': '2017-08-29 10:31:43.000000000', 'files': ['ironic/conf/pxe.py', 'releasenotes/notes/dynamic-ipxe-895182d052c3faeb.yaml', 'ironic/tests/unit/api/controllers/v1/test_ramdisk.py', 'ironic/api/controllers/v1/__init__.py', 'ironic/api/controllers/v1/ramdisk.py', 'ironic/api/config.py', 'ironic/conf/api.py', 'doc/source/contributor/webapi-version-history.rst', 'ironic/api/controllers/v1/types.py', 'devstack/lib/ironic', 'etc/ironic/ironic.conf.sample', 'ironic/api/controllers/v1/versions.py', 'ironic/common/policy.py', 'ironic/tests/unit/api/controllers/v1/test_types.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/b6bb81b6c965181825bcb388349eb58c0929f322', 'message': ""Serve boot configs from Ironic API\n\nAdds new endpoint `v1/boot_config` to serve boot scripts and\nconfigs directly from Ironic API (currently supported are iPXE boot\nscript).\n\nThe endpoint is public and requires no authentication.\nNote that although API version is bumped, this endpoint does not\ncheck the requested API version and relies solely on config options\nenabling it, as iPXE firmware can not insert custom HTTP request headers.\n\nNew config option `CONF.api.restrict_boot_config` which is similar to\n`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if\nnode is using 'netboot'.\n\nBoot script API:\n\nRequest: GET v1/boot_config\n\nDescription:\nresolves Ironic API and renders Jinja template\nfor (iPXE) boot script directly in API w/o RPC calls to conductor.\n\nBoot config:\n\nRequest: GET v1/boot_config/<MAC>\n\nDescription:\nresolves the node by provided MAC address,\ngets complete rendered boot config over RPC from the conductor\nwhich manages this node and then serves it from API.\nFor compatibility reasons supports both dash-separated and\ncolon-separated MAC address forms.\n\nChange-Id: Ic908b276a0719a2e23bb8634011203932a33349d\nPartial-Bug: #1526275\n""}]",41,369438,b6bb81b6c965181825bcb388349eb58c0929f322,359,18,56,9542,,,0,"Serve boot configs from Ironic API

Adds new endpoint `v1/boot_config` to serve boot scripts and
configs directly from Ironic API (currently supported are iPXE boot
script).

The endpoint is public and requires no authentication.
Note that although API version is bumped, this endpoint does not
check the requested API version and relies solely on config options
enabling it, as iPXE firmware can not insert custom HTTP request headers.

New config option `CONF.api.restrict_boot_config` which is similar to
`CONF.api.restrict_lookup` but also allows nodes in ACTIVE state if
node is using 'netboot'.

Boot script API:

Request: GET v1/boot_config

Description:
resolves Ironic API and renders Jinja template
for (iPXE) boot script directly in API w/o RPC calls to conductor.

Boot config:

Request: GET v1/boot_config/<MAC>

Description:
resolves the node by provided MAC address,
gets complete rendered boot config over RPC from the conductor
which manages this node and then serves it from API.
For compatibility reasons supports both dash-separated and
colon-separated MAC address forms.

Change-Id: Ic908b276a0719a2e23bb8634011203932a33349d
Partial-Bug: #1526275
",git fetch https://review.opendev.org/openstack/ironic refs/changes/38/369438/19 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/api/controllers/v1/__init__.py', 'ironic/api/controllers/v1/versions.py', 'ironic/api/controllers/v1/ramdisk.py', 'ironic/api/config.py', 'ironic/common/policy.py', 'ironic/conf/api.py']",6,15a23ed5862ac3200fe0976675f385fe3237ea03,bug/1526275," cfg.BoolOpt('serve_ipxe_config', default=False, help=_('Enable serving iPXE configs from Ionic API.')),",,160,1
openstack%2Fswift~master~Ifa2e563a7cd784483e17bdfea6a84d7c9769119c,openstack/swift,master,Ifa2e563a7cd784483e17bdfea6a84d7c9769119c,Swift Automated Tiering,NEW,2016-03-02 09:29:55.000000000,2017-12-18 02:20:41.000000000,,"[{'_account_id': 1179}, {'_account_id': 4608}, {'_account_id': 9625}, {'_account_id': 12050}, {'_account_id': 13052}, {'_account_id': 13852}, {'_account_id': 14766}, {'_account_id': 17156}, {'_account_id': 17901}, {'_account_id': 23825}]","[{'number': 1, 'created': '2016-03-02 09:29:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e8ee56b52de6e7d1c863e75ee01048d70e3e7a8d', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 2, 'created': '2016-03-03 09:39:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/47895862f0df6800c7cd74eb4a06fb08f5818c6e', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 3, 'created': '2016-03-08 04:21:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/bc23de554601b320f44dbfa3d944ee8dcb15047b', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 4, 'created': '2016-03-08 06:47:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/b5e49b680bee3a98f68ed4812d067f23f99e8685', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 5, 'created': '2016-03-08 07:40:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6812f28ce71c674ded986692ff9bbe0713f52687', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 6, 'created': '2016-03-11 06:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0b597fb517768f363020d5aac0117e1182a39c05', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 7, 'created': '2016-03-11 07:11:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e754dab1438289125df7f60c63e9f086aed8a9b9', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 8, 'created': '2016-03-15 04:10:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4c462774d0dd7ade258a0afa2af608e4bd6eaaf5', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 9, 'created': '2016-03-15 04:27:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6a191ffd07eb29d6b61c63d873ff7ac991d2da57', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 10, 'created': '2016-03-16 09:13:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e116f364db555cb8b46fe46156e989143fd5eb28', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 11, 'created': '2016-03-22 05:58:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/83679419da8551619228c5336ed68283d097a9ae', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 12, 'created': '2016-03-22 10:44:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/871c7d9fd2534339c5b265a8770a95866c9445d7', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 13, 'created': '2016-03-23 04:38:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f731d32d25edd31db3f5b037210a37b431d7b2ea', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 14, 'created': '2016-03-23 06:40:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/9cb554a6b86fe950e3c9f16de3d9263ac2da155c', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 15, 'created': '2016-03-23 10:25:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/9b77f06a2f412581b155d1da87ee9ac748806fe5', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 16, 'created': '2016-03-24 06:36:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8679073c83a98af9e564ae54704c5d37ed700228', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 17, 'created': '2016-03-30 01:51:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2d3e904c548f338790dab540016f5cf10998caf1', 'message': 'WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\nSeemingly, containers have variable storage policy objects in tiered Swift.\n\nOverview slide:\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n'}, {'number': 18, 'created': '2016-04-26 14:51:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ecbc39b32d6ff75e503d8e2806395e0750939c4e', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 19, 'created': '2016-04-27 15:42:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/46a7f3c2bb180089143b47f5f3b0cbaab53b693a', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 20, 'created': '2016-04-27 21:39:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ec656e33b5ccea76277b1205f5ea52db2b74ba6c', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 21, 'created': '2016-04-28 13:54:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/60f513f132771f81d7fc87d2e44e0eb4da71251e', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 22, 'created': '2016-04-28 21:25:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/b4f6d2833c5acd59231dedc03b92b1c1c493e1c7', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 23, 'created': '2016-04-28 21:31:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ad10969e84b3c028056947de924906960c89ba46', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 24, 'created': '2016-05-17 07:56:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7a411bf23c5b61ffb677c3b77f59bf1a6578af30', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 25, 'created': '2016-05-18 06:54:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7630e71efbe10f8f6a7afafa02db24ba62a30cdd', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 26, 'created': '2016-05-23 09:37:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/eebf443179ac61c7b84d23a2623f2153ffc3dc6b', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 27, 'created': '2016-05-23 10:45:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/799228e3f1e2e5ac6305ef76b60a76f48a720303', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 28, 'created': '2016-05-24 08:17:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/90c35fb2b5e4cd9c0ea9339ca6e7fafdd998361c', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 29, 'created': '2016-05-24 10:58:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/544dc82829362ba433fca26cd54ce43464d54fde', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 30, 'created': '2016-05-26 02:46:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ebb636daa32076997602f8f3a48efab625feee3e', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 31, 'created': '2016-06-21 11:15:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ff94b1b6febfe27e4ffed6e7cdd80cce37dc3dd3', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 32, 'created': '2016-06-22 02:36:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/16888fae3945cdf8a75ebb945a4c594320498950', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 33, 'created': '2016-07-04 12:43:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f38b5c828945003d0912feadbaa68f177142aefa', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 34, 'created': '2016-07-25 10:06:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/13c48e1a7fd6692bc9de5c8ab6adeacd1b28c23c', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 35, 'created': '2016-08-18 10:45:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0265cd9ed198043da24c1a93b9ab3cc5aa7fc54b', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 36, 'created': '2016-08-19 02:16:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ecd7a74048cb9bc221bd66aefa3031d631c90475', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 37, 'created': '2016-10-06 10:40:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/84de3272c5bc2d8806674c604003665a5160350c', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 38, 'created': '2016-10-20 07:52:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/bd0902098be3283a144f2eb096553774f9639694', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 39, 'created': '2016-10-20 09:11:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/06b5b0133172b03e70cc9e6c16040fbc27be4d14', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 40, 'created': '2016-10-21 02:26:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4ebabee30e0f41faf3caba14cd30132993e775e9', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 41, 'created': '2016-10-21 07:43:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/408d067023ed221962654b24d2bd7ce2ed43529d', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 42, 'created': '2016-10-21 09:55:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8c438f1e81f5e42eb4ae4fd944cc2f2d53786ea0', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 43, 'created': '2016-10-26 22:34:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/67b197165ee62c06367a49fa4e807e6c9b30af9c', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 44, 'created': '2016-10-27 19:32:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/d842f745b42d10905eb57b94834c7b17e2711d68', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 45, 'created': '2016-10-27 19:33:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4952c896012c8d5b2ba4fc5c2e5112ba9e6de16c', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 46, 'created': '2016-10-27 20:10:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/bc8bc67aa56515ec4e2e7e0dd6c5b6fb354b1bc8', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 47, 'created': '2016-10-27 20:11:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/3ea1d658c093cffc64a1c87b47df2a92c98540a3', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 48, 'created': '2016-10-27 20:22:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2d2e868ac5ab3f32ac52a1953967e61f396dfc88', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 49, 'created': '2016-10-28 07:08:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e5585989406baaba9c8d6bde1a3a353d503b0eca', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 50, 'created': '2016-10-28 07:15:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6b6bb216e937fb481fc800ec77d614dfa38edbbc', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 51, 'created': '2017-01-05 12:36:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/72a88a907e5e72dec44747cdd413e6a1a0f95247', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 52, 'created': '2017-01-25 09:49:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/64af404478c25c625ca4fcbcfe2dddb118c1b326', 'message': ""WIP: Swift Automated Tiering\n\nImplementaion for automated-tiering.\n\nIn this implementation, there are following two changes.\n  1. Objects move among some different storage policy containers automatically.\n  2. Swift ensures transparent access to the automatically moved objects.\n\nThis Automated-Tiering implemementation has two main daemons,\ndeliverer and addresser. Deliverer moves and links objecs among tiers.\nAddresser determines tiers whitch objects are moved to by deliverer.\n\nWe can control addresser's behavior with config in object-server.conf and\nimplementation of AddressingRule subclass. ElpsedTimeAddressingRule is\nAddressingRule subclass sample.\n\nOverview slide (deliverer):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-swift-hackathon-bristol\nOverview slide (addresser):\nhttp://www.slideshare.net/miyaharakazuhiro/swift-automated-tiering-openstacksummit-austin\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 53, 'created': '2017-02-06 07:42:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4f9bf1870a733746944d2249483e4813705fe1f6', 'message': ""Swift Automated Tiering\n\nAdd an optional automated tiering storage policy support to Swift.\nObjects in an automated tiering storage policy container will be moved among\nthe automated tiering storage policy container and other containers with other\nstorage policies automatically. And end-users can access the moved objects with\nthe original object paths transparently.\n\nSwift determines which container objects in automated-tiering storage policies\ncontainer should be moved to by AddressingRule class. AddressingRule determines\nthe objects' destination container by metadata of each objects. Third party can\nimplement their own AddressingRule classes and deploy them. In this patch,\nthere are some sample implementations of AddressingRule class. One of the\nsample implementations is ElapsedTimeAddressingRule. With this class, Swift\nwill determine destination containers by elapsed time from objects are created.\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 54, 'created': '2017-02-06 10:08:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4a60f0bd62be114f242956f6e1cff2d8159e5d55', 'message': ""Swift Automated Tiering\n\nAdd an optional automated tiering storage policy support to Swift.\nObjects in an automated tiering storage policy container will be moved among\nthe automated tiering storage policy container and other containers with other\nstorage policies automatically. And end-users can access the moved objects with\nthe original object paths transparently.\n\nSwift determines which container objects in automated-tiering storage policies\ncontainer should be moved to by AddressingRule class. AddressingRule determines\nthe objects' destination container by metadata of each objects. Third party can\nimplement their own AddressingRule classes and deploy them. In this patch,\nthere are some sample implementations of AddressingRule class. One of the\nsample implementations is ElapsedTimeAddressingRule. With this class, Swift\nwill determine destination containers by elapsed time from objects are created.\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 55, 'created': '2017-03-30 01:37:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/be232b503e5eaf11b2e47b13a4908c6ad75b2d4c', 'message': ""Swift Automated Tiering\n\nAdd an optional automated tiering storage policy support to Swift.\nObjects in an automated tiering storage policy container will be moved among\nthe automated tiering storage policy container and other containers with other\nstorage policies automatically. And end-users can access the moved objects with\nthe original object paths transparently.\n\nSwift determines which container objects in automated-tiering storage policies\ncontainer should be moved to by AddressingRule class. AddressingRule determines\nthe objects' destination container by metadata of each objects. Third party can\nimplement their own AddressingRule classes and deploy them. In this patch,\nthere are some sample implementations of AddressingRule class. One of the\nsample implementations is ElapsedTimeAddressingRule. With this class, Swift\nwill determine destination containers by elapsed time from objects are created.\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}, {'number': 56, 'created': '2017-09-11 16:56:56.000000000', 'files': ['test/probe/common.py', 'bin/swift-object-addresser', 'test/unit/obj/test_addresser.py', 'etc/object-deliverer.conf-sample', 'doc/source/middleware.rst', 'swift/obj/deliverer.py', 'test/probe/test_automated_tiering.py', 'etc/object-server.conf-sample', 'swift/common/middleware/automated_tiering.py', 'doc/source/index.rst', 'test/unit/obj/test_deliverer.py', 'doc/source/overview_encryption.rst', 'etc/container-reconciler.conf-sample', 'etc/proxy-server.conf-sample', 'bin/swift-object-deliverer', 'swift/common/manager.py', 'doc/source/overview_tiering.rst', 'swift/obj/addresser.py', 'setup.cfg', 'swift/obj/diskfile.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/7b1c50d9a2dd4854104285e776034c9e0e19834f', 'message': ""Swift Automated Tiering\n\nAdd an optional automated tiering storage policy support to Swift.\nObjects in an automated tiering storage policy container will be moved among\nthe automated tiering storage policy container and other containers with other\nstorage policies automatically. And end-users can access the moved objects with\nthe original object paths transparently.\n\nSwift determines which container objects in automated-tiering storage policies\ncontainer should be moved to by AddressingRule class. AddressingRule determines\nthe objects' destination container by metadata of each objects. Third party can\nimplement their own AddressingRule classes and deploy them. In this patch,\nthere are some sample implementations of AddressingRule class. One of the\nsample implementations is ElapsedTimeAddressingRule. With this class, Swift\nwill determine destination containers by elapsed time from objects are created.\n\nChange-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c\n""}]",29,287057,7b1c50d9a2dd4854104285e776034c9e0e19834f,176,10,56,14766,,,0,"Swift Automated Tiering

Add an optional automated tiering storage policy support to Swift.
Objects in an automated tiering storage policy container will be moved among
the automated tiering storage policy container and other containers with other
storage policies automatically. And end-users can access the moved objects with
the original object paths transparently.

Swift determines which container objects in automated-tiering storage policies
container should be moved to by AddressingRule class. AddressingRule determines
the objects' destination container by metadata of each objects. Third party can
implement their own AddressingRule classes and deploy them. In this patch,
there are some sample implementations of AddressingRule class. One of the
sample implementations is ElapsedTimeAddressingRule. With this class, Swift
will determine destination containers by elapsed time from objects are created.

Change-Id: Ifa2e563a7cd784483e17bdfea6a84d7c9769119c
",git fetch https://review.opendev.org/openstack/swift refs/changes/57/287057/8 && git format-patch -1 --stdout FETCH_HEAD,"['etc/object-deliverer.conf-sample', 'swift/common/middleware/delivery_address.py', 'bin/swift-object-deliverer', 'swift/common/manager.py', 'setup.cfg', 'swift/obj/deliverer.py']",6,e8ee56b52de6e7d1c863e75ee01048d70e3e7a8d,automated-tiering,"import time import json from StringIO import StringIO from collections import defaultdict from eventlet import Timeout from swift import gettext_ as _ from swift.common.bufferedhttp import http_connect from swift.common.daemon import Daemon from swift.common.exceptions import ConnectionTimeout from swift.common.http import is_success from swift.common.storage_policy import POLICIES from swift.common.utils import (Timestamp, split_path, get_logger, PRECISION, FileLikeIter, whataremyips, last_modified_date_to_timestamp) from swift.common.ring import Ring from swift.common.ring.utils import is_local_device from swift.common.internal_client import InternalClient, UnexpectedResponse from swift.common.exceptions import SwiftException DELIVERY_TASKS_ACCOUNT = '.delivery_tasks' DELIVERY_TARGET_ACCOUNT = '.delivery_target' DELIVERY_LINK_META = 'x-object-sysmeta-delivery-link-policy-index' DELIVERY_ADDRESS_META = 'x-object-delivery-address-policy' DERIVATIVE_CONTAINER_META_PREFIX =\ 'x-container-sysmeta-derivative-container-name-' DELIVERY_TASK_CONTAINER_DIVISOR = 3600 # 1 hour def get_deliverer_container_name(obj_timestamp): return str(int(Timestamp(obj_timestamp)) // DELIVERY_TASK_CONTAINER_DIVISOR * DELIVERY_TASK_CONTAINER_DIVISOR) def get_deliverer_obj_name(policy_index, acc, con, obj): return ""%(policy_index)d:/%(acc)s/%(con)s/%(obj)s"" % { 'policy_index': policy_index, 'acc': acc, 'con': con, 'obj': obj} class ObjectDeliverer(Daemon): """""" Daemon that moves objects with x-delivery-address-storage-policy to a container with the addres storage policy. After moving a object, this daemon updates symlink which refers to the object in order to access the object with original path. Temporarily, this daemon use SLO instead of symlink. :param """""" def __init__(self, conf, logger=None, swift=None): self.deliverer_containers = {} self.deliverer_cleanups = {} self.conf = conf self.interval = int(conf.get('interval') or 300) self.logger = logger or get_logger(conf, log_route='object-deliverer') conf_path = conf.get('__file__') or '/etc/swift/object-deliverer.conf' request_tries = int(conf.get('request_tries') or 3) self.swift = swift or InternalClient( conf_path, 'Swift Object Deliverer', request_tries) self.stats = defaultdict(int) self.port = int(conf.get('bind_port', 6000)) self.swift_dir = conf.get('swift_dir', '/etc/swift') self.container_ring = None self.conn_timeout = float(conf.get('conn_timeout', 0.5)) self.node_timeout = float(conf.get('node_timeout', 10)) self.ips = whataremyips() def execute_delivery_task(self, delivery_task): self.logger.info('Start processing %s' % delivery_task) _, resp_headers, content_iter =\ self.swift.get_object( delivery_task.src_account, delivery_task.src_container, delivery_task.src_obj, {}) self.upload_object_to_destination_and_update_link( delivery_task, content_iter, resp_headers) delete_unnecessary_object =\ delivery_task.get_delete_unnecessary_object_process() delete_unnecessary_object(self.swift) self.logger.info('End processing %s' % delivery_task) def assigned_task_check(self, delivery_task): nodes = delivery_task.get_original_primary_nodes return any([is_local_device(self.ips, self.port, node[""ip""], node[""port""]) for node in nodes]) def upload_object_to_destination_and_update_link( self, delivery_task, content_iter, headers): self.swift.create_container( delivery_task.dst_account, delivery_task.dst_container, headers = {'X-Storage-Policy': delivery_task.dst_policy.name}) self.swift.set_container_metadata( delivery_task.target_account, delivery_task.target_container, metadata =\ { DERIVATIVE_CONTAINER_META_PREFIX + delivery_task.dst_policy.name : delivery_task.dst_container }) headers.update( {'X-Timestamp': delivery_task.delivery_timestamp.internal}) self.swift.upload_object( FileLikeIter(content_iter), delivery_task.dst_account, delivery_task.dst_container, delivery_task.dst_obj, headers) update_link = delivery_task.get_update_link_process() update_link(self.swift, headers) def iter_delivery_tasks(self): for con_info in self.swift.iter_containers(DELIVERY_TASKS_ACCOUNT): for obj_info in self.swift.iter_objects(DELIVERY_TASKS_ACCOUNT, con_info['name']): try: self.logger.info( 'start generating %s to delivery task.' % obj_info['name']) delivery_task =\ self.generate_delivery_task( con_info['name'], obj_info['name'], last_modified_date_to_timestamp( obj_info['last_modified'])) self.logger.info( 'end generating %s to delivery task.' % obj_info['name']) except: self.logger.exception( 'Unhandled Exception trying to generate delivery task ' + 'from %s.' % obj_info['name']) continue else: yield delivery_task def deliver(self): for delivery_task in self.iter_delivery_tasks(): if self.assigned_task_check(delivery_task): try: try: self.execute_delivery_task(delivery_task) except UnnecessaryTaskException: # the delivery task object should be deleted. pass except TaskNotDoneError: pass else: headers = { 'user-agent': 'object-deliverer', } self.swift.delete_object( DELIVERY_TASKS_ACCOUNT, delivery_task.task_container, delivery_task.task_obj, headers=headers) self.logger.info('%s is dequeued from %s/%s container' % (delivery_task, DELIVERY_TASKS_ACCOUNT, delivery_task.task_container)) def run_once(self, *args, **kwargs): try: self.deliver() except: self.logger.exception('Unhandled Exception trying to deliver') def run_forever(self, *args, **kwargs): while True: self.run_once(*args, **kwargs) self.logger.info('sleeping between intervals (%ss)', self.interval) time.sleep(self.interval) def generate_delivery_task( self, task_container_name, task_obj_name, task_created_at): dst_policy_index_str, target_obj_path = task_obj_name.split(':', 1) dst_policy = POLICIES.get_by_index(int(dst_policy_index_str)) target_account, target_container, target_obj =\ split_path(target_obj_path, 3, 3, rest_with_last=True) try: target_container_meta =\ self.swift.get_container_metadata( target_account, target_container) target_obj_meta =\ self.swift.get_object_metadata( target_account, target_container, target_obj) except UnexpectedResponse as err: if '404 Not Found' in err.resp.status: raise UnnecessaryTaskException() else: raise # TODO add error handling target_obj_created_at =\ Timestamp(target_obj_meta['x-backend-timestamp']) delivery_timestamp =\ DeliveryTimestamp(target_obj_created_at, task_created_at) original_policy =\ POLICIES.get_by_index( int(target_container_meta['x-backend-storage-policy-index'])) target_is_link = DELIVERY_LINK_META in target_obj_meta if target_is_link: src_storage_policy_index = int(target_obj_meta[DELIVERY_LINK_META]) src_policy = POLICIES.get_by_index(src_storage_policy_index) else: src_storage_policy_index\ = int(target_container_meta['x-backend-storage-policy-index']) src_policy = POLICIES.get_by_index(src_storage_policy_index) if original_policy is None: self.logger.warning( 'original Policy of The Delivery Task is not found') raise InvalidPolicy() if src_policy is None: self.logger.warning( 'Source Policy of The Delivery Task is not found') raise InvalidPolicy() if dst_policy is None: self.logger.warning( 'Destination Policy of The Delivery Task is not found') raise InvalidPolicy() if src_policy == dst_policy: raise UnnecessaryTaskException() if src_policy is not original_policy: src_container =\ target_container_meta[ DERIVATIVE_CONTAINER_META_PREFIX + src_policy.name] else: src_container = target_container if dst_policy is not original_policy: # To enable deliverer can create contaienr, the dst_container name should be variable. #With variable dst_container name, split brain may cause 2 src_containers for a src policy, #so, src_container name should be got from link's information dst_container =\ target_container_meta.get( DERIVATIVE_CONTAINER_META_PREFIX + dst_policy.name, target_container + '_' + dst_policy.name) else: dst_container = target_container return DeliveryTask(task_container_name, task_obj_name, target_account, target_container, target_obj, src_container, dst_container, original_policy, src_policy, dst_policy, delivery_timestamp, target_is_link) class DeliveryTimestamp(Timestamp): def __init__(self, target_obj_created_at, task_created_at): task_created_at_raw =\ int(round(task_created_at.timestamp / PRECISION)) super(DeliveryTimestamp, self)\ .__init__(target_obj_created_at, task_created_at_raw) class DeliveryTask(object): def __init__(self, task_container, task_obj, target_account, target_container, target_obj, src_container, dst_container, original_policy, src_policy, dst_policy, delivery_timestamp, target_is_link): self.task_container = task_container self.task_obj= task_obj self.target_account = target_account self.target_container = target_container self.target_obj = target_obj self.src_container = src_container self.dst_container = dst_container self.original_policy = original_policy self.src_policy = src_policy self.dst_policy = dst_policy self.delivery_timestamp = delivery_timestamp self.target_is_link = target_is_link @property def get_original_primary_nodes(self): _, nodes = self.original_policy.object_ring.get_nodes( self.target_account, self.target_container, self.target_obj) return nodes @property def src_is_original(self): """""" split brain may cause the following case: original container policy index == 1 derivative container policy index == 1 Therefore we can not determine src_is_original or not by comparing the two policy indices. """""" return not self.target_is_link @property def dst_is_original(self): return self.dst_policy == self.original_policy @property def src_account(self): return self.target_account @property def src_obj(self): return self.target_obj @property def dst_account(self): return self.target_account @property def dst_obj(self): return self.target_obj def get_update_link_process(self): if self.dst_is_original: def update_link(swift, original_headers): pass else: def update_link(swift, original_headers): # if symlink is released, link object should be made of symlink. # With SLO manifest link object, we cannnot use POST / HEAD in automated-tiering. json_data = json.dumps([ {'name': '%s/%s' % (self.dst_container, self.dst_obj), 'hash': original_headers['etag'], 'bytes': original_headers['content-length']}]) headers = {DELIVERY_LINK_META: int(self.dst_policy), 'Content-Length': str(len(json_data)), 'X-Static-Large-Object': 'true', 'X-Timestamp': self.delivery_timestamp.internal} fobj = StringIO(json_data) swift.upload_object( fobj, self.target_account, self.target_container, self.target_obj, headers) return update_link def get_delete_unnecessary_object_process(self): if self.src_is_original: def delete_unnecessary_object(swift): pass else: def delete_unnecessary_object(swift): headers = {'X-Timestamp': self.delivery_timestamp.internal} swift.delete_object( self.src_account, self.src_container, self.src_obj, headers=headers) return delete_unnecessary_object def __str__(self): return 'Delivery Task on \'%s/%s/%s\' destination index : %d' %\ (self.target_account, self.target_container, self.target_obj, int(self.dst_policy)) class TaskNotDoneError(SwiftException): pass class UnnecessaryTaskException(SwiftException): pass class InvalidPolicy(SwiftException): pass ",,565,2
openstack%2Frally~master~I1cd693a10748fcc928495338f8776582262894e6,openstack/rally,master,I1cd693a10748fcc928495338f8776582262894e6,Add MonascaMetrics.list_statistics test,NEW,2016-11-16 02:34:37.000000000,2017-12-18 02:20:25.000000000,,"[{'_account_id': 9545}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 16558}, {'_account_id': 19011}]","[{'number': 1, 'created': '2016-11-16 02:34:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b3468b6c8a6efe60e25ca6e2dcdbd4f6a77a6e89', 'message': 'Add MonascaMetrics.list_statistics test\n\nChange-Id: I1cd693a10748fcc928495338f8776582262894e6\n'}, {'number': 2, 'created': '2016-11-18 00:28:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/2715a2b7ad947be81764e5304cba0305ce9b79cf', 'message': 'Add MonascaMetrics.list_statistics test\n\n* allow pass metric name in create metric. Metric name is required\nfilter for metric statistic and measurement query\n\nChange-Id: I1cd693a10748fcc928495338f8776582262894e6\n'}, {'number': 3, 'created': '2016-11-26 01:06:36.000000000', 'files': ['tests/unit/plugins/openstack/context/monasca/test_metrics.py', 'rally/plugins/openstack/scenarios/monasca/metrics.py', 'tests/unit/plugins/openstack/scenarios/monasca/test_utils.py', 'tests/unit/plugins/openstack/scenarios/monasca/test_metrics.py', 'samples/tasks/scenarios/monasca/list-statistics.yaml', 'rally/plugins/openstack/scenarios/monasca/utils.py', 'rally/plugins/openstack/context/monasca/metrics.py', 'samples/tasks/scenarios/monasca/list-statistics.json', 'rally-jobs/rally-monasca.yaml'], 'web_link': 'https://opendev.org/openstack/rally/commit/46ae06cf15945a23fb8b1974a45e31f53b112deb', 'message': 'Add MonascaMetrics.list_statistics test\n\n* allow pass metric name in create metric. Metric name is required\nfilter for metric statistic and measurement query\n\nChange-Id: I1cd693a10748fcc928495338f8776582262894e6\n'}]",18,398043,46ae06cf15945a23fb8b1974a45e31f53b112deb,34,5,3,16558,,,0,"Add MonascaMetrics.list_statistics test

* allow pass metric name in create metric. Metric name is required
filter for metric statistic and measurement query

Change-Id: I1cd693a10748fcc928495338f8776582262894e6
",git fetch https://review.opendev.org/openstack/rally refs/changes/43/398043/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/plugins/openstack/context/monasca/test_metrics.py', 'rally/plugins/openstack/scenarios/monasca/metrics.py', 'tests/unit/plugins/openstack/scenarios/monasca/test_utils.py', 'samples/tasks/scenarios/monasca/list-statistics.yaml', 'tests/unit/plugins/openstack/scenarios/monasca/test_metrics.py', 'rally/plugins/openstack/context/monasca/metrics.py', 'rally/plugins/openstack/scenarios/monasca/utils.py', 'samples/tasks/scenarios/monasca/list-statistics.json']",8,b3468b6c8a6efe60e25ca6e2dcdbd4f6a77a6e89,monasca_statistics,"{ ""MonascaMetrics.list_statistics"": [ { ""runner"": { ""type"": ""constant"", ""times"": 10, ""concurrency"": 1 }, ""context"": { ""users"": { ""tenants"": 1, ""users_per_tenant"": 1 }, ""roles"": [ ""monasca-user"" ], ""monasca_metrics"": { ""name"": ""host_alive_status"", ""dimensions"":{ ""region"": ""RegionOne"", ""service"": ""monitoring"", ""hostname"": ""fake_host"" }, ""metrics_per_tenant"": 10 } }, ""args"": { ""name"": ""host_alive_status"", ""statistics"": ""avg,min,max,sum,count"", ""dimensions"": { ""region"": ""RegionOne"", ""service"": ""monitoring"", ""hostname"": ""fake_host"" } } } ] }",,153,10
openstack%2Fswift~master~I768334dbf2fb100a3beb9e9c7947fa9a5eebd7e9,openstack/swift,master,I768334dbf2fb100a3beb9e9c7947fa9a5eebd7e9,Break less abstractions in test_auditor,NEW,2017-01-24 00:52:36.000000000,2017-12-18 02:18:35.000000000,,"[{'_account_id': 1179}, {'_account_id': 12261}, {'_account_id': 13052}]","[{'number': 1, 'created': '2017-01-24 00:52:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4f90dc68226367a7b5ef593545769e698ab77ab9', 'message': ""Break less abstractions in test_auditor\n\nIn order to get reclaimable tombstones reaped more promptly the object\nauditor started using diskfile's invalidate_hash interface.\n\nThe interface is stable, but the implementation has changed recently\n(see related change) and may still have opportunities to improve!\n\nIf we limit ourselves to only using (or mocking) the get_hashes and\ninvalidate_hashes interfaces [1] in our auditor tests - we have a much\nbetter chance of maintaining strong assertions on the behavior of the\nauditor without coupling these tests with needless additional\nmaintenance burden when changing the implementation of suffix hashing.\n\nRelated-Change-Id: I2b48238d9d684e831d9777a7b18f91a3cef57cd1\n\nChange-Id: I768334dbf2fb100a3beb9e9c7947fa9a5eebd7e9\n""}, {'number': 2, 'created': '2017-01-24 08:02:10.000000000', 'files': ['test/unit/obj/test_auditor.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/dfeb9f9bd85333900f2d264143a9273cfd31d293', 'message': ""Break less abstractions in test_auditor\n\nIn order to get reclaimable tombstones reaped more promptly the object\nauditor started using diskfile's invalidate_hash interface.\n\nThe interface is stable, but the implementation has changed recently\n(see related change) and may still have opportunities to improve!\n\nIf we limit ourselves to only using (or mocking) the get_hashes and\ninvalidate_hashes interfaces in our auditor tests - we have a much\nbetter chance of maintaining strong assertions on the behavior of the\nauditor without coupling these tests with needless additional\nmaintenance burden when changing the implementation of suffix hashing.\n\nRelated-Change-Id: I2b48238d9d684e831d9777a7b18f91a3cef57cd1\n\nChange-Id: I768334dbf2fb100a3beb9e9c7947fa9a5eebd7e9\n""}]",6,424393,dfeb9f9bd85333900f2d264143a9273cfd31d293,11,3,2,1179,,,0,"Break less abstractions in test_auditor

In order to get reclaimable tombstones reaped more promptly the object
auditor started using diskfile's invalidate_hash interface.

The interface is stable, but the implementation has changed recently
(see related change) and may still have opportunities to improve!

If we limit ourselves to only using (or mocking) the get_hashes and
invalidate_hashes interfaces in our auditor tests - we have a much
better chance of maintaining strong assertions on the behavior of the
auditor without coupling these tests with needless additional
maintenance burden when changing the implementation of suffix hashing.

Related-Change-Id: I2b48238d9d684e831d9777a7b18f91a3cef57cd1

Change-Id: I768334dbf2fb100a3beb9e9c7947fa9a5eebd7e9
",git fetch https://review.opendev.org/openstack/swift refs/changes/93/424393/1 && git format-patch -1 --stdout FETCH_HEAD,['test/unit/obj/test_auditor.py'],1,4f90dc68226367a7b5ef593545769e698ab77ab9,follow-up-auditor-tests," get_auditor_status) self._make_diskfiles() def _make_diskfiles(self): initial_hashes = self.disk_file.manager.get_hashes( # sanity check tombstone exists # .. and it in initial hashes self.assertEqual([suffix], initial_hashes.keys()) path = 'swift.obj.diskfile.DiskFileManager.%s' with mock.patch(path % 'invalidate_hash', side_effect=self.df_mgr.invalidate_hash) \ as mock_invalidate_hash: self.auditor.run_audit(mode='once', zero_byte_fps=zero_byte_fps) # sanity check - auditor should *never* remove tombstone file return initial_hashes, mock_invalidate_hash initial_hashes, mock_invalidate_hash = self._audit_tombstone( self.conf, ts_tomb) # no need to recalculate any hashes self.assertEqual(mock_invalidate_hash.call_args_list, []) # no change to hashes new_hashes = self.disk_file.manager.get_hashes( self.devices + '/sda', '0', [], self.disk_file.policy) self.assertEqual(new_hashes, initial_hashes) initial_hashes, mock_invalidate_hash = self._audit_tombstone( self.conf, ts_tomb) # call invalidate_hash ... self.assertEqual(mock_invalidate_hash.call_args_list, [mock.call(dirname(self.disk_file._datadir))]) # ... and hash is invalidated and cleared! new_hashes = self.disk_file.manager.get_hashes( self.devices + '/sda', '0', [], self.disk_file.policy) self.assertEqual(new_hashes, {}) self.conf['reclaim_age'] = 2 * 604800 self._make_diskfiles() initial_hashes, mock_invalidate_hash = self._audit_tombstone( self.conf, ts_tomb) # no need to recalculate any hashes self.assertEqual(mock_invalidate_hash.call_args_list, []) # no change to hashes new_hashes = self.disk_file.manager.get_hashes( self.devices + '/sda', '0', [], self.disk_file.policy) self.assertEqual(new_hashes, initial_hashes) self.conf['reclaim_age'] = 10 self._make_diskfiles() initial_hashes, mock_invalidate_hash = self._audit_tombstone( self.conf, ts_tomb) # call invalidate_hash ... self.assertEqual(mock_invalidate_hash.call_args_list, [mock.call(dirname(self.disk_file._datadir))]) # ... and hash is invalidated and cleared! new_hashes = self.disk_file.manager.get_hashes( self.devices + '/sda', '0', [], self.disk_file.policy) self.assertEqual(new_hashes, {}) initial_hashes, mock_invalidate_hash = self._audit_tombstone( self.conf, ts_tomb) # call invalidate_hash ... self.assertEqual(mock_invalidate_hash.call_args_list, [mock.call(dirname(self.disk_file._datadir))]) # ... and hash is invalidated and cleared! new_hashes = self.disk_file.manager.get_hashes( self.devices + '/sda', '0', [], self.disk_file.policy) self.assertEqual(new_hashes, {}) # run the auditor over the new invalid suffix path = 'swift.obj.diskfile.DiskFileManager.%s' with mock.patch.object(auditor, 'dump_recon_cache'), \ mock.patch(path % 'invalidate_hash', side_effect=self.df_mgr.invalidate_hash) \ as mock_invalidate_hash: # the auditor didn't try to invalidate the suffix self.assertEqual([], mock_invalidate_hash.call_args_list) # the next call to get hashes will hash the invalid suffix with mock.patch( path % '_hash_suffix', side_effect=self.df_mgr._hash_suffix) as mock_hash_suffix: hashes = self.disk_file.manager.get_hashes( self.devices + '/sda', '0', [], self.disk_file.policy) suffix = basename(dirname(self.disk_file._datadir)) self.assertIn(suffix, hashes) expected = [mock.call(dirname(self.disk_file._datadir))] self.assertEqual(mock_hash_suffix.call_args_list, expected) # ... run auditor again with mock.patch.object(auditor, 'dump_recon_cache'), \ mock.patch(path % 'invalidate_hash', side_effect=self.df_mgr.invalidate_hash) \ as mock_invalidate_hash: audit.run_audit(mode='once', zero_byte_fps=zero_byte_fps) # still nothing changed self.assertTrue(os.path.exists(self.disk_file._datadir)) self.assertEqual(files, os.listdir(self.disk_file._datadir)) self.assertFalse(audit.logger.get_lines_for_level('error')) self.assertFalse(audit.logger.get_lines_for_level('warning')) # the auditor still didn't try to invalidate the suffix self.assertEqual([], mock_invalidate_hash.call_args_list) # the next call to get hashes will use the cached hash for the suffix path = 'swift.obj.diskfile.DiskFileManager.%s' with mock.patch( path % '_hash_suffix', side_effect=self.df_mgr._hash_suffix) as mock_hash_suffix: hashes = self.disk_file.manager.get_hashes( self.devices + '/sda', '0', [], self.disk_file.policy) self.assertIn(suffix, hashes) self.assertEqual(mock_hash_suffix.call_args_list, [])"," get_auditor_status, HASH_FILE, HASH_INVALIDATIONS_FILE) self.disk_file.manager.get_hashes( part_dir = dirname(dirname(self.disk_file._datadir)) # sanity checks... self.assertTrue(os.path.exists(os.path.join(part_dir, HASH_FILE))) hash_invalid = os.path.join(part_dir, HASH_INVALIDATIONS_FILE) self.assertTrue(os.path.exists(hash_invalid)) with open(hash_invalid, 'rb') as fp: self.assertEqual('', fp.read().strip('\n')) self.auditor.run_audit(mode='once', zero_byte_fps=zero_byte_fps) # sanity check - auditor should not remove tombstone file return part_dir, suffix part_dir, suffix = self._audit_tombstone(self.conf, ts_tomb) self.assertTrue(os.path.exists(os.path.join(part_dir, HASH_FILE))) hash_invalid = os.path.join(part_dir, HASH_INVALIDATIONS_FILE) self.assertTrue(os.path.exists(hash_invalid)) with open(hash_invalid, 'rb') as fp: self.assertEqual('', fp.read().strip('\n')) part_dir, suffix = self._audit_tombstone(self.conf, ts_tomb) self.assertTrue(os.path.exists(os.path.join(part_dir, HASH_FILE))) hash_invalid = os.path.join(part_dir, HASH_INVALIDATIONS_FILE) self.assertTrue(os.path.exists(hash_invalid)) with open(hash_invalid, 'rb') as fp: hash_val = fp.read() self.assertEqual(suffix, hash_val.strip('\n')) conf = dict(self.conf) conf['reclaim_age'] = 2 * 604800 part_dir, suffix = self._audit_tombstone(conf, ts_tomb) self.assertTrue(os.path.exists(os.path.join(part_dir, HASH_FILE))) hash_invalid = os.path.join(part_dir, HASH_INVALIDATIONS_FILE) self.assertTrue(os.path.exists(hash_invalid)) with open(hash_invalid, 'rb') as fp: self.assertEqual('', fp.read().strip('\n')) conf = dict(self.conf) conf['reclaim_age'] = 10 part_dir, suffix = self._audit_tombstone(conf, ts_tomb) self.assertTrue(os.path.exists(os.path.join(part_dir, HASH_FILE))) hash_invalid = os.path.join(part_dir, HASH_INVALIDATIONS_FILE) self.assertTrue(os.path.exists(hash_invalid)) with open(hash_invalid, 'rb') as fp: hash_val = fp.read() self.assertEqual(suffix, hash_val.strip('\n')) part_dir, suffix = self._audit_tombstone( self.conf, ts_tomb, zero_byte_fps=50) self.assertTrue(os.path.exists(os.path.join(part_dir, HASH_FILE))) hash_invalid = os.path.join(part_dir, HASH_INVALIDATIONS_FILE) self.assertTrue(os.path.exists(hash_invalid)) with open(hash_invalid, 'rb') as fp: self.assertEqual('', fp.read().strip('\n')) # diskfile write appends to invalid hashes file part_dir = dirname(dirname(self.disk_file._datadir)) hash_invalid = os.path.join(part_dir, HASH_INVALIDATIONS_FILE) with open(hash_invalid, 'rb') as fp: self.assertEqual(basename(dirname(self.disk_file._datadir)), fp.read().strip('\n')) # sanity check # run the auditor... with mock.patch.object(auditor, 'dump_recon_cache'): audit.run_audit(mode='once', zero_byte_fps=zero_byte_fps) # the auditor doesn't touch anything on the invalidation file # (i.e. not truncate and add no entry) with open(hash_invalid, 'rb') as fp: self.assertEqual(basename(dirname(self.disk_file._datadir)), fp.read().strip('\n')) # sanity check # this get_hashes call will truncate the invalid hashes entry self.disk_file.manager.get_hashes( self.devices + '/sda', '0', [], self.disk_file.policy) with open(hash_invalid, 'rb') as fp: self.assertEqual('', fp.read().strip('\n')) # sanity check # run the auditor, again... with mock.patch.object(auditor, 'dump_recon_cache'): # and there was no hash invalidation with open(hash_invalid, 'rb') as fp: self.assertEqual('', fp.read().strip('\n'))",108,76
openstack%2Fswift~master~Idfafdc15624916ced61d73713fa89a9c7f43ced7,openstack/swift,master,Idfafdc15624916ced61d73713fa89a9c7f43ced7,"Add contains parameter to the {container, account} GET",NEW,2016-03-03 05:23:28.000000000,2017-12-18 02:18:33.000000000,,"[{'_account_id': 2622}, {'_account_id': 7233}, {'_account_id': 12071}, {'_account_id': 13052}, {'_account_id': 19590}]","[{'number': 1, 'created': '2016-03-03 05:23:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ba0e82c424b4ee266fc1b75dea780a38f3c1b6b4', 'message': ""Add contains parameter to the {container, account} GET\n\nThe horizon team are working on a new Swift interface, and in order to\nmake the interface usable when there is a large amount of objects in a\ncontainer or containers in an account adding a simple substring match\nis requested.\n\nThis patch adds a contains parameter to object and container listing:\n\n  GET /v1/a/cont?contains=<string>\n\nThis will return objects that contain the specific substring.\nFor example on a list of say ['apples', 'apply', 'please', 'putter']\nthen:\n\n  GET /v1/a/cont?contains=pl\n\nWould return:\n\n  apples\n  apply\n  please\n\nChange-Id: Idfafdc15624916ced61d73713fa89a9c7f43ced7\n""}, {'number': 2, 'created': '2016-03-03 05:29:52.000000000', 'files': ['doc/source/api/object_api_v1_overview.rst', 'swift/account/backend.py', 'test/unit/account/test_server.py', 'swift/container/server.py', 'swift/account/utils.py', 'swift/account/server.py', 'test/unit/container/test_server.py', 'swift/container/backend.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/6951f829229e0ae76b8a89571c9be1d878a0c0ae', 'message': ""Add contains parameter to the {container, account} GET\n\nThe horizon team are working on a new Swift interface, and in order to\nmake the interface usable when there is a large amount of objects in a\ncontainer or containers in an account adding a simple substring match\nis requested.\n\nThis patch adds a contains parameter to object and container listing:\n\n  GET /v1/a/cont?contains=<string>\n\nThis will return objects that contain the specific substring.\nFor example on a list of say ['apples', 'apply', 'please', 'putter']\nthen:\n\n  GET /v1/a/cont?contains=pl\n\nWould return:\n\n  apples\n  apply\n  please\n\nChange-Id: Idfafdc15624916ced61d73713fa89a9c7f43ced7\n""}]",0,287592,6951f829229e0ae76b8a89571c9be1d878a0c0ae,17,5,2,7233,,,0,"Add contains parameter to the {container, account} GET

The horizon team are working on a new Swift interface, and in order to
make the interface usable when there is a large amount of objects in a
container or containers in an account adding a simple substring match
is requested.

This patch adds a contains parameter to object and container listing:

  GET /v1/a/cont?contains=<string>

This will return objects that contain the specific substring.
For example on a list of say ['apples', 'apply', 'please', 'putter']
then:

  GET /v1/a/cont?contains=pl

Would return:

  apples
  apply
  please

Change-Id: Idfafdc15624916ced61d73713fa89a9c7f43ced7
",git fetch https://review.opendev.org/openstack/swift refs/changes/92/287592/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/api/object_api_v1_overview.rst', 'swift/account/backend.py', 'swift/container/server.py', 'test/unit/account/test_server.py', 'swift/account/utils.py', 'swift/account/server.py', 'test/unit/container/test_server.py', 'swift/container/backend.py']",8,ba0e82c424b4ee266fc1b75dea780a38f3c1b6b4,substr_search," path=None, storage_policy_index=0, reverse=False, contains=None): :param contains: match objects whose name contains the value (marker, end_marker, prefix, delimiter, path, contains) = utf8encode( marker, end_marker, prefix, delimiter, path, contains) if contains: contains = '%' + contains + '%' query += ' name LIKE ? AND' query_args.append(contains)"," path=None, storage_policy_index=0, reverse=False): (marker, end_marker, prefix, delimiter, path) = utf8encode( marker, end_marker, prefix, delimiter, path)",135,9
openstack%2Fswift~master~I1e76702e884fb863f4631e27b8d940b5045b16f6,openstack/swift,master,I1e76702e884fb863f4631e27b8d940b5045b16f6,Update hacking and enable off-by-default checks,NEW,2017-07-24 14:15:01.000000000,2017-12-18 02:17:31.000000000,,"[{'_account_id': 330}, {'_account_id': 13052}, {'_account_id': 18602}, {'_account_id': 20764}, {'_account_id': 25720}]","[{'number': 1, 'created': '2017-07-24 14:15:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7a69eaf27c29c8737a19bbf86232eb42a0d34a43', 'message': 'Update hacking and enable off-by-default checks\n\nHacking is not the part os the automatically proposed requirement,\nso we have to update hacking manually.\n\nAfter this, we can enable the hacking extension framework, and\nenable some off-by-default checks. Also, enabling them discovers\nsome lines which are affected and not covered. Those are updated.\n\nChange-Id: I1e76702e884fb863f4631e27b8d940b5045b16f6\n'}, {'number': 2, 'created': '2017-08-23 15:31:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7d8286632a0a7cfbd865cd40b0e731cfb55a3b08', 'message': 'Update hacking and enable off-by-default checks\n\nHacking is not the part os the automatically proposed requirement,\nso we have to update hacking manually.\n\nAfter this, we can enable the hacking extension framework, and\nenable some off-by-default checks. Also, enabling them discovers\nsome lines which are affected and not covered. Those are updated.\n\nChange-Id: I1e76702e884fb863f4631e27b8d940b5045b16f6\n'}, {'number': 3, 'created': '2017-08-31 15:26:23.000000000', 'files': ['test/unit/common/middleware/test_keystoneauth.py', 'test-requirements.txt', 'test/unit/obj/test_server.py', 'test/unit/common/middleware/test_ratelimit.py', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/swift/commit/0681ee1cc64e9c675572f3d5653efe0a841e7314', 'message': 'Update hacking and enable off-by-default checks\n\nHacking is not the part os the automatically proposed requirement,\nso we have to update hacking manually.\n\nAfter this, we can enable the hacking extension framework, and\nenable some off-by-default checks. Also, enabling them discovers\nsome lines which are affected and not covered. Those are updated.\n\nChange-Id: I1e76702e884fb863f4631e27b8d940b5045b16f6\n'}]",0,486627,0681ee1cc64e9c675572f3d5653efe0a841e7314,22,5,3,18602,,,0,"Update hacking and enable off-by-default checks

Hacking is not the part os the automatically proposed requirement,
so we have to update hacking manually.

After this, we can enable the hacking extension framework, and
enable some off-by-default checks. Also, enabling them discovers
some lines which are affected and not covered. Those are updated.

Change-Id: I1e76702e884fb863f4631e27b8d940b5045b16f6
",git fetch https://review.opendev.org/openstack/swift refs/changes/27/486627/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/common/middleware/test_keystoneauth.py', 'test-requirements.txt', 'test/unit/obj/test_server.py', 'test/unit/common/middleware/test_ratelimit.py', 'tox.ini']",5,7a69eaf27c29c8737a19bbf86232eb42a0d34a43,,"# H106: Don't put vim configuration in source files # H203: Use assertIs(Not)None to check for None enable-extensions = H106,H203",,22,24
openstack%2Fswift~master~Icf28dc57e589f9be20937947095800d7ce57b2f7,openstack/swift,master,Icf28dc57e589f9be20937947095800d7ce57b2f7,Expose encryption status in GET/HEAD/PUT/POST responses,NEW,2016-07-29 01:30:29.000000000,2017-12-18 02:16:59.000000000,,"[{'_account_id': 12261}, {'_account_id': 12279}, {'_account_id': 13052}, {'_account_id': 15343}]","[{'number': 1, 'created': '2016-07-29 01:30:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/43538b92283c0e883230a5c9ed866d02a991ccdf', 'message': ""Expose encryption status in GET/HEAD/PUT/POST responses\n\n... as X-Backend-* headers, so they'll be dropped at the gatekeeper.\nThis allows other middlewares (swift3, keymaster) to make more nuanced\ndecisions than was possible with just the admin-info setting. The new\nheaders are:\n\n * X-Backend-Encryption-Body-Is-Encrypted\n   - 'true' if the stored object was encrypted; not present otherwise.\n     Note that empty objects are considered not-encrypted.\n\n * X-Backend-Encryption-Meta-Is-Encrypted\n   - 'true' if any of the stored object user-metadata was encrypted; not\n     present otherwise. If no user-metadata was stored, it will be\n     considered not-encrypted. In theory, there is the potential for\n     unencrypted user-metadata to exist (if there was some third-party\n     middleware after encryption that would insert it), but in practice\n     the presence of *any* encrypted user-metadata should imply that\n     *all* user-metadata was encrypted.\n\nChange-Id: Icf28dc57e589f9be20937947095800d7ce57b2f7\nPartial-Bug: #1607116\n""}, {'number': 2, 'created': '2016-08-19 01:05:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4e2f1b95773fbf2a547928c5e8af8c5dafe54dd9', 'message': ""Expose encryption status in GET/HEAD/PUT/POST responses\n\n... as X-Backend-* headers, so they'll be dropped at the gatekeeper.\nThis allows other middlewares (swift3, keymaster) to make more nuanced\ndecisions than was possible with just the admin-info setting. The new\nheaders are:\n\n * X-Backend-Encryption-Body-Is-Encrypted\n   - 'true' if the stored object was encrypted; not present otherwise.\n     Note that empty objects are considered not-encrypted.\n\n * X-Backend-Encryption-Meta-Is-Encrypted\n   - 'true' if any of the stored object user-metadata was encrypted; not\n     present otherwise. If no user-metadata was stored, it will be\n     considered not-encrypted. In theory, there is the potential for\n     unencrypted user-metadata to exist (if there was some third-party\n     middleware after encryption that would insert it), but in practice\n     the presence of *any* encrypted user-metadata should imply that\n     *all* user-metadata was encrypted.\n\nChange-Id: Icf28dc57e589f9be20937947095800d7ce57b2f7\nPartial-Bug: #1607116\n""}, {'number': 3, 'created': '2016-08-19 17:27:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/9f4235bcfa7eba27e003d4460f3ed96a0783fe91', 'message': ""Expose encryption status in GET/HEAD/PUT/POST responses\n\n... as X-Backend-* headers, so they'll be dropped at the gatekeeper.\nThis allows other middlewares (swift3, keymaster) to make more nuanced\ndecisions than was possible with just the admin-info setting. The new\nheaders are:\n\n * X-Backend-Encryption-Body-Is-Encrypted\n   - 'true' if the stored object was encrypted; not present otherwise.\n     Note that empty objects are considered not-encrypted.\n\n * X-Backend-Encryption-Meta-Is-Encrypted\n   - 'true' if any of the stored object user-metadata was encrypted; not\n     present otherwise. If no user-metadata was stored, it will be\n     considered not-encrypted. In theory, there is the potential for\n     unencrypted user-metadata to exist (if there was some third-party\n     middleware after encryption that would insert it), but in practice\n     the presence of *any* encrypted user-metadata should imply that\n     *all* user-metadata was encrypted.\n\nChange-Id: Icf28dc57e589f9be20937947095800d7ce57b2f7\nPartial-Bug: #1607116\n""}, {'number': 4, 'created': '2016-08-26 00:47:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/288371dafd05b31b9b89b1d51ff6a1dd10dec6af', 'message': ""Expose encryption status in GET/HEAD/PUT/POST responses\n\n... as X-Backend-* headers, so they'll be dropped at the gatekeeper.\nThis allows other middlewares (swift3, keymaster) to make more nuanced\ndecisions than was possible with just the admin-info setting. The new\nheaders are:\n\n * X-Backend-Encryption-Body-Is-Encrypted\n   - 'true' if the stored object was encrypted; not present otherwise.\n     Note that empty objects are considered not-encrypted.\n\n * X-Backend-Encryption-Meta-Is-Encrypted\n   - 'true' if any of the stored object user-metadata was encrypted; not\n     present otherwise. If no user-metadata was stored, it will be\n     considered not-encrypted. In theory, there is the potential for\n     unencrypted user-metadata to exist (if there was some third-party\n     middleware after encryption that would insert it), but in practice\n     the presence of *any* encrypted user-metadata should imply that\n     *all* user-metadata was encrypted.\n\nChange-Id: Icf28dc57e589f9be20937947095800d7ce57b2f7\nPartial-Bug: #1607116\n""}, {'number': 5, 'created': '2016-08-26 18:57:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a75a8c2bbf91acb184db81f894625f936b8eb2be', 'message': ""Expose encryption status in GET/HEAD/PUT/POST responses\n\n... as X-Backend-* headers, so they'll be dropped at the gatekeeper.\nThis allows other middlewares (swift3, keymaster) to make more nuanced\ndecisions than was possible with just the admin-info setting. The new\nheaders are:\n\n * X-Backend-Encryption-Body-Is-Encrypted\n   - 'true' if the stored object was encrypted; not present otherwise.\n     Note that empty objects are considered not-encrypted.\n\n * X-Backend-Encryption-Meta-Is-Encrypted\n   - 'true' if any of the stored object user-metadata was encrypted; not\n     present otherwise. If no user-metadata was stored, it will be\n     considered not-encrypted. In theory, there is the potential for\n     unencrypted user-metadata to exist (if there was some third-party\n     middleware after encryption that would insert it), but in practice\n     the presence of *any* encrypted user-metadata should imply that\n     *all* user-metadata was encrypted.\n\nChange-Id: Icf28dc57e589f9be20937947095800d7ce57b2f7\nPartial-Bug: #1607116\n""}, {'number': 6, 'created': '2016-09-19 22:00:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/93193b9e7157ff77df6ee83bfc69674dc6217167', 'message': ""Expose encryption status in GET/HEAD/PUT/POST responses\n\n... as X-Backend-* headers, so they'll be dropped at the gatekeeper.\nThis allows other middlewares (swift3, keymaster) to make more nuanced\ndecisions than was possible with just the admin-info setting. The new\nheaders are:\n\n * X-Backend-Encryption-Body-Is-Encrypted\n   - 'true' if the stored object was encrypted; not present otherwise.\n     Note that empty objects are considered not-encrypted.\n\n * X-Backend-Encryption-Meta-Is-Encrypted\n   - 'true' if any of the stored object user-metadata was encrypted; not\n     present otherwise. If no user-metadata was stored, it will be\n     considered not-encrypted. In theory, there is the potential for\n     unencrypted user-metadata to also exist (if there was some\n     third-party middleware after encryption that would insert it), but\n     in practice the presence of *any* encrypted user-metadata should\n     imply that *all* user-metadata was encrypted.\n\nChange-Id: Icf28dc57e589f9be20937947095800d7ce57b2f7\nPartial-Bug: #1607116\n""}, {'number': 7, 'created': '2016-09-19 22:56:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/b79e9c6e90334bdb53d6d51665fb3e4a3f383da3', 'message': ""Expose encryption status in GET/HEAD/PUT/POST responses\n\n... as X-Backend-* headers, so they'll be dropped at the gatekeeper.\nThis allows other middlewares (swift3, keymaster) to make more nuanced\ndecisions than was possible with just the admin-info setting. The new\nheaders are:\n\n * X-Backend-Encryption-Body-Is-Encrypted\n   - 'true' if the stored object was encrypted; not present otherwise.\n     Note that empty objects are considered not-encrypted.\n\n * X-Backend-Encryption-Meta-Is-Encrypted\n   - 'true' if any of the stored object user-metadata was encrypted; not\n     present otherwise. If no user-metadata was stored, it will be\n     considered not-encrypted. In theory, there is the potential for\n     unencrypted user-metadata to also exist (if there was some\n     third-party middleware after encryption that would insert it), but\n     in practice the presence of *any* encrypted user-metadata should\n     imply that *all* user-metadata was encrypted.\n\nChange-Id: Icf28dc57e589f9be20937947095800d7ce57b2f7\nPartial-Bug: #1607116\n""}, {'number': 8, 'created': '2017-01-26 22:58:32.000000000', 'files': ['swift/common/middleware/crypto/encrypter.py', 'doc/source/overview_encryption.rst', 'test/unit/common/middleware/crypto/test_encryption.py', 'test/unit/common/middleware/crypto/test_decrypter.py', 'swift/common/middleware/crypto/decrypter.py', 'test/unit/common/middleware/crypto/test_encrypter.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/ce441a94090293e07660d179f6893f687fcf96d0', 'message': ""Expose encryption status in GET/HEAD/PUT/POST responses\n\n... as X-Backend-* headers, so they'll be dropped at the gatekeeper.\nThis allows other middlewares (swift3, keymaster) to make more nuanced\ndecisions than was possible with just the admin-info setting. The new\nheaders are:\n\n * X-Backend-Encryption-Body-Is-Encrypted\n   - 'true' if the stored object was encrypted; not present otherwise.\n     Note that empty objects are considered not-encrypted.\n\n * X-Backend-Encryption-Meta-Is-Encrypted\n   - 'true' if any of the stored object user-metadata was encrypted; not\n     present otherwise. If no user-metadata was stored, it will be\n     considered not-encrypted. In theory, there is the potential for\n     unencrypted user-metadata to also exist (if there was some\n     third-party middleware after encryption that would insert it), but\n     in practice the presence of *any* encrypted user-metadata should\n     imply that *all* user-metadata was encrypted.\n\nChange-Id: Icf28dc57e589f9be20937947095800d7ce57b2f7\nPartial-Bug: #1607116\n""}]",14,348603,ce441a94090293e07660d179f6893f687fcf96d0,31,4,8,15343,,,0,"Expose encryption status in GET/HEAD/PUT/POST responses

... as X-Backend-* headers, so they'll be dropped at the gatekeeper.
This allows other middlewares (swift3, keymaster) to make more nuanced
decisions than was possible with just the admin-info setting. The new
headers are:

 * X-Backend-Encryption-Body-Is-Encrypted
   - 'true' if the stored object was encrypted; not present otherwise.
     Note that empty objects are considered not-encrypted.

 * X-Backend-Encryption-Meta-Is-Encrypted
   - 'true' if any of the stored object user-metadata was encrypted; not
     present otherwise. If no user-metadata was stored, it will be
     considered not-encrypted. In theory, there is the potential for
     unencrypted user-metadata to also exist (if there was some
     third-party middleware after encryption that would insert it), but
     in practice the presence of *any* encrypted user-metadata should
     imply that *all* user-metadata was encrypted.

Change-Id: Icf28dc57e589f9be20937947095800d7ce57b2f7
Partial-Bug: #1607116
",git fetch https://review.opendev.org/openstack/swift refs/changes/03/348603/8 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/middleware/crypto/encrypter.py', 'test/unit/common/middleware/crypto/test_decrypter.py', 'swift/common/middleware/crypto/decrypter.py', 'test/unit/common/middleware/crypto/test_encrypter.py']",4,43538b92283c0e883230a5c9ed866d02a991ccdf,bug/1607116," for header in ('X-Backend-Encryption-Body-Is-Encrypted', 'X-Backend-Encryption-Meta-Is-Encrypted'): self.assertIn(header, resp.headers) self.assertEqual(resp.headers[header], 'true') self.assertNotIn('X-Backend-Encryption-Body-Is-Encrypted', resp.headers) header = 'X-Backend-Encryption-Meta-Is-Encrypted' self.assertIn(header, resp.headers) self.assertEqual(resp.headers[header], 'true') for header in ('X-Backend-Encryption-Body-Is-Encrypted', 'X-Backend-Encryption-Meta-Is-Encrypted'): self.assertNotIn(header, resp.headers)@mock.patch('swift.common.middleware.crypto.crypto_utils.Crypto.create_iv', lambda *args: FAKE_IV) class TestDisabledEncrypter(unittest.TestCase): def setUp(self): self.app = FakeSwift() self.encrypter = encrypter.Encrypter(self.app, { 'disable_encryption': 'true'}) self.encrypter.logger = FakeLogger() def test_PUT_no_encryption(self): body = 'FAKE APP' env = {'REQUEST_METHOD': 'PUT'} hdrs = {'content-type': 'text/plain', 'content-length': str(len(body)), 'x-object-meta-color': 'blue'} req = Request.blank('/v1/a/c/o', environ=env, body=body, headers=hdrs) self.app.register('PUT', '/v1/a/c/o', HTTPCreated, {}) resp = req.get_response(self.encrypter) self.assertEqual('201 Created', resp.status) for header in ('X-Backend-Encryption-Body-Is-Encrypted', 'X-Backend-Encryption-Meta-Is-Encrypted'): self.assertNotIn(header, resp.headers) # verify object is NOT encrypted by getting direct from the app get_req = Request.blank('/v1/a/c/o', environ={'REQUEST_METHOD': 'GET'}) get_resp = get_req.get_response(self.app) self.assertEqual(body, get_resp.body) self.assertEqual('blue', get_resp.headers['X-Object-Meta-Color']) def test_POST_no_encryption(self): env = {'REQUEST_METHOD': 'POST'} hdrs = {'x-object-meta-color': 'red'} req = Request.blank('/v1/a/c/o', environ=env, headers=hdrs) self.app.register('POST', '/v1/a/c/o', HTTPAccepted, {}) resp = req.get_response(self.encrypter) self.assertEqual('202 Accepted', resp.status) for header in ('X-Backend-Encryption-Body-Is-Encrypted', 'X-Backend-Encryption-Meta-Is-Encrypted'): self.assertNotIn(header, resp.headers) # verify meta is NOT encrypted self.assertEqual('red', self.app.headers[0]['X-Object-Meta-Color']) ",,92,1
openstack%2Fswift~master~I94ed9a2c64c6c9e650e8757421f414b5262974f8,openstack/swift,master,I94ed9a2c64c6c9e650e8757421f414b5262974f8,Invalidate cached tokens api,NEW,2016-09-14 17:12:38.000000000,2017-12-18 02:16:51.000000000,,"[{'_account_id': 330}, {'_account_id': 7233}, {'_account_id': 7433}, {'_account_id': 12279}, {'_account_id': 13052}, {'_account_id': 15343}, {'_account_id': 17361}, {'_account_id': 18334}, {'_account_id': 18838}, {'_account_id': 20508}]","[{'number': 1, 'created': '2016-09-14 17:12:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/94969bba5f4b74460fb492b6168264a03a7964dd', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Op use only\n\nIf the path is /sw_admin and ‘enable_sw_admin’ in the\nproxy-server.conf is set to ‘true\' -\n$ curl http://127.0.0.1:8080/sw_admin -X DELETE -H ‘X-DELETE-TOKEN: test:tester\'\nwill delete the cached auth tokens for user- tester in account- test.\n\nIf ‘enable_sw_admin’ is set to ‘false’ it will respond with\n503 “FEATURE DISABLED BY ADMIN"".\n\ne.g. output- http://paste.openstack.org/show/576299/\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper healthcheck sw_admin proxy-logging ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\nenable_sw_admin = true\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}, {'number': 2, 'created': '2016-09-27 22:26:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ae3b0417c768b521aac381e7972048d7bd98bd2a', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Op use only\n\nIf the path is /sw_admin and ‘enable_sw_admin’ in the\nproxy-server.conf is set to ‘true\' -\n$ curl http://127.0.0.1:8080/sw_admin -X DELETE -H ‘X-DELETE-TOKEN: test:tester\'\n-H \'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649\'\nwill delete the cached auth tokens for user- tester in account- test.\n\nIf ‘enable_sw_admin’ is set to ‘false’ it will respond with\n412 “Bad URL"".\n\ne.g. output- http://paste.openstack.org/show/583179/\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper ...tempauth sw_admin ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\nenable_sw_admin = true\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}, {'number': 3, 'created': '2016-10-03 18:43:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1576e02f2ec8428010c7157ff882f8f576f8adbe', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Op use only\n\nIf the path is /sw_admin and ‘enable_sw_admin’ in the\nproxy-server.conf is set to ‘true\' -\n$ curl http://127.0.0.1:8080/sw_admin -X DELETE -H ‘X-DELETE-TOKEN: test:tester\'\n-H \'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649\'\nwill delete the cached auth tokens for user- tester in account- test.\n\nIf ‘enable_sw_admin’ is set to ‘false’ it will respond with\n412 “Bad URL"".\n\ne.g. output- http://paste.openstack.org/show/583179/\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper ...tempauth sw_admin ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\nenable_sw_admin = true\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}, {'number': 4, 'created': '2016-10-04 19:15:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/19e99cd4bcf2187eecc9d9bb9960b674fe64dfe6', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Op use only\n\nIf the path is /sw_admin and ‘enable_sw_admin’ in the\nproxy-server.conf is set to ‘true\' -\n$ curl http://127.0.0.1:8080/sw_admin -X DELETE -H ‘X-DELETE-TOKEN: test:tester\'\n-H \'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649\'\nwill delete the cached auth tokens for user- tester in account- test.\n\nIf ‘enable_sw_admin’ is set to ‘false’ it will respond with\n412 “Bad URL"".\n\ne.g. output- http://paste.openstack.org/show/583179/\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper ...tempauth sw_admin ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\nenable_sw_admin = true\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}, {'number': 5, 'created': '2016-10-06 18:42:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/cddacbef00637fb07b80dbdaf58038f569a9b2f6', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Op use only\n\nIf the path is /sw_admin and ‘enable_sw_admin’ in the\nproxy-server.conf is set to ‘true\' -\n$ curl http://127.0.0.1:8080/sw_admin -X DELETE -H ‘X-DELETE-TOKEN: test:tester\'\n-H \'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649\'\nwill delete the cached auth tokens for user- tester in account- test.\n\nIf ‘enable_sw_admin’ is set to ‘false’ it will respond with\n412 “Bad URL"".\n\ne.g. output- http://paste.openstack.org/show/583179/\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper ...tempauth sw_admin ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\nenable_sw_admin = true\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}, {'number': 6, 'created': '2016-10-17 20:08:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e2f53e30cd367b28aac002789d2da235217efa31', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Ops use only\n\nIf the path is /sw_admin and ‘enable_sw_admin’ in the\nproxy-server.conf is set to ‘true\' -\n$ curl http://127.0.0.1:8080/sw_admin -X DELETE -H ‘X-DELETE-TOKEN: test:tester\'\n-H \'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649\'\nwill delete the cached auth token for user- tester in account- test.\n\nIf ‘enable_sw_admin’ is set to ‘false’ it will respond with\n412 “Bad URL"".\n\ne.g. output- http://paste.openstack.org/show/583179/\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper ...tempauth sw_admin ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\nenable_sw_admin = true\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}, {'number': 7, 'created': '2016-10-18 19:21:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8a5e3d04fc7ddcf0d71217a38da54501cf82c4b6', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Ops use only\n\nIf the path is /sw_admin and ‘enable_sw_admin’ in the\nproxy-server.conf is set to ‘true\' -\n$ curl http://127.0.0.1:8080/sw_admin -X DELETE -H ‘X-DELETE-TOKEN: test:tester\'\n-H \'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649\'\nwill delete the cached auth token for user- tester in account- test.\n\nIf ‘enable_sw_admin’ is set to ‘false’ it will respond with\n412 “Bad URL"".\n\ne.g. output- http://paste.openstack.org/show/583179/\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper ...tempauth sw_admin ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\nenable_sw_admin = true\n\nnote: sw_admin middleware is meant to be extended for other types of Admin/Ops\nfunctions\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}, {'number': 8, 'created': '2016-10-19 20:34:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/78f3761ac6b9c2db2081843227ac47222f554a5a', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Ops use only\n\nIf the path is /sw_admin and ‘enable_sw_admin’ in the\nproxy-server.conf is set to ‘true\' -\n$ curl http://127.0.0.1:8080/sw_admin -X DELETE -H ‘X-DELETE-TOKEN: test:tester\'\n-H \'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649\'\nwill delete the cached auth token for user- tester in account- test.\n\nIf ‘enable_sw_admin’ is set to ‘false’ it will respond with\n412 “Bad URL"".\n\ne.g. output- http://paste.openstack.org/show/583179/\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper ...tempauth sw_admin ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\nenable_sw_admin = true\n\nnote: sw_admin middleware is meant to be extended for other types of Admin/Ops\nfunctions\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}, {'number': 9, 'created': '2016-10-31 15:36:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8deb59ea2fdaa3f7f14ef11ffb41905c6051a07b', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Ops use only\n\nIf the path is /sw_admin and ‘enable_sw_admin’ in the\nproxy-server.conf is set to ‘true\' -\n$ curl http://127.0.0.1:8080/sw_admin -X DELETE -H ‘X-DELETE-TOKEN:\ntest:tester\' -H \'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649\'\nwill delete the cached auth token for user- tester in account- test.\n\nIf ‘sw_admin’ middleware is not set in pipeline, it will respond with\n412 “Bad URL"".\n\ne.g. output- http://paste.openstack.org/show/583179/\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper ...tempauth sw_admin ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\n\nnote: sw_admin middleware is meant to be extended for other types of Admin/Ops\nfunctions\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}, {'number': 10, 'created': '2016-11-14 15:11:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/35f40a90b14d86eeb1ac88325cffad2c0f460663', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Ops use only\n\nIf the request path is /sw_admin and ‘sw_admin’ middleware in the\nproxy-server.conf pipeline is set -\n$ curl http://127.0.0.1:8080/sw_admin -X DELETE -H ‘X-DELETE-TOKEN:\ntest:tester\' -H \'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649\'\nwill delete the cached auth token for user- tester in account- test.\n\nIf ‘sw_admin’ middleware is not set in pipeline, it will respond with\n412 “Bad URL"".\n\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper ...tempauth sw_admin ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\n\nnote: sw_admin middleware is meant to be extended for other types of Admin/Ops\nfunctions\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}, {'number': 11, 'created': '2016-12-02 14:13:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f312bc00cf62316114b35196103d231c846bd0d0', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Ops use only\n\nIf the request path is /sw_admin and ‘sw_admin’ middleware in the\nproxy-server.conf pipeline is set -\n$ curl http://127.0.0.1:8080/sw_admin -X DELETE -H ‘X-DELETE-TOKEN:\ntest:tester\' -H \'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649\'\nwill delete the cached auth token for user- tester in account- test.\n\nIf ‘sw_admin’ middleware is not set in pipeline, it will respond with\n412 “Bad URL"".\n\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper ...tempauth sw_admin ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\n\nnote: sw_admin middleware is meant to be extended for other types of Admin/Ops\nfunctions\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}, {'number': 12, 'created': '2017-02-21 20:08:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e6426401bf205d9267f553a8fa8696b997277971', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Ops use only\n\nIf the request path is /sw_admin/<function*> and ‘sw_admin’ middleware\nin the proxy-server.conf pipeline is set -\n$ curl http://127.0.0.1:8080/sw_admin/tokens -X DELETE -H ‘X-DELETE-TOKEN:\ntest:tester\' -H \'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649\'\nwill delete the cached auth token for user- tester in account- test.\n\nIf ‘sw_admin’ middleware is not set in pipeline, it will respond with\n412 “Bad URL"".\n\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper ...tempauth sw_admin ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\n\n*note: sw_admin middleware is meant to be extended for other types of\nAdmin/Ops functions\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}, {'number': 13, 'created': '2017-03-13 20:23:00.000000000', 'files': ['swift/common/middleware/sw_admin.py', 'etc/proxy-server.conf-sample', 'doc/manpages/proxy-server.conf.5', 'doc/source/middleware.rst', 'test/unit/common/middleware/test_sw_admin.py', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/swift/commit/92b51be76a9b7faa5599ba8b6154b7ba72d39c4a', 'message': 'Invalidate cached tokens api\n\nAdding sw_admin as an api endpoint to invalidate\ncached auth tokens.\nsw_admin middleware for Admin/Ops use only\n\nIf the request path is /sw_admin/<function*> and ‘sw_admin’ middleware\nin the proxy-server.conf pipeline is set -\n$ curl http://127.0.0.1:8080/sw_admin/tokens -X DELETE -H ‘X-DELETE-TOKEN:\ntest:tester\' -H \'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649\'\nwill delete the cached auth token for user- tester in account- test.\n\nIf ‘sw_admin’ middleware is not set in pipeline, it will respond with\n412 “Bad URL"".\n\nchanges to proxy-server.conf to add/enable sw-admin middleware-\n[pipeline:main]\npipeline = catch_errors gatekeeper ...tempauth sw_admin ...\n\n[filter:sw_admin]\nuse = egg:swift#sw_admin\n\n*note: sw_admin middleware is meant to be extended for other types of\nAdmin/Ops functions\n\nChange-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8\nCloses-Bug: #1521357\n'}]",70,370319,92b51be76a9b7faa5599ba8b6154b7ba72d39c4a,66,10,13,17361,,,0,"Invalidate cached tokens api

Adding sw_admin as an api endpoint to invalidate
cached auth tokens.
sw_admin middleware for Admin/Ops use only

If the request path is /sw_admin/<function*> and ‘sw_admin’ middleware
in the proxy-server.conf pipeline is set -
$ curl http://127.0.0.1:8080/sw_admin/tokens -X DELETE -H ‘X-DELETE-TOKEN:
test:tester' -H 'x-auth-token: AUTH_tk90701b3135704effbab7d1438e2ed649'
will delete the cached auth token for user- tester in account- test.

If ‘sw_admin’ middleware is not set in pipeline, it will respond with
412 “Bad URL"".

changes to proxy-server.conf to add/enable sw-admin middleware-
[pipeline:main]
pipeline = catch_errors gatekeeper ...tempauth sw_admin ...

[filter:sw_admin]
use = egg:swift#sw_admin

*note: sw_admin middleware is meant to be extended for other types of
Admin/Ops functions

Change-Id: I94ed9a2c64c6c9e650e8757421f414b5262974f8
Closes-Bug: #1521357
",git fetch https://review.opendev.org/openstack/swift refs/changes/19/370319/1 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/middleware/sw_admin.py', 'etc/proxy-server.conf-sample', 'doc/source/middleware.rst', 'setup.cfg', 'test/unit/common/middleware/test_sw_admin.py']",5,94969bba5f4b74460fb492b6168264a03a7964dd,bug/1521357,"# Author: Shashirekha Gundur <shashirekha.j.gundur@intel.com> # Copyright (c) 2016 OpenStack Foundation # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. import unittest from time import time from eventlet import Timeout from contextlib import contextmanager from swift.common.swob import Request, Response, HTTPRequestTimeout from swift.common.middleware import sw_admin class FakeApp(object): def __init__(self, fakememcache=None): self.memcache = fakememcache def __call__(self, env, start_response): req = Request(env) return Response(request=req, body='FAKE APP')( env, start_response) class FakeMemcacheRing(object): def __init__(self, io_timeout= 2.0): self.store = {} self._io_timeout = io_timeout self.forceTimeout = False def get(self, key): return self.store.get(key) def set(self, user_id='foo'): self.store['AUTH_/user/%s' % user_id] = 'dummy_token' self.store['AUTH_/token/dummy_token'] = 'dummy_value' return True def incr(self, key, time=0): self.store[key] = self.store.setdefault(key, 0) + 1 return self.store[key] @contextmanager def soft_lock(self, key, timeout=0, retries=5): yield True def delete(self, key): try: with Timeout(self._io_timeout): if self.forceTimeout: time.sleep(5) del self.store[key] return except (Exception, Timeout) as e: raise HTTPRequestTimeout class TestSWAdmin(unittest.TestCase): def setUp(self): self.enable_sw_admin = self.get_app(FakeApp(), {}).enable_sw_admin self.got_statuses = [] def get_app(self, app, global_conf, **local_conf): factory = sw_admin.filter_factory(global_conf, **local_conf) return factory(app) def start_response(self, status, headers): self.got_statuses.append(status) # pass case , 200 OK def test_swadmin_pass(self): req = Request.blank('/', environ={'REQUEST_METHOD': 'DELETE'}) app = self.get_app(FakeApp(), {}) resp = app(req.environ, self.start_response) self.assertEqual(['200 OK'], self.got_statuses) self.assertEqual(resp, ['FAKE APP']) # fail with enable_sw_admin disabled/set to False def test_swadmin_pass_disabled(self): self.enable_sw_admin = False req = Request.blank('/sw_admin', environ={ 'REQUEST_METHOD': 'DELETE'}, headers={ 'X-DELETE-TOKEN': 'test_sw_admin' }) app = self.get_app(FakeApp(), {}, enable_sw_admin=self.enable_sw_admin) resp = app(req.environ, self.start_response) self.assertEqual(['503 Service Unavailable'], self.got_statuses) self.assertEqual(resp, ['FEATURE DISABLED BY ADMIN']) # test_delete_cached_token 1, pass with valid inputs def test_delete_cached_token_pass(self): self.enable_sw_admin = True fakememcache = FakeMemcacheRing() user_id = 'foo' fakememcache.set(user_id) app = self.get_app(FakeApp(), {}, enable_sw_admin=self.enable_sw_admin) app.memcache = fakememcache req = Request.blank('/sw_admin', environ={ 'REQUEST_METHOD': 'DELETE'}, headers={ 'X-DELETE-TOKEN': 'foo' }) resp = app(req.environ, self.start_response) self.assertEqual(['204 No Content'], self.got_statuses) self.assertEqual(resp, ['Deleted Tokens']) # test_delete_cached_token 2, fail for invalid inputs for account/user_id def test_delete_cached_token_fail_token_error(self): self.enable_sw_admin = True fakememcache = FakeMemcacheRing() user_id = 'foo' fakememcache.set(user_id) app = self.get_app(FakeApp(), {}, enable_sw_admin=self.enable_sw_admin) app.memcache = fakememcache req = Request.blank('/sw_admin', environ={ 'REQUEST_METHOD': 'DELETE'}, headers={ 'X-DELETE-TOKEN': 'not_foo' }) resp = app(req.environ, self.start_response) self.assertEqual(['404 Not Found'], self.got_statuses) self.assertEqual(resp, ['Invalid Account Name: not_foo \n']) # test_delete_cached_token 3, fail by server timeout exception def test_delete_cached_token_fail_memcache_error(self): self.enable_sw_admin = True fakememcache = FakeMemcacheRing() user_id = 'foo' fakememcache.set(user_id) fakememcache.forceTimeout = True app = self.get_app(FakeApp(), {}, enable_sw_admin=self.enable_sw_admin) app.memcache = fakememcache req = Request.blank('/sw_admin', environ={ 'REQUEST_METHOD': 'DELETE'}, headers={ 'X-DELETE-TOKEN': 'foo' }) resp = app(req.environ, self.start_response) self.assertEqual(['5XX Server Error'], self.got_statuses) self.assertEqual(resp,['Internal server error.\n']) # test_delete_cached_token 4, fail with invalid header inputs def test_delete_cached_token_fail_missing_header_value(self): self.enable_sw_admin = True req = Request.blank('/sw_admin', environ={ 'REQUEST_METHOD': 'DELETE'}, headers={ 'X-DELETE-TOKEN': '' }) app = self.get_app(FakeApp(), {}, enable_sw_admin=self.enable_sw_admin) resp = app(req.environ, self.start_response) self.assertEqual(['400 Bad Request'], self.got_statuses) self.assertEqual(resp, ['Request method DELETE is missing Headers/Header values.\n']) # test_delete_cached_token 5, fail with missing header input def test_delete_cached_token_fail_missing_header(self): self.enable_sw_admin = True req = Request.blank('/sw_admin', environ={ 'REQUEST_METHOD': 'DELETE' }) app = self.get_app(FakeApp(), {}, enable_sw_admin=self.enable_sw_admin) resp = app(req.environ, self.start_response) self.assertEqual(['400 Bad Request'], self.got_statuses) self.assertEqual(resp, ['Request method DELETE is missing Headers/Header values.\n']) # test_delete_cached_token 6, fail with incorrect request method def test_delete_cached_token_fail_incorrect_method(self): self.enable_sw_admin = True #not_allowed_methods = ['PUT', 'HEAD', 'GET', 'POST'] method = 'PUT' req = Request.blank('/sw_admin', environ={ 'REQUEST_METHOD': method }) app = self.get_app(FakeApp(), {}, enable_sw_admin=self.enable_sw_admin) resp = app(req.environ, self.start_response) self.assertEqual(['405 Method Not Allowed'], self.got_statuses) self.assertEqual(resp, ['Request method %s is not supported.\n' % (method)]) if __name__ == '__main__': unittest.main() ",,360,2
openstack%2Frally~master~Ia213b5234cabb120d308ac979e03d6c916aae53c,openstack/rally,master,Ia213b5234cabb120d308ac979e03d6c916aae53c,Add Neutron.CreateAndShowFloatingip scenario,NEW,2017-02-24 08:08:13.000000000,2017-12-18 02:16:02.000000000,,"[{'_account_id': 8871}, {'_account_id': 9545}, {'_account_id': 12395}, {'_account_id': 14817}, {'_account_id': 16803}, {'_account_id': 22960}, {'_account_id': 23094}, {'_account_id': 23435}, {'_account_id': 26576}]","[{'number': 1, 'created': '2017-02-24 08:08:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/3ea8d07a38b47ee0f274dcbb504f1d64118abf7a', 'message': 'Add Neutron.CreateAndShowFloatingip scenario\n\nCreate Floatingip and show detailed information of the floatingip.\n\nChange-Id: Ia213b5234cabb120d308ac979e03d6c916aae53c\n'}, {'number': 2, 'created': '2017-02-27 03:05:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d923fdf555e482d1f89e0e323973592ccd665056', 'message': 'Add Neutron.CreateAndShowFloatingip scenario\n\nCreate Floatingip and show detailed information of the floatingip.\n\nChange-Id: Ia213b5234cabb120d308ac979e03d6c916aae53c\n'}, {'number': 3, 'created': '2017-03-13 03:36:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ebbc5dcd87a7939cd6a4eb51c7cdc0553fea6276', 'message': 'Add Neutron.CreateAndShowFloatingip scenario\n\nCreate Floatingip and show detailed information of the floatingip.\n\nChange-Id: Ia213b5234cabb120d308ac979e03d6c916aae53c\n'}, {'number': 4, 'created': '2017-04-20 02:01:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/7e58993da00f82b219ad74e21775866819926af7', 'message': 'Add Neutron.CreateAndShowFloatingip scenario\n\nCreate Floatingip and show detailed information of the floatingip.\n\nChange-Id: Ia213b5234cabb120d308ac979e03d6c916aae53c\n'}, {'number': 5, 'created': '2017-05-22 03:27:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/fa70b10194847f71667c402a5b8e002d3fe8be5d', 'message': 'Add Neutron.CreateAndShowFloatingip scenario\n\nCreate Floatingip and show detailed information of the floatingip.\n\nChange-Id: Ia213b5234cabb120d308ac979e03d6c916aae53c\n'}, {'number': 6, 'created': '2017-06-02 11:00:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a8c9d3372beff0d3a1ce18828ed48ede9f58243a', 'message': 'Add Neutron.CreateAndShowFloatingip scenario\n\nCreate Floatingip and show detailed information of the floatingip.\n\nChange-Id: Ia213b5234cabb120d308ac979e03d6c916aae53c\n'}, {'number': 7, 'created': '2017-06-05 03:24:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/be3d28af0ad770a1067df20c5ecd485f40781529', 'message': 'Add Neutron.CreateAndShowFloatingip scenario\n\nCreate Floatingip and show detailed information of the floatingip.\n\nChange-Id: Ia213b5234cabb120d308ac979e03d6c916aae53c\n'}, {'number': 8, 'created': '2017-06-14 02:31:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/92db009250d9b8c2bbf2d72a0e312e49aa6ef9af', 'message': 'Add Neutron.CreateAndShowFloatingip scenario\n\nCreate Floatingip and show detailed information of the floatingip.\n\nand replace floating_ip with floatingip\n\nChange-Id: Ia213b5234cabb120d308ac979e03d6c916aae53c\n'}, {'number': 9, 'created': '2017-09-15 03:28:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/22621ff071c3ac9f56d49f0c4446f39d9cf2cb42', 'message': 'Add Neutron.CreateAndShowFloatingip scenario\n\nCreate Floatingip and show detailed information of the floatingip.\n\nChange-Id: Ia213b5234cabb120d308ac979e03d6c916aae53c\n'}, {'number': 10, 'created': '2017-09-15 06:28:57.000000000', 'files': ['samples/tasks/scenarios/neutron/create-and-delete-floating-ips.json', 'samples/tasks/scenarios/neutron/create-and-show-floatingip.json', 'tests/unit/plugins/openstack/scenarios/neutron/test_utils.py', 'tests/unit/plugins/openstack/scenarios/neutron/test_network.py', 'samples/tasks/scenarios/neutron/create-and-list-floating-ips.json', 'samples/tasks/scenarios/neutron/create-and-show-floatingip.yaml', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'rally-jobs/rally-neutron-existing-users.yaml', 'samples/tasks/scenarios/neutron/create-and-list-floating-ips.yaml', 'rally-jobs/rally-neutron.yaml', 'samples/tasks/scenarios/neutron/create-and-delete-floating-ips.yaml', 'rally/plugins/openstack/scenarios/neutron/network.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/01cfbd31d10ebe3adb21d36f4d2a27db25f4133f', 'message': 'Add Neutron.CreateAndShowFloatingip scenario\n\nCreate Floatingip and show detailed information of the floatingip.\n\nChange-Id: Ia213b5234cabb120d308ac979e03d6c916aae53c\n'}]",12,437829,01cfbd31d10ebe3adb21d36f4d2a27db25f4133f,48,9,10,23435,,,0,"Add Neutron.CreateAndShowFloatingip scenario

Create Floatingip and show detailed information of the floatingip.

Change-Id: Ia213b5234cabb120d308ac979e03d6c916aae53c
",git fetch https://review.opendev.org/openstack/rally refs/changes/29/437829/7 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/plugins/openstack/scenarios/neutron/test_network.py', 'samples/tasks/scenarios/neutron/create-and-show-floatingip.json', 'samples/tasks/scenarios/neutron/create-and-show-floatingip.yaml', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'rally-jobs/rally-neutron.yaml', 'rally/plugins/openstack/scenarios/neutron/network.py', 'tests/unit/plugins/openstack/scenarios/neutron/test_utils.py']",7,3ea8d07a38b47ee0f274dcbb504f1d64118abf7a,Neutron.CreateAndShowFloatingip," def test_show_floatingip(self): fip = {""floatingip"": {""id"": ""fake-id""}} fip_info = {""floatingip"": {""id"": ""fake-id"", ""name"": ""fake""}} self.clients(""neutron"").show_floatingip.return_value = fip_info result = self.scenario._show_floatingip(fip[""floatingip""][""id""]) self.assertEqual(fip_info, result) self.clients(""neutron"").show_floatingip.assert_called_once_with( fip[""floatingip""][""id""]) self._test_atomic_action_timer(self.scenario.atomic_actions(), ""neutron.show_floatingip"") ",,181,1
openstack%2Fswift~master~I2b2ae9c33fef5140d2d1807d96e2009902ff3c8c,openstack/swift,master,I2b2ae9c33fef5140d2d1807d96e2009902ff3c8c,Correcting status code of Container APIs,NEW,2017-01-19 06:39:39.000000000,2017-12-18 02:15:10.000000000,,"[{'_account_id': 1179}, {'_account_id': 13052}, {'_account_id': 19935}]","[{'number': 1, 'created': '2017-01-19 06:39:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/be373c68403d612e9e942b0c3b998648430efab9', 'message': 'Correcting response codes of Container GET api\n\nFollowing source code:\nhttps://github.com/openstack/swift/blob/master/swift/container/server.py#L463-L499\n\nSuffix comma and space after colon: https://review.openstack.org/#/c/421759/\n\nChange-Id: I2b2ae9c33fef5140d2d1807d96e2009902ff3c8c\n'}, {'number': 2, 'created': '2017-01-20 02:11:52.000000000', 'files': ['api-ref/source/storage-container-services.inc'], 'web_link': 'https://opendev.org/openstack/swift/commit/2ce9168de063cd80d2548626e069ae0ee317045b', 'message': 'Correcting status code of Container APIs\n\nSuffix comma and space after colon: https://review.openstack.org/#/c/421759/\n\nChange-Id: I2b2ae9c33fef5140d2d1807d96e2009902ff3c8c\n'}]",5,422410,2ce9168de063cd80d2548626e069ae0ee317045b,11,3,2,19935,,,0,"Correcting status code of Container APIs

Suffix comma and space after colon: https://review.openstack.org/#/c/421759/

Change-Id: I2b2ae9c33fef5140d2d1807d96e2009902ff3c8c
",git fetch https://review.opendev.org/openstack/swift refs/changes/10/422410/2 && git format-patch -1 --stdout FETCH_HEAD,['api-ref/source/storage-container-services.inc'],1,be373c68403d612e9e942b0c3b998648430efab9,api/container,"Normal response codes: 200,204, Error response codes: 404,412,507,","Normal response codes: 200 Error response codes:416,404,204,",3,2
openstack%2Fswift~master~Ia6c64ff4ff293cca362388b55818d0662ba010aa,openstack/swift,master,Ia6c64ff4ff293cca362388b55818d0662ba010aa,Initial placement gets one go,NEW,2016-12-02 13:13:25.000000000,2017-12-18 02:13:19.000000000,,"[{'_account_id': 1179}, {'_account_id': 6968}, {'_account_id': 13052}]","[{'number': 1, 'created': '2016-12-02 13:13:25.000000000', 'files': ['test/unit/common/ring/test_utils.py', 'swift/common/ring/builder.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/d9b7cd6b2ae8ae2abbc4e3835789fcc1244f1bc1', 'message': ""Initial placement gets one go\n\nTo some extent, during a single invocation of rebalance we may actually\nperform a few gather & assign cycles - but of course once any one of\nthose cycles reassigns a replica of a part - it can't move another\ngather another replica of that part until min_part_hours has passed (or\nthe next rebalance if min_part_hours is zero).\n\nMost of the time we mark parts as moved when we gather, but during\ninitial placement we do it during reassign.  This has the effect of\nmarking every part as moved during the first gather & assign cycle in\nrebalance.  We could investigate if special case handling for initial\nplacement might lead to better initial assignments - but for now this\nchange just removes a special case for min_part_hours 0 rings that was\nleft over from the related change.\n\nAlso drive-by optimization to free some memory in the part move bitmap\nas soon as we're done with it.\n\nRelated-Change: Ia1629abd5ce6e1b3acc2e94f818ed8223eed993a\nChange-Id: Ia6c64ff4ff293cca362388b55818d0662ba010aa\n""}]",4,406111,d9b7cd6b2ae8ae2abbc4e3835789fcc1244f1bc1,7,3,1,1179,,,0,"Initial placement gets one go

To some extent, during a single invocation of rebalance we may actually
perform a few gather & assign cycles - but of course once any one of
those cycles reassigns a replica of a part - it can't move another
gather another replica of that part until min_part_hours has passed (or
the next rebalance if min_part_hours is zero).

Most of the time we mark parts as moved when we gather, but during
initial placement we do it during reassign.  This has the effect of
marking every part as moved during the first gather & assign cycle in
rebalance.  We could investigate if special case handling for initial
placement might lead to better initial assignments - but for now this
change just removes a special case for min_part_hours 0 rings that was
left over from the related change.

Also drive-by optimization to free some memory in the part move bitmap
as soon as we're done with it.

Related-Change: Ia1629abd5ce6e1b3acc2e94f818ed8223eed993a
Change-Id: Ia6c64ff4ff293cca362388b55818d0662ba010aa
",git fetch https://review.opendev.org/openstack/swift refs/changes/11/406111/1 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/ring/builder.py', 'test/unit/common/ring/test_utils.py']",2,d9b7cd6b2ae8ae2abbc4e3835789fcc1244f1bc1,bug/1586167," part_replicas_moved = rb.rebalance(seed=100)[0] self.assertEqual(part_replicas_moved, 768) self.assertEqual(rb.get_balance(), 34.89583333333333) self.assertEqual(rb.dispersion, 59.765625) # it's fascinating that initial placement can be improved, but as it # stands this min_part_hours 0 ring can rebalance again right away for # better results part_replicas_moved = rb.rebalance(seed=100)[0] self.assertEqual(part_replicas_moved, 51) rb.validate() self.assertEqual(rb.get_balance(), 1.5625) # ... seriously, that's it tho part_replicas_moved = rb.rebalance(seed=100)[0] self.assertEqual(part_replicas_moved, 0) rb.validate() self.assertEqual(rb.get_balance(), 1.5625) self.assertEqual(rb.dispersion, 39.84375) # when the biggest tier has the smallest devices things get ugly # at first we can't move all the part-replicas in one rebalance # so the worst tier is still back in r1z1 self.assertEqual(report['worst_tier'], 'r1z1-127.0.0.2') self.assertEqual(report['max_dispersion'], 7.228915662650603) # but do a second rebalance ... # ... and now the overcompensation r1z0 kicks in"," rb.rebalance(seed=100) # when the biggest tier has the smallest devices things get ugly # can't move all the part-replicas in one rebalance self.assertEqual(report['worst_tier'], 'r1z1-127.0.0.1') self.assertEqual(report['max_dispersion'], 7.18562874251497) # do a sencond rebalance",28,9
openstack%2Fkolla-kubernetes~master~Ief6501425bbea4b9128154b5a1193b0a2c4a6dd8,openstack/kolla-kubernetes,master,Ief6501425bbea4b9128154b5a1193b0a2c4a6dd8,Add the checking for nova-libvirt start,ABANDONED,2017-11-24 07:52:53.000000000,2017-12-18 02:11:57.000000000,,"[{'_account_id': 22076}, {'_account_id': 22348}, {'_account_id': 24043}, {'_account_id': 26553}]","[{'number': 1, 'created': '2017-11-24 07:52:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-kubernetes/commit/556e11fa394a633eee0cddeaa21f23e6ff116f58', 'message': 'Add the checking for nova-libvirt start\n\nPartial-Bug: #1733806\n\nJira: ENTWLS-475\n\nChange-Id: Ief6501425bbea4b9128154b5a1193b0a2c4a6dd8\nSigned-off-by: Kevin Zhao <kevin.zhao@arm.com>\n'}, {'number': 2, 'created': '2017-11-27 08:31:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-kubernetes/commit/adcf939b17db491fa7993f545a827695bca5e4d7', 'message': 'Add the checking for nova-libvirt start\n\nPartial-Bug: #1733806\n\nChange-Id: Ief6501425bbea4b9128154b5a1193b0a2c4a6dd8\nSigned-off-by: Kevin Zhao <kevin.zhao@arm.com>\n'}, {'number': 3, 'created': '2017-11-27 08:39:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-kubernetes/commit/d794e067945581c477217fd65fa36557ea2dd679', 'message': 'Add the checking for nova-libvirt start\n\nIf use ceph as the backend, user need to copy the secrets\nto the /etc/libvirt. If not use the ceph backend, this process\nwill not occur. So add the checking here.\nThe fix just solve 1 issues when booting nova-libvirt. Will need\nmore to fix the libvirtd could not start issue as following.\n\nPartial-Bug: #1733806\n\nChange-Id: Ief6501425bbea4b9128154b5a1193b0a2c4a6dd8\nSigned-off-by: Kevin Zhao <kevin.zhao@arm.com>\n'}, {'number': 4, 'created': '2017-11-27 08:40:51.000000000', 'files': ['helm/microservice/nova-libvirt-daemonset/templates/nova-libvirt.yaml'], 'web_link': 'https://opendev.org/openstack/kolla-kubernetes/commit/7cbd62acbf92d03327fa047335d5367dcc2a07af', 'message': 'Add the checking for nova-libvirt start\n\nIf use ceph as the backend, user need to copy the secrets\nto the /etc/libvirt. If not use the ceph backend, this process\nwill not occur. So add the checking here.\nThe fix just solve 1 issues when booting nova-libvirt. Will need\nmore to fix the libvirtd could not start issue as following.\n\nPartial-Bug: #1733806\n\nChange-Id: Ief6501425bbea4b9128154b5a1193b0a2c4a6dd8\nSigned-off-by: Kevin Zhao <kevin.zhao@arm.com>\n'}]",3,522738,7cbd62acbf92d03327fa047335d5367dcc2a07af,10,4,4,22076,,,0,"Add the checking for nova-libvirt start

If use ceph as the backend, user need to copy the secrets
to the /etc/libvirt. If not use the ceph backend, this process
will not occur. So add the checking here.
The fix just solve 1 issues when booting nova-libvirt. Will need
more to fix the libvirtd could not start issue as following.

Partial-Bug: #1733806

Change-Id: Ief6501425bbea4b9128154b5a1193b0a2c4a6dd8
Signed-off-by: Kevin Zhao <kevin.zhao@arm.com>
",git fetch https://review.opendev.org/openstack/kolla-kubernetes refs/changes/38/522738/2 && git format-patch -1 --stdout FETCH_HEAD,['helm/microservice/nova-libvirt-daemonset/templates/nova-libvirt.yaml'],1,556e11fa394a633eee0cddeaa21f23e6ff116f58,bug/1733806,{{- if $localVals.ceph_backend }}{{- end }},,2,0
openstack%2Fironic~master~Id63dbe269d2e640b59ff267a2ddd65dde843e9d5,openstack/ironic,master,Id63dbe269d2e640b59ff267a2ddd65dde843e9d5,Fix docs on how deploy API under HTTPD + mod_wsgi,NEW,2015-11-04 11:42:46.000000000,2017-12-18 02:11:41.000000000,,[{'_account_id': 6618}],"[{'number': 1, 'created': '2015-11-04 11:42:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/87aa10f82dfd42b10093351cd253f038a8f6df7a', 'message': 'Fix docs on how deploy API under HTTPD + mod_wsgi\n\nChange-Id: Id63dbe269d2e640b59ff267a2ddd65dde843e9d5\nDepends-On: I245e8fbc966d7809705ba3b458345d376d9b532c\n'}, {'number': 2, 'created': '2015-11-05 12:11:02.000000000', 'files': ['doc/source/dev/dev-quickstart.rst', 'doc/source/deploy/install-guide.rst'], 'web_link': 'https://opendev.org/openstack/ironic/commit/0cfeee55ab7482e6207421aeba332fe8e672185b', 'message': 'Fix docs on how deploy API under HTTPD + mod_wsgi\n\nChange-Id: Id63dbe269d2e640b59ff267a2ddd65dde843e9d5\nDepends-On: I245e8fbc966d7809705ba3b458345d376d9b532c\n'}]",0,241575,0cfeee55ab7482e6207421aeba332fe8e672185b,7,1,2,13636,,,0,"Fix docs on how deploy API under HTTPD + mod_wsgi

Change-Id: Id63dbe269d2e640b59ff267a2ddd65dde843e9d5
Depends-On: I245e8fbc966d7809705ba3b458345d376d9b532c
",git fetch https://review.opendev.org/openstack/ironic refs/changes/75/241575/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/dev/dev-quickstart.rst', 'doc/source/deploy/install-guide.rst']",2,87aa10f82dfd42b10093351cd253f038a8f6df7a,apache-template," an appropriate user on your server(replace ``%USER%`` and ``%GROUP%`` vars). - Replace ``%SSLENGINE%``, ``%SSLCERTFILE%``, ``%SSLKEYFILE%`` with appropriate options to service requests via HTTPS. Instructions and example can be found at `<https://httpd.apache.org/docs/2.4/ssl/ssl_howto.html>`_.", an appropriate user on your server.,13,1
openstack%2Fironic~master~Ib1d1602f037ace71c13e32bf971e673d00fbbdc7,openstack/ironic,master,Ib1d1602f037ace71c13e32bf971e673d00fbbdc7,Update docs with Ironic API with SSL instructions,NEW,2015-11-03 15:35:38.000000000,2017-12-18 02:11:38.000000000,,"[{'_account_id': 2889}, {'_account_id': 6618}, {'_account_id': 6773}, {'_account_id': 10765}, {'_account_id': 11655}, {'_account_id': 13362}, {'_account_id': 13636}, {'_account_id': 14614}, {'_account_id': 14810}]","[{'number': 1, 'created': '2015-11-03 15:35:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/200d003ce5a19ecec931cd02610a23e7eb3bee07', 'message': 'Add instructions how setup Ironic API with SSL\n\nChange-Id: Ib1d1602f037ace71c13e32bf971e673d00fbbdc7\nDepends-On: Ia0cdb04e0d8fe67431d9acf73b9ad9b8a54da93c\n'}, {'number': 2, 'created': '2015-11-04 08:59:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/868ec04dda250903ba8048b57ab1473cf3e002d8', 'message': 'Add instructions how setup Ironic API with SSL\n\nChange-Id: Ib1d1602f037ace71c13e32bf971e673d00fbbdc7\nDepends-On: Ia0cdb04e0d8fe67431d9acf73b9ad9b8a54da93c\n'}, {'number': 3, 'created': '2015-11-11 11:05:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a5835412131d6ceb13b6e98d0c37389e95aa3829', 'message': 'Add instructions how setup Ironic API with SSL\n\nChange-Id: Ib1d1602f037ace71c13e32bf971e673d00fbbdc7\nDepends-On: Ia0cdb04e0d8fe67431d9acf73b9ad9b8a54da93c\n'}, {'number': 4, 'created': '2015-11-17 09:58:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6f8daf534fb444959f2b2d01159f98454cec9ffd', 'message': 'Add instructions how setup Ironic API with SSL\n\nChange-Id: Ib1d1602f037ace71c13e32bf971e673d00fbbdc7\nDepends-On: Ia0cdb04e0d8fe67431d9acf73b9ad9b8a54da93c\n'}, {'number': 5, 'created': '2015-12-15 09:12:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3ec4140b8b0fff5c1f9076483a2bd6b951f55414', 'message': 'Add instructions how setup Ironic API with SSL\n\nChange-Id: Ib1d1602f037ace71c13e32bf971e673d00fbbdc7\n'}, {'number': 6, 'created': '2015-12-23 09:01:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2ad56cede891125fdb3005bfca3630dab2d7d13c', 'message': 'Add instructions how setup Ironic API with SSL\n\nChange-Id: Ib1d1602f037ace71c13e32bf971e673d00fbbdc7\n'}, {'number': 7, 'created': '2015-12-23 09:11:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/356da2a4ae28093d6bd8b852191602fa84f79b2d', 'message': 'Add instructions how setup Ironic API with SSL\n\nChange-Id: Ib1d1602f037ace71c13e32bf971e673d00fbbdc7\n'}, {'number': 8, 'created': '2016-02-10 11:58:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6ba6a733fe1492452dfbd2415c924b9fd3a5b4d2', 'message': 'Add instructions how setup Ironic API with SSL\n\nChange-Id: Ib1d1602f037ace71c13e32bf971e673d00fbbdc7\n'}, {'number': 9, 'created': '2016-02-10 12:50:19.000000000', 'files': ['doc/source/dev/dev-quickstart.rst', 'doc/source/deploy/install-guide.rst'], 'web_link': 'https://opendev.org/openstack/ironic/commit/157d4594c2505bacbd2d2474fa1d4371320b3384', 'message': 'Update docs with Ironic API with SSL instructions\n\nChange-Id: Ib1d1602f037ace71c13e32bf971e673d00fbbdc7\n'}]",9,241250,157d4594c2505bacbd2d2474fa1d4371320b3384,38,9,9,13636,,,0,"Update docs with Ironic API with SSL instructions

Change-Id: Ib1d1602f037ace71c13e32bf971e673d00fbbdc7
",git fetch https://review.opendev.org/openstack/ironic refs/changes/50/241250/9 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/dev/dev-quickstart.rst', 'doc/source/deploy/install-guide.rst']",2,200d003ce5a19ecec931cd02610a23e7eb3bee07,ssl-support,"#. Configure the Bare Metal API to service requests via HTTPS instead of HTTP, to enable set ``enable_ssl_api`` with True value, and specify private key and certificate file. Examples how generate the SSL certificate with openssl, you can find on the `openssl page`_:: [api] # Enable the integrated stand-alone API to service requests # via HTTPS instead of HTTP. If there is a front-end service # performing HTTPS offloading from the service, this option # should be False; note, you will want to change public API # endpoint to represent SSL termination URL with # 'public_endpoint' option. (boolean value) #enable_ssl_api=false [ssl] # CA certificate file to use to verify connecting clients. # (string value) #ca_file=<None> # Certificate file to use when starting the server securely. # (string value) #cert_file=<None> # Private key file to use when starting the server securely. # (string value) #key_file=<None>",,50,0
openstack%2Fkarbor~master~I0dfb57c925b305e770e513586a1a1dea0df354f3,openstack/karbor,master,I0dfb57c925b305e770e513586a1a1dea0df354f3,Fix releasenotes build,MERGED,2017-12-03 22:47:02.000000000,2017-12-18 02:04:30.000000000,2017-12-18 02:04:30.000000000,"[{'_account_id': 2023}, {'_account_id': 13070}, {'_account_id': 17151}, {'_account_id': 19346}, {'_account_id': 21224}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-03 22:47:02.000000000', 'files': ['releasenotes/source/conf.py'], 'web_link': 'https://opendev.org/openstack/karbor/commit/209e700ea4b2084c70197c616ff364548b46b8f0', 'message': 'Fix releasenotes build\n\nChange I8ff60cc3793aea723a77d1b47a0f68c6c5c6ec62 left a broken\nline in the tree such that releasenotes did not build anymore if karbor\nwas not installed, remove it.\n\nChange-Id: I0dfb57c925b305e770e513586a1a1dea0df354f3\n'}]",0,525013,209e700ea4b2084c70197c616ff364548b46b8f0,7,6,1,6547,,,0,"Fix releasenotes build

Change I8ff60cc3793aea723a77d1b47a0f68c6c5c6ec62 left a broken
line in the tree such that releasenotes did not build anymore if karbor
was not installed, remove it.

Change-Id: I0dfb57c925b305e770e513586a1a1dea0df354f3
",git fetch https://review.opendev.org/openstack/karbor refs/changes/13/525013/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/source/conf.py'],1,209e700ea4b2084c70197c616ff364548b46b8f0,fix-releasenotes,,"# The full version, including alpha/beta/rc tags. release = karbor_version.version_string_with_vcs() ",0,3
openstack%2Fkeystone~master~I4ef256da9843be6f4204dc85a009e38cc60c35e3,openstack/keystone,master,I4ef256da9843be6f4204dc85a009e38cc60c35e3,Remove useless function,ABANDONED,2017-12-07 03:50:51.000000000,2017-12-18 01:42:33.000000000,,"[{'_account_id': 2218}, {'_account_id': 5046}, {'_account_id': 13063}, {'_account_id': 15054}, {'_account_id': 21420}, {'_account_id': 22348}, {'_account_id': 23659}]","[{'number': 1, 'created': '2017-12-07 03:50:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/3ce24e0b3695834f56918a20d97b8829bee54b08', 'message': 'Remove useless function\n\n""get_token_version"" is useless now and is called nowhere.\n\nChange-Id: I4ef256da9843be6f4204dc85a009e38cc60c35e3\nbp: removed-as-of-queens\n'}, {'number': 2, 'created': '2017-12-07 06:15:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/1e22eeb2ba9f8f0448c2eff262462b6a091f6b2f', 'message': 'Remove useless function\n\n""get_token_version"" is useless now and is called nowhere.\n\nChange-Id: I4ef256da9843be6f4204dc85a009e38cc60c35e3\nbp: removed-as-of-queens\n'}, {'number': 3, 'created': '2017-12-08 03:46:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/7cdf4292e607cadf9b2da94f7c350dd66412013b', 'message': 'Remove useless function\n\n""get_token_version"" is useless now and is called nowhere.\n\nChange-Id: I4ef256da9843be6f4204dc85a009e38cc60c35e3\nbp: removed-as-of-queens\n'}, {'number': 4, 'created': '2017-12-08 03:51:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/9cb1d625584ea2ee394d454d33b827bace0cfe9f', 'message': 'Remove useless function\n\n""get_token_version"" is useless now and is called nowhere.\n\nChange-Id: I4ef256da9843be6f4204dc85a009e38cc60c35e3\nbp: removed-as-of-queens\n'}, {'number': 5, 'created': '2017-12-08 03:54:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/b4e74533792bf7f9d4c4bb76bfb463c317e224e7', 'message': 'Remove useless function\n\n""get_token_version"" is useless now and is called nowhere.\n\nChange-Id: I4ef256da9843be6f4204dc85a009e38cc60c35e3\nbp: removed-as-of-queens\n'}, {'number': 6, 'created': '2017-12-11 03:14:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/3e39003f6369d2313576b7ba89fe8d57c0ab7dc5', 'message': 'Remove useless function\n\n""get_token_version"" is useless now and is called nowhere.\n\nChange-Id: I4ef256da9843be6f4204dc85a009e38cc60c35e3\nbp: removed-as-of-queens\n'}, {'number': 7, 'created': '2017-12-15 03:57:58.000000000', 'files': ['keystone/token/providers/base.py', 'keystone/token/providers/common.py', 'keystone/tests/unit/test_token_provider.py', 'releasenotes/notes/removed-as-of-queens-94c04e88c08f89aa.yaml'], 'web_link': 'https://opendev.org/openstack/keystone/commit/552415c12558e52da88ade82dee53fbd14ae4abb', 'message': 'Remove useless function\n\n""get_token_version"" is useless now and is called nowhere.\n\nChange-Id: I4ef256da9843be6f4204dc85a009e38cc60c35e3\nbp: removed-as-of-queens\n'}]",5,526262,552415c12558e52da88ade82dee53fbd14ae4abb,28,7,7,15054,,,0,"Remove useless function

""get_token_version"" is useless now and is called nowhere.

Change-Id: I4ef256da9843be6f4204dc85a009e38cc60c35e3
bp: removed-as-of-queens
",git fetch https://review.opendev.org/openstack/keystone refs/changes/62/526262/2 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/token/providers/base.py', 'keystone/token/providers/common.py', 'keystone/tests/unit/test_token_provider.py']",3,3ce24e0b3695834f56918a20d97b8829bee54b08,bp/removed-as-of-queens,," def test_get_token_version(self): self.assertEqual( token.provider.V3, self.token_provider_api.get_token_version(SAMPLE_V3_TOKEN)) self.assertEqual( token.provider.V3, self.token_provider_api.get_token_version( SAMPLE_V3_TOKEN_WITH_EMBEDED_VERSION)) self.assertRaises(exception.UnsupportedTokenVersionException, self.token_provider_api.get_token_version, 'bogus') ",0,41
openstack%2Fzun~master~I0544a807753292e614428cc70441aed8a02edfc3,openstack/zun,master,I0544a807753292e614428cc70441aed8a02edfc3,Add image_tag support - zun create,NEW,2017-08-21 11:35:53.000000000,2017-12-18 01:38:30.000000000,,"[{'_account_id': 10206}, {'_account_id': 11536}, {'_account_id': 16190}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-08-21 11:35:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/8bc4be777d37f355bc50fd11ca8e615bb432c760', 'message': 'Add image_tag support - zun create\n\nChange-Id: I0544a807753292e614428cc70441aed8a02edfc3\nCloses-Bug: #1709238\n'}, {'number': 2, 'created': '2017-09-04 12:05:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/5ec0e7221803284a4fcb4274474d04e6bcf63b84', 'message': 'Add image_tag support - zun create\n\nChange-Id: I0544a807753292e614428cc70441aed8a02edfc3\nCloses-Bug: #1709238\n'}, {'number': 3, 'created': '2017-09-06 12:04:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/aa644ebf0ec1d6d3bfa931c20d70354f720c39e3', 'message': 'Add image_tag support - zun create\n\nChange-Id: I0544a807753292e614428cc70441aed8a02edfc3\nCloses-Bug: #1709238\n'}, {'number': 4, 'created': '2017-09-20 13:31:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/3d9f10ff645c8adc965005fee4058b8a5c63505a', 'message': 'Add image_tag support - zun create\n\nChange-Id: I0544a807753292e614428cc70441aed8a02edfc3\nCloses-Bug: #1709238\n'}, {'number': 5, 'created': '2017-09-20 13:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/d40d14c0b9dd7ef8235e4f9b366be3be627006f7', 'message': 'Add image_tag support - zun create\n\nChange-Id: I0544a807753292e614428cc70441aed8a02edfc3\nCloses-Bug: #1709238\n'}, {'number': 6, 'created': '2017-10-04 13:34:34.000000000', 'files': ['zun/common/utils.py', 'zun/tests/unit/db/utils.py', 'zun/compute/api.py', 'zun/tests/unit/compute/test_compute_manager.py', 'zun/image/driver.py', 'zun/tests/unit/objects/test_objects.py', 'zun/tests/tempest/api/common/datagen.py', 'zun/compute/rpcapi.py', 'zun/image/docker/driver.py', 'zun/db/sqlalchemy/models.py', 'zun/api/controllers/v1/schemas/containers.py', 'zun/api/controllers/v1/views/containers_view.py', 'zun/image/glance/driver.py', 'zun/image/glance/utils.py', 'zun/compute/manager.py', 'zun/tests/unit/image/docker/test_driver.py', 'zun/objects/container.py', 'zun/api/controllers/v1/containers.py', 'zun/common/validation/parameter_types.py'], 'web_link': 'https://opendev.org/openstack/zun/commit/0e516c0273b7a31af329a74835f3e77f021813c0', 'message': 'Add image_tag support - zun create\n\nChange-Id: I0544a807753292e614428cc70441aed8a02edfc3\nCloses-Bug: #1709238\n'}]",0,495825,0e516c0273b7a31af329a74835f3e77f021813c0,28,4,6,16190,,,0,"Add image_tag support - zun create

Change-Id: I0544a807753292e614428cc70441aed8a02edfc3
Closes-Bug: #1709238
",git fetch https://review.opendev.org/openstack/zun refs/changes/25/495825/4 && git format-patch -1 --stdout FETCH_HEAD,"['zun/compute/api.py', 'zun/compute/rpcapi.py', 'zun/api/controllers/v1/schemas/containers.py', 'zun/compute/manager.py', 'zun/image/driver.py', 'zun/objects/container.py', 'zun/api/controllers/v1/containers.py', 'zun/common/validation/parameter_types.py']",8,8bc4be777d37f355bc50fd11ca8e615bb432c760,bug/1709238,"image_tag = { 'type': 'string', 'minLength': 2, 'maxLength': 255, 'pattern': '[a-zA-Z0-9][a-zA-Z0-9_.-]+$' } ",,19,8
openstack%2Frally~master~I5944299dc8bac2e39c957192c513272d040a0e6f,openstack/rally,master,I5944299dc8bac2e39c957192c513272d040a0e6f,Make resource name generation pattern configurable,NEW,2017-05-22 12:23:16.000000000,2017-12-18 01:38:23.000000000,,"[{'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 20031}, {'_account_id': 21528}, {'_account_id': 22960}]","[{'number': 1, 'created': '2017-05-22 12:23:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/30de938eda32ad19d713edc9324d5931416f7878', 'message': ""Make resource name generation pattern configurable\n\nNeeded in our cloud for several reasons, the main one is that DNS protocol\ndoesn't support _ in names.\nAlso change the default to remove _, according to some overriding of the\ngeneration pattern we are not the only one with _ problems.\n\nChange-Id: I5944299dc8bac2e39c957192c513272d040a0e6f\n""}, {'number': 2, 'created': '2017-05-22 13:04:10.000000000', 'files': ['rally/task/context.py', 'tests/hacking/checks.py', 'rally/common/opts.py', 'rally/plugins/openstack/context/manila/manila_share_networks.py', 'rally/plugins/openstack/verification/tempest/context.py', 'rally/plugins/openstack/scenarios/murano/utils.py', 'rally/task/scenario.py', 'rally/common/utils.py', 'rally/plugins/openstack/scenarios/ironic/utils.py', 'rally/plugins/openstack/scenarios/sahara/utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/a710995608e280fcc71f930b925d6430ed658f9f', 'message': ""Make resource name generation pattern configurable\n\nNeeded in our cloud for several reasons, the main one is that DNS protocol\ndoesn't support _ in names.\nAlso change the default to remove _, according to some overriding of the\ngeneration pattern we are not the only one with _ problems.\n\nChange-Id: I5944299dc8bac2e39c957192c513272d040a0e6f\n""}]",7,466713,a710995608e280fcc71f930b925d6430ed658f9f,17,5,2,20031,,,0,"Make resource name generation pattern configurable

Needed in our cloud for several reasons, the main one is that DNS protocol
doesn't support _ in names.
Also change the default to remove _, according to some overriding of the
generation pattern we are not the only one with _ problems.

Change-Id: I5944299dc8bac2e39c957192c513272d040a0e6f
",git fetch https://review.opendev.org/openstack/rally refs/changes/13/466713/2 && git format-patch -1 --stdout FETCH_HEAD,"['rally/task/context.py', 'rally/common/opts.py', 'rally/plugins/openstack/verification/tempest/context.py', 'rally/plugins/openstack/scenarios/murano/utils.py', 'rally/common/utils.py', 'rally/task/scenario.py', 'rally/plugins/openstack/scenarios/ironic/utils.py', 'rally/plugins/openstack/scenarios/sahara/utils.py']",8,30de938eda32ad19d713edc9324d5931416f7878,,," # NOTE(sskripnick): Some sahara resource names are validated as hostnames. # Since underscores are not allowed in hostnames we should not use them. RESOURCE_NAME_FORMAT = ""rally-sahara-XXXXXX-XXXXXXXXXXXXXXXX"" ",23,30
openstack%2Ftacker~master~I14b5af8a170c33728c13a3aa270ada32f7b186c9,openstack/tacker,master,I14b5af8a170c33728c13a3aa270ada32f7b186c9,[WIP] Enable auto-healing and scaling for VNFFG,NEW,2017-08-21 07:50:17.000000000,2017-12-18 01:37:28.000000000,,"[{'_account_id': 20560}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-08-21 07:50:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/87cb147b3da51d8326003dc8270382523c438a45', 'message': '[WIP] Enable auto-healing and scaling for VNFFG\n\nCurrently, VNFFG is orchestrated by the combination of VNFs i Tacker.\nIn lower layer, Tacker used Neutron SFC to chain VNFs via their connection\npoints. In case the VNFs are broken, it leads to the failure of the chains.\nIn this blueprint, tacker conductor is used to emit failure events from VNFM\nto NFVO so that VNFFG can recognize.\n\nImplement blueprint: #vnffg-healing\nImplement blueprint: #vnffg-scaling\n\nChange-Id: I14b5af8a170c33728c13a3aa270ada32f7b186c9\n'}, {'number': 2, 'created': '2017-08-23 02:31:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/909caf42d8706fc0276ac276784b6c8aa01c9489', 'message': '[WIP] Enable auto-healing and scaling for VNFFG\n\nCurrently, VNFFG is orchestrated by the combination of VNFs i Tacker.\nIn lower layer, Tacker used Neutron SFC to chain VNFs via their connection\npoints. In case the VNFs are broken, it leads to the failure of the chains.\nIn this blueprint, tacker conductor is used to emit failure events from VNFM\nto NFVO so that VNFFG can recognize.\n\nImplement blueprint: #vnffg-healing\nImplement blueprint: #vnffg-scaling\n\nCo-Authored-By: Hoang Phuoc <hongphuocbk2.07@gmail.com>\n\nChange-Id: I14b5af8a170c33728c13a3aa270ada32f7b186c9\n'}, {'number': 3, 'created': '2017-08-23 17:00:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/6c89bf8b554863a2710f68586f05003fc3228c13', 'message': '[WIP] Enable auto-healing and scaling for VNFFG\n\nCurrently, VNFFG is orchestrated by the combination of VNFs i Tacker.\nIn lower layer, Tacker used Neutron SFC to chain VNFs via their connection\npoints. In case the VNFs are broken, it leads to the failure of the chains.\nIn this blueprint, tacker conductor is used to emit failure events from VNFM\nto NFVO so that VNFFG can recognize.\n\nImplement blueprint: #vnffg-healing\nImplement blueprint: #vnffg-scaling\n\nCo-Authored-By: Hoang Phuoc <hongphuocbk2.07@gmail.com>\n\nChange-Id: I14b5af8a170c33728c13a3aa270ada32f7b186c9\n'}, {'number': 4, 'created': '2017-09-22 11:39:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/62f72433cee943aad4f0158b6effc99ed39895a5', 'message': '[WIP] Enable auto-healing and scaling for VNFFG\n\nCurrently, VNFFG is orchestrated by the combination of VNFs i Tacker.\nIn lower layer, Tacker used Neutron SFC to chain VNFs via their connection\npoints. In case the VNFs are broken, it leads to the failure of the chains.\nIn this blueprint, tacker conductor is used to emit failure events from VNFM\nto NFVO so that VNFFG can recognize.\n\nImplement blueprint: #vnffg-healing\nImplement blueprint: #vnffg-scaling\n\nCo-Authored-By: Hoang Phuoc <hongphuocbk2.07@gmail.com>\n\nChange-Id: I14b5af8a170c33728c13a3aa270ada32f7b186c9\n'}, {'number': 5, 'created': '2017-10-01 23:22:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/7c7aeb0f6b08f5cfb3bf6ef9456125cb85b92435', 'message': '[WIP] Enable auto-healing and scaling for VNFFG\n\nCurrently, VNFFG is orchestrated by the combination of VNFs i Tacker.\nIn lower layer, Tacker used Neutron SFC to chain VNFs via their connection\npoints. In case the VNFs are broken, it leads to the failure of the chains.\nIn this blueprint, tacker conductor is used to emit failure events from VNFM\nto NFVO so that VNFFG can recognize.\n\nImplement blueprint: #vnffg-healing\nImplement blueprint: #vnffg-scaling\n\nCo-Authored-By: Hoang Phuoc <hongphuocbk2.07@gmail.com>\n\nChange-Id: I14b5af8a170c33728c13a3aa270ada32f7b186c9\n'}, {'number': 6, 'created': '2017-10-07 21:31:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/f0c825c785d59cbc81b59a8bdbb8a18907ec0269', 'message': '[WIP] Enable auto-healing and scaling for VNFFG\n\nCurrently, VNFFG is orchestrated by the combination of VNFs i Tacker.\nIn lower layer, Tacker used Neutron SFC to chain VNFs via their connection\npoints. In case the VNFs are broken, it leads to the failure of the chains.\nIn this blueprint, tacker conductor is used to emit failure events from VNFM\nto NFVO so that VNFFG can recognize.\n\nImplement blueprint: #vnffg-healing\nImplement blueprint: #vnffg-scaling\n\nCo-Authored-By: Hoang Phuoc <hongphuocbk2.07@gmail.com>\n\nChange-Id: I14b5af8a170c33728c13a3aa270ada32f7b186c9\n'}, {'number': 7, 'created': '2017-10-08 19:37:33.000000000', 'files': ['tacker/nfvo/drivers/vnffg/absttract_vnffg_policies.py', 'samples/tosca-templates/vnffgd/tosca-vnffg-vnfd2-alarm-respawn.yaml', 'tacker/db/nfvo/vnffg_db.py', 'samples/tosca-templates/vnffgd/tosca-vnffg-vnfd1-alarm-respawn.yaml', 'tacker/extensions/vnfm.py', 'tacker/nfvo/drivers/vnffg/vnffg_scaling/vnffg_scaling.py', 'tacker/extensions/nfvo.py', 'tacker/vnfm/policy_actions/notify/notify.py', 'tacker/nfvo/drivers/vnffg/vnffg_healing/vnffg_healing.py', 'tacker/vnfm/plugin.py', 'tacker/nfvo/drivers/vim/openstack_driver.py', 'tacker/nfvo/nfvo_plugin.py', 'tacker/vnfm/policy_actions/notify/__init__.py', 'tacker/conductor/conductorrpc/AutoScalingRPC.py', 'tacker/conductor/conductorrpc/AutoHealingRPC.py', 'tacker/nfvo/drivers/vnffg/vnffg_healing/__init__.py', 'setup.cfg', 'tacker/plugins/common/constants.py', 'tacker/conductor/conductor_server.py', 'etc/config-generator.conf', 'tacker/nfvo/drivers/vnffg/vnffg_scaling/__init__.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/820bdae219fb027c5166d08d767c634711dcb76f', 'message': '[WIP] Enable auto-healing and scaling for VNFFG\n\nCurrently, VNFFG is orchestrated by the combination of VNFs i Tacker.\nIn lower layer, Tacker used Neutron SFC to chain VNFs via their connection\npoints. In case the VNFs are broken, it leads to the failure of the chains.\nIn this blueprint, tacker conductor is used to emit failure events from VNFM\nto NFVO so that VNFFG can recognize.\n\nImplement blueprint: #vnffg-healing\nImplement blueprint: #vnffg-scaling\n\nCo-Authored-By: Hoang Phuoc <hongphuocbk2.07@gmail.com>\n\nChange-Id: I14b5af8a170c33728c13a3aa270ada32f7b186c9\n'}]",0,495748,820bdae219fb027c5166d08d767c634711dcb76f,18,2,7,20560,,,0,"[WIP] Enable auto-healing and scaling for VNFFG

Currently, VNFFG is orchestrated by the combination of VNFs i Tacker.
In lower layer, Tacker used Neutron SFC to chain VNFs via their connection
points. In case the VNFs are broken, it leads to the failure of the chains.
In this blueprint, tacker conductor is used to emit failure events from VNFM
to NFVO so that VNFFG can recognize.

Implement blueprint: #vnffg-healing
Implement blueprint: #vnffg-scaling

Co-Authored-By: Hoang Phuoc <hongphuocbk2.07@gmail.com>

Change-Id: I14b5af8a170c33728c13a3aa270ada32f7b186c9
",git fetch https://review.opendev.org/openstack/tacker refs/changes/48/495748/4 && git format-patch -1 --stdout FETCH_HEAD,"['tacker/nfvo/drivers/vnffg/absttract_vnffg_policies.py', 'samples/tosca-templates/vnffgd/tosca-vnffg-vnfd2-alarm-respawn.yaml', 'samples/tosca-templates/vnffgd/tosca-vnffg-vnfd1-alarm-respawn.yaml', 'tacker/nfvo/drivers/vnffg/vnffg_scaling/vnffg_scaling.py', 'tacker/vnfm/policy_actions/notify/notify.py', 'tacker/nfvo/drivers/vnffg/vnffg_healing/vnffg_healing.py', 'tacker/vnfm/plugin.py', 'tacker/nfvo/drivers/vim/openstack_driver.py', 'tacker/nfvo/nfvo_plugin.py', 'tacker/vnfm/policy_actions/notify/__init__.py', 'tacker/conductor/conductorrpc/AutoScalingRPC.py', 'tacker/conductor/conductorrpc/AutoHealingRPC.py', 'tacker/nfvo/drivers/vnffg/vnffg_healing/__init__.py', 'setup.cfg', 'tacker/conductor/conductor_server.py', 'etc/config-generator.conf', 'tacker/nfvo/drivers/vnffg/vnffg_scaling/__init__.py']",17,87cb147b3da51d8326003dc8270382523c438a45,tacker/vnffg-healing-scaling,,,619,5
openstack%2Fzun~master~Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18,openstack/zun,master,Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18,Nova Integration Spec,NEW,2016-08-12 07:09:15.000000000,2017-12-18 01:36:37.000000000,,"[{'_account_id': 5638}, {'_account_id': 9775}, {'_account_id': 10206}, {'_account_id': 11536}, {'_account_id': 12175}, {'_account_id': 12297}, {'_account_id': 12407}, {'_account_id': 13248}, {'_account_id': 16277}, {'_account_id': 19457}, {'_account_id': 21785}]","[{'number': 1, 'created': '2016-08-12 07:09:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/aa5a1d2311b77453408213579e0bc821d1afbcfb', 'message': 'Nova Integration Spec\n\nThis specification proposes to add a virt driver for Nova  to enable Nova\nto deploy container resources on Openstack ecosystem using Container service\nfor Openstack, Zun.\n\nChange-Id: Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18\nCo-Authored-By:Madhuri Kumari\nPartial-implements:blueprint nova-integration\n'}, {'number': 2, 'created': '2016-08-12 07:56:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/819419bd06c5bee17a087a90102be39cce1ac650', 'message': 'Nova Integration Spec\n\nThis specification proposes to add a virt driver for Nova  to enable Nova\nto deploy container resources on Openstack ecosystem using Container service\nfor Openstack, Zun.\n\nChange-Id: Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18\nCo-Authored-By:Madhuri Kumari\nPartial-implements:blueprint nova-integration\n'}, {'number': 3, 'created': '2016-08-12 08:01:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/39c32fd05c6d1cdcbc903d86e46b0c275f51480d', 'message': 'Nova Integration Spec\n\nThis specification proposes to add a virt driver for Nova  to enable Nova\nto deploy container resources on Openstack ecosystem using Container service\nfor Openstack, Zun.\n\nChange-Id: Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18\nCo-Authored-By:Madhuri Kumari\nPartial-implements:blueprint nova-integration\n'}, {'number': 4, 'created': '2016-08-12 08:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/7ac1dd6e3abe44d3e641e3f49392b183502bee87', 'message': 'Nova Integration Spec\n\nThis specification proposes to add a virt driver for Nova  to enable Nova\nto deploy container resources on Openstack ecosystem using Container service\nfor Openstack, Zun.\n\nChange-Id: Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18\nCo-Authored-By:Madhuri Kumari\nPartial-implements:blueprint nova-integration\n'}, {'number': 5, 'created': '2016-08-12 08:12:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/814d2b5ea557216c6f4ab6574e3d166fdf7a69bb', 'message': 'Nova Integration Spec\n\nThis specification proposes to add a virt driver for Nova  to enable\nNova to deploy container resources on Openstack ecosystem using\nContainer service for Openstack,Zun.\n\nCo-Authored-By: Madhuri Kumari <madhuri.kumari@intel.com>\n\nChange-Id: Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18\nPartial-implements:blueprint nova-integration\n'}, {'number': 6, 'created': '2016-09-05 19:06:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/cd10b766c5b8c7bcda1cb6dc9670c38f50e45196', 'message': 'Nova Integration Spec\n\nThis specification proposes to add a virt driver for Nova  to enable\nNova to deploy container resources on Openstack ecosystem using\nContainer service for Openstack,Zun.\n\nCo-Authored-By: Madhuri Kumari <madhuri.kumari@intel.com>\n\nChange-Id: Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18\nPartial-implements:blueprint nova-integration\n'}, {'number': 7, 'created': '2016-09-05 19:21:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/d1b3a2864afc6527830e03bafd3acc953a6ec859', 'message': 'Nova Integration Spec\n\nThis specification proposes to add a virt driver for Nova  to enable\nNova to deploy container resources on Openstack ecosystem using\nContainer service for Openstack,Zun.\n\nCo-Authored-By: Madhuri Kumari <madhuri.kumari@intel.com>\n\nChange-Id: Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18\nPartial-implements:blueprint nova-integration\n'}, {'number': 8, 'created': '2016-09-12 08:27:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/d70896c72253aa9a0e3de48f59a3bb72ed61317b', 'message': 'Nova Integration Spec\n\nThis specification proposes to add a virt driver for Nova  to enable\nNova to deploy container resources on Openstack ecosystem using\nContainer service for Openstack,Zun.\n\nCo-Authored-By: Madhuri Kumari <madhuri.kumari@intel.com>\n\nChange-Id: Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18\nPartial-implements:blueprint nova-integration\n'}, {'number': 9, 'created': '2016-09-26 17:37:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/02258582f3ea238dbee67797efdd7b4110778680', 'message': 'Nova Integration Spec\n\nThis specification proposes to add a virt driver for Nova  to enable\nNova to deploy container resources on Openstack ecosystem using\nContainer service for Openstack,Zun.\n\nCo-Authored-By: Madhuri Kumari <madhuri.kumari@intel.com>\n\nChange-Id: Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18\nPartial-implements:blueprint nova-integration\n'}, {'number': 10, 'created': '2016-10-13 10:12:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/25c626fe94be3d53ac86a54dbff2d8ca7627205c', 'message': 'Nova Integration Spec\n\nThis specification proposes to add a virt driver for Nova  to enable\nNova to deploy container resources on Openstack ecosystem using\nContainer service for Openstack,Zun.\n\nCo-Authored-By: Madhuri Kumari <madhuri.kumari@intel.com>\n\nChange-Id: Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18\nPartial-implements:blueprint nova-integration\n'}, {'number': 11, 'created': '2016-11-11 17:24:57.000000000', 'files': ['specs/nova-integration.rst'], 'web_link': 'https://opendev.org/openstack/zun/commit/5fc05e5e7000dd886bec626d0906f07f22b1ce32', 'message': 'Nova Integration Spec\n\nThis specification proposes to add a virt driver for Nova  to enable\nNova to deploy container resources on Openstack ecosystem using\nContainer service for Openstack,Zun.\n\nCo-Authored-By: Madhuri Kumari <madhuri.kumari@intel.com>\n\nChange-Id: Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18\nPartial-implements:blueprint nova-integration\n'}]",51,354553,5fc05e5e7000dd886bec626d0906f07f22b1ce32,46,11,11,21785,,,0,"Nova Integration Spec

This specification proposes to add a virt driver for Nova  to enable
Nova to deploy container resources on Openstack ecosystem using
Container service for Openstack,Zun.

Co-Authored-By: Madhuri Kumari <madhuri.kumari@intel.com>

Change-Id: Ib78ab4a9c3ddb7d3b5e5673eaf184fe21292dc18
Partial-implements:blueprint nova-integration
",git fetch https://review.opendev.org/openstack/zun refs/changes/53/354553/10 && git format-patch -1 --stdout FETCH_HEAD,['specs/nova-integration.rst'],1,aa5a1d2311b77453408213579e0bc821d1afbcfb,bp/nova-integration," Nova Integration Spec ==================== This specification proposes to add a virt driver for Nova to enable Nova to deploy container resources on Openstack ecosystem using Container service for Openstack, Zun. Problem description ================= The community has split out the functionality of provisioning containers into a seperate program,which includes the nova-docker project. But nova-docker is deadnow. Later Openstack community came up with Magnum project with an intent to enable container as a first class resource in Openstack. But eventually went in different direction and now it is managing infrastructure to run containers. So wasZun evolved from Magnum to support containers as good as Nova does for VM. The orignal intent of zun.virt.nova driver is to deploy containers on using NovaAPIs same as we do for baremetal provisioning from Ironic. Proposed Change ============== This proposal aims to enable Nova to create containers via zun.virt.nova.This abstracts the details of physical hardware within Zun,such that user interacts with Nova in the same way when deploying instances to virtual or physical machines.The hardware-specific details are only exposed to cloud operator. Specifically, this will: Add the zun.virt.nova driver, which will use the python-zunclient library to interact with Zun’s REST API for the purpose of provisioning container. This driver will initially implement a subset of the Nova virt driver API which suits container usecase also. Example boot, delete, start, stop, pause, unpause,reboot. This driver will expose the complete resources of the zun service it is connected to. Data Model impact =============== Adding the zun.virt.nova driver will not impact the db model. REST API impact ============== The zun.virt.nova driver will not add any REST API extensions or require changes to any of Zun's APIs. Security Impact =============== None Notifications Impact ==================== None Other end user impact ==================== None Performance Impact ================== No imapct on Zun itself. The performance profile of the zun.virt nova driver will be different than other virt drivers due to the nature of managing container which is pretty much fast than VMs. Other deployer impact ==================== Deploying Nova with the zun.virt.nova driver will be considerably different to deploying Nova with other virt drivers. Main areas of difference are: Different system libraries will be required. No local hypervisor needs be installed. Required container runtime tool like docker, rocket etc needs to be installed on the compute host itself. the Zun Container services must be properly set up and discoverable via Keystone in order for the zun.virt.nova driver to function properly. Nova must be supplied with admin credentials capable of interacting with Zun. Implementation ============= This virt driver will be implemented in Zun tree and will be used when Nova is installed with ""compute_driver = zun.virt.nova"". This library will be loaded at runtime when any Nova API call is made to operate containers. Since Zun also will aslo have its scheduler. There are two implementations that can be induced. Implementation #1: Considering Nova scheduler In this implementation, host information will be provided by Nova scheduler. Implementation #2: Considering Zun scheduler In this implementation. host information will be provided by Zun scheduler. In first phase, we will go with implementation #1 as we dont have zun scheduler. Assignee(s) ========== Primary assignee: Namrata ",,80,0
openstack%2Fstorlets~master~Ia4b39e363e45915ea195d3018b864f841e6deccf,openstack/storlets,master,Ia4b39e363e45915ea195d3018b864f841e6deccf,Java SDaemon to log to host syslog,NEW,2017-01-17 14:50:37.000000000,2017-12-18 01:35:53.000000000,,"[{'_account_id': 9816}, {'_account_id': 11317}]","[{'number': 1, 'created': '2017-01-17 14:50:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/storlets/commit/29b85dda3f5cb0db97c9d411811b722f362ef8a6', 'message': 'Java SDaemon to log to host syslog\n\nUnlike Python, Java does not have a log appender that works with\nunix domain sockets. Thus, so far we were not able to get the SDaemon\nlogs to the hosts syslog.\nThis has caused a lot of problems debugging SDaemon. Not to mention\nit does not allow to monitor per storlet resource usage for billing.\n\nThis patch fixes the problem by using a small JNI class that wraps\nthe ""C"" runtime syslog library. And so we replace sl4j with syslog\ngetting rid from some of our Java dependencies.\n\nChange-Id: Ia4b39e363e45915ea195d3018b864f841e6deccf\n'}, {'number': 2, 'created': '2017-03-01 08:00:55.000000000', 'files': ['src/java/SLog/src/main/org/openstack/storlet/slog/SLogJNI.java', 'tests/unit/agent/daemon_factory/test_manager.py', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SHaltTask.java', 'install/storlets/roles/docker_storlet_engine_image/tasks/main.yml', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SExecutionTask.java', 'install/storlets/roles/docker_storlet_engine_image/templates/ubuntu_16.04_jre8_storlets_Dockerfile', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/STaskFactory.java', 'src/java/SDaemon/build.xml', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SDescriptorTask.java', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SDaemon.java', 'install/storlets/roles/docker_base_jre_image/tasks/ubuntu_16.04_jre8.yml', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SAbstractTask.java', 'install/storlets/roles/docker_storlet_engine_image/files/logback.xml', 'src/java/SLog/src/main/org/openstack/storlet/slog/Logger.java', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SPingTask.java', 'doc/source/engine_dev_installation.rst', 'src/java/SLog/SLogJNI.c', 'src/java/SCommon/build.xml', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SCancelTask.java', 'src/c/slog/slog.c', 'src/c/slog/slog.h', 'storlets/agent/daemon_factory/manager.py', 'src/java/SLog/org_openstack_storlet_slog_SLogJNI.h', 'src/java/build.xml', 'src/java/SLog/build.xml', 'install/storlets/roles/docker_base_jre_image/templates/ubuntu_16.04_jre8_Dockerfile'], 'web_link': 'https://opendev.org/openstack/storlets/commit/95971ec54476a68a0b1f45c2badf5ccbdb89cbdb', 'message': 'Java SDaemon to log to host syslog\n\nUnlike Python, Java does not have a log appender that works with\nunix domain sockets. Thus, so far we were not able to get the SDaemon\nlogs to the hosts syslog.\nThis has caused a lot of problems debugging SDaemon. Not to mention\nit does not allow to monitor per storlet resource usage for billing.\n\nThis patch fixes the problem by using a small JNI class that wraps\nthe ""C"" runtime syslog library. And so we replace sl4j with syslog\ngetting rid from some of our Java dependencies.\n\nChange-Id: Ia4b39e363e45915ea195d3018b864f841e6deccf\n'}]",0,421303,95971ec54476a68a0b1f45c2badf5ccbdb89cbdb,9,2,2,11317,,,0,"Java SDaemon to log to host syslog

Unlike Python, Java does not have a log appender that works with
unix domain sockets. Thus, so far we were not able to get the SDaemon
logs to the hosts syslog.
This has caused a lot of problems debugging SDaemon. Not to mention
it does not allow to monitor per storlet resource usage for billing.

This patch fixes the problem by using a small JNI class that wraps
the ""C"" runtime syslog library. And so we replace sl4j with syslog
getting rid from some of our Java dependencies.

Change-Id: Ia4b39e363e45915ea195d3018b864f841e6deccf
",git fetch https://review.opendev.org/openstack/storlets refs/changes/03/421303/1 && git format-patch -1 --stdout FETCH_HEAD,"['src/java/SLog/src/main/org/openstack/storlet/slog/SLogJNI.java', 'tests/unit/agent/daemon_factory/test_manager.py', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SHaltTask.java', 'install/storlets/roles/docker_storlet_engine_image/tasks/main.yml', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SExecutionTask.java', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/STaskFactory.java', 'src/java/SDaemon/build.xml', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SDescriptorTask.java', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SDaemon.java', 'install/storlets/roles/docker_base_jre_image/tasks/ubuntu_14.04_jre8.yml', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SAbstractTask.java', 'install/storlets/roles/docker_storlet_engine_image/files/logback.xml', 'src/java/SLog/src/main/org/openstack/storlet/slog/Logger.java', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SPingTask.java', 'install/storlets/roles/docker_storlet_engine_image/templates/ubuntu_14.04_jre8_storlets_Dockerfile', 'doc/source/engine_dev_installation.rst', 'src/java/SLog/SLogJNI.c', 'src/java/SCommon/build.xml', 'src/java/SDaemon/src/main/org/openstack/storlet/daemon/SCancelTask.java', 'src/c/slog/slog.c', 'src/c/slog/slog.h', 'storlets/agent/daemon_factory/manager.py', 'src/java/SLog/org_openstack_storlet_slog_SLogJNI.h', 'src/java/build.xml', 'src/java/SLog/build.xml', 'install/storlets/roles/docker_base_jre_image/templates/ubuntu_14.04_jre8_Dockerfile']",26,29b85dda3f5cb0db97c9d411811b722f362ef8a6,sdaemon_slog,"COPY [""json_simple-1.1.jar"", ""/opt/storlets/""] RUN [""chmod"", ""0744"", ""/opt/storlets/json_simple-1.1.jar""]","COPY [""logback-classic-1.1.2.jar"", ""logback-core-1.1.2.jar"", ""slf4j-api-1.7.7.jar"", ""json_simple-1.1.jar"", ""/opt/storlets/""] RUN [""chmod"", ""0744"", ""/opt/storlets/logback-classic-1.1.2.jar"", ""/opt/storlets/logback-core-1.1.2.jar"", ""/opt/storlets/slf4j-api-1.7.7.jar"", ""/opt/storlets/json_simple-1.1.jar""]",383,122
openstack%2Frally~master~I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b,openstack/rally,master,I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b,Add CreateAndListPools [lbaasv2],NEW,2017-06-09 03:41:48.000000000,2017-12-18 01:35:42.000000000,,"[{'_account_id': 6172}, {'_account_id': 8871}, {'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 21528}, {'_account_id': 22960}, {'_account_id': 26136}]","[{'number': 1, 'created': '2017-06-09 03:41:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e0370230bd402533cecc727a2ac57b9a12633835', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 2, 'created': '2017-06-09 07:10:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/196938ef537cae67bc6fae18141938405b2f9ab6', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 3, 'created': '2017-06-19 12:33:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c2322ac99585b27435378d01fdc094b113407702', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 4, 'created': '2017-06-19 13:22:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/9af86644a3be2b2b418284def2a293f0ce8b08a3', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 5, 'created': '2017-06-26 09:04:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0b8268b71b8a44e3ee3f0f653d888669acf047c4', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 6, 'created': '2017-06-26 09:14:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ef31618e3e233fc7ac39db78f75973b0dd42f495', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 7, 'created': '2017-06-26 11:08:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0ad11310de2eaef1eadae965b2dbfd0a50eaf55b', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 8, 'created': '2017-06-26 11:59:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c4754d0a1c2d2aa5c7007eb6d94165c72f286737', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 9, 'created': '2017-06-26 15:02:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4309b16718b6c526d06fe2e7859a141a9c4bf0bd', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 10, 'created': '2017-06-27 00:39:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1b410fe741277abb7be8bad98e83ed06214ef679', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 11, 'created': '2017-06-27 11:29:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/7ff8ddd850c8186e20b813f35ded5d07be78e79f', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 12, 'created': '2017-06-27 14:21:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/32e7595cc56a841b13744c74690cc57f44848525', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 13, 'created': '2017-06-28 01:13:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/96802e2eaf189060ee75a5a7bdd7a63c7410302f', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 14, 'created': '2017-06-28 03:39:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/594c5f56db0118bec76644074eb0585f86a6cafc', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 15, 'created': '2017-06-28 04:19:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1aff351af691e69420f15b6f132e036ad7dfd994', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 16, 'created': '2017-06-28 06:46:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/36f3b4114943a8519f07c0b9b23ba4b72b1541e0', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 17, 'created': '2017-06-28 08:18:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1ede39337b50b870d9866e33c2574c4c18e3ac36', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 18, 'created': '2017-06-28 09:51:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/30f126a2711505a20e8a526395316b7ae6bb0271', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 19, 'created': '2017-06-28 12:36:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ef61533b59346c8009d6fcf054433590d8a0e74e', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 20, 'created': '2017-06-28 14:30:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/eb1d2d1b19ac0793824a1eb5b382c28d70e14f20', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 21, 'created': '2017-06-28 15:58:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d674b0b6f3a2d87374bbc33395d16ef9a1915034', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 22, 'created': '2017-06-29 00:16:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c7a7103f8ae625ae173a052f39e1db86febcb9b9', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 23, 'created': '2017-06-29 04:17:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/97762e4cb2379af64266b70871813d72a92f0a0f', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 24, 'created': '2017-06-29 05:20:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/aa264f4816274e802c48230e69ffadfa358f836a', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 25, 'created': '2017-07-03 15:04:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/aa14c255eaf2ec97ea71e32840712e724fc494dc', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 26, 'created': '2017-07-03 15:29:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/9c50647373840402cf3faccc7915397ed5dde176', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 27, 'created': '2017-07-04 14:53:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/655a9c17985b13b2a589a9d072a15cb223bd1cb5', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 28, 'created': '2017-07-05 01:59:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/7548e3d691221035ee17fbe793b9ea772fc4426a', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 29, 'created': '2017-07-06 14:41:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/cf6252c7bfcf4fce12d8c712e7366c50aed3b556', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 30, 'created': '2017-07-06 14:47:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8740d9cf0f5b8f5681373cf3f2ccaf4ee36dae68', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 31, 'created': '2017-07-06 22:29:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c33d4a2a72402f664a0c857dd9619ccb5136e7fb', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}, {'number': 32, 'created': '2017-07-07 00:22:28.000000000', 'files': ['rally/plugins/openstack/cfg/neutron.py', 'samples/tasks/scenarios/neutron/create-and-list-lbaasv2-pools.json', 'tests/unit/plugins/openstack/scenarios/neutron/test_loadbalancer_v2.py', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'samples/tasks/scenarios/neutron/create-and-list-lbaasv2-pools.yaml', 'rally/plugins/openstack/scenarios/neutron/loadbalancer_v2.py', 'rally-jobs/rally-neutron-extensions.yaml', 'tests/unit/plugins/openstack/scenarios/neutron/test_utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/433854c5deb15e76868e429a490e41a0708ac1b3', 'message': 'Add CreateAndListPools [lbaasv2]\n\nChange-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b\n'}]",7,472510,433854c5deb15e76868e429a490e41a0708ac1b3,143,7,32,26136,,,0,"Add CreateAndListPools [lbaasv2]

Change-Id: I5c7f3ed50fdefb0ef1a004878d4fe2227fbddf2b
",git fetch https://review.opendev.org/openstack/rally refs/changes/10/472510/26 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/openstack/cfg/neutron.py', 'samples/tasks/scenarios/neutron/create-and-list-lbaasv2-pools.json', 'rally/plugins/openstack/scenarios/neutron/utils.py', 'samples/tasks/scenarios/neutron/create-and-list-lbaasv2-pools.yaml', 'rally/plugins/openstack/scenarios/neutron/loadbalancer_v2.py']",5,e0370230bd402533cecc727a2ac57b9a12633835,," @validation.add(""required_neutron_extensions"", extensions=[""lbaasv2""]) @validation.add(""required_services"", services=[consts.Service.NEUTRON]) @validation.add(""required_platform"", platform=""openstack"", users=True) @validation.add(""required_contexts"", contexts=(""network"")) @scenario.configure(context={""cleanup"": [""neutron""]}, name=""NeutronLoadbalancerV2.create_and_list_pools"") class CreateAndListPools(utils.NeutronScenario): def run(self, network_create_args=None, subnet_create_args=None, lb_create_args=None, listener_create_args=None, pool_create_args=None): """"""Create a loadbalancers pools (v2) and then list loadbalancers pools(v2). Measure the ""neutron lbaas-pool-list"" command performance. The scenario creates a loadbalancer pool for every loadbalancer and then lists pools. :param lb_create_args: dict, POST /lbaas/pools request options """""" lb_create_args = lb_create_args or {} listener_create_args = listener_create_args or {} pool_create_args = pool_create_args or {} subnets = [] networks = self.context.get(""tenant"", {}).get(""networks"", []) for network in networks: subnets.extend(network.get(""subnets"", [])) for subnet_id in subnets: lb = self._create_lbaasv2_loadbalancer(subnet_id, **lb_create_args) lb_id = lb['id'] listener = self._create_lbaasv2_listener(lb_id, **listener_create_args) listener_id = listener['listener']['id'] self._create_lbaasv2_pool(listener_id, **pool_create_args) self._list_lbaasv2_pools()",,150,1
openstack%2Frally~master~I67bfc3136205f7546df97ba04f0d315b17f5befd,openstack/rally,master,I67bfc3136205f7546df97ba04f0d315b17f5befd,Add nova.BootServerAttachVolumeAndUpdateAttachment scenario,NEW,2017-01-10 01:32:34.000000000,2017-12-18 01:35:25.000000000,,"[{'_account_id': 6172}, {'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 22348}, {'_account_id': 22960}, {'_account_id': 23435}, {'_account_id': 26576}]","[{'number': 1, 'created': '2017-01-10 01:32:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/57d8757a0186b6921e76c19798eebf45a6e68e10', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 2, 'created': '2017-03-31 02:29:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6d2b9f83abafcc542af6591efcfa7c79216fc6fe', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 3, 'created': '2017-07-20 08:22:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/fed347a7882971c7b4ba780fda449a69cd099ddb', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 4, 'created': '2017-07-20 10:32:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ce6d0e1d1596fa985e90f6c48b7f475ffe1cfeb1', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 5, 'created': '2017-07-21 01:28:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/074a0e1543f677ea19bc614b26288a8a070ef5b7', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 6, 'created': '2017-07-24 11:11:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/95b1ba1139335e93f16d706a730b2d2b7101f2f0', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 7, 'created': '2017-07-25 07:43:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6ea7fe0fe6d3360d0109fc4e0569dab3819fd209', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 8, 'created': '2017-08-02 08:30:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/caf8b0b26e582c7432c3be992d0fcba9cbe07972', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 9, 'created': '2017-08-15 09:43:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8c67523a1e739912c185f5ebbccd1fa5c1454fa1', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 10, 'created': '2017-08-21 12:05:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/41cbdac5730e7792df68826dce7053c81c4c6464', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 11, 'created': '2017-09-15 02:57:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d567e112751dc59e1beb08a8d239d16acf0910e1', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 12, 'created': '2017-09-15 06:01:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8920066d60a803bd5199e8f013b730119f4e0a05', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 13, 'created': '2017-09-15 08:37:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ff90fb9f910669f6153a1dd22a355757b1bc644f', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}, {'number': 14, 'created': '2017-09-17 15:42:51.000000000', 'files': ['rally/plugins/openstack/scenarios/nova/utils.py', 'samples/tasks/scenarios/nova/boot-server-attach-volume-and-update-attachment.json', 'rally/plugins/openstack/scenarios/nova/servers.py', 'tests/unit/plugins/openstack/scenarios/nova/test_servers.py', 'rally-jobs/nova.yaml', 'samples/tasks/scenarios/nova/boot-server-attach-volume-and-update-attachment.yaml', 'tests/unit/plugins/openstack/scenarios/nova/test_utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/e40c87916399c2ffd50bde0db6fd3902416f480e', 'message': 'Add nova.BootServerAttachVolumeAndUpdateAttachment scenario\n\nthe scenario first create a VM and two volume, one is used to\nattach, the other is used to update server\'s attachment.\nFinally detach the volume and delete volume and VM.\n\nMeasure the ""nova volume-update"" command performance.\n\nChange-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd\n'}]",20,418171,e40c87916399c2ffd50bde0db6fd3902416f480e,55,7,14,23435,,,0,"Add nova.BootServerAttachVolumeAndUpdateAttachment scenario

the scenario first create a VM and two volume, one is used to
attach, the other is used to update server's attachment.
Finally detach the volume and delete volume and VM.

Measure the ""nova volume-update"" command performance.

Change-Id: I67bfc3136205f7546df97ba04f0d315b17f5befd
",git fetch https://review.opendev.org/openstack/rally refs/changes/71/418171/3 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/openstack/scenarios/nova/utils.py', 'samples/tasks/scenarios/nova/boot-server-attach-volume-and-update-attachment.json', 'rally/plugins/openstack/scenarios/nova/servers.py', 'rally-jobs/nova.yaml', 'tests/unit/plugins/openstack/scenarios/nova/test_servers.py', 'samples/tasks/scenarios/nova/boot-server-attach-volume-and-update-attachment.yaml', 'tests/unit/plugins/openstack/scenarios/nova/test_utils.py']",7,57d8757a0186b6921e76c19798eebf45a6e68e10,nova.BootServerAttachVolumeAndUpdateAttachment," def test_update_attachment(self): fake_attach = mock.MagicMock() expect_attach = mock.MagicMock() new_volume = mock.MagicMock() server = mock.MagicMock() (self.admin_clients(""nova"").volumes.update_server_volume .return_value) = expect_attach nova_scenario = utils.NovaScenario(context=self.context) result_attach = nova_scenario._update_attachment(server, fake_attach, new_volume) self.assertEqual(expect_attach, result_attach) (self.admin_clients(""nova"").volumes.update_server_volume .assert_called_once_with(server.id, fake_attach.id, new_volume.id)) self._test_atomic_action_timer(nova_scenario.atomic_actions(), ""nova.update_attachment"") ",,223,0
openstack%2Frally~master~I442abbe6a9604cc91d5339bc5621c0dc4e55fa00,openstack/rally,master,I442abbe6a9604cc91d5339bc5621c0dc4e55fa00,WIP: Gracefull shutdown on CTRL+C or SIGINT,NEW,2015-10-13 08:35:55.000000000,2017-12-18 01:35:06.000000000,,"[{'_account_id': 6172}, {'_account_id': 7227}, {'_account_id': 8491}, {'_account_id': 10475}, {'_account_id': 11748}, {'_account_id': 13609}, {'_account_id': 14817}, {'_account_id': 22348}]","[{'number': 1, 'created': '2015-10-13 08:35:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c8d08545e7f3e932c2dd472546492e0eeaf97487', 'message': 'Gracefull shutdown on CTRL+C or SIGINT\n\nWe are changing the default signal handler for sigint during the\ntime of execution of Engine.run().\n\nIf signal occures task.abort() is called.\n\nThere is a bit of complexity related to the fact that signal handler\nis called in each process and we should call it only once. To achive\nthat we are using shared int (which is used as detector of whatever\nmethod is called or not) and Lock instance shared between all processes\n\nChange-Id: I442abbe6a9604cc91d5339bc5621c0dc4e55fa00\nCloses-bug: #1326051\n'}, {'number': 2, 'created': '2016-03-30 08:04:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/7888e56088474b1bc4445726dbe5a0a7860b4614', 'message': '[wip] Gracefull shutdown on CTRL+C or SIGINT\n\nWe are changing the default signal handler for sigint during the\ntime of execution of Engine.run().\n\nIf signal occures task.abort() is called.\n\nThere is a bit of complexity related to the fact that signal handler\nis called in each process and we should call it only once. To achive\nthat we are using shared int (which is used as detector of whatever\nmethod is called or not) and Lock instance shared between all processes\n\nChange-Id: I442abbe6a9604cc91d5339bc5621c0dc4e55fa00\nCloses-bug: #1326051\n'}, {'number': 3, 'created': '2017-10-11 07:12:02.000000000', 'files': ['rally/task/engine.py', 'rally/common/graceful.py', 'tests/unit/common/test_graceful.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/9b61446c6f8223be9d1a6db2cefeca55ade4926a', 'message': 'WIP: Gracefull shutdown on CTRL+C or SIGINT\n\nWe are changing the default signal handler for sigint during the\ntime of execution of Engine.run().\n\nIf signal occures task.abort() is called.\n\nThere is a bit of complexity related to the fact that signal handler\nis called in each process and we should call it only once. To achive\nthat we are using shared int (which is used as detector of whatever\nmethod is called or not) and Lock instance shared between all processes\n\nTODO:\n- Improve Unit Tests\n- Add Functional Tests\n\nChange-Id: I442abbe6a9604cc91d5339bc5621c0dc4e55fa00\nCloses-bug: #1326051\n'}]",6,234031,9b61446c6f8223be9d1a6db2cefeca55ade4926a,18,8,3,6172,,,0,"WIP: Gracefull shutdown on CTRL+C or SIGINT

We are changing the default signal handler for sigint during the
time of execution of Engine.run().

If signal occures task.abort() is called.

There is a bit of complexity related to the fact that signal handler
is called in each process and we should call it only once. To achive
that we are using shared int (which is used as detector of whatever
method is called or not) and Lock instance shared between all processes

TODO:
- Improve Unit Tests
- Add Functional Tests

Change-Id: I442abbe6a9604cc91d5339bc5621c0dc4e55fa00
Closes-bug: #1326051
",git fetch https://review.opendev.org/openstack/rally refs/changes/31/234031/1 && git format-patch -1 --stdout FETCH_HEAD,['rally/task/engine.py'],1,c8d08545e7f3e932c2dd472546492e0eeaf97487,bug/1326051,"import signalimport multiprocessing import decorator@decorator.decorator def gracefull_shutdown(f, self, *args, **kwargs): counter = multiprocessing.Value(""I"", 0) def signal_handler(signal, frame): with self.lock: if counter.value == 0: counter.value += 1 LOG.info(""Task %s is aborted!"" % self.task[""uuid""]) self.task.abort() counter.value += 1 original_signal_handler = signal.getsignal(signal.SIGINT) signal.signal(signal.SIGINT, signal_handler) try: return f(self, *args, **kwargs) finally: signal.signal(signal.SIGINT, original_signal_handler) self.lock = multiprocessing.Lock() @gracefull_shutdown",,26,0
openstack%2Fmasakari~master~I6920e05c365efb9ae8752b8f6ce1fa7483905c7a,openstack/masakari,master,I6920e05c365efb9ae8752b8f6ce1fa7483905c7a,Update log translations hacking rule,NEW,2017-07-10 02:38:34.000000000,2017-12-18 01:33:46.000000000,,"[{'_account_id': 8988}, {'_account_id': 19554}, {'_account_id': 22255}, {'_account_id': 22348}, {'_account_id': 25267}, {'_account_id': 26072}]","[{'number': 1, 'created': '2017-07-10 02:38:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/masakari/commit/8bc3cd5f7d0f64ded046ee7bed6627f160310b51', 'message': 'Update log translations hacking rule\n\nStarting with the Pike series, OpenStack no longer supports log\ntranslation.\nUpdate rule to prevent translation at all log levels instead of only\ndebug level.\n\nChange-Id: I6920e05c365efb9ae8752b8f6ce1fa7483905c7a\n'}, {'number': 2, 'created': '2017-09-17 03:17:11.000000000', 'files': ['masakari/tests/unit/test_hacking.py', 'tox.ini', 'masakari/hacking/checks.py', 'HACKING.rst'], 'web_link': 'https://opendev.org/openstack/masakari/commit/e6b414ae2397e58bb204e0bd368fc8fa4d824801', 'message': 'Update log translations hacking rule\n\nStarting with the Pike series, OpenStack no longer supports log\ntranslation.\nUpdate rule to prevent translation at all log levels instead of only\ndebug level.\n\nChange-Id: I6920e05c365efb9ae8752b8f6ce1fa7483905c7a\n'}]",1,481985,e6b414ae2397e58bb204e0bd368fc8fa4d824801,16,6,2,25720,,,0,"Update log translations hacking rule

Starting with the Pike series, OpenStack no longer supports log
translation.
Update rule to prevent translation at all log levels instead of only
debug level.

Change-Id: I6920e05c365efb9ae8752b8f6ce1fa7483905c7a
",git fetch https://review.opendev.org/openstack/masakari refs/changes/85/481985/2 && git format-patch -1 --stdout FETCH_HEAD,"['masakari/tests/unit/test_hacking.py', 'tox.ini', 'masakari/hacking/checks.py', 'HACKING.rst']",4,8bc3cd5f7d0f64ded046ee7bed6627f160310b51,translated_logs,- [M308] Validate that logs are not translated.,- [M308] Validate that debug level logs are not translated.- [M314] Log messages require translations!,28,26
openstack%2Frally~master~I063b6da11cca43298c1e26b4d2e77b6c2e476f9c,openstack/rally,master,I063b6da11cca43298c1e26b4d2e77b6c2e476f9c,Speed Up load of Rally API for 300+ ms,NEW,2017-10-12 08:15:41.000000000,2017-12-18 01:33:35.000000000,,"[{'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-10-12 08:15:41.000000000', 'files': ['rally/cli/envutils.py', 'rally/plugins/openstack/validators.py', 'rally/verification/manager.py', 'rally/plugins/__init__.py', 'rally/plugins/openstack/wrappers/network.py', 'rally/plugins/openstack/cleanup/resources.py', 'rally/task/utils.py', 'rally/plugins/common/verification/testr.py', 'rally/plugins/openstack/scenario.py', 'rally/plugins/openstack/wrappers/keystone.py', 'rally/plugins/openstack/context/dataplane/heat.py', 'rally/plugins/openstack/context/magnum/ca_certs.py', 'rally/plugins/openstack/wrappers/glance.py', 'rally/plugins/openstack/services/image/glance_common.py', 'rally/common/utils.py', 'rally/plugins/openstack/scenarios/magnum/utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/9c0ab4ede9fabe14ba9f1aaec8d7923a0493af2a', 'message': 'Speed Up load of Rally API for 300+ ms\n\nDo not import on module level what is not used all the time:\n\n- OpenStack clients have really long time to import\n- K8 clinet is super slow 200+ ms\n- oslo_utils.str_utils are super slow 50ms +\n\nTODO: removing import of jsonschema will reduce 120-150ms more\n\nat the end instead of 700+ ms load is going to take about 250ms\n\nChange-Id: I063b6da11cca43298c1e26b4d2e77b6c2e476f9c\n'}]",2,511417,9c0ab4ede9fabe14ba9f1aaec8d7923a0493af2a,7,3,1,6172,,,0,"Speed Up load of Rally API for 300+ ms

Do not import on module level what is not used all the time:

- OpenStack clients have really long time to import
- K8 clinet is super slow 200+ ms
- oslo_utils.str_utils are super slow 50ms +

TODO: removing import of jsonschema will reduce 120-150ms more

at the end instead of 700+ ms load is going to take about 250ms

Change-Id: I063b6da11cca43298c1e26b4d2e77b6c2e476f9c
",git fetch https://review.opendev.org/openstack/rally refs/changes/17/511417/1 && git format-patch -1 --stdout FETCH_HEAD,"['rally/cli/envutils.py', 'rally/plugins/openstack/validators.py', 'rally/verification/manager.py', 'rally/plugins/__init__.py', 'rally/plugins/openstack/wrappers/network.py', 'rally/plugins/openstack/cleanup/resources.py', 'rally/task/utils.py', 'rally/plugins/common/verification/testr.py', 'rally/plugins/openstack/scenario.py', 'rally/plugins/openstack/wrappers/keystone.py', 'rally/plugins/openstack/context/dataplane/heat.py', 'rally/plugins/openstack/context/magnum/ca_certs.py', 'rally/plugins/openstack/wrappers/glance.py', 'rally/plugins/openstack/services/image/glance_common.py', 'rally/common/utils.py', 'rally/plugins/openstack/scenarios/magnum/utils.py']",16,9c0ab4ede9fabe14ba9f1aaec8d7923a0493af2a,speedup_imports, config = client.ConfigurationObject() return client.apis.core_v1_api.CoreV1Api( client.api_client.ApiClient(config=config)) from kubernetes.client import rest except rest.ApiException as e:,from kubernetes import client as k8s_config from kubernetes.client import api_client from kubernetes.client.apis import core_v1_api from kubernetes.client.rest import ApiException config = k8s_config.ConfigurationObject() client = api_client.ApiClient(config=config) return core_v1_api.CoreV1Api(client) except ApiException as e:,110,78
openstack%2Fstorlets~master~Ia6cb782d65111e1414c6b01bc6329f3fee3aa126,openstack/storlets,master,Ia6cb782d65111e1414c6b01bc6329f3fee3aa126,Doc: Add a new document describing existing swift changes,NEW,2017-01-24 07:32:32.000000000,2017-12-18 01:33:30.000000000,,[],"[{'number': 1, 'created': '2017-01-24 07:32:32.000000000', 'files': ['doc/source/deployer_swift_changes.rst', 'doc/source/index.rst'], 'web_link': 'https://opendev.org/openstack/storlets/commit/1b13054439b84fcd5cafea4eba9bff406c631d59', 'message': 'Doc: Add a new document describing existing swift changes\n\nA new document describes what changes are being done to an existing\nSwift installation when adding storlets\n\nChange-Id: Ia6cb782d65111e1414c6b01bc6329f3fee3aa126\n'}]",0,424483,1b13054439b84fcd5cafea4eba9bff406c631d59,4,0,1,11317,,,0,"Doc: Add a new document describing existing swift changes

A new document describes what changes are being done to an existing
Swift installation when adding storlets

Change-Id: Ia6cb782d65111e1414c6b01bc6329f3fee3aa126
",git fetch https://review.opendev.org/openstack/storlets refs/changes/83/424483/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/deployer_swift_changes.rst', 'doc/source/index.rst']",2,1b13054439b84fcd5cafea4eba9bff406c631d59,doc_swift_changes, deployer_swift_changes,,74,0
openstack%2Fdiskimage-builder~master~Id9ba43cbb43f2f2b3c2072b4f00f463ad91cc4a8,openstack/diskimage-builder,master,Id9ba43cbb43f2f2b3c2072b4f00f463ad91cc4a8,Easy DIB development: add dibdevel-run element,NEW,2017-03-12 20:08:43.000000000,2017-12-18 01:32:50.000000000,,"[{'_account_id': 7118}, {'_account_id': 10118}, {'_account_id': 21741}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-03-12 20:08:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/d5aece1d8941feb8174ae18167665679b15c6891', 'message': ""Easy DIB development: add dibdevel-run element\n\nThis is a first patch that tries to make setting up a development\nenvironment easy: When dibdevel-run is included, the created VM\ncontains all packages / setups that are needed to run\ndiskimage-builder.\n\nBecause there is currently no easy way of starting the images and\nusing them inside test cases, only the basic image creation test\ncases for the major distributions are included.\n\n(This is a split-off of patch\n'Easy diskimage-builder development environment setup'\nhttps://review.openstack.org/#/c/419655/)\n\nChange-Id: Id9ba43cbb43f2f2b3c2072b4f00f463ad91cc4a8\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}, {'number': 2, 'created': '2017-04-09 20:22:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/7619cab891f7424071186c046464f7561aaaa0b5', 'message': ""Easy DIB development: add dibdevel-run element\n\nThis is a first patch that tries to make setting up a development\nenvironment easy: When dibdevel-run is included, the created VM\ncontains all packages / setups that are needed to run\ndiskimage-builder.\n\nBecause there is currently no easy way of starting the images and\nusing them inside test cases, only the basic image creation test\ncases for the major distributions are included.\n\n(This is a split-off of patch\n'Easy diskimage-builder development environment setup'\nhttps://review.openstack.org/#/c/419655/)\n\nChange-Id: Id9ba43cbb43f2f2b3c2072b4f00f463ad91cc4a8\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}, {'number': 3, 'created': '2017-04-09 21:03:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/3d461a3c82bb9cee71a92f2c31273004d17f6316', 'message': ""Easy DIB development: add dibdevel-run element\n\nThis is a first patch that tries to make setting up a development\nenvironment easy: When dibdevel-run is included, the created VM\ncontains all packages / setups that are needed to run\ndiskimage-builder.\n\nBecause there is currently no easy way of starting the images and\nusing them inside test cases, only the basic image creation test\ncases for the major distributions are included.\n\n(This is a split-off of patch\n'Easy diskimage-builder development environment setup'\nhttps://review.openstack.org/#/c/419655/)\n\nChange-Id: Id9ba43cbb43f2f2b3c2072b4f00f463ad91cc4a8\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}, {'number': 4, 'created': '2017-06-21 17:54:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/add57baa925112c3468d27c26cef7ee4d8cfaf16', 'message': ""Easy DIB development: add dibdevel-run element\n\nThis is a first patch that tries to make setting up a development\nenvironment easy: When dibdevel-run is included, the created VM\ncontains all packages / setups that are needed to run\ndiskimage-builder.\n\nBecause there is currently no easy way of starting the images and\nusing them inside test cases, only the basic image creation test\ncases for the major distributions are included.\n\n(This is a split-off of patch\n'Easy diskimage-builder development environment setup'\nhttps://review.openstack.org/#/c/419655/)\n\nChange-Id: Id9ba43cbb43f2f2b3c2072b4f00f463ad91cc4a8\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}, {'number': 5, 'created': '2017-10-14 06:53:59.000000000', 'files': ['diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-fedora/element-deps', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-gentoo/environment.d/10-set-distro.bash', 'diskimage_builder/elements/dibdevel-run/releasenotes/notes/easy-development-setup-dibdevel-run-48c7119c61bbf858.yaml', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-gentoo/Readme.rst', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-gentoo/element-deps', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-debian/element-deps', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-ubuntu/environment.d/10-set-distro.bash', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-fedora/environment.d/10-set-distro.bash', 'diskimage_builder/elements/dibdevel-run/pkg-map', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-ubuntu/Readme.rst', 'diskimage_builder/elements/dibdevel-run/environment.d/05-set-environment', 'diskimage_builder/elements/dibdevel-run/Readme.rst', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-ubuntu/element-deps', 'diskimage_builder/elements/dibdevel-run/element-deps', 'diskimage_builder/elements/dibdevel-run/package-installs.yaml', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-fedora/Readme.rst', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-debian/Readme.rst', 'diskimage_builder/elements/dibdevel-run/pre-install.d/10-portage-adaptions', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-debian/environment.d/10-set-distro.bash'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/5b823f1e325de42696e21cdaf1d24d03491755b6', 'message': ""Easy DIB development: add dibdevel-run element\n\nThis is a first patch that tries to make setting up a development\nenvironment easy: When dibdevel-run is included, the created VM\ncontains all packages / setups that are needed to run\ndiskimage-builder.\n\nBecause there is currently no easy way of starting the images and\nusing them inside test cases, only the basic image creation test\ncases for the major distributions are included.\n\n(This is a split-off of patch\n'Easy diskimage-builder development environment setup'\nhttps://review.openstack.org/#/c/419655/)\n\nChange-Id: Id9ba43cbb43f2f2b3c2072b4f00f463ad91cc4a8\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}]",0,444666,5b823f1e325de42696e21cdaf1d24d03491755b6,62,4,5,21741,,,0,"Easy DIB development: add dibdevel-run element

This is a first patch that tries to make setting up a development
environment easy: When dibdevel-run is included, the created VM
contains all packages / setups that are needed to run
diskimage-builder.

Because there is currently no easy way of starting the images and
using them inside test cases, only the basic image creation test
cases for the major distributions are included.

(This is a split-off of patch
'Easy diskimage-builder development environment setup'
https://review.openstack.org/#/c/419655/)

Change-Id: Id9ba43cbb43f2f2b3c2072b4f00f463ad91cc4a8
Signed-off-by: Andreas Florath <andreas@florath.net>
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/66/444666/3 && git format-patch -1 --stdout FETCH_HEAD,"['diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-fedora/element-deps', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-gentoo/environment.d/10-set-distro.bash', 'diskimage_builder/elements/dibdevel-run/releasenotes/notes/easy-development-setup-dibdevel-run-48c7119c61bbf858.yaml', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-gentoo/Readme.rst', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-gentoo/element-deps', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-debian/element-deps', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-ubuntu/environment.d/10-set-distro.bash', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-fedora/environment.d/10-set-distro.bash', 'diskimage_builder/elements/dibdevel-run/pkg-map', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-ubuntu/Readme.rst', 'diskimage_builder/elements/dibdevel-run/environment.d/05-set-environment', 'diskimage_builder/elements/dibdevel-run/Readme.rst', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-ubuntu/element-deps', 'diskimage_builder/elements/dibdevel-run/element-deps', 'diskimage_builder/elements/dibdevel-run/package-installs.yaml', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-fedora/Readme.rst', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-debian/Readme.rst', 'diskimage_builder/elements/dibdevel-run/pre-install.d/10-portage-adaptions', 'diskimage_builder/elements/dibdevel-run/test-elements/dibdevel-run-debian/environment.d/10-set-distro.bash']",19,d5aece1d8941feb8174ae18167665679b15c6891,easy-development-setup/100-dibdevel-run,export DISTRO_NAME=debian ,,140,0
openstack%2Fmonasca-transform~master~I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0,openstack/monasca-transform,master,I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0,Add tempest plugin,NEW,2016-07-14 10:27:34.000000000,2017-12-18 01:32:08.000000000,,"[{'_account_id': 9276}, {'_account_id': 10311}, {'_account_id': 11580}, {'_account_id': 18179}]","[{'number': 1, 'created': '2016-07-14 10:27:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/97f88c226bef1f357f07f16cb303a7c18516ebe4', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 2, 'created': '2016-07-14 11:04:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/177277a9a3f4e7aa86c75600f3408c04c48b456a', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 3, 'created': '2016-07-28 15:08:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/73120e5f55b5845088f86d69a1ce58abae4b9632', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 4, 'created': '2016-08-12 16:02:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/a5fe8a07fd0652745880c9b1db04cfdac1944c98', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 5, 'created': '2016-10-25 16:24:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/d8cdcae9f6d47614689a15d847b46e32856aeb99', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 6, 'created': '2016-10-27 15:59:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/4da37810b4b0b4a4da3aa09e046ad2dcc42f6162', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 7, 'created': '2016-10-28 16:05:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/7cf3cd8551e2c289be1887578f01fd2c97561e20', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 8, 'created': '2016-10-29 16:19:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/668cba9bcab12e0dc156f660b3e0f1aeeeb09c1f', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 9, 'created': '2016-10-31 13:53:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/7dbe2cf257a85806a058e0da978ea6687de6ec57', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 10, 'created': '2016-11-01 10:19:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/2b7710375a8f3b9d347858557fe082937d881f38', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 11, 'created': '2016-11-09 21:13:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/6e7a8ccd9b04946cce3a8c7b1a17bab84d7850ec', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 12, 'created': '2016-11-09 21:14:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/0b427692e998c47826af4868a6f9a7ba692f1b56', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 13, 'created': '2016-11-10 13:31:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/864969a5751bb2655f613eb7af55d9c5c619208b', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 14, 'created': '2016-11-10 13:33:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/f10dbd16c4a67ef4523972d57ec91cac2e03b997', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 15, 'created': '2016-11-17 11:56:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/aff45e0119e25d358cb48766d779ef66b71ec208', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 16, 'created': '2016-11-17 12:21:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/5fa55c5dbef9fee0fa0ee4cedfa5a1c26a01a02f', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 17, 'created': '2016-11-17 13:00:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/9fd1afe95320139473a06a406f7825dc3a5df92f', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 18, 'created': '2016-11-22 11:07:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/dce3688a7abbba21d5f8303f2194f85d3e5fe098', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 19, 'created': '2016-11-22 12:55:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/1fea1ea42adffd406257fe5bbd33f2b8be8d6adf', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 20, 'created': '2016-12-05 21:16:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/996370359982887567c3c65389589b3183fe611b', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 21, 'created': '2016-12-06 08:57:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/df99a0674e613f963143da7e4d1924338be265a9', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 22, 'created': '2016-12-06 11:08:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/319905d5829ef2103492180cc856ad31c9c49cd0', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 23, 'created': '2016-12-06 12:22:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/2dbc0663cf2bf28206e66ced88b02edcd43fdd8c', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 24, 'created': '2016-12-06 14:09:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/c63049091b8c7b86244fa035b2c46a2c2d2b1e97', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 25, 'created': '2016-12-06 14:38:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/50b98b46adc44b1fb473e07872081c3f7ca0484a', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 26, 'created': '2016-12-07 13:20:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/9069ec3710038366892cfa0a85cb0ebcb8ce9f81', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 27, 'created': '2016-12-07 14:23:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/716f295a68b76b1582813366a48423022e69cfc8', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 28, 'created': '2016-12-07 14:50:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/cef188428cb4b21dbb6bee394b5630b2ca937282', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 29, 'created': '2016-12-08 09:16:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/1305cfabd4d85131f8dc61b84eee18e4d3d4218c', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 30, 'created': '2016-12-09 12:19:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/5681dcdfd63bc2d3bf1e5f9434be08330b0b899f', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 31, 'created': '2016-12-09 21:39:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/5d24bbce0f5c104bfdf3a3c858bff3b2a4ff9a86', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 32, 'created': '2016-12-09 23:16:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/6079d1030cd5ef1caaae59b39981e0dd8ca0c19b', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 33, 'created': '2016-12-13 14:47:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/25754737226bfaddffa4ceef268f1b9aa86bee59', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 34, 'created': '2016-12-15 13:37:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/cffae500f810414458971280334a03c48b72cc1f', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 35, 'created': '2016-12-15 16:53:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/5c11bd68795c3df74cb39a2121a7e8d80bf22489', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nCreate service client for monasca-api so that we can interrogate monasca\nfor the metrics that should be generated by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 36, 'created': '2016-12-16 09:11:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/1ba0ded0a8256184e986a3b12d4d344c74c89a71', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nAdded test to interrogate kafka and ensure that the pre-hourly metrics are\nbeing submitted by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}, {'number': 37, 'created': '2017-01-03 11:03:12.000000000', 'files': ['monasca_transform/tests/tempest/plugin.py', 'test-requirements.txt', 'devstack/post_test_hook.sh', 'monasca_transform/tests/tempest/scenario/base.py', 'devstack/files/monasca-transform/monasca-transform.conf', 'monasca_transform/tests/tempest/clients.py', 'monasca_transform/tests/__init__.py', 'tools/dev_scripts/aggregated_metric_names.py', 'devstack/files/tempest/tempest.conf', 'monasca_transform/tests/tempest/__init__.py', 'monasca_transform/tests/tempest/services/monasca_client.py', 'monasca_transform/tests/tempest/scenario/__init__.py', 'monasca_transform/tests/tempest/scenario/test_pre_hourly_metrics.py', 'setup.cfg', 'monasca_transform/tests/tempest/config.py', 'monasca_transform/tests/tempest/services/__init__.py'], 'web_link': 'https://opendev.org/openstack/monasca-transform/commit/f77e0d20dc14acd07bce1337e7ea74d882145228', 'message': 'Add tempest plugin\n\nAdded tempest plugin to project.  At this stage any api test suite is\nomitted as monasca-transform has no api.\nDecreased the time between aggregations in order to minimise the time taken\nfor tempest tests to complete.\nAdded test to interrogate kafka and ensure that the pre-hourly metrics are\nbeing submitted by monasca-transform.\n\nChange-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0\n'}]",16,342085,f77e0d20dc14acd07bce1337e7ea74d882145228,94,4,37,9276,,,0,"Add tempest plugin

Added tempest plugin to project.  At this stage any api test suite is
omitted as monasca-transform has no api.
Decreased the time between aggregations in order to minimise the time taken
for tempest tests to complete.
Added test to interrogate kafka and ensure that the pre-hourly metrics are
being submitted by monasca-transform.

Change-Id: I0b80c1afd45377d4cb9a7e75c9e0276ae30afae0
",git fetch https://review.opendev.org/openstack/monasca-transform refs/changes/85/342085/37 && git format-patch -1 --stdout FETCH_HEAD,"['monasca_transform_tempest_plugin/__init__.py', 'monasca_transform_tempest_plugin/plugin.py', 'test-requirements.txt', 'monasca_transform_tempest_plugin/tests/scenario/base.py', 'monasca_transform_tempest_plugin/tests/__init__.py', 'monasca_transform_tempest_plugin/tests/scenario/__init__.py', 'setup.cfg', 'monasca_transform_tempest_plugin/services/__init__.py']",8,97f88c226bef1f357f07f16cb303a7c18516ebe4,tempest_tests,,,46,0
openstack%2Frally~master~If07f4f194ffd65e65391799de19a7eedd6d86021,openstack/rally,master,If07f4f194ffd65e65391799de19a7eedd6d86021,[WIP][RAAS] raas based on flask and support rest-api,NEW,2017-02-04 18:33:35.000000000,2017-12-18 01:30:30.000000000,,"[{'_account_id': 6172}, {'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 22348}, {'_account_id': 22960}]","[{'number': 1, 'created': '2017-02-04 18:33:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8563cca3e34105495100a149a550b13f6d96a4b2', 'message': '[WIP][RAAS] raas based on flask and support rest-api\n\nFlask is a good web-framework, the docs: http://flask.pocoo.org/\nWe supply rest-api based on Flask.\nHow to run rally as raas:\n  1) start server\n    $ export FLASK_APP = <your-work-path>/rally/rally/aas/server.py\n    $ flask run -p 8080\n  2) view it in browser\n    Input URL(such as: http://127.0.0.1/api/deployment/list) in browser,\n    you would see the json result.\n    You could get all urls in @app.route(""/xx/xx/xx"")\n\nChange-Id: If07f4f194ffd65e65391799de19a7eedd6d86021\n'}, {'number': 2, 'created': '2017-02-04 18:35:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/cda232ff4a3cd12ed6eaec2cc932bdd987eefb6d', 'message': '[WIP][RAAS] raas based on flask and apply rest-api\n\nFlask is a good web-framework, the docs: http://flask.pocoo.org/\nWe supply rest-api based on Flask.\n\nHow to run rally as raas:\n  1) start server\n    $ export FLASK_APP = <your-work-path>/rally/rally/aas/server.py\n    $ flask run -p 8080\n  2) view it in browser\n    Input URL(such as: http://127.0.0.1：8080/api/deployment/list)\n    in browser, you would see the json result.\n    You could get all urls in @app.route(""/xx/xx/xx"")\n\nChange-Id: If07f4f194ffd65e65391799de19a7eedd6d86021\n'}, {'number': 3, 'created': '2017-02-06 03:33:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0e3ffc1e8c6a07fc13ccac5f30d03dc0cfe45296', 'message': '[WIP][RAAS] raas based on flask and support rest-api\n\nFlask is a good web-framework, the docs: http://flask.pocoo.org/\nWe supply rest-api based on Flask.\n\nHow to run rally as raas:\n  1) start server\n    $ export FLASK_APP = <your-work-path>/rally/rally/aas/server.py\n    $ flask run -p 8080\n  2) view it in browser\n    Input URL(such as: http://127.0.0.1/api/deployment/list) in browser,\n    you would see the json result.\n    You could get all urls in @app.route(""/xx/xx/xx"")\n\nHow to run remote-cli\n  1) export enviroments\n    $ export remote_request=True\n    $ export remote_address=http://127.0.0.1:8080/api\n\nChange-Id: If07f4f194ffd65e65391799de19a7eedd6d86021\n'}, {'number': 4, 'created': '2017-02-06 04:35:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/067edc866f2010f8d65543d36ed30b2bd34ad1b6', 'message': '[WIP][RAAS] raas based on flask and support rest-api\n\nFlask is a good web-framework, the docs: http://flask.pocoo.org/\nWe supply rest-api based on Flask.\n\nHow to run rally as raas:\n  1) start server\n    $ export FLASK_APP = <your-work-path>/rally/rally/aas/server.py\n    $ flask run -p 8080\n  2) view it in browser\n    Input URL(such as: http://127.0.0.1:8080/api/deployment/list) in browser,\n    you would see the json result.\n    You could get all urls in @app.route(""/xx/xx/xx"")\n\nHow to run remote-cli\n  1) export enviroments\n    $ export remote_request=True\n    $ export remote_address=http://127.0.0.1:8080/api\n\nChange-Id: If07f4f194ffd65e65391799de19a7eedd6d86021\n'}, {'number': 5, 'created': '2017-10-10 17:37:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/f17eceeca239a578335bad48559f9a306676c758', 'message': '[WIP][RAAS] raas based on flask and support rest-api\n\nChange-Id: If07f4f194ffd65e65391799de19a7eedd6d86021\n'}, {'number': 6, 'created': '2017-10-10 17:43:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c3f0c9f30718396d72c3dd81fa83e78d953aa523', 'message': '[WIP][RAAS] raas based on flask and support rest-api\n\nChange-Id: If07f4f194ffd65e65391799de19a7eedd6d86021\n'}, {'number': 7, 'created': '2017-10-10 17:45:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/2a6f992a81a0fabeede8462f7dba630bf4f761a7', 'message': '[WIP][RAAS] raas based on flask and support rest-api\n\nChange-Id: If07f4f194ffd65e65391799de19a7eedd6d86021\n'}, {'number': 8, 'created': '2017-10-15 07:06:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d6b8c3ef9fd9350152a59b8884ecb1a71c6bc2b7', 'message': '[WIP][RAAS] raas based on flask and support rest-api\n\nThe command of ""rally server start"" can start rally-service.\n\nChange-Id: If07f4f194ffd65e65391799de19a7eedd6d86021\n'}, {'number': 9, 'created': '2017-10-15 07:39:48.000000000', 'files': ['requirements.txt', 'rally/cli/commands/server.py', 'rally/api.py', 'rally/aas/server.py', 'rally/cli/main.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/e949987f8ebd888aa4311e54393dbb70afa69745', 'message': '[WIP][RAAS] raas based on flask and support rest-api\n\nThe command of ""rally server start"" can start rally-service.\n\nChange-Id: If07f4f194ffd65e65391799de19a7eedd6d86021\n'}]",6,429289,e949987f8ebd888aa4311e54393dbb70afa69745,36,5,9,22960,,,0,"[WIP][RAAS] raas based on flask and support rest-api

The command of ""rally server start"" can start rally-service.

Change-Id: If07f4f194ffd65e65391799de19a7eedd6d86021
",git fetch https://review.opendev.org/openstack/rally refs/changes/89/429289/8 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'rally/aas/server.py']",2,8563cca3e34105495100a149a550b13f6d96a4b2,raas,"# All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from flask import Flask from flask import json from rally import api app = Flask(__name__) api_instance = api.API() @app.route(""/api/deployment/list"") def deployment_list(): deployments = api_instance.deployment.list() return json.dumps(deployments) @app.route(""/api/deployment/get/<deployment_id>"") def deployment_get(deployment_id): deployment = api_instance.deployment.get(deployment_id) return json.dumps(deployment.deployment) ",,35,0
openstack%2Fmanila-specs~master~I16f4ee1fce33d9dc2548743ed3e9f082616f7e71,openstack/manila-specs,master,I16f4ee1fce33d9dc2548743ed3e9f082616f7e71,Functional testing framework for manila,NEW,2017-07-02 18:13:47.000000000,2017-12-18 01:30:23.000000000,,"[{'_account_id': 8851}, {'_account_id': 9003}, {'_account_id': 21698}]","[{'number': 1, 'created': '2017-07-02 18:13:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/27077103d8c59ce3fa5bdc616af41d03df244c06', 'message': 'Functional testing framework for manila\n\nThis spec adds details on how a new functional testing\nframework for manila should look like. Specifically, we\ndefine the criteria used to define what a functional test is,\nwhere should be the test be located, when should these test be run\nand what tooling should be used to write the tests.\n\nChange-Id: I16f4ee1fce33d9dc2548743ed3e9f082616f7e71\nPartially-Implements: bp manila-functional-testing\n'}, {'number': 2, 'created': '2017-07-02 18:38:46.000000000', 'files': ['doc/source/index.rst', 'specs/ongoing/functional-testing.rst'], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/54054e6b48c9e16930339a8bcfea73fa297020cf', 'message': 'Functional testing framework for manila\n\nThis spec adds details on how a new functional testing\nframework for manila should look like. Specifically, we\ndefine the criteria used to define what a functional test is,\nwhere should be the test be located, when should these test be run\nand what tooling should be used to write the tests.\n\nChange-Id: I16f4ee1fce33d9dc2548743ed3e9f082616f7e71\nPartially-Implements: bp manila-functional-testing\n'}]",1,479624,54054e6b48c9e16930339a8bcfea73fa297020cf,8,3,2,6413,,,0,"Functional testing framework for manila

This spec adds details on how a new functional testing
framework for manila should look like. Specifically, we
define the criteria used to define what a functional test is,
where should be the test be located, when should these test be run
and what tooling should be used to write the tests.

Change-Id: I16f4ee1fce33d9dc2548743ed3e9f082616f7e71
Partially-Implements: bp manila-functional-testing
",git fetch https://review.opendev.org/openstack/manila-specs refs/changes/24/479624/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/index.rst', 'specs/ongoing/functional-testing.rst']",2,27077103d8c59ce3fa5bdc616af41d03df244c06,bp/manila-functional-testing,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================= Manila Functional Testing ========================= https://blueprints.launchpad.net/manila/+spec/manila-functional-testing Problem description =================== In the past there has been an initiative to `move functional testing`_ out of tempest and into the projects. This requires us to provide guidance for how this is to be done in Manila. This spec answers the following questions: 1. What is considered functional tests and how are they different from unit tests? 2. Where should the tests be located? 3. When should these tests be run? 4. What tooling should be used to write the tests? Considering that adding functional tests in this new fashion will require making several changes, this spec should remain open for different commiters to reflect these. In other words, if someone decides that they would like to add tests to verify feature X, they will submit a change to this spec to add a subsection to Work Items based on the template. This spec may lead to moving unit tests into functional tests, but is not advocating any substantial changes to how unit tests work today. Use Cases ========= * Having our own framework for functional testing instead of relying in tempest would allow us to have a more flexible way for testing manila features and will relieve us from the burden of having our functional tests following other project development processes. Proposed change =============== Definitions ----------- shared scenario tests Tests that **must** pass on all manila configurations. (also known as 'shared tests') configuration specific scenario tests Tests that require manila is configured in a specific way. For example, to test notifications, you should have manila configured to trigger notifications. (also known as 'config specific tests') Functional or Unit Testing -------------------------- From the `move functional testing`_ post: Put the burden for a bunch of these tests back on the projects as ""functional"" tests. Basically a custom devstack environment that a project can create with a set of services that they minimally need to do their job. These functional tests will live in the project tree, not in Tempest, so can be atomically landed as part of the project normal development process. What exactly is a functional test? If the answer of any of the following questions is *'yes'*, then you have a functional test. 1. Does the test scenario need any specific components setup? (e.g. driver-specific features, notifications) 2. Does the test need to test a manila instance as a black box? 3. Does the test need to be run as a part of the test suite that runs against a variety of different configurations? .. example directory structure: Directory structures -------------------- .. code:: $ pwd /opt/stack/manila manila/tests/functional ├── notifications │ ├── __init__.py │ └── {{test_something}}.py └── shared ├── __init__.py └── {{test_something}}.py 2 directories, 4 files Running Functional Tests ------------------------ Functional shared tests should be run by developers before submitting changes to Gerrit. This should not be much of a barrier because those are tests that need to run on any running manila configuration. When developers are working on feature for a specific configuration they should run the config specific tests. To run all of the shared tests (unlikely that you want to do this): .. code:: tox -e functional To run all of the notifications config tests: .. code:: tox -e functional -- keystone.tests.functional.notifications How To Write Functional Tests ----------------------------- The tests will be written in a style that is similar to unit tests. `tox` will be used to run the tests. Test classes are subclasses to `testtools`. The difference comes in when you start talking about the contents of the tests themselves. Functional tests should be written using the black box model. The test should have no knowledge of the backends being used. Ideally all tests can be written using primarily `manilaclient` and `requests` to interact with manila. Certain elements of the tests *must* be controllable via environment variables. For example, the base URL for manila. This allows the developer running the tests to point to any manila instance. Beyond the manila URLs there is no official list of what must be configurable. Alternatives ------------ Data model impact ----------------- None REST API impact --------------- None Driver impact ------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- None Performance Impact ------------------ None Other deployer impact --------------------- None Developer impact ---------------- * Developers will have to understand what functional tests are and where they go in the tree. * Developers will have the responsability to know how and when run functional tests. * Developers will have to submit their functional tests for any new feature they submit that require (or, at least, makes sense to do) functional testing. * Some existing unit tests may be moved to the functional test suite. Implementation ============== Assignee(s) ----------- Primary assignee: None Work Items ---------- 1. Update developer documentation. 2. Start baseline for functional testing framework on manila. 3. Start adding functional testing for main features in manila. Dependencies ============ Certain functional tests may require a specific environment to be available. For example, to run the notifications tests your manila instance will have to be configured to trigger notifications. Testing ======= None. Documentation Impact ==================== The developer documentation will need to be updated to explain how and when to run the functional tests. A new document will need to be created to explain how to create and extend the functional tests. References ========== * _`move functional testing`: http://lists.openstack.org/pipermail/openstack-dev/2014-July/041057.html ",,239,0
openstack%2Fswift~master~I6040e6480e0ad79f1d4012b657e1530ad969f321,openstack/swift,master,I6040e6480e0ad79f1d4012b657e1530ad969f321,Use more specific asserts in test/unit/obj tests,NEW,2016-07-15 13:01:01.000000000,2017-12-18 01:27:51.000000000,,"[{'_account_id': 13052}, {'_account_id': 14766}, {'_account_id': 18599}, {'_account_id': 18602}, {'_account_id': 18603}, {'_account_id': 21239}, {'_account_id': 26242}]","[{'number': 1, 'created': '2016-07-15 13:01:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/92ff525ff67b999845bce15a15dfadd88e9e35de', 'message': 'Use more specific aserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 2, 'created': '2016-07-15 13:31:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/bdfcb4e149fbc4b3b65c98a07ea9f01905042aab', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 3, 'created': '2016-07-20 12:04:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f4a7d6c72e89062c28e72fc06b404a6127caabec', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 4, 'created': '2016-07-20 13:37:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/10f4f1b27c93faf67abf02628aa6abb2a7d7c47d', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 5, 'created': '2016-08-03 12:20:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ce305ef492f1beba1f40e66394ad0a1ba0160513', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 6, 'created': '2016-09-02 10:20:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c106822b4b77fc86598821179eeebc879b6d3402', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 7, 'created': '2016-11-02 12:17:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/5d4e9805ac4fae8d7bb7c1eba9e8c343fa72b39e', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 8, 'created': '2016-11-08 10:20:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/fb3021d50a1d338b7bd2a9e1233e592b1ffb37ac', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 9, 'created': '2016-12-05 14:55:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/66b1576069bb0cec8aa23905aa5e959177d8a4a8', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 10, 'created': '2016-12-06 11:32:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/cbd8712170aee4b75ba18f86d04ca2aa76a2d7c7', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 11, 'created': '2016-12-09 16:55:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/5152285786b6502243587b826bd3a3905b9e6fc0', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 12, 'created': '2016-12-13 10:28:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/d4a390e2842c3a813240e2000443e9c278be7f32', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 13, 'created': '2017-02-15 18:35:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0d73988f94a0a770d57c226e20d4e56d8bb9b122', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 14, 'created': '2017-02-27 18:56:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c3b37c66d841b4d1daf9e824651e10f7fd5fadb7', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 15, 'created': '2017-03-20 17:55:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/5e69554ba7c245d2bf3094c34871748cd59ae530', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 16, 'created': '2017-05-03 13:29:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c904a1e627314f01d617f9904d7b4d4ac98cfe15', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 17, 'created': '2017-06-13 09:21:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e255b38c0d028c5e47ff6b9a18d088d202a67b99', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 18, 'created': '2017-07-24 14:15:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/d410a5f6ebf3110068c4de42e0d65d01de2208e5', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 19, 'created': '2017-08-23 15:31:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/925d18b790a965ee0ed8a7453623d0beac7dcf0f', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}, {'number': 20, 'created': '2017-08-31 15:26:16.000000000', 'files': ['test/unit/obj/test_auditor.py', 'test/unit/obj/test_expirer.py', 'test/unit/obj/test_ssync_sender.py', 'test/unit/obj/test_server.py', 'test/unit/obj/test_replicator.py', 'test/unit/obj/test_ssync.py', 'test/unit/obj/test_diskfile.py', 'test/unit/obj/test_updater.py', 'test/unit/obj/test_ssync_receiver.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/3c165f34d9af92931d9cb7fcfc1aa2d1abf6a183', 'message': 'Use more specific asserts in test/unit/obj tests\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321\n'}]",13,342830,3c165f34d9af92931d9cb7fcfc1aa2d1abf6a183,79,7,20,18602,,,0,"Use more specific asserts in test/unit/obj tests

I changed asserts with more specific assert methods.
e.g.: from assertTrue(sth == None) to assertIsNone(*) or
assertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)
or assertTrue(not sth) to assertFalse(sth).

The code gets more readable, and a better description will be shown on fail.

Change-Id: I6040e6480e0ad79f1d4012b657e1530ad969f321
",git fetch https://review.opendev.org/openstack/swift refs/changes/30/342830/16 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/obj/test_auditor.py', 'test/unit/obj/test_expirer.py', 'test/unit/obj/test_ssync_sender.py', 'test/unit/obj/test_server.py', 'test/unit/obj/test_replicator.py', 'test/unit/obj/test_ssync.py', 'test/unit/obj/test_diskfile.py', 'test/unit/obj/test_updater.py', 'test/unit/obj/test_reconstructor.py', 'test/unit/obj/test_ssync_receiver.py']",10,92ff525ff67b999845bce15a15dfadd88e9e35de,use_more_specific_asserts," self.assertIsNone(rcvr.frag_index) self.assertIsNone(rcvr.node_index) self.assertNotIn('Specialty-Header', df.get_metadata()) self.assertIsNone(req.content_length) self.assertIsNone(_BONK_request[0]) self.assertIsNone(req.content_length)"," self.assertEqual(rcvr.frag_index, None) self.assertEqual(rcvr.node_index, None) self.assertFalse('Specialty-Header' in df.get_metadata()) self.assertEqual(req.content_length, None) self.assertEqual(_BONK_request[0], None) self.assertEqual(req.content_length, None)",135,136
openstack%2Fswift~master~I325b2db4df505ac91e1c79a6f5e7638d5c4213de,openstack/swift,master,I325b2db4df505ac91e1c79a6f5e7638d5c4213de,Use more specific asserts in test/unit/common/middleware,NEW,2016-07-15 12:44:36.000000000,2017-12-18 01:27:49.000000000,,"[{'_account_id': 2622}, {'_account_id': 7233}, {'_account_id': 9664}, {'_account_id': 13052}, {'_account_id': 18599}, {'_account_id': 18602}, {'_account_id': 18603}, {'_account_id': 21239}, {'_account_id': 26242}]","[{'number': 1, 'created': '2016-07-15 12:44:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0ab8663742f6287acab422f2beef0a101f0eca8b', 'message': 'Use more specific aserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 2, 'created': '2016-07-15 13:28:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ece1b4dc4d1bb714df335de375e5ea9c0165c2ae', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 3, 'created': '2016-08-03 12:19:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6cc8dafb120ca6111e183c3f16ed04bb218af296', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 4, 'created': '2016-08-26 17:07:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1a2628ed52dffe9ca8aac597dca1789d7eb05abd', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 5, 'created': '2016-09-15 01:55:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7a8754c20170cd8025924a28be9f0de91e2d330c', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 6, 'created': '2016-10-02 05:48:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f516c6df717ba42e89b7cf7e239241211ab7a4fa', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 7, 'created': '2016-11-03 10:34:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/12cacf1e8ed2c05aeb371c2eaf25a09d2a640588', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 8, 'created': '2016-12-09 16:36:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f45271e5a02d229583cba5b87a8af3338a21336f', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 9, 'created': '2016-12-13 12:26:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6c29e03206c38245463a385d1879407e19506512', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 10, 'created': '2016-12-13 15:15:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e13357360ffae2d7abbfb2498f2428cfc29752e2', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(sth) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 11, 'created': '2017-01-25 11:10:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6155b8e7e704aafd2bddfb17437bafe859aea7b8', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(sth) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 12, 'created': '2017-06-12 15:52:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a9cc266b9488f0135d44e6e79a90c565abc09266', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(sth) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 13, 'created': '2017-06-21 12:24:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/3523eadab75fa027192680fd6c12926518793018', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(sth) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 14, 'created': '2017-07-24 14:15:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/b604e63102db3c830f3a4bb1b9dbf3a2202061b6', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(sth) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 15, 'created': '2017-08-23 15:29:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/6daadabc62f8e31ca3a0676e5ea8501cf006fe85', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(sth) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}, {'number': 16, 'created': '2017-08-31 14:59:20.000000000', 'files': ['test/unit/common/middleware/test_container_sync.py', 'test/unit/common/middleware/test_formpost.py', 'test/unit/common/middleware/test_acl.py', 'test/unit/common/middleware/test_ratelimit.py', 'test/unit/common/middleware/test_cname_lookup.py', 'test/unit/common/middleware/test_xprofile.py', 'test/unit/common/middleware/test_dlo.py', 'test/unit/common/middleware/test_copy.py', 'test/unit/common/middleware/test_memcache.py', 'test/unit/common/middleware/test_name_check.py', 'test/unit/common/middleware/test_domain_remap.py', 'test/unit/common/middleware/test_tempauth.py', 'test/unit/common/middleware/crypto/test_crypto_utils.py', 'test/unit/common/middleware/test_keystoneauth.py', 'test/unit/common/middleware/test_bulk.py', 'test/unit/common/middleware/test_gatekeeper.py', 'test/unit/common/middleware/test_slo.py', 'test/unit/common/middleware/test_proxy_logging.py', 'test/unit/common/middleware/test_tempurl.py', 'test/unit/common/middleware/test_subrequest_logging.py', 'test/unit/common/middleware/test_staticweb.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/0019d0f29f0a3524c4ee1813d890c43a6d8e1fc1', 'message': 'Use more specific asserts in test/unit/common/middleware\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(sth) or\nassertTrue(isinstance(inst, type)) to assertIsInstance(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de\n'}]",43,342770,0019d0f29f0a3524c4ee1813d890c43a6d8e1fc1,67,9,16,18602,,,0,"Use more specific asserts in test/unit/common/middleware

I changed asserts with more specific assert methods.
e.g.: from assertTrue(sth == None) to assertIsNone(sth) or
assertTrue(isinstance(inst, type)) to assertIsInstance(inst, type)
or assertTrue(not sth) to assertFalse(sth).

The code gets more readable, and a better description will be shown on fail.

Change-Id: I325b2db4df505ac91e1c79a6f5e7638d5c4213de
",git fetch https://review.opendev.org/openstack/swift refs/changes/70/342770/8 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/common/middleware/test_container_sync.py', 'test/unit/common/middleware/test_formpost.py', 'test/unit/common/middleware/test_acl.py', 'test/unit/common/middleware/test_ratelimit.py', 'test/unit/common/middleware/test_list_endpoints.py', 'test/unit/common/middleware/test_xprofile.py', 'test/unit/common/middleware/test_except.py', 'test/unit/common/middleware/test_dlo.py', 'test/unit/common/middleware/test_copy.py', 'test/unit/common/middleware/test_memcache.py', 'test/unit/common/middleware/test_domain_remap.py', 'test/unit/common/middleware/test_tempauth.py', 'test/unit/common/middleware/crypto/test_crypto_utils.py', 'test/unit/common/middleware/test_keystoneauth.py', 'test/unit/common/middleware/test_versioned_writes.py', 'test/unit/common/middleware/test_bulk.py', 'test/unit/common/middleware/test_gatekeeper.py', 'test/unit/common/middleware/test_slo.py', 'test/unit/common/middleware/test_proxy_logging.py', 'test/unit/common/middleware/test_tempurl.py', 'test/unit/common/middleware/test_staticweb.py']",21,0ab8663742f6287acab422f2beef0a101f0eca8b,use_more_specific_asserts," self.assertIn('Test main index.html file.', resp.body) self.assertIn('Listing of /v1/a/c3/subdir/', resp.body) self.assertIn('</style>', resp.body) self.assertNotIn('<link', resp.body) self.assertNotIn('listing.css', resp.body) self.assertNotIn(""Chrome's 404 fancy-page sucks."", resp.body) self.assertIn('Listing of /v1/a/c4/', resp.body) self.assertIn('href=""listing.css""', resp.body) self.assertIn(""Chrome's 404 fancy-page sucks."", resp.body) self.assertIn('Listing of /v1/a/c4/subdir/', resp.body) self.assertNotIn('</style>', resp.body) self.assertIn('<link', resp.body) self.assertIn('href=""../listing.css""', resp.body) self.assertNotIn(""Chrome's 404 fancy-page sucks."", resp.body) self.assertIn('Listing of /v1/a/c8/', resp.body) self.assertIn('<link', resp.body) self.assertIn( 'href=""http://localhost/stylesheets/listing.css""', resp.body) self.assertIn('Listing of /v1/a/c8/subdir/', resp.body) self.assertIn('<link', resp.body) self.assertIn( 'href=""http://localhost/stylesheets/listing.css""', resp.body) self.assertIn('Listing of /v1/a/c9/', resp.body) self.assertIn('<link', resp.body) self.assertIn('href=""/absolute/listing.css""', resp.body) self.assertIn('Listing of /v1/a/c9/subdir/', resp.body) self.assertIn('<link', resp.body) self.assertIn('href=""/absolute/listing.css""', resp.body) self.assertIn('Listing of /v1/a/c10/', resp.body) self.assertIn('Listing of /v1/a/c10/\xe2\x98\x83/', resp.body) self.assertIn( 'Listing of /v1/a/c10/\xe2\x98\x83/\xe2\x98\x83/', resp.body) self.assertIn('<h2>c11 subdir index</h2>', resp.body) self.assertIn('Index File Not Found', resp.body) self.assertIn('index file', resp.body) self.assertIn('listing.css', resp.body) self.assertNotIn('listing.css', resp.body) self.assertIn('<style', resp.body)"," self.assertTrue('Test main index.html file.' in resp.body) self.assertTrue('Listing of /v1/a/c3/subdir/' in resp.body) self.assertTrue('</style>' in resp.body) self.assertTrue('<link' not in resp.body) self.assertTrue('listing.css' not in resp.body) self.assertTrue(""Chrome's 404 fancy-page sucks."" not in resp.body) self.assertTrue('Listing of /v1/a/c4/' in resp.body) self.assertTrue('href=""listing.css""' in resp.body) self.assertTrue(""Chrome's 404 fancy-page sucks."" in resp.body) self.assertTrue('Listing of /v1/a/c4/subdir/' in resp.body) self.assertTrue('</style>' not in resp.body) self.assertTrue('<link' in resp.body) self.assertTrue('href=""../listing.css""' in resp.body) self.assertTrue(""Chrome's 404 fancy-page sucks."" not in resp.body) self.assertTrue('Listing of /v1/a/c8/' in resp.body) self.assertTrue('<link' in resp.body) self.assertTrue( 'href=""http://localhost/stylesheets/listing.css""' in resp.body) self.assertTrue('Listing of /v1/a/c8/subdir/' in resp.body) self.assertTrue('<link' in resp.body) self.assertTrue( 'href=""http://localhost/stylesheets/listing.css""' in resp.body) self.assertTrue('Listing of /v1/a/c9/' in resp.body) self.assertTrue('<link' in resp.body) self.assertTrue('href=""/absolute/listing.css""' in resp.body) self.assertTrue('Listing of /v1/a/c9/subdir/' in resp.body) self.assertTrue('<link' in resp.body) self.assertTrue('href=""/absolute/listing.css""' in resp.body) self.assertTrue('Listing of /v1/a/c10/' in resp.body) self.assertTrue('Listing of /v1/a/c10/\xe2\x98\x83/' in resp.body) self.assertTrue( 'Listing of /v1/a/c10/\xe2\x98\x83/\xe2\x98\x83/' in resp.body) self.assertTrue('<h2>c11 subdir index</h2>' in resp.body) self.assertTrue('Index File Not Found' in resp.body) self.assertTrue('index file' in resp.body) self.assertTrue('listing.css' in resp.body) self.assertTrue('listing.css' not in resp.body) self.assertTrue('<style' in resp.body)",493,498
openstack%2Fswift~master~Idbbc425872ad332ce16cbc505f19f4b99e08e853,openstack/swift,master,Idbbc425872ad332ce16cbc505f19f4b99e08e853,Use more specific asserts in test/unit/common,NEW,2016-07-15 12:51:43.000000000,2017-12-18 01:27:43.000000000,,"[{'_account_id': 8871}, {'_account_id': 9625}, {'_account_id': 9664}, {'_account_id': 13052}, {'_account_id': 18599}, {'_account_id': 18602}, {'_account_id': 18603}, {'_account_id': 21239}, {'_account_id': 26242}]","[{'number': 1, 'created': '2016-07-15 12:51:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c5759009ffbac43e6b7775e0559e5a291ea5ce02', 'message': 'Use more specific aserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 2, 'created': '2016-07-15 13:29:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/91926c022d8c419225ad2889e864f3853d5bb1ad', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 3, 'created': '2016-08-03 12:19:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/32552e8ed56a551f3636c5d66c634084acd1e300', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 4, 'created': '2016-08-24 08:42:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/3dafb8176c9b25673ce9adaf5bb19d0ba8fa362b', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 5, 'created': '2016-11-03 10:43:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/13f5ff67a0f467096a2934e151ec0691bb9e6bda', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 6, 'created': '2016-11-08 09:59:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a3b2b237a076c257fdd8109747de5efca7e8ac84', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 7, 'created': '2016-12-13 10:42:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a59b27551e193d6ebc8f5a2a48bdae89913edbbe', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 8, 'created': '2016-12-15 12:21:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/17c9e986245b5512657052a03618b0d09cfb818b', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 9, 'created': '2017-02-27 18:55:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/44e5c6644a3b6769a1061275b3b50d218f9b5595', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 10, 'created': '2017-02-28 13:49:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ef2792c35acef3d691cb14d908f03e769e439a75', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 11, 'created': '2017-03-01 10:03:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4ec9fb3c093293e58e4c5873968533795360d0af', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 12, 'created': '2017-03-28 16:15:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/74d4c627894d231953f6dce865dbe9b19c8bae38', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 13, 'created': '2017-03-30 09:01:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/886b394b45c30f9fa9724e495d715a63cc5a68a0', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 14, 'created': '2017-06-09 15:37:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/fb67addd8f52757ae014557f4b6bfc7d31f6cae8', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 15, 'created': '2017-06-21 09:04:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c6765f476f4cb1e20692e68d5f3de3e48328fb47', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 16, 'created': '2017-07-03 16:35:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f5c29fd31fe00b2539b8b725d547a837cb08ca89', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 17, 'created': '2017-07-24 14:15:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/5fb5ab1147e0a80316d41d85df4ecafc695e8325', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 18, 'created': '2017-08-23 15:29:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/710da86829dab35ea441ca52a01d033a7df2f6d1', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}, {'number': 19, 'created': '2017-08-31 15:26:03.000000000', 'files': ['test/unit/common/test_constraints.py', 'test/unit/common/test_storage_policy.py', 'test/unit/common/test_request_helpers.py', 'test/unit/common/test_db_replicator.py', 'test/unit/common/test_direct_client.py', 'test/unit/common/test_db.py', 'test/unit/common/test_memcached.py', 'test/unit/common/test_manager.py', 'test/unit/common/test_wsgi.py', 'test/unit/common/test_internal_client.py', 'test/unit/common/test_header_key_dict.py', 'test/unit/common/test_daemon.py', 'test/unit/common/test_swob.py', 'test/unit/common/test_exceptions.py', 'test/unit/common/test_utils.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/6c7750396f9f90bc44753fc242d36ca6cd2c3d7a', 'message': 'Use more specific asserts in test/unit/common\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853\n'}]",15,342781,6c7750396f9f90bc44753fc242d36ca6cd2c3d7a,85,9,19,18602,,,0,"Use more specific asserts in test/unit/common

I changed asserts with more specific assert methods.
e.g.: from assertTrue(sth == None) to assertIsNone(*) or
assertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)
or assertTrue(not sth) to assertFalse(sth).

The code gets more readable, and a better description will be shown on fail.

Change-Id: Idbbc425872ad332ce16cbc505f19f4b99e08e853
",git fetch https://review.opendev.org/openstack/swift refs/changes/81/342781/17 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/common/test_constraints.py', 'test/unit/common/test_storage_policy.py', 'test/unit/common/test_request_helpers.py', 'test/unit/common/test_db_replicator.py', 'test/unit/common/test_direct_client.py', 'test/unit/common/test_db.py', 'test/unit/common/test_memcached.py', 'test/unit/common/test_manager.py', 'test/unit/common/test_wsgi.py', 'test/unit/common/test_internal_client.py', 'test/unit/common/test_bufferedhttp.py', 'test/unit/common/test_header_key_dict.py', 'test/unit/common/test_daemon.py', 'test/unit/common/test_swob.py', 'test/unit/common/test_exceptions.py', 'test/unit/common/test_utils.py', 'test/unit/common/test_container_sync_realms.py']",17,c5759009ffbac43e6b7775e0559e5a291ea5ce02,use_more_specific_asserts," self.assertIsNone(csr.key2('US')) self.assertIsNone(csr.key2('US')) self.assertIsNone(csr.key('US')) self.assertIsNone(csr.key2('US')) self.assertIsNone(csr.endpoint('US', 'JUST_TESTING'))"," self.assertEqual(csr.key2('US'), None) self.assertEqual(csr.key2('US'), None) self.assertEqual(csr.key('US'), None) self.assertEqual(csr.key2('US'), None) self.assertEqual(csr.endpoint('US', 'JUST_TESTING'), None)",538,538
openstack%2Fswift~master~Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d,openstack/swift,master,Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d,Use more specific asserts in test/unit/container,NEW,2016-07-15 12:58:20.000000000,2017-12-18 01:27:30.000000000,,"[{'_account_id': 9664}, {'_account_id': 13052}, {'_account_id': 18599}, {'_account_id': 18602}, {'_account_id': 18603}, {'_account_id': 21239}, {'_account_id': 26242}]","[{'number': 1, 'created': '2016-07-15 12:58:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1f90790a250c0d206bb94462d98f810895a223b3', 'message': 'Use more specific aserts in test/unit/container\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d\n'}, {'number': 2, 'created': '2016-07-15 13:30:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/92867344774ddb908ad7791736b069908324c8d3', 'message': 'Use more specific asserts in test/unit/container\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d\n'}, {'number': 3, 'created': '2016-08-03 12:20:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ea0dfda77ea5c45052cce1ffe3cf222b08d06ad4', 'message': 'Use more specific asserts in test/unit/container\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d\n'}, {'number': 4, 'created': '2016-10-13 07:14:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/39700ebdfd73906f521c0cd437326334ae555106', 'message': 'Use more specific asserts in test/unit/container\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d\n'}, {'number': 5, 'created': '2017-03-31 10:10:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/b9c31cb297c8710670f6fff980a816f3b21d7469', 'message': 'Use more specific asserts in test/unit/container\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d\n'}, {'number': 6, 'created': '2017-04-06 10:03:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/466e75ea6595d361143efdc689d98081d84585aa', 'message': 'Use more specific asserts in test/unit/container\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d\n'}, {'number': 7, 'created': '2017-06-13 09:21:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a30f0b23b7debafeeb3b2cd5b51b93878231c9d0', 'message': 'Use more specific asserts in test/unit/container\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d\n'}, {'number': 8, 'created': '2017-06-21 09:08:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0554c82f7470e2f03339327f82fc00aeff64c4bc', 'message': 'Use more specific asserts in test/unit/container\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d\n'}, {'number': 9, 'created': '2017-07-24 14:15:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/046b7fa909868b2d7b92b213f613e49193f1f4f7', 'message': 'Use more specific asserts in test/unit/container\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d\n'}, {'number': 10, 'created': '2017-08-23 15:31:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a58c1b5f3891826b46e604364651065389b8b979', 'message': 'Use more specific asserts in test/unit/container\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d\n'}, {'number': 11, 'created': '2017-08-31 15:26:10.000000000', 'files': ['test/unit/container/test_replicator.py', 'test/unit/container/test_sync.py', 'test/unit/container/test_updater.py', 'test/unit/container/test_server.py', 'test/unit/container/test_backend.py', 'test/unit/container/test_reconciler.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/2347859b37625381481ebfc19e1b6159aca4a76a', 'message': 'Use more specific asserts in test/unit/container\n\nI changed asserts with more specific assert methods.\ne.g.: from assertTrue(sth == None) to assertIsNone(*) or\nassertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)\nor assertTrue(not sth) to assertFalse(sth).\n\nThe code gets more readable, and a better description will be shown on fail.\n\nChange-Id: Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d\n'}]",27,342808,2347859b37625381481ebfc19e1b6159aca4a76a,57,7,11,18602,,,0,"Use more specific asserts in test/unit/container

I changed asserts with more specific assert methods.
e.g.: from assertTrue(sth == None) to assertIsNone(*) or
assertTrue(isinstance(inst, type)) to assertIsInstace(inst, type)
or assertTrue(not sth) to assertFalse(sth).

The code gets more readable, and a better description will be shown on fail.

Change-Id: Ifa04898fb22f0fb6361bc2cf9a26c0b9e176d36d
",git fetch https://review.opendev.org/openstack/swift refs/changes/08/342808/10 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/container/test_replicator.py', 'test/unit/container/test_sync.py', 'test/unit/container/test_updater.py', 'test/unit/container/test_server.py', 'test/unit/container/test_backend.py', 'test/unit/container/test_reconciler.py']",6,1f90790a250c0d206bb94462d98f810895a223b3,use_more_specific_asserts," self.assertIsNone(oldest_spi) self.assertIsNone(oldest_spi) self.assertIsNone(rv) self.assertIn(header, args['headers'], '%r was missing request headers %r' % ( header, args['headers'])) self.assertIn(header, args['headers'], '%r was missing request headers %r' % ( header, args['headers']))"," self.assertEqual(oldest_spi, None) self.assertEqual(oldest_spi, None) self.assertEqual(rv, None) self.assertTrue(header in args['headers'], '%r was missing request headers %r' % ( header, args['headers'])) self.assertTrue(header in args['headers'], '%r was missing request headers %r' % ( header, args['headers']))",93,92
openstack%2Ftrove~master~I562f6f271242b925bd71cc51546a0170cd3dba43,openstack/trove,master,I562f6f271242b925bd71cc51546a0170cd3dba43,Support MongoDB 3.2,NEW,2016-09-09 18:15:32.000000000,2017-12-18 01:27:25.000000000,,"[{'_account_id': 9664}, {'_account_id': 9746}, {'_account_id': 9782}, {'_account_id': 10215}, {'_account_id': 10295}, {'_account_id': 15321}, {'_account_id': 22694}, {'_account_id': 23192}, {'_account_id': 26895}]","[{'number': 1, 'created': '2016-09-09 18:15:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/f559f9815b71ea7231e8b3c34b34abc3d7680633', 'message': 'Support MongoDB 3.2\n\nMongoDB 3.2 solves a number of issues present in 3.0. This change\nsupports the new changes, and cleans up a number confusing patterns in\nthe code.\n\nCloses-Bug: 1582712\nDepends-On: I7575746ac445a875349640ffa91d6d3fb6680cea\nChange-Id: I562f6f271242b925bd71cc51546a0170cd3dba43\n'}, {'number': 2, 'created': '2016-09-09 19:11:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/26e37b0e3be52acb13e07859e615cdc1f7b75a74', 'message': 'Support MongoDB 3.2\n\nMongoDB 3.2 solves a number of issues present in 3.0. This change\nsupports the new changes, and cleans up a number confusing patterns in\nthe code.\n\nCloses-Bug: 1582712\nDepends-On: I7575746ac445a875349640ffa91d6d3fb6680cea\nChange-Id: I562f6f271242b925bd71cc51546a0170cd3dba43\n'}, {'number': 3, 'created': '2016-09-09 19:20:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/55d0632c173e8abcd88aae0b3523c54f0ac38cc0', 'message': 'Support MongoDB 3.2\n\nMongoDB 3.2 solves a number of issues present in 3.0. This change\nsupports the new changes, and cleans up a number confusing patterns in\nthe code.\n\nCloses-Bug: 1582712\nDepends-On: I7575746ac445a875349640ffa91d6d3fb6680cea\nChange-Id: I562f6f271242b925bd71cc51546a0170cd3dba43\n'}, {'number': 4, 'created': '2016-09-12 12:39:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/72fb8fe6834f38b70ef9e202d1605d3cb8fc0298', 'message': 'Support MongoDB 3.2\n\nMongoDB 3.2 solves a number of issues present in 3.0. This change\nsupports the new changes, and cleans up a number confusing patterns in\nthe code.\n\nCloses-Bug: 1582712\nDepends-On: I7575746ac445a875349640ffa91d6d3fb6680cea\nChange-Id: I562f6f271242b925bd71cc51546a0170cd3dba43\n'}, {'number': 5, 'created': '2016-10-13 13:58:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/3fc8e321b61ce9774b7d58fa6aa7b4d5372e9df8', 'message': 'Support MongoDB 3.2\n\nMongoDB 3.2 solves a number of issues present in 3.0. This change\nsupports the new changes, and cleans up a number confusing patterns in\nthe code.\n\nCloses-Bug: 1582712\nDepends-On: I7575746ac445a875349640ffa91d6d3fb6680cea\nChange-Id: I562f6f271242b925bd71cc51546a0170cd3dba43\n'}, {'number': 6, 'created': '2016-11-10 16:01:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/f7152044704b4b3417ed5cde26d88490a549f945', 'message': 'Support MongoDB 3.2\n\nMongoDB 3.2 solves a number of issues present in 3.0. This change\nsupports the new changes, and cleans up a number confusing patterns in\nthe code.\n\nCloses-Bug: 1582712\nDepends-On: I7575746ac445a875349640ffa91d6d3fb6680cea\nChange-Id: I562f6f271242b925bd71cc51546a0170cd3dba43\n'}, {'number': 7, 'created': '2016-11-29 00:09:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/0e58f87ccfc95cd85a3825e429a68bac60403e15', 'message': 'Support MongoDB 3.2\n\nMongoDB 3.2 solves a number of issues present in 3.0. This change\nsupports the new changes, and cleans up a number confusing patterns in\nthe code.\n\nCloses-Bug: 1582712\nDepends-On: I7575746ac445a875349640ffa91d6d3fb6680cea\nChange-Id: I562f6f271242b925bd71cc51546a0170cd3dba43\n'}, {'number': 8, 'created': '2016-11-29 15:57:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/914fb777dceaf1d0c51fece6dab7ffae65e86601', 'message': 'Support MongoDB 3.2\n\nMongoDB 3.2 solves a number of issues present in 3.0. This change\nsupports the new changes, and cleans up a number confusing patterns in\nthe code.\n\nCloses-Bug: 1582712\nDepends-On: I7575746ac445a875349640ffa91d6d3fb6680cea\nChange-Id: I562f6f271242b925bd71cc51546a0170cd3dba43\n'}, {'number': 9, 'created': '2016-11-29 16:24:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/c9d32cb20b018c0a3a549832f60147ca9225ce66', 'message': 'Support MongoDB 3.2\n\nMongoDB 3.2 solves a number of issues present in 3.0. This change\nsupports the new changes, and cleans up a number confusing patterns in\nthe code.\n\nCloses-Bug: 1582712\nDepends-On: I7575746ac445a875349640ffa91d6d3fb6680cea\nChange-Id: I562f6f271242b925bd71cc51546a0170cd3dba43\n'}, {'number': 10, 'created': '2017-04-26 12:33:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/794b94dfef1cdec36ffb30bb51c0d56b5ee93078', 'message': 'Support MongoDB 3.2\n\nMongoDB 3.2 solves a number of issues present in 3.0. This change\nsupports the new changes, and cleans up a number confusing patterns in\nthe code.\n\nCloses-Bug: 1582712\nDepends-On: I7575746ac445a875349640ffa91d6d3fb6680cea\nChange-Id: I562f6f271242b925bd71cc51546a0170cd3dba43\n'}, {'number': 11, 'created': '2017-04-26 14:26:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/e73ef2d8aca44b9b863d7fecb7c75db40ab3730f', 'message': 'Support MongoDB 3.2\n\nMongoDB 3.2 solves a number of issues present in 3.0. This change\nsupports the new changes, and cleans up a number confusing patterns in\nthe code.\n\nCloses-Bug: 1582712\nDepends-On: I7575746ac445a875349640ffa91d6d3fb6680cea\nChange-Id: I562f6f271242b925bd71cc51546a0170cd3dba43\n'}, {'number': 12, 'created': '2017-05-26 20:11:21.000000000', 'files': ['trove/tests/unittests/guestagent/test_mongodb_manager.py', 'trove/common/strategies/cluster/experimental/mongodb/api.py', 'trove/templates/mongodb/config.template', 'trove/tests/unittests/guestagent/test_mongodb_cluster_manager.py', 'releasenotes/notes/mongodb-3.2-a3ab8e43884262e7.yaml', 'trove/guestagent/datastore/experimental/mongodb/service.py', 'trove/guestagent/datastore/experimental/mongodb/manager.py', 'trove/guestagent/datastore/experimental/mongodb/system.py'], 'web_link': 'https://opendev.org/openstack/trove/commit/662e0d4efec979c884c2efd7f1ced05d897d0f85', 'message': 'Support MongoDB 3.2\n\nMongoDB 3.2 solves a number of issues present in 3.0. This change\nsupports the new changes, and cleans up a number confusing patterns in\nthe code.\n\nCloses-Bug: 1582712\nDepends-On: I2f2a12207581a94fb8561a6d65a3a79b4a29b063\nDepends-On: I7575746ac445a875349640ffa91d6d3fb6680cea\nChange-Id: I562f6f271242b925bd71cc51546a0170cd3dba43\n'}]",5,368174,662e0d4efec979c884c2efd7f1ced05d897d0f85,73,9,12,15321,,,0,"Support MongoDB 3.2

MongoDB 3.2 solves a number of issues present in 3.0. This change
supports the new changes, and cleans up a number confusing patterns in
the code.

Closes-Bug: 1582712
Depends-On: I2f2a12207581a94fb8561a6d65a3a79b4a29b063
Depends-On: I7575746ac445a875349640ffa91d6d3fb6680cea
Change-Id: I562f6f271242b925bd71cc51546a0170cd3dba43
",git fetch https://review.opendev.org/openstack/trove refs/changes/74/368174/8 && git format-patch -1 --stdout FETCH_HEAD,"['trove/tests/unittests/guestagent/test_mongodb_manager.py', 'trove/templates/mongodb/config.template', 'trove/tests/unittests/guestagent/test_mongodb_cluster_manager.py', 'trove/guestagent/datastore/experimental/mongodb/service.py', 'trove/guestagent/datastore/experimental/mongodb/manager.py', 'trove/guestagent/datastore/experimental/mongodb/system.py']",6,f559f9815b71ea7231e8b3c34b34abc3d7680633,bug/1582712,"CONFIG_DIR = ""/etc"" MONGOD_CONFIG_CANDIDATES = [path.join(CONFIG_DIR, name) for name in ['mongodb.conf', 'mongod.conf']] MONGOD_CONFIG_FILE = operating_system.file_discovery(MONGOD_CONFIG_CANDIDATES) MONGOD_CONFIG_OVERRIDES_DIR = path.join(CONFIG_DIR, 'mongod_overrides') MONGOS_CONFIG_CANDIDATES = [path.join(CONFIG_DIR, name) for name in ['mongodb.conf', 'mongos.conf']] MONGOS_CONFIG_FILE = operating_system.file_discovery(MONGOS_CONFIG_CANDIDATES) MONGOS_CONFIG_OVERRIDES_DIR = path.join(CONFIG_DIR, 'mongos_overrides') MONGO_USER = ""mongodb""","MONGODB_MOUNT_POINT = ""/var/lib/mongodb"" MONGO_PID_FILE = '/var/run/mongodb/mongodb.pid' MONGO_LOG_FILE = '/var/log/mongodb/mongod.log' CONFIG_CANDIDATES = [""/etc/mongodb.conf"", ""/etc/mongod.conf""]MONGO_USER = {operating_system.REDHAT: ""mongod"", operating_system.DEBIAN: ""mongodb"", operating_system.SUSE: ""mongod""}[OS_NAME]",73,145
openstack%2Ftrove~master~I7f06db632ad5159e39f1011245911127768846e5,openstack/trove,master,I7f06db632ad5159e39f1011245911127768846e5,Stream MongoDB backup/restore,NEW,2016-11-10 16:01:27.000000000,2017-12-18 01:27:05.000000000,,"[{'_account_id': 9664}, {'_account_id': 15321}, {'_account_id': 22818}, {'_account_id': 26895}]","[{'number': 1, 'created': '2016-11-10 16:01:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/32f5212fa7bd3c0cf028165c576062a5c712dc24', 'message': 'Stream MongoDB backup/restore\n\nMongoDB 3.2 introduced the ability to stream mongodump/mongorestore\ninstead of just writing out files. This means Trove can avoid dumping\nthe data to disk during backup/restore.\n\nThis change also adds mongod.log as a guest log. See the comments in the\nmongo_impl files for details.\n\nChange-Id: I7f06db632ad5159e39f1011245911127768846e5\nCloses-Bug: 1594414\n'}, {'number': 2, 'created': '2016-11-14 14:29:09.000000000', 'files': ['releasenotes/notes/mongodb-streaming-f2ce6fba42ae9d68.yaml', 'trove/guestagent/strategies/backup/experimental/mongo_impl.py', 'trove/guestagent/strategies/restore/experimental/mongo_impl.py', 'trove/tests/unittests/backup/test_backupagent.py', 'trove/guestagent/datastore/experimental/mongodb/manager.py', 'trove/tests/unittests/guestagent/test_backups.py'], 'web_link': 'https://opendev.org/openstack/trove/commit/f8e5f7c9b7b869b81362dd74ae245c2b65681ad7', 'message': 'Stream MongoDB backup/restore\n\nMongoDB 3.2 introduced the ability to stream mongodump/mongorestore\ninstead of just writing out files. This means Trove can avoid dumping\nthe data to disk during backup/restore.\n\nThis change also adds mongod.log as a guest log. See the comments in the\nmongo_impl files for details.\n\nChange-Id: I7f06db632ad5159e39f1011245911127768846e5\nCloses-Bug: 1594414\n'}]",0,396306,f8e5f7c9b7b869b81362dd74ae245c2b65681ad7,8,4,2,15321,,,0,"Stream MongoDB backup/restore

MongoDB 3.2 introduced the ability to stream mongodump/mongorestore
instead of just writing out files. This means Trove can avoid dumping
the data to disk during backup/restore.

This change also adds mongod.log as a guest log. See the comments in the
mongo_impl files for details.

Change-Id: I7f06db632ad5159e39f1011245911127768846e5
Closes-Bug: 1594414
",git fetch https://review.opendev.org/openstack/trove refs/changes/06/396306/1 && git format-patch -1 --stdout FETCH_HEAD,"['trove/guestagent/strategies/backup/experimental/mongo_impl.py', 'trove/guestagent/strategies/restore/experimental/mongo_impl.py', 'trove/tests/unittests/backup/test_backupagent.py', 'trove/guestagent/datastore/experimental/mongodb/manager.py', 'trove/tests/unittests/guestagent/test_backups.py']",5,32f5212fa7bd3c0cf028165c576062a5c712dc24,bug/1594414,"MONGODUMP_CMD = ""mongodump --archive --quiet"" MONGODUMP_RESTORE = ""mongorestore --archive --quiet"" @patch('trove.guestagent.datastore.experimental.' 'mongodb.service.MongoDBApp') def test_backup_encrypted_mongodump_command(self, mock_app, _): mock_app().admin_cmd_auth_params.return_value = [] @patch('trove.guestagent.datastore.experimental.' 'mongodb.service.MongoDBApp') def test_backup_not_encrypted_mongodump_command(self, mock_app, _): mock_app().admin_cmd_auth_params.return_value = [] @patch('trove.guestagent.datastore.experimental.' 'mongodb.service.MongoDBApp') def test_restore_decrypted_mongodump_command(self, mock_app, _): mock_app().admin_cmd_auth_params.return_value = [] @patch('trove.guestagent.datastore.experimental.' 'mongodb.service.MongoDBApp') def test_restore_encrypted_mongodump_command(self, mock_app, _): mock_app().admin_cmd_auth_params.return_value = [] self.app_patch = patch('trove.guestagent.datastore.experimental.' 'mongodb.service.MongoDBApp') self.app_mock = self.app_patch.start() self.app_mock().admin_cmd_auth_params.return_value = [] self.addCleanup(self.app_patch.stop) self.app_patch = patch('trove.guestagent.datastore.experimental.' 'mongodb.service.MongoDBApp') self.app_mock = self.app_patch.start() self.app_mock().admin_cmd_auth_params.return_value = [] self.addCleanup(self.app_patch.stop) ","MONGODUMP_CMD = ""sudo tar cPf - /var/lib/mongodb/dump"" MONGODUMP_RESTORE = ""sudo tar xPf -"" def test_backup_encrypted_mongodump_command(self, _): def test_backup_not_encrypted_mongodump_command(self, _): def test_restore_decrypted_mongodump_command(self, _): def test_restore_encrypted_mongodump_command(self, _):",67,115
openstack%2Ftrove~master~I1da64e2cdf51cd0b934546e3a254a723823704d0,openstack/trove,master,I1da64e2cdf51cd0b934546e3a254a723823704d0,Remove unnecessary setUp()and tearDown() calls in tests,NEW,2017-08-01 11:22:41.000000000,2017-12-18 01:27:02.000000000,,"[{'_account_id': 8871}, {'_account_id': 13562}, {'_account_id': 18602}, {'_account_id': 20191}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-08-01 11:22:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/2f06d2719e597c02d2374249acec393fd2a55bdb', 'message': 'Remove unnecessary setUp() calls in tests\n\nTrivialFix\n\nChange-Id: I1da64e2cdf51cd0b934546e3a254a723823704d0\n'}, {'number': 2, 'created': '2017-08-23 09:35:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/0e9ca41c1c858fc5e14460736ee8e00abc106720', 'message': 'Remove unnecessary setUp() calls in tests\n\nTrivialFix\n\nChange-Id: I1da64e2cdf51cd0b934546e3a254a723823704d0\n'}, {'number': 3, 'created': '2017-08-24 15:44:50.000000000', 'files': ['trove/tests/unittests/backup/test_backup_models.py', 'trove/tests/unittests/conductor/test_conf.py', 'trove/tests/unittests/common/test_remote.py', 'trove/tests/unittests/cluster/test_cassandra_cluster.py', 'trove/tests/unittests/common/test_stream_codecs.py', 'trove/tests/unittests/db/test_migration_utils.py', 'trove/tests/unittests/api/common/test_extensions.py', 'trove/tests/unittests/api/common/test_limits.py', 'trove/tests/unittests/mgmt/test_clusters.py', 'trove/tests/unittests/guestagent/test_service.py', 'trove/tests/unittests/backup/test_storage.py', 'trove/tests/unittests/common/test_dbmodels.py', 'trove/tests/unittests/common/test_template.py', 'trove/tests/unittests/common/test_timeutils.py', 'trove/tests/unittests/common/test_secure_serializer.py', 'trove/tests/unittests/api/test_versions.py', 'trove/tests/unittests/cluster/test_redis_cluster.py', 'trove/tests/unittests/guestagent/test_backups.py', 'trove/tests/api/mgmt/instances_actions.py', 'trove/tests/unittests/module/test_module_views.py', 'trove/tests/unittests/configuration/test_configuration_controller.py', 'trove/tests/unittests/guestagent/test_couchbase_manager.py', 'trove/tests/unittests/common/test_serializer.py', 'trove/tests/unittests/guestagent/test_mongodb_cluster_manager.py', 'trove/tests/unittests/guestagent/test_volume.py', 'trove/tests/unittests/upgrade/test_models.py', 'trove/tests/unittests/cluster/test_vertica_cluster.py', 'trove/tests/unittests/instance/test_instance_models.py', 'trove/tests/unittests/taskmanager/test_models.py', 'trove/tests/unittests/cluster/test_cluster_views.py', 'trove/tests/unittests/guestagent/test_mongodb_manager.py', 'trove/tests/unittests/datastore/test_datastore_version_metadata.py', 'trove/tests/unittests/guestagent/test_agent_heartbeats_models.py', 'trove/tests/unittests/guestagent/test_manager.py', 'trove/tests/unittests/cluster/test_mongodb_cluster.py', 'trove/tests/unittests/guestagent/test_dbaas.py', 'trove/tests/unittests/quota/test_quota.py', 'trove/tests/unittests/common/test_crypto_utils.py', 'trove/tests/unittests/flavor/test_flavor_views.py', 'trove/tests/unittests/mgmt/test_datastores.py', 'trove/tests/unittests/cluster/test_galera_cluster.py', 'trove/tests/unittests/conductor/test_methods.py', 'trove/tests/unittests/guestagent/test_query.py', 'trove/tests/unittests/datastore/test_capability.py', 'trove/tests/unittests/domain-name-service/test_designate_driver.py', 'trove/tests/api/mgmt/quotas.py', 'trove/tests/unittests/mysql/test_common.py'], 'web_link': 'https://opendev.org/openstack/trove/commit/2b0c6f4551cf849336f8d091a71856cfbeefa69c', 'message': 'Remove unnecessary setUp()and tearDown() calls in tests\n\nTrivialFix\n\nChange-Id: I1da64e2cdf51cd0b934546e3a254a723823704d0\n'}]",0,489567,2b0c6f4551cf849336f8d091a71856cfbeefa69c,20,5,3,18602,,,0,"Remove unnecessary setUp()and tearDown() calls in tests

TrivialFix

Change-Id: I1da64e2cdf51cd0b934546e3a254a723823704d0
",git fetch https://review.opendev.org/openstack/trove refs/changes/67/489567/1 && git format-patch -1 --stdout FETCH_HEAD,"['trove/tests/unittests/conductor/test_conf.py', 'trove/tests/unittests/module/test_module_views.py', 'trove/tests/unittests/common/test_remote.py', 'trove/tests/unittests/configuration/test_configuration_controller.py', 'trove/tests/unittests/common/test_stream_codecs.py', 'trove/tests/unittests/db/test_migration_utils.py', 'trove/tests/unittests/upgrade/test_models.py', 'trove/tests/unittests/api/common/test_extensions.py', 'trove/tests/unittests/api/common/test_limits.py', 'trove/tests/unittests/instance/test_instance_models.py', 'trove/tests/unittests/guestagent/test_service.py', 'trove/tests/unittests/guestagent/test_dbaas.py', 'trove/tests/unittests/backup/test_storage.py', 'trove/tests/unittests/common/test_timeutils.py', 'trove/tests/unittests/common/test_crypto_utils.py', 'trove/tests/unittests/guestagent/test_query.py', 'trove/tests/unittests/api/test_versions.py', 'trove/tests/unittests/datastore/test_capability.py', 'trove/tests/unittests/domain-name-service/test_designate_driver.py', 'trove/tests/api/mgmt/quotas.py', 'trove/tests/unittests/mysql/test_common.py']",21,2f06d2719e597c02d2374249acec393fd2a55bdb,,," def setUp(self): super(MySqlCommonTest, self).setUp() ",0,64
openstack%2Ftricircle~master~Iab3d1160bc4ef32ff19f68333ff49f2c3b3e0293,openstack/tricircle,master,Iab3d1160bc4ef32ff19f68333ff49f2c3b3e0293,[Urgent] Fix unit test error,MERGED,2017-12-17 13:08:35.000000000,2017-12-18 01:25:21.000000000,2017-12-18 01:25:21.000000000,"[{'_account_id': 11819}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-17 13:08:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tricircle/commit/278391f05e1308b717b1b60b4fa6581db7711acc', 'message': '[Urgent] Fix unit test error\n\n1. What is the problem\nUnit tests fail due to database operation changes in neutron.\n\n2. What is the solution for the problem\nModify unit tests to adapt the change.\n\n3. What features need to be implemented to the Tricircle to\nrealize the solution\nN/A\n\nChange-Id: Iab3d1160bc4ef32ff19f68333ff49f2c3b3e0293\n'}, {'number': 2, 'created': '2017-12-17 14:19:32.000000000', 'files': ['tricircle/tests/unit/network/test_central_plugin.py', 'tricircle/network/central_plugin.py'], 'web_link': 'https://opendev.org/openstack/tricircle/commit/61087e1649fd188a01da9b886f9348be0a33bd23', 'message': '[Urgent] Fix unit test error\n\n1. What is the problem\nUnit tests fail due to database operation changes in neutron.\n\n2. What is the solution for the problem\nModify unit tests to adapt the change.\n\n3. What features need to be implemented to the Tricircle to\nrealize the solution\nN/A\n\nChange-Id: Iab3d1160bc4ef32ff19f68333ff49f2c3b3e0293\n'}]",0,528552,61087e1649fd188a01da9b886f9348be0a33bd23,8,2,2,12076,,,0,"[Urgent] Fix unit test error

1. What is the problem
Unit tests fail due to database operation changes in neutron.

2. What is the solution for the problem
Modify unit tests to adapt the change.

3. What features need to be implemented to the Tricircle to
realize the solution
N/A

Change-Id: Iab3d1160bc4ef32ff19f68333ff49f2c3b3e0293
",git fetch https://review.opendev.org/openstack/tricircle refs/changes/52/528552/2 && git format-patch -1 --stdout FETCH_HEAD,"['tricircle/tests/unit/network/test_central_plugin.py', 'tricircle/network/central_plugin.py']",2,278391f05e1308b717b1b60b4fa6581db7711acc,fix-UT,"import neutron.objects.router as router_object router_object.FloatingIP.update_objects(context, org_data, id=_id)", with context.session.begin(): fip_qry = context.session.query(l3_db.FloatingIP) floating_ips = fip_qry.filter_by(id=_id) for floating_ip in floating_ips: floating_ip.update(org_data),56,23
openstack%2Fdevstack-gate~master~I0a65ec2c750aed9ab1b1b83ebfe93e948a525b68,openstack/devstack-gate,master,I0a65ec2c750aed9ab1b1b83ebfe93e948a525b68,Adding zero downtime upgrades gates for cinder,NEW,2017-01-14 18:57:07.000000000,2017-12-18 01:24:37.000000000,,"[{'_account_id': 4}, {'_account_id': 11904}, {'_account_id': 23310}]","[{'number': 1, 'created': '2017-01-14 18:57:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/4725befce159ef0e29d715c3cf2d048ee9b2a682', 'message': 'Adding zero downtime upgrades gates for cinder\n\nThere is need to test cinder services in environments wherein an old & new\nversion of each service needs to be run concurrently. This is mandatory to\nget the assert:supports-zero-downtime-upgrade tag for cinder.\n\nHence, there is a need to test services in below four combinations.\n\nprimary: c-api\nsubnode: c-api c-sch c-vol c-bak\n\nprimary: c-api c-sch\nsubnode: c-sch c-vol c-bak\n\nprimary: c-api c-sch c-vol\nsubnode: c-bak\n\nChange-Id: I0a65ec2c750aed9ab1b1b83ebfe93e948a525b68\nprimary: c-api c-sch c-vol c-bak\nsubnode: c-bak\n'}, {'number': 2, 'created': '2017-01-14 19:28:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/01f0d56b2747c7982ad9af1f7374b9916065e63b', 'message': 'Adding zero downtime upgrades gates for cinder\n\nThere is need to test cinder services in environments wherein an old & new\nversion of each service needs to be run concurrently. This is mandatory to\nget the assert:supports-zero-downtime-upgrade tag for cinder.\n\nHence, there is a need to test services in below four combinations.\n\nprimary: c-api\nsubnode: c-api c-sch c-vol c-bak\n\nprimary: c-api c-sch\nsubnode: c-sch c-vol c-bak\n\nprimary: c-api c-sch c-vol\nsubnode: c-bak\n\nprimary: c-api c-sch c-vol c-bak\nsubnode: c-bak\n\nChange-Id: I0a65ec2c750aed9ab1b1b83ebfe93e948a525b68\n'}, {'number': 3, 'created': '2017-01-18 16:29:52.000000000', 'files': ['features.yaml'], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/f95605b7a1ed8b37890704cb1ac2cbef99a86c63', 'message': 'Adding zero downtime upgrades gates for cinder\n\nThere is need to test cinder services in environments wherein an old & new\nversion of each service needs to be run concurrently. This is mandatory to\nget the assert:supports-zero-downtime-upgrade tag for cinder.\n\nHence, there is a need to test c-api in the below combination.\n\nprimary: c-api\nsubnode: c-api c-sch c-vol c-bak\n\nChange-Id: I0a65ec2c750aed9ab1b1b83ebfe93e948a525b68\n'}]",1,420362,f95605b7a1ed8b37890704cb1ac2cbef99a86c63,12,3,3,20140,,,0,"Adding zero downtime upgrades gates for cinder

There is need to test cinder services in environments wherein an old & new
version of each service needs to be run concurrently. This is mandatory to
get the assert:supports-zero-downtime-upgrade tag for cinder.

Hence, there is a need to test c-api in the below combination.

primary: c-api
subnode: c-api c-sch c-vol c-bak

Change-Id: I0a65ec2c750aed9ab1b1b83ebfe93e948a525b68
",git fetch https://review.opendev.org/openstack/devstack-gate refs/changes/62/420362/1 && git format-patch -1 --stdout FETCH_HEAD,['features.yaml'],1,4725befce159ef0e29d715c3cf2d048ee9b2a682,zerodowntimeupgragdesjobs," cinder_zerodowntime_api: features: [cinder-mn-grenade-zerodnupg-api] cinder_zerodowntime_sch: features: [cinder-mn-grenade-zerodnupg-sch] cinder_zerodowntime_vol: features: [cinder-mn-grenade-zerodnupg-vol] cinder_zerodowntime_bak: features: [cinder-mn-grenade-zerodnupg-bak] neutron_dvr: # This will be to test cinder patches in a zero down upgrades scenario # This has new c-api on primary and an old c-api on subnode cinder-mn-grenade-zerodnupg-api: base: rm-services: [c-vol, c-sch, c-bak] cinder-mn-grenade-zerodnupg-sch: base: rm-services: [c-vol, c-bak] cinder-mn-grenade-zerodnupg-vol: base: rm-services: [c-bak] cinder-mn-grenade-zerodnupg-bak: base: rm-services: [] cinder-mn-grenade-zerodnupg-api: base: services: [c-api, c-sch] cinder-mn-grenade-zerodnupg-sch: base: services: [c-sch] cinder-mn-grenade-zerodnupg-vol: base: services: [] cinder-mn-grenade-zerodnupg-bak: base: rm-services: [c-vol] ", neutron_dvr:,43,1
openstack%2Ftacker~master~Ifaafbb920210f988c51eae6df5abf50d3c7bb7dd,openstack/tacker,master,Ifaafbb920210f988c51eae6df5abf50d3c7bb7dd,Uploading OpenWRT cloud image while installation,NEW,2017-01-17 16:06:55.000000000,2017-12-18 01:23:33.000000000,,"[{'_account_id': 2874}, {'_account_id': 13380}, {'_account_id': 16034}, {'_account_id': 18955}, {'_account_id': 19690}, {'_account_id': 24634}]","[{'number': 1, 'created': '2017-01-17 16:06:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/7c5a0c521a7431b3b46f2509f6e8e07a176675d1', 'message': 'Uploading OpenWRT cloud image while installation\n\nThis patch will download a cloud image of OpenWRT instead of the\nregular one, to solve the issues of ping failures with tacker OpenWRT\nsample vnf while using a regular image of OpenWRT.\n\nChange-Id: Ifaafbb920210f988c51eae6df5abf50d3c7bb7dd\nCloses-Bug: #1557008\n'}, {'number': 2, 'created': '2017-02-06 10:43:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/3721729be92959d9eae2ebdee23928d2456ce0b3', 'message': 'Uploading OpenWRT cloud image while installation\n\nThis patch will download a cloud image of OpenWRT instead of the\nregular one, to solve the issues of ping failures with tacker OpenWRT\nsample vnf while using a regular image of OpenWRT.\n\nChange-Id: Ifaafbb920210f988c51eae6df5abf50d3c7bb7dd\nCloses-Bug: #1557008\n'}, {'number': 3, 'created': '2017-02-06 10:49:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/9c3b8383bcbeed33ad5fb381b1fdc999e75cfab3', 'message': 'Uploading OpenWRT cloud image while installation\n\nThis patch will download a cloud image of OpenWRT instead of the\nregular one, to solve the issues of ping failures with tacker OpenWRT\nsample vnf while using a regular image of OpenWRT.\n\nChange-Id: Ifaafbb920210f988c51eae6df5abf50d3c7bb7dd\nCloses-Bug: #1557008\n'}, {'number': 4, 'created': '2017-02-06 13:50:36.000000000', 'files': ['devstack/lib/tacker', 'devstack/settings'], 'web_link': 'https://opendev.org/openstack/tacker/commit/f918646d9a5d64fff0de110195c87f21e718f946', 'message': 'Uploading OpenWRT cloud image while installation\n\nThis patch will download a cloud image of OpenWRT instead of the\nregular one, to solve the issues of ping failures with tacker OpenWRT\nsample vnf while using a regular image of OpenWRT.\n\nChange-Id: Ifaafbb920210f988c51eae6df5abf50d3c7bb7dd\nCloses-Bug: #1557008\n'}]",5,421340,f918646d9a5d64fff0de110195c87f21e718f946,20,6,4,19690,,,0,"Uploading OpenWRT cloud image while installation

This patch will download a cloud image of OpenWRT instead of the
regular one, to solve the issues of ping failures with tacker OpenWRT
sample vnf while using a regular image of OpenWRT.

Change-Id: Ifaafbb920210f988c51eae6df5abf50d3c7bb7dd
Closes-Bug: #1557008
",git fetch https://review.opendev.org/openstack/tacker refs/changes/40/421340/1 && git format-patch -1 --stdout FETCH_HEAD,['devstack/lib/tacker'],1,7c5a0c521a7431b3b46f2509f6e8e07a176675d1,, local image_url=https://github.com/samos123/openstack-openwrt-image/releases/download/0.1/openwrt-x86-kvm_guest-combined-ext4.img, local image_url=https://downloads.openwrt.org/chaos_calmer/15.05/x86/kvm_guest/openwrt-15.05-x86-kvm_guest-combined-ext4.img.gz,1,1
openstack%2Fpython-heatclient~master~I6fe642de7cc16212de6a5b19f0585e505e65d128,openstack/python-heatclient,master,I6fe642de7cc16212de6a5b19f0585e505e65d128,Deprecate heat CLI,NEW,2016-03-22 15:31:30.000000000,2017-12-18 01:22:09.000000000,,[],"[{'number': 1, 'created': '2016-03-22 15:31:30.000000000', 'files': ['heatclient/tests/unit/test_shell.py', 'heatclient/shell.py'], 'web_link': 'https://opendev.org/openstack/python-heatclient/commit/e84449d5ea8bf1556aa99a3b20359345938d4f3e', 'message': ""Deprecate heat CLI\n\nThe heat CLI is now deprecated. Every time you run it it's\ngoing to print out an annoying message saying how deprecated it\nis.  This is based upon what keystone did and allows the warning\nto eventually show up in the CLI reference manual.\n\nChange-Id: I6fe642de7cc16212de6a5b19f0585e505e65d128\n""}]",0,295904,e84449d5ea8bf1556aa99a3b20359345938d4f3e,8,0,1,7128,,,0,"Deprecate heat CLI

The heat CLI is now deprecated. Every time you run it it's
going to print out an annoying message saying how deprecated it
is.  This is based upon what keystone did and allows the warning
to eventually show up in the CLI reference manual.

Change-Id: I6fe642de7cc16212de6a5b19f0585e505e65d128
",git fetch https://review.opendev.org/openstack/python-heatclient refs/changes/04/295904/1 && git format-patch -1 --stdout FETCH_HEAD,"['heatclient/tests/unit/test_shell.py', 'heatclient/shell.py']",2,e84449d5ea8bf1556aa99a3b20359345938d4f3e,add-deprecation-warning,"import warnings def _show_deprecated(self): # Since Python 2.7, DeprecationWarning is ignored by default, enable # it so that the deprecation message is displayed. warnings.simplefilter('once', category=DeprecationWarning) warnings.warn( 'The heat CLI is deprecated in favor of ' 'python-openstackclient. For a Python library, continue using ' 'python-heatclient.', DeprecationWarning) # And back to normal! warnings.resetwarnings() def main(self, argv): self._show_deprecated() "," def main(self, argv):",15,0
openstack%2Fwatcher~master~I0af2c9fd266f925af5e3e8731b37a00dab91d6a8,openstack/watcher,master,I0af2c9fd266f925af5e3e8731b37a00dab91d6a8,Updated from global requirements,MERGED,2017-12-15 22:24:19.000000000,2017-12-18 01:21:59.000000000,2017-12-18 01:21:59.000000000,"[{'_account_id': 19457}, {'_account_id': 21692}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 22:24:19.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/watcher/commit/9ccd17e40b4ae038413e6e5ab98914a1cd7d0630', 'message': 'Updated from global requirements\n\nChange-Id: I0af2c9fd266f925af5e3e8731b37a00dab91d6a8\n'}]",0,528434,9ccd17e40b4ae038413e6e5ab98914a1cd7d0630,7,3,1,11131,,,0,"Updated from global requirements

Change-Id: I0af2c9fd266f925af5e3e8731b37a00dab91d6a8
",git fetch https://review.opendev.org/openstack/watcher refs/changes/34/528434/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,9ccd17e40b4ae038413e6e5ab98914a1cd7d0630,openstack/requirements,"oslo.service!=1.28.1,>=1.24.0 # Apache-2.0",oslo.service>=1.24.0 # Apache-2.0,1,1
openstack%2Ftrove~master~I9635f202e67ece60870b305cb5a3ab5e63111428,openstack/trove,master,I9635f202e67ece60870b305cb5a3ab5e63111428,Trove cluster does not reset task status after cluster_usage_timout.,NEW,2017-04-22 14:25:25.000000000,2017-12-18 01:21:20.000000000,,"[{'_account_id': 5293}, {'_account_id': 9664}, {'_account_id': 10215}, {'_account_id': 13562}, {'_account_id': 22348}, {'_account_id': 22484}, {'_account_id': 22608}, {'_account_id': 22694}, {'_account_id': 23326}]","[{'number': 1, 'created': '2017-04-22 14:25:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/5e277631d0ed3b09797a6851a3786db684449a59', 'message': 'Trove cluster does not reset task status after cluster_usage_timout.\n\nFor example: after timeout determined by parameter cluster_usage_timeout for building cluster, the trove cluster task status is still ""Running"" which does not turn to ""None"".\n\nThis will influence cluser create, grow and shrink.\n\nChange-Id: I9635f202e67ece60870b305cb5a3ab5e63111428\nCloses-Bug: #1685510\n'}, {'number': 2, 'created': '2017-04-24 02:35:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/290a1611063ece644934d3f0d1d4928238248fd7', 'message': ""Trove cluster does not reset task status after cluster_usage_timout.\n\nFor example: after timeout determined by parameter cluster_usage_timeout for building cluster, the trove cluster task status is still 'Running' which does not turn to 'None'.\n\nThis bug is caused by that after Timeout exception, it just update the cluster instances' statuses by execute self.update_statuses_on_failure(cluster_id) which will not update the cluster task's status.\n\nThis will influence cluser create, grow and shrink.\n\nChange-Id: I9635f202e67ece60870b305cb5a3ab5e63111428\nCloses-Bug: #1685510\n""}, {'number': 3, 'created': '2017-04-24 03:16:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/e64f33f231451713e83a138946649cad412735ad', 'message': ""Trove cluster does not reset task status after cluster_usage_timout.\n\nFor example: after timeout determined by parameter cluster_usage_timeout\nfor building cluster, the trove cluster task status is still 'Building'\nwhich does not turn to 'None'.\n\nThis bug is caused by that after Timeout exception, it just update the\ncluster instances' statuses by execute self.update_statuses_on_failure(cluster_id)\nwhich will not update the cluster task's status.\n\nThis will influence cluser create, grow and shrink.\n\nChange-Id: I9635f202e67ece60870b305cb5a3ab5e63111428\nCloses-Bug: #1685510\n""}, {'number': 4, 'created': '2017-04-24 08:53:50.000000000', 'files': ['trove/taskmanager/models.py'], 'web_link': 'https://opendev.org/openstack/trove/commit/cd2f1634264ea9e84121a4e8149cc9de513632f1', 'message': ""Trove cluster does not reset task status after cluster_usage_timout.\n\nFor example: after timeout determined by parameter cluster_usage_timeout\nfor building cluster, the trove cluster task status is still 'Building'\nwhich does not turn to 'None'.\n\nThis bug is caused by that after Timeout exception, it just update the\ncluster instances' statuses by execute self.update_statuses_on_failure(cluster_id)\nwhich will not update the cluster task's status.\n\nThis will influence cluser create, grow and shrink.\n\nChange-Id: I9635f202e67ece60870b305cb5a3ab5e63111428\nCloses-Bug: #1685510\n""}]",1,459059,cd2f1634264ea9e84121a4e8149cc9de513632f1,23,9,4,23326,,,0,"Trove cluster does not reset task status after cluster_usage_timout.

For example: after timeout determined by parameter cluster_usage_timeout
for building cluster, the trove cluster task status is still 'Building'
which does not turn to 'None'.

This bug is caused by that after Timeout exception, it just update the
cluster instances' statuses by execute self.update_statuses_on_failure(cluster_id)
which will not update the cluster task's status.

This will influence cluser create, grow and shrink.

Change-Id: I9635f202e67ece60870b305cb5a3ab5e63111428
Closes-Bug: #1685510
",git fetch https://review.opendev.org/openstack/trove refs/changes/59/459059/1 && git format-patch -1 --stdout FETCH_HEAD,"['trove/common/strategies/cluster/experimental/redis/taskmanager.py', 'trove/common/strategies/cluster/experimental/cassandra/taskmanager.py', 'trove/common/strategies/cluster/experimental/vertica/taskmanager.py', 'trove/common/strategies/cluster/experimental/mongodb/taskmanager.py', 'trove/common/strategies/cluster/experimental/galera_common/taskmanager.py']",5,5e277631d0ed3b09797a6851a3786db684449a59,bug/1685510, self.reset_task() self.reset_task() self.reset_task(),,15,0
openstack%2Fironic~master~I90ba44d7170270f511b8a2aa8610df8caaa20b21,openstack/ironic,master,I90ba44d7170270f511b8a2aa8610df8caaa20b21,Add negative tests for VIF attach/detach operations,NEW,2017-02-20 12:00:18.000000000,2017-12-18 01:20:35.000000000,,"[{'_account_id': 7002}, {'_account_id': 10118}, {'_account_id': 10239}, {'_account_id': 11878}, {'_account_id': 12356}, {'_account_id': 13295}, {'_account_id': 14525}, {'_account_id': 14614}, {'_account_id': 14629}, {'_account_id': 15784}, {'_account_id': 17270}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}]","[{'number': 1, 'created': '2017-02-20 12:00:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/4ac0839ccae1e566aa226f0a75839532dc74499b', 'message': 'Add negative test for VIF attach/detach operations\n\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 2, 'created': '2017-02-20 14:59:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3f6d6ef958b3405c294edd7614db39a2b51da205', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_set_no_free_port\n  test_vif_set_no_any_port\n  test_vif_no_args\n\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 3, 'created': '2017-02-20 15:39:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bd4ea34a85705414ffe7b68543a5e0a977fa9aa8', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_set_no_free_port\n  test_vif_set_no_any_port\n  test_vif_attach_wrong_node\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 4, 'created': '2017-02-21 14:26:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ff87d152ed90b214e5729b62c53e05467866b73f', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_already_attached_on_internal_info\n  test_vif_attach_no_free_port\n  test_vif_attach_no_any_port\n  test_vif_attach_node_doesnt_exist\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nCloses-Bug: #1666534\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 5, 'created': '2017-02-21 17:18:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8241011e7cf683c7545be12d0a9cd7b827b5f1f5', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_already_attached_on_internal_info\n  test_vif_attach_no_free_port\n  test_vif_attach_no_port\n  test_vif_attach_node_doesnt_exist\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nCloses-Bug: #1666534\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 6, 'created': '2017-02-23 10:55:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ecc2425d5e1e33aee3d85093d5f974a15feb5dfa', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_already_attached_on_internal_info\n  test_vif_attach_no_free_port\n  test_vif_attach_no_port\n  test_vif_attach_node_doesnt_exist\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nCloses-Bug: #1666534\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 7, 'created': '2017-02-23 14:40:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8e258a1e7b17e315e73d78bb81baa14763548958', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_already_attached_on_internal_info\n  test_vif_attach_no_free_port\n  test_vif_attach_no_port\n  test_vif_attach_node_doesnt_exist\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nCloses-Bug: #1666534\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 8, 'created': '2017-03-03 16:54:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/503b18742d91c6e2c36b3e168e923b16653034df', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_already_attached_on_internal_info\n  test_vif_already_attached_with_portgroups\n  test_vif_attach_no_free_port\n  test_vif_attach_no_port\n  test_vif_attach_with_portgroup_no_port\n  test_vif_attach_port_and_portgroup_unplugged\n  test_vif_attach_node_doesnt_exist\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nCloses-Bug: #1666534\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 9, 'created': '2017-03-16 13:19:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/4318b36d380d18e964531cd78028bbe9b21c3f78', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_already_attached_on_internal_info\n  test_vif_already_attached_with_portgroups\n  test_vif_attach_no_free_port\n  test_vif_attach_no_port\n  test_vif_attach_with_portgroup_no_port\n  test_vif_attach_port_and_portgroup_unplugged\n  test_vif_attach_node_doesnt_exist\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nCloses-Bug: #1666534\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 10, 'created': '2017-03-20 11:40:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8b9bae4df2bc4ddd30ea37dd1825a98385bab26b', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_already_attached_on_internal_info\n  test_vif_already_attached_with_portgroups\n  test_vif_attach_no_free_port\n  test_vif_attach_no_port\n  test_vif_attach_with_portgroup_no_port\n  test_vif_attach_port_and_portgroup_unplugged\n  test_vif_attach_node_doesnt_exist\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nCloses-Bug: #1666534\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 11, 'created': '2017-03-20 15:39:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cbc871e282a18a6c4027b42de60bda33a6e8a541', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_already_attached_on_internal_info\n  test_vif_already_attached_with_portgroups\n  test_vif_attach_no_free_port\n  test_vif_attach_no_port\n  test_vif_attach_with_portgroup_no_port\n  test_vif_attach_port_and_portgroup_unplugged\n  test_vif_attach_node_doesnt_exist\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nCloses-Bug: #1666534\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 12, 'created': '2017-03-20 15:40:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2ae01ff5d74fe15d445ce66ee67f78bc9e6a9b47', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_already_attached_on_internal_info\n  test_vif_already_attached_with_portgroups\n  test_vif_attach_no_free_port\n  test_vif_attach_no_port\n  test_vif_attach_with_portgroup_no_port\n  test_vif_attach_port_and_portgroup_unplugged\n  test_vif_attach_node_doesnt_exist\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nCloses-Bug: #1666534\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 13, 'created': '2017-03-21 16:13:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b1d0ecf6607114fadc93209425610bcf6b9136d5', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_already_attached_on_internal_info\n  test_vif_already_attached_with_portgroups\n  test_vif_attach_no_free_port\n  test_vif_attach_no_port\n  test_vif_attach_with_empty_portgroup\n  test_vif_attach_port_not_in_portgroup\n  test_vif_attach_node_doesnt_exist\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nCloses-Bug: #1666534\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 14, 'created': '2017-03-23 12:37:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a253beb9436bb493b482d27cedb9854f7bbca212', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_already_attached_on_internal_info\n  test_vif_already_attached_with_portgroups\n  test_vif_attach_no_free_port\n  test_vif_attach_no_port\n  test_vif_attach_with_empty_portgroup\n  test_vif_attach_port_not_in_portgroup\n  test_vif_attach_node_doesnt_exist\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nCloses-Bug: #1666534\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}, {'number': 15, 'created': '2017-08-04 14:27:50.000000000', 'files': ['ironic_tempest_plugin/tests/api/admin/test_nodes.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/ad3cbb40342159aafde62bb8fc44dbd0664c98c5', 'message': 'Add negative tests for VIF attach/detach operations\n\nAdd tests:\n  test_vif_already_attached_on_internal_info\n  test_vif_already_attached_with_portgroups\n  test_vif_attach_no_free_port\n  test_vif_attach_no_port\n  test_vif_attach_with_empty_portgroup\n  test_vif_attach_port_not_in_portgroup\n  test_vif_attach_node_doesnt_exist\n  test_vif_attach_no_args\n  test_vif_detach_not_existing\n  test_vif_detach_no_args\n\nCloses-Bug: #1666534\nChange-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21\n'}]",48,435952,ad3cbb40342159aafde62bb8fc44dbd0664c98c5,115,14,15,14614,,,0,"Add negative tests for VIF attach/detach operations

Add tests:
  test_vif_already_attached_on_internal_info
  test_vif_already_attached_with_portgroups
  test_vif_attach_no_free_port
  test_vif_attach_no_port
  test_vif_attach_with_empty_portgroup
  test_vif_attach_port_not_in_portgroup
  test_vif_attach_node_doesnt_exist
  test_vif_attach_no_args
  test_vif_detach_not_existing
  test_vif_detach_no_args

Closes-Bug: #1666534
Change-Id: I90ba44d7170270f511b8a2aa8610df8caaa20b21
",git fetch https://review.opendev.org/openstack/ironic refs/changes/52/435952/7 && git format-patch -1 --stdout FETCH_HEAD,['ironic_tempest_plugin/tests/api/admin/test_nodes.py'],1,4ac0839ccae1e566aa226f0a75839532dc74499b,bug/1666534," @decorators.idempotent_id('628350f8-4498-4204-b546-f3c01b93c7e3') def test_vif_already_set_on_internal_info(self): """"""Negative test for duplicated attachment/detachment of VIFs. Steps: 1) Create chassis and node in setUp. 2) Create port for the node. 3) Attach VIF to the node. 4) Try to attach one more VIF to the same node with port. 5) Detach VIF from the node. 6) Try to detach one VIF from the same node one more time. """""" self.useFixture( api_microversion_fixture.APIMicroversionFixture('1.28')) _, self.port = self.create_port(self.node['uuid'], data_utils.rand_mac_address()) self.client.vif_attach(self.node['uuid'], 'test-vif') _, body = self.client.vif_list(self.node['uuid']) self.assertEqual(body, {'vifs': [{'id': 'test-vif'}]}) self.assertRaises(lib_exc.Conflict, self.client.vif_attach, self.node['uuid'], 'test-vif') self.client.vif_detach(self.node['uuid'], 'test-vif') self.assertRaises(lib_exc.BadRequest, self.client.vif_detach, self.node['uuid'], 'test-vif')",,29,0
openstack%2Fironic~master~I8d96a89f92665ba4ca2c71623c0578c5442a5ac9,openstack/ironic,master,I8d96a89f92665ba4ca2c71623c0578c5442a5ac9,Add tempest plugin API tests for node,NEW,2016-12-15 09:48:51.000000000,2017-12-18 01:20:32.000000000,,"[{'_account_id': 6773}, {'_account_id': 10118}, {'_account_id': 10206}, {'_account_id': 10239}, {'_account_id': 11878}, {'_account_id': 12356}, {'_account_id': 14525}, {'_account_id': 14614}, {'_account_id': 14629}, {'_account_id': 17270}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 19604}, {'_account_id': 20675}, {'_account_id': 22724}, {'_account_id': 23330}]","[{'number': 1, 'created': '2016-12-15 09:48:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6f06346f27d17991ce973fdf66c194de3cd8bd2f', 'message': 'This adds a tempest tests for creating a node:\n\n* test_create_node_uuid\n* test_create_node_name\n* test_create_node_maintenance\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}, {'number': 2, 'created': '2016-12-15 12:06:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f237f3e54295a260cd4125499ce8b8179bf5c9cc', 'message': 'This adds a tempest tests for creating a node:\n\n* test_create_node_uuid\n* test_create_node_name\n* test_create_node_maintenance\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}, {'number': 3, 'created': '2016-12-16 15:27:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9cb5e04ea61e017698ea4be46249d8beef99d8eb', 'message': 'This adds a tempest tests for creating a node:\n\n* test_create_node_uuid\n* test_create_node_name\n* test_create_node_maintenance\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}, {'number': 4, 'created': '2017-01-24 11:19:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a20e892283d977092b37694c74a6f3095c66a066', 'message': 'This adds a tempest tests for creating a node:\n\n* test_create_node_uuid\n* test_create_node_name\n* test_create_node_maintenance\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}, {'number': 5, 'created': '2017-02-08 15:31:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3e9d3618bab439732763545e3cd6fb7b44d37c72', 'message': 'This adds a tempest tests for creating a node:\n\n* test_create_node_uuid\n* test_create_node_name\n* test_create_node_maintenance\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}, {'number': 6, 'created': '2017-02-09 12:19:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/01e1727a1cd282e4b01f359a9a1026f02019ba6b', 'message': 'This adds a tempest tests for creating a node:\n\n* test_create_node_uuid\n* test_create_node_name\n* test_create_node_maintenance\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}, {'number': 7, 'created': '2017-02-16 10:05:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0f57265c15d6e1e109e74b7fc94298eb28a381eb', 'message': 'Add tempest plugin API tests for node\n\nTest for the following actions were added:\n\n* creating node with specific uuid\n* creating node with specific name\n* creating node with specific maintenance\n* getting node with specific name\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}, {'number': 8, 'created': '2017-03-02 15:31:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e3c02c3c001a6a029c0870088ff10133de8802ea', 'message': 'Add tempest plugin API tests for node\n\nTest for the following actions were added:\n\n* creating node with specific uuid\n* creating node with specific name\n* creating node with specific maintenance\n* getting node with specific name\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}, {'number': 9, 'created': '2017-03-27 14:38:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/02e05da04a055929e8c9dbc7eac9542993656b13', 'message': 'Add tempest plugin API tests for node\n\nTest for the following actions were added:\n\n* creating node with specific uuid\n* creating node with specific name\n* creating node with specific maintenance\n* getting node with specific name\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}, {'number': 10, 'created': '2017-03-30 11:24:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/03ce006911dd447d960cf3c9b64b7b5546567129', 'message': 'Add tempest plugin API tests for node\n\nTest for the following actions were added:\n\n* creating node with specific uuid\n* creating node with specific name\n* creating node with specific maintenance\n* getting node with specific name\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}, {'number': 11, 'created': '2017-04-03 07:36:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bb975c2c2b6da9a995f654533257f9bc78c5c5c6', 'message': 'Add tempest plugin API tests for node\n\nTest for the following actions were added:\n\n* creating node with specific uuid\n* creating node with specific name\n* creating node with specific maintenance\n* getting node with specific name\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}, {'number': 12, 'created': '2017-04-03 08:09:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f0c17fe497d9fdf24d9a605ac270370730e7ae18', 'message': 'Add tempest plugin API tests for node\n\nTest for the following actions were added:\n\n* creating node with specific uuid\n* creating node with specific name\n* creating node with specific maintenance\n* getting node with specific name\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}, {'number': 13, 'created': '2017-05-05 13:47:53.000000000', 'files': ['ironic_tempest_plugin/tests/api/admin/test_nodes.py', 'ironic_tempest_plugin/services/baremetal/v1/json/baremetal_client.py', 'ironic_tempest_plugin/tests/api/admin/base.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/04d4d869af070fa1d5cf71d7ce59ec3c8eb111f5', 'message': 'Add tempest plugin API tests for node\n\nTest for the following actions were added:\n\n* creating node with specific uuid\n* creating node with specific name\n* creating node with specific maintenance\n* getting node with specific name\n\nChange-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9\n'}]",41,411197,04d4d869af070fa1d5cf71d7ce59ec3c8eb111f5,116,17,13,20675,,,0,"Add tempest plugin API tests for node

Test for the following actions were added:

* creating node with specific uuid
* creating node with specific name
* creating node with specific maintenance
* getting node with specific name

Change-Id: I8d96a89f92665ba4ca2c71623c0578c5442a5ac9
",git fetch https://review.opendev.org/openstack/ironic refs/changes/97/411197/4 && git format-patch -1 --stdout FETCH_HEAD,"['ironic_tempest_plugin/tests/api/admin/test_nodes.py', 'ironic_tempest_plugin/services/baremetal/v1/json/baremetal_client.py', 'ironic_tempest_plugin/tests/api/admin/base.py']",3,6f06346f27d17991ce973fdf66c194de3cd8bd2f,tests_ironic_api_node," memory_mb=4096, **kwargs): driver=cls.driver, **kwargs)", memory_mb=4096): driver=cls.driver),39,2
openstack%2Fironic~master~I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359,openstack/ironic,master,I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359,Add negative functional API tests for portgroups,NEW,2017-02-23 17:22:46.000000000,2017-12-18 01:20:30.000000000,,"[{'_account_id': 10118}, {'_account_id': 10206}, {'_account_id': 10239}, {'_account_id': 12356}, {'_account_id': 14525}, {'_account_id': 14614}, {'_account_id': 14629}, {'_account_id': 17270}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 22255}, {'_account_id': 22724}]","[{'number': 1, 'created': '2017-02-23 17:22:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/31b6319251c3a3a0fb631b318b515f85c2fd6648', 'message': 'Add negative functional API tests for portgroups\n\nChange-Id: I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359\nPartial-Bug: #1666858\n'}, {'number': 2, 'created': '2017-02-24 13:49:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8d56483aa9c7e5d2b1e88319f92749ada592722a', 'message': 'Add negative functional API tests for portgroups\n\nAdd tests for replugging resources in setups of\nnodes, ports, port groups.\n\nChange-Id: I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359\nPartial-Bug: #1666858\n'}, {'number': 3, 'created': '2017-03-02 15:05:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/aaf852e34a0229aec79ef042a3c2350cf1c708f6', 'message': 'Add negative functional API tests for portgroups\n\nAdd tests for replugging resources in setups of\nnodes, ports, port groups.\n\nChange-Id: I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359\nPartial-Bug: #1666858\n'}, {'number': 4, 'created': '2017-03-03 16:58:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e80fc8627436a8f8401591392e8161318cfb9b6f', 'message': 'Add negative functional API tests for portgroups\n\nAdd tests for replugging resources in setups of\nnodes, ports, port groups.\n\nChange-Id: I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359\nPartial-Bug: #1666858\n'}, {'number': 5, 'created': '2017-03-13 16:29:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9aab2d1d3620db118dd01b718f87abbef23ac3e6', 'message': 'Add negative functional API tests for portgroups\n\nAdd tests for replugging resources in setups of\nnodes, ports, port groups.\nAdd simple negative test cases also.\n\nChange-Id: I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359\nPartial-Bug: #1666858\n'}, {'number': 6, 'created': '2017-03-16 13:14:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/945cb3c9a70e69aa0db8f7812a1de780731811e9', 'message': 'Add negative functional API tests for portgroups\n\nAdd tests for replugging resources in setups of\nnodes, ports, port groups.\nAdd simple negative test cases also.\n\nChange-Id: I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359\nPartial-Bug: #1666858\n'}, {'number': 7, 'created': '2017-03-20 11:39:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/4be505a95a4674f2ba337a1cb614d288fb7369f4', 'message': 'Add negative functional API tests for portgroups\n\nAdd tests for replugging resources in setups of\nnodes, ports, port groups.\nAdd simple negative test cases also.\n\nChange-Id: I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359\nPartial-Bug: #1666858\n'}, {'number': 8, 'created': '2017-03-20 15:18:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fcdefed2ed9ca162a8f5f2e4b27dbdf37b19aef3', 'message': 'Add negative functional API tests for portgroups\n\nAdd tests for replugging resources in setups of\nnodes, ports, port groups.\nAdd simple negative test cases also.\n\nChange-Id: I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359\nPartial-Bug: #1666858\n'}, {'number': 9, 'created': '2017-03-23 12:37:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b50b5a695cdd0837369de79c3526426ffb4cd500', 'message': 'Add negative functional API tests for portgroups\n\nAdd tests for replugging resources in setups of\nnodes, ports, port groups.\nAdd simple negative test cases also.\n\nChange-Id: I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359\nPartial-Bug: #1666858\n'}, {'number': 10, 'created': '2017-03-24 12:18:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b5116353f7b7cdb5b341f47e104633819a47f243', 'message': 'Add negative functional API tests for portgroups\n\nAdd tests for replugging resources in setups of\nnodes, ports, port groups.\nAdd simple negative test cases also.\n\nChange-Id: I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359\nPartial-Bug: #1666858\n'}, {'number': 11, 'created': '2017-03-24 16:46:36.000000000', 'files': ['ironic_tempest_plugin/services/baremetal/v1/json/baremetal_client.py', 'ironic_tempest_plugin/tests/api/admin/test_portgroups_negative.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/d3ac4a70a4f59ec3e74f13b7aa83125abb7e65ef', 'message': 'Add negative functional API tests for portgroups\n\nAdd tests for replugging resources in setups of\nnodes, ports, port groups.\nAdd simple negative test cases also.\n\nChange-Id: I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359\nPartial-Bug: #1666858\n'}]",10,437535,d3ac4a70a4f59ec3e74f13b7aa83125abb7e65ef,83,13,11,14614,,,0,"Add negative functional API tests for portgroups

Add tests for replugging resources in setups of
nodes, ports, port groups.
Add simple negative test cases also.

Change-Id: I91cc667614cb0e95f92a6ebdc7d368e3cdc9f359
Partial-Bug: #1666858
",git fetch https://review.opendev.org/openstack/ironic refs/changes/35/437535/11 && git format-patch -1 --stdout FETCH_HEAD,['ironic_tempest_plugin/tests/api/admin/test_portgroups_negative.py'],1,31b6319251c3a3a0fb631b318b515f85c2fd6648,bug/1666858,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from tempest.lib.common.utils import data_utils from tempest.lib import decorators from tempest.lib import exceptions as lib_exc from ironic_tempest_plugin.tests.api.admin import api_microversion_fixture from ironic_tempest_plugin.tests.api.admin import base class TestPortGroupsNegative(base.BaseBaremetalTest): """"""Negative tests for port groups."""""" def setUp(self): super(TestPortGroupsNegative, self).setUp() self.useFixture( api_microversion_fixture.APIMicroversionFixture('1.25')) _, self.chassis = self.create_chassis() @decorators.idempotent_id('36b11297-ca12-409b-88ed-5ca628c601d7') def test_portgroup_plug_to_another_node(self): """"""Try to plug port group to another node."""""" _, node1 = self.create_node(self.chassis['uuid']) _, port1 = self.create_port(node1['uuid'], address=data_utils.rand_mac_address()) _, portgroup1 = self.create_portgroup( node_id=node1['uuid'], address=data_utils.rand_mac_address()) _, node2 = self.create_node(self.chassis['uuid']) _, port2 = self.create_port(node2['uuid'], address=data_utils.rand_mac_address()) patch = [{'path': '/portgroup_uuid', 'op': 'add', 'value': portgroup1['uuid']}] self.assertRaises(lib_exc.ServerFault, self.client.update_port, port2['uuid'], patch) ",,48,0
openstack%2Fironic~master~I29364dd63a6ef9d4ae40e368430394b90b59b0a5,openstack/ironic,master,I29364dd63a6ef9d4ae40e368430394b90b59b0a5,Tempest tests for portgroups,NEW,2016-10-05 14:50:32.000000000,2017-12-18 01:20:25.000000000,,"[{'_account_id': 7229}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10239}, {'_account_id': 10342}, {'_account_id': 10379}, {'_account_id': 11655}, {'_account_id': 11929}, {'_account_id': 13295}, {'_account_id': 14525}, {'_account_id': 14614}, {'_account_id': 14629}, {'_account_id': 14943}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 21511}, {'_account_id': 22724}, {'_account_id': 23883}]","[{'number': 1, 'created': '2016-10-05 14:50:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1fd76c075058feddc1ed8fc0f9fd72b255adce6f', 'message': 'WIP: tempest tests for bonding\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 2, 'created': '2016-10-06 16:08:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/34a1ef1387563297c31e1e770432ebae1c12751c', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 3, 'created': '2016-10-07 08:23:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/94df4631af63ce2a5f7c4a6a6771e1c5bcb20867', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 4, 'created': '2016-10-07 10:50:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a04b03e4e3f1c4aded73833d7885969d6159bf89', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 5, 'created': '2016-10-19 12:16:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3a202db7107939ff5d13c1283d1468f645f62a05', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 6, 'created': '2016-10-19 15:24:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bbd180eeb0ee4a1d71a0ff173a0cb77d27a55d40', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 7, 'created': '2016-10-19 17:42:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9895e95352d90e1c10d632da1c6d3a38eb5932ca', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 8, 'created': '2016-10-20 12:51:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0e3e99bdadaa3c0c4e022f27a3fdabf5aea65621', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 9, 'created': '2016-10-20 14:10:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/debd84e702508e0db08af0d35ef4c3e8125f4d89', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 10, 'created': '2016-10-20 19:42:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/886e456d32f4a2237a8ca216915f66f1b9527b63', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 11, 'created': '2016-10-31 11:43:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6c297c456d0b03c9eaf6feeb6eb4a3a0050c943a', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 12, 'created': '2016-11-03 11:37:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fa5d3b860e723d0c1f7d32deab2a7e69a72bc18c', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 13, 'created': '2016-11-03 14:10:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8a172769972a9fb5c6aa1334a3b61462245c4fbb', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 14, 'created': '2016-11-03 14:27:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/28eae56fac4375a22bf773159714742d194795fb', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 15, 'created': '2016-11-07 09:19:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/84f52d4fb1610e85ab32de242727ae3684cdc7b0', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 16, 'created': '2016-11-08 17:39:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/869404346aa669dd4b77fe6b32d1a1fa75562684', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 17, 'created': '2016-11-08 19:55:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e554ee621694f6d5febdc41a15028c5d40e0288b', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 18, 'created': '2016-11-09 11:38:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9dd6d05540dac8756602e61509c1acc60adc6fb6', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 19, 'created': '2016-11-10 16:08:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1bb0218fc82323871faba3955aa01340d7ed103b', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 20, 'created': '2016-11-11 10:58:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1ce70fdcbf5bbf55aa3c6d26b2222034ac57d829', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 21, 'created': '2016-11-16 12:36:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b1791b821dfe0ef4adeb6c1e8d7c900a89ffb7c0', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 22, 'created': '2016-12-20 12:41:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f4466b15ab59cc65bdf6b84ff8867ffbb379718a', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 23, 'created': '2016-12-20 16:53:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3488139671a9f1e41d95f9c5cda0ea8622dd1287', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 24, 'created': '2016-12-20 16:58:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b6f44aafd94d80e2d0bf5de6574104450914a4c9', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 25, 'created': '2016-12-21 10:16:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/29ea2938a28526d091adf0d56518ac0bccb2e73e', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 26, 'created': '2016-12-21 12:00:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2037018afea9a93080d62690a140b4208ba1d7b4', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 27, 'created': '2016-12-21 12:54:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/344be631ad929407a101d864350a4f871dd58fdc', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 28, 'created': '2017-01-16 09:36:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1fb1f9f74a8c177c7159a8ef9359745adc850890', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 29, 'created': '2017-01-16 12:50:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/536e63f70ea1fdf861c49166b31b2eca32a6326f', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 30, 'created': '2017-01-16 14:06:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a8befc5f46eb5d5a06f67de923927384c748e756', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 31, 'created': '2017-01-16 14:32:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8dd96899e4932c0c035c193c69541a4748aab758', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 32, 'created': '2017-01-16 16:37:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/043468d62b010670771bb6a9caac1f9bdf872b20', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}, {'number': 33, 'created': '2017-01-18 19:11:37.000000000', 'files': ['ironic_tempest_plugin/tests/scenario/test_baremetal_multitenancy.py', 'ironic_tempest_plugin/tests/scenario/baremetal_manager.py', 'ironic_tempest_plugin/config.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/0154efb6eb29c6f2f76909393b404a8d8827e2a9', 'message': 'Tempest tests for portgroups\n\nThis patch adds a tempest tests to baremetal_multitenancy test\nscenario that allows to test static portgroups.\n\nPartial-bug: #1618754\nChange-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5\n'}]",15,382476,0154efb6eb29c6f2f76909393b404a8d8827e2a9,176,19,33,14525,,,0,"Tempest tests for portgroups

This patch adds a tempest tests to baremetal_multitenancy test
scenario that allows to test static portgroups.

Partial-bug: #1618754
Change-Id: I29364dd63a6ef9d4ae40e368430394b90b59b0a5
",git fetch https://review.opendev.org/openstack/ironic refs/changes/76/382476/14 && git format-patch -1 --stdout FETCH_HEAD,"['ironic_tempest_plugin/tests/scenario/test_baremetal_multitenancy.py', 'devstack/lib/ironic', 'ironic_tempest_plugin/config.py']",3,1fd76c075058feddc1ed8fc0f9fd72b255adce6f,bug/1618754," help=""Whether the Ironic/Neutron tenant isolation is enabled""), cfg.BoolOpt('use_portgroups', help=""Whether the Ironic portgroups are enabled.""),"," help=""Whether the Ironic/Neutron tenant isolation is enabled"")",32,1
openstack%2Fironic~master~I1ff80c7b0083568d443fd53feb45b86bcdd2f6f6,openstack/ironic,master,I1ff80c7b0083568d443fd53feb45b86bcdd2f6f6,Add headers and extra_headers to arguments in tempest plugin,NEW,2016-09-23 15:22:24.000000000,2017-12-18 01:20:22.000000000,,"[{'_account_id': 10118}, {'_account_id': 10239}, {'_account_id': 12356}, {'_account_id': 13636}, {'_account_id': 14525}, {'_account_id': 14614}, {'_account_id': 14629}, {'_account_id': 14943}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 20311}, {'_account_id': 22724}]","[{'number': 1, 'created': '2016-09-23 15:22:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/25409abb3f4c16721512fb9d9bfac00038686fec', 'message': 'Fix tempest plugin to update port connectivity data\n\nAdd headers and extra_headers arguments to ironic tempest plugin\nto make possible updates of port connectivity data in tests.\n\nChange-Id: I1ff80c7b0083568d443fd53feb45b86bcdd2f6f6\nCloses-Bug: #1626969\n'}, {'number': 2, 'created': '2016-09-28 14:08:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5e1793ae23dc4342c8781d830002a190cd33a880', 'message': 'Add headers and extra_headers to arguments in tempest plugin\n\nFix to enable updates of port connectivity data in tests.\nFix to enable updates for nodes and chassis via _patch_request.\n\nChange-Id: I1ff80c7b0083568d443fd53feb45b86bcdd2f6f6\nCloses-Bug: #1626969\n'}, {'number': 3, 'created': '2016-10-10 11:28:56.000000000', 'files': ['ironic_tempest_plugin/services/baremetal/v1/json/baremetal_client.py', 'ironic_tempest_plugin/services/baremetal/base.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/a4dd3d23c1db46df349f7e4d330017125675e615', 'message': 'Add headers and extra_headers to arguments in tempest plugin\n\nFix to enable updates of port connectivity data in tests.\nFix to enable updates for nodes and chassis via _patch_request.\n\nChange-Id: I1ff80c7b0083568d443fd53feb45b86bcdd2f6f6\nCloses-Bug: #1626969\n'}]",0,375570,a4dd3d23c1db46df349f7e4d330017125675e615,28,12,3,14614,,,0,"Add headers and extra_headers to arguments in tempest plugin

Fix to enable updates of port connectivity data in tests.
Fix to enable updates for nodes and chassis via _patch_request.

Change-Id: I1ff80c7b0083568d443fd53feb45b86bcdd2f6f6
Closes-Bug: #1626969
",git fetch https://review.opendev.org/openstack/ironic refs/changes/70/375570/2 && git format-patch -1 --stdout FETCH_HEAD,"['ironic_tempest_plugin/services/baremetal/v1/json/baremetal_client.py', 'ironic_tempest_plugin/services/baremetal/base.py']",2,25409abb3f4c16721512fb9d9bfac00038686fec,bug/1626969," def _patch_request(self, resource, uuid, patch_object, extra_headers=False, headers=None): :param dict headers: The headers to use for the request :param bool extra_headers: indicates additional headers in the request to pass them as a dict. resp, body = self.patch(uri, body=patch_body, headers=headers, extra_headers=extra_headers)"," def _patch_request(self, resource, uuid, patch_object): resp, body = self.patch(uri, body=patch_body)",14,4
openstack%2Fironic~master~I46ce83f8c74bad76cce05e1904071dfe8eb69edb,openstack/ironic,master,I46ce83f8c74bad76cce05e1904071dfe8eb69edb,Added test for root_device hints functionality.,NEW,2016-11-29 11:57:38.000000000,2017-12-18 01:20:17.000000000,,"[{'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10239}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 23891}]","[{'number': 1, 'created': '2016-11-29 11:57:38.000000000', 'files': ['ironic_tempest_plugin/tests/scenario/test_root_device_hints.py', 'ironic_tempest_plugin/config.py', 'ironic_tempest_plugin/services/baremetal/v1/json/baremetal_client.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/2b309fd071d46dd4f125109cb1bc1f0c927f7686', 'message': 'Added test for root_device hints functionality.\n\nChange-Id: I46ce83f8c74bad76cce05e1904071dfe8eb69edb\n'}]",0,404174,2b309fd071d46dd4f125109cb1bc1f0c927f7686,10,7,1,23891,,,0,"Added test for root_device hints functionality.

Change-Id: I46ce83f8c74bad76cce05e1904071dfe8eb69edb
",git fetch https://review.opendev.org/openstack/ironic refs/changes/74/404174/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic_tempest_plugin/tests/scenario/test_root_device_hints.py', 'ironic_tempest_plugin/config.py', 'ironic_tempest_plugin/services/baremetal/v1/json/baremetal_client.py']",3,2b309fd071d46dd4f125109cb1bc1f0c927f7686,root_device_hints_test," 'properties/root_device',",,98,1
openstack%2Fironic-python-agent~master~I852cc6745daf545fdd68c157a366fb016c313a7d,openstack/ironic-python-agent,master,I852cc6745daf545fdd68c157a366fb016c313a7d,Support for downloading by torrent in standby extension,NEW,2015-03-29 15:46:41.000000000,2017-12-18 01:19:47.000000000,,"[{'_account_id': 9315}, {'_account_id': 10239}, {'_account_id': 10380}]","[{'number': 1, 'created': '2015-03-29 15:46:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/7aa80db3937aa92287d563c6cb826971014b98b3', 'message': ""Support for downloading by torrent in standby extension\n\nThis commit adds ability to download the image\nby a torrent. The base64 encoded torrent data may\nby passed along with image_info instead of 'urls'.\nIt uses transmission daemon to download the image.\n\nChange-Id: I852cc6745daf545fdd68c157a366fb016c313a7d\n""}, {'number': 2, 'created': '2015-03-29 15:48:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/b40ba9cfb388f848cf87a032e91b0cb5b904bfa4', 'message': ""Support for downloading by torrent in standby extension\n\nThis commit adds ability to download the image\nby a torrent. The base64 encoded torrent data may\nby passed along with image_info instead of 'urls'.\nIt uses transmission daemon to download the image.\n\nChange-Id: I852cc6745daf545fdd68c157a366fb016c313a7d\n""}, {'number': 3, 'created': '2015-04-18 09:59:14.000000000', 'files': ['Dockerfile', 'requirements.txt', 'ironic_python_agent/tests/extensions/standby.py', 'ironic_python_agent/image_service.py', 'ironic_python_agent/tests/image_service.py', 'ironic_python_agent/extensions/standby.py'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/dbb68622950ec828559c2575121d4e988ad6610e', 'message': ""Support for downloading by torrent in standby extension\n\nThis commit adds ability to download the image\nby a torrent. The base64 encoded torrent data may\nby passed along with image_info instead of 'urls'.\nIt uses transmission daemon to download the image.\n\nChange-Id: I852cc6745daf545fdd68c157a366fb016c313a7d\n""}]",21,168718,dbb68622950ec828559c2575121d4e988ad6610e,11,3,3,9315,,,0,"Support for downloading by torrent in standby extension

This commit adds ability to download the image
by a torrent. The base64 encoded torrent data may
by passed along with image_info instead of 'urls'.
It uses transmission daemon to download the image.

Change-Id: I852cc6745daf545fdd68c157a366fb016c313a7d
",git fetch https://review.opendev.org/openstack/ironic-python-agent refs/changes/18/168718/2 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'ironic_python_agent/extensions/standby.py']",2,7aa80db3937aa92287d563c6cb826971014b98b3,torrent,"import globimport tempfileimport transmissionrpcTORRENT_START_TIMEOUT_SECONDS = 120 class ImageService(object): def validate(image_info): pass def download(image_info): pass def is_image_service(image_info): pass class TorrentImageService(ImageService): def is_image_service(self, image_info): if 'torrent' in image_info: return True return False def validate(self, image_info): # TODO(rameshg87): Validate if it's a valid torrent data. pass def download(self, image_info): torrent_data = base64.b64decode(image_info['torrent']) tc = transmissionrpc.Client('localhost', port=9091) download_started_states = ['downloading', 'seed pending', 'seeding'] completed_states = ['seed pending', 'seeding'] image_location = _image_location(image_info) download_dir = tempfile.mkdtemp() with tempfile.NamedTemporaryFile() as tmpfile: tmpfile.write(torrent_data) tmpfile.flush() retval = tc.add_torrent(tmpfile, download_dir=download_dir) torrent_id = retval.keys()[0] starttime = time.time() currenttime = starttime while (starttime + TORRENT_START_TIMEOUT_SECONDS) > currenttime: torrent = tc.get_torrent(torrent_id)[torrent_id] if torrent.status in download_started_states: break else: msg = (""Torrent didn't start downloading in %s seconds"" % TORRENT_START_TIMEOUT_SECONDS) raise errors.ImageDownloadError(image_info['id'], msg) # TODO(rameshg87): Put a timeout for download because it will keep # on waiting for seeders. while(True): torrent = tc.get_torrent(torrent_id)[torrent_id] current_status = torrent.status if current_status in completed_states: break if current_status != 'downloading': msg = (""Downloading torrent failed. "" ""Transmission reported '%s' status"" % current_status) raise errors.ImageDownloadError(image_info['id'], msg) files_downloaded = glob.glob(os.path.join(download_dir, ""*"")) if len(files_downloaded) > 1: msg = ""Invalid image torrent. Got more than one file"" raise errors.ImageDownloadError(image_info['id'], msg) os.link(files_downloaded[0], image_location) class HttpImageService(ImageService): def is_image_service(self, image_info): if 'urls' in image_info: return True return False def validate(self, image_info): if type(image_info['urls']) != list or not image_info['urls']: raise errors.InvalidCommandParamsError( 'Image \'urls\' must be a list with at least one element.') def download(self, image_info): starttime = time.time() resp = None for url in image_info['urls']: try: LOG.info(""Attempting to download image from {0}"".format(url)) resp = _request_url(image_info, url) except errors.ImageDownloadError as e: failtime = time.time() - starttime log_msg = ('Image download failed. URL: {0}; ' 'time: {1} seconds. Error: {2}') LOG.warning(log_msg.format(url, failtime, e.details)) continue else: break if resp is None: msg = 'Image download failed for all URLs.' raise errors.ImageDownloadError(image_info['id'], msg) image_location = _image_location(image_info) with open(image_location, 'wb') as f: try: for chunk in resp.iter_content(IMAGE_CHUNK_SIZE): f.write(chunk) except Exception as e: msg = 'Unable to write image to {0}. Error: {1}'.format( image_location, str(e)) raise errors.ImageDownloadError(image_info['id'], msg) totaltime = time.time() - starttime LOG.info(""Image downloaded from {0} in {1} seconds"".format( image_location, totaltime)) if not _verify_image(image_info, image_location): raise errors.ImageChecksumError(image_info['id']) KNOWN_IMAGE_SERVICES = [HttpImageService(), TorrentImageService()] def get_image_service(image_info): for image_service in KNOWN_IMAGE_SERVICES: if image_service.is_image_service(image_info): return image_service image_service = get_image_service(image_info) image_service.download(image_info) for field in ['id', 'checksum']: image_service = get_image_service(image_info) if not image_service: msg = 'No valid image service found' raise errors.InvalidCommandParamsError(msg) image_service.validate(image_info)"," starttime = time.time() resp = None for url in image_info['urls']: try: LOG.info(""Attempting to download image from {0}"".format(url)) resp = _request_url(image_info, url) except errors.ImageDownloadError as e: failtime = time.time() - starttime log_msg = ('Image download failed. URL: {0}; time: {1} seconds. ' 'Error: {2}') LOG.warning(log_msg.format(url, failtime, e.details)) continue else: break if resp is None: msg = 'Image download failed for all URLs.' raise errors.ImageDownloadError(image_info['id'], msg) image_location = _image_location(image_info) with open(image_location, 'wb') as f: try: for chunk in resp.iter_content(IMAGE_CHUNK_SIZE): f.write(chunk) except Exception as e: msg = 'Unable to write image to {0}. Error: {1}'.format( image_location, str(e)) raise errors.ImageDownloadError(image_info['id'], msg) totaltime = time.time() - starttime LOG.info(""Image downloaded from {0} in {1} seconds"".format(image_location, totaltime)) if not _verify_image(image_info, image_location): raise errors.ImageChecksumError(image_info['id']) for field in ['id', 'urls', 'checksum']: if type(image_info['urls']) != list or not image_info['urls']: raise errors.InvalidCommandParamsError( 'Image \'urls\' must be a list with at least one element.')",142,38
openstack%2Fironic~master~Id876117c73582e974e4ab60978516aabedf743f9,openstack/ironic,master,Id876117c73582e974e4ab60978516aabedf743f9,Changed error code for port list filtering,NEW,2017-02-21 23:41:17.000000000,2017-12-18 01:19:32.000000000,,"[{'_account_id': 7711}, {'_account_id': 10118}, {'_account_id': 10206}, {'_account_id': 11878}, {'_account_id': 12356}, {'_account_id': 14525}, {'_account_id': 14629}, {'_account_id': 17998}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 19593}, {'_account_id': 19686}, {'_account_id': 23883}]","[{'number': 1, 'created': '2017-02-21 23:41:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6a9ea106ff08882f02db73a2de5dd332643bce86', 'message': 'Changed error code for port list filtering\n\nAPI returned 403 Forbidden when user tried to filter port list\nby both node and portgroup. This commit changed the response to\n400 Bad Request with corresponding, meaningful message.\n\nChange-Id: Id876117c73582e974e4ab60978516aabedf743f9\n'}, {'number': 2, 'created': '2017-02-21 23:48:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/776ac38bec27142affcfea380c9d2f29efeeadf7', 'message': 'Changed error code for port list filtering\n\nAPI returned 403 Forbidden when user tried to filter port list\nby both node and portgroup. This commit changed the response to\n400 Bad Request with corresponding, meaningful message.\n\nChange-Id: Id876117c73582e974e4ab60978516aabedf743f9\n'}, {'number': 3, 'created': '2017-02-22 22:30:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7bbbfd2feb8adfdae48fbc3915cb1bd583cf7d7f', 'message': 'Changed error code for port list filtering\n\nAPI returned 403 Forbidden when user tried to filter port list\nby both node and portgroup using query parameters.\nThis commit changed the response to 400 Bad Request with corresponding,\nmeaningful message.\n\nChange-Id: Id876117c73582e974e4ab60978516aabedf743f9\n'}, {'number': 4, 'created': '2017-02-23 18:57:45.000000000', 'files': ['ironic/api/controllers/v1/port.py', 'doc/source/dev/webapi-version-history.rst', 'ironic/tests/unit/api/v1/test_ports.py', 'ironic/api/controllers/v1/versions.py', 'releasenotes/notes/port-filtering-response-fix-000b713e10496436.yaml', 'ironic/api/controllers/v1/utils.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/1c6aeb24ac99ab5fe89c1186f8f734aebd7bd1cc', 'message': 'Changed error code for port list filtering\n\nAPI returned 403 Forbidden when user tried to filter port list\nby both node and portgroup using query parameters.\nThis commit changed the response to 400 Bad Request with corresponding,\nmeaningful message.\n\nChange-Id: Id876117c73582e974e4ab60978516aabedf743f9\n'}]",23,436713,1c6aeb24ac99ab5fe89c1186f8f734aebd7bd1cc,46,13,4,23883,,,0,"Changed error code for port list filtering

API returned 403 Forbidden when user tried to filter port list
by both node and portgroup using query parameters.
This commit changed the response to 400 Bad Request with corresponding,
meaningful message.

Change-Id: Id876117c73582e974e4ab60978516aabedf743f9
",git fetch https://review.opendev.org/openstack/ironic refs/changes/13/436713/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/api/controllers/v1/port.py', 'ironic/tests/unit/api/v1/test_nodes.py', 'ironic/tests/unit/api/v1/test_ports.py']",3,6a9ea106ff08882f02db73a2de5dd332643bce86,port-filtering-response-fix," self.assertEqual(http_client.BAD_REQUEST, response.status_int)"," self.assertEqual(http_client.FORBIDDEN, response.status_int)",6,5
openstack%2Fironic~master~I5e35daed781eaab838f8090fefe810e252da610f,openstack/ironic,master,I5e35daed781eaab838f8090fefe810e252da610f,Allow ~ in the PATCH path attribute.,NEW,2016-07-18 20:41:42.000000000,2017-12-18 01:19:00.000000000,,"[{'_account_id': 10118}, {'_account_id': 10358}, {'_account_id': 13295}, {'_account_id': 17998}]","[{'number': 1, 'created': '2016-07-18 20:41:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6c2b95e378ed2c50b89b7f9a8524282850179305', 'message': ""Allow ~ in the PATCH path attribue.\n\nThis allows escaping of '/' and '~' literals, so objects that\nhave keys that use them can be operated on via the API.\n\nChange-Id: I5e35daed781eaab838f8090fefe810e252da610f\nCloses-Bug: 1604148\n""}, {'number': 2, 'created': '2016-07-18 22:09:16.000000000', 'files': ['ironic/api/controllers/v1/types.py', 'ironic/tests/unit/api/v1/test_types.py', 'ironic/tests/unit/api/v1/test_utils.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/13b011a1c5c690468a0baa0501967627812aa7f0', 'message': ""Allow ~ in the PATCH path attribute.\n\nThis allows escaping of '/' and '~' literals, so objects that\nhave keys that use them can be operated on via the API.\n\nChange-Id: I5e35daed781eaab838f8090fefe810e252da610f\nCloses-Bug: 1604148\n""}]",2,343911,13b011a1c5c690468a0baa0501967627812aa7f0,11,4,2,10358,,,0,"Allow ~ in the PATCH path attribute.

This allows escaping of '/' and '~' literals, so objects that
have keys that use them can be operated on via the API.

Change-Id: I5e35daed781eaab838f8090fefe810e252da610f
Closes-Bug: 1604148
",git fetch https://review.opendev.org/openstack/ironic refs/changes/11/343911/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/api/controllers/v1/types.py', 'ironic/tests/unit/api/v1/test_types.py', 'ironic/tests/unit/api/v1/test_utils.py']",3,6c2b95e378ed2c50b89b7f9a8524282850179305,bug/1604148,"import copy def test_apply_jsonpatch(self): doc = { 'foo': 'bar', 'test': {'key1': True, 'key2': 0} } expected = copy.deepcopy(doc) expected['foo'] = 'baz' expected['test']['key1'] = False del expected['test']['key2'] expected['test']['key3'] = 'value3' patch = [{'path': '/test/key1', 'op': 'replace', 'value': False}, {'path': '/test/key2', 'op': 'remove'}, {'path': '/test/key3', 'op': 'add', 'value': 'value3'}, {'path': '/foo', 'op': 'replace', 'value': 'baz'}] result = utils.apply_jsonpatch(doc, patch) self.assertEqual(result, expected) def test_apply_jsonpatch_rfc6901(self): doc = { 'test': { '/': True, '~': True, '~1': True, '~0': True, 'foo/bar': True, 'foo/bar/baz': True, 'foo~bar': True, 'foo~bar~baz': True, 'foo/bar~baz': True, } } expected = copy.deepcopy(doc) for k, v in expected['test'].items(): expected['test'][k] = False patches = [] for k, v in doc['test'].items(): k = k.replace('~', '~0') k = k.replace('/', '~1') p = {'path': '/test/%s' % (k), 'op': 'replace', 'value': False} patches.append(p) result = utils.apply_jsonpatch(doc, patches) self.assertEqual(result, expected) def test_apply_jsonpatch_add_root_attr_fails(self): doc = {} patch = [{'path': '/test', 'op': 'add', 'value': True}] self.assertRaises(wsme.exc.ClientSideError, utils.apply_jsonpatch, doc, patch) ",,61,2
openstack%2Fswift~master~I149ac2da7150e50aaf6b72e9d7097c52005613d4,openstack/swift,master,I149ac2da7150e50aaf6b72e9d7097c52005613d4,Enable updateable object sysmeta,NEW,2014-07-25 14:35:53.000000000,2017-12-18 01:12:53.000000000,,[{'_account_id': 7847}],"[{'number': 1, 'created': '2014-07-25 14:35:53.000000000', 'files': ['swift/obj/server.py', 'swift/common/utils.py', 'test/unit/obj/test_server.py', 'test/probe/test_object_metadata_replication.py', 'test/unit/proxy/test_sysmeta.py', 'test/unit/obj/test_diskfile.py', 'swift/obj/diskfile.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/c68080077ca42decde403d8a1e1e0517bab3914a', 'message': ""Enable updateable object sysmeta\n\n** Work In Progress **\n\nMake it so that individual items of object sysmeta\ncan be updated with a fast-POST request (unlike user\nmetadata which is replaced as a whole).\n\nThere is a probe test that simulates sysmeta merge during\nPOST and rsync replication after object server 'failure':\n\nnosetests test/probe/test_object_metadata_replication.py:Test.test_updatable_sysmeta\n\nChanges:\n\n- With fast POST, read existing sysmeta, update individual items\nand write merged results to new meta file\n\n- To cope with concurrent POSTs and replicated meta files, only\ndelete meta files when content has been read/merged/written\nto new meta file\n\n- Read all meta files when opening diskfile, so after replication\nsysmeta is consistent across cluster\n\n- This can result in multiple meta files after\nfailure conditions/replication with rsync, so merge during\nreplication too.\n\n- Meta files with same timestamp could have different content\non different servers due to failures, so append md5 hash to\nmeta file name (yuk)\n\nWork still left to do includes:\n\n- prevent user meta being wiped when sysmeta is POSTed (introduce\nX-Preserve-User-Meta header?)\n\n- find a better (more concise) meta file naming scheme\n\n- ssync\n\n- more tests needed\n\nSpec: https://review.openstack.org/#/c/109314/\nChange-Id: I149ac2da7150e50aaf6b72e9d7097c52005613d4\n""}]",0,109609,c68080077ca42decde403d8a1e1e0517bab3914a,6,1,1,7847,,,0,"Enable updateable object sysmeta

** Work In Progress **

Make it so that individual items of object sysmeta
can be updated with a fast-POST request (unlike user
metadata which is replaced as a whole).

There is a probe test that simulates sysmeta merge during
POST and rsync replication after object server 'failure':

nosetests test/probe/test_object_metadata_replication.py:Test.test_updatable_sysmeta

Changes:

- With fast POST, read existing sysmeta, update individual items
and write merged results to new meta file

- To cope with concurrent POSTs and replicated meta files, only
delete meta files when content has been read/merged/written
to new meta file

- Read all meta files when opening diskfile, so after replication
sysmeta is consistent across cluster

- This can result in multiple meta files after
failure conditions/replication with rsync, so merge during
replication too.

- Meta files with same timestamp could have different content
on different servers due to failures, so append md5 hash to
meta file name (yuk)

Work still left to do includes:

- prevent user meta being wiped when sysmeta is POSTed (introduce
X-Preserve-User-Meta header?)

- find a better (more concise) meta file naming scheme

- ssync

- more tests needed

Spec: https://review.openstack.org/#/c/109314/
Change-Id: I149ac2da7150e50aaf6b72e9d7097c52005613d4
",git fetch https://review.opendev.org/openstack/swift refs/changes/09/109609/1 && git format-patch -1 --stdout FETCH_HEAD,"['swift/obj/server.py', 'swift/common/utils.py', 'test/unit/obj/test_server.py', 'test/probe/test_object_metadata_replication.py', 'test/unit/proxy/test_sysmeta.py', 'test/unit/obj/test_diskfile.py', 'swift/obj/diskfile.py']",7,c68080077ca42decde403d8a1e1e0517bab3914a,p-updatable-sysmeta,"from os.path import basename, dirname, exists, getmtime, join, splitfrom functools import partial config_true_value, listdir, split_path, ismount, remove_file, \ make_on_disk_filename * ts_file is not None, data_file is None, meta_files is empty data_file = ts_file = None meta_files = [] meta_files = [] if afile.endswith('.meta'): meta_files.append(join(datadir, afile)) assert ((data_file is None and not meta_files and ts_file is None) and not meta_files) "" %s, meta_files: %s, ts_file: %s"" % (data_file, meta_files, ts_file) return data_file, meta_files, ts_file # TODO this function is begging to be refactored into hash_cleanup_listdir # BUT I ran into all kinds of thread switching errors, hence use of the # special case DiskFileWriter subclass in this function. def merge_meta_files(hsh_path, mgr): """""" Merges any meta files found in hash directory and write a single meta file if required. """""" files = listdir(hsh_path) if len(files) > 1: files.sort(reverse=True) data_file, meta_files, ts_file = get_ondisk_files(files, '') newest_file = data_file or ts_file for filename in list(meta_files): if filename < newest_file: meta_files.remove(filename) if len(meta_files) > 1: # merge meta by doing a 'mini POST' path, hash = split(hsh_path) path, _junk = split(path) path, part = split(path) path, _junk = split(path) path, dev = split(path) #TODO retrieve correct policy index from path metadata = read_metadata(os.path.join(hsh_path, meta_files[0])) account, container, obj = split_path( metadata.get('name', ''), 3, 3, True) df = mgr.get_diskfile(dev, part, account, container, obj, policy_idx=0, writer=DiskFileWriter2) metadata = df.read_metadata() df.write_metadata(metadata) def hash_cleanup_listdir(hsh_path, reclaim_age=ONE_WEEK, stale_meta=None): :param stale_meta: meta files that are no longer required stale_meta_files = stale_meta if stale_meta else [] data_file, meta_files, ts_file = get_ondisk_files(files, '') if filename < newest_file: remove_file(join(hsh_path, filename)) files.remove(filename) elif filename in stale_meta_files:def hash_suffix(path, reclaim_age, mgr=None): # TODO attempt to cleanup multiple meta files if mgr: merge_meta_files(hsh_path, mgr) reclaim_age=ONE_WEEK, mgr=None): hashes[suffix] = hash_suffix(suffix_dir, reclaim_age, mgr) get_hashes, partition_path, recalculate=suffixes, mgr=self) def __init__(self, name, datadir, fd, tmppath, bytes_per_sync, threadpool, diskfile=None): self._diskfile = diskfile self._failsafe_hash_cleanup_listdir(self._diskfile.src_meta_files) def _failsafe_hash_cleanup_listdir(self, stale_meta): try: hash_cleanup_listdir(self._datadir, stale_meta=stale_meta) def _run(self, func, *args): self._threadpool.force_run_in_thread(func, *args) target_file = make_on_disk_filename(timestamp, metadata, self._extension) target_path = join(self._datadir, target_file) src_meta_files = list(self._diskfile.src_meta_files) if target_file in src_meta_files: # no need to write same file, just attempt clean up src_meta_files.remove(target_file) self._run(self._failsafe_hash_cleanup_listdir, src_meta_files) else: self._run(self._finalize_put, metadata, target_path) class DiskFileWriter2(DiskFileWriter): def _run(self, func, *args): func(*args) policy_idx=0, writer=DiskFileWriter): self._writer_class = writer self.src_meta_files = [] fileset = (None, [], None) def _construct_from_data_file(self, data_file, meta_files): from the fast-POST `.meta` files as well if any exist, merging them :param meta_files: on-disk fast-POST `.meta` file being considered if meta_files: m = meta_files[0] # will be newest timestamp self._metadata = self._failsafe_read_metadata(m, m) self.src_meta_files.append(split(m)[1]) for m in meta_files[1:]: src = self._failsafe_read_metadata(m, m) for k, v in src.iteritems(): if is_sys_meta('object', k): v_exist, t_exist = self._metadata.get(k, (0, 0)) if Timestamp(t_exist) < Timestamp(v[1]): self._metadata[k] = v self.src_meta_files.append(split(m)[1]) src = datafile_metadata t_datafile = Timestamp(src['X-Timestamp']) for k, v in src.iteritems(): if is_sys_meta('object', k): v_exist, t_exist = self._metadata.get(k, (0, 0)) if Timestamp(t_exist) < t_datafile: self._metadata[k] = (v, t_datafile.internal) if key.lower() in DATAFILE_SYSTEM_META]) src = datafile_metadata t_datafile = Timestamp(src['X-Timestamp']) self._metadata = {} for k, v in src.iteritems(): if is_sys_meta('object', k): self._metadata[k] = (v, t_datafile.internal) else: self._metadata[k] = v yield self._writer_class(self._name, self._datadir, fd, tmppath, self._bytes_per_sync, self._threadpool, diskfile=self)","from os.path import basename, dirname, exists, getmtime, join config_true_value, listdir, split_path, ismount, remove_filefrom functools import partial * ts_file is not None, data_file is None, meta_file is None data_file = meta_file = ts_file = None meta_file = None if afile.endswith('.meta') and not meta_file: meta_file = join(datadir, afile) assert ((data_file is None and meta_file is None and ts_file is None) and meta_file is None) "" %s, meta_file: %s, ts_file: %s"" % (data_file, meta_file, ts_file) return data_file, meta_file, ts_file def hash_cleanup_listdir(hsh_path, reclaim_age=ONE_WEEK): data_file, meta_file, ts_file = get_ondisk_files(files, '') if ((filename < newest_file) or (meta_file and filename.endswith('.meta') and filename < meta_file)):def hash_suffix(path, reclaim_age): reclaim_age=ONE_WEEK): hashes[suffix] = hash_suffix(suffix_dir, reclaim_age) get_hashes, partition_path, recalculate=suffixes) def __init__(self, name, datadir, fd, tmppath, bytes_per_sync, threadpool): try: hash_cleanup_listdir(self._datadir) target_path = join(self._datadir, timestamp + self._extension) self._threadpool.force_run_in_thread( self._finalize_put, metadata, target_path) policy_idx=0): fileset = (None, None, None) def _construct_from_data_file(self, data_file, meta_file): from the fast-POST `.meta` file as well if it exists, merging them :param meta_file: on-disk fast-POST `.meta` file being considered if meta_file: self._metadata = self._failsafe_read_metadata(meta_file, meta_file) if key.lower() in DATAFILE_SYSTEM_META or is_sys_meta('object', key)]) self._metadata = datafile_metadata yield DiskFileWriter(self._name, self._datadir, fd, tmppath, self._bytes_per_sync, self._threadpool)",451,70
openstack%2Fpython-zaqarclient~master~Id76df81bf35c2fd6b0de492fb185a0982c5dd0cf,openstack/python-zaqarclient,master,Id76df81bf35c2fd6b0de492fb185a0982c5dd0cf,Add Keystone v3 auth support to zaqar/marconi python client,NEW,2014-08-08 21:54:01.000000000,2017-12-18 01:12:34.000000000,,"[{'_account_id': 10068}, {'_account_id': 11717}]","[{'number': 1, 'created': '2014-08-08 21:54:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-zaqarclient/commit/cb24f66a1bcf227e344c94fe44f6c8e9a7709f90', 'message': 'Add Keystone v3 auth support to zaqar/marconi python clietn\n\nChange-Id: Id76df81bf35c2fd6b0de492fb185a0982c5dd0cf\n'}, {'number': 2, 'created': '2014-08-08 22:20:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-zaqarclient/commit/2099ebb8617032efa9c01797c16a073a0a4c5096', 'message': 'Add Keystone v3 auth support to zaqar/marconi python clietn\n\nChange-Id: Id76df81bf35c2fd6b0de492fb185a0982c5dd0cf\n'}, {'number': 3, 'created': '2014-08-08 22:39:16.000000000', 'files': ['test-requirements.txt', 'zaqarclient/openstack/common/jsonutils.py', 'zaqarclient/auth/keystone.py', 'tests/unit/auth/test_keystone.py', 'zaqarclient/tests/keystone_client_fixtures.py'], 'web_link': 'https://opendev.org/openstack/python-zaqarclient/commit/b33ffaf59053917a483a38fc82ee9d41e90e034c', 'message': 'Add Keystone v3 auth support to zaqar/marconi python client\n\nChange-Id: Id76df81bf35c2fd6b0de492fb185a0982c5dd0cf\n'}]",0,113033,b33ffaf59053917a483a38fc82ee9d41e90e034c,12,2,3,11717,,,0,"Add Keystone v3 auth support to zaqar/marconi python client

Change-Id: Id76df81bf35c2fd6b0de492fb185a0982c5dd0cf
",git fetch https://review.opendev.org/openstack/python-zaqarclient refs/changes/33/113033/1 && git format-patch -1 --stdout FETCH_HEAD,"['test-requirements.txt', 'zaqarclient/openstack/common/jsonutils.py', 'tests/unit/auth/test_keystone.py', 'zaqarclient/auth/keystone.py', 'zaqarclient/tests/keystone_client_fixtures.py']",5,cb24f66a1bcf227e344c94fe44f6c8e9a7709f90,keystone_v3_support,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from keystoneclient.fixture import v2 as ks_v2_fixture from keystoneclient.fixture import v3 as ks_v3_fixture from zaqarclient.openstack.common import jsonutils import uuid # from heatclient.openstack.common import jsonutils # these are copied from python-keystoneclient tests BASE_HOST = 'http://keystone.example.com' BASE_URL = ""%s:5000/"" % BASE_HOST UPDATED = '2013-03-06T00:00:00Z' V2_URL = ""%sv2.0"" % BASE_URL V2_DESCRIBED_BY_HTML = {'href': 'http://docs.openstack.org/api/' 'openstack-identity-service/2.0/content/', 'rel': 'describedby', 'type': 'text/html'} V2_DESCRIBED_BY_PDF = {'href': 'http://docs.openstack.org/api/openstack-ident' 'ity-service/2.0/identity-dev-guide-2.0.pdf', 'rel': 'describedby', 'type': 'application/pdf'} V2_VERSION = {'id': 'v2.0', 'links': [{'href': V2_URL, 'rel': 'self'}, V2_DESCRIBED_BY_HTML, V2_DESCRIBED_BY_PDF], 'status': 'stable', 'updated': UPDATED} V3_URL = ""%sv3"" % BASE_URL V3_MEDIA_TYPES = [{'base': 'application/json', 'type': 'application/vnd.openstack.identity-v3+json'}, {'base': 'application/xml', 'type': 'application/vnd.openstack.identity-v3+xml'}] V3_VERSION = {'id': 'v3.0', 'links': [{'href': V3_URL, 'rel': 'self'}], 'media-types': V3_MEDIA_TYPES, 'status': 'stable', 'updated': UPDATED} TOKENID = uuid.uuid4().hex def _create_version_list(versions): return jsonutils.dumps({'versions': {'values': versions}}) def _create_single_version(version): return jsonutils.dumps({'version': version}) V3_VERSION_LIST = _create_version_list([V3_VERSION, V2_VERSION]) V2_VERSION_LIST = _create_version_list([V2_VERSION]) V3_VERSION_ENTRY = _create_single_version(V3_VERSION) V2_VERSION_ENTRY = _create_single_version(V2_VERSION) ZAQAR_ENDPOINT = 'http://www.zaqar.com/v1' def keystone_request_callback(request, uri, headers): response_headers = {""content-type"": ""application/json""} token_id = TOKENID if uri == BASE_URL: return (200, headers, V3_VERSION_LIST) elif uri == BASE_URL + ""/v2.0"": v2_token = ks_v2_fixture.Token(token_id) return (200, response_headers, jsonutils.dumps(v2_token)) elif uri == BASE_URL + ""/v3"": v3_token = ks_v3_fixture.Token() response_headers[""X-Subject-Token""] = token_id return (201, response_headers, jsonutils.dumps(v3_token)) ",,570,11
openstack%2Fdiskimage-builder~master~I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3,openstack/diskimage-builder,master,I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3,Add umount_safe function to get better debug info,NEW,2017-10-22 17:40:16.000000000,2017-12-18 00:20:41.000000000,,"[{'_account_id': 7118}, {'_account_id': 10118}, {'_account_id': 21741}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2017-10-22 17:40:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/6990abac3a1b917baef20812ea675c1217dcca09', 'message': 'Add umount_safe function to get better debug info\n\nFrom time to time DIB fails with errors like:\n\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev\numount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy\n        (In some cases useful info about processes that\n         use the device is found by lsof(8) or fuser(1).)\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj\n\nNevertheless it is later mostly impossible to get the\nreason why this failed.\n\nThis patch adds a function that gives better / more\ndebug information when an umount fails.\n\nChange-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3\nSigned-off-by: Andreas Florath <andreas@florath.net>\n'}, {'number': 2, 'created': '2017-10-22 18:02:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/7c6b5a2efb396b7ddb2b1b8c6634ce16a8394a6c', 'message': 'Add umount_safe function to get better debug info\n\nFrom time to time DIB fails with errors like:\n\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev\numount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy\n        (In some cases useful info about processes that\n         use the device is found by lsof(8) or fuser(1).)\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj\n\nNevertheless it is later mostly impossible to get the\nreason why this failed.\n\nThis patch adds a function that gives better / more\ndebug information when an umount fails.\n\nChange-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3\nSigned-off-by: Andreas Florath <andreas@florath.net>\n'}, {'number': 3, 'created': '2017-10-22 19:10:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/4bb1004f746443a9e8c732f33fddee50ba6c4b6d', 'message': 'Add umount_safe function to get better debug info\n\nFrom time to time DIB fails with errors like:\n\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev\numount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy\n        (In some cases useful info about processes that\n         use the device is found by lsof(8) or fuser(1).)\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj\n\nNevertheless it is later mostly impossible to get the\nreason why this failed.\n\nThis patch adds a function that gives better / more\ndebug information when an umount fails.\n\nChange-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3\nSigned-off-by: Andreas Florath <andreas@florath.net>\n'}, {'number': 4, 'created': '2017-10-22 19:35:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/d84f8cdb8b76f41cff617e0b83372e962b26cc79', 'message': 'Add umount_safe function to get better debug info\n\nFrom time to time DIB fails with errors like:\n\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev\numount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy\n        (In some cases useful info about processes that\n         use the device is found by lsof(8) or fuser(1).)\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj\n\nNevertheless it is later mostly impossible to get the\nreason why this failed.\n\nThis patch adds a function that gives better / more\ndebug information when an umount fails.\n\nChange-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3\nSigned-off-by: Andreas Florath <andreas@florath.net>\n'}, {'number': 5, 'created': '2017-10-22 19:50:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/23742552ec96d25262e599135fdd67bc5299f8aa', 'message': 'Add umount_safe function to get better debug info\n\nFrom time to time DIB fails with errors like:\n\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev\numount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy\n        (In some cases useful info about processes that\n         use the device is found by lsof(8) or fuser(1).)\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj\n\nNevertheless it is later mostly impossible to get the\nreason why this failed.\n\nThis patch adds a function that gives better / more\ndebug information when an umount fails.\n\nChange-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3\nSigned-off-by: Andreas Florath <andreas@florath.net>\n'}, {'number': 6, 'created': '2017-10-23 06:43:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/13b078f8ec27d32e9d772915860149b14366ed0a', 'message': 'Add umount_safe function to get better debug info\n\nFrom time to time DIB fails with errors like:\n\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev\numount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy\n        (In some cases useful info about processes that\n         use the device is found by lsof(8) or fuser(1).)\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj\n\nNevertheless it is later mostly impossible to get the\nreason why this failed.\n\nThis patch adds a function that gives better / more\ndebug information when an umount fails.\n\nChange-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3\nSigned-off-by: Andreas Florath <andreas@florath.net>\n'}, {'number': 7, 'created': '2017-11-03 16:43:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/2a6a49ae452d5c8f95574af9fe19ecbe0bcd6989', 'message': ""[WIP] Add umount_safe function to get better debug info\n\nMarked as WIP because Ian reason that 'umount -l' should\nnot be used - and I'm not sure if this really succeeds...\n\nFrom time to time DIB fails with errors like:\n\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev\numount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy\n        (In some cases useful info about processes that\n         use the device is found by lsof(8) or fuser(1).)\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj\n\nNevertheless it is later mostly impossible to get the\nreason why this failed.\n\nThis patch adds a function that gives better / more\ndebug information when an umount fails.\n\nChange-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}, {'number': 8, 'created': '2017-12-09 08:36:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/873211496f7508320d2334cbe6e174fa0deb5a2f', 'message': ""Add umount_safe function to get better debug info\n\nFrom time to time DIB fails with errors like:\n\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev\numount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy\n        (In some cases useful info about processes that\n         use the device is found by lsof(8) or fuser(1).)\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj\n\nNevertheless it is later mostly impossible to get the\nreason why this failed.\n\nThis patch adds a function that gives better / more\ndebug information when an umount fails.\n\nRemark for Ubuntu Trusty: this fails if the '-fl' is not\nused for umounting.  Therefore the current (old) behavior for\nthis distribution is still in place.\n\nChange-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}, {'number': 9, 'created': '2017-12-09 09:16:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/1485e81c6c11aaada6cced31e79247a807731188', 'message': ""Add umount_safe function to get better debug info\n\nFrom time to time DIB fails with errors like:\n\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev\numount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy\n        (In some cases useful info about processes that\n         use the device is found by lsof(8) or fuser(1).)\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj\n\nNevertheless it is later mostly impossible to get the\nreason why this failed.\n\nThis patch adds a function that gives better / more\ndebug information when an umount fails.\n\nRemark for Ubuntu Trusty: this fails if the '-fl' is not\nused for umounting.  Therefore the current (old) behavior for\nthis distribution is still in place.\n\nChange-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}, {'number': 10, 'created': '2017-12-09 21:42:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/9120373d170f1726a110dc56b6cc187330bcda95', 'message': ""Add umount_safe function to get better debug info\n\nFrom time to time DIB fails with errors like:\n\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev\numount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy\n        (In some cases useful info about processes that\n         use the device is found by lsof(8) or fuser(1).)\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj\n\nNevertheless it is later mostly impossible to get the\nreason why this failed.\n\nThis patch adds a function that gives better / more\ndebug information when an umount fails.\n\nRemark for Ubuntu Trusty: this fails if the '-fl' is not\nused for umounting.  Therefore the current (old) behavior for\nthis distribution is still in place.\n\nChange-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}, {'number': 11, 'created': '2017-12-10 08:37:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/46b52f4d166b7455decc23fd5f93a9ba19c729ff', 'message': ""Add umount_safe function to get better debug info\n\nFrom time to time DIB fails with errors like:\n\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev\numount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy\n        (In some cases useful info about processes that\n         use the device is found by lsof(8) or fuser(1).)\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj\n\nNevertheless it is later mostly impossible to get the\nreason why this failed.\n\nThis patch adds a function that gives better / more\ndebug information when an umount fails.\n\nRemark for Ubuntu Trusty: this fails if the '-fl' is not\nused for umounting.  Therefore the current (old) behavior for\nthis distribution is still in place.\n\nChange-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}, {'number': 12, 'created': '2017-12-10 09:47:08.000000000', 'files': ['diskimage_builder/elements/yum-minimal/root.d/08-yum-chroot', 'diskimage_builder/lib/common-functions', 'diskimage_builder/elements/zypper-minimal/root.d/08-zypper-chroot', 'diskimage_builder/elements/ubuntu-minimal/root.d/75-ubuntu-minimal-baseinstall', 'diskimage_builder/elements/debian-minimal/root.d/75-debian-minimal-baseinstall'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/2026da5b5b5000a4aeed6672ac45c64f12d6dd23', 'message': ""Add umount_safe function to get better debug info\n\nFrom time to time DIB fails with errors like:\n\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev\numount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy\n        (In some cases useful info about processes that\n         use the device is found by lsof(8) or fuser(1).)\n+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj\n\nNevertheless it is later mostly impossible to get the\nreason why this failed.\n\nThis patch adds a function that gives better / more\ndebug information when an umount fails.\n\nRemark for Ubuntu Trusty: this fails if the '-fl' is not\nused for umounting.  Therefore the current (old) behavior for\nthis distribution is still in place.\n\nChange-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3\nSigned-off-by: Andreas Florath <andreas@florath.net>\n""}]",4,514081,2026da5b5b5000a4aeed6672ac45c64f12d6dd23,54,5,12,21741,,,0,"Add umount_safe function to get better debug info

From time to time DIB fails with errors like:

+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:297                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/tmp/yum
+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:298                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/proc
+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:299                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev/pts
+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:300                      :   sudo umount /tmp/dib_build.sRF6KeQX/mnt/dev
umount: /tmp/dib_build.sRF6KeQX/mnt/dev: target is busy
        (In some cases useful info about processes that
         use the device is found by lsof(8) or fuser(1).)
+ /tmp/dib_build.sRF6KeQX/hooks/root.d/08-yum-chroot:main:1                        :   rm -r /tmp/tmp.IDtSnNTDaj

Nevertheless it is later mostly impossible to get the
reason why this failed.

This patch adds a function that gives better / more
debug information when an umount fails.

Remark for Ubuntu Trusty: this fails if the '-fl' is not
used for umounting.  Therefore the current (old) behavior for
this distribution is still in place.

Change-Id: I8c50a54f64a6ba91ccc46ecfefb50994d01d21f3
Signed-off-by: Andreas Florath <andreas@florath.net>
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/81/514081/6 && git format-patch -1 --stdout FETCH_HEAD,"['diskimage_builder/elements/yum-minimal/root.d/08-yum-chroot', 'diskimage_builder/lib/common-functions', 'diskimage_builder/elements/zypper-minimal/root.d/08-zypper-chroot', 'diskimage_builder/elements/debian-minimal/root.d/75-debian-minimal-baseinstall', 'diskimage_builder/elements/ubuntu-minimal/root.d/75-ubuntu-minimal-baseinstall']",5,6990abac3a1b917baef20812ea675c1217dcca09,safe_umount,"source $_LIB/common-functions trap ""umount_safe $TARGET_ROOT/proc; umount_safe $TARGET_ROOT/sys"" EXIT","trap ""sudo umount $TARGET_ROOT/proc; sudo umount $TARGET_ROOT/sys"" EXIT",52,22
openstack%2Fpython-manilaclient~master~Ib9e007b9eeae5f2079dd682df0c7ef2294961f4f,openstack/python-manilaclient,master,Ib9e007b9eeae5f2079dd682df0c7ef2294961f4f,Avoid tox_install.sh for constraints support,MERGED,2017-12-02 08:29:18.000000000,2017-12-18 00:11:14.000000000,2017-12-18 00:11:14.000000000,"[{'_account_id': 6491}, {'_account_id': 9003}, {'_account_id': 22348}, {'_account_id': 24441}]","[{'number': 1, 'created': '2017-12-02 08:29:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/cc4a90e70b7754c8caee2192a2c0ef195fe3dc1f', 'message': 'Avoid tox_install.sh for constraints support\n\nWe do not need tox_install.sh, pip can handle constraints itself\nand install the project correctly. Thus update tox.ini and remove\nthe now obsolete tools/tox_install.sh file.\n\nThis follows https://review.openstack.org/#/c/508061 to remove\ntools/tox_install.sh.\n\nChange-Id: Ib9e007b9eeae5f2079dd682df0c7ef2294961f4f\n'}, {'number': 2, 'created': '2017-12-02 16:56:06.000000000', 'files': ['tools/tox_install.sh', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/3b5e6bc24fa1674cb7269ee21ba1f617df6b9357', 'message': 'Avoid tox_install.sh for constraints support\n\nWe do not need tox_install.sh, pip can handle constraints itself\nand install the project correctly. Thus update tox.ini and remove\nthe now obsolete tools/tox_install.sh file.\n\nThis follows https://review.openstack.org/#/c/508061 to remove\ntools/tox_install.sh.\n\nChange-Id: Ib9e007b9eeae5f2079dd682df0c7ef2294961f4f\n'}]",0,524829,3b5e6bc24fa1674cb7269ee21ba1f617df6b9357,10,4,2,6547,,,0,"Avoid tox_install.sh for constraints support

We do not need tox_install.sh, pip can handle constraints itself
and install the project correctly. Thus update tox.ini and remove
the now obsolete tools/tox_install.sh file.

This follows https://review.openstack.org/#/c/508061 to remove
tools/tox_install.sh.

Change-Id: Ib9e007b9eeae5f2079dd682df0c7ef2294961f4f
",git fetch https://review.opendev.org/openstack/python-manilaclient refs/changes/29/524829/1 && git format-patch -1 --stdout FETCH_HEAD,"['tools/tox_install.sh', 'tox.ini']",2,cc4a90e70b7754c8caee2192a2c0ef195fe3dc1f,rm-tox_install,install_command = pip install -U {opts} {packages}deps = -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt} -r{toxinidir}/requirements.txt,install_command = {toxinidir}/tools/tox_install.sh {env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt} {opts} {packages} BRANCH_NAME=master CLIENT_NAME=python-manilaclientdeps = -r{toxinidir}/requirements.txt,4,34
openstack%2Fopenstack-helm-infra~master~I2a6a28832c0b1beedeb3e280572b3717628f7b88,openstack/openstack-helm-infra,master,I2a6a28832c0b1beedeb3e280572b3717628f7b88,Elasticsearch: Move default storage access-mode to ReadWriteOnce,MERGED,2017-12-17 14:06:50.000000000,2017-12-17 23:56:25.000000000,2017-12-17 23:56:25.000000000,"[{'_account_id': 17591}, {'_account_id': 17966}, {'_account_id': 20466}, {'_account_id': 22348}, {'_account_id': 22477}, {'_account_id': 23928}]","[{'number': 1, 'created': '2017-12-17 14:06:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/677e88a802fd99fd735ade564bcd2edfa0eeedad', 'message': 'Elasticsearch: Move default storage access-mode to ReadWriteOnce\n\nThis PS moves the default storage access-mode to ReadWriteOnce, as\nthe PVC is created inline with the statefulset. So ReadWriteMany will\nhave no effect, as a volume is created per pod.\n\nChange-Id: I2a6a28832c0b1beedeb3e280572b3717628f7b88\n'}, {'number': 2, 'created': '2017-12-17 14:07:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/3eb5bf62321696e697edd4ffd3c883b403b5fea2', 'message': 'Elasticsearch: Move default storage access-mode to ReadWriteOnce\n\nThis PS moves the default storage access-mode to ReadWriteOnce, as\nthe PVC is created inline with the statefulset. So ReadWriteMany will\nhave no effect, as a volume is created per pod.\n\nChange-Id: I2a6a28832c0b1beedeb3e280572b3717628f7b88\n'}, {'number': 3, 'created': '2017-12-17 17:08:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/a669e5cbb68860d3d151597d43fd90d51fd12bb3', 'message': 'Elasticsearch: Move default storage access-mode to ReadWriteOnce\n\nThis PS moves the default storage access-mode to ReadWriteOnce, as\nthe PVC is created inline with the statefulset. So ReadWriteMany will\nhave no effect, as a volume is created per pod.\n\nChange-Id: I2a6a28832c0b1beedeb3e280572b3717628f7b88\n'}, {'number': 4, 'created': '2017-12-17 21:34:10.000000000', 'files': ['elasticsearch/values.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/6e5fe71d9c541e8901fb40cdff898775abe64310', 'message': 'Elasticsearch: Move default storage access-mode to ReadWriteOnce\n\nThis PS moves the default storage access-mode to ReadWriteOnce, as\nthe PVC is created inline with the statefulset. So ReadWriteMany will\nhave no effect, as a volume is created per pod.\n\nChange-Id: I2a6a28832c0b1beedeb3e280572b3717628f7b88\n'}]",0,528561,6e5fe71d9c541e8901fb40cdff898775abe64310,18,6,4,23928,,,0,"Elasticsearch: Move default storage access-mode to ReadWriteOnce

This PS moves the default storage access-mode to ReadWriteOnce, as
the PVC is created inline with the statefulset. So ReadWriteMany will
have no effect, as a volume is created per pod.

Change-Id: I2a6a28832c0b1beedeb3e280572b3717628f7b88
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/61/528561/2 && git format-patch -1 --stdout FETCH_HEAD,['elasticsearch/values.yaml'],1,677e88a802fd99fd735ade564bcd2edfa0eeedad,el-storageclass," access_mode: [ ""ReadWriteOnce"" ]"," access_mode: [ ""ReadWriteMany"" ]",1,1
openstack%2Fopenstack-helm-infra~master~I751c497ce471a1df49dfd9e300bd058779be3183,openstack/openstack-helm-infra,master,I751c497ce471a1df49dfd9e300bd058779be3183,Add role and rolebinding descriptions to post-run checks,ABANDONED,2017-12-17 23:22:07.000000000,2017-12-17 23:25:53.000000000,,[],"[{'number': 1, 'created': '2017-12-17 23:22:07.000000000', 'files': ['tools/gate/playbooks/describe-kubernetes-resources/tasks/main.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/eafabd09a4a6dcc09d874727f8b12d36c57884ee', 'message': 'Add role and rolebinding descriptions to post-run checks\n\nAdds role and rolebinding descriptions as part of the describe-\nkubernetes-resources tasks that run as part of the post-run\nplaybooks in the gate as they were previously omitted\n\nChange-Id: I751c497ce471a1df49dfd9e300bd058779be3183\n'}]",0,528590,eafabd09a4a6dcc09d874727f8b12d36c57884ee,2,0,1,17591,,,0,"Add role and rolebinding descriptions to post-run checks

Adds role and rolebinding descriptions as part of the describe-
kubernetes-resources tasks that run as part of the post-run
playbooks in the gate as they were previously omitted

Change-Id: I751c497ce471a1df49dfd9e300bd058779be3183
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/90/528590/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/gate/playbooks/describe-kubernetes-resources/tasks/main.yaml'],1,eafabd09a4a6dcc09d874727f8b12d36c57884ee,update_descriptions, - role - rolebinding,,2,0
openstack%2Fpython-ironicclient~master~I95471925f8344cb1a5d79bc127b358317a31b718,openstack/python-ironicclient,master,I95471925f8344cb1a5d79bc127b358317a31b718,Add a command to show power state and supported power states,NEW,2017-11-14 08:46:11.000000000,2017-12-17 22:33:15.000000000,,"[{'_account_id': 13689}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-11-14 08:46:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/ad180811145d286a6c61a0fd962b8f77713477ab', 'message': 'Add supported_power_states to node\n\nThis patch adds support for the new field ""supported_power_states"" of\nnodes, which was introduced in API version 1.36.\n\nBecause the ironic CLI is already deprecated, the ironic CLI doesn\'t\nsupport the field. For the purpose, NODE_DETAILED_RESOURCE_IRONICCLI\nis added to resource_fields.py.\n\nChange-Id: I95471925f8344cb1a5d79bc127b358317a31b718\nDepends-On: Ic4ce99d58febdb5c702ebbfc9276979e6ca6fba0\nCloses-Bug: #1526226\n'}, {'number': 2, 'created': '2017-11-14 10:57:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/a2370909dd6939fe7b796ada9268c020448e0f30', 'message': 'Add supported_power_states to node\n\nThis patch adds support for the new field ""supported_power_states"" of\nnodes, which was introduced in API version 1.36.\n\nBecause the ironic CLI is already deprecated, the ironic CLI doesn\'t\nsupport the field. For the purpose, NODE_DETAILED_RESOURCE_IRONICCLI\nis added to resource_fields.py.\n\nChange-Id: I95471925f8344cb1a5d79bc127b358317a31b718\nDepends-On: Ic4ce99d58febdb5c702ebbfc9276979e6ca6fba0\nCloses-Bug: #1526226\n'}, {'number': 3, 'created': '2017-11-14 12:51:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/c3a79f0c64c4cfa88f8186fba9a36dc0d3b5e7f1', 'message': 'Add supported_power_states to node\n\nThis patch adds support for the new field ""supported_power_states"" of\nnodes, which was introduced in API version 1.36.\n\nBecause the ironic CLI is already deprecated, the ironic CLI doesn\'t\nsupport the field. For the purpose, NODE_DETAILED_RESOURCE_IRONICCLI\nis added to resource_fields.py.\n\nChange-Id: I95471925f8344cb1a5d79bc127b358317a31b718\nDepends-On: Ic4ce99d58febdb5c702ebbfc9276979e6ca6fba0\nCloses-Bug: #1526226\n'}, {'number': 4, 'created': '2017-12-17 13:18:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/6bf72fc72db063ec6723e2e5bfaae7822e36e907', 'message': ""Add a command to show power state and supported power states\n\nThis patch adds a new command, 'openstack baremetal node power show'.\nThis command displays a power state of a node. If '--supported' option\nis specified, this command displays supported power states of a node.\nThis command is available with Bare Metal API version 1.37 or later.\n\nChange-Id: I95471925f8344cb1a5d79bc127b358317a31b718\nDepends-On: Ic4ce99d58febdb5c702ebbfc9276979e6ca6fba0\nCloses-Bug: #1734827\n""}, {'number': 5, 'created': '2017-12-17 21:50:55.000000000', 'files': ['releasenotes/notes/osc-node-power-show-75a5fe9079b9f7d5.yaml', 'ironicclient/v1/node.py', 'ironicclient/tests/unit/v1/test_node.py', 'ironicclient/osc/plugin.py', 'setup.cfg', 'ironicclient/v1/resource_fields.py', 'ironicclient/tests/unit/osc/v1/test_baremetal_node.py', 'ironicclient/osc/v1/baremetal_node.py'], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/fa00ccba82b9f0f5b4ffdc9cba2bad6018ac6dfe', 'message': ""Add a command to show power state and supported power states\n\nThis patch adds a new command, 'openstack baremetal node power show'.\nThis command displays a power state of a node. If '--supported' option\nis specified, this command displays supported power states of a node.\nThis command is available with Bare Metal API version 1.37 or later.\n\nChange-Id: I95471925f8344cb1a5d79bc127b358317a31b718\nDepends-On: Ic4ce99d58febdb5c702ebbfc9276979e6ca6fba0\nCloses-Bug: #1734827\n""}]",0,519581,fa00ccba82b9f0f5b4ffdc9cba2bad6018ac6dfe,11,2,5,13689,,,0,"Add a command to show power state and supported power states

This patch adds a new command, 'openstack baremetal node power show'.
This command displays a power state of a node. If '--supported' option
is specified, this command displays supported power states of a node.
This command is available with Bare Metal API version 1.37 or later.

Change-Id: I95471925f8344cb1a5d79bc127b358317a31b718
Depends-On: Ic4ce99d58febdb5c702ebbfc9276979e6ca6fba0
Closes-Bug: #1734827
",git fetch https://review.opendev.org/openstack/python-ironicclient refs/changes/81/519581/5 && git format-patch -1 --stdout FETCH_HEAD,"['ironicclient/v1/node_shell.py', 'ironicclient/tests/functional/osc/v1/test_baremetal_node_fields.py', 'ironicclient/tests/unit/v1/test_node.py', 'ironicclient/tests/functional/test_json_response.py', 'ironicclient/osc/plugin.py', 'ironicclient/v1/chassis_shell.py', 'releasenotes/notes/add-node-supported-power-states-73559b43f731fa72.yaml', 'ironicclient/v1/resource_fields.py', 'ironicclient/tests/unit/osc/v1/test_baremetal_node.py']",9,ad180811145d286a6c61a0fd962b8f77713477ab,bug/1734827," 'Resource Class', 'Supported Power States', '',"," 'Resource Class',",103,18
openstack%2Fopenstack-ansible~master~Ic907ee734a95e8af7c4c51568cc0870607b7b2f9,openstack/openstack-ansible,master,Ic907ee734a95e8af7c4c51568cc0870607b7b2f9,ceph: Switch to central ceph-ansible repository,MERGED,2017-12-14 10:42:55.000000000,2017-12-17 21:57:05.000000000,2017-12-17 21:57:05.000000000,"[{'_account_id': 538}, {'_account_id': 6816}, {'_account_id': 7353}, {'_account_id': 17068}, {'_account_id': 17799}, {'_account_id': 22348}, {'_account_id': 23163}, {'_account_id': 24468}]","[{'number': 1, 'created': '2017-12-14 10:42:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/25bab6a0f87d8c8e8a80fbe78935e1a00fe2efb3', 'message': 'ceph: Switch to central ceph-ansible repository\n\nAll the ceph roles are generated from the central ceph-ansible\nrepository. However, it appears that the script (or CI) that\ngenerates these roles has been broken for a while. The last update\nfor generated roles was nearly 2 months ago. This means that we\nare missing a lot of ceph features and fixes that exist in the central\nrepo but not on the generated roles. Lets switch to the main\nceph-ansible repository for our Ceph deployments in order to ensure\nthat we are always using the latest code.\n\nChange-Id: Ic907ee734a95e8af7c4c51568cc0870607b7b2f9\n'}, {'number': 2, 'created': '2017-12-14 16:13:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/a3dd3cc7479c1f0a29adbdaff3a4a0fce0923c24', 'message': 'ceph: Switch to central ceph-ansible repository\n\nAll the ceph roles are generated from the central ceph-ansible\nrepository. However, it appears that the script (or CI) that\ngenerates these roles has been broken for a while. The last update\nfor generated roles was nearly 2 months ago. This means that we\nare missing a lot of ceph features and fixes that exist in the central\nrepo but not on the generated roles. Lets switch to the main\nceph-ansible repository for our Ceph deployments in order to ensure\nthat we are always using the latest code.\n\nChange-Id: Ic907ee734a95e8af7c4c51568cc0870607b7b2f9\n'}, {'number': 3, 'created': '2017-12-14 16:35:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/4e0540ef43fef127fa95ab0434e7c632cfc5f057', 'message': 'ceph: Switch to central ceph-ansible repository\n\nAll the ceph roles are generated from the central ceph-ansible\nrepository. However, it appears that the script (or CI) that\ngenerates these roles has been broken for a while. The last update\nfor generated roles was nearly 2 months ago. This means that we\nare missing a lot of ceph features and fixes that exist in the central\nrepo but not on the generated roles. Lets switch to the main\nceph-ansible repository for our Ceph deployments in order to ensure\nthat we are always using the latest code.\n\nChange-Id: Ic907ee734a95e8af7c4c51568cc0870607b7b2f9\n'}, {'number': 4, 'created': '2017-12-14 16:53:00.000000000', 'files': ['scripts/openstack-ansible.rc', 'tox.ini', 'ansible-role-requirements.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/01501f33ad965e8174bcbdd05a7341dc691d171d', 'message': 'ceph: Switch to central ceph-ansible repository\n\nAll the ceph roles are generated from the central ceph-ansible\nrepository. However, it appears that the script (or CI) that\ngenerates these roles has been broken for a while. The last update\nfor generated roles was nearly 2 months ago. This means that we\nare missing a lot of ceph features and fixes that exist in the central\nrepo but not on the generated roles. Lets switch to the main\nceph-ansible repository for our Ceph deployments in order to ensure\nthat we are always using the latest code.\n\nChange-Id: Ic907ee734a95e8af7c4c51568cc0870607b7b2f9\n'}]",0,527929,01501f33ad965e8174bcbdd05a7341dc691d171d,32,8,4,23163,,,0,"ceph: Switch to central ceph-ansible repository

All the ceph roles are generated from the central ceph-ansible
repository. However, it appears that the script (or CI) that
generates these roles has been broken for a while. The last update
for generated roles was nearly 2 months ago. This means that we
are missing a lot of ceph features and fixes that exist in the central
repo but not on the generated roles. Lets switch to the main
ceph-ansible repository for our Ceph deployments in order to ensure
that we are always using the latest code.

Change-Id: Ic907ee734a95e8af7c4c51568cc0870607b7b2f9
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/29/527929/4 && git format-patch -1 --stdout FETCH_HEAD,"['scripts/openstack-ansible.rc', 'ansible-role-requirements.yml']",2,25bab6a0f87d8c8e8a80fbe78935e1a00fe2efb3,switch-to-ceph-monolithic,- name: ceph-ansible src: https://github.com/ceph/ceph-ansible,- name: ceph-defaults src: https://github.com/ceph/ansible-ceph-defaults version: master - name: ceph-common scm: git src: https://github.com/ceph/ansible-ceph-common version: master - name: ceph-config scm: git src: https://github.com/ceph/ansible-ceph-config version: master - name: ceph-mon scm: git src: https://github.com/ceph/ansible-ceph-mon version: master - name: ceph-mgr scm: git src: https://github.com/ceph/ansible-ceph-mgr version: master - name: ceph-osd scm: git src: https://github.com/ceph/ansible-ceph-osd,3,23
openstack%2Fopenstack-ansible~master~Iac4167a63387f645e91d4529218c9d2de87ea5a7,openstack/openstack-ansible,master,Iac4167a63387f645e91d4529218c9d2de87ea5a7,"Revert ""Async clone for git roles""",MERGED,2017-12-15 18:11:17.000000000,2017-12-17 21:30:50.000000000,2017-12-17 21:30:50.000000000,"[{'_account_id': 3}, {'_account_id': 538}, {'_account_id': 6816}, {'_account_id': 7353}, {'_account_id': 14805}, {'_account_id': 15993}, {'_account_id': 17068}, {'_account_id': 17799}, {'_account_id': 22348}, {'_account_id': 24468}]","[{'number': 1, 'created': '2017-12-15 18:11:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/23cd3623ba832aa22c4e142b241d4a6226e5d3c9', 'message': 'Revert ""Async clone for git roles""\n\nThis reverts commit 7a1e46b2c55d35037030454533fd436442706e7d.\n\nTasks executed asynchronously do not implement retries\nlike normal tasks do. As the role clones are failing quite\noften due to TLS failures we need the retries.\n\nChange-Id: Iac4167a63387f645e91d4529218c9d2de87ea5a7\n'}, {'number': 2, 'created': '2017-12-15 18:11:36.000000000', 'files': ['tests/get-ansible-role-requirements.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/4338a648d3b45ced1345dce92d71ec049ffe001d', 'message': 'Revert ""Async clone for git roles""\n\nThis reverts commit 7a1e46b2c55d35037030454533fd436442706e7d.\n\nTasks executed asynchronously do not implement retries\nlike normal tasks do. As the role clones are failing quite\noften due to TLS failures we need the retries.\n\nChange-Id: Iac4167a63387f645e91d4529218c9d2de87ea5a7\n'}]",0,528365,4338a648d3b45ced1345dce92d71ec049ffe001d,18,10,2,6816,,,0,"Revert ""Async clone for git roles""

This reverts commit 7a1e46b2c55d35037030454533fd436442706e7d.

Tasks executed asynchronously do not implement retries
like normal tasks do. As the role clones are failing quite
often due to TLS failures we need the retries.

Change-Id: Iac4167a63387f645e91d4529218c9d2de87ea5a7
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/65/528365/1 && git format-patch -1 --stdout FETCH_HEAD,['tests/get-ansible-role-requirements.yml'],1,23cd3623ba832aa22c4e142b241d4a6226e5d3c9,async-role-clone, until: git_clone | success," async: 1800 poll: 0 - name: Wait for git clones to complete async_status: jid: ""{{ item['ansible_job_id'] }}"" register: _git_jobs until: _git_jobs['finished'] | bool delay: 5 retries: 360 with_items: ""{{ git_clone['results'] }}"" when: - item['ansible_job_id'] is defined",1,13
openstack%2Fopenstack-ansible~master~Ibdd0fa8b152004b450693498d61b84b204f74e9b,openstack/openstack-ansible,master,Ibdd0fa8b152004b450693498d61b84b204f74e9b,make include tasks dynamic when there is rescue,MERGED,2017-12-13 00:16:42.000000000,2017-12-17 21:18:52.000000000,2017-12-17 21:18:51.000000000,"[{'_account_id': 6671}, {'_account_id': 6816}, {'_account_id': 7353}, {'_account_id': 22348}, {'_account_id': 23163}, {'_account_id': 24468}]","[{'number': 1, 'created': '2017-12-13 00:16:42.000000000', 'files': ['tests/roles/bootstrap-host/tasks/main.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/a8e99d296ecac96155da8e21c8812f9292ae48a1', 'message': 'make include tasks dynamic when there is rescue\n\nthere is an ansible bug that prevents rescue in failing tasks from\nbeing called if the task is included. so change it to\ndynamic to run rescue in preprocessing.\n\nhttps://github.com/ansible/ansible/issues/16254\n\nChange-Id: Ibdd0fa8b152004b450693498d61b84b204f74e9b\n'}]",0,527546,a8e99d296ecac96155da8e21c8812f9292ae48a1,28,6,1,6671,,,0,"make include tasks dynamic when there is rescue

there is an ansible bug that prevents rescue in failing tasks from
being called if the task is included. so change it to
dynamic to run rescue in preprocessing.

https://github.com/ansible/ansible/issues/16254

Change-Id: Ibdd0fa8b152004b450693498d61b84b204f74e9b
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/46/527546/1 && git format-patch -1 --stdout FETCH_HEAD,['tests/roles/bootstrap-host/tasks/main.yml'],1,a8e99d296ecac96155da8e21c8812f9292ae48a1,ansible_bug, static: no,,1,0
openstack%2Fopenstack-helm-infra~master~I3bf9baf9824e1b7f7e46c4fcae292240566d9153,openstack/openstack-helm-infra,master,I3bf9baf9824e1b7f7e46c4fcae292240566d9153,Fluent-Logging: Update fluent-bit to use common OSH entrypoint pattern,MERGED,2017-12-17 14:20:03.000000000,2017-12-17 21:05:59.000000000,2017-12-17 21:05:59.000000000,"[{'_account_id': 17591}, {'_account_id': 17966}, {'_account_id': 20466}, {'_account_id': 22348}, {'_account_id': 22477}]","[{'number': 1, 'created': '2017-12-17 14:20:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/59f62b068bee4ec6766d2db40cccf50489311f11', 'message': 'Fluent-Logging: Update fluent-bit to use common OSH entrypoint pattern\n\nThis PS updates the fluent-logging chart to use the same entrypoint\npattern as other OSH components.\n\nChange-Id: I3bf9baf9824e1b7f7e46c4fcae292240566d9153\n'}, {'number': 2, 'created': '2017-12-17 17:30:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/24adf78ec045cac8de1689e8a6d4d165bfa6bfda', 'message': 'Fluent-Logging: Update fluent-bit to use common OSH entrypoint pattern\n\nThis PS updates the fluent-logging chart to use the same entrypoint\npattern as other OSH components.\n\nChange-Id: I3bf9baf9824e1b7f7e46c4fcae292240566d9153\n'}, {'number': 3, 'created': '2017-12-17 17:33:24.000000000', 'files': ['fluent-logging/templates/daemonset-fluent-bit.yaml', 'fluent-logging/templates/bin/_fluent-bit.sh.tpl', 'fluent-logging/templates/configmap-bin.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/611a78fb349d9902eee837d38711023bf7987854', 'message': 'Fluent-Logging: Update fluent-bit to use common OSH entrypoint pattern\n\nThis PS updates the fluent-logging chart to use the same entrypoint\npattern as other OSH components.\n\nChange-Id: I3bf9baf9824e1b7f7e46c4fcae292240566d9153\n'}]",0,528564,611a78fb349d9902eee837d38711023bf7987854,10,5,3,23928,,,0,"Fluent-Logging: Update fluent-bit to use common OSH entrypoint pattern

This PS updates the fluent-logging chart to use the same entrypoint
pattern as other OSH components.

Change-Id: I3bf9baf9824e1b7f7e46c4fcae292240566d9153
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/64/528564/2 && git format-patch -1 --stdout FETCH_HEAD,"['fluent-logging/templates/daemonset-fluent-bit.yaml', 'fluent-logging/templates/bin/_fluent-bit.sh.tpl', 'fluent-logging/templates/configmap-bin.yaml']",3,59f62b068bee4ec6766d2db40cccf50489311f11,fluent-bit/updates," fluent-bit.sh: | {{ tuple ""bin/_fluent-bit.sh.tpl"" . | include ""helm-toolkit.utils.template"" | indent 4 }}",,29,0
openstack%2Fopenstack-helm-infra~master~I2c2d8180a6e05ac6babc72f6347f00a19bf7e0fd,openstack/openstack-helm-infra,master,I2c2d8180a6e05ac6babc72f6347f00a19bf7e0fd,Kubernetes: Update to v1.9.0,MERGED,2017-12-16 20:13:55.000000000,2017-12-17 20:35:37.000000000,2017-12-17 20:35:37.000000000,"[{'_account_id': 17591}, {'_account_id': 20466}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-16 20:13:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/bf818b5c397aabb10229eb1d46f17863bed6e206', 'message': 'Kubernetes: Update to v1.9.0\n\nThis PS updates the OpenStack-Infra Gate to use Kubernetes v1.9.0\n\nChange-Id: I2c2d8180a6e05ac6babc72f6347f00a19bf7e0fd\n'}, {'number': 2, 'created': '2017-12-17 13:36:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/17c0c39d74a035885b798400b1386e17ed3ba3ad', 'message': 'Kubernetes: Update to v1.9.0\n\nThis PS updates the OpenStack-Infra Gate to use Kubernetes v1.9.0\n\nChange-Id: I2c2d8180a6e05ac6babc72f6347f00a19bf7e0fd\n'}, {'number': 3, 'created': '2017-12-17 17:06:40.000000000', 'files': ['tools/gate/playbooks/vars.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/c189522fdb737aff53c459256f2934429b54e6e6', 'message': 'Kubernetes: Update to v1.9.0\n\nThis PS updates the OpenStack-Infra Gate to use Kubernetes v1.9.0\n\nChange-Id: I2c2d8180a6e05ac6babc72f6347f00a19bf7e0fd\n'}]",0,528495,c189522fdb737aff53c459256f2934429b54e6e6,11,3,3,23928,,,0,"Kubernetes: Update to v1.9.0

This PS updates the OpenStack-Infra Gate to use Kubernetes v1.9.0

Change-Id: I2c2d8180a6e05ac6babc72f6347f00a19bf7e0fd
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/95/528495/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/gate/playbooks/vars.yaml'],1,bf818b5c397aabb10229eb1d46f17863bed6e206,k8s/1.9.0, kubernetes: v1.9.0, kubernetes: v1.8.3,1,1
openstack%2Fopenstack-helm-infra~master~I8aa876aa96119d9a1a0e06c28873e3c4c1e3ace5,openstack/openstack-helm-infra,master,I8aa876aa96119d9a1a0e06c28873e3c4c1e3ace5,Docker: user json-file on Fedora and CentOS,MERGED,2017-12-16 21:25:01.000000000,2017-12-17 20:29:25.000000000,2017-12-17 20:29:25.000000000,"[{'_account_id': 17591}, {'_account_id': 20466}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-16 21:25:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/38ca4ad06ea2685723c0a3bd15598158850928f4', 'message': 'WIP/DNM: Docker, user json-file on Fedora and CentOS\n\nChange-Id: I8aa876aa96119d9a1a0e06c28873e3c4c1e3ace5\n'}, {'number': 2, 'created': '2017-12-17 13:35:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/8f8c0257e1926141f6fcf4dfe55ae0be3c7495fb', 'message': 'Docker: user json-file on Fedora and CentOS\n\nThe current fluent-bit implementation only supports the json-file\nlog driver for docker, this PS moves CentOS and Fedora to use that\nuntil we can support Journald.\n\nChange-Id: I8aa876aa96119d9a1a0e06c28873e3c4c1e3ace5\n'}, {'number': 3, 'created': '2017-12-17 17:06:16.000000000', 'files': ['tools/gate/playbooks/deploy-docker/templates/fedora-docker.service.j2', 'tools/gate/playbooks/deploy-docker/templates/centos-docker.service.j2'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/f472531ace96556123f9e4ca594dfda2f12626df', 'message': 'Docker: user json-file on Fedora and CentOS\n\nThe current fluent-bit implementation only supports the json-file\nlog driver for docker, this PS moves CentOS and Fedora to use that\nuntil we can support Journald.\n\nChange-Id: I8aa876aa96119d9a1a0e06c28873e3c4c1e3ace5\n'}]",0,528502,f472531ace96556123f9e4ca594dfda2f12626df,11,3,3,23928,,,0,"Docker: user json-file on Fedora and CentOS

The current fluent-bit implementation only supports the json-file
log driver for docker, this PS moves CentOS and Fedora to use that
until we can support Journald.

Change-Id: I8aa876aa96119d9a1a0e06c28873e3c4c1e3ace5
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/02/528502/2 && git format-patch -1 --stdout FETCH_HEAD,"['tools/gate/playbooks/deploy-docker/templates/fedora-docker.service.j2', 'tools/gate/playbooks/deploy-docker/templates/centos-docker.service.j2']",2,38ca4ad06ea2685723c0a3bd15598158850928f4,fix-fluentd, --log-driver=json-file, --log-driver=journald,2,2
openstack%2Fopenstack-helm-infra~master~If3a437e0756a363b8cefaa9a8bdd1c3498fedbfd,openstack/openstack-helm-infra,master,If3a437e0756a363b8cefaa9a8bdd1c3498fedbfd,CentOS: Fix jq and pip installation,MERGED,2017-11-25 01:18:43.000000000,2017-12-17 20:26:46.000000000,2017-12-17 20:26:46.000000000,"[{'_account_id': 7769}, {'_account_id': 17591}, {'_account_id': 17966}, {'_account_id': 20466}, {'_account_id': 22348}, {'_account_id': 22477}, {'_account_id': 23928}]","[{'number': 1, 'created': '2017-11-25 01:18:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/45ca479f255286972ab8c7f09fa5372c9af91a49', 'message': 'WIP: Fix centos package\n\nThis patch set should fix an issue where centos cannot find\npython-pip in the EPEL.\n\nChange-Id: If3a437e0756a363b8cefaa9a8bdd1c3498fedbfd\n'}, {'number': 2, 'created': '2017-11-26 17:24:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/0026219d5d521ad23135136cd744272efa96e715', 'message': 'WIP: Fix centos package\n\nThis patch set should fix an issue where centos cannot find\npython-pip in the EPEL.\n\nChange-Id: If3a437e0756a363b8cefaa9a8bdd1c3498fedbfd\n'}, {'number': 3, 'created': '2017-12-16 05:54:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/e1f91e673050f4c2a49cded350770952ba199b72', 'message': 'WIP: Fix centos package\n\nThis patch set should fix an issue where centos cannot find\npython-pip in the EPEL.\n\nChange-Id: If3a437e0756a363b8cefaa9a8bdd1c3498fedbfd\n'}, {'number': 4, 'created': '2017-12-16 06:12:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/c6cd69ff894c0eda324a6180ea1c9deb234b9870', 'message': 'WIP: Fix centos package\n\nThis patch set should fix an issue where centos cannot find\npython-pip in the EPEL.\n\nChange-Id: If3a437e0756a363b8cefaa9a8bdd1c3498fedbfd\n'}, {'number': 5, 'created': '2017-12-17 13:40:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/8e428dd3c63da45c703778f0ae3188f054079ad6', 'message': 'WIP: Fix centos package\n\nThis patch set should fix an issue where centos cannot find\npython-pip in the EPEL.\n\nChange-Id: If3a437e0756a363b8cefaa9a8bdd1c3498fedbfd\n'}, {'number': 6, 'created': '2017-12-17 15:18:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/59cd557d252868ae1df8c18e65f616ef7b9627e2', 'message': 'WIP: Fix centos package\n\nThis patch set should fix an issue where centos cannot find\npython-pip in the EPEL.\n\nChange-Id: If3a437e0756a363b8cefaa9a8bdd1c3498fedbfd\n'}, {'number': 7, 'created': '2017-12-17 15:37:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/dcec31f3d7b80132a1086e2f72e1a7ef58dbf5ac', 'message': 'WIP: Fix centos package\n\nThis patch set should fix an issue where centos cannot find\npython-pip in the EPEL.\n\nChange-Id: If3a437e0756a363b8cefaa9a8bdd1c3498fedbfd\n'}, {'number': 8, 'created': '2017-12-17 15:56:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/6a03a13ca8ca1c96e41d1850d9542497db6af319', 'message': 'WIP: Fix centos package\n\nThis patch set should fix an issue where centos cannot find\npython-pip in the EPEL.\n\nChange-Id: If3a437e0756a363b8cefaa9a8bdd1c3498fedbfd\n'}, {'number': 9, 'created': '2017-12-17 17:05:50.000000000', 'files': ['tools/gate/playbooks/deploy-yq/tasks/main.yaml', 'tools/gate/playbooks/deploy-helm-packages/tasks/helm-setup-dev-environment.yaml', 'tools/gate/playbooks/deploy-python-pip/tasks/main.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/e4de36d97b5c67cb30e885dedf0e2e17af25ac6c', 'message': 'CentOS: Fix jq and pip installation\n\nThis PS fixes pip and jq installation on CentOS. It also removes\nsome duplicate code in the gate playbooks.\n\nCo-Authored-By: portdirect <pete@port.direct>\n\nThis patch set should fix an issue where centos cannot find\npython-pip in the EPEL.\n\nChange-Id: If3a437e0756a363b8cefaa9a8bdd1c3498fedbfd\n'}]",0,522908,e4de36d97b5c67cb30e885dedf0e2e17af25ac6c,22,7,9,20466,,,0,"CentOS: Fix jq and pip installation

This PS fixes pip and jq installation on CentOS. It also removes
some duplicate code in the gate playbooks.

Co-Authored-By: portdirect <pete@port.direct>

This patch set should fix an issue where centos cannot find
python-pip in the EPEL.

Change-Id: If3a437e0756a363b8cefaa9a8bdd1c3498fedbfd
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/08/522908/2 && git format-patch -1 --stdout FETCH_HEAD,['tools/gate/playbooks/deploy-python-pip/tasks/main.yaml'],1,45ca479f255286972ab8c7f09fa5372c9af91a49,fix-centos, name: python-devel, name: python-pip,1,1
openstack%2Fopenstack-ansible-tests~master~I8c3dce39c17175edfb7c37dc0de3e9b9d1d0babb,openstack/openstack-ansible-tests,master,I8c3dce39c17175edfb7c37dc0de3e9b9d1d0babb,"Revert ""Revert ""Use Ansible 2.4""""",MERGED,2017-12-11 18:05:55.000000000,2017-12-17 19:18:27.000000000,2017-12-17 19:18:27.000000000,"[{'_account_id': 3}, {'_account_id': 538}, {'_account_id': 6816}, {'_account_id': 7353}, {'_account_id': 17068}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-11 18:05:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-tests/commit/c92070bb5e14a26b2503462ada1062eb5d0d891d', 'message': 'Revert ""Revert ""Use Ansible 2.4""""\n\nThis reverts commit 6713b5e9f7b334fef88173a939608a1af6fe5d9e.\n\nWith the facility [1] to add parameters (like exclusions) to the\nansible-lint test, and the implementation of an appropriate exclusion\n[2] to the integrated repo, we can move forward with updating both\nansible and ansible-lint here.\n\n[1] https://review.openstack.org/527195\n[2] https://review.openstack.org/527198\n\nDepends-On: If5746d35ee1b8ce5d6fd1a14a2abca16e29cb899\nDepends-On: Icaa997a37d9e31c70e952a80a3f75050965d7ef5\nChange-Id: I8c3dce39c17175edfb7c37dc0de3e9b9d1d0babb\n'}, {'number': 2, 'created': '2017-12-13 09:29:44.000000000', 'files': ['test-ansible-deps.txt'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-tests/commit/ac53b4d6464e4c14ac25c50b034468e8cf0b42d8', 'message': 'Revert ""Revert ""Use Ansible 2.4""""\n\nThis reverts commit 6713b5e9f7b334fef88173a939608a1af6fe5d9e.\n\nWith the facility [1] to add parameters (like exclusions) to the\nansible-lint test, and the implementation of an appropriate exclusion\n[2] to the integrated repo, we can move forward with updating both\nansible and ansible-lint here.\n\n[1] https://review.openstack.org/527195\n[2] https://review.openstack.org/527198\n\nDepends-On: If5746d35ee1b8ce5d6fd1a14a2abca16e29cb899\nDepends-On: Icaa997a37d9e31c70e952a80a3f75050965d7ef5\nDepends-On: I774829efd763da2400062574bc8266e544d6f75d\nChange-Id: I8c3dce39c17175edfb7c37dc0de3e9b9d1d0babb\n'}]",0,527200,ac53b4d6464e4c14ac25c50b034468e8cf0b42d8,19,6,2,6816,,,0,"Revert ""Revert ""Use Ansible 2.4""""

This reverts commit 6713b5e9f7b334fef88173a939608a1af6fe5d9e.

With the facility [1] to add parameters (like exclusions) to the
ansible-lint test, and the implementation of an appropriate exclusion
[2] to the integrated repo, we can move forward with updating both
ansible and ansible-lint here.

[1] https://review.openstack.org/527195
[2] https://review.openstack.org/527198

Depends-On: If5746d35ee1b8ce5d6fd1a14a2abca16e29cb899
Depends-On: Icaa997a37d9e31c70e952a80a3f75050965d7ef5
Depends-On: I774829efd763da2400062574bc8266e544d6f75d
Change-Id: I8c3dce39c17175edfb7c37dc0de3e9b9d1d0babb
",git fetch https://review.opendev.org/openstack/openstack-ansible-tests refs/changes/00/527200/1 && git format-patch -1 --stdout FETCH_HEAD,['test-ansible-deps.txt'],1,c92070bb5e14a26b2503462ada1062eb5d0d891d,bug/1737310,ansible==2.4.2.0ansible-lint==3.4.18,ansible==2.3.2.0ansible-lint==3.4.12,2,2
openstack%2Fdragonflow~master~Ia9bd70f2cf680e24d0bc02bdfa2aa615fea4b440,openstack/dragonflow,master,Ia9bd70f2cf680e24d0bc02bdfa2aa615fea4b440,Re-add a test that was removed from the gate,MERGED,2017-12-13 08:06:42.000000000,2017-12-17 19:18:09.000000000,2017-12-17 19:18:09.000000000,"[{'_account_id': 17880}, {'_account_id': 22348}, {'_account_id': 23235}, {'_account_id': 23766}, {'_account_id': 26131}]","[{'number': 1, 'created': '2017-12-13 08:06:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/dragonflow/commit/2ecb12a90b94cf4cdd260f8428071c079e235687', 'message': 'Re-add test that was removed to the gate\n\nAfter resolving the bug, this patch should re-add the test that was\nremoved from the gate.\n\nChange-Id: Ia9bd70f2cf680e24d0bc02bdfa2aa615fea4b440\nCloses-Bug: #1737889\n'}, {'number': 2, 'created': '2017-12-13 14:38:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/dragonflow/commit/e0e88ff6bceda5ec8137e4c70748b18440739a07', 'message': '[WIP][DNM]Re-add test that was removed to the gate\n\nAfter resolving the bug, this patch should re-add the test that was\nremoved from the gate.\n\nChange-Id: Ia9bd70f2cf680e24d0bc02bdfa2aa615fea4b440\nCloses-Bug: #1737889\n'}, {'number': 3, 'created': '2017-12-17 05:53:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/dragonflow/commit/e8cc59d0694d58988d7f9ada6f719021bd905759', 'message': 'Re-add a test that was removed from the gate\n\nIn some cases the dragonflow-dsvm-fullstack-etcd-zmq failed on the\ntest_reconnect_of_controller check. This was because we did not allow\nsufficient time for the controller to reconnect if the host was very\nbusy and/or the VM was slow.\nAllowing double the time resolved the issue.\n\nChange-Id: Ia9bd70f2cf680e24d0bc02bdfa2aa615fea4b440\nCloses-Bug: #1737889\n'}, {'number': 4, 'created': '2017-12-17 11:51:46.000000000', 'files': ['dragonflow/tests/fullstack/test_apps.py'], 'web_link': 'https://opendev.org/openstack/dragonflow/commit/3079462810213917ea6b4730df7c4c1dcb21981b', 'message': 'Re-add a test that was removed from the gate\n\nIn some cases the dragonflow-dsvm-fullstack-etcd-zmq failed on the\ntest_reconnect_of_controller check. This was because we did not allow\nsufficient time for the controller to reconnect if the host was very\nbusy and/or the VM was slow.\nAllowing double the time resolved the issue.\n\nChange-Id: Ia9bd70f2cf680e24d0bc02bdfa2aa615fea4b440\nCloses-Bug: #1737889\n'}]",2,527625,3079462810213917ea6b4730df7c4c1dcb21981b,24,5,4,17880,,,0,"Re-add a test that was removed from the gate

In some cases the dragonflow-dsvm-fullstack-etcd-zmq failed on the
test_reconnect_of_controller check. This was because we did not allow
sufficient time for the controller to reconnect if the host was very
busy and/or the VM was slow.
Allowing double the time resolved the issue.

Change-Id: Ia9bd70f2cf680e24d0bc02bdfa2aa615fea4b440
Closes-Bug: #1737889
",git fetch https://review.opendev.org/openstack/dragonflow refs/changes/25/527625/4 && git format-patch -1 --stdout FETCH_HEAD,['dragonflow/tests/fullstack/test_apps.py'],1,2ecb12a90b94cf4cdd260f8428071c079e235687,bug/1737889,," @testtools.skip(""bug/1737889"")",0,1
openstack%2Fvitrage~master~Iea1a9b16599cc4a9a014df27ea805c4f05fe8ff1,openstack/vitrage,master,Iea1a9b16599cc4a9a014df27ea805c4f05fe8ff1,Updated from global requirements,MERGED,2017-12-15 22:23:22.000000000,2017-12-17 18:05:26.000000000,2017-12-17 18:05:26.000000000,"[{'_account_id': 19134}, {'_account_id': 19184}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 22:23:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/86cd44d222f1fda5a3c5cf03ecda82c83a6f098b', 'message': 'Updated from global requirements\n\nChange-Id: Iea1a9b16599cc4a9a014df27ea805c4f05fe8ff1\n'}, {'number': 2, 'created': '2017-12-17 11:34:38.000000000', 'files': ['requirements.txt', 'test-requirements.txt'], 'web_link': 'https://opendev.org/openstack/vitrage/commit/2f044275bde07aa7db3d4b1f522a6512b3a81ef2', 'message': 'Updated from global requirements\n\nChange-Id: Iea1a9b16599cc4a9a014df27ea805c4f05fe8ff1\n'}]",0,528431,2f044275bde07aa7db3d4b1f522a6512b3a81ef2,17,3,2,11131,,,0,"Updated from global requirements

Change-Id: Iea1a9b16599cc4a9a014df27ea805c4f05fe8ff1
",git fetch https://review.opendev.org/openstack/vitrage refs/changes/31/528431/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'test-requirements.txt']",2,86cd44d222f1fda5a3c5cf03ecda82c83a6f098b,openstack/requirements,"oslo.service!=1.28.1,>=1.24.0 # Apache-2.0",oslo.service>=1.24.0 # Apache-2.0,2,2
openstack%2Fkolla~stable%2Fpike~I2d0a3422ba6a077a309c1fe967a2a62ef5ce6a36,openstack/kolla,stable/pike,I2d0a3422ba6a077a309c1fe967a2a62ef5ce6a36,Creates directory /var/run/mysqld on aarch64 when building MariaDB docker image,MERGED,2017-12-05 15:36:11.000000000,2017-12-17 17:32:23.000000000,2017-12-17 17:32:23.000000000,"[{'_account_id': 1390}, {'_account_id': 2834}, {'_account_id': 7488}, {'_account_id': 10787}, {'_account_id': 11869}, {'_account_id': 17261}, {'_account_id': 19316}, {'_account_id': 22076}, {'_account_id': 22348}, {'_account_id': 22582}, {'_account_id': 23717}, {'_account_id': 26576}]","[{'number': 1, 'created': '2017-12-05 15:36:11.000000000', 'files': ['docker/mariadb/Dockerfile.j2'], 'web_link': 'https://opendev.org/openstack/kolla/commit/229dccd17ee17ca13b42b48296a6fb76663a0bf1', 'message': 'Creates directory /var/run/mysqld on aarch64 when building MariaDB docker image\n\nCloses-bug: #1733262\n\nChange-Id: I2d0a3422ba6a077a309c1fe967a2a62ef5ce6a36\nSigned-off-by: Kevin Zhao <kevin.zhao@arm.com>\n'}]",0,525663,229dccd17ee17ca13b42b48296a6fb76663a0bf1,11,12,1,24072,,,0,"Creates directory /var/run/mysqld on aarch64 when building MariaDB docker image

Closes-bug: #1733262

Change-Id: I2d0a3422ba6a077a309c1fe967a2a62ef5ce6a36
Signed-off-by: Kevin Zhao <kevin.zhao@arm.com>
",git fetch https://review.opendev.org/openstack/kolla refs/changes/63/525663/1 && git format-patch -1 --stdout FETCH_HEAD,['docker/mariadb/Dockerfile.j2'],1,229dccd17ee17ca13b42b48296a6fb76663a0bf1,bug/1733262,"{% if base_distro in ['debian', 'ubuntu'] %} {% if base_arch == ""aarch64"" %} RUN mkdir -p /var/run/mysqld && chown mysql /var/run/mysqld && chmod 755 /var/run/mysqld {% endif %} {% endif %}",,7,0
openstack%2Fcinder~master~Icc2c7b411bb87f5897582bf09506c3cd3551d30a,openstack/cinder,master,Icc2c7b411bb87f5897582bf09506c3cd3551d30a,Tempest plugin: add skip check for volume revert feature,ABANDONED,2017-12-16 09:00:51.000000000,2017-12-17 15:44:15.000000000,,"[{'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10622}, {'_account_id': 11904}, {'_account_id': 12032}, {'_account_id': 13144}, {'_account_id': 14384}, {'_account_id': 15941}, {'_account_id': 21863}, {'_account_id': 22165}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 24230}, {'_account_id': 24236}, {'_account_id': 24502}, {'_account_id': 25243}, {'_account_id': 25678}, {'_account_id': 26537}]","[{'number': 1, 'created': '2017-12-16 09:00:51.000000000', 'files': ['cinder/tests/tempest/opts.py', 'cinder/tests/tempest/api/volume/test_volume_revert.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/13156fad0e675fc859e22fe16cbd0a0b7a0c5ae3', 'message': 'Tempest plugin: add skip check for volume revert feature\n\nVolume revert is a new feature added in microversion 3.40, and now not all\nstorage backend drivers have supported this feature. So it is necessary to\nadd a skip check for it in Tempest tests.\n\nChange-Id: Icc2c7b411bb87f5897582bf09506c3cd3551d30a\n'}]",0,528460,13156fad0e675fc859e22fe16cbd0a0b7a0c5ae3,21,19,1,23081,,,0,"Tempest plugin: add skip check for volume revert feature

Volume revert is a new feature added in microversion 3.40, and now not all
storage backend drivers have supported this feature. So it is necessary to
add a skip check for it in Tempest tests.

Change-Id: Icc2c7b411bb87f5897582bf09506c3cd3551d30a
",git fetch https://review.opendev.org/openstack/cinder refs/changes/60/528460/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/tempest/opts.py', 'cinder/tests/tempest/api/volume/test_volume_revert.py']",2,13156fad0e675fc859e22fe16cbd0a0b7a0c5ae3,tempest_volume_revert_config," def skip_checks(cls): super(VolumeRevertTests, cls).skip_checks() if not CONF.volume_feature_enabled.volume_revert: raise cls.skipException(""Cinder volume revert feature disabled"") @classmethod",,9,0
openstack%2Fvitrage~master~I4c3afd735a8301c215cfc50f970ecde6b2a04c6e,openstack/vitrage,master,I4c3afd735a8301c215cfc50f970ecde6b2a04c6e,Add snmp_parsing service,MERGED,2017-12-06 10:04:53.000000000,2017-12-17 14:59:59.000000000,2017-12-17 14:59:59.000000000,"[{'_account_id': 18783}, {'_account_id': 19134}, {'_account_id': 19159}, {'_account_id': 21414}, {'_account_id': 22348}, {'_account_id': 25301}, {'_account_id': 26091}]","[{'number': 1, 'created': '2017-12-06 10:04:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/87b3dcc9b2dc89982920bdf3a41153619c9ba27c', 'message': 'Add snmp_parsing service\n\nParsing part and devstack deploy part will be added\nlator, with unit tests and tempest tests.\n\nChange-Id: I4c3afd735a8301c215cfc50f970ecde6b2a04c6e\nSigned-off-by: xupeipei <xu.peipei1@zte.com.cn>\n'}, {'number': 2, 'created': '2017-12-07 03:39:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/19f4f3d777b4da7a986917150d32100b3e5b76dc', 'message': 'Add snmp_parsing service\n\nParsing part and devstack deploy part will be added\nlater, with unit tests and tempest tests.\n\nImplements: blueprint snmp-support\nChange-Id: I4c3afd735a8301c215cfc50f970ecde6b2a04c6e\nSigned-off-by: xupeipei <xu.peipei1@zte.com.cn>\n'}, {'number': 3, 'created': '2017-12-12 07:19:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/b38638eb2f32287560419e44214c8cef30c75230', 'message': 'Add snmp_parsing service\n\nParsing part and devstack deploy part will be added\nlater, with unit tests and tempest tests.\n\nImplements: blueprint snmp-support\nChange-Id: I4c3afd735a8301c215cfc50f970ecde6b2a04c6e\nSigned-off-by: xupeipei <xu.peipei1@zte.com.cn>\n'}, {'number': 4, 'created': '2017-12-12 09:12:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/6b186dc759de13c777f0fcdb4e58e115e61424d4', 'message': 'Add snmp_parsing service\n\nParsing part and devstack deploy part will be added\nlater, with unit tests and tempest tests.\n\nImplements: blueprint snmp-support\nChange-Id: I4c3afd735a8301c215cfc50f970ecde6b2a04c6e\nSigned-off-by: xupeipei <xu.peipei1@zte.com.cn>\n'}, {'number': 5, 'created': '2017-12-13 01:21:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/608f02662c3c50e1ccf8521c00774017e93a4924', 'message': 'Add snmp_parsing service\n\nParsing part and devstack deploy part will be added\nlater, with unit tests and tempest tests.\n\nImplements: blueprint snmp-support\nChange-Id: I4c3afd735a8301c215cfc50f970ecde6b2a04c6e\nSigned-off-by: xupeipei <xu.peipei1@zte.com.cn>\n'}, {'number': 6, 'created': '2017-12-13 09:06:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/29cf6c3951cf81228f0f07b8f5be64891e11a6c8', 'message': 'Add snmp_parsing service\n\nParsing part and devstack deploy part will be added\nlater, with unit tests and tempest tests.\n\nImplements: blueprint snmp-support\nChange-Id: I4c3afd735a8301c215cfc50f970ecde6b2a04c6e\nSigned-off-by: xupeipei <xu.peipei1@zte.com.cn>\n'}, {'number': 7, 'created': '2017-12-14 08:20:49.000000000', 'files': ['vitrage/snmp_parsing/__init__.py', 'vitrage/cli/snmp_parsing.py', 'setup.cfg', 'vitrage/opts.py', 'vitrage/snmp_parsing/service.py'], 'web_link': 'https://opendev.org/openstack/vitrage/commit/90778cf90317ed801a2a04af48f38d1d663c18cc', 'message': 'Add snmp_parsing service\n\nParsing part and devstack deploy part will be added\nlater, with unit tests and tempest tests.\n\nImplements: blueprint snmp-support\nChange-Id: I4c3afd735a8301c215cfc50f970ecde6b2a04c6e\nSigned-off-by: xupeipei <xu.peipei1@zte.com.cn>\n'}]",16,526014,90778cf90317ed801a2a04af48f38d1d663c18cc,52,7,7,25301,,,0,"Add snmp_parsing service

Parsing part and devstack deploy part will be added
later, with unit tests and tempest tests.

Implements: blueprint snmp-support
Change-Id: I4c3afd735a8301c215cfc50f970ecde6b2a04c6e
Signed-off-by: xupeipei <xu.peipei1@zte.com.cn>
",git fetch https://review.opendev.org/openstack/vitrage refs/changes/14/526014/2 && git format-patch -1 --stdout FETCH_HEAD,"['vitrage/snmp_parsing/__init__.py', 'vitrage/cli/snmp_parsing.py', 'setup.cfg', 'vitrage/opts.py', 'vitrage/snmp_parsing/service.py']",5,87b3dcc9b2dc89982920bdf3a41153619c9ba27c,,"# Copyright 2017 - ZTE # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from pysnmp.carrier.asyncore.dgram import udp from pysnmp.carrier.asyncore.dgram import udp6 from pysnmp.carrier.asyncore.dispatch import AsyncoreDispatcher from oslo_log import log from oslo_service import service as os_service LOG = log.getLogger(__name__) class SnmpParsingService(os_service.Service): def __init__(self, conf=None): super(SnmpParsingService, self).__init__() self.conf = conf self.listening_port = conf.snmp_parsing.snmp_listening_port def start(self): LOG.info(""Vitrage Snmp_parsing Service - Starting..."") super(SnmpParsingService, self).start() transportDispatcher = AsyncoreDispatcher() transportDispatcher.registerRecvCbFun(self.cbFun) trans = udp.UdpSocketTransport() udp_transport = trans.openServerMode(('0.0.0.0', self.listening_port)) trans = udp6.Udp6SocketTransport() udp6_transport = trans.openServerMode(('::1', self.listening_port)) transportDispatcher.registerTransport(udp.domainName, udp_transport) transportDispatcher.registerTransport(udp6.domainName, udp6_transport) LOG.info(""Vitrage Snmp_parsing Service - Started!"") transportDispatcher.jobStarted(1) try: transportDispatcher.runDispatcher() except Exception: transportDispatcher.closeDispatcher() raise def stop(self, graceful=False): LOG.info(""Vitrage Snmp_parsing Service - Stopping..."") super(SnmpParsingService, self).stop(graceful) LOG.info(""Vitrage Snmp_parsing Service - Stopped!"") # noinspection PyUnusedLocal def cbFun(self, transportDispatcher, transportDomain, transportAddress, wholeMsg): # TODO(peipei): need to parse wholeMsg and send to message queue pass ",,125,0
openstack%2Fvitrage~master~I09468f503ab65b4ec7d995db42a61ab147e2be1f,openstack/vitrage,master,I09468f503ab65b4ec7d995db42a61ab147e2be1f,load manager tool fix,MERGED,2017-12-13 06:55:20.000000000,2017-12-17 14:58:11.000000000,2017-12-17 14:58:11.000000000,"[{'_account_id': 3}, {'_account_id': 18783}, {'_account_id': 19134}, {'_account_id': 19159}, {'_account_id': 22348}, {'_account_id': 26091}]","[{'number': 1, 'created': '2017-12-13 06:55:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/ddad050c521185d1814998f41d250108fa327050', 'message': 'load manager tool fix\n\nChange-Id: I09468f503ab65b4ec7d995db42a61ab147e2be1f\n'}, {'number': 2, 'created': '2017-12-13 14:05:24.000000000', 'files': ['tools/load_generator/load_generator.py'], 'web_link': 'https://opendev.org/openstack/vitrage/commit/6dc1e3aa0d3cc0c0c38a81f7faf3f5bbd76a22b0', 'message': 'load manager tool fix\n\nDepends-On: I428d04598910edfe67e8b8deb608bcf1233d672d\nChange-Id: I09468f503ab65b4ec7d995db42a61ab147e2be1f\n'}]",0,527602,6dc1e3aa0d3cc0c0c38a81f7faf3f5bbd76a22b0,23,6,2,19184,,,0,"load manager tool fix

Depends-On: I428d04598910edfe67e8b8deb608bcf1233d672d
Change-Id: I09468f503ab65b4ec7d995db42a61ab147e2be1f
",git fetch https://review.opendev.org/openstack/vitrage refs/changes/02/527602/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/load_generator/load_generator.py'],1,ddad050c521185d1814998f41d250108fa327050,load_generator_tool_fixed, topics = conf.datasources.notification_topics topics=topics), topic = conf.datasources.notification_topic topics=[topic]),2,2
openstack%2Fvitrage~master~I65dc70c3fa2fd7083bbabf6af47907b01fb93ebb,openstack/vitrage,master,I65dc70c3fa2fd7083bbabf6af47907b01fb93ebb,fix error message when resource not found,MERGED,2017-12-13 08:19:00.000000000,2017-12-17 14:58:10.000000000,2017-12-17 14:58:10.000000000,"[{'_account_id': 3}, {'_account_id': 18783}, {'_account_id': 19134}, {'_account_id': 19159}, {'_account_id': 19184}, {'_account_id': 22348}, {'_account_id': 26091}]","[{'number': 1, 'created': '2017-12-13 08:19:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/d1ff9194b799aea33d907e6508d36322bbd126cc', 'message': 'fix error message when resource not found\n\nChange-Id: I65dc70c3fa2fd7083bbabf6af47907b01fb93ebb\n'}, {'number': 2, 'created': '2017-12-13 08:33:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/5b51a52f98587eb13f47921825687cc6eee0069d', 'message': 'fix error message when resource not found\n\nChange-Id: I65dc70c3fa2fd7083bbabf6af47907b01fb93ebb\n'}, {'number': 3, 'created': '2017-12-13 09:52:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/c7e8d6f538860555774a969b8a1a46f3467d1035', 'message': 'fix error message when resource not found\n\nDepends-On: I4b2d12d9be01fa8243ceab9b6457eb4e2e99331e\nChange-Id: I65dc70c3fa2fd7083bbabf6af47907b01fb93ebb\n'}, {'number': 4, 'created': '2017-12-13 13:38:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/796c5e7a6ecaa7371355b2cc6a67e386286af406', 'message': 'fix error message when resource not found\n\nDepends-On:  I428d04598910edfe67e8b8deb608bcf1233d672d\nChange-Id: I65dc70c3fa2fd7083bbabf6af47907b01fb93ebb\n'}, {'number': 5, 'created': '2017-12-13 13:40:47.000000000', 'files': ['vitrage/api/controllers/v1/resource.py'], 'web_link': 'https://opendev.org/openstack/vitrage/commit/dbedd6b6513605ca3ec1313387eb871e1c5046af', 'message': 'fix error message when resource not found\n\nDepends-On: I428d04598910edfe67e8b8deb608bcf1233d672d\nChange-Id: I65dc70c3fa2fd7083bbabf6af47907b01fb93ebb\n'}]",1,527628,dbedd6b6513605ca3ec1313387eb871e1c5046af,29,7,5,19134,,,0,"fix error message when resource not found

Depends-On: I428d04598910edfe67e8b8deb608bcf1233d672d
Change-Id: I65dc70c3fa2fd7083bbabf6af47907b01fb93ebb
",git fetch https://review.opendev.org/openstack/vitrage refs/changes/28/527628/1 && git format-patch -1 --stdout FETCH_HEAD,['vitrage/api/controllers/v1/resource.py'],1,d1ff9194b799aea33d907e6508d36322bbd126cc,eyalb/resource," return self._show_resource(vitrage_id) LOG.info('resource found = %s', resource) if not resource: abort(404, ""Failed to find resource %s"" % vitrage_id) "," try: return self._show_resource(vitrage_id) except Exception as e: LOG.exception('failed to show resource %s, %s' % vitrage_id, e) abort(404, str(e)) LOG.info(resource)",5,6
openstack%2Ftripleo-quickstart~master~I35309ec5c1abe7eecb67e675bd3a4ad62719243c,openstack/tripleo-quickstart,master,I35309ec5c1abe7eecb67e675bd3a4ad62719243c,Enable tripleo-validations runs in featureset010,MERGED,2017-12-05 14:57:28.000000000,2017-12-17 14:09:38.000000000,2017-12-17 14:09:37.000000000,"[{'_account_id': 3153}, {'_account_id': 6926}, {'_account_id': 9317}, {'_account_id': 10969}, {'_account_id': 11491}, {'_account_id': 17888}, {'_account_id': 18846}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 24752}]","[{'number': 1, 'created': '2017-12-05 14:57:28.000000000', 'files': ['config/general_config/featureset010.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart/commit/9a71cc81763d7e90c16f533581ab5f8482b40b7c', 'message': 'Enable tripleo-validations runs in featureset010\n\nChange-Id: I35309ec5c1abe7eecb67e675bd3a4ad62719243c\nSigned-off-by: Gael Chamoulaud <gchamoul@redhat.com>\n'}]",4,525637,9a71cc81763d7e90c16f533581ab5f8482b40b7c,25,10,1,11491,,,0,"Enable tripleo-validations runs in featureset010

Change-Id: I35309ec5c1abe7eecb67e675bd3a4ad62719243c
Signed-off-by: Gael Chamoulaud <gchamoul@redhat.com>
",git fetch https://review.opendev.org/openstack/tripleo-quickstart refs/changes/37/525637/1 && git format-patch -1 --stdout FETCH_HEAD,['config/general_config/featureset010.yml'],1,9a71cc81763d7e90c16f533581ab5f8482b40b7c,tripleo-validations,# This enables the run of several tripleo-validations tests through Mistral run_tripleo_validations: True # This enables the run of tripleo-validations negative tests through shell # scripts run_tripleo_validations_negative_tests: True # Exit tripleo-quickstart on validations failure exit_on_validations_failure: False ,,8,0
openstack%2Fvitrage~master~Iac44437bb3b1a55e002c421cc72cdb6ddddbd057,openstack/vitrage,master,Iac44437bb3b1a55e002c421cc72cdb6ddddbd057,Update template validation status codes doc,MERGED,2017-12-07 10:53:19.000000000,2017-12-17 13:49:33.000000000,2017-12-17 13:49:33.000000000,"[{'_account_id': 19134}, {'_account_id': 19159}, {'_account_id': 21414}, {'_account_id': 22348}, {'_account_id': 26091}]","[{'number': 1, 'created': '2017-12-07 10:53:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/fe5c4f9095b0b11119ed834d819116164aa9f3eb', 'message': 'Update template validation status codes doc\n\nUpdate the documentation according to status_messages.py\n\nChange-Id: Iac44437bb3b1a55e002c421cc72cdb6ddddbd057\n'}, {'number': 2, 'created': '2017-12-10 07:39:41.000000000', 'files': ['vitrage/evaluator/template_validation/status_messages.py', 'doc/source/contributor/template_validation_status_code.rst'], 'web_link': 'https://opendev.org/openstack/vitrage/commit/87d5a2bccd19c5590bc04ac331366cdcc67e2ba7', 'message': 'Update template validation status codes doc\n\nUpdate the documentation according to status_messages.py\n\nChange-Id: Iac44437bb3b1a55e002c421cc72cdb6ddddbd057\n'}]",2,526336,87d5a2bccd19c5590bc04ac331366cdcc67e2ba7,20,5,2,19159,,,0,"Update template validation status codes doc

Update the documentation according to status_messages.py

Change-Id: Iac44437bb3b1a55e002c421cc72cdb6ddddbd057
",git fetch https://review.opendev.org/openstack/vitrage refs/changes/36/526336/2 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/contributor/template_validation_status_code.rst'],1,fe5c4f9095b0b11119ed834d819116164aa9f3eb,update_status_doc,| 5 | Invalid yaml format | syntax | +------------------+---------------------------------------------------------+-------------------------------+| 47 | Invalid regular expression defined in field | content | +------------------+---------------------------------------------------------+-------------------------------+| 63 | Unsupported version. Version must be one of: {versions} | content | +------------------+---------------------------------------------------------+-------------------------------+| 86 | Not operator can be used only on relationships. | content | +------------------+---------------------------------------------------------+-------------------------------+| 140 | At least one template must be included | syntax | +------------------+---------------------------------------------------------+-------------------------------+ | 141 | Name field is unspecified for include | syntax | +------------------+---------------------------------------------------------+-------------------------------+ | 142 | Trying to include a template that does not exist | content | +------------------+---------------------------------------------------------+-------------------------------+ | 143 | Includable cannot have Includes or Scenarios | syntax | +------------------+---------------------------------------------------------+-------------------------------+,,16,0
openstack%2Fvitrage~master~I810da2682f3c1804312b26041b73280c040432a5,openstack/vitrage,master,I810da2682f3c1804312b26041b73280c040432a5,Refactor template loading,MERGED,2017-12-07 09:09:12.000000000,2017-12-17 13:49:33.000000000,2017-12-17 13:49:33.000000000,"[{'_account_id': 19134}, {'_account_id': 19159}, {'_account_id': 21414}, {'_account_id': 22348}, {'_account_id': 26091}, {'_account_id': 26464}]","[{'number': 1, 'created': '2017-12-07 09:09:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/19f9653b5cde435e9efc3ededbb1ee048fb6ba70', 'message': 'Refactor template loading\n\nThe template loading code was extracted from the TemplateData class and placed in separate files, to allow future support of per-version loading (i.e. different loaders for different template versions). No real change was done in the code, it  was just split to separate files.\n\nThe changes:\n* Separate the loading logic from the data\n* Split the template loading to specific loaders: template, scenario and equivalences\n\nChange-Id: I810da2682f3c1804312b26041b73280c040432a5\n'}, {'number': 2, 'created': '2017-12-07 09:39:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/29145bc74ac2b98ae104d097c62b84425ef91d54', 'message': 'Refactor template loading\n\nThe template loading code was extracted from the TemplateData class and placed in separate files, to allow future support of per-version loading (i.e. different loaders for different template versions). No real change was done in the code, it  was just split to separate files.\n\nThe changes:\n* Separate the loading logic from the data\n* Split the template loading to specific loaders: template, scenario and equivalences\n\nChange-Id: I810da2682f3c1804312b26041b73280c040432a5\nImplements: blueprint refactor-execute-mistral-definition\n'}, {'number': 3, 'created': '2017-12-10 07:19:17.000000000', 'files': ['vitrage/evaluator/template_loading/__init__.py', 'vitrage/tests/unit/evaluator/test_condition.py', 'vitrage/evaluator/scenario_repository.py', 'vitrage/evaluator/template_loading/equivalence_loader.py', 'vitrage/evaluator/template_loading/props_converter.py', 'vitrage/evaluator/equivalence_repository.py', 'vitrage/tests/unit/evaluator/test_equivalence_loader.py', 'vitrage/evaluator/template_data.py', 'vitrage/evaluator/template_loading/subgraph_builder.py', 'vitrage/evaluator/condition.py', 'vitrage/evaluator/template_loading/scenario_loader.py', 'vitrage/tests/unit/evaluator/test_template_loader.py', 'vitrage/evaluator/template_loading/template_loader.py'], 'web_link': 'https://opendev.org/openstack/vitrage/commit/fec7f59687b85d13d96afa823671bb7faa77d4e6', 'message': 'Refactor template loading\n\nThe template loading code was extracted from the TemplateData class and\nplaced in separate files, to allow future support of per-version loading\n(i.e. different loaders for different template versions).\nNo real change was done in the code, it  was just split to separate files.\n\nThe changes:\n* Separate the loading logic from the data\n* Split the template loading to specific loaders: template, scenario and equivalences\n\nChange-Id: I810da2682f3c1804312b26041b73280c040432a5\nImplements: blueprint refactor-execute-mistral-definition\n'}]",6,526320,fec7f59687b85d13d96afa823671bb7faa77d4e6,19,6,3,19159,,,0,"Refactor template loading

The template loading code was extracted from the TemplateData class and
placed in separate files, to allow future support of per-version loading
(i.e. different loaders for different template versions).
No real change was done in the code, it  was just split to separate files.

The changes:
* Separate the loading logic from the data
* Split the template loading to specific loaders: template, scenario and equivalences

Change-Id: I810da2682f3c1804312b26041b73280c040432a5
Implements: blueprint refactor-execute-mistral-definition
",git fetch https://review.opendev.org/openstack/vitrage refs/changes/20/526320/1 && git format-patch -1 --stdout FETCH_HEAD,"['vitrage/evaluator/template_loading/__init__.py', 'vitrage/tests/unit/evaluator/test_condition.py', 'vitrage/evaluator/scenario_repository.py', 'vitrage/evaluator/template_loading/equivalence_loader.py', 'vitrage/evaluator/template_loading/props_converter.py', 'vitrage/tests/unit/evaluator/test_equivalence_data.py', 'vitrage/evaluator/equivalence_repository.py', 'vitrage/evaluator/template_data.py', 'vitrage/tests/unit/evaluator/test_template_data.py', 'vitrage/evaluator/template_loading/subgraph_builder.py', 'vitrage/evaluator/condition.py', 'vitrage/evaluator/template_loading/scenario_loader.py', 'vitrage/evaluator/template_loading/template_loader.py']",13,19f9653b5cde435e9efc3ededbb1ee048fb6ba70,bp/refactor-execute-mistral-definition,"# Copyright 2016 - Nokia # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from vitrage.common.constants import VertexProperties as VProps from vitrage.evaluator.template_data import EdgeDescription from vitrage.evaluator.template_data import TemplateData from vitrage.evaluator.template_fields import TemplateFields as TFields from vitrage.evaluator.template_loading.props_converter import PropsConverter from vitrage.evaluator.template_loading.scenario_loader import ScenarioLoader from vitrage.graph import Edge from vitrage.graph import Vertex from vitrage.utils import evaluator as evaluator_utils class TemplateLoader(object): PROPS_CONVERSION = { 'category': VProps.VITRAGE_CATEGORY, 'type': VProps.VITRAGE_TYPE, 'resource_id': VProps.VITRAGE_RESOURCE_ID, 'sample_timestamp': VProps.VITRAGE_SAMPLE_TIMESTAMP, 'is_deleted': VProps.VITRAGE_IS_DELETED, 'is_placeholder': VProps.VITRAGE_IS_PLACEHOLDER, 'aggregated_state': VProps.VITRAGE_AGGREGATED_STATE, 'operational_state': VProps.VITRAGE_OPERATIONAL_STATE, 'aggregated_severity': VProps.VITRAGE_AGGREGATED_SEVERITY, 'operational_severity': VProps.VITRAGE_OPERATIONAL_SEVERITY } def __init__(self): self.entities = {} self.relationships = {} def load(self, template_def, def_templates=None): name = template_def[TFields.METADATA][TFields.NAME] if def_templates is None: def_templates = {} defs = {} if TFields.DEFINITIONS in template_def: defs = template_def[TFields.DEFINITIONS] if TFields.ENTITIES in defs: self.entities = self._build_entities(defs[TFields.ENTITIES]) # Add definitions from template then from definition templates. if TFields.INCLUDES in template_def: includes = template_def[TFields.INCLUDES] self._build_entities_from_def_templates( includes, def_templates, self.entities) if TFields.RELATIONSHIPS in defs: self.relationships = self._build_relationships( defs[TFields.RELATIONSHIPS]) if TFields.INCLUDES in template_def: includes = template_def[TFields.INCLUDES] self._build_relationships_with_def_templates(includes, def_templates, self.relationships) scenarios = ScenarioLoader(name, self.entities, self.relationships)\ .build_scenarios(template_def[TFields.SCENARIOS]) return TemplateData(name, self.entities, self.relationships, scenarios) def _build_entities(self, entities_defs): entities = {} for entity_def in entities_defs: entity_dict = entity_def[TFields.ENTITY] template_id = entity_dict[TFields.TEMPLATE_ID] properties = PropsConverter.convert_props_with_dictionary( self._extract_properties(entity_dict)) entities[template_id] = Vertex(template_id, properties) return entities def _build_entities_from_def_templates( self, includes, def_templates, entities): for def_template_dict in includes: name = def_template_dict[TFields.NAME] def_template = evaluator_utils.find_def_template( name, def_templates) defs = def_template[TFields.DEFINITIONS] entities_defs = defs[TFields.ENTITIES] for entity_def in entities_defs: entity_dict = entity_def[TFields.ENTITY] template_id = entity_dict[TFields.TEMPLATE_ID] if template_id not in entities: properties = \ PropsConverter.convert_props_with_dictionary( self._extract_properties(entity_dict)) entities[template_id] = Vertex(template_id, properties) def _build_relationships(self, relationships_defs): relationships = {} for relationship_def in relationships_defs: relationship_dict = relationship_def[TFields.RELATIONSHIP] relationship = self._extract_relationship_info(relationship_dict) template_id = relationship_dict[TFields.TEMPLATE_ID] relationships[template_id] = relationship return relationships def _build_relationships_with_def_templates( self, includes, def_templates, relationships): for def_template_dict in includes: name = def_template_dict[TFields.NAME] def_template = evaluator_utils.find_def_template( name, def_templates) if TFields.RELATIONSHIPS in def_template[TFields.DEFINITIONS]: defs = def_template[TFields.DEFINITIONS] relationship_defs = defs[TFields.RELATIONSHIPS] for relationship_def in relationship_defs: relationship_dict = relationship_def[TFields.RELATIONSHIP] template_id = relationship_dict[TFields.TEMPLATE_ID] if template_id not in relationships: relationship = self._extract_relationship_info( relationship_dict) relationships[template_id] = relationship def _extract_relationship_info(self, relationship_dict): source_id = relationship_dict[TFields.SOURCE] target_id = relationship_dict[TFields.TARGET] edge = Edge(source_id, target_id, relationship_dict[TFields.RELATIONSHIP_TYPE], self._extract_properties(relationship_dict)) source = self.entities[source_id] target = self.entities[target_id] return EdgeDescription(edge, source, target) @staticmethod def _extract_properties(var_dict): ignore_ids = [TFields.TEMPLATE_ID, TFields.SOURCE, TFields.TARGET] return \ {key: var_dict[key] for key in var_dict if key not in ignore_ids} ",,506,402
openstack%2Fhorizon~master~I621fb1072d1753f9b51a6a21e2f7994eb04dd05b,openstack/horizon,master,I621fb1072d1753f9b51a6a21e2f7994eb04dd05b,Add release note on policy dirs support,MERGED,2017-11-28 21:52:35.000000000,2017-12-17 13:36:36.000000000,2017-12-17 13:36:36.000000000,"[{'_account_id': 841}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-11-28 21:52:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/c7305d82e07433291c3850158cc16a80bbb689da', 'message': 'Add release note on policy dirs support\n\nPart of blueprint bp/policy-dirs\n\noslo.policy policy dirs support has been implemented before merging\nopenstack-auth but there is no corresponding release note.\nThis commit adds it.\n\nChange-Id: I621fb1072d1753f9b51a6a21e2f7994eb04dd05b\n'}, {'number': 2, 'created': '2017-11-29 14:26:36.000000000', 'files': ['releasenotes/notes/openstack-auth-policy-dirs-c5d77665eac415ea.yaml'], 'web_link': 'https://opendev.org/openstack/horizon/commit/b541debf3e3f85d6d1d11c36cf1252aa5043d251', 'message': 'Add release note on policy dirs support\n\nPart of blueprint policy-dirs\n\noslo.policy policy dirs support has been implemented before merging\nopenstack-auth but there is no corresponding release note.\nThis commit adds it.\n\nChange-Id: I621fb1072d1753f9b51a6a21e2f7994eb04dd05b\n'}]",0,523542,b541debf3e3f85d6d1d11c36cf1252aa5043d251,9,2,2,841,,,0,"Add release note on policy dirs support

Part of blueprint policy-dirs

oslo.policy policy dirs support has been implemented before merging
openstack-auth but there is no corresponding release note.
This commit adds it.

Change-Id: I621fb1072d1753f9b51a6a21e2f7994eb04dd05b
",git fetch https://review.opendev.org/openstack/horizon refs/changes/42/523542/2 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/notes/openstack-auth-policy-dirs-c5d77665eac415ea.yaml'],1,c7305d82e07433291c3850158cc16a80bbb689da,bp/policy-dirs,"--- features: - | The policy framework in horizon now supports policy directories per service. This corresponds to ``policy_dirs`` configuration option from ""oslo.policy"" library. The new setting ``POLICY_DIRS`` was introduced. The setting allows to define multiple policy directories per service. For example, it is useful for a case where multiple projects provide policy files like neutron stadium projects. For detail, see `the horizon Setting Reference <https://docs.openstack.org/horizon/latest/configuration/settings.html#policy-dirs>`__. ",,11,0
openstack%2Fhorizon~master~Ida5b7aad33c6024d8bdd1ea6d1cb2b35b9fd6ba6,openstack/horizon,master,Ida5b7aad33c6024d8bdd1ea6d1cb2b35b9fd6ba6,Fix warning message not showing on launch instance modal,MERGED,2017-09-04 04:30:03.000000000,2017-12-17 13:36:34.000000000,2017-12-17 13:36:34.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 1736}, {'_account_id': 8313}, {'_account_id': 14151}, {'_account_id': 16352}, {'_account_id': 17645}, {'_account_id': 22348}, {'_account_id': 26304}]","[{'number': 1, 'created': '2017-09-04 04:30:03.000000000', 'files': ['openstack_dashboard/dashboards/project/static/dashboard/project/workflow/launch-instance/source/source.html', 'openstack_dashboard/dashboards/project/static/dashboard/project/workflow/launch-instance/flavor/flavor.html'], 'web_link': 'https://opendev.org/openstack/horizon/commit/9c3d01a77cdbd64843afc81491f39bdfe1293672', 'message': 'Fix warning message not showing on launch instance modal\n\nThe warning message should be shown when hovering over the\nwarning icon on launch instance modal.\n\nUpdated the name and attribute of the popover directive.\n\nChange-Id: Ida5b7aad33c6024d8bdd1ea6d1cb2b35b9fd6ba6\nCloses-bug: #1713591\n'}]",3,500437,9c3d01a77cdbd64843afc81491f39bdfe1293672,17,9,1,11885,,,0,"Fix warning message not showing on launch instance modal

The warning message should be shown when hovering over the
warning icon on launch instance modal.

Updated the name and attribute of the popover directive.

Change-Id: Ida5b7aad33c6024d8bdd1ea6d1cb2b35b9fd6ba6
Closes-bug: #1713591
",git fetch https://review.opendev.org/openstack/horizon refs/changes/37/500437/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/static/dashboard/project/workflow/launch-instance/source/source.html', 'openstack_dashboard/dashboards/project/static/dashboard/project/workflow/launch-instance/flavor/flavor.html']",2,9c3d01a77cdbd64843afc81491f39bdfe1293672,bug/1713591," uib-popover=""{$ item.errors.vcpus $}"" popover-trigger=""'mouseenter'""/> uib-popover=""{$ item.errors.ram $}"" popover-trigger=""'mouseenter'""/> uib-popover=""{$ item.errors.disk $}"" popover-trigger=""'mouseenter'""/> uib-popover=""{$ item.errors.vcpus $}"" popover-trigger=""'mouseenter'""/> uib-popover=""{$ item.errors.ram $}"" popover-trigger=""'mouseenter'""/> uib-popover=""{$ item.errors.disk $}"" popover-trigger=""'mouseenter'""/>"," popover=""{$ item.errors.vcpus $}"" popover-trigger=""mouseenter mouseleave""/> popover=""{$ item.errors.ram $}"" popover-trigger=""mouseenter mouseleave""/> popover=""{$ item.errors.disk $}"" popover-trigger=""mouseenter mouseleave""/> popover=""{$ item.errors.vcpus $}"" popover-trigger=""mouseenter mouseleave""/> popover=""{$ item.errors.ram $}"" popover-trigger=""mouseenter mouseleave""/> popover=""{$ item.errors.disk $}"" popover-trigger=""mouseenter mouseleave""/>",16,16
openstack%2Fhorizon~master~I00715e699b55f351b781aed08a1ce9e1d73c07c6,openstack/horizon,master,I00715e699b55f351b781aed08a1ce9e1d73c07c6,Order the projects and the rules choices when creating a user,ABANDONED,2017-12-17 10:41:37.000000000,2017-12-17 13:05:44.000000000,,"[{'_account_id': 841}, {'_account_id': 22348}, {'_account_id': 26907}]","[{'number': 1, 'created': '2017-12-17 10:41:37.000000000', 'files': ['openstack_dashboard/dashboards/identity/users/forms.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/0eefa907c59f3e413e12d49c925e62642e427ec0', 'message': 'Order the projects and the rules choices when creating a user\n\nWhen creating a user in the identity dashboard, the primary project\nand rule combo options are not ordered. To solve this, sorted function\nwas used to sort the list of projects and rules. To make the sorting\ncase-insensitive it is necessary to lowercasing the list values.\n\nChange-Id: I00715e699b55f351b781aed08a1ce9e1d73c07c6\nCloses-Bug: #1713497\n'}]",0,528538,0eefa907c59f3e413e12d49c925e62642e427ec0,7,3,1,26907,,,0,"Order the projects and the rules choices when creating a user

When creating a user in the identity dashboard, the primary project
and rule combo options are not ordered. To solve this, sorted function
was used to sort the list of projects and rules. To make the sorting
case-insensitive it is necessary to lowercasing the list values.

Change-Id: I00715e699b55f351b781aed08a1ce9e1d73c07c6
Closes-Bug: #1713497
",git fetch https://review.opendev.org/openstack/horizon refs/changes/38/528538/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack_dashboard/dashboards/identity/users/forms.py'],1,0eefa907c59f3e413e12d49c925e62642e427ec0,bug/1713497," for project in sorted(projects, key=lambda p: p.name.lower()): role_choices = [(role.id, role.name) for role in sorted(roles, key=lambda r: r.name.lower())]"," for project in projects: role_choices = [(role.id, role.name) for role in roles]",3,2
openstack%2Fpyeclib~master~I2b9e6e1db27d190e949c4a3e86bb07437ae01436,openstack/pyeclib,master,I2b9e6e1db27d190e949c4a3e86bb07437ae01436,DNM: Test converted job,ABANDONED,2017-12-17 12:47:37.000000000,2017-12-17 12:52:08.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2017-12-17 12:47:37.000000000', 'files': ['pyeclib/core.py'], 'web_link': 'https://opendev.org/openstack/pyeclib/commit/2c216407d16463114c014fffb21f58b240137b52', 'message': 'DNM: Test converted job\n\nTrigger testing of jobs\n\nChange-Id: I2b9e6e1db27d190e949c4a3e86bb07437ae01436\n'}]",0,528549,2c216407d16463114c014fffb21f58b240137b52,3,1,1,6547,,,0,"DNM: Test converted job

Trigger testing of jobs

Change-Id: I2b9e6e1db27d190e949c4a3e86bb07437ae01436
",git fetch https://review.opendev.org/openstack/pyeclib refs/changes/49/528549/1 && git format-patch -1 --stdout FETCH_HEAD,['pyeclib/core.py'],1,2c216407d16463114c014fffb21f58b240137b52,zuulv3-native,# Just for testing,,1,0
openstack%2Fvitrage~master~I125f0f15aebe2a0a09dba4b18312b857ce40b6ad,openstack/vitrage,master,I125f0f15aebe2a0a09dba4b18312b857ce40b6ad,fix changing the policy file of heat,MERGED,2017-12-11 14:10:36.000000000,2017-12-17 12:51:29.000000000,2017-12-17 12:51:29.000000000,"[{'_account_id': 3}, {'_account_id': 18783}, {'_account_id': 19134}, {'_account_id': 19159}, {'_account_id': 21414}, {'_account_id': 22348}, {'_account_id': 26091}]","[{'number': 1, 'created': '2017-12-11 14:10:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/60e036a83bf67b127be832f6291ec14ab1154eeb', 'message': 'fix changing the policy file of heat\n\nheat removed the stacks global index rule\nwhich defaults to deny everybody\nvitrage needs to change it\nfix it so it will replace or add the rule\ninstead of just replacing the old rule\n\nChange-Id: I125f0f15aebe2a0a09dba4b18312b857ce40b6ad\n'}, {'number': 2, 'created': '2017-12-15 16:55:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/ab7bbf4d71d0347c53934f9c3142f9dd8e17b8f1', 'message': 'fix changing the policy file of heat\n\nheat removed the stacks global index rule\nwhich defaults to deny everybody\nvitrage needs to change it\nfix it so it will replace or add the rule\ninstead of just replacing the old rule\n\nDepends-On: I16f75fb75ee01f575b252ed2418a55d7e29357ea\nChange-Id: I125f0f15aebe2a0a09dba4b18312b857ce40b6ad\n'}, {'number': 3, 'created': '2017-12-15 19:40:14.000000000', 'files': ['devstack/plugin.sh'], 'web_link': 'https://opendev.org/openstack/vitrage/commit/0454e6b42a4bc6a5fe37c7f147517fd8cb4569b4', 'message': 'fix changing the policy file of heat\n\nheat removed the stacks global index rule\nwhich defaults to deny everybody\nvitrage needs to change it\nfix it so it will replace or add the rule\ninstead of just replacing the old rule\n\nDepends-On: I428d04598910edfe67e8b8deb608bcf1233d672d\nChange-Id: I125f0f15aebe2a0a09dba4b18312b857ce40b6ad\n'}]",0,527098,0454e6b42a4bc6a5fe37c7f147517fd8cb4569b4,19,7,3,19134,,,0,"fix changing the policy file of heat

heat removed the stacks global index rule
which defaults to deny everybody
vitrage needs to change it
fix it so it will replace or add the rule
instead of just replacing the old rule

Depends-On: I428d04598910edfe67e8b8deb608bcf1233d672d
Change-Id: I125f0f15aebe2a0a09dba4b18312b857ce40b6ad
",git fetch https://review.opendev.org/openstack/vitrage refs/changes/98/527098/3 && git format-patch -1 --stdout FETCH_HEAD,['devstack/plugin.sh'],1,60e036a83bf67b127be832f6291ec14ab1154eeb,eyalb/heat," local policy_file=$HEAT_CONF_DIR/policy.json local rule_to_change='""stacks:global_index"": ""rule:deny_everybody""' local rule_to_add='""stacks:global_index"": ""rule:deny_stack_user""' # replace only if exists deny_everybody if grep -q ""$rule_to_change"" $policy_file; then sed -i ""s/$rule_to_change/$rule_to_add/"" $policy_file # add only if not exists deny_stack_user elif ! grep -q ""$rule_to_add"" $policy_file; then sed -i ""/}/i\\ \\ \\ ,$rule_to_add"" $policy_file fi"," local policy_file=$HEAT_CONF_DIR/policy.json sed -i 's/""stacks:global_index"": ""rule:deny_everybody""/""stacks:global_index"": ""rule:deny_stack_user""/' $policy_file",11,2
openstack%2Fpython-magnumclient~master~I4398f5d08572daf77739df33bff6ad5de6fe11d6,openstack/python-magnumclient,master,I4398f5d08572daf77739df33bff6ad5de6fe11d6,Updated from global requirements,MERGED,2017-11-12 17:47:22.000000000,2017-12-17 12:45:03.000000000,2017-12-17 12:45:03.000000000,"[{'_account_id': 13861}, {'_account_id': 20498}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-11-12 17:47:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/fdaa5d74a3e72a30f090f134a2c67372c642ae49', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 2, 'created': '2017-11-12 21:17:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/522620a3c33e7ac73d5d693757da0729d6565da7', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 3, 'created': '2017-11-13 10:27:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/3595a68af0934c3ee8a11f15e67fe7d08cc22b5f', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 4, 'created': '2017-11-13 21:18:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/ded6e73fdbad7ecd0237f3ecdcdc0200264ec989', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 5, 'created': '2017-11-13 21:18:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/3e8f33c84d2f607d6c03381d411fc774e3716167', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 6, 'created': '2017-11-13 21:26:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/d030a98c561354f34a758a9b1dbed08ff3dd6b4a', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 7, 'created': '2017-11-13 21:28:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/6df39e0d85be2fc6a13bc8bceeed06806464f8fe', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 8, 'created': '2017-11-13 21:47:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/c23fe03f5ec147b476d4b5d0553aab27170586b5', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 9, 'created': '2017-11-14 04:18:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/9022165aa819b7a90e7791b10288b8c019781fa8', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 10, 'created': '2017-11-14 19:06:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/d9ba6db047c8cfe65fa43079a0bd528015491fc0', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 11, 'created': '2017-11-14 19:27:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/af3cbecc74bdc2a034a78d04d4ead27636d736a9', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 12, 'created': '2017-11-14 22:35:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/10563cbe944f42561e12047ddc0d69764c127f39', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 13, 'created': '2017-11-14 22:48:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/78c05ef86411854b4770210afee845b50b71335a', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 14, 'created': '2017-11-15 00:41:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/db66253d84459bf70522e5dacf3b2f5a4750919c', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 15, 'created': '2017-11-15 01:52:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/69d2facd788162f75f50ddccca59322ece853702', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 16, 'created': '2017-11-15 12:01:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/62a3e8387094b8b640b26f3c2ca8294e412d1c99', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 17, 'created': '2017-11-15 18:28:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/d9535bbe4ed64ce747fe1f86a10d9d1949d97ba3', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 18, 'created': '2017-11-16 11:24:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/056ade257f7f6e5c0f89c9b5ccb1562b7080c4ed', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}, {'number': 19, 'created': '2017-12-05 03:32:32.000000000', 'files': ['requirements.txt', 'test-requirements.txt'], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/10c9fd1a3afd16ca80f72c044768503dd39029db', 'message': 'Updated from global requirements\n\nChange-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6\n'}]",0,519158,10c9fd1a3afd16ca80f72c044768503dd39029db,36,3,19,11131,,,0,"Updated from global requirements

Change-Id: I4398f5d08572daf77739df33bff6ad5de6fe11d6
",git fetch https://review.opendev.org/openstack/python-magnumclient refs/changes/58/519158/16 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,fdaa5d74a3e72a30f090f134a2c67372c642ae49,openstack/requirements,oslo.utils>=3.31.0 # Apache-2.0,oslo.utils>=3.28.0 # Apache-2.0,1,1
openstack%2Fpython-magnumclient~master~Ib00514b66ee2d0a2ee4ddafb5b8c2ff8fa82de77,openstack/python-magnumclient,master,Ib00514b66ee2d0a2ee4ddafb5b8c2ff8fa82de77,OSC: Add --flavor to coe cluster create,MERGED,2017-08-01 14:46:20.000000000,2017-12-17 12:43:19.000000000,2017-12-17 12:43:19.000000000,"[{'_account_id': 3}, {'_account_id': 9995}, {'_account_id': 10206}, {'_account_id': 13861}, {'_account_id': 20498}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-08-01 14:46:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/320c5136e68870e31741f028ba6ae42268fe164d', 'message': 'OSC: Add --flavor to coe cluster create\n\nAdd a flavor override parameter on osc cluster create to override\nthe value present on the cluster template.\n\nPartial-Bug: #1699245\nDepends-On: Ib60c05cce1cf2639ca4740abdd264403033433f9\nChange-Id: Ib00514b66ee2d0a2ee4ddafb5b8c2ff8fa82de77\n'}, {'number': 2, 'created': '2017-08-02 09:03:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/cd1543da90f3aae1a6258e7adec3434105746c8d', 'message': 'OSC: Add --flavor to coe cluster create\n\nAdd a flavor override parameter on osc cluster create to override\nthe value present on the cluster template.\n\nPartial-Bug: #1699245\nDepends-On: Ib60c05cce1cf2639ca4740abdd264403033433f9\nChange-Id: Ib00514b66ee2d0a2ee4ddafb5b8c2ff8fa82de77\n'}, {'number': 3, 'created': '2017-10-12 13:44:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/e2f061aed1312067c9352791b057a2deca4abbcf', 'message': 'OSC: Add --flavor to coe cluster create\n\nAdd a flavor override parameter on osc cluster create to override\nthe value present on the cluster template.\n\nPartial-Bug: #1699245\nDepends-On: Ib60c05cce1cf2639ca4740abdd264403033433f9\nChange-Id: Ib00514b66ee2d0a2ee4ddafb5b8c2ff8fa82de77\n'}, {'number': 4, 'created': '2017-11-01 09:22:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/1e7c07432893466f141480a0915a404fe3c14061', 'message': 'OSC: Add --flavor to coe cluster create\n\nAdd a flavor override parameter on osc cluster create to override\nthe value present on the cluster template.\n\nPartial-Bug: #1699245\nDepends-On: Ib60c05cce1cf2639ca4740abdd264403033433f9\nChange-Id: Ib00514b66ee2d0a2ee4ddafb5b8c2ff8fa82de77\n'}, {'number': 5, 'created': '2017-12-14 13:49:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/98993fcf369c1bea0aedcf368a1ba8cd5caa1b69', 'message': 'OSC: Add --flavor to coe cluster create\n\nAdd a flavor override parameter on osc cluster create to override\nthe value present on the cluster template.\n\nPartial-Bug: #1699245\nDepends-On: Ib60c05cce1cf2639ca4740abdd264403033433f9\nChange-Id: Ib00514b66ee2d0a2ee4ddafb5b8c2ff8fa82de77\n'}, {'number': 6, 'created': '2017-12-15 14:32:29.000000000', 'files': ['magnumclient/tests/osc/unit/v1/test_clusters.py', 'magnumclient/v1/clusters.py', 'magnumclient/osc/v1/clusters.py', 'magnumclient/tests/osc/unit/v1/fakes.py'], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/dd0958860d5df5b7da9be50a294912a47e016087', 'message': 'OSC: Add --flavor to coe cluster create\n\nAdd a flavor override parameter on osc cluster create to override\nthe value present on the cluster template.\n\nPartial-Bug: #1699245\nDepends-On: Ib60c05cce1cf2639ca4740abdd264403033433f9\nChange-Id: Ib00514b66ee2d0a2ee4ddafb5b8c2ff8fa82de77\n'}]",0,489627,dd0958860d5df5b7da9be50a294912a47e016087,29,6,6,9995,,,0,"OSC: Add --flavor to coe cluster create

Add a flavor override parameter on osc cluster create to override
the value present on the cluster template.

Partial-Bug: #1699245
Depends-On: Ib60c05cce1cf2639ca4740abdd264403033433f9
Change-Id: Ib00514b66ee2d0a2ee4ddafb5b8c2ff8fa82de77
",git fetch https://review.opendev.org/openstack/python-magnumclient refs/changes/27/489627/1 && git format-patch -1 --stdout FETCH_HEAD,"['magnumclient/tests/osc/unit/v1/test_clusters.py', 'magnumclient/osc/v1/clusters.py', 'magnumclient/tests/osc/unit/v1/fakes.py']",3,320c5136e68870e31741f028ba6ae42268fe164d,bug/1699245," 'name': 'fake-cluster', 'flavor_id': 'm1.medium',", 'name': 'fake-cluster',12,2
openstack%2Fmagnum~master~Ib60c05cce1cf2639ca4740abdd264403033433f9,openstack/magnum,master,Ib60c05cce1cf2639ca4740abdd264403033433f9,Allow flavor_id on cluster create,MERGED,2017-08-01 07:34:46.000000000,2017-12-17 12:15:25.000000000,2017-12-17 12:15:25.000000000,"[{'_account_id': 3}, {'_account_id': 8871}, {'_account_id': 9995}, {'_account_id': 10206}, {'_account_id': 11869}, {'_account_id': 13861}, {'_account_id': 20498}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-08-01 07:34:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/1805afad10e927c411fca8b8b60292d0e1694470', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 2, 'created': '2017-08-01 09:00:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/1cb0c6f1eba4ae60014d02d47d0bafde327b4a12', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 3, 'created': '2017-08-01 11:14:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/bcdf0bd783bf70c22235e431e844c32a0922a1c4', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 4, 'created': '2017-08-02 09:46:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/b81b7c3933f421b876448bf1f3fdaeb0473765ab', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 5, 'created': '2017-08-21 12:29:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/afbf90284dad94af688180314113202042fc1be1', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 6, 'created': '2017-08-28 13:22:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/3272509734c339017cf5e175616282c9abcd3df4', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 7, 'created': '2017-09-11 08:28:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/dd692bf0eb737166647dd36a587b855df9c5ae37', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 8, 'created': '2017-09-13 11:16:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/7d455042ee8851fad250e28f078b6c7f7e97d44d', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 9, 'created': '2017-09-27 09:37:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/7c9a353967df7b87f56b13e66fe02715354f5403', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 10, 'created': '2017-09-27 15:19:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/92611742e7caf23f6d546018344624a225c6caef', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 11, 'created': '2017-10-02 08:57:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/50c67564b701e3aca4e0c223e1b037d24fb758a0', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 12, 'created': '2017-11-02 13:19:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/28053d619e3ef05e478f923d207da8fe792b3fd8', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 13, 'created': '2017-11-23 11:08:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/7065d6b62ecf5c2659ad94f565cd08021995e7af', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 14, 'created': '2017-12-14 11:26:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/15c70abcc68c5802635f2a0d195e46a801e03c1b', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 15, 'created': '2017-12-15 14:31:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/59d7b4d0765c9ee935332c8bd2d0170783e3b724', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}, {'number': 16, 'created': '2017-12-15 16:32:52.000000000', 'files': ['magnum/tests/unit/conductor/handlers/test_swarm_cluster_conductor.py', 'doc/source/user/index.rst', 'magnum/api/controllers/v1/bay.py', 'magnum/api/attr_validator.py', 'magnum/tests/unit/db/utils.py', 'magnum/drivers/heat/swarm_mode_template_def.py', 'magnum/tests/unit/conductor/handlers/test_k8s_cluster_conductor.py', 'magnum/db/sqlalchemy/alembic/versions/041d9a0f1159_add_flavor_id_to_cluster.py', 'magnum/drivers/heat/k8s_template_def.py', 'api-ref/source/clusters.inc', 'magnum/drivers/heat/swarm_fedora_template_def.py', 'api-ref/source/samples/cluster-create-req.json', 'magnum/api/controllers/v1/cluster.py', 'magnum/tests/unit/api/controllers/v1/test_cluster.py', 'magnum/db/sqlalchemy/models.py', 'magnum/tests/unit/api/test_attr_validator.py', 'magnum/drivers/mesos_ubuntu_v1/template_def.py', 'magnum/objects/cluster.py', 'magnum/tests/unit/objects/test_objects.py', 'magnum/tests/unit/conductor/handlers/test_mesos_cluster_conductor.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/84006f63d721e0e0048a09b79745625f08a42aae', 'message': 'Allow flavor_id on cluster create\n\nAdd flavor_id as an option during cluster create. If not given,\nthe default is taken from the cluster template.\n\nAdd flavor_id in the Cluster object and use that instead\nof the one from ClusterTemplate.\n\nUpdate both magnum and magnum cli documentation to reflect the above changes.\n\nPartial-Bug: #1699245\nChange-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9\n'}]",1,489512,84006f63d721e0e0048a09b79745625f08a42aae,52,8,16,9995,,,0,"Allow flavor_id on cluster create

Add flavor_id as an option during cluster create. If not given,
the default is taken from the cluster template.

Add flavor_id in the Cluster object and use that instead
of the one from ClusterTemplate.

Update both magnum and magnum cli documentation to reflect the above changes.

Partial-Bug: #1699245
Change-Id: Ib60c05cce1cf2639ca4740abdd264403033433f9
",git fetch https://review.opendev.org/openstack/magnum refs/changes/12/489512/16 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/user/index.rst', 'magnum/api/controllers/v1/bay.py', 'magnum/db/sqlalchemy/alembic/versions/041d9a0f1159_.py', 'magnum/drivers/heat/swarm_fedora_template_def.py', 'magnum/api/controllers/v1/cluster.py', 'magnum/tests/unit/api/controllers/v1/test_cluster.py', 'magnum/db/sqlalchemy/models.py', 'magnum/tests/unit/db/utils.py', 'magnum/objects/cluster.py', 'magnum/tests/unit/objects/test_objects.py']",10,1805afad10e927c411fca8b8b60292d0e1694470,bug/1699245," 'Cluster': '1.15-60c917a93a0d3c1619a5b71892e3b784',"," 'Cluster': '1.14-281c582b16291c4f0666371e53975a5c',",74,6
openstack%2Fhorizon~master~Icaad5b3518e52b2b3e0a91467e8a24d5a2f3606d,openstack/horizon,master,Icaad5b3518e52b2b3e0a91467e8a24d5a2f3606d,Imported Translations from Zanata,MERGED,2017-12-17 07:37:13.000000000,2017-12-17 11:46:27.000000000,2017-12-17 11:46:27.000000000,"[{'_account_id': 841}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-17 07:37:13.000000000', 'files': ['openstack_dashboard/locale/en_GB/LC_MESSAGES/djangojs.po', 'releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'], 'web_link': 'https://opendev.org/openstack/horizon/commit/6164fe00796f8149d646f81f05ab2bfefb8f589c', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: Icaad5b3518e52b2b3e0a91467e8a24d5a2f3606d\n'}]",0,528527,6164fe00796f8149d646f81f05ab2bfefb8f589c,6,2,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: Icaad5b3518e52b2b3e0a91467e8a24d5a2f3606d
",git fetch https://review.opendev.org/openstack/horizon refs/changes/27/528527/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/locale/en_GB/LC_MESSAGES/djangojs.po', 'releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po']",2,6164fe00796f8149d646f81f05ab2bfefb8f589c,zanata/translations,"""POT-Creation-Date: 2017-12-16 13:44+0000\n""""PO-Revision-Date: 2017-12-17 02:07+0000\n""msgid ""13.0.0.0b2-24"" msgstr ""13.0.0.0b2-24""","""POT-Creation-Date: 2017-12-15 19:33+0000\n""""PO-Revision-Date: 2017-12-16 12:34+0000\n""msgid ""13.0.0.0b2-20"" msgstr ""13.0.0.0b2-20""",10,7
openstack%2Fvitrage~master~I428d04598910edfe67e8b8deb608bcf1233d672d,openstack/vitrage,master,I428d04598910edfe67e8b8deb608bcf1233d672d,"Revert ""Add database configuration to unit tests""",MERGED,2017-12-13 13:35:45.000000000,2017-12-17 11:33:13.000000000,2017-12-17 11:33:13.000000000,"[{'_account_id': 19134}, {'_account_id': 19159}, {'_account_id': 19184}, {'_account_id': 19194}, {'_account_id': 22348}, {'_account_id': 26091}, {'_account_id': 26095}, {'_account_id': 26339}]","[{'number': 1, 'created': '2017-12-13 13:35:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/64733c8c89821fb27109ffc0aca0daa29af11e3d', 'message': 'Revert ""Add database configuration to unit tests""\n\nThis reverts commit b166be36d54a70ddee2c59fcb8ab411d8cea41db.\n\nChange-Id: I428d04598910edfe67e8b8deb608bcf1233d672d\n'}, {'number': 2, 'created': '2017-12-15 14:52:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/92e9d94232030284278118bde7eec89727663cc6', 'message': 'Revert ""Add database configuration to unit tests""\n\nThis reverts commit b166be36d54a70ddee2c59fcb8ab411d8cea41db.\n\nDepends-On: Icb39a334b82834b592932facf93be0687563c316\nChange-Id: I428d04598910edfe67e8b8deb608bcf1233d672d\n'}, {'number': 3, 'created': '2017-12-15 17:10:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/01765eca76b4bd85d922e70291e287400ba598e3', 'message': 'Revert ""Add database configuration to unit tests""\n\nThis reverts commit b166be36d54a70ddee2c59fcb8ab411d8cea41db.\n\nDepends-On: I16f75fb75ee01f575b252ed2418a55d7e29357ea\nChange-Id: I428d04598910edfe67e8b8deb608bcf1233d672d\n'}, {'number': 4, 'created': '2017-12-16 22:43:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/49770b595b692279a843b5edb0b7ad8a9e2bfcfa', 'message': 'Revert ""Add database configuration to unit tests""\n\nThis reverts commit b166be36d54a70ddee2c59fcb8ab411d8cea41db.\n\nChange-Id: I428d04598910edfe67e8b8deb608bcf1233d672d\n'}, {'number': 5, 'created': '2017-12-16 22:46:51.000000000', 'files': ['vitrage/tests/unit/evaluator/test_scenario_repository.py', 'vitrage/tests/functional/entity_graph/consistency/test_consistency.py', 'vitrage/tests/functional/evaluator/test_scenario_evaluator.py', 'vitrage/tests/test_configuration.py', 'vitrage/tests/functional/evaluator/test_action_executor.py'], 'web_link': 'https://opendev.org/openstack/vitrage/commit/539042f7b69684a8b7bf061e8e635d6fca2cbe12', 'message': 'Revert ""Add database configuration to unit tests""\n\nThis reverts commit b166be36d54a70ddee2c59fcb8ab411d8cea41db.\n\nChange-Id: I428d04598910edfe67e8b8deb608bcf1233d672d\n'}]",0,527688,539042f7b69684a8b7bf061e8e635d6fca2cbe12,37,8,5,19184,,,0,"Revert ""Add database configuration to unit tests""

This reverts commit b166be36d54a70ddee2c59fcb8ab411d8cea41db.

Change-Id: I428d04598910edfe67e8b8deb608bcf1233d672d
",git fetch https://review.opendev.org/openstack/vitrage refs/changes/88/527688/1 && git format-patch -1 --stdout FETCH_HEAD,"['vitrage/tests/unit/evaluator/test_scenario_repository.py', 'vitrage/tests/functional/entity_graph/consistency/test_consistency.py', 'vitrage/tests/functional/evaluator/test_scenario_evaluator.py', 'vitrage/tests/test_configuration.py', 'vitrage/tests/functional/evaluator/test_action_executor.py']",5,64733c8c89821fb27109ffc0aca0daa29af11e3d,,"from oslo_db.options import database_optsfrom vitrage import storage from vitrage.storage.sqlalchemy import models class TestActionExecutor(TestFunctionalBase): cls.conf.register_opts(database_opts, group='database') cls.conf.set_override('connection', 'sqlite:///:memory:', group='database') cls._db = storage.get_connection_from_config(cls.conf) engine = cls._db._engine_facade.get_engine() models.Base.metadata.create_all(engine)","from vitrage.tests.test_configuration import TestConfiguration class TestActionExecutor(TestFunctionalBase, TestConfiguration): cls.add_db(cls.conf)",35,57
openstack%2Fneutron~master~Ib97091c931c6c079ef5009bcd3871e698017ede7,openstack/neutron,master,Ib97091c931c6c079ef5009bcd3871e698017ede7,doc: Clarify RFE Triaged state a bit,MERGED,2017-12-08 15:22:28.000000000,2017-12-17 10:52:55.000000000,2017-12-17 10:52:55.000000000,"[{'_account_id': 841}, {'_account_id': 4694}, {'_account_id': 6854}, {'_account_id': 8655}, {'_account_id': 11975}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-08 15:22:28.000000000', 'files': ['doc/source/contributor/policies/blueprints.rst'], 'web_link': 'https://opendev.org/openstack/neutron/commit/fa7bc9d4b3ff08b2df6be4c25694d3c0c4242d72', 'message': 'doc: Clarify RFE Triaged state a bit\n\nChange-Id: Ib97091c931c6c079ef5009bcd3871e698017ede7\n'}]",7,526696,fa7bc9d4b3ff08b2df6be4c25694d3c0c4242d72,14,6,1,6854,,,0,"doc: Clarify RFE Triaged state a bit

Change-Id: Ib97091c931c6c079ef5009bcd3871e698017ede7
",git fetch https://review.opendev.org/openstack/neutron refs/changes/96/526696/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/contributor/policies/blueprints.rst'],1,fa7bc9d4b3ff08b2df6be4c25694d3c0c4242d72,rfe,"* A member of the neutron-drivers team moves the bug from ""Confirmed"" to ""Triaged"" when he/she thinks it's ready to be discussed in the drivers meeting. The bug will be in the ""Triaged"" state while the discussion is ongoing.","* The bug goes into the ""Triaged"" state while the discussion is ongoing.",4,1
openstack%2Fnova~master~Iab542cfb4d09b26f1d9a3db8a8678a8dba173eb9,openstack/nova,master,Iab542cfb4d09b26f1d9a3db8a8678a8dba173eb9,[placement] Separate API schemas (inventory),MERGED,2017-11-16 15:04:39.000000000,2017-12-17 10:33:19.000000000,2017-12-17 10:33:18.000000000,"[{'_account_id': 5754}, {'_account_id': 6062}, {'_account_id': 6125}, {'_account_id': 6167}, {'_account_id': 7634}, {'_account_id': 8556}, {'_account_id': 9008}, {'_account_id': 9732}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 11564}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15286}, {'_account_id': 16128}, {'_account_id': 16898}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 25625}, {'_account_id': 26515}]","[{'number': 1, 'created': '2017-11-16 15:04:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/0c0881c6f60424435470a304f25df4c3a6623306', 'message': '[placement] Separate API schemas (inventory)\n\nIn compute APIs, they have their schemas in the\nindependent directory (nova/api/openstack/compute/schemas).\nPlacement APIs should be like that as well.\n\nThis patch separates API schemas to an independent directory\n(nova/api/openstack/placement/schemas)\nfrom nova/api/openstack/placement/handlers/inventory.py.\n\nSubsequent patches will move schemas of other handlers.\n\nChange-Id: Iab542cfb4d09b26f1d9a3db8a8678a8dba173eb9\n'}, {'number': 2, 'created': '2017-11-23 21:01:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9d841a413d0718091619a8735a0fb9b7696df57b', 'message': '[placement] Separate API schemas (inventory)\n\nIn compute APIs, they have their schemas in the\nindependent directory (nova/api/openstack/compute/schemas).\nPlacement APIs should be like that as well.\n\nThis patch separates API schemas to an independent directory\n(nova/api/openstack/placement/schemas)\nfrom nova/api/openstack/placement/handlers/inventory.py.\n\nSubsequent patches will move schemas of other handlers.\n\nChange-Id: Iab542cfb4d09b26f1d9a3db8a8678a8dba173eb9\n'}, {'number': 3, 'created': '2017-12-07 20:59:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1aec1c8d771398eba25baa9815e3678b56edcb50', 'message': '[placement] Separate API schemas (inventory)\n\nIn compute APIs, they have their schemas in the\nindependent directory (nova/api/openstack/compute/schemas).\nPlacement APIs should be like that as well.\n\nThis patch separates API schemas to an independent directory\n(nova/api/openstack/placement/schemas)\nfrom nova/api/openstack/placement/handlers/inventory.py.\n\nSubsequent patches will move schemas of other handlers.\n\nChange-Id: Iab542cfb4d09b26f1d9a3db8a8678a8dba173eb9\n'}, {'number': 4, 'created': '2017-12-14 15:17:14.000000000', 'files': ['nova/api/openstack/placement/schemas/inventory.py', 'nova/api/openstack/placement/handlers/inventory.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/432aeae554b850bd91c227199c5f21ab9ab6a0bc', 'message': '[placement] Separate API schemas (inventory)\n\nIn compute APIs, they have their schemas in the\nindependent directory (nova/api/openstack/compute/schemas).\nPlacement APIs should be like that as well.\n\nThis patch separates API schemas to an independent directory\n(nova/api/openstack/placement/schemas)\nfrom nova/api/openstack/placement/handlers/inventory.py.\n\nSubsequent patches will move schemas of other handlers.\n\nChange-Id: Iab542cfb4d09b26f1d9a3db8a8678a8dba173eb9\n'}]",0,520613,432aeae554b850bd91c227199c5f21ab9ab6a0bc,98,20,4,7634,,,0,"[placement] Separate API schemas (inventory)

In compute APIs, they have their schemas in the
independent directory (nova/api/openstack/compute/schemas).
Placement APIs should be like that as well.

This patch separates API schemas to an independent directory
(nova/api/openstack/placement/schemas)
from nova/api/openstack/placement/handlers/inventory.py.

Subsequent patches will move schemas of other handlers.

Change-Id: Iab542cfb4d09b26f1d9a3db8a8678a8dba173eb9
",git fetch https://review.opendev.org/openstack/nova refs/changes/13/520613/4 && git format-patch -1 --stdout FETCH_HEAD,"['nova/api/openstack/placement/schemas/inventory.py', 'nova/api/openstack/placement/handlers/inventory.py']",2,0c0881c6f60424435470a304f25df4c3a6623306,placement_schema_separation,"from nova.api.openstack.placement.schemas import inventory as schema data = _extract_inventory(req.body, schema.POST_INVENTORY_SCHEMA) data = _extract_inventories(req.body, schema.PUT_INVENTORY_SCHEMA) data = _extract_inventory(req.body, schema.BASE_INVENTORY_SCHEMA)","RESOURCE_CLASS_IDENTIFIER = ""^[A-Z0-9_]+$"" BASE_INVENTORY_SCHEMA = { ""type"": ""object"", ""properties"": { ""resource_provider_generation"": { ""type"": ""integer"" }, ""total"": { ""type"": ""integer"", ""maximum"": db.MAX_INT, ""minimum"": 1, }, ""reserved"": { ""type"": ""integer"", ""maximum"": db.MAX_INT, ""minimum"": 0, }, ""min_unit"": { ""type"": ""integer"", ""maximum"": db.MAX_INT, ""minimum"": 1 }, ""max_unit"": { ""type"": ""integer"", ""maximum"": db.MAX_INT, ""minimum"": 1 }, ""step_size"": { ""type"": ""integer"", ""maximum"": db.MAX_INT, ""minimum"": 1 }, ""allocation_ratio"": { ""type"": ""number"", ""maximum"": db.SQL_SP_FLOAT_MAX }, }, ""required"": [ ""total"", ""resource_provider_generation"" ], ""additionalProperties"": False } POST_INVENTORY_SCHEMA = copy.deepcopy(BASE_INVENTORY_SCHEMA) POST_INVENTORY_SCHEMA['properties']['resource_class'] = { ""type"": ""string"", ""pattern"": RESOURCE_CLASS_IDENTIFIER, } POST_INVENTORY_SCHEMA['required'].append('resource_class') POST_INVENTORY_SCHEMA['required'].remove('resource_provider_generation') PUT_INVENTORY_RECORD_SCHEMA = copy.deepcopy(BASE_INVENTORY_SCHEMA) PUT_INVENTORY_RECORD_SCHEMA['required'].remove('resource_provider_generation') PUT_INVENTORY_SCHEMA = { ""type"": ""object"", ""properties"": { ""resource_provider_generation"": { ""type"": ""integer"" }, ""inventories"": { ""type"": ""object"", ""patternProperties"": { RESOURCE_CLASS_IDENTIFIER: PUT_INVENTORY_RECORD_SCHEMA, } } }, ""required"": [ ""resource_provider_generation"", ""inventories"" ], ""additionalProperties"": False } data = _extract_inventory(req.body, POST_INVENTORY_SCHEMA) data = _extract_inventories(req.body, PUT_INVENTORY_SCHEMA) data = _extract_inventory(req.body, BASE_INVENTORY_SCHEMA)",97,74
openstack%2Floci~master~I2123aa5bb9234062a88f641bb3136d8af3372248,openstack/loci,master,I2123aa5bb9234062a88f641bb3136d8af3372248,Always install libvirt-python for nova,MERGED,2017-12-02 02:31:42.000000000,2017-12-17 09:10:58.000000000,2017-12-17 09:10:58.000000000,"[{'_account_id': 2243}, {'_account_id': 7822}, {'_account_id': 14119}, {'_account_id': 22348}, {'_account_id': 23928}]","[{'number': 1, 'created': '2017-12-02 02:31:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/loci/commit/0667b6afae56dde2e09c6cd233af9da1c62a3957', 'message': 'Always install libvirt-python for nova\n\nWe already make assumptions that kvm/qemu will be installed. This makes\nsure nova can talk to libvirt.\n\nChange-Id: I2123aa5bb9234062a88f641bb3136d8af3372248\n'}, {'number': 2, 'created': '2017-12-11 20:56:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/loci/commit/87b4c31dab404eff0bf2e08643e1e4e16bf9cf6f', 'message': 'Always install libvirt-python for nova\n\nWe already make assumptions that kvm/qemu will be installed. This makes\nsure nova can talk to libvirt.\n\nChange-Id: I2123aa5bb9234062a88f641bb3136d8af3372248\n'}, {'number': 3, 'created': '2017-12-17 08:41:21.000000000', 'files': ['scripts/install.sh'], 'web_link': 'https://opendev.org/openstack/loci/commit/ba7037d7ece1b88654b5a62fea91c150dd46111b', 'message': 'Always install libvirt-python for nova\n\nWe already make assumptions that kvm/qemu will be installed. This makes\nsure nova can talk to libvirt.\n\nChange-Id: I2123aa5bb9234062a88f641bb3136d8af3372248\n'}]",0,524775,ba7037d7ece1b88654b5a62fea91c150dd46111b,20,5,3,14119,,,0,"Always install libvirt-python for nova

We already make assumptions that kvm/qemu will be installed. This makes
sure nova can talk to libvirt.

Change-Id: I2123aa5bb9234062a88f641bb3136d8af3372248
",git fetch https://review.opendev.org/openstack/loci refs/changes/75/524775/2 && git format-patch -1 --stdout FETCH_HEAD,['scripts/install.sh'],1,0667b6afae56dde2e09c6cd233af9da1c62a3957,libvirt-python, $(dirname $0)/pip_install.sh libvirt-python,,1,0
openstack%2Fneutron-dynamic-routing~master~Ibb60cff222cb9ac947467dbed160d6d5e7231ff5,openstack/neutron-dynamic-routing,master,Ibb60cff222cb9ac947467dbed160d6d5e7231ff5,Update the doc link,MERGED,2017-11-24 07:35:14.000000000,2017-12-17 08:59:20.000000000,2017-12-17 08:59:20.000000000,"[{'_account_id': 1653}, {'_account_id': 7787}, {'_account_id': 13252}, {'_account_id': 14605}, {'_account_id': 21430}, {'_account_id': 22348}, {'_account_id': 24209}]","[{'number': 1, 'created': '2017-11-24 07:35:14.000000000', 'files': ['doc/source/admin/system-design.rst', 'doc/source/admin/bgp-speaker.rst'], 'web_link': 'https://opendev.org/openstack/neutron-dynamic-routing/commit/aebe78405e40cc21e84ce5b91328bd7ff1380dde', 'message': 'Update the doc link\n\nChange-Id: Ibb60cff222cb9ac947467dbed160d6d5e7231ff5\n'}]",0,522733,aebe78405e40cc21e84ce5b91328bd7ff1380dde,15,7,1,24209,,,0,"Update the doc link

Change-Id: Ibb60cff222cb9ac947467dbed160d6d5e7231ff5
",git fetch https://review.opendev.org/openstack/neutron-dynamic-routing refs/changes/33/522733/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/admin/system-design.rst', 'doc/source/admin/bgp-speaker.rst']",2,aebe78405e40cc21e84ce5b91328bd7ff1380dde,doc-migration,framework that allows different `BGP drivers <../contributor/dragent-drivers.html>`_ to be plugged into a `dynamic routing agent <./agent-scheduler.html>`_.`BGP drivers <../contributor/dragent-drivers.html>`_.For details refer to `Testing <../contributor/testing.html>`_.,framework that allows different `BGP drivers <../design/drivers.html>`_ to be plugged into a `dynamic routing agent <../design/agent-scheduler.html>`_.`BGP drivers <../design/drivers.html>`_.For details refer to `Testing <../others/testing.html>`_.,7,7
openstack%2Fvitrage~master~I16f75fb75ee01f575b252ed2418a55d7e29357ea,openstack/vitrage,master,I16f75fb75ee01f575b252ed2418a55d7e29357ea,oslo service version issue,ABANDONED,2017-12-14 15:20:40.000000000,2017-12-17 08:52:29.000000000,,"[{'_account_id': 19134}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-14 15:20:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/vitrage/commit/0329f01bc23ed117b374c65474d94863638a03bc', 'message': 'oslo service version issue\n\nChange-Id: I16f75fb75ee01f575b252ed2418a55d7e29357ea\n'}, {'number': 2, 'created': '2017-12-15 15:47:19.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/vitrage/commit/fda522b329e5d0fa95b8308a7c1578d4c0dbec29', 'message': 'oslo service version issue\n\nDepends-On: I8ad18b54bc9cdacf5bd6ba467a0c46793a50ebbd\nChange-Id: I16f75fb75ee01f575b252ed2418a55d7e29357ea\n'}]",0,527989,fda522b329e5d0fa95b8308a7c1578d4c0dbec29,13,2,2,19184,,,0,"oslo service version issue

Depends-On: I8ad18b54bc9cdacf5bd6ba467a0c46793a50ebbd
Change-Id: I16f75fb75ee01f575b252ed2418a55d7e29357ea
",git fetch https://review.opendev.org/openstack/vitrage refs/changes/89/527989/2 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,0329f01bc23ed117b374c65474d94863638a03bc,,"oslo.service!=1.28.1,>=1.24.0 # Apache-2.0",oslo.service>=1.24.0 # Apache-2.0,1,1
openstack%2Fopenstack-ansible~master~If50e2412c3fd6575d7041deb8ecc9480b04184cc,openstack/openstack-ansible,master,If50e2412c3fd6575d7041deb8ecc9480b04184cc,Move inventory files to folder in root of repo,MERGED,2017-10-28 23:18:53.000000000,2017-12-17 07:11:42.000000000,2017-12-17 07:11:42.000000000,"[{'_account_id': 538}, {'_account_id': 6816}, {'_account_id': 7353}, {'_account_id': 14805}, {'_account_id': 17068}, {'_account_id': 22348}, {'_account_id': 24468}]","[{'number': 1, 'created': '2017-10-28 23:18:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/39f61dd476339954a76eb37951fcb523227ea815', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 2, 'created': '2017-10-28 23:25:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/74a7409efbe8edaa9773dd5f78f8cf76df806ce5', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 3, 'created': '2017-10-29 06:17:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/f3dd3c6a87f5fdccef2c0cc3ac6f32cf4565e57c', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 4, 'created': '2017-10-31 17:06:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/5948f9a70cd8e61ebe36f12946685e7b88cdfa5c', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 5, 'created': '2017-11-01 18:36:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/26f8133aceee9973b67f64029617b7d9232645ee', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 6, 'created': '2017-11-06 18:57:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/adc7c41f8b119b0da85a27ff9f91b020f1ffa2c6', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 7, 'created': '2017-11-06 22:55:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/b3dfa98d8d383ca50ac8147e8c288e3cadfce45a', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 8, 'created': '2017-11-07 17:06:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/b89e5e3c1c176bf12b0a609939323d11b497cecb', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 9, 'created': '2017-11-10 18:49:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/10a4a8a44acda4066b3284ddd418f88ecebf5a48', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 10, 'created': '2017-11-10 21:53:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/f8c49bcd1cb6d5b549e76842766e6bf8bc35b31b', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 11, 'created': '2017-11-19 19:09:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/2140fba0849631027433d9a13032ccf7f094918e', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 12, 'created': '2017-12-12 15:55:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/64c8b5ef5358d0f06bbab690aa119e780b2d01bd', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 13, 'created': '2017-12-13 23:04:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/267aeec791f4438b823373c61a8d4c43d0003104', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 14, 'created': '2017-12-14 11:36:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/64e95d45e58b8c0a534adfbb98114e12de4279ca', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}, {'number': 15, 'created': '2017-12-16 10:34:45.000000000', 'files': ['inventory/env.d/octavia.yml', 'inventory/group_vars/barbican_all.yml', 'scripts/upgrade-utilities/playbooks/deploy-config-changes.yml', 'inventory/env.d/galera.yml', 'inventory/host_vars/localhost/cinder.yml', 'inventory/env.d/barbican.yml', 'deploy-guide/source/conf.py', 'inventory/env.d/cinder.yml', 'inventory/env.d/designate.yml', 'inventory/env.d/keystone.yml', 'inventory/group_vars/ceph_all.yml', 'inventory/group_vars/qemu-compute_hosts.yml', 'inventory/group_vars/nova_all.yml', 'inventory/group_vars/horizon_all.yml', 'doc/source/reference/generate-inventory.rst', 'inventory/env.d/nova.yml', 'inventory/group_vars/all/cinder.yml', 'inventory/group_vars/all/infra.yml', 'inventory/host_vars/localhost/nova.yml', 'inventory/env.d/swift.yml', 'inventory/group_vars/memcached.yml', 'tests/test_filesystem.py', 'inventory/group_vars/rsyslog.yml', 'tests/test_inventory.py', 'inventory/group_vars/kvm-compute_hosts.yml', 'inventory/localhost', 'inventory/group_vars/haproxy_all/haproxy.yml', 'inventory/host_vars/localhost/ceilometer.yml', 'inventory/host_vars/localhost/swift.yml', 'inventory/group_vars/utility_all.yml', 'inventory/group_vars/neutron_agent.yml', 'inventory/group_vars/all/ceph.yml', 'inventory/group_vars/designate_all.yml', 'inventory/group_vars/cinder_all.yml', 'inventory/group_vars/network_hosts.yml', 'inventory/env.d/rabbitmq.yml', 'inventory/group_vars/tacker_all.yml', 'inventory/env.d/ceilometer.yml', 'inventory/env.d/rsyslog.yml', 'inventory/env.d/magnum.yml', 'inventory/env.d/aodh.yml', 'inventory/group_vars/all/designate.yml', 'inventory/env.d/utility.yml', 'inventory/group_vars/magnum_all.yml', 'inventory/env.d/ceph.yml', 'inventory/env.d/tacker.yml', 'inventory/group_vars/octavia_all.yml', 'inventory/env.d/os-infra.yml', 'inventory/group_vars/gnocchi_all.yml', 'inventory/env.d/swift-remote.yml', 'inventory/group_vars/all/pip.yml', 'inventory/host_vars/localhost/unbound.yml', 'inventory/env.d/memcache.yml', 'inventory/group_vars/ironic_compute.yml', 'inventory/env.d/glance.yml', 'inventory/env.d/shared-infra.yml', 'inventory/group_vars/sahara_all.yml', 'inventory/env.d/pkg_repo.yml', 'inventory/group_vars/all/nova.yml', 'inventory/group_vars/ceilometer_all.yml', 'inventory/group_vars/all_containers.yml', 'inventory/group_vars/neutron_calico_dhcp_agent.yml', 'inventory/group_vars/keystone_all.yml', 'inventory/host_vars/localhost/neutron.yml', 'inventory/group_vars/galera_all.yml', 'inventory/env.d/gnocchi.yml', 'inventory/env.d/unbound.yml', 'scripts/bootstrap-ansible.sh', 'doc/source/reference/configure-inventory.rst', 'inventory/group_vars/rabbitmq_all.yml', 'inventory/group_vars/repo_all.yml', 'inventory/env.d/etcd.yml', 'inventory/group_vars/aodh_all.yml', 'inventory/group_vars/ironic_all.yml', 'inventory/env.d/neutron.yml', 'doc/source/admin/maintenance-tasks/scale-environment.rst', 'inventory/group_vars/all/octavia.yml', 'tox.ini', 'inventory/dynamic_inventory.py', 'inventory/env.d/molteniron.yml', 'inventory/group_vars/lxd-compute_hosts.yml', 'inventory/env.d/trove.yml', 'inventory/env.d/haproxy.yml', 'inventory/group_vars/glance_all.yml', 'inventory/group_vars/heat_all.yml', 'inventory/group_vars/all/neutron.yml', 'doc/source/admin/advanced-config.rst', 'inventory/env.d/horizon.yml', 'inventory/group_vars/all/keystone.yml', 'inventory/group_vars/cinder_volume.yml', 'inventory/group_vars/powervm-compute_hosts.yml', 'inventory/group_vars/ironic-compute_hosts.yml', 'doc/source/contributor/additional-roles.rst', 'inventory/group_vars/all/glance.yml', 'inventory/env.d/sahara.yml', 'doc/source/conf.py', 'inventory/group_vars/all/all.yml', 'inventory/group_vars/swift_all.yml', 'inventory/group_vars/all/ssl.yml', 'inventory/env.d/ironic.yml', 'inventory/group_vars/neutron_all.yml', 'inventory/group_vars/hosts.yml', 'inventory/group_vars/trove_all.yml', 'inventory/group_vars/haproxy_all/keepalived.yml', 'inventory/env.d/heat.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/c5551f2c8cf641250517ceb978f9c94ba337ebfb', 'message': ""Move inventory files to folder in root of repo\n\nMove the playbooks/inventory folder, group_vars, and host_vars to\ninventory/ in the root of the OpenStack-Ansible repo. This helps better\norganize the repo structure since playbooks/ will now only contain\nplaybooks, shared task files, and included repo package var files.\n\ngroup_vars and host_vars are moved alongside the inventory since that's\nthe default place that Ansible expects those folders and to help better\nprepare for Ansible 2.4 where multiple inventories can be loaded,\nautomatically including relative group and host var files.\n\nEffected docs, scripts, and variables have been updated with the new\npaths.\n\nChange-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc\n""}]",0,516032,c5551f2c8cf641250517ceb978f9c94ba337ebfb,69,7,15,14805,,,0,"Move inventory files to folder in root of repo

Move the playbooks/inventory folder, group_vars, and host_vars to
inventory/ in the root of the OpenStack-Ansible repo. This helps better
organize the repo structure since playbooks/ will now only contain
playbooks, shared task files, and included repo package var files.

group_vars and host_vars are moved alongside the inventory since that's
the default place that Ansible expects those folders and to help better
prepare for Ansible 2.4 where multiple inventories can be loaded,
automatically including relative group and host var files.

Effected docs, scripts, and variables have been updated with the new
paths.

Change-Id: If50e2412c3fd6575d7041deb8ecc9480b04184cc
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/32/516032/15 && git format-patch -1 --stdout FETCH_HEAD,"['inventory/env.d/octavia.yml', 'inventory/group_vars/barbican_all.yml', 'scripts/upgrade-utilities/playbooks/deploy-config-changes.yml', 'inventory/env.d/galera.yml', 'inventory/host_vars/localhost/cinder.yml', 'inventory/env.d/barbican.yml', 'deploy-guide/source/conf.py', 'inventory/env.d/cinder.yml', 'inventory/env.d/designate.yml', 'inventory/env.d/keystone.yml', 'inventory/group_vars/ceph_all.yml', 'inventory/group_vars/qemu-compute_hosts.yml', 'inventory/group_vars/nova_all.yml', 'inventory/group_vars/horizon_all.yml', 'inventory/env.d/nova.yml', 'doc/source/contributor/inventory.rst', 'inventory/group_vars/all/cinder.yml', 'inventory/group_vars/all/infra.yml', 'inventory/host_vars/localhost/nova.yml', 'inventory/env.d/swift.yml', 'inventory/group_vars/memcached.yml', 'tests/test_filesystem.py', 'inventory/group_vars/rsyslog.yml', 'tests/test_inventory.py', 'inventory/group_vars/kvm-compute_hosts.yml', 'inventory/localhost', 'inventory/group_vars/haproxy_all/haproxy.yml', 'inventory/group_vars/utility_all.yml', 'inventory/group_vars/neutron_agent.yml', 'inventory/group_vars/all/ceph.yml', 'inventory/group_vars/designate_all.yml', 'inventory/group_vars/cinder_all.yml', 'inventory/env.d/rabbitmq.yml', 'inventory/group_vars/tacker_all.yml', 'inventory/env.d/ceilometer.yml', 'inventory/env.d/rsyslog.yml', 'inventory/env.d/magnum.yml', 'inventory/env.d/aodh.yml', 'inventory/env.d/utility.yml', 'inventory/group_vars/magnum_all.yml', 'inventory/env.d/ceph.yml', 'inventory/env.d/tacker.yml', 'inventory/group_vars/octavia_all.yml', 'inventory/env.d/os-infra.yml', 'inventory/group_vars/gnocchi_all.yml', 'inventory/env.d/swift-remote.yml', 'inventory/group_vars/all/pip.yml', 'inventory/host_vars/localhost/unbound.yml', 'inventory/env.d/memcache.yml', 'inventory/group_vars/ironic_compute.yml', 'inventory/env.d/glance.yml', 'inventory/env.d/shared-infra.yml', 'inventory/group_vars/sahara_all.yml', 'inventory/env.d/pkg_repo.yml', 'inventory/group_vars/all/nova.yml', 'inventory/group_vars/ceilometer_all.yml', 'inventory/group_vars/all_containers.yml', 'inventory/group_vars/neutron_calico_dhcp_agent.yml', 'inventory/group_vars/keystone_all.yml', 'doc/source/reference/inventory.rst', 'inventory/host_vars/localhost/neutron.yml', 'inventory/group_vars/galera_all.yml', 'inventory/env.d/gnocchi.yml', 'inventory/env.d/unbound.yml', 'scripts/bootstrap-ansible.sh', 'doc/source/reference/configure-inventory.rst', 'inventory/group_vars/rabbitmq_all.yml', 'inventory/group_vars/repo_all.yml', 'inventory/env.d/etcd.yml', 'inventory/group_vars/aodh_all.yml', 'inventory/group_vars/ironic_all.yml', 'inventory/env.d/neutron.yml', 'doc/source/admin/maintenance-tasks/scale-environment.rst', 'tox.ini', 'inventory/dynamic_inventory.py', 'inventory/env.d/molteniron.yml', 'inventory/group_vars/lxd-compute_hosts.yml', 'inventory/env.d/trove.yml', 'inventory/env.d/haproxy.yml', 'inventory/group_vars/glance_all.yml', 'inventory/group_vars/heat_all.yml', 'inventory/group_vars/all/neutron.yml', 'doc/source/admin/advanced-config.rst', 'inventory/env.d/horizon.yml', 'inventory/group_vars/all/keystone.yml', 'inventory/group_vars/cinder_volume.yml', 'inventory/group_vars/powervm-compute_hosts.yml', 'inventory/group_vars/ironic-compute_hosts.yml', 'doc/source/contributor/additional-roles.rst', 'inventory/group_vars/all/glance.yml', 'inventory/env.d/sahara.yml', 'doc/source/conf.py', 'inventory/group_vars/all/all.yml', 'inventory/group_vars/swift_all.yml', 'inventory/group_vars/all/ssl.yml', 'inventory/env.d/ironic.yml', 'inventory/group_vars/neutron_all.yml', 'inventory/group_vars/hosts.yml', 'inventory/group_vars/trove_all.yml', 'inventory/group_vars/haproxy_all/keepalived.yml', 'inventory/env.d/heat.yml']",101,39f61dd476339954a76eb37951fcb523227ea815,move_inventory_folder,,,28,28
openstack%2Fopenstack-ansible~master~I7f083ecf8e19030c621324fa2b162f22cb831bd0,openstack/openstack-ansible,master,I7f083ecf8e19030c621324fa2b162f22cb831bd0,Updated from global requirements,MERGED,2017-11-16 11:11:48.000000000,2017-12-17 04:21:24.000000000,2017-12-17 04:21:24.000000000,"[{'_account_id': 538}, {'_account_id': 6816}, {'_account_id': 7353}, {'_account_id': 22348}, {'_account_id': 23163}, {'_account_id': 24468}]","[{'number': 1, 'created': '2017-11-16 11:11:48.000000000', 'files': ['requirements.txt', 'test-requirements.txt'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/cbd552eee74346bc7591e5ef31448dd3e85d26ab', 'message': 'Updated from global requirements\n\nChange-Id: I7f083ecf8e19030c621324fa2b162f22cb831bd0\n'}]",0,520442,cbd552eee74346bc7591e5ef31448dd3e85d26ab,22,6,1,11131,,,0,"Updated from global requirements

Change-Id: I7f083ecf8e19030c621324fa2b162f22cb831bd0
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/42/520442/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'test-requirements.txt']",2,cbd552eee74346bc7591e5ef31448dd3e85d26ab,openstack/requirements,virtualenv>=14.0.6 # MIT,virtualenv>=13.1.0 # MIT,2,2
openstack%2Fopenstack-ansible-os_tempest~stable%2Fpike~Ic15b9902e3b0b51040f3f32938b30a060c592e97,openstack/openstack-ansible-os_tempest,stable/pike,Ic15b9902e3b0b51040f3f32938b30a060c592e97,Define cache_timeout by default,MERGED,2017-12-12 12:16:08.000000000,2017-12-17 03:03:13.000000000,2017-12-17 03:03:13.000000000,"[{'_account_id': 6816}, {'_account_id': 7353}, {'_account_id': 17068}, {'_account_id': 22348}, {'_account_id': 23163}]","[{'number': 1, 'created': '2017-12-12 12:16:08.000000000', 'files': ['vars/ubuntu-16.04.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_tempest/commit/97d9f12a3982a5fd6bb838d783b4a4959fba2f5e', 'message': ""Define cache_timeout by default\n\nBy default, the role fails in the task\n``Install distro packages`` if the variable cache_timeout isn't\ndefined.\n\nThis should fix the issue, and be more consistent with other roles.\n\nChange-Id: Ic15b9902e3b0b51040f3f32938b30a060c592e97\n(cherry picked from commit 2681695f4680e3bf8ea4bc49d07270b66ec0013e)\n""}]",0,527384,97d9f12a3982a5fd6bb838d783b4a4959fba2f5e,16,5,1,17068,,,0,"Define cache_timeout by default

By default, the role fails in the task
``Install distro packages`` if the variable cache_timeout isn't
defined.

This should fix the issue, and be more consistent with other roles.

Change-Id: Ic15b9902e3b0b51040f3f32938b30a060c592e97
(cherry picked from commit 2681695f4680e3bf8ea4bc49d07270b66ec0013e)
",git fetch https://review.opendev.org/openstack/openstack-ansible-os_tempest refs/changes/84/527384/1 && git format-patch -1 --stdout FETCH_HEAD,['vars/ubuntu-16.04.yml'],1,97d9f12a3982a5fd6bb838d783b4a4959fba2f5e,fix_tempest-stable/pike,## APT Cache options cache_timeout: 600 ,,3,0
openstack%2Fnetworking-midonet~master~I87efb48f454a10f6e8f02b243e4f67b0052a05d5,openstack/networking-midonet,master,I87efb48f454a10f6e8f02b243e4f67b0052a05d5,DNM: sync with I436353690839281ca7e13eaf792249306b71dd4b,ABANDONED,2017-12-17 02:03:31.000000000,2017-12-17 03:02:44.000000000,,[],"[{'number': 1, 'created': '2017-12-17 02:03:31.000000000', 'files': ['midonet/neutron/db/l3_db_midonet.py'], 'web_link': 'https://opendev.org/openstack/networking-midonet/commit/12a891b068903470da49b8ffc58360d77db40e07', 'message': 'DNM: sync with I436353690839281ca7e13eaf792249306b71dd4b\n\nChange-Id: I87efb48f454a10f6e8f02b243e4f67b0052a05d5\n'}]",0,528517,12a891b068903470da49b8ffc58360d77db40e07,2,0,1,6854,,,0,"DNM: sync with I436353690839281ca7e13eaf792249306b71dd4b

Change-Id: I87efb48f454a10f6e8f02b243e4f67b0052a05d5
",git fetch https://review.opendev.org/openstack/networking-midonet refs/changes/17/528517/1 && git format-patch -1 --stdout FETCH_HEAD,['midonet/neutron/db/l3_db_midonet.py'],1,12a891b068903470da49b8ffc58360d77db40e07,bug/1737467," # Both subnet_id and floating_ip_address are accepted, if # floating_ip_address is not in the subnet, # InvalidIpForSubnet exception will be raised. fixed_ip = {} if fip['subnet_id']: fixed_ip['subnet_id'] = fip['subnet_id'] if fip['floating_ip_address']: fixed_ip['ip_address'] = fip['floating_ip_address'] if fixed_ip: port['fixed_ips'] = [fixed_ip]", if fip.get('floating_ip_address'): port['fixed_ips'] = [ {'ip_address': fip['floating_ip_address']}] if fip.get('subnet_id'): port['fixed_ips'] = [ {'subnet_id': fip['subnet_id']}],10,7
openstack%2Fopenstack-ansible-os_neutron~master~I8d3d4e855c031513224dfba2e2497bbc886b069f,openstack/openstack-ansible-os_neutron,master,I8d3d4e855c031513224dfba2e2497bbc886b069f,Add and document the Octavia proxy plugin,MERGED,2017-12-06 15:58:54.000000000,2017-12-17 02:11:19.000000000,2017-12-17 02:11:18.000000000,"[{'_account_id': 538}, {'_account_id': 7353}, {'_account_id': 10850}, {'_account_id': 11628}, {'_account_id': 12024}, {'_account_id': 17068}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-06 15:58:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_neutron/commit/79d04c5ebf3b4aa2b98c8f5a9e9bf2c87e0337bf', 'message': 'Add and document the Octavia proxy plugin\n\nImproove the Octavia related LBaaS documentation and add the new\nOctavia proxy plugin.\n\nChange-Id: I8d3d4e855c031513224dfba2e2497bbc886b069f\n'}, {'number': 2, 'created': '2017-12-06 21:57:44.000000000', 'files': ['templates/neutron.conf.j2', 'doc/source/configure-network-services.rst', 'defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_neutron/commit/d5d49e57919fdec28f43d136a982f44c66438ac0', 'message': 'Add and document the Octavia proxy plugin\n\nImproove the Octavia related LBaaS documentation and add the new\nOctavia proxy plugin.\n\nChange-Id: I8d3d4e855c031513224dfba2e2497bbc886b069f\n'}]",1,526104,d5d49e57919fdec28f43d136a982f44c66438ac0,14,7,2,10850,,,0,"Add and document the Octavia proxy plugin

Improove the Octavia related LBaaS documentation and add the new
Octavia proxy plugin.

Change-Id: I8d3d4e855c031513224dfba2e2497bbc886b069f
",git fetch https://review.opendev.org/openstack/openstack-ansible-os_neutron refs/changes/04/526104/2 && git format-patch -1 --stdout FETCH_HEAD,"['templates/neutron.conf.j2', 'doc/source/configure-network-services.rst', 'defaults/main.yml']",3,79d04c5ebf3b4aa2b98c8f5a9e9bf2c87e0337bf,octavia-proxy,# Use the Octavia proxy neutron_octavia_proxy_plugin: False ,,58,4
openstack%2Fopenstack-ansible-os_nova~master~I9f9a130be3a4920170295b041b94dd435e3781d1,openstack/openstack-ansible-os_nova,master,I9f9a130be3a4920170295b041b94dd435e3781d1,Add MySQL connection SSL support,MERGED,2017-12-14 16:08:24.000000000,2017-12-17 01:27:44.000000000,2017-12-17 01:27:44.000000000,"[{'_account_id': 6816}, {'_account_id': 7353}, {'_account_id': 14805}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-14 16:08:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_nova/commit/dfe7fb6c2392da5a1a461743a1d31dbebff85d9d', 'message': ""Add MySQL connection SSL support\n\nWhen 'nova_galera_use_ssl' is True, use an encrypted connection to\nthe database using either a self-signed or user-provided CA certificate.\n\nA new non-voting test has been added to verify that the role remains\nfunctional when enabling SSL features.\n\nChange-Id: I9f9a130be3a4920170295b041b94dd435e3781d1\nPartial-Bug: 1667789\n""}, {'number': 2, 'created': '2017-12-15 04:18:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_nova/commit/36d18a38cb0006839992e9a8607b18d0f3363b37', 'message': ""Add MySQL connection SSL support\n\nWhen 'nova_galera_use_ssl' is True, use an encrypted connection to\nthe database using either a self-signed or user-provided CA certificate.\n\nA new non-voting test has been added to verify that the role remains\nfunctional when enabling SSL features.\n\nChange-Id: I9f9a130be3a4920170295b041b94dd435e3781d1\nPartial-Bug: 1667789\n""}, {'number': 3, 'created': '2017-12-15 15:30:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_nova/commit/ae3569f14250a0bdae411fc00e8f1d5f9233f83a', 'message': ""Add MySQL connection SSL support\n\nWhen 'nova_galera_use_ssl' is True, use an encrypted connection to\nthe database using either a self-signed or user-provided CA certificate.\n\nA new non-voting test has been added to verify that the role remains\nfunctional when enabling SSL features.\n\nChange-Id: I9f9a130be3a4920170295b041b94dd435e3781d1\nPartial-Bug: 1667789\n""}, {'number': 4, 'created': '2017-12-15 17:20:54.000000000', 'files': ['templates/nova.conf.j2', 'zuul.d/project.yaml', 'tox.ini', 'zuul.d/jobs.yaml', 'defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_nova/commit/70c663391be81446d458b4a9f2dc509b27e72777', 'message': ""Add MySQL connection SSL support\n\nWhen 'nova_galera_use_ssl' is True, use an encrypted connection to\nthe database using either a self-signed or user-provided CA certificate.\n\nA new non-voting test has been added to verify that the role remains\nfunctional when enabling SSL features.\n\nChange-Id: I9f9a130be3a4920170295b041b94dd435e3781d1\nPartial-Bug: 1667789\n""}]",0,528001,70c663391be81446d458b4a9f2dc509b27e72777,16,4,4,14805,,,0,"Add MySQL connection SSL support

When 'nova_galera_use_ssl' is True, use an encrypted connection to
the database using either a self-signed or user-provided CA certificate.

A new non-voting test has been added to verify that the role remains
functional when enabling SSL features.

Change-Id: I9f9a130be3a4920170295b041b94dd435e3781d1
Partial-Bug: 1667789
",git fetch https://review.opendev.org/openstack/openstack-ansible-os_nova refs/changes/01/528001/3 && git format-patch -1 --stdout FETCH_HEAD,"['templates/nova.conf.j2', 'zuul.d/project.yaml', 'tox.ini', 'zuul.d/jobs.yaml', 'defaults/main.yml']",5,dfe7fb6c2392da5a1a461743a1d31dbebff85d9d,bug/1667789,"## Database infonova_galera_use_ssl: ""{{ galera_use_ssl | default(False) }}""nova_galera_ssl_ca_cert: ""{{ galera_ssl_ca_cert | default('/etc/ssl/certs/galera-ca.pem') }}""",## DBnova_galera_use_ssl: Falsenova_galera_ssl_ca_cert: /etc/ssl/certs/galera-ca.pem,25,8
openstack%2Fopenstack-ansible-os_octavia~master~I7a43d313474e17d7e968a5a9510368e3abdf6682,openstack/openstack-ansible-os_octavia,master,I7a43d313474e17d7e968a5a9510368e3abdf6682,Add MySQL connection SSL support,MERGED,2017-12-14 19:37:43.000000000,2017-12-17 01:20:11.000000000,2017-12-17 01:20:10.000000000,"[{'_account_id': 6816}, {'_account_id': 7353}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-14 19:37:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_octavia/commit/6c38abd9a515bdfc948b9028e72ab857244855a2', 'message': ""Add MySQL connection SSL support\n\nWhen 'octavia_galera_use_ssl' is True, use an encrypted connection to\nthe database using either a self-signed or user-provided CA certificate.\n\nA new non-voting test has been added to verify that the role remains\nfunctional when enabling SSL features.\n\nChange-Id: I7a43d313474e17d7e968a5a9510368e3abdf6682\nPartial-Bug: 1667789\n""}, {'number': 2, 'created': '2017-12-15 16:11:39.000000000', 'files': ['zuul.d/project.yaml', 'templates/octavia.conf.j2', 'tox.ini', 'zuul.d/jobs.yaml', 'defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_octavia/commit/8cc05a3d00291aafc6a9d3fd4f430f2bf78c0c14', 'message': ""Add MySQL connection SSL support\n\nWhen 'octavia_galera_use_ssl' is True, use an encrypted connection to\nthe database using either a self-signed or user-provided CA certificate.\n\nA new non-voting test has been added to verify that the role remains\nfunctional when enabling SSL features.\n\nChange-Id: I7a43d313474e17d7e968a5a9510368e3abdf6682\nPartial-Bug: 1667789\n""}]",0,528062,8cc05a3d00291aafc6a9d3fd4f430f2bf78c0c14,10,3,2,14805,,,0,"Add MySQL connection SSL support

When 'octavia_galera_use_ssl' is True, use an encrypted connection to
the database using either a self-signed or user-provided CA certificate.

A new non-voting test has been added to verify that the role remains
functional when enabling SSL features.

Change-Id: I7a43d313474e17d7e968a5a9510368e3abdf6682
Partial-Bug: 1667789
",git fetch https://review.opendev.org/openstack/openstack-ansible-os_octavia refs/changes/62/528062/1 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/project.yaml', 'templates/octavia.conf.j2', 'tox.ini', 'zuul.d/jobs.yaml', 'defaults/main.yml']",5,6c38abd9a515bdfc948b9028e72ab857244855a2,bug/1667789,"## Database infooctavia_galera_use_ssl: ""{{ galera_use_ssl | default(False) }}"" octavia_galera_ssl_ca_cert: ""{{ galera_ssl_ca_cert | default('/etc/ssl/certs/galera-ca.pem') }}""",## DB,38,2
openstack%2Fopenstack-ansible-repo_build~master~I298dd45ada552b78ef7d6fcb79481035581bd77a,openstack/openstack-ansible-repo_build,master,I298dd45ada552b78ef7d6fcb79481035581bd77a,Add timestamps for pip wheel build logs,MERGED,2017-12-15 14:16:19.000000000,2017-12-17 00:49:06.000000000,2017-12-17 00:49:06.000000000,"[{'_account_id': 538}, {'_account_id': 7353}, {'_account_id': 14805}, {'_account_id': 22348}, {'_account_id': 23163}]","[{'number': 1, 'created': '2017-12-15 14:16:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-repo_build/commit/1facf0b295d71211c92d4e23ba700a9cad48c39b', 'message': 'Add timestamps for pip wheel build logs\n\nThis patch adds timestamps to pip wheel builds logs. This should\nallow us to profile the wheel build runs and discover why CentOS\nand OpenSUSE are so much slower than Ubuntu when building wheels.\n\nPartial-Bug: 1738424\nChange-Id: I298dd45ada552b78ef7d6fcb79481035581bd77a\n'}, {'number': 2, 'created': '2017-12-15 15:03:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-repo_build/commit/5b1df80d9721dd910fd4115af38681c05e1891c5', 'message': 'Add timestamps for pip wheel build logs\n\nThis patch adds timestamps to pip wheel builds logs. This should\nallow us to profile the wheel build runs and discover why CentOS\nand OpenSUSE are so much slower than Ubuntu when building wheels.\n\nThe path also moves the wheel build process into a templated script\nin /opt, which makes it easier to redirect output and prepend\ntimestamps to the log.\n\nPartial-Bug: 1738424\nChange-Id: I298dd45ada552b78ef7d6fcb79481035581bd77a\n'}, {'number': 3, 'created': '2017-12-15 15:27:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-repo_build/commit/407534acc92b6cfa510f906230d3074c350dcdd8', 'message': 'Add timestamps for pip wheel build logs\n\nThis patch adds timestamps to pip wheel builds logs. This should\nallow us to profile the wheel build runs and discover why CentOS\nand OpenSUSE are so much slower than Ubuntu when building wheels.\n\nThe path also moves the wheel build process into a templated script\nin /opt, which makes it easier to redirect output and prepend\ntimestamps to the log.\n\nPartial-Bug: 1738424\nChange-Id: I298dd45ada552b78ef7d6fcb79481035581bd77a\n'}, {'number': 4, 'created': '2017-12-15 16:11:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-repo_build/commit/80949bf6d6521f2fcc38ca388b9853b0c9dcfd12', 'message': 'Add timestamps for pip wheel build logs\n\nThis patch adds timestamps to pip wheel builds logs. This should\nallow us to profile the wheel build runs and discover why CentOS\nand OpenSUSE are so much slower than Ubuntu when building wheels.\n\nThe path also moves the wheel build process into a templated script\nin /opt, which makes it easier to redirect output and prepend\ntimestamps to the log.\n\nPartial-Bug: 1738424\nChange-Id: I298dd45ada552b78ef7d6fcb79481035581bd77a\n'}, {'number': 5, 'created': '2017-12-15 17:29:44.000000000', 'files': ['tasks/repo_build_prepare.yml', 'templates/wheel-build-script.sh.j2', 'vars/redhat.yml', 'vars/suse-42.yml', 'vars/ubuntu-16.04.yml', 'tasks/repo_build_wheels.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-repo_build/commit/782a4b248ff0b67bef03c252d078cfaa573c57c2', 'message': 'Add timestamps for pip wheel build logs\n\nThis patch adds timestamps to pip wheel builds logs. This should\nallow us to profile the wheel build runs and discover why CentOS\nand OpenSUSE are so much slower than Ubuntu when building wheels.\n\nThe path also moves the wheel build process into a templated script\nin /opt, which makes it easier to redirect output and prepend\ntimestamps to the log.\n\nPartial-Bug: 1738424\nChange-Id: I298dd45ada552b78ef7d6fcb79481035581bd77a\n'}]",3,528279,782a4b248ff0b67bef03c252d078cfaa573c57c2,17,5,5,538,,,0,"Add timestamps for pip wheel build logs

This patch adds timestamps to pip wheel builds logs. This should
allow us to profile the wheel build runs and discover why CentOS
and OpenSUSE are so much slower than Ubuntu when building wheels.

The path also moves the wheel build process into a templated script
in /opt, which makes it easier to redirect output and prepend
timestamps to the log.

Partial-Bug: 1738424
Change-Id: I298dd45ada552b78ef7d6fcb79481035581bd77a
",git fetch https://review.opendev.org/openstack/openstack-ansible-repo_build refs/changes/79/528279/5 && git format-patch -1 --stdout FETCH_HEAD,"['vars/redhat.yml', 'vars/suse-42.yml', 'vars/ubuntu-16.04.yml', 'tasks/repo_build_wheels.yml']",4,1facf0b295d71211c92d4e23ba700a9cad48c39b,bug/1738424, 2>&1 | ts > /var/log/repo/repo_builder.log, --log /var/log/repo/repo_builder.log,4,1
openstack%2Fopenstack-ansible-ops~master~Idc5207a933d1dd865ec2d480dfe368f9f5a7d3dd,openstack/openstack-ansible-ops,master,Idc5207a933d1dd865ec2d480dfe368f9f5a7d3dd,Add reserved IP of old neutron_agent container,MERGED,2017-12-05 15:32:17.000000000,2017-12-17 00:30:15.000000000,2017-12-17 00:30:15.000000000,"[{'_account_id': 538}, {'_account_id': 7353}, {'_account_id': 13095}, {'_account_id': 22348}, {'_account_id': 23163}]","[{'number': 1, 'created': '2017-12-05 15:32:17.000000000', 'files': ['leap-upgrades/upgrade-utilities/neutron-container-forget.sh'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-ops/commit/074a2e95302d376b52a651398df844b33d781142', 'message': 'Add reserved IP of old neutron_agent container\n\nthis should prevent the system to allocate those IP to the new neutron_agent containers.\n\nChange-Id: Idc5207a933d1dd865ec2d480dfe368f9f5a7d3dd\n'}]",3,525654,074a2e95302d376b52a651398df844b33d781142,10,5,1,13095,,,0,"Add reserved IP of old neutron_agent container

this should prevent the system to allocate those IP to the new neutron_agent containers.

Change-Id: Idc5207a933d1dd865ec2d480dfe368f9f5a7d3dd
",git fetch https://review.opendev.org/openstack/openstack-ansible-ops refs/changes/54/525654/1 && git format-patch -1 --stdout FETCH_HEAD,['leap-upgrades/upgrade-utilities/neutron-container-forget.sh'],1,074a2e95302d376b52a651398df844b33d781142,fencing-neutron-agent-ip," RESERVED_IPS="""" RESERVED_IPS+=""$(get_inv_items 'neutron_agent' | awk '{print $12}') "" for ips in ${RESERVED_IPS}; do echo ""$ips"" >> /etc/openstack_deploy/leapfrog_old_ips_reservation sed -i ""/^used_ips:/a \ \ - $ips"" /etc/openstack_deploy/openstack_user_config.yml done",,6,0
openstack%2Ftempest~master~I4f9a677a921106d4f59e7337fe797f610447ca35,openstack/tempest,master,I4f9a677a921106d4f59e7337fe797f610447ca35,Update media type used by update_image,ABANDONED,2017-12-16 20:27:37.000000000,2017-12-16 22:08:06.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2017-12-16 20:27:37.000000000', 'files': ['tempest/lib/services/image/v2/images_client.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/4be2ec0280b113c93d3ee6ca6040b20555b39e62', 'message': 'Update media type used by update_image\n\nThe media type application/openstack-images-v2.0-json-patch has been\ndeprecated since API v2.1 [1].\nUse application/openstack-images-v2.1-json-patch from now onward.\n\n[1] https://developer.openstack.org/api-ref/image/v2/#update-an-image\n\nChange-Id: I4f9a677a921106d4f59e7337fe797f610447ca35\n'}]",0,528496,4be2ec0280b113c93d3ee6ca6040b20555b39e62,3,1,1,24711,,,0,"Update media type used by update_image

The media type application/openstack-images-v2.0-json-patch has been
deprecated since API v2.1 [1].
Use application/openstack-images-v2.1-json-patch from now onward.

[1] https://developer.openstack.org/api-ref/image/v2/#update-an-image

Change-Id: I4f9a677a921106d4f59e7337fe797f610447ca35
",git fetch https://review.opendev.org/openstack/tempest refs/changes/96/528496/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/lib/services/image/v2/images_client.py'],1,4be2ec0280b113c93d3ee6ca6040b20555b39e62,update_glance_image_media_type," headers = {""Content-Type"": ""application/openstack-images-v2.1"""," headers = {""Content-Type"": ""application/openstack-images-v2.0""",1,1
openstack%2Fpython-novaclient~master~I33dfc326dbf600c3faa29331ba43c8a10d7ac18a,openstack/python-novaclient,master,I33dfc326dbf600c3faa29331ba43c8a10d7ac18a,Update functional tests for microversion 2.57,ABANDONED,2017-12-16 22:02:25.000000000,2017-12-16 22:05:09.000000000,,[],"[{'number': 1, 'created': '2017-12-16 22:02:25.000000000', 'files': ['novaclient/tests/functional/v2/test_quotas.py'], 'web_link': 'https://opendev.org/openstack/python-novaclient/commit/be7e523e482a4634e8bc3f8ba57107a07e206c95', 'message': 'Update functional tests for microversion 2.57\n\nThe 2.57 microversion removes the injected_files,\ninjected_file_content_bytes and injected_file_path_bytes quotas from the\nos-quota-sets and os-quota-class-sets APIs. [1]\n\n[1] https://docs.openstack.org/nova/latest/reference/api-microversion-history.html#id51\n\nChange-Id: I33dfc326dbf600c3faa29331ba43c8a10d7ac18a\n'}]",0,528508,be7e523e482a4634e8bc3f8ba57107a07e206c95,2,0,1,24711,,,0,"Update functional tests for microversion 2.57

The 2.57 microversion removes the injected_files,
injected_file_content_bytes and injected_file_path_bytes quotas from the
os-quota-sets and os-quota-class-sets APIs. [1]

[1] https://docs.openstack.org/nova/latest/reference/api-microversion-history.html#id51

Change-Id: I33dfc326dbf600c3faa29331ba43c8a10d7ac18a
",git fetch https://review.opendev.org/openstack/python-novaclient refs/changes/08/528508/1 && git format-patch -1 --stdout FETCH_HEAD,['novaclient/tests/functional/v2/test_quotas.py'],1,be7e523e482a4634e8bc3f8ba57107a07e206c95,test_quotas_v257," COMPUTE_API_VERSION = ""2.36"" class TestQuotasNovaClient2_57(TestQuotasNovaClient2_35): """"""Nova quotas functional tests."""""" COMPUTE_API_VERSION = ""2.latest"" # The 2.57 microversion removes the injected_files, # injected_file_content_bytes and injected_file_path_bytes quotas # from the os-quota-sets and os-quota-class-sets APIs. _quota_resources = ['instances', 'cores', 'ram', 'metadata_items', 'key_pairs', 'server_groups', 'server_group_members']"," COMPUTE_API_VERSION = ""2.latest""",13,1
openstack%2Fopenstack-zuul-jobs~master~I5ecf23a3d12c3301f4506e3d46540ef38866f3df,openstack/openstack-zuul-jobs,master,I5ecf23a3d12c3301f4506e3d46540ef38866f3df,Remove legacy jobs in Craton,ABANDONED,2017-10-10 10:19:05.000000000,2017-12-16 21:44:07.000000000,,"[{'_account_id': 6547}, {'_account_id': 8556}, {'_account_id': 22348}, {'_account_id': 22582}]","[{'number': 1, 'created': '2017-10-10 10:19:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-zuul-jobs/commit/1ac576873ba6406312e7b4671fca3e11a6c4cae9', 'message': 'Remove legacy jobs in Craton\n\nDepends-On: I05e1df55fd6711970742ad2df8aadc65c28eca95\nNeeded-By: I3f2bb831925ccad1c54ace8bd6cbd38988d2809a\n\nChange-Id: I5ecf23a3d12c3301f4506e3d46540ef38866f3df\n'}, {'number': 2, 'created': '2017-10-11 01:27:56.000000000', 'files': ['zuul.d/zuul-legacy-jobs.yaml', 'playbooks/legacy/craton-tox-functional/post.yaml', 'playbooks/legacy/craton-tox-functional/run.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-zuul-jobs/commit/11643698842ed232fc9cdf66be8c0bc58432a215', 'message': 'Remove legacy jobs in Craton\n\nDepends-On: I05e1df55fd6711970742ad2df8aadc65c28eca95\nDepends-On: I3f2bb831925ccad1c54ace8bd6cbd38988d2809a\n\nChange-Id: I5ecf23a3d12c3301f4506e3d46540ef38866f3df\n'}]",2,510824,11643698842ed232fc9cdf66be8c0bc58432a215,9,4,2,22582,,,0,"Remove legacy jobs in Craton

Depends-On: I05e1df55fd6711970742ad2df8aadc65c28eca95
Depends-On: I3f2bb831925ccad1c54ace8bd6cbd38988d2809a

Change-Id: I5ecf23a3d12c3301f4506e3d46540ef38866f3df
",git fetch https://review.opendev.org/openstack/openstack-zuul-jobs refs/changes/24/510824/2 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/zuul-legacy-jobs.yaml'],1,1ac576873ba6406312e7b4671fca3e11a6c4cae9,migrate-to-zuulv3,, name: legacy-craton-tox-functional parent: legacy-base run: playbooks/legacy/craton-tox-functional/run post-run: playbooks/legacy/craton-tox-functional/post timeout: 2400 required-projects: - openstack/requirements - job:,0,9
openstack%2Fproject-config~master~I3f2bb831925ccad1c54ace8bd6cbd38988d2809a,openstack/project-config,master,I3f2bb831925ccad1c54ace8bd6cbd38988d2809a,Remove legacy jobs in Craton,ABANDONED,2017-10-10 10:13:11.000000000,2017-12-16 21:43:36.000000000,,"[{'_account_id': 3}, {'_account_id': 6547}, {'_account_id': 22348}, {'_account_id': 22582}]","[{'number': 1, 'created': '2017-10-10 10:13:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/e7902c54329194dd356558416120f453d95f7ed7', 'message': 'Remove legacy jobs in Craton\n\nDepends-On: I05e1df55fd6711970742ad2df8aadc65c28eca95\nDepends-On: I5ecf23a3d12c3301f4506e3d46540ef38866f3df\n\nChange-Id: I3f2bb831925ccad1c54ace8bd6cbd38988d2809a\n'}, {'number': 2, 'created': '2017-10-11 01:26:19.000000000', 'files': ['zuul.d/projects.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/0c40016b5b31d1849aaa88408fc8d0b77e6dd6e1', 'message': 'Remove legacy jobs in Craton\n\nDepends-On: I05e1df55fd6711970742ad2df8aadc65c28eca95\nNeeded-By: I5ecf23a3d12c3301f4506e3d46540ef38866f3df\n\nChange-Id: I3f2bb831925ccad1c54ace8bd6cbd38988d2809a\n'}]",1,510820,0c40016b5b31d1849aaa88408fc8d0b77e6dd6e1,12,4,2,22582,,,0,"Remove legacy jobs in Craton

Depends-On: I05e1df55fd6711970742ad2df8aadc65c28eca95
Needed-By: I5ecf23a3d12c3301f4506e3d46540ef38866f3df

Change-Id: I3f2bb831925ccad1c54ace8bd6cbd38988d2809a
",git fetch https://review.opendev.org/openstack/project-config refs/changes/20/510820/2 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/projects.yaml'],1,e7902c54329194dd356558416120f453d95f7ed7,migrate-to-zuulv3,, - legacy-craton-tox-functional - legacy-craton-tox-functional,0,2
openstack%2Fproject-config~master~I9f3b200700363810a61ebf12dfe4eed1f4613be4,openstack/project-config,master,I9f3b200700363810a61ebf12dfe4eed1f4613be4,Added Freezer tempest plugin repo,ABANDONED,2017-06-09 14:44:08.000000000,2017-12-16 21:37:15.000000000,,[{'_account_id': 6547}],"[{'number': 1, 'created': '2017-06-09 14:44:08.000000000', 'files': ['gerritbot/channels.yaml', 'jenkins/jobs/projects.yaml', 'gerrit/projects.yaml', 'gerrit/acls/openstack/freezer-tempest-plugin.config', 'zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/d2451c2027efe2560b33244b9b90d9de418bcd37', 'message': 'Added Freezer tempest plugin repo\n\n   * openstack/freezer-tempest-plugin\n\nChange-Id: I9f3b200700363810a61ebf12dfe4eed1f4613be4\n'}]",0,472710,d2451c2027efe2560b33244b9b90d9de418bcd37,3,1,1,13940,,,0,"Added Freezer tempest plugin repo

   * openstack/freezer-tempest-plugin

Change-Id: I9f3b200700363810a61ebf12dfe4eed1f4613be4
",git fetch https://review.opendev.org/openstack/project-config refs/changes/10/472710/1 && git format-patch -1 --stdout FETCH_HEAD,"['gerritbot/channels.yaml', 'jenkins/jobs/projects.yaml', 'gerrit/projects.yaml', 'gerrit/acls/openstack/freezer-tempest-plugin.config', 'zuul/layout.yaml']",5,d2451c2027efe2560b33244b9b90d9de418bcd37,freezer-tempest-plugin, - name: openstack/freezer-tempest-plugin template: - name: merge-check - name: check-requirements - name: publish-to-pypi - name: openstack-server-publish-jobs check: - gate-freezer-tempest-plugin-pep8-ubuntu-xenial gate: - gate-freezer-tempest-plugin-pep8-ubuntu-xenial ,,39,0
openstack%2Fproject-config~master~Ic81b6f78344a61a7ccc84fc17bf05fe22d3d9a84,openstack/project-config,master,Ic81b6f78344a61a7ccc84fc17bf05fe22d3d9a84,Add a py35 tempest job for networking midonet,ABANDONED,2017-08-03 12:04:28.000000000,2017-12-16 21:36:40.000000000,,"[{'_account_id': 3}, {'_account_id': 6547}, {'_account_id': 6854}, {'_account_id': 19307}]","[{'number': 1, 'created': '2017-08-03 12:04:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/8fa4b9438edf86e4108492c53599c85c47c4f210', 'message': ""Add a p35 tempest job for networking midonet\n\nOne of the Pike's goals is to support python3.5 in all testing jobs.\nThis patchs add 2 news tempest jobs to the project networking-midonet\nto run tempest with ml2 and ml2-full flavors of devstack.\n\nRelated-Bug: #1670255\n\nChange-Id: Ic81b6f78344a61a7ccc84fc17bf05fe22d3d9a84\nSigned-off-by: Antonio Ojea <aojea@midokura.com>\n""}, {'number': 2, 'created': '2017-08-03 12:05:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/906ecf70cdd58ffab38b589826107c4449622267', 'message': ""Add a py35 tempest job for networking midonet\n\nOne of the Pike's goals is to support python3.5 in all testing jobs.\nThis patchs add 2 news tempest jobs to the project networking-midonet\nto run tempest with ml2 and ml2-full flavors of devstack.\n\nRelated-Bug: #1670255\n\nChange-Id: Ic81b6f78344a61a7ccc84fc17bf05fe22d3d9a84\nSigned-off-by: Antonio Ojea <aojea@midokura.com>\n""}, {'number': 3, 'created': '2017-08-03 13:32:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/1622b61c20201069430329fb8ccf07e9f3a91400', 'message': ""Add a py35 tempest job for networking midonet\n\nOne of the Pike's goals is to support python3.5 in all testing jobs.\nThis patchs add 2 news tempest jobs to the project networking-midonet\nto run tempest with ml2 and ml2-full flavors of devstack.\n\nRelated-Bug: #1670255\n\nChange-Id: Ic81b6f78344a61a7ccc84fc17bf05fe22d3d9a84\nSigned-off-by: Antonio Ojea <aojea@midokura.com>\n""}, {'number': 4, 'created': '2017-08-03 14:03:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/7c8751e5a3ec2745c6d3be885ce52056d0735fbf', 'message': ""Add a py35 tempest job for networking midonet\n\nOne of the Pike's goals is to support python3.5 in all testing jobs.\nThis patchs add 2 news tempest jobs to the project networking-midonet\nto run tempest with ml2 and ml2-full flavors of devstack.\n\nRelated-Bug: #1670255\nDepends-On: If0a0ddffb1323e76ca9dc151d12f57e692d5dae5\n\nChange-Id: Ic81b6f78344a61a7ccc84fc17bf05fe22d3d9a84\nSigned-off-by: Antonio Ojea <aojea@midokura.com>\n""}, {'number': 5, 'created': '2017-08-04 09:06:28.000000000', 'files': ['jenkins/jobs/networking-midonet.yaml', 'jenkins/jobs/projects.yaml', 'zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/5ec0992578c7fbdcfd18434e13c7438651a33acd', 'message': ""Add a py35 tempest job for networking midonet\n\nOne of the Pike's goals is to support python3.5 in all testing jobs.\nThis patchs add 2 news tempest jobs to the project networking-midonet\nto run tempest with ml2 and ml2-full flavors of devstack.\n\nRelated-Bug: #1670255\nDepends-On: If0a0ddffb1323e76ca9dc151d12f57e692d5dae5\n\nChange-Id: Ic81b6f78344a61a7ccc84fc17bf05fe22d3d9a84\nSigned-off-by: Antonio Ojea <aojea@midokura.com>\n""}]",5,490447,5ec0992578c7fbdcfd18434e13c7438651a33acd,19,4,5,19307,,,0,"Add a py35 tempest job for networking midonet

One of the Pike's goals is to support python3.5 in all testing jobs.
This patchs add 2 news tempest jobs to the project networking-midonet
to run tempest with ml2 and ml2-full flavors of devstack.

Related-Bug: #1670255
Depends-On: If0a0ddffb1323e76ca9dc151d12f57e692d5dae5

Change-Id: Ic81b6f78344a61a7ccc84fc17bf05fe22d3d9a84
Signed-off-by: Antonio Ojea <aojea@midokura.com>
",git fetch https://review.opendev.org/openstack/project-config refs/changes/47/490447/4 && git format-patch -1 --stdout FETCH_HEAD,"['jenkins/jobs/networking-midonet.yaml', 'jenkins/jobs/projects.yaml', 'zuul/layout.yaml']",3,8fa4b9438edf86e4108492c53599c85c47c4f210,bug/1670255, - gate-tempest-dsvm-networking-midonet-python3-ml2-full-ubuntu-xenial-nv - gate-tempest-dsvm-networking-midonet-python3-ml2-ubuntu-xenial-nv,,62,0
openstack%2Fproject-config~master~I856c98938ebb35a076f73e08e119c2f0353b9749,openstack/project-config,master,I856c98938ebb35a076f73e08e119c2f0353b9749,Add Kolla Ansible Plugins project,ABANDONED,2017-08-07 09:23:34.000000000,2017-12-16 21:34:29.000000000,,"[{'_account_id': 3}, {'_account_id': 5263}, {'_account_id': 6547}, {'_account_id': 10787}, {'_account_id': 19741}, {'_account_id': 22582}]","[{'number': 1, 'created': '2017-08-07 09:23:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/4b15267bc67c456da1728419fe70d3fb770140b5', 'message': 'Add Kolla Ansible Plugins project\n\n* kolla-ansible-plugins\n\nChange-Id: I856c98938ebb35a076f73e08e119c2f0353b9749\nDepends-On: Ic79c70abeb0fe60f8d2e85cfe8b9d6677a26e14a\n'}, {'number': 2, 'created': '2017-08-07 10:06:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/bc7c85e7261b32d10b4c8c1bcf82397613f74364', 'message': 'Add Kolla Ansible Plugins project\n\n* kolla-ansible-plugins\n\nThis project is created as result from Kolla IRC meeting [1]\n\n[1] http://eavesdrop.openstack.org/meetings/kolla/2017/kolla.2017-08-02-15.59.log.html#l-78\n\nChange-Id: I856c98938ebb35a076f73e08e119c2f0353b9749\nDepends-On: Ic79c70abeb0fe60f8d2e85cfe8b9d6677a26e14a\n'}, {'number': 3, 'created': '2017-08-09 01:18:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/5a727ce6f05ee733fdc9185b01c11304e789025d', 'message': 'Add Kolla Ansible Plugins project\n\n* kolla-ansible-plugins\n\nThis project is created as result from Kolla IRC meeting [1]\n\n[1] http://eavesdrop.openstack.org/meetings/kolla/2017/kolla.2017-08-02-15.59.log.html#l-78\n\nChange-Id: I856c98938ebb35a076f73e08e119c2f0353b9749\nNeeded-By: Ic79c70abeb0fe60f8d2e85cfe8b9d6677a26e14a\n'}, {'number': 4, 'created': '2017-08-17 07:55:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/5b93d95830a6f8c4aba06a372d223f1c2e2bec0a', 'message': 'Add Kolla Ansible Plugins project\n\n* kolla-ansible-plugins\n\nThis project is created as result from Kolla IRC meeting [1]\n\n[1] http://eavesdrop.openstack.org/meetings/kolla/2017/kolla.2017-08-02-15.59.log.html#l-78\n\nChange-Id: I856c98938ebb35a076f73e08e119c2f0353b9749\nNeeded-By: Ic79c70abeb0fe60f8d2e85cfe8b9d6677a26e14a\n'}, {'number': 5, 'created': '2017-08-17 20:47:27.000000000', 'files': ['gerrit/acls/openstack/kolla-ansible-plugins.config', 'gerrit/projects.yaml', 'zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/edb96a5d4ea9ea64225bc9c2c86e402d23c3bb1b', 'message': 'Add Kolla Ansible Plugins project\n\n* kolla-ansible-plugins\n\nThis project is created as result from Kolla IRC meeting [1]\n\n[1] http://eavesdrop.openstack.org/meetings/kolla/2017/kolla.2017-08-02-15.59.log.html#l-78\n\nChange-Id: I856c98938ebb35a076f73e08e119c2f0353b9749\nNeeded-By: Ic79c70abeb0fe60f8d2e85cfe8b9d6677a26e14a\n'}]",9,491412,edb96a5d4ea9ea64225bc9c2c86e402d23c3bb1b,21,6,5,22582,,,0,"Add Kolla Ansible Plugins project

* kolla-ansible-plugins

This project is created as result from Kolla IRC meeting [1]

[1] http://eavesdrop.openstack.org/meetings/kolla/2017/kolla.2017-08-02-15.59.log.html#l-78

Change-Id: I856c98938ebb35a076f73e08e119c2f0353b9749
Needed-By: Ic79c70abeb0fe60f8d2e85cfe8b9d6677a26e14a
",git fetch https://review.opendev.org/openstack/project-config refs/changes/12/491412/2 && git format-patch -1 --stdout FETCH_HEAD,"['gerrit/acls/kolla-ansible-plugins.config', 'gerrit/acls/openstack/kolla-ansible-plugins.config', 'jenkins/jobs/projects.yaml', 'gerrit/projects.yaml', 'zuul/layout.yaml']",5,4b15267bc67c456da1728419fe70d3fb770140b5,new-project, - name: openstack/kolla-ansible-plugins template: - name: merge-check - name: noop-jobs ,,40,0
openstack%2Fproject-config~master~Ibe579abb30510dd7b86cf1664a192b2505d66c99,openstack/project-config,master,Ibe579abb30510dd7b86cf1664a192b2505d66c99,Jobs to move temporary tarballs to regularc,ABANDONED,2017-06-23 17:47:06.000000000,2017-12-16 21:32:58.000000000,,"[{'_account_id': 3}, {'_account_id': 6547}, {'_account_id': 10787}]","[{'number': 1, 'created': '2017-06-23 17:47:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/b76769496c2690c008f9a0dc4301cf7db5b131d1', 'message': 'Turn publish jobs to periodic\n\nAlso add remaining distros and move target of save to regular images\n\nChange-Id: Ibe579abb30510dd7b86cf1664a192b2505d66c99\n'}, {'number': 2, 'created': '2017-06-23 17:49:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/06e3a62045425bd0e6add409fc48829f75501a7e', 'message': 'Turn publish jobs to periodic\n\nAlso add remaining distros and move target of save to regular images\n\nDepends-on: If578edcd4e431c0b93cb9cedcc3d9db47d41a637\nChange-Id: Ibe579abb30510dd7b86cf1664a192b2505d66c99\n'}, {'number': 3, 'created': '2017-06-23 19:07:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/ede112d9a4cdfe51959c953ccd9cbcf4dffcef65', 'message': 'Turn publish jobs to periodic\n\nAlso add remaining distros and move target of save to regular images\n\nDepends-on: If578edcd4e431c0b93cb9cedcc3d9db47d41a637\nChange-Id: Ibe579abb30510dd7b86cf1664a192b2505d66c99\n'}, {'number': 4, 'created': '2017-07-19 15:50:24.000000000', 'files': ['jenkins/jobs/kolla.yaml', 'jenkins/jobs/projects.yaml', 'zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/369500bf2775cf19f41bee1c038d30be61155744', 'message': 'Jobs to move temporary tarballs to regularc\n\nAfter this one we can make this job periodic and turn off per-commit\nsaving\n\nDepends-on: If578edcd4e431c0b93cb9cedcc3d9db47d41a637\nChange-Id: Ibe579abb30510dd7b86cf1664a192b2505d66c99\n'}]",0,476989,369500bf2775cf19f41bee1c038d30be61155744,11,3,4,10787,,,0,"Jobs to move temporary tarballs to regularc

After this one we can make this job periodic and turn off per-commit
saving

Depends-on: If578edcd4e431c0b93cb9cedcc3d9db47d41a637
Change-Id: Ibe579abb30510dd7b86cf1664a192b2505d66c99
",git fetch https://review.opendev.org/openstack/project-config refs/changes/89/476989/4 && git format-patch -1 --stdout FETCH_HEAD,"['jenkins/jobs/kolla.yaml', 'jenkins/jobs/projects.yaml', 'zuul/layout.yaml']",3,b76769496c2690c008f9a0dc4301cf7db5b131d1,, periodic: - publish-kolla-dsvm-build-ubuntu-binary-ubuntu-xenial: - publish-kolla-dsvm-ansible-deploy-multinode-ubuntu-binary-ubuntu-xenial-2-node: - publish-kolla-dsvm-kubernetes-deploy-multinode-ubuntu-binary-ubuntu-xenial-2-node: - publish-kolla-dsvm-save-ubuntu-binary-ubuntu-xenial - publish-kolla-dsvm-build-centos-source-centos-7: - publish-kolla-dsvm-ansible-deploy-multinode-centos-source-centos-7-2-node: - publish-kolla-dsvm-kubernetes-deploy-multinode-centos-source-centos-7-2-node: - publish-kolla-dsvm-save-centos-source-centos-7 - publish-kolla-dsvm-build-centos-binary-centos-7: - publish-kolla-dsvm-ansible-deploy-multinode-centos-binary-centos-7-2-node: - publish-kolla-dsvm-kubernetes-deploy-multinode-centos-binary-centos-7-2-node: - publish-kolla-dsvm-save-centos-binary-centos-7 - publish-kolla-dsvm-build-oraclelinux-source-centos-7: - publish-kolla-dsvm-ansible-deploy-multinode-oraclelinux-source-centos-7-2-node: - publish-kolla-dsvm-kubernetes-deploy-multinode-oraclelinux-source-centos-7-2-node: - publish-kolla-dsvm-save-oraclelinux-source-centos-7 - publish-kolla-dsvm-build-oraclelinux-binary-centos-7: - publish-kolla-dsvm-ansible-deploy-multinode-oraclelinux-binary-centos-7-2-node: - publish-kolla-dsvm-kubernetes-deploy-multinode-oraclelinux-binary-centos-7-2-node: - publish-kolla-dsvm-save-oraclelinux-binary-centos-7, experimental:,84,4
openstack%2Fproject-config~master~Id318296425bd0a01057864571cc39c4ddb3a8948,openstack/project-config,master,Id318296425bd0a01057864571cc39c4ddb3a8948,Add new repo for networking-sfc tempest plugin,ABANDONED,2017-10-09 12:28:29.000000000,2017-12-16 21:32:36.000000000,,"[{'_account_id': 3}, {'_account_id': 748}, {'_account_id': 4694}, {'_account_id': 6547}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-10-09 12:28:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/2c91ea5ed5510a0f84beb2daf783cd56a56870e0', 'message': 'Add new repo for networking-sfc tempest plugin\n\nFor the Queens ""Split Tempest Plugins into Separate Repos/Projects""\ngoal, this imports from  a temporary github repository that preserves\nthe history of the plugin and additionally applies the openstack\ncookiecutter to it.\n\nNeeded-By: Id1f964ed801bd8f041705b7b04a7e74e8cae16d0\nChange-Id: Id318296425bd0a01057864571cc39c4ddb3a8948\n'}, {'number': 2, 'created': '2017-10-09 13:12:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/6d891156bce6132bae07d9e26dba380e2a5630d2', 'message': 'Add new repo for networking-sfc tempest plugin\n\nFor the Queens ""Split Tempest Plugins into Separate Repos/Projects""\ngoal, this imports from  a temporary github repository that preserves\nthe history of the plugin and additionally applies the openstack\ncookiecutter to it.\n\nNeeded-By: Id1f964ed801bd8f041705b7b04a7e74e8cae16d0\nChange-Id: Id318296425bd0a01057864571cc39c4ddb3a8948\n'}, {'number': 3, 'created': '2017-10-09 13:52:12.000000000', 'files': ['gerritbot/channels.yaml', 'zuul.d/projects.yaml', 'zuul/main.yaml', 'gerrit/projects.yaml', 'zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/09fbfddceed3c74243ce8209f53168ccbc6b745d', 'message': 'Add new repo for networking-sfc tempest plugin\n\nFor the Queens ""Split Tempest Plugins into Separate Repos/Projects""\ngoal, this imports from  a temporary github repository that preserves\nthe history of the plugin and additionally applies the openstack\ncookiecutter to it.\n\nNeeded-By: Id1f964ed801bd8f041705b7b04a7e74e8cae16d0\nChange-Id: Id318296425bd0a01057864571cc39c4ddb3a8948\n'}]",0,510553,09fbfddceed3c74243ce8209f53168ccbc6b745d,14,6,3,21798,,,0,"Add new repo for networking-sfc tempest plugin

For the Queens ""Split Tempest Plugins into Separate Repos/Projects""
goal, this imports from  a temporary github repository that preserves
the history of the plugin and additionally applies the openstack
cookiecutter to it.

Needed-By: Id1f964ed801bd8f041705b7b04a7e74e8cae16d0
Change-Id: Id318296425bd0a01057864571cc39c4ddb3a8948
",git fetch https://review.opendev.org/openstack/project-config refs/changes/53/510553/1 && git format-patch -1 --stdout FETCH_HEAD,"['gerritbot/channels.yaml', 'gerrit/projects.yaml', 'zuul.d/projects.yaml', 'zuul/main.yaml', 'zuul/layout.yaml']",5,2c91ea5ed5510a0f84beb2daf783cd56a56870e0,goal-split-tempest-plugins, - name: openstack/networking-sfc-tempest-plugin template: - name: merge-check - name: python-jobs - name: openstack-unified-publish-jobs - name: check-requirements - name: publish-to-pypi - name: python35-jobs check: - gate-tempest-dsvm-networking-sfc-ubuntu-xenial - gate-tempest-dsvm-networking-sfc-multinode-ubuntu-xenial-nv gate: - gate-networking-sfc-functional-dsvm-ubuntu-xenial - gate-tempest-dsvm-networking-sfc-ubuntu-xenial ,,25,0
openstack%2Fproject-config~master~Iea210491be3892c7c2b8bc9473a4a5ce96c6bf2d,openstack/project-config,master,Iea210491be3892c7c2b8bc9473a4a5ce96c6bf2d,Remove legacy jobs in Freezer,ABANDONED,2017-10-11 06:48:23.000000000,2017-12-16 21:28:19.000000000,,"[{'_account_id': 3}, {'_account_id': 8556}, {'_account_id': 11278}, {'_account_id': 22348}, {'_account_id': 22582}]","[{'number': 1, 'created': '2017-10-11 06:48:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/450f3c9e81ebc53ede25795bcce73eca2e41d3f7', 'message': 'Remove legacy jobs in Freezer\n\nDepends-On: I5d52c17734bbea3482e8b8831bdd13ea6d50d54a\nDepends-On: Ic3583bdee237a099fe932ea9be2b1ee9e66e8c94\nDepends-On: If4a85f32a0ab31eee80f6e6f931933e6710e01b8\nDepends-On: I764168378ea36d8bd757fccbb2a2d1f35f158733\nNeeded-By: I31e893beb4363e783d1ea8d7d9440e98b3c7a07b\n\nChange-Id: Iea210491be3892c7c2b8bc9473a4a5ce96c6bf2d\n'}, {'number': 2, 'created': '2017-10-16 04:18:55.000000000', 'files': ['zuul.d/projects.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/e3abe1015d886a1678496336653804ca55f57940', 'message': 'Remove legacy jobs in Freezer\n\nDepends-On: I5d52c17734bbea3482e8b8831bdd13ea6d50d54a\nDepends-On: Ic3583bdee237a099fe932ea9be2b1ee9e66e8c94\nDepends-On: I764168378ea36d8bd757fccbb2a2d1f35f158733\nNeeded-By: I31e893beb4363e783d1ea8d7d9440e98b3c7a07b\n\nChange-Id: Iea210491be3892c7c2b8bc9473a4a5ce96c6bf2d\n'}]",6,511152,e3abe1015d886a1678496336653804ca55f57940,10,5,2,22582,,,0,"Remove legacy jobs in Freezer

Depends-On: I5d52c17734bbea3482e8b8831bdd13ea6d50d54a
Depends-On: Ic3583bdee237a099fe932ea9be2b1ee9e66e8c94
Depends-On: I764168378ea36d8bd757fccbb2a2d1f35f158733
Needed-By: I31e893beb4363e783d1ea8d7d9440e98b3c7a07b

Change-Id: Iea210491be3892c7c2b8bc9473a4a5ce96c6bf2d
",git fetch https://review.opendev.org/openstack/project-config refs/changes/52/511152/2 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/projects.yaml'],1,450f3c9e81ebc53ede25795bcce73eca2e41d3f7,migrate-to-zuulv3,, - legacy-osbackup-freezer-dsvm - legacy-osbackup-freezer-dsvm-centos-7: voting: false - legacy-osbackup-freezer-dsvm experimental: jobs: - legacy-osbackup-freezer-dsvm-opensuse-423: voting: false - legacy-osbackup-freezer-api-dsvm-centos-7: voting: false - legacy-osbackup-freezer-api-dsvm - legacy-osbackup-freezer-api-dsvm - legacy-osbackup-freezer-web-ui-dsvm - legacy-osbackup-freezer-web-ui-dsvm,0,14
openstack%2Fopenstack-helm-infra~master~I0a41bfc3edc17725ca7b9a056925aea9311f9b28,openstack/openstack-helm-infra,master,I0a41bfc3edc17725ca7b9a056925aea9311f9b28,DNM/WIP: Kubernetes 1.9 support,ABANDONED,2017-12-07 15:41:40.000000000,2017-12-16 20:14:02.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2017-12-07 15:41:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/145f7daa603bd1939067976f97ec1b44947456d0', 'message': 'DNM/WIP: Kubernetes 1.9 support\n\nChange-Id: I0a41bfc3edc17725ca7b9a056925aea9311f9b28\n'}, {'number': 2, 'created': '2017-12-15 13:59:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/3630e7b03f70e5115be6f58ce4c72c07cea79128', 'message': 'DNM/WIP: Kubernetes 1.9 support\n\nChange-Id: I0a41bfc3edc17725ca7b9a056925aea9311f9b28\n'}, {'number': 3, 'created': '2017-12-16 20:11:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/d78ef3c66ec28f36272c65a0267984c13faa0e3d', 'message': 'DNM/WIP: Kubernetes 1.9 support\n\nChange-Id: I0a41bfc3edc17725ca7b9a056925aea9311f9b28\n'}]",0,526434,d78ef3c66ec28f36272c65a0267984c13faa0e3d,6,1,3,23928,,,0,"DNM/WIP: Kubernetes 1.9 support

Change-Id: I0a41bfc3edc17725ca7b9a056925aea9311f9b28
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/34/526434/3 && git format-patch -1 --stdout FETCH_HEAD,['tools/gate/playbooks/vars.yaml'],1,145f7daa603bd1939067976f97ec1b44947456d0,k8s/1.9.0, kubernetes: v1.9.0-beta.1, kubernetes: v1.8.3,1,1
openstack%2Fnova~stable%2Focata~I8849ae0f54605e003d5b294ca3d66dcef89d7d27,openstack/nova,stable/ocata,I8849ae0f54605e003d5b294ca3d66dcef89d7d27,Call terminate_connection when shelve_offloading,MERGED,2017-09-14 22:46:42.000000000,2017-12-16 17:49:38.000000000,2017-12-16 17:49:38.000000000,"[{'_account_id': 3}, {'_account_id': 6873}, {'_account_id': 9732}, {'_account_id': 10118}, {'_account_id': 10135}, {'_account_id': 10385}, {'_account_id': 12898}, {'_account_id': 14595}, {'_account_id': 16128}, {'_account_id': 22348}, {'_account_id': 26515}, {'_account_id': 27272}]","[{'number': 1, 'created': '2017-09-14 22:46:42.000000000', 'files': ['nova/tests/unit/compute/test_shelve.py', 'nova/compute/manager.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/286fa12ad128ad22d2e9c5002c54dfdb54faac16', 'message': 'Call terminate_connection when shelve_offloading\n\nWhen nova performs a shelve offload for an instance, it needs to terminate\nall the volume connections for that instance as with the shelve offload\nit is not guaranteed that the instance will be placed on the same host once\nit gets unshelved.\nThis change adds the call to the terminate_volume_connections on the\n_shelve_offload_instance method in the compute manager.\n\nCloses-Bug: #1547142\n\nConflicts:\n      nova/tests/unit/compute/test_shelve.py\n\nNOTE(mriedem): The conflicts in the test are just due to not having\nresource allocation cleanup for placement in shelve offload in Ocata.\n\nChange-Id: I8849ae0f54605e003d5b294ca3d66dcef89d7d27\n(cherry picked from commit e89e1bdc60211622440c964f8be8563da89341ac)\n(cherry picked from commit 8365eb6cb987c834b1f35c04be13aa97db36a4a1)\n'}]",0,504273,286fa12ad128ad22d2e9c5002c54dfdb54faac16,22,12,1,6873,,,0,"Call terminate_connection when shelve_offloading

When nova performs a shelve offload for an instance, it needs to terminate
all the volume connections for that instance as with the shelve offload
it is not guaranteed that the instance will be placed on the same host once
it gets unshelved.
This change adds the call to the terminate_volume_connections on the
_shelve_offload_instance method in the compute manager.

Closes-Bug: #1547142

Conflicts:
      nova/tests/unit/compute/test_shelve.py

NOTE(mriedem): The conflicts in the test are just due to not having
resource allocation cleanup for placement in shelve offload in Ocata.

Change-Id: I8849ae0f54605e003d5b294ca3d66dcef89d7d27
(cherry picked from commit e89e1bdc60211622440c964f8be8563da89341ac)
(cherry picked from commit 8365eb6cb987c834b1f35c04be13aa97db36a4a1)
",git fetch https://review.opendev.org/openstack/nova refs/changes/73/504273/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/compute/test_shelve.py', 'nova/compute/manager.py']",2,286fa12ad128ad22d2e9c5002c54dfdb54faac16,bug/1547142," bdms = objects.BlockDeviceMappingList.get_by_instance_uuid( context, instance.uuid) instance, bdms=bdms) # the instance is going to be removed from the host so we want to # terminate all the connections with the volume server and the host self._terminate_volume_connections(context, instance, bdms) ", instance),19,3
openstack%2Fironic~master~I0efd83426fa745b925915e21905e4a8f53dcdba2,openstack/ironic,master,I0efd83426fa745b925915e21905e4a8f53dcdba2,Add new endpoints for power state to api-ref,NEW,2017-01-31 02:29:00.000000000,2017-12-16 17:48:14.000000000,,"[{'_account_id': 3}, {'_account_id': 3211}, {'_account_id': 10118}, {'_account_id': 10206}, {'_account_id': 10379}, {'_account_id': 11076}, {'_account_id': 11297}, {'_account_id': 11878}, {'_account_id': 12356}, {'_account_id': 13689}, {'_account_id': 13719}, {'_account_id': 14525}, {'_account_id': 14629}, {'_account_id': 17814}, {'_account_id': 19339}, {'_account_id': 22348}, {'_account_id': 22724}, {'_account_id': 24828}, {'_account_id': 27039}]","[{'number': 1, 'created': '2017-01-31 02:29:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e6ab6a029a678e2a68a48e13309d09695c339677', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation with supported_power_state\nfield in response of ""Create Node"", ""List Nodes"", ""List Nodes\nDetailed"", ""Show Node Details"", ""Update Node"" and ""Node State\nSummary"", which were added in API microversion 1.30.\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 2, 'created': '2017-01-31 05:08:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7beea1390af52b94824c32ea88eff0fd3886c0cd', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation with supported_power_state\nfield in response of ""Create Node"", ""List Nodes"", ""List Nodes\nDetailed"", ""Show Node Details"", ""Update Node"" and ""Node State\nSummary"", which were added in API microversion 1.30.\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 3, 'created': '2017-02-01 02:46:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1531a102e36edec38aa6330777fc8f164459ed6f', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation with supported_power_state\nfield in response of ""Create Node"", ""List Nodes"", ""List Nodes\nDetailed"", ""Show Node Details"", ""Update Node"" and ""Node State\nSummary"", which were added in API microversion 1.32.\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 4, 'created': '2017-02-02 03:01:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0dbdf2f8182776239ea5becdc9adbd7d9056fa67', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation with supported_power_state\nfield in response of ""Create Node"", ""List Nodes"", ""List Nodes\nDetailed"", ""Show Node Details"", ""Update Node"" and ""Node State\nSummary"", which were added in API microversion 1.32.\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 5, 'created': '2017-02-06 02:30:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/4fdea4d02720b25702e9f555ab358dbd9cd0d2aa', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nresponse field in the following Ironic APIs. This feature is supported\nin Ironic API microversion 1.32 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 6, 'created': '2017-02-13 01:50:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ea4bbfec5a72f76c28d73b7fdbb4467ebbf8c488', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nresponse field in the following Ironic APIs. This feature is supported\nin Ironic API microversion 1.32 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 7, 'created': '2017-02-14 06:52:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9eb73ab3d83d1e8b9961271f21094e9d159072a2', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nfield in the responses of the requests to the following Ironic APIs.\nThis feature is supported in Ironic API microversion 1.32 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 8, 'created': '2017-02-16 02:29:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/607a61b294f8df9c55cbb4e407ccd84ab11aecce', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nfield in the responses of the requests to the following Ironic APIs.\nThis feature is supported in Ironic API microversion 1.32 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 9, 'created': '2017-02-17 00:51:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f72f9bf286df80cdc3e4e6a127ec6fbaeef4f34b', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nfield in the responses of the requests to the following Ironic APIs.\nThis feature is supported in Ironic API microversion 1.32 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 10, 'created': '2017-02-27 04:22:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1712fa9c34c4209ff2879eae5c0cd005090f5734', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nfield in the responses of the requests to the following Ironic APIs.\nThis feature is supported in Ironic API microversion 1.32 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 11, 'created': '2017-04-17 12:50:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/71ffcc3b792b16449045b4090ae5315a5f8beb1d', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nfield in the responses of the requests to the following Ironic APIs.\nThis feature is supported in Ironic API microversion 1.32 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 12, 'created': '2017-07-28 13:37:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/77a665e5df47e0694ef852e811707661f44b2772', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nfield in the responses of the requests to the following Ironic APIs.\nThis feature is supported in Ironic API microversion 1.35 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 13, 'created': '2017-08-08 14:41:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/855dd30cd53ddcebdcb400f08e62c368a74bcd6f', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nfield in the responses of the requests to the following Ironic APIs.\nThis feature is supported in Ironic API microversion 1.35 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 14, 'created': '2017-08-29 13:05:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d4961402b1cb77e85a1f7acec4502d7efa1c0022', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nfield in the responses of the requests to the following Ironic APIs.\nThis feature is supported in Ironic API microversion 1.35 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\n'}, {'number': 15, 'created': '2017-09-07 02:55:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8f09aee91dc68ebc49f712c20145442498380c66', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nfield in the responses of the requests to the following Ironic APIs.\nThis feature is supported in Ironic API microversion 1.35 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\nCo-Authored-By: Hironori Shiina <shiina.hironori@jp.fujitsu.com>\n'}, {'number': 16, 'created': '2017-09-08 11:25:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/17b5b3850f8bcc6eb98bc5348c31012134de4064', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nfield in the responses of the requests to the following Ironic APIs.\nThis feature is supported in Ironic API microversion 1.35 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\nCo-Authored-By: Hironori Shiina <shiina.hironori@jp.fujitsu.com>\n'}, {'number': 17, 'created': '2017-11-14 05:26:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6fdafbabcab722c88772a1600c9c341ec409d4ab', 'message': 'Add supported_power_states field to api-ref\n\nThis patch updates api-ref documentation to add ``supported_power_state``\nfield in the responses of the requests to the following Ironic APIs.\nThis feature is supported in Ironic API microversion 1.35 and above.\n\n    * Create Node, `/v1/nodes`\n    * List Nodes Detailed, `/v1/nodes/detail`\n    * Show Node Details, `/v1/nodes/{node_ident}`\n    * Update Node, `/v1/nodes/{node_ident}`\n    * Node State Summary, `/v1/nodes/{node_ident}/states`\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: #1526226\nCo-Authored-By: Hironori Shiina <shiina.hironori@jp.fujitsu.com>\n'}, {'number': 18, 'created': '2017-12-16 16:01:47.000000000', 'files': ['api-ref/source/parameters.yaml', 'api-ref/regenerate-samples.sh', 'api-ref/source/baremetal-api-v1-node-management.inc', 'api-ref/source/samples/node-get-supported-power-states-response.json', 'api-ref/source/samples/node-get-power-state-response.json'], 'web_link': 'https://opendev.org/openstack/ironic/commit/5a7f904bf918966ca6c30e38b20f7c1a0f505180', 'message': 'Add new endpoints for power state to api-ref\n\nThis patch adds new api endpoints api-ref. These are for showing the\ncurrent power state and the supported power states of a node as\nfollows:\n - GET /v1/nodes/<node>/states/power\n - GET /v1/nodes/<node>/states/power/supported\nThese endpoints are available from API version 1.37.\n\nChange-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2\nPartial-Bug: 1734827\nCo-Authored-By: Hironori Shiina <shiina.hironori@jp.fujitsu.com>\n'}]",9,427000,5a7f904bf918966ca6c30e38b20f7c1a0f505180,116,19,18,13719,,,0,"Add new endpoints for power state to api-ref

This patch adds new api endpoints api-ref. These are for showing the
current power state and the supported power states of a node as
follows:
 - GET /v1/nodes/<node>/states/power
 - GET /v1/nodes/<node>/states/power/supported
These endpoints are available from API version 1.37.

Change-Id: I0efd83426fa745b925915e21905e4a8f53dcdba2
Partial-Bug: 1734827
Co-Authored-By: Hironori Shiina <shiina.hironori@jp.fujitsu.com>
",git fetch https://review.opendev.org/openstack/ironic refs/changes/00/427000/10 && git format-patch -1 --stdout FETCH_HEAD,"['api-ref/source/parameters.yaml', 'api-ref/source/samples/node-get-state-response.json', 'api-ref/source/samples/node-update-driver-info-response.json', 'api-ref/source/baremetal-api-v1-node-management.inc', 'api-ref/source/baremetal-api-v1-nodes.inc', 'api-ref/source/samples/nodes-list-details-response.json', 'api-ref/source/samples/nodes-list-response.json', 'api-ref/source/samples/node-show-response.json', 'api-ref/source/samples/api-root-response.json', 'api-ref/source/samples/node-create-response.json']",10,e6ab6a029a678e2a68a48e13309d09695c339677,bug/1734827," ""supported_power_states"": [ ""power on"", ""power off"", ""rebooting"", ""soft rebooting"", ""soft power off"" ],",,67,3
openstack%2Fnova~master~Ibc2285102f5c23e38badfcf1ed6dba073704a213,openstack/nova,master,Ibc2285102f5c23e38badfcf1ed6dba073704a213,Updated from global requirements,MERGED,2017-12-15 21:49:55.000000000,2017-12-16 17:08:12.000000000,2017-12-16 00:17:16.000000000,"[{'_account_id': 679}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15286}, {'_account_id': 16128}, {'_account_id': 16898}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26515}]","[{'number': 1, 'created': '2017-12-15 21:49:55.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/nova/commit/0fd2b6746f7e22e51feb10585cb7c33109687be3', 'message': 'Updated from global requirements\n\nChange-Id: Ibc2285102f5c23e38badfcf1ed6dba073704a213\n'}]",0,528421,0fd2b6746f7e22e51feb10585cb7c33109687be3,15,11,1,11131,,,0,"Updated from global requirements

Change-Id: Ibc2285102f5c23e38badfcf1ed6dba073704a213
",git fetch https://review.opendev.org/openstack/nova refs/changes/21/528421/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,0fd2b6746f7e22e51feb10585cb7c33109687be3,openstack/requirements,"oslo.service!=1.28.1,>=1.24.0 # Apache-2.0",oslo.service>=1.24.0 # Apache-2.0,1,1
openstack%2Fkolla~master~I4ba0159a1068d7ecdaa0551c609db81c19a2ca4e,openstack/kolla,master,I4ba0159a1068d7ecdaa0551c609db81c19a2ca4e,Configure the name of network interface manually,ABANDONED,2016-10-23 02:23:17.000000000,2017-12-16 16:31:40.000000000,,"[{'_account_id': 3}, {'_account_id': 1390}, {'_account_id': 19316}, {'_account_id': 22582}]","[{'number': 1, 'created': '2016-10-23 02:23:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/71287c036b0b5b57330bd9c60bc16860466fd282', 'message': 'Configure the name of network interface manually\n\nThe name of network interface may different ,we should\nconfigure it manually.\n\nTrivialFix\n\nChange-Id: I4ba0159a1068d7ecdaa0551c609db81c19a2ca4e\n'}, {'number': 2, 'created': '2016-10-23 02:33:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/e3bd22d7d4ca2cee2e22a06e377bcd7077cd299f', 'message': 'Configure the name of network interface manually\n\nThe name of network interface may different ,we should\nconfigure it manually.\n\nTrivialFix\n\nChange-Id: I4ba0159a1068d7ecdaa0551c609db81c19a2ca4e\n'}, {'number': 3, 'created': '2016-10-25 12:37:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/488f3ccec6d722a62fc6ee0d3fc03643a55444e0', 'message': 'Configure the name of network interface manually\n\nThe name of network interface may different ,we should\nconfigure it manually.\n\nTrivialFix\n\nChange-Id: I4ba0159a1068d7ecdaa0551c609db81c19a2ca4e\n'}, {'number': 4, 'created': '2016-10-25 12:39:30.000000000', 'files': ['contrib/dev/vagrant/bootstrap.sh'], 'web_link': 'https://opendev.org/openstack/kolla/commit/77c7720e616f3295ff23426d6f18d4c0f588baaa', 'message': 'Configure the name of network interface manually\n\nThe name of network interface may different, we should\nconfigure it manually.\n\nTrivialFix\n\nChange-Id: I4ba0159a1068d7ecdaa0551c609db81c19a2ca4e\n'}]",1,390032,77c7720e616f3295ff23426d6f18d4c0f588baaa,13,4,4,22165,,,0,"Configure the name of network interface manually

The name of network interface may different, we should
configure it manually.

TrivialFix

Change-Id: I4ba0159a1068d7ecdaa0551c609db81c19a2ca4e
",git fetch https://review.opendev.org/openstack/kolla refs/changes/32/390032/1 && git format-patch -1 --stdout FETCH_HEAD,['contrib/dev/vagrant/bootstrap.sh'],1,71287c036b0b5b57330bd9c60bc16860466fd282,,"NETWORK_INTERFACE=$4 NEUTRON_EXTERNAL_INTERFACE=$5 sed -i -r ""s,^[# ]*network_interface:.+$,network_interface: ${NETWORK_INTERFACE},"" /etc/kolla/globals.yml sed -i -r ""s,^[# ]*neutron_external_interface:.+$,neutron_external_interface: ${NEUTRON_EXTERNAL_INTERFACE},"" /etc/kolla/globals.yml"," sed -i -r ""s,^[# ]*network_interface:.+$,network_interface: \""eth1\"","" /etc/kolla/globals.yml sed -i -r ""s,^[# ]*neutron_external_interface:.+$,neutron_external_interface: \""eth2\"","" /etc/kolla/globals.yml",5,2
openstack%2Fkolla-ansible~master~I17ec37c95fa806c6f386f34df3241b7c7aecceaa,openstack/kolla-ansible,master,I17ec37c95fa806c6f386f34df3241b7c7aecceaa,"Remove the deprecated option ""event_connection""",ABANDONED,2017-05-15 15:47:03.000000000,2017-12-16 16:22:18.000000000,,"[{'_account_id': 3}, {'_account_id': 1390}, {'_account_id': 7779}, {'_account_id': 8157}, {'_account_id': 11869}, {'_account_id': 19316}, {'_account_id': 22165}, {'_account_id': 23717}]","[{'number': 1, 'created': '2017-05-15 15:47:03.000000000', 'files': ['ansible/roles/ceilometer/templates/ceilometer.conf.j2'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/1d4d8f7723a1ef49131bf72d39ccfaeeb9b44b16', 'message': 'Remove the deprecated option ""event_connection""\n\nOption ""event_connection"" from group ""database"" is deprecated for\nremoval.  Its value may be silently ignored in the future.\n\nChange-Id: I17ec37c95fa806c6f386f34df3241b7c7aecceaa\nCloses-Bug: #1690844\n'}]",0,464673,1d4d8f7723a1ef49131bf72d39ccfaeeb9b44b16,12,8,1,22165,,,0,"Remove the deprecated option ""event_connection""

Option ""event_connection"" from group ""database"" is deprecated for
removal.  Its value may be silently ignored in the future.

Change-Id: I17ec37c95fa806c6f386f34df3241b7c7aecceaa
Closes-Bug: #1690844
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/73/464673/1 && git format-patch -1 --stdout FETCH_HEAD,['ansible/roles/ceilometer/templates/ceilometer.conf.j2'],1,1d4d8f7723a1ef49131bf72d39ccfaeeb9b44b16,bug/1690844,,event_connection = mongodb://{{ ceilometer_database_user }}:{{ ceilometer_database_password }}@{{ ceilometer_database_mongodb_address }}/{{ ceilometer_database_name }}event_connection = mysql+pymysql://{{ ceilometer_database_user }}:{{ ceilometer_database_password }}@{{ ceilometer_database_mysql_address }}:{{ ceilometer_database_port }}/{{ ceilometer_database_name }},0,2
openstack%2Fmistral~master~I62873ff21eca3b55a9b1dfc39bc31ad1d846943b,openstack/mistral,master,I62873ff21eca3b55a9b1dfc39bc31ad1d846943b,Updated from global requirements,MERGED,2017-12-15 21:38:03.000000000,2017-12-16 16:00:32.000000000,2017-12-16 16:00:32.000000000,"[{'_account_id': 9712}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 21:38:03.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/mistral/commit/f46825aad51e62ce16ec6062b899b709ad6fc182', 'message': 'Updated from global requirements\n\nChange-Id: I62873ff21eca3b55a9b1dfc39bc31ad1d846943b\n'}]",0,528409,f46825aad51e62ce16ec6062b899b709ad6fc182,6,2,1,11131,,,0,"Updated from global requirements

Change-Id: I62873ff21eca3b55a9b1dfc39bc31ad1d846943b
",git fetch https://review.opendev.org/openstack/mistral refs/changes/09/528409/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,f46825aad51e62ce16ec6062b899b709ad6fc182,openstack/requirements,"oslo.service!=1.28.1,>=1.24.0 # Apache-2.0",oslo.service>=1.24.0 # Apache-2.0,1,1
openstack%2Fhorizon~master~I360aef8c34304f3abe76a90787ab63647cd78491,openstack/horizon,master,I360aef8c34304f3abe76a90787ab63647cd78491,Do not make duplicate requests to Glance for image names in admin panel,MERGED,2017-09-26 07:18:26.000000000,2017-12-16 13:40:11.000000000,2017-12-16 13:40:11.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 1736}, {'_account_id': 8478}, {'_account_id': 22348}, {'_account_id': 26861}]","[{'number': 1, 'created': '2017-09-26 07:18:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/38dc15ccc39eceab025a9e96841e454f051153d8', 'message': ""Do not make duplicate requests to Glance for image names in admin panel\n\nAdmin panel generates duplicate (N per instance) requests to Glance for\ninstances that boot from image. The commit fixes it by getting all\nimages information in one shot.\n\nI also move the code that process non-API filters to a new function\nbecause otherwise my fix couldn't pass McCabe complexity. Project panel\nis changed similarly to make the code a bit consistent.\n\nChange-Id: I360aef8c34304f3abe76a90787ab63647cd78491\nCloses-Bug: #1711486\n""}, {'number': 2, 'created': '2017-10-05 14:58:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/dfb669179365d7dc304f32c5ca1dc58ecc9fb3c9', 'message': ""Do not make duplicate requests to Glance for image names in admin panel\n\nAdmin panel generates duplicate (N per instance) requests to Glance for\ninstances that boot from image. The commit fixes it by getting all\nimages information in one shot.\n\nI also move the code that process non-API filters to a new function\nbecause otherwise my fix couldn't pass McCabe complexity. Project panel\nis changed similarly to make the code a bit consistent.\n\nChange-Id: I360aef8c34304f3abe76a90787ab63647cd78491\nCloses-Bug: #1711486\n""}, {'number': 3, 'created': '2017-12-01 01:04:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/190a96d712c463d300a75ae0f0010122f5516c5b', 'message': 'Do not make duplicate requests to Glance for image names in admin panel\n\nAdmin panel generates duplicate (N per instance) requests to Glance for\ninstances that boot from image. The commit fixes it by getting all\nimages information in one shot.\n\nChange-Id: I360aef8c34304f3abe76a90787ab63647cd78491\nCloses-Bug: #1711486\n'}, {'number': 4, 'created': '2017-12-01 02:45:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/7986c6b20a2e90179ce41640355bf661e907a6ee', 'message': 'Do not make duplicate requests to Glance for image names in admin panel\n\nAdmin panel generates duplicate (N per instance) requests to Glance for\ninstances that boot from image. The commit fixes it by getting all\nimages information in one shot.\n\nChange-Id: I360aef8c34304f3abe76a90787ab63647cd78491\nCloses-Bug: #1711486\n'}, {'number': 5, 'created': '2017-12-16 12:41:48.000000000', 'files': ['openstack_dashboard/dashboards/admin/instances/views.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/f929d39ab471e2462a2bfcd35d880e2aafb724bb', 'message': 'Do not make duplicate requests to Glance for image names in admin panel\n\nAdmin panel generates duplicate (N per instance) requests to Glance for\ninstances that boot from image. The commit fixes it by getting all\nimages information in one shot.\n\nChange-Id: I360aef8c34304f3abe76a90787ab63647cd78491\nCloses-Bug: #1711486\n'}]",18,507388,f929d39ab471e2462a2bfcd35d880e2aafb724bb,35,6,5,26861,,,0,"Do not make duplicate requests to Glance for image names in admin panel

Admin panel generates duplicate (N per instance) requests to Glance for
instances that boot from image. The commit fixes it by getting all
images information in one shot.

Change-Id: I360aef8c34304f3abe76a90787ab63647cd78491
Closes-Bug: #1711486
",git fetch https://review.opendev.org/openstack/horizon refs/changes/88/507388/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/admin/instances/views.py', 'openstack_dashboard/dashboards/project/instances/views.py']",2,38dc15ccc39eceab025a9e96841e454f051153d8,bug/1711486," # The list describes non-API filters to be converted. Each # item is a tuple of the following format: # <fake field name>, <real field name>, <resource> # Resource is used to find the matched value of real field. naf_info = [('image_name', 'image', images), ('flavor_name', 'flavor', flavors)] if not process_non_api_filters(search_opts, naf_info):def process_non_api_filters(search_opts, naf_info): for fake_field, real_field, resources in naf_info: if fake_field in search_opts and not swap_filter( resources, search_opts, fake_field, real_field): return False return True "," if 'image_name' in search_opts and \ not swap_filter(images, search_opts, 'image_name', 'image'): self._more = False return instances elif 'flavor_name' in search_opts and \ not swap_filter(flavors, search_opts, 'flavor_name', 'flavor'):",36,22
openstack%2Fopenstack-ansible~stable%2Fnewton~Iea5f639dd7c8348167d0a7f8079de19ad5dcf8a8,openstack/openstack-ansible,stable/newton,Iea5f639dd7c8348167d0a7f8079de19ad5dcf8a8,Fix release-yaml-file-prep.py to include ansible-hardening,MERGED,2017-12-13 16:25:06.000000000,2017-12-16 13:18:01.000000000,2017-12-16 13:18:01.000000000,"[{'_account_id': 538}, {'_account_id': 2799}, {'_account_id': 6816}, {'_account_id': 22348}, {'_account_id': 23163}]","[{'number': 1, 'created': '2017-12-13 16:25:06.000000000', 'files': ['scripts/release-yaml-file-prep.py'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/64e9f2c68f26b8bfc1abebef1212712d277bcdae', 'message': ""Fix release-yaml-file-prep.py to include ansible-hardening\n\nSince openstack-ansible-security renamed to ansible-hardening we haven't\nbeen releasing/tagging ansible-hardening.\n\nThis patch changes the regex to include ansible-hardening so we will\nrelease and tag ansible-hardening as we were before.\n\nChange-Id: Iea5f639dd7c8348167d0a7f8079de19ad5dcf8a8\n(cherry picked from commit be2c81b47a0f80c223ba05d2946156158c1ddfb2)\n""}]",0,527736,64e9f2c68f26b8bfc1abebef1212712d277bcdae,11,5,1,6816,,,0,"Fix release-yaml-file-prep.py to include ansible-hardening

Since openstack-ansible-security renamed to ansible-hardening we haven't
been releasing/tagging ansible-hardening.

This patch changes the regex to include ansible-hardening so we will
release and tag ansible-hardening as we were before.

Change-Id: Iea5f639dd7c8348167d0a7f8079de19ad5dcf8a8
(cherry picked from commit be2c81b47a0f80c223ba05d2946156158c1ddfb2)
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/36/527736/1 && git format-patch -1 --stdout FETCH_HEAD,['scripts/release-yaml-file-prep.py'],1,64e9f2c68f26b8bfc1abebef1212712d277bcdae,, regex = re.compile('^.*openstack/(ansible-hardening|openstack-ansible.*)$'), regex = re.compile('^.*openstack/openstack-ansible.*$'),1,1
openstack%2Fneutron~master~If2e66e06b83e15ee2851ea2bc3b64ad366e675dd,openstack/neutron,master,If2e66e06b83e15ee2851ea2bc3b64ad366e675dd,use l3 api def from neutron-lib,MERGED,2017-01-20 17:23:12.000000000,2017-12-16 13:01:14.000000000,2017-12-16 13:01:13.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 1653}, {'_account_id': 4694}, {'_account_id': 5367}, {'_account_id': 6854}, {'_account_id': 9732}, {'_account_id': 9845}, {'_account_id': 10184}, {'_account_id': 10385}, {'_account_id': 11975}, {'_account_id': 14208}, {'_account_id': 14571}, {'_account_id': 15752}, {'_account_id': 16376}, {'_account_id': 17776}, {'_account_id': 20330}, {'_account_id': 22348}, {'_account_id': 25564}, {'_account_id': 27374}]","[{'number': 1, 'created': '2017-01-20 17:23:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/b18178c3c6de80625276bda2e68f795b0df897a3', 'message': ""Use l3 api-def from neutron-lib\n\nNeutron-lib 1.1.0 is now out and contains the l3\nAPI definition ([1]). This patch moves neutron\nreferences over to the neutron-lib version.\n\nNeutronLibImpact\n- Consumers using the public constants witin neutron's\nl3 API extension must now use the values from\nneutron-lib.\n\n[1] https://review.openstack.org/411974/\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 2, 'created': '2017-02-28 13:36:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8803e78d6dfb2e7ec1c46fbe3ed175819cbd45eb', 'message': ""Use l3 api-def from neutron-lib\n\nNeutron-lib 1.1.0 is now out and contains the l3\nAPI definition ([1]). This patch moves neutron\nreferences over to the neutron-lib version.\n\nNeutronLibImpact\n- Consumers using the public constants witin neutron's\nl3 API extension must now use the values from\nneutron-lib.\n\n[1] https://review.openstack.org/411974/\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 3, 'created': '2017-03-07 13:39:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3aa189656d226e1d40d84606bb92da63d8a49e86', 'message': ""Use l3 api-def from neutron-lib\n\nNeutron-lib 1.1.0 is now out and contains the l3\nAPI definition ([1]). This patch moves neutron\nreferences over to the neutron-lib version.\n\nNeutronLibImpact\n- Consumers using the public constants witin neutron's\nl3 API extension must now use the values from\nneutron-lib.\n\n[1] https://review.openstack.org/411974/\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 4, 'created': '2017-03-07 23:36:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/75f378bd97b243165b04d431bde4e3f80ecc80dc', 'message': ""Use l3 api-def from neutron-lib\n\nNeutron-lib 1.1.0 is now out and contains the l3\nAPI definition ([1]). This patch moves neutron\nreferences over to the neutron-lib version.\n\nNeutronLibImpact\n- Consumers using the public constants within neutron's\nl3 API extension must now use the values from\nneutron-lib. See neutron/extensions/l3.py herein.\n\n[1] https://review.openstack.org/411974/\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 5, 'created': '2017-03-14 17:51:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/2d4d20e9c6b7c8c54e76dc7288799bba1812da6d', 'message': ""Use l3 api-def from neutron-lib\n\nNeutron-lib 1.1.0 is now out and contains the l3\nAPI definition ([1]). This patch moves neutron\nreferences over to the neutron-lib version.\n\nNeutronLibImpact\n- Consumers using the public constants within neutron's\nl3 API extension must now use the values from\nneutron-lib. See neutron/extensions/l3.py herein.\n\n[1] https://review.openstack.org/411974/\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 6, 'created': '2017-03-15 15:02:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/0d10149d853c3a3a9a527a98dde81db6823f341f', 'message': ""Use l3 api-def from neutron-lib\n\nNeutron-lib 1.1.0 is now out and contains the l3\nAPI definition ([1]). This patch moves neutron\nreferences over to the neutron-lib version.\n\nNeutronLibImpact\n- Consumers using the public constants within neutron's\nl3 API extension must now use the values from\nneutron-lib. See neutron/extensions/l3.py herein.\n\n[1] https://review.openstack.org/411974/\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 7, 'created': '2017-03-30 12:22:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/cfffb67364506ef325b3221557a385392436e815', 'message': ""Use l3 api-def from neutron-lib\n\nNeutron-lib 1.1.0 is now out and contains the l3\nAPI definition ([1]). This patch moves neutron\nreferences over to the neutron-lib version.\n\nNeutronLibImpact\n- Consumers using the public constants within neutron's\nl3 API extension must now use the values from\nneutron-lib. See neutron/extensions/l3.py herein.\n\n[1] https://review.openstack.org/411974/\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 8, 'created': '2017-04-27 16:45:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/03d2e3c2a89219110263eb369172ab0a5590b827', 'message': ""use l3 api-def from neutron-lib\n\nNeutron-lib 1.1.0 is now out and contains the l3 API definition ([1]).\nThis patch moves neutron references over to the neutron-lib version.\n\nNeutronLibImpact\n- Consumers using the public constants and exceptions within neutron's\nl3 API extension must now use the values from neutron-lib.\n\n[1] I81748aa0e48b1275df3e1ea41b1d36a117d0097d\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 9, 'created': '2017-10-26 15:18:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/6fa1bfa7532d7851ef9878ee67fa5c62fdd982ce', 'message': 'use l3 api def from neutron-lib\n\nCommit I81748aa0e48b1275df3e1ea41b1d36a117d0097d added the l3 extension\nAPI definition to neutron-lib and commit\nI2324a3a02789c798248cab41c278a2d9981d24be rehomed the l3 exceptions.\nWhile Ifd79eb1a92853e49bd4ef028e7a7bd89811c6957 shims the l3\nexceptions, this patch consumes the l3 API definition from neutron-lib\nremoving the rehomed code as applicable in neutron.\n\nNeutronLibImpact\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n'}, {'number': 10, 'created': '2017-10-28 20:55:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/5a7184c90041e8b92133abaadabe54dfc044bb29', 'message': 'use l3 api def from neutron-lib\n\nCommit I81748aa0e48b1275df3e1ea41b1d36a117d0097d added the l3 extension\nAPI definition to neutron-lib and commit\nI2324a3a02789c798248cab41c278a2d9981d24be rehomed the l3 exceptions.\nWhile Ifd79eb1a92853e49bd4ef028e7a7bd89811c6957 shims the l3\nexceptions, this patch consumes the l3 API definition from neutron-lib\nremoving the rehomed code as applicable in neutron.\n\nNeutronLibImpact\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n'}, {'number': 11, 'created': '2017-11-17 17:18:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/1733c8b9dc0faaa4877bd283344f7f08eac95412', 'message': 'use l3 api def from neutron-lib\n\nCommit I81748aa0e48b1275df3e1ea41b1d36a117d0097d added the l3 extension\nAPI definition to neutron-lib and commit\nI2324a3a02789c798248cab41c278a2d9981d24be rehomed the l3 exceptions.\nWhile Ifd79eb1a92853e49bd4ef028e7a7bd89811c6957 shims the l3\nexceptions, this patch consumes the l3 API definition from neutron-lib\nremoving the rehomed code as applicable in neutron.\n\nNeutronLibImpact\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n'}, {'number': 12, 'created': '2017-11-21 22:10:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/f1be0752641ff4aecb10cf64bbab5758eb62cd2e', 'message': 'use l3 api def from neutron-lib\n\nCommit I81748aa0e48b1275df3e1ea41b1d36a117d0097d added the l3 extension\nAPI definition to neutron-lib and commit\nI2324a3a02789c798248cab41c278a2d9981d24be rehomed the l3 exceptions.\nWhile Ifd79eb1a92853e49bd4ef028e7a7bd89811c6957 shims the l3\nexceptions, this patch consumes the l3 API definition from neutron-lib\nremoving the rehomed code as applicable in neutron.\n\nNeutronLibImpact\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n'}, {'number': 13, 'created': '2017-12-01 17:22:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ee6da0f17c2dcf79ef7f587fce8bef0a98d04e09', 'message': ""use l3 api def from neutron-lib\n\nCommit I81748aa0e48b1275df3e1ea41b1d36a117d0097d added the l3 extension\nAPI definition to neutron-lib and commit\nI2324a3a02789c798248cab41c278a2d9981d24be rehomed the l3 exceptions,\nwhile Ifd79eb1a92853e49bd4ef028e7a7bd89811c6957 shims the l3\nexceptions.\n\nThis patch consumes the l3 api def by:\n- Removing the code from neutron that's now in lib.\n- Using lib's version of the code where applicable.\n- Tidying up the related unit tests as now that the l3 api def from lib\nis used the necessary fixture is already setup in the parent chain when\nsetting up the unit test class.\n\nNeutronLibImpact\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 14, 'created': '2017-12-05 21:05:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c915e85151f092680da65987abf26f428f7ae4a1', 'message': ""use l3 api def from neutron-lib\n\nCommit I81748aa0e48b1275df3e1ea41b1d36a117d0097d added the l3 extension\nAPI definition to neutron-lib and commit\nI2324a3a02789c798248cab41c278a2d9981d24be rehomed the l3 exceptions,\nwhile Ifd79eb1a92853e49bd4ef028e7a7bd89811c6957 shims the l3\nexceptions.\n\nThis patch consumes the l3 api def by:\n- Removing the code from neutron that's now in lib.\n- Using lib's version of the code where applicable.\n- Tidying up the related unit tests as now that the l3 api def from lib\nis used the necessary fixture is already setup in the parent chain when\nsetting up the unit test class.\n\nNeutronLibImpact\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 15, 'created': '2017-12-07 17:05:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8f9a7e33a1d29c30e75884157b36c2f94e7bb8b2', 'message': ""use l3 api def from neutron-lib\n\nCommit I81748aa0e48b1275df3e1ea41b1d36a117d0097d added the l3 extension\nAPI definition to neutron-lib and commit\nI2324a3a02789c798248cab41c278a2d9981d24be rehomed the l3 exceptions,\nwhile Ifd79eb1a92853e49bd4ef028e7a7bd89811c6957 shims the l3\nexceptions.\n\nThis patch consumes the l3 api def by:\n- Removing the code from neutron that's now in lib.\n- Using lib's version of the code where applicable.\n- Tidying up the related unit tests as now that the l3 api def from lib\nis used the necessary fixture is already setup in the parent chain when\nsetting up the unit test class.\n\nNeutronLibImpact\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 16, 'created': '2017-12-11 18:09:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3edf8e98d8e8d6e981d8748e5103b2e71cca466c', 'message': ""use l3 api def from neutron-lib\n\nCommit I81748aa0e48b1275df3e1ea41b1d36a117d0097d added the l3 extension\nAPI definition to neutron-lib and commit\nI2324a3a02789c798248cab41c278a2d9981d24be rehomed the l3 exceptions,\nwhile Ifd79eb1a92853e49bd4ef028e7a7bd89811c6957 shims the l3\nexceptions.\n\nThis patch consumes the l3 api def by:\n- Removing the code from neutron that's now in lib.\n- Using lib's version of the code where applicable.\n- Tidying up the related unit tests as now that the l3 api def from lib\nis used the necessary fixture is already setup in the parent chain when\nsetting up the unit test class.\n\nNeutronLibImpact\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 17, 'created': '2017-12-14 17:24:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9333290ccbba56f862d922f8f581231a614e47d9', 'message': ""use l3 api def from neutron-lib\n\nCommit I81748aa0e48b1275df3e1ea41b1d36a117d0097d added the l3 extension\nAPI definition to neutron-lib and commit\nI2324a3a02789c798248cab41c278a2d9981d24be rehomed the l3 exceptions,\nwhile Ifd79eb1a92853e49bd4ef028e7a7bd89811c6957 shims the l3\nexceptions.\n\nThis patch consumes the l3 api def by:\n- Removing the code from neutron that's now in lib.\n- Using lib's version of the code where applicable.\n- Tidying up the related unit tests as now that the l3 api def from lib\nis used the necessary fixture is already setup in the parent chain when\nsetting up the unit test class.\n\nNeutronLibImpact\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}, {'number': 18, 'created': '2017-12-15 14:39:08.000000000', 'files': ['neutron/extensions/tag_ext.py', 'neutron/tests/unit/db/test_l3_dvr_db.py', 'neutron/db/l3_gwmode_db.py', 'neutron/db/l3_hamode_db.py', 'neutron/db/l3_dvr_db.py', 'neutron/db/db_base_plugin_v2.py', 'neutron/db/l3_fip_qos.py', 'neutron/db/models/l3.py', 'neutron/tests/unit/db/test_l3_db.py', 'neutron/services/auto_allocate/db.py', 'neutron/tests/unit/extensions/test_l3_ext_gw_mode.py', 'neutron/tests/functional/services/l3_router/test_l3_dvr_ha_router_plugin.py', 'neutron/scheduler/l3_agent_scheduler.py', 'neutron/db/dns_db.py', 'neutron/tests/unit/extensions/test_qos_fip.py', 'neutron/tests/unit/extensions/test_router_availability_zone.py', 'neutron/tests/unit/extensions/test_l3.py', 'neutron/tests/unit/scheduler/test_l3_agent_scheduler.py', 'neutron/db/l3_db.py', 'neutron/db/l3_attrs_db.py', 'neutron/tests/unit/extensions/test_extraroute.py', 'neutron/api/rpc/handlers/l3_rpc.py', 'neutron/db/extraroute_db.py', 'neutron/extensions/l3.py', 'neutron/db/availability_zone/router.py', 'neutron/services/l3_router/l3_router_plugin.py', 'neutron/tests/functional/services/l3_router/test_l3_dvr_router_plugin.py', 'neutron/tests/unit/db/test_l3_hamode_db.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/54444407f40b26969a9d10e25e3fc368b6c89b77', 'message': ""use l3 api def from neutron-lib\n\nCommit I81748aa0e48b1275df3e1ea41b1d36a117d0097d added the l3 extension\nAPI definition to neutron-lib and commit\nI2324a3a02789c798248cab41c278a2d9981d24be rehomed the l3 exceptions,\nwhile Ifd79eb1a92853e49bd4ef028e7a7bd89811c6957 shims the l3\nexceptions.\n\nThis patch consumes the l3 api def by:\n- Removing the code from neutron that's now in lib.\n- Using lib's version of the code where applicable.\n- Tidying up the related unit tests as now that the l3 api def from lib\nis used the necessary fixture is already setup in the parent chain when\nsetting up the unit test class.\n\nNeutronLibImpact\n\nChange-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd\n""}]",7,423382,54444407f40b26969a9d10e25e3fc368b6c89b77,178,20,18,5367,,,0,"use l3 api def from neutron-lib

Commit I81748aa0e48b1275df3e1ea41b1d36a117d0097d added the l3 extension
API definition to neutron-lib and commit
I2324a3a02789c798248cab41c278a2d9981d24be rehomed the l3 exceptions,
while Ifd79eb1a92853e49bd4ef028e7a7bd89811c6957 shims the l3
exceptions.

This patch consumes the l3 api def by:
- Removing the code from neutron that's now in lib.
- Using lib's version of the code where applicable.
- Tidying up the related unit tests as now that the l3 api def from lib
is used the necessary fixture is already setup in the parent chain when
setting up the unit test class.

NeutronLibImpact

Change-Id: If2e66e06b83e15ee2851ea2bc3b64ad366e675dd
",git fetch https://review.opendev.org/openstack/neutron refs/changes/82/423382/18 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/extensions/test_router_availability_zone.py', 'neutron/db/l3_gwmode_db.py', 'neutron/extensions/dns.py', 'neutron/db/l3_dvr_db.py', 'neutron/extensions/l3_ext_gw_mode.py', 'neutron/tests/unit/extensions/test_l3.py', 'neutron/db/l3_db.py', 'neutron/db/l3_attrs_db.py', 'neutron/tests/unit/extensions/test_extraroute.py', 'neutron/db/models/l3.py', 'neutron/db/extraroute_db.py', 'neutron/services/auto_allocate/db.py', 'neutron/tests/unit/extensions/test_l3_ext_gw_mode.py', 'neutron/extensions/l3.py', 'neutron/tests/unit/services/metering/test_metering_plugin.py', 'neutron/tests/functional/services/l3_router/test_l3_dvr_ha_router_plugin.py', 'neutron/db/availability_zone/router.py', 'neutron/services/l3_router/l3_router_plugin.py', 'neutron/tests/functional/services/l3_router/test_l3_dvr_router_plugin.py', 'neutron/db/dns_db.py']",20,b18178c3c6de80625276bda2e68f795b0df897a3,use-lib-l3-apidef,from neutron_lib.api.definitions import l3,from neutron.extensions import l3,62,136
openstack%2Fsahara-dashboard~master~Ia56639d5958af502ee33978bfae77d03d88ae894,openstack/sahara-dashboard,master,Ia56639d5958af502ee33978bfae77d03d88ae894,Imported Translations from Zanata,MERGED,2017-12-16 06:59:05.000000000,2017-12-16 12:02:55.000000000,2017-12-16 12:02:55.000000000,"[{'_account_id': 10459}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-16 06:59:05.000000000', 'files': ['releasenotes/source/locale/id/LC_MESSAGES/releasenotes.po'], 'web_link': 'https://opendev.org/openstack/sahara-dashboard/commit/f1a4018f65b330b667db3e74e6ed0d93a0e3105f', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: Ia56639d5958af502ee33978bfae77d03d88ae894\n'}]",0,528446,f1a4018f65b330b667db3e74e6ed0d93a0e3105f,6,2,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: Ia56639d5958af502ee33978bfae77d03d88ae894
",git fetch https://review.opendev.org/openstack/sahara-dashboard refs/changes/46/528446/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/source/locale/id/LC_MESSAGES/releasenotes.po'],1,f1a4018f65b330b667db3e74e6ed0d93a0e3105f,zanata/translations,"""POT-Creation-Date: 2017-12-13 13:11+0000\n""""PO-Revision-Date: 2017-12-15 10:29+0000\n""msgid ""8.0.0.0b2"" msgstr ""8.0.0.0b2"" ","""POT-Creation-Date: 2017-12-08 19:03+0000\n""""PO-Revision-Date: 2017-12-03 04:11+0000\n""",5,2
openstack%2Fhorizon~master~I3069c6c9e66f32f53a64124f1d9d191d6aa703a3,openstack/horizon,master,I3069c6c9e66f32f53a64124f1d9d191d6aa703a3,Refactor swap_filter in instance views,MERGED,2017-11-23 20:33:28.000000000,2017-12-16 11:51:33.000000000,2017-12-16 11:51:33.000000000,"[{'_account_id': 841}, {'_account_id': 22348}, {'_account_id': 23866}]","[{'number': 1, 'created': '2017-11-23 20:33:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/93ad9747ca5af7e12664dd381ed31bf19b16de02', 'message': 'Refactor swap_filter in instance views\n\nChange-Id: I3069c6c9e66f32f53a64124f1d9d191d6aa703a3\n'}, {'number': 2, 'created': '2017-11-24 13:48:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/361a079662e494e76173bad3a9e22f4192a8fa99', 'message': 'Refactor swap_filter in instance views\n\nChange-Id: I3069c6c9e66f32f53a64124f1d9d191d6aa703a3\n'}, {'number': 3, 'created': '2017-11-24 14:03:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/291ba1a9118e196e2ffea09b263436f27ec41924', 'message': 'Refactor swap_filter in instance views\n\nPreviously we check if a fake_field is contained in search_opts\nbefore calling swap_filter, but we can move it into the inside\nof swap_filter.\n\nThis commit also adds docstring of swap_filter()\nto make it easier understand the logic.\n\nChange-Id: I3069c6c9e66f32f53a64124f1d9d191d6aa703a3\n'}, {'number': 4, 'created': '2017-11-24 23:39:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/e0529eda6e763fe97e463a758b2e0791b15f7660', 'message': 'Refactor swap_filter in instance views\n\nPreviously we check if a fake_field is contained in search_opts\nbefore calling swap_filter, but we can move it into the inside\nof swap_filter.\n\nThis commit also adds docstring of swap_filter()\nto make it easier understand the logic.\n\nChange-Id: I3069c6c9e66f32f53a64124f1d9d191d6aa703a3\n'}, {'number': 5, 'created': '2017-11-29 13:47:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/b216c13366c171603e59300ba00c21eed30a1ea1', 'message': 'Refactor swap_filter in instance views\n\nPreviously we check if a fake_field is contained in search_opts\nbefore calling swap_filter, but we can move it into the inside\nof swap_filter.\n\nThis commit also adds docstring of swap_filter()\nto make it easier understand the logic.\n\nChange-Id: I3069c6c9e66f32f53a64124f1d9d191d6aa703a3\n'}, {'number': 6, 'created': '2017-12-01 02:18:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/67af8ce5a73fbc35cb5ea187f38e6cd191b06bc0', 'message': 'Refactor swap_filter in instance views\n\nPreviously we check if a fake_field is contained in search_opts\nbefore calling swap_filter, but we can move it into the inside\nof swap_filter.\n\nThis commit also adds docstring of swap_filter()\nto make it easier understand the logic.\n\nAlso push away unnecessary function definitions\nin dashboards.admin.instances.views.\n\nChange-Id: I3069c6c9e66f32f53a64124f1d9d191d6aa703a3\n'}, {'number': 7, 'created': '2017-12-01 02:45:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/3d1651aa5e53a4d31794221b0915050f6ed1ae42', 'message': 'Refactor swap_filter in instance views\n\nPreviously we check if a fake_field is contained in search_opts\nbefore calling swap_filter, but we can move it into the inside\nof swap_filter.\n\nThis commit also adds docstring of swap_filter()\nto make it easier understand the logic.\n\nChange-Id: I3069c6c9e66f32f53a64124f1d9d191d6aa703a3\n'}, {'number': 8, 'created': '2017-12-16 10:08:54.000000000', 'files': ['openstack_dashboard/dashboards/admin/instances/views.py', 'openstack_dashboard/dashboards/project/instances/views.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/6c4d254b4af14a4884efae9220619392c465dddb', 'message': 'Refactor swap_filter in instance views\n\nPreviously we check if a fake_field is contained in search_opts\nbefore calling swap_filter, but we can move it into the inside\nof swap_filter.\n\nThis commit is split from https://review.openstack.org/#/c/507388/\nto avoid doing a fix and refactoring in a single commit.\n\nCo-Authored-By: Huan Xiong <huan.xiong@hxt-semitech.com>\nChange-Id: I3069c6c9e66f32f53a64124f1d9d191d6aa703a3\n'}]",8,522656,6c4d254b4af14a4884efae9220619392c465dddb,20,3,8,841,,,0,"Refactor swap_filter in instance views

Previously we check if a fake_field is contained in search_opts
before calling swap_filter, but we can move it into the inside
of swap_filter.

This commit is split from https://review.openstack.org/#/c/507388/
to avoid doing a fix and refactoring in a single commit.

Co-Authored-By: Huan Xiong <huan.xiong@hxt-semitech.com>
Change-Id: I3069c6c9e66f32f53a64124f1d9d191d6aa703a3
",git fetch https://review.opendev.org/openstack/horizon refs/changes/56/522656/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/admin/instances/views.py', 'openstack_dashboard/dashboards/project/instances/views.py']",2,93ad9747ca5af7e12664dd381ed31bf19b16de02,bug/1711486," if not all((swap_filter(image_dict.values(), search_opts, 'image_name', 'image'), swap_filter(flavor_dict.values(), search_opts, 'flavor_name', 'flavor'))):def swap_filter(resources, search_opts, fake_field, real_field): """"""Swap a fake filter field into a corresponding real field. There are cases where it is useful to provide filter fields which has no corresponding fields in backend services. For example, nova server list provides 'image' field with image ID but 'image name' is more useufl for GUI users. This function replaces a fake field into a corresponding real field. This returns True if further lookup is required. It returns False if it turns out there are no matching resources, for example, if no corresponding real field does not exist. """""" if fake_field not in search_opts: return True filter_string = search_opts[fake_field] matched = [resource for resource in resources if resource.name.lower() == filter_string.lower()] if not matched: return False search_opts[real_field] = matched[0].id del search_opts[fake_field] return True"," if ('image_name' in search_opts and not swap_filter(image_dict.values(), search_opts, 'image_name', 'image')): self._more = False return [] elif ('flavor_name' in search_opts and not swap_filter(flavor_dict.values(), search_opts, 'flavor_name', 'flavor')):def swap_filter(resources, filters, fake_field, real_field): if fake_field in filters: filter_string = filters[fake_field] for resource in resources: if resource.name.lower() == filter_string.lower(): filters[real_field] = resource.id del filters[fake_field] return True",32,28
openstack%2Fansible-hardening~stable%2Fpike~I07df3decf5e70b85a7dc48b8a8d1ca86e8878d09,openstack/ansible-hardening,stable/pike,I07df3decf5e70b85a7dc48b8a8d1ca86e8878d09,tasks: auth: Use standard Grub2 authentication mechanism,MERGED,2017-12-15 15:22:29.000000000,2017-12-16 10:40:14.000000000,2017-12-16 10:40:14.000000000,"[{'_account_id': 538}, {'_account_id': 6816}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 15:22:29.000000000', 'files': ['tasks/rhel7stig/auth.yml', 'vars/redhat.yml', 'vars/debian.yml', 'vars/main.yml', 'vars/suse.yml'], 'web_link': 'https://opendev.org/openstack/ansible-hardening/commit/af8774dd93584436a596fde4819b8d9f0395e185', 'message': 'tasks: auth: Use standard Grub2 authentication mechanism\n\nGRUB_PASSWORD is not understood by vanilla grub2 installations. As such,\nwe can use the recommended method by setting the superusers\nenvironment variable and using the password_pbkdf2 command\n\nChange-Id: I07df3decf5e70b85a7dc48b8a8d1ca86e8878d09\nLink: https://www.gnu.org/software/grub/manual/grub/grub.html#Security\nCloses-Bug: 1735709\n(cherry picked from commit a0810a9ca1d568c052d75b91b65159e21b764789)\n'}]",0,528307,af8774dd93584436a596fde4819b8d9f0395e185,7,3,1,23163,,,0,"tasks: auth: Use standard Grub2 authentication mechanism

GRUB_PASSWORD is not understood by vanilla grub2 installations. As such,
we can use the recommended method by setting the superusers
environment variable and using the password_pbkdf2 command

Change-Id: I07df3decf5e70b85a7dc48b8a8d1ca86e8878d09
Link: https://www.gnu.org/software/grub/manual/grub/grub.html#Security
Closes-Bug: 1735709
(cherry picked from commit a0810a9ca1d568c052d75b91b65159e21b764789)
",git fetch https://review.opendev.org/openstack/ansible-hardening refs/changes/07/528307/1 && git format-patch -1 --stdout FETCH_HEAD,"['tasks/rhel7stig/auth.yml', 'vars/redhat.yml', 'vars/debian.yml', 'vars/main.yml', 'vars/suse.yml']",5,af8774dd93584436a596fde4819b8d9f0395e185,bug/1735709-stable/pike,,grub_defaults_file: /etc/default/grub,14,11
openstack%2Fopenstack-ansible~master~I02345d23ac4b4085352033cac0b59bfd6b08ac44,openstack/openstack-ansible,master,I02345d23ac4b4085352033cac0b59bfd6b08ac44,Only whitelist internal network for placement api,MERGED,2017-11-29 15:42:54.000000000,2017-12-16 10:17:23.000000000,2017-12-16 10:17:23.000000000,"[{'_account_id': 538}, {'_account_id': 6816}, {'_account_id': 7353}, {'_account_id': 13095}, {'_account_id': 14805}, {'_account_id': 22348}, {'_account_id': 24468}]","[{'number': 1, 'created': '2017-11-29 15:42:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/8ea1c74710a549206694ea292342621b80a90dd6', 'message': 'Only whitelist internal network for placement api\n\nChange-Id: I02345d23ac4b4085352033cac0b59bfd6b08ac44\n'}, {'number': 2, 'created': '2017-12-01 01:35:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/2f2e7143b9ab12a91e78114d1ec88265be8bad75', 'message': 'Only whitelist internal network for placement api\n\nChange-Id: I02345d23ac4b4085352033cac0b59bfd6b08ac44\n'}, {'number': 3, 'created': '2017-12-06 19:05:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/c2f9265682ccaa1c463d5bc116f7a5d069878b5c', 'message': 'Only whitelist internal network for placement api\n\nChange-Id: I02345d23ac4b4085352033cac0b59bfd6b08ac44\n'}, {'number': 4, 'created': '2017-12-14 12:24:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/0b849e5b20d1908f02de68732d96b66ae2faae03', 'message': 'Only whitelist internal network for placement api\n\nChange-Id: I02345d23ac4b4085352033cac0b59bfd6b08ac44\n'}, {'number': 5, 'created': '2017-12-15 15:13:36.000000000', 'files': ['playbooks/inventory/group_vars/haproxy_all/haproxy.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/ef130bf266b4d57071006778d3df03969330a7b7', 'message': 'Only whitelist internal network for placement api\n\nChange-Id: I02345d23ac4b4085352033cac0b59bfd6b08ac44\n'}]",0,523913,ef130bf266b4d57071006778d3df03969330a7b7,46,7,5,13095,,,0,"Only whitelist internal network for placement api

Change-Id: I02345d23ac4b4085352033cac0b59bfd6b08ac44
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/13/523913/4 && git format-patch -1 --stdout FETCH_HEAD,['group_vars/haproxy_all/haproxy.yml'],1,8ea1c74710a549206694ea292342621b80a90dd6,nova-placement-api,"haproxy_nova_placement_whitelist_networks: ""{{ haproxy_whitelist_networks }}"" haproxy_whitelist_networks: ""{{ haproxy_nova_placement_whitelist_networks }}""",,2,0
openstack%2Fopenstack-ansible-pip_install~stable%2Fpike~Ic62e43288bf6386c8b2233ea305f65d226a0ae73,openstack/openstack-ansible-pip_install,stable/pike,Ic62e43288bf6386c8b2233ea305f65d226a0ae73,Use state=present for EPEL packages,MERGED,2017-12-15 14:45:44.000000000,2017-12-16 10:14:20.000000000,2017-12-16 10:14:20.000000000,"[{'_account_id': 6816}, {'_account_id': 14805}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 14:45:44.000000000', 'files': ['tasks/pre_install_yum.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-pip_install/commit/5a22b8840ed7596b0e1118b5d6ff25c43160cd3b', 'message': ""Use state=present for EPEL packages\n\nRequiring the latest EPEL and yum packages is not necessary and\ncauses deployment delays. This patch ensures that EPEL and\nyum-related packages are installed the system, even if they\naren't the absolute latest.\n\nChange-Id: Ic62e43288bf6386c8b2233ea305f65d226a0ae73\n""}]",0,528288,5a22b8840ed7596b0e1118b5d6ff25c43160cd3b,7,3,1,538,,,0,"Use state=present for EPEL packages

Requiring the latest EPEL and yum packages is not necessary and
causes deployment delays. This patch ensures that EPEL and
yum-related packages are installed the system, even if they
aren't the absolute latest.

Change-Id: Ic62e43288bf6386c8b2233ea305f65d226a0ae73
",git fetch https://review.opendev.org/openstack/openstack-ansible-pip_install refs/changes/88/528288/1 && git format-patch -1 --stdout FETCH_HEAD,['tasks/pre_install_yum.yml'],1,5a22b8840ed7596b0e1118b5d6ff25c43160cd3b,speed-up-epel, state: present," state: ""{{ pip_install_package_state }}""",1,1
openstack%2Fhorizon~master~Ic86dbded8f68659763c78b773146f2a61c5cb662,openstack/horizon,master,Ic86dbded8f68659763c78b773146f2a61c5cb662,Trunks panel: display the MAC of ports,MERGED,2017-12-05 15:57:09.000000000,2017-12-16 10:08:01.000000000,2017-12-16 10:08:01.000000000,"[{'_account_id': 841}, {'_account_id': 8313}, {'_account_id': 15554}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-05 15:57:09.000000000', 'files': ['openstack_dashboard/static/app/core/trunks/steps/trunk-subports.html', 'openstack_dashboard/static/app/core/trunks/steps/trunk-parent-port.html'], 'web_link': 'https://opendev.org/openstack/horizon/commit/93d45426cb2d07e151b10b2fd8bd09724ea1a56e', 'message': 'Trunks panel: display the MAC of ports\n\nDisplay some additional useful information while selecting ports for a\ntrunk. Since in a common use case the subports should use the same MAC\naddress as the parent port it is especially important to display the\nMAC address. Displaying the device owner was requested in a previous\nreview.\n\nChange-Id: Ic86dbded8f68659763c78b773146f2a61c5cb662\nPartially-Implements: blueprint neutron-trunk-ui\n'}]",0,525670,93d45426cb2d07e151b10b2fd8bd09724ea1a56e,14,4,1,15554,,,0,"Trunks panel: display the MAC of ports

Display some additional useful information while selecting ports for a
trunk. Since in a common use case the subports should use the same MAC
address as the parent port it is especially important to display the
MAC address. Displaying the device owner was requested in a previous
review.

Change-Id: Ic86dbded8f68659763c78b773146f2a61c5cb662
Partially-Implements: blueprint neutron-trunk-ui
",git fetch https://review.opendev.org/openstack/horizon refs/changes/70/525670/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/static/app/core/trunks/steps/trunk-subports.html', 'openstack_dashboard/static/app/core/trunks/steps/trunk-parent-port.html']",2,93d45426cb2d07e151b10b2fd8bd09724ea1a56e,bp/neutron-trunk-ui, <dt translate>MAC Address</dt> <dd>{$ item.mac_address $}</dd> <dt translate>MAC Address</dt> <dd>{$ item.mac_address $}</dd> <dt translate>Device Owner</dt> <dd>{$ item.device_owner $}</dd>,,10,0
openstack%2Fbarbican~master~I107e4b09b95eb6ea6cb34a6a8a811e46c6f900c7,openstack/barbican,master,I107e4b09b95eb6ea6cb34a6a8a811e46c6f900c7,[WIP] Implement OVO [10],ABANDONED,2017-09-06 03:41:07.000000000,2017-12-16 09:50:13.000000000,,"[{'_account_id': 3}, {'_account_id': 15471}, {'_account_id': 19554}, {'_account_id': 19741}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-09-06 03:41:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/c516c9b3e7d5ea1f9071588ee7d44ae6e535887a', 'message': '[WIP] Implemente OVO [10]\n\n- Change api/controllers/consumers.py to use OVO instead.\n\nChange-Id: I107e4b09b95eb6ea6cb34a6a8a811e46c6f900c7\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 2, 'created': '2017-09-07 07:11:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/959919c63e29ed863ee28a3075e4faf082e86d68', 'message': '[WIP] Implemente OVO [10]\n\n- Change api/controllers/consumers.py to use OVO instead.\n\nChange-Id: I107e4b09b95eb6ea6cb34a6a8a811e46c6f900c7\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 3, 'created': '2017-09-07 07:11:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/05c1cade1be0d4373a2163f02d1f1d0d12c7b6a6', 'message': '[WIP] Implement OVO [10]\n\n- Change api/controllers/consumers.py to use OVO instead.\n\nChange-Id: I107e4b09b95eb6ea6cb34a6a8a811e46c6f900c7\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 4, 'created': '2017-09-21 06:34:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/dfbd6dce1c4bdd6f1f06ee8bd634b58856bf8d78', 'message': '[WIP] Implement OVO [10]\n\n- Change api/controllers/consumers.py to use OVO instead.\n\nChange-Id: I107e4b09b95eb6ea6cb34a6a8a811e46c6f900c7\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 5, 'created': '2017-09-22 10:08:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/0930313a83b0491045aa160adc529880a0304df2', 'message': '[WIP] Implement OVO [10]\n\n- Change api/controllers/consumers.py to use OVO instead.\n\nChange-Id: I107e4b09b95eb6ea6cb34a6a8a811e46c6f900c7\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 6, 'created': '2017-11-27 06:57:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/7517ec16ea63765c9e6d308be16423a7cf1cdfeb', 'message': '[WIP] Implement OVO [10]\n\n- Change api/controllers/consumers.py to use OVO instead.\n\nChange-Id: I107e4b09b95eb6ea6cb34a6a8a811e46c6f900c7\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 7, 'created': '2017-11-27 07:15:20.000000000', 'files': ['barbican/api/controllers/consumers.py'], 'web_link': 'https://opendev.org/openstack/barbican/commit/c4a83344a5de103fb14b3f18a5beeecbf03f0e52', 'message': '[WIP] Implement OVO [10]\n\n- Change api/controllers/consumers.py to use OVO instead.\n\nChange-Id: I107e4b09b95eb6ea6cb34a6a8a811e46c6f900c7\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}]",1,501101,c4a83344a5de103fb14b3f18a5beeecbf03f0e52,14,5,7,22406,,,0,"[WIP] Implement OVO [10]

- Change api/controllers/consumers.py to use OVO instead.

Change-Id: I107e4b09b95eb6ea6cb34a6a8a811e46c6f900c7
Co-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>
Partial-Implements: blueprint rolling-upgrade
",git fetch https://review.opendev.org/openstack/barbican refs/changes/01/501101/6 && git format-patch -1 --stdout FETCH_HEAD,['barbican/api/controllers/consumers.py'],1,c516c9b3e7d5ea1f9071588ee7d44ae6e535887a,bp/rolling-upgrade,"from barbican import objects consumer = objects.ContainerConsumerMetadatum.get( external_project_id=external_project_id, self.quota_enforcer = quota.QuotaEnforcer( 'consumers', objects.ContainerConsumerMetadatum) result = objects.ContainerConsumerMetadatum.get_by_container_id( new_consumer = objects.ContainerConsumerMetadatum() new_consumer.create(container_id=self.container_id, project_id=project.id, parsed_request=data) container.consumers.append(new_consumer) container.save() project = objects.Project.find_by_external_project_id( consumer = objects.ContainerConsumerMetadatum.get_by_values( objects.ContainerConsumerMetadatum. \ delete_entity_by_id(consumer.id, external_project_id) container = objects.Container.get_container_by_id(","from barbican.model import models from barbican.model import repositories as repo self.consumer_repo = repo.get_container_consumer_repository() consumer = self.consumer_repo.get( self.consumer_repo = repo.get_container_consumer_repository() self.container_repo = repo.get_container_repository() self.project_repo = repo.get_project_repository() self.quota_enforcer = quota.QuotaEnforcer('consumers', self.consumer_repo) result = self.consumer_repo.get_by_container_id( new_consumer = models.ContainerConsumerMetadatum(self.container_id, project.id, data) self.consumer_repo.create_or_update_from(new_consumer, container) project = self.project_repo.find_by_external_project_id( consumer = self.consumer_repo.get_by_values( self.consumer_repo.delete_entity_by_id(consumer.id, external_project_id) container = self.container_repo.get_container_by_id(",19,19
openstack%2Fbarbican~master~I016067a84b3a8b9231354725bcbc559ad9e09fd2,openstack/barbican,master,I016067a84b3a8b9231354725bcbc559ad9e09fd2,[WIP] Implement OVO [9],ABANDONED,2017-09-06 03:00:45.000000000,2017-12-16 09:50:07.000000000,,"[{'_account_id': 3}, {'_account_id': 19554}, {'_account_id': 19741}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-09-06 03:00:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/2571449ffbcd0a3e27a10fdbaa0f200fc1221c17', 'message': '[WIP] Implemente OVO [9]\n\n- Change SecretACL and SecretACLUser to OVO.\n- Change api/controllers/acls.py to use OVO instead.\n\nChange-Id: I016067a84b3a8b9231354725bcbc559ad9e09fd2\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 2, 'created': '2017-09-06 03:14:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/b38f0900befd9807f66975382979bea4d8efa989', 'message': '[WIP] Implemente OVO [9]\n\n- Change SecretACL and SecretACLUser to OVO.\n- Change api/controllers/acls.py to use OVO instead.\n\nChange-Id: I016067a84b3a8b9231354725bcbc559ad9e09fd2\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 3, 'created': '2017-09-07 07:10:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/4e89cec6004d91dc80a5918091d44acb88d4ae38', 'message': '[WIP] Implemente OVO [9]\n\n- Change SecretACL and SecretACLUser to OVO.\n- Change api/controllers/acls.py to use OVO instead.\n\nChange-Id: I016067a84b3a8b9231354725bcbc559ad9e09fd2\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 4, 'created': '2017-09-07 07:10:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/0eff8e85c0cc6745c0e341c8f1aeb8ec10e81f8c', 'message': '[WIP] Implement OVO [9]\n\n- Change SecretACL and SecretACLUser to OVO.\n- Change api/controllers/acls.py to use OVO instead.\n\nChange-Id: I016067a84b3a8b9231354725bcbc559ad9e09fd2\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 5, 'created': '2017-09-21 06:33:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/04bb2d8e4f387374a6a08ce933423e853c834336', 'message': '[WIP] Implement OVO [9]\n\n- Change SecretACL and SecretACLUser to OVO.\n- Change api/controllers/acls.py to use OVO instead.\n\nChange-Id: I016067a84b3a8b9231354725bcbc559ad9e09fd2\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 6, 'created': '2017-09-22 10:07:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/4bad15010e37f60a751115100a7c508179fc667d', 'message': '[WIP] Implement OVO [9]\n\n- Change SecretACL and SecretACLUser to OVO.\n- Change api/controllers/acls.py to use OVO instead.\n\nChange-Id: I016067a84b3a8b9231354725bcbc559ad9e09fd2\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 7, 'created': '2017-11-27 06:56:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/0d3e605d92191128d8ec3c7aa5e0396449adb42e', 'message': '[WIP] Implement OVO [9]\n\n- Change SecretACL and SecretACLUser to OVO.\n- Change api/controllers/acls.py to use OVO instead.\n\nChange-Id: I016067a84b3a8b9231354725bcbc559ad9e09fd2\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 8, 'created': '2017-11-27 07:15:06.000000000', 'files': ['barbican/api/controllers/acls.py', 'barbican/objects/__init__.py', 'barbican/objects/secret.py', 'barbican/objects/secret_acl.py', 'barbican/objects/secret_acl_user.py'], 'web_link': 'https://opendev.org/openstack/barbican/commit/ead136cbb4f1bb5884d48d52458cca0dd636ed63', 'message': '[WIP] Implement OVO [9]\n\n- Change SecretACL and SecretACLUser to OVO.\n- Change api/controllers/acls.py to use OVO instead.\n\nChange-Id: I016067a84b3a8b9231354725bcbc559ad9e09fd2\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}]",0,501086,ead136cbb4f1bb5884d48d52458cca0dd636ed63,14,4,8,22406,,,0,"[WIP] Implement OVO [9]

- Change SecretACL and SecretACLUser to OVO.
- Change api/controllers/acls.py to use OVO instead.

Change-Id: I016067a84b3a8b9231354725bcbc559ad9e09fd2
Co-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>
Partial-Implements: blueprint rolling-upgrade
",git fetch https://review.opendev.org/openstack/barbican refs/changes/86/501086/8 && git format-patch -1 --stdout FETCH_HEAD,"['barbican/api/controllers/acls.py', 'barbican/objects/container_acl.py', 'barbican/objects/__init__.py', 'barbican/objects/secret.py', 'barbican/objects/secret_acl.py', 'barbican/objects/secret_acl_user.py']",6,2571449ffbcd0a3e27a10fdbaa0f200fc1221c17,bp/rolling-upgrade,"# Copyright 2017 Fujitsu. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from oslo_versionedobjects import base as object_base from barbican.objects import base from barbican.objects import fields @object_base.VersionedObjectRegistry.register class SecretACLUser(base.BarbicanObject, base.BarbicanPersistentObject, object_base.VersionedObjectDictCompat): fields = { 'acl_id': fields.StringField(), 'user_id': fields.StringField(), } @staticmethod def _from_db_object(secret_acl_user, secret_acl_user_db): for field in secret_acl_user.fields: setattr(secret_acl_user, field, secret_acl_user_db[field]) secret_acl_user.obj_reset_changes() return secret_acl_user def _do_extra_dict_fields(self): """"""Sub-class hook method: return dict of fields."""""" return {'acl_id': self.acl_id, 'user_id': self.user_id} ",,224,38
openstack%2Fbarbican~master~I3af6a5f0c758682e0ea006a42864444b053c9f94,openstack/barbican,master,I3af6a5f0c758682e0ea006a42864444b053c9f94,[WIP] Implement OVO [8],ABANDONED,2017-09-06 01:02:06.000000000,2017-12-16 09:49:59.000000000,,"[{'_account_id': 3}, {'_account_id': 8871}, {'_account_id': 19554}, {'_account_id': 19741}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-09-06 01:02:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/e0aa5918e57d2715ea7c3e15774a9c45d05fe4e3', 'message': '[WIP] Implemente OVO [8]\n\n- Change common/quotas.py.\n- Change api/controllers/orders.py to use OVO instead.\n\nChange-Id: I3af6a5f0c758682e0ea006a42864444b053c9f94\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 2, 'created': '2017-09-06 03:09:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/82c3c06c81a34ac7c1af53e03835836a4040f8b7', 'message': '[WIP] Implemente OVO [8]\n\n- Change common/quotas.py.\n- Change api/controllers/orders.py to use OVO instead.\n\nChange-Id: I3af6a5f0c758682e0ea006a42864444b053c9f94\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 3, 'created': '2017-09-06 03:14:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/ebabc5542302120bdbf5128066a3fe49a9798589', 'message': '[WIP] Implemente OVO [8]\n\n- Change common/quotas.py.\n- Change api/controllers/orders.py to use OVO instead.\n\nChange-Id: I3af6a5f0c758682e0ea006a42864444b053c9f94\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 4, 'created': '2017-09-07 07:07:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/2bbdcad5ef3c6bb199bfca358d4b71c37ec2ed0d', 'message': '[WIP] Implemente OVO [8]\n\n- Change common/quotas.py.\n- Change api/controllers/orders.py to use OVO instead.\n\nChange-Id: I3af6a5f0c758682e0ea006a42864444b053c9f94\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 5, 'created': '2017-09-07 07:09:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/53c41b804feb4e050ea388619c50b5859ffc0afb', 'message': '[WIP] Implement OVO [8]\n\n- Change common/quotas.py.\n- Change api/controllers/orders.py to use OVO instead.\n\nChange-Id: I3af6a5f0c758682e0ea006a42864444b053c9f94\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 6, 'created': '2017-09-21 06:33:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/64bff519e85fba1e6c4ba9daba884f1009074298', 'message': '[WIP] Implement OVO [8]\n\n- Change common/quotas.py.\n- Change api/controllers/orders.py to use OVO instead.\n\nChange-Id: I3af6a5f0c758682e0ea006a42864444b053c9f94\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 7, 'created': '2017-09-22 10:07:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/a39c836a2ad8e8b91e62e7c825934a7698f007d4', 'message': '[WIP] Implement OVO [8]\n\n- Change common/quotas.py.\n- Change api/controllers/orders.py to use OVO instead.\n\nChange-Id: I3af6a5f0c758682e0ea006a42864444b053c9f94\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 8, 'created': '2017-11-27 06:36:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/00bbf0bccdeefaed15be81421a7190ba8eff5d65', 'message': '[WIP] Implement OVO [8]\n\n- Change common/quotas.py.\n- Change api/controllers/orders.py to use OVO instead.\n\nChange-Id: I3af6a5f0c758682e0ea006a42864444b053c9f94\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}, {'number': 9, 'created': '2017-11-27 07:14:48.000000000', 'files': ['barbican/api/controllers/orders.py', 'barbican/plugin/resources.py', 'barbican/objects/order.py'], 'web_link': 'https://opendev.org/openstack/barbican/commit/acba8c9742df9035d17c668e602fcc13dd4455c8', 'message': '[WIP] Implement OVO [8]\n\n- Change common/quotas.py.\n- Change api/controllers/orders.py to use OVO instead.\n\nChange-Id: I3af6a5f0c758682e0ea006a42864444b053c9f94\nCo-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>\nPartial-Implements: blueprint rolling-upgrade\n'}]",0,501050,acba8c9742df9035d17c668e602fcc13dd4455c8,16,5,9,22406,,,0,"[WIP] Implement OVO [8]

- Change common/quotas.py.
- Change api/controllers/orders.py to use OVO instead.

Change-Id: I3af6a5f0c758682e0ea006a42864444b053c9f94
Co-Authored-By: Nam Nguyen Hoai <namnh@vn.fujitsu.com>
Partial-Implements: blueprint rolling-upgrade
",git fetch https://review.opendev.org/openstack/barbican refs/changes/50/501050/1 && git format-patch -1 --stdout FETCH_HEAD,"['barbican/api/controllers/orders.py', 'barbican/plugin/resources.py', 'barbican/objects/order.py']",3,e0aa5918e57d2715ea7c3e15774a9c45d05fe4e3,bp/rolling-upgrade," 'order_barbican_metadata': fields.DictOfObjectsField('OrderBarbicanMetadatum', nullable=True), 'order_plugin_metadata': fields.DictOfObjectsField('OrderPluginMetadatum', nullable=True) {key: obm.OrderBarbicanMetadatum. _from_db_object( obm.OrderBarbicanMetadatum(), value) for key, value in order_barbican_metadata_obj.items()} elif field == 'order_plugin_metadata': order_plugin_metadata_obj = db_order[field] if order_plugin_metadata_obj: order_plugin_metadata_obj = \ {key: opm.OrderPluginMetadatum. _from_db_object( opm.OrderPluginMetadatum(), value) for key, value in order_plugin_metadata_obj} order[field] = order_plugin_metadata_obj"," 'order_barbican_metadata': fields.ObjectField('OrderBarbicanMetadatum', nullable=True), 'order_plugin_medata': fields.ObjectField('OrderPluginMetadatum', nullable=True) obm.OrderBarbicanMetadatum._from_db_object( obm.OrderBarbicanMetadatum(), order_barbican_metadata_obj ) elif field == 'order_plugin_medata': order_plugin_medata_obj = db_order[field] if order_plugin_medata_obj: order_plugin_medata_obj = \ opm.OrderPluginMetadatum._from_db_object( opm.OrderPluginMetadatum(), order_plugin_medata_obj ) order[field] = order_plugin_medata_obj",40,38
openstack%2Fopenstack-ansible-pip_install~stable%2Fpike~I6f5bfbb09e1fa45c8d79efb8963ba8cd7fe5d315,openstack/openstack-ansible-pip_install,stable/pike,I6f5bfbb09e1fa45c8d79efb8963ba8cd7fe5d315,Allow all boolean values for uca_enable,MERGED,2017-12-15 16:55:51.000000000,2017-12-16 09:39:30.000000000,2017-12-15 23:29:13.000000000,"[{'_account_id': 538}, {'_account_id': 14805}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 16:55:51.000000000', 'files': ['tasks/pre_install_apt.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-pip_install/commit/4e39e427667914c323ccf5dfd3fc53a4d9d85d93', 'message': 'Allow all boolean values for uca_enable\n\nCurrently uca_enable has to be [Tt]rue or [Ffalse].\nThis patch allows it to be any boolean value recognised\nby ansible.\n\nChange-Id: I6f5bfbb09e1fa45c8d79efb8963ba8cd7fe5d315\n'}]",0,528343,4e39e427667914c323ccf5dfd3fc53a4d9d85d93,9,3,1,6816,,,0,"Allow all boolean values for uca_enable

Currently uca_enable has to be [Tt]rue or [Ffalse].
This patch allows it to be any boolean value recognised
by ansible.

Change-Id: I6f5bfbb09e1fa45c8d79efb8963ba8cd7fe5d315
",git fetch https://review.opendev.org/openstack/openstack-ansible-pip_install refs/changes/43/528343/1 && git format-patch -1 --stdout FETCH_HEAD,['tasks/pre_install_apt.yml'],1,4e39e427667914c323ccf5dfd3fc53a4d9d85d93,uca-bool, - uca_enable | bool, - uca_enable,1,1
openstack%2Fcoverage2sql~master~Ia16ad5ed6d9ed7be9dc6902f875b0144b24a938a,openstack/coverage2sql,master,Ia16ad5ed6d9ed7be9dc6902f875b0144b24a938a,Switch to use stestr instead of os-testr,MERGED,2017-12-15 08:22:50.000000000,2017-12-16 09:00:38.000000000,2017-12-16 09:00:38.000000000,"[{'_account_id': 5689}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 08:22:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/coverage2sql/commit/8908dd30be55184782a463ddbf2e18998be0772b', 'message': 'Switch to use stestr instead of os-testr\n\nThis commit switches to use stestr instead of os-testr. It should be\nbetter to reduce the test runner complexity.\n\nChange-Id: Ia16ad5ed6d9ed7be9dc6902f875b0144b24a938a\n'}, {'number': 2, 'created': '2017-12-16 08:46:09.000000000', 'files': ['.gitignore', 'test-requirements.txt', '.testr.conf', 'tox.ini', '.stestr.conf'], 'web_link': 'https://opendev.org/openstack/coverage2sql/commit/e541da94aabbe47f80ab55f9fcbb0c6ef9027fe2', 'message': 'Switch to use stestr instead of os-testr\n\nThis commit switches to use stestr instead of os-testr. It should be\nbetter to reduce the test runner complexity.\n\nChange-Id: Ia16ad5ed6d9ed7be9dc6902f875b0144b24a938a\n'}]",0,528198,e541da94aabbe47f80ab55f9fcbb0c6ef9027fe2,8,2,2,5689,,,0,"Switch to use stestr instead of os-testr

This commit switches to use stestr instead of os-testr. It should be
better to reduce the test runner complexity.

Change-Id: Ia16ad5ed6d9ed7be9dc6902f875b0144b24a938a
",git fetch https://review.opendev.org/openstack/coverage2sql refs/changes/98/528198/1 && git format-patch -1 --stdout FETCH_HEAD,"['.gitignore', 'test-requirements.txt', '.testr.conf', 'tox.ini', '.stestr.conf']",5,8908dd30be55184782a463ddbf2e18998be0772b,use-stestr,[DEFAULT] test_path=coverage2sql/tests top_dir=./ ,,6,9
openstack%2Fopenstack-helm~master~Idfaecd42a0aa7d22eda459a802f22cf6edbfa440,openstack/openstack-helm,master,Idfaecd42a0aa7d22eda459a802f22cf6edbfa440,Fluent-logging helm chart,ABANDONED,2017-10-24 12:02:39.000000000,2017-12-16 08:27:43.000000000,,"[{'_account_id': 8181}, {'_account_id': 11636}, {'_account_id': 17591}, {'_account_id': 22348}, {'_account_id': 25764}, {'_account_id': 26201}, {'_account_id': 26931}]","[{'number': 1, 'created': '2017-10-24 12:02:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/72eda759cd40c691f1bb6431c8ea20207332a828', 'message': 'Fluent-logging helm chart\n\nThis introduces an initial helm chart for fluent logging.\nIt provides a functional fluent-bit and fluentd deployment to\nuse in conjunction with elasticsearch and kibana to consume\nand aggregate logs from all resource types in a cluster.\nIt can deliver logs to kafka for external tools to consume.\n\nThis PS moves fluent-logging chart from osh-addons to osh repo.\nprevious ps:  https://review.openstack.org/#/c/507023/\n\nSpecification: https://review.openstack.org/#/c/505491/\nPartially implements: blueprint osh-logging-framework\n\nChange-Id: Idfaecd42a0aa7d22eda459a802f22cf6edbfa440\n'}, {'number': 2, 'created': '2017-10-25 13:16:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/84c1195acc42c6621360c7225ef61ec3bc88ac28', 'message': 'Fluent-logging helm chart\n\nThis introduces an initial helm chart for fluent logging.\nIt provides a functional fluent-bit and fluentd deployment to\nuse in conjunction with elasticsearch and kibana to consume\nand aggregate logs from all resource types in a cluster.\nIt can deliver logs to kafka for external tools to consume.\n\nThis PS moves fluent-logging chart from osh-addons to osh repo.\nprevious ps:  https://review.openstack.org/#/c/507023/\n\nSpecification: https://review.openstack.org/#/c/505491/\nPartially implements: blueprint osh-logging-framework\n\nChange-Id: Idfaecd42a0aa7d22eda459a802f22cf6edbfa440\n'}, {'number': 3, 'created': '2017-10-25 13:31:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/42802054ffe4769ee7be70eb2414820d7bf28863', 'message': 'Fluent-logging helm chart\n\nThis introduces an initial helm chart for fluent logging.\nIt provides a functional fluent-bit and fluentd deployment to\nuse in conjunction with elasticsearch and kibana to consume\nand aggregate logs from all resource types in a cluster.\nIt can deliver logs to kafka for external tools to consume.\n\nThis PS moves fluent-logging chart from osh-addons to osh repo.\nprevious ps:  https://review.openstack.org/#/c/507023/\n\nSpecification: https://review.openstack.org/#/c/505491/\nPartially implements: blueprint osh-logging-framework\n\nChange-Id: Idfaecd42a0aa7d22eda459a802f22cf6edbfa440\n'}, {'number': 4, 'created': '2017-10-26 13:19:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/010ca0a2b01b5e0fd6a3a5858b4008b756e4bc38', 'message': 'Fluent-logging helm chart\n\nThis introduces an initial helm chart for fluent logging.\nIt provides a functional fluent-bit and fluentd deployment to\nuse in conjunction with elasticsearch and kibana to consume\nand aggregate logs from all resource types in a cluster.\nIt can deliver logs to kafka for external tools to consume.\n\nThis PS moves fluent-logging chart from osh-addons to osh repo.\nprevious ps:  https://review.openstack.org/#/c/507023/\n\nSpecification: https://review.openstack.org/#/c/505491/\nPartially implements: blueprint osh-logging-framework\n\nChange-Id: Idfaecd42a0aa7d22eda459a802f22cf6edbfa440\n'}, {'number': 5, 'created': '2017-10-27 02:09:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/86026764a9a52b78bc6913ca9aaa05e1280c3935', 'message': 'Fluent-logging helm chart\n\nThis introduces an initial helm chart for fluent logging.\nIt provides a functional fluent-bit and fluentd deployment to\nuse in conjunction with elasticsearch and kibana to consume\nand aggregate logs from all resource types in a cluster.\nIt can deliver logs to kafka for external tools to consume.\n\nThis PS moves fluent-logging chart from osh-addons to osh repo.\nprevious ps:  https://review.openstack.org/#/c/507023/\n\nSpecification: https://review.openstack.org/#/c/505491/\nPartially implements: blueprint osh-logging-framework\n\nChange-Id: Idfaecd42a0aa7d22eda459a802f22cf6edbfa440\n'}, {'number': 6, 'created': '2017-10-31 13:08:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/296107b9831d811a668b96cea15b4ab0332d06ad', 'message': 'Fluent-logging helm chart\n\nThis introduces an initial helm chart for fluent logging.\nIt provides a functional fluent-bit and fluentd deployment to\nuse in conjunction with elasticsearch and kibana to consume\nand aggregate logs from all resource types in a cluster.\nIt can deliver logs to kafka for external tools to consume.\n\nThis PS moves fluent-logging chart from osh-addons to osh repo.\nprevious ps:  https://review.openstack.org/#/c/507023/\n\nSpecification: https://review.openstack.org/#/c/505491/\nPartially implements: blueprint osh-logging-framework\n\nChange-Id: Idfaecd42a0aa7d22eda459a802f22cf6edbfa440\n'}, {'number': 7, 'created': '2017-11-02 13:37:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/684babfbed41156ec63257c51bd3acb086ca2926', 'message': 'Fluent-logging helm chart\n\nThis introduces an initial helm chart for fluent logging.\nIt provides a functional fluent-bit and fluentd deployment to\nuse in conjunction with elasticsearch and kibana to consume\nand aggregate logs from all resource types in a cluster.\nIt can deliver logs to kafka for external tools to consume.\n\nThis PS moves fluent-logging chart from osh-addons to osh repo.\nprevious ps:  https://review.openstack.org/#/c/507023/\n\nSpecification: https://review.openstack.org/#/c/505491/\nPartially implements: blueprint osh-logging-framework\n\nChange-Id: Idfaecd42a0aa7d22eda459a802f22cf6edbfa440\n'}, {'number': 8, 'created': '2017-11-03 01:05:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/16f806eebba9d7e9febef6630da7643f9528f294', 'message': 'Fluent-logging helm chart\n\nThis introduces an initial helm chart for fluent logging.\nIt provides a functional fluent-bit and fluentd deployment to\nuse in conjunction with elasticsearch and kibana to consume\nand aggregate logs from all resource types in a cluster.\nIt can deliver logs to kafka for external tools to consume.\n\nThis PS moves fluent-logging chart from osh-addons to osh repo.\nprevious ps:  https://review.openstack.org/#/c/507023/\n\nSpecification: https://review.openstack.org/#/c/505491/\nPartially implements: blueprint osh-logging-framework\n\nChange-Id: Idfaecd42a0aa7d22eda459a802f22cf6edbfa440\n'}, {'number': 9, 'created': '2017-11-09 01:50:50.000000000', 'files': ['fluent-logging/templates/daemonset-fluent-bit.yaml', 'fluent-logging/README.rst', 'fluent-logging/templates/configmap-bin.yaml', 'fluent-logging/values.yaml', 'fluent-logging/templates/etc/_parsers.conf.tpl', 'fluent-logging/Chart.yaml', 'fluent-logging/templates/deployment-fluentd.yaml', 'fluent-logging/templates/service-fluentd.yaml', 'fluent-logging/requirements.yaml', 'fluent-logging/templates/bin/_fluentd.sh.tpl', 'fluent-logging/templates/configmap-etc.yaml', 'fluent-logging/templates/etc/_fluent-bit.conf.tpl', 'fluent-logging/templates/etc/_td-agent.conf.tpl'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/71576152fdfe2f07a7846df8d0a99fbaab539f46', 'message': 'Fluent-logging helm chart\n\nThis introduces an initial helm chart for fluent logging.\nIt provides a functional fluent-bit and fluentd deployment to\nuse in conjunction with elasticsearch and kibana to consume\nand aggregate logs from all resource types in a cluster.\nIt can deliver logs to kafka for external tools to consume.\n\nThis PS moves fluent-logging chart from osh-addons to osh repo.\nprevious ps:  https://review.openstack.org/#/c/507023/\n\nSpecification: https://review.openstack.org/#/c/505491/\nPartially implements: blueprint osh-logging-framework\n\nChange-Id: Idfaecd42a0aa7d22eda459a802f22cf6edbfa440\n'}]",58,514622,71576152fdfe2f07a7846df8d0a99fbaab539f46,63,7,9,26931,,,0,"Fluent-logging helm chart

This introduces an initial helm chart for fluent logging.
It provides a functional fluent-bit and fluentd deployment to
use in conjunction with elasticsearch and kibana to consume
and aggregate logs from all resource types in a cluster.
It can deliver logs to kafka for external tools to consume.

This PS moves fluent-logging chart from osh-addons to osh repo.
previous ps:  https://review.openstack.org/#/c/507023/

Specification: https://review.openstack.org/#/c/505491/
Partially implements: blueprint osh-logging-framework

Change-Id: Idfaecd42a0aa7d22eda459a802f22cf6edbfa440
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/22/514622/8 && git format-patch -1 --stdout FETCH_HEAD,"['fluent-logging/templates/daemonset-fluent-bit.yaml', 'fluent-logging/README.rst', 'fluent-logging/templates/configmap-bin.yaml', 'fluent-logging/values.yaml', 'fluent-logging/templates/etc/_parsers.conf.tpl', 'fluent-logging/Chart.yaml', 'fluent-logging/templates/deployment-fluentd.yaml', 'fluent-logging/templates/service-fluentd.yaml', 'fluent-logging/requirements.yaml', 'fluent-logging/templates/bin/_fluentd.sh.tpl', 'fluent-logging/templates/configmap-etc.yaml', 'fluent-logging/templates/etc/_fluent-bit.conf.tpl', 'fluent-logging/templates/etc/_td-agent.conf.tpl']",13,72eda759cd40c691f1bb6431c8ea20207332a828,bp/osh-logging-framework,"{{/* Copyright 2017 The Openstack-Helm Authors. Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */}} <source> @type forward port {{ tuple ""aggregator"" ""fluentd"" ""service"" . | include ""helm-toolkit.endpoints.endpoint_port_lookup"" }} bind 0.0.0.0 </source> <filter kube.**> type kubernetes_metadata </filter> <match ** > {{ if .Values.conf.fluentd.kafka.enabled }} @type copy <store> @type kafka_buffered # list of seed brokers brokers {{ tuple ""kafka"" ""kafka"" . | include ""helm-toolkit.endpoints.hostname_short_endpoint_lookup"" }}:{{ tuple ""kafka"" ""kafka"" ""service"" . | include ""helm-toolkit.endpoints.endpoint_port_lookup"" }} # buffer settings buffer_type file buffer_path /var/log/td-agent/buffer/td flush_interval {{ .Values.conf.fluentd.kafka.flush_interval }} # topic settings default_topic {{ .Values.conf.fluentd.kafka.topic_name }} # data type settings output_data_type {{ .Values.conf.fluentd.kafka.output_data_type }} compression_codec gzip # producer settings max_send_retries 1 required_acks -1 </store> <store> {{- end }} @type elasticsearch include_tag_key true host {{ tuple ""elasticsearch"" ""elasticsearch"" . | include ""helm-toolkit.endpoints.hostname_short_endpoint_lookup"" }} port {{ tuple ""elasticsearch"" ""elasticsearch"" ""client"" . | include ""helm-toolkit.endpoints.endpoint_port_lookup"" }} logstash_format {{ .Values.conf.fluentd.elasticsearch.logstash }} # Set the chunk limit the same as for fluentd-gcp. buffer_chunk_limit {{ .Values.conf.fluentd.elasticsearch.buffer_chunk_limit }} # Cap buffer memory usage to 2MiB/chunk * 32 chunks = 64 MiB buffer_queue_limit {{ .Values.conf.fluentd.elasticsearch.buffer_queue_limit }} # Flush buffer every 30s to write to Elasticsearch flush_interval {{ .Values.conf.fluentd.elasticsearch.flush_interval }} # Never wait longer than 5 minutes between retries. max_retry_wait {{ .Values.conf.fluentd.elasticsearch.max_retry_wait }} {{- if .Values.conf.fluentd.elasticsearch.disable_retry_limit }} # Disable the limit on the number of retries (retry forever). disable_retry_limit {{- end }} # Use multiple threads for processing. num_threads {{ .Values.conf.fluentd.elasticsearch.num_threads }} {{ if .Values.conf.fluentd.kafka.enabled }} </store> {{- end }} </match> ",,713,0
openstack%2Fzun~master~Idb3a036fc085bfbaa4af1d7fe150ca1abf86d611,openstack/zun,master,Idb3a036fc085bfbaa4af1d7fe150ca1abf86d611,Updated from global requirements,MERGED,2017-12-15 22:25:07.000000000,2017-12-16 08:18:48.000000000,2017-12-16 08:18:48.000000000,"[{'_account_id': 22348}, {'_account_id': 23055}, {'_account_id': 23365}]","[{'number': 1, 'created': '2017-12-15 22:25:07.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/zun/commit/4927686fefab8d061443de914c4bed4a2525e1b0', 'message': 'Updated from global requirements\n\nChange-Id: Idb3a036fc085bfbaa4af1d7fe150ca1abf86d611\n'}]",0,528435,4927686fefab8d061443de914c4bed4a2525e1b0,7,3,1,11131,,,0,"Updated from global requirements

Change-Id: Idb3a036fc085bfbaa4af1d7fe150ca1abf86d611
",git fetch https://review.opendev.org/openstack/zun refs/changes/35/528435/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,4927686fefab8d061443de914c4bed4a2525e1b0,openstack/requirements,"oslo.service!=1.28.1,>=1.24.0 # Apache-2.0",oslo.service>=1.24.0 # Apache-2.0,1,1
openstack%2Fhorizon~master~Ie0e6a2d8d68cc0ae31d8851b8b332260741a6147,openstack/horizon,master,Ie0e6a2d8d68cc0ae31d8851b8b332260741a6147,Imported Translations from Zanata,MERGED,2017-12-16 07:35:14.000000000,2017-12-16 08:10:09.000000000,2017-12-16 08:10:09.000000000,"[{'_account_id': 841}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-16 07:35:14.000000000', 'files': ['horizon/locale/zh_CN/LC_MESSAGES/django.po', 'releasenotes/source/locale/id/LC_MESSAGES/releasenotes.po', 'openstack_dashboard/locale/ko_KR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_CN/LC_MESSAGES/djangojs.po', 'horizon/locale/ko_KR/LC_MESSAGES/djangojs.po', 'openstack_dashboard/locale/en_GB/LC_MESSAGES/djangojs.po', 'openstack_dashboard/locale/zh_CN/LC_MESSAGES/django.po', 'releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po', 'horizon/locale/zh_CN/LC_MESSAGES/djangojs.po', 'openstack_auth/locale/zh_CN/LC_MESSAGES/django.po', 'openstack_dashboard/locale/id/LC_MESSAGES/djangojs.po'], 'web_link': 'https://opendev.org/openstack/horizon/commit/fae118de37a5174e9ff0a9dafec2bb01f633bc10', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: Ie0e6a2d8d68cc0ae31d8851b8b332260741a6147\n'}]",0,528454,fae118de37a5174e9ff0a9dafec2bb01f633bc10,6,2,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: Ie0e6a2d8d68cc0ae31d8851b8b332260741a6147
",git fetch https://review.opendev.org/openstack/horizon refs/changes/54/528454/1 && git format-patch -1 --stdout FETCH_HEAD,"['horizon/locale/zh_CN/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ko_KR/LC_MESSAGES/django.po', 'releasenotes/source/locale/id/LC_MESSAGES/releasenotes.po', 'openstack_dashboard/locale/zh_CN/LC_MESSAGES/djangojs.po', 'horizon/locale/ko_KR/LC_MESSAGES/djangojs.po', 'openstack_dashboard/locale/en_GB/LC_MESSAGES/djangojs.po', 'openstack_dashboard/locale/zh_CN/LC_MESSAGES/django.po', 'releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po', 'horizon/locale/zh_CN/LC_MESSAGES/djangojs.po', 'openstack_auth/locale/zh_CN/LC_MESSAGES/django.po', 'openstack_dashboard/locale/id/LC_MESSAGES/djangojs.po']",11,fae118de37a5174e9ff0a9dafec2bb01f633bc10,zanata/translations,"""Project-Id-Version: horizon 13.0.0.0b3.dev31\n""""POT-Creation-Date: 2017-12-15 19:34+0000\n""""PO-Revision-Date: 2017-12-15 07:37+0000\n""msgid ""Import Public Key"" msgstr ""Impor Public Key"" msgid """" ""Key Pairs are how you login to your instance after it is launched.\n"" "" Choose a key pair name you will recognize and paste your SSH public key "" ""into the\n"" "" space provided."" msgstr """" ""Key Pairs (pasangan kunci) adalah bagaimana Anda login ke instance Anda "" ""setelah diluncurkan.\n"" "" Pilih nama pasangan kunci yang akan Anda kenali dan tempel kunci publik "" ""SSH Anda ke dalam\n"" "" ruang yang disediakan."" msgid """" ""Key Pairs are how you login to your instance after it is launched.\n"" "" Choose a key pair name you will recognize.\n"" "" Names may only include alphanumeric characters, spaces, or dashes."" msgstr """" ""Key Pairs (pasangan kunci) adalah bagaimana Anda login ke instance Anda "" ""setelah diluncurkan.\n"" "" Pilih nama pasangan kunci yang akan Anda kenali.\n"" "" Nama mungkin hanya menyertakan karakter alfanumerik, spasi, atau tanda "" ""hubung."" #, python-format msgid ""Key pair %(name)s was successfully created."" msgstr ""Key pair %(name)s berhasil dibuat."" msgid ""Key pair already exists."" msgstr ""Key Pairs (pasangan kunci) sudah ada."" ","""Project-Id-Version: horizon 13.0.0.0b3.dev14\n""""POT-Creation-Date: 2017-12-12 05:47+0000\n""""PO-Revision-Date: 2017-12-03 05:13+0000\n""",622,63
openstack%2Fzun-tempest-plugin~master~Ib030d7ddfa30c7ec1fe071746c4600ea6532d2e1,openstack/zun-tempest-plugin,master,Ib030d7ddfa30c7ec1fe071746c4600ea6532d2e1,Fix git repo link error in README.,MERGED,2017-12-14 01:42:42.000000000,2017-12-16 07:17:50.000000000,2017-12-16 07:17:50.000000000,"[{'_account_id': 8264}, {'_account_id': 22348}, {'_account_id': 22406}, {'_account_id': 23055}]","[{'number': 1, 'created': '2017-12-14 01:42:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun-tempest-plugin/commit/6b11fd878e4496cd1ca3419fa0c29530ced62ea7', 'message': 'Fix git repo link error in README.\n\nSince http://git.openstack.org/cgit/openstack/zun-tempest-plugin\nis not clonable, change it to\nhttps://git.openstack.org/cgit/openstack/zun-tempest-plugin\n\nChange-Id: Ib030d7ddfa30c7ec1fe071746c4600ea6532d2e1\n'}, {'number': 2, 'created': '2017-12-14 01:43:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun-tempest-plugin/commit/1aac09409ec12d9975e9d9b1c373b20c8f95b74b', 'message': 'Fix git repo link error in README.\n\nSince http://git.openstack.org/cgit/openstack/zun-tempest-plugin\nis not clonable, change it to\nhttps://git.openstack.org/cgit/openstack/zun-tempest-plugin\n\nChange-Id: Ib030d7ddfa30c7ec1fe071746c4600ea6532d2e1\n'}, {'number': 3, 'created': '2017-12-14 01:59:35.000000000', 'files': ['README.rst'], 'web_link': 'https://opendev.org/openstack/zun-tempest-plugin/commit/23ae1b0f3ad20d9ac2085bffd15b96aaea79067d', 'message': 'Fix git repo link error in README.\n\nSince http://git.openstack.org/cgit/openstack/zun-tempest-plugin\nis not clonable, change it to\nhttps://git.openstack.org/cgit/openstack/zun-tempest-plugin\n\nChange-Id: Ib030d7ddfa30c7ec1fe071746c4600ea6532d2e1\n'}]",4,527832,23ae1b0f3ad20d9ac2085bffd15b96aaea79067d,12,4,3,8264,,,0,"Fix git repo link error in README.

Since http://git.openstack.org/cgit/openstack/zun-tempest-plugin
is not clonable, change it to
https://git.openstack.org/cgit/openstack/zun-tempest-plugin

Change-Id: Ib030d7ddfa30c7ec1fe071746c4600ea6532d2e1
",git fetch https://review.opendev.org/openstack/zun-tempest-plugin refs/changes/32/527832/1 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,6b11fd878e4496cd1ca3419fa0c29530ced62ea7,doc-error,* Source: https://git.openstack.org/cgit/openstack/zun-tempest-plugin,* Source: http://git.openstack.org/cgit/openstack/zun-tempest-plugin,1,1
openstack%2Fmurano-tempest-plugin~master~I678cb0efd36742a61a1e69543302ab4d88b4f286,openstack/murano-tempest-plugin,master,I678cb0efd36742a61a1e69543302ab4d88b4f286,Fix murano gate volume attach device na,ABANDONED,2017-12-16 05:52:50.000000000,2017-12-16 07:13:28.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2017-12-16 05:52:50.000000000', 'files': ['murano_tempest_tests/tests/scenario/application_catalog/base.py'], 'web_link': 'https://opendev.org/openstack/murano-tempest-plugin/commit/871b8711e1050578d743c6b6c33d8968aa14ed4a', 'message': 'Fix murano gate volume attach device na\n\nChange-Id: I678cb0efd36742a61a1e69543302ab4d88b4f286\n'}]",0,528443,871b8711e1050578d743c6b6c33d8968aa14ed4a,3,1,1,14107,,,0,"Fix murano gate volume attach device na

Change-Id: I678cb0efd36742a61a1e69543302ab4d88b4f286
",git fetch https://review.opendev.org/openstack/murano-tempest-plugin refs/changes/43/528443/1 && git format-patch -1 --stdout FETCH_HEAD,['murano_tempest_tests/tests/scenario/application_catalog/base.py'],1,871b8711e1050578d743c6b6c33d8968aa14ed4a,," def get_volume_attachments_from_nova(self, instance_id): attached_volumes = self.servers_client.\ list_volume_attachments(instance_id)['volumeAttachments'] return attached_volumes if attachment.get('device').startswith('/dev/'): self.assertTrue(attachment.get('device').startswith('/dev/')) # NOTE(zhurong): add a recheck for attachment from nova # if volume api return was failed else: volume_attachments = self.get_volume_attachments_from_nova( instance_id) for attachment in volume_attachments: self.assertTrue( attachment.get('device').startswith('/dev/'))", self.assertTrue(attachment.get('device').startswith('/dev/')),15,1
openstack%2Fcinder~master~I484099bf266edb4626b843aedc55f42902ad1787,openstack/cinder,master,I484099bf266edb4626b843aedc55f42902ad1787,Fix for getting manageable volume in cinder/volume/manager.py,ABANDONED,2017-12-14 15:20:07.000000000,2017-12-16 07:02:16.000000000,,"[{'_account_id': 9008}, {'_account_id': 10058}, {'_account_id': 10118}, {'_account_id': 10622}, {'_account_id': 11904}, {'_account_id': 13628}, {'_account_id': 14384}, {'_account_id': 15831}, {'_account_id': 15941}, {'_account_id': 16422}, {'_account_id': 18120}, {'_account_id': 21863}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23081}, {'_account_id': 24230}, {'_account_id': 24236}, {'_account_id': 24502}, {'_account_id': 25243}, {'_account_id': 25677}, {'_account_id': 25678}]","[{'number': 1, 'created': '2017-12-14 15:20:07.000000000', 'files': ['cinder/volume/manager.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/1df6b4e79dd581adfddbfe79f5e9075bda5ac047', 'message': 'Fix for getting manageable volume in cinder/volume/manager.py\n\nIn function `get_manageable_volumes` in cinder/volume/manager.py,\ncinder volumes filtered by `host` or `cluster` will be passed to the\nstorage backend driver. This will cause problems when different hosts\nconfigured to same storage backend. For example, host1@rbd#rbd and\nhost2@rbd#rbd are two different hosts that both configured to the same\npool in Ceph cluster, volume1 is created on host1@rbd#rbd, volume2 is\ncreated on host2@rbd#rbd. Then the return of `list manageable volume`\napi will turn out to be that volume1 is already managed, while the\nvolume2 would still be safe to manage (In fact volume2 is already\nmanaged). So it is necessary to pass all cinder volumes to storage\nbackend driver to verify which volume is already managed and which is\nnot.\n\nCloses-Bug: #1738201\nChange-Id: I484099bf266edb4626b843aedc55f42902ad1787\n'}]",1,527988,1df6b4e79dd581adfddbfe79f5e9075bda5ac047,23,21,1,23081,,,0,"Fix for getting manageable volume in cinder/volume/manager.py

In function `get_manageable_volumes` in cinder/volume/manager.py,
cinder volumes filtered by `host` or `cluster` will be passed to the
storage backend driver. This will cause problems when different hosts
configured to same storage backend. For example, host1@rbd#rbd and
host2@rbd#rbd are two different hosts that both configured to the same
pool in Ceph cluster, volume1 is created on host1@rbd#rbd, volume2 is
created on host2@rbd#rbd. Then the return of `list manageable volume`
api will turn out to be that volume1 is already managed, while the
volume2 would still be safe to manage (In fact volume2 is already
managed). So it is necessary to pass all cinder volumes to storage
backend driver to verify which volume is already managed and which is
not.

Closes-Bug: #1738201
Change-Id: I484099bf266edb4626b843aedc55f42902ad1787
",git fetch https://review.opendev.org/openstack/cinder refs/changes/88/527988/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/manager.py'],1,1df6b4e79dd581adfddbfe79f5e9075bda5ac047,bug/1738201," cinder_volumes = getattr(objects.VolumeList, 'get_all')(ctxt)", cinder_volumes = self._get_my_volumes(ctxt),1,1
openstack%2Fproject-config~master~I3d4b5b9c6cfce1f8c933b58883854dae1bfd13ae,openstack/project-config,master,I3d4b5b9c6cfce1f8c933b58883854dae1bfd13ae,Normalize projects.yaml,MERGED,2017-12-16 06:06:21.000000000,2017-12-16 06:47:27.000000000,2017-12-16 06:47:27.000000000,"[{'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-16 06:06:21.000000000', 'files': ['gerrit/projects.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/c84060b44ca964159812e15564e27d2f7f51e52e', 'message': 'Normalize projects.yaml\n\nChange-Id: I3d4b5b9c6cfce1f8c933b58883854dae1bfd13ae\n'}]",0,528444,c84060b44ca964159812e15564e27d2f7f51e52e,6,2,1,11131,,,0,"Normalize projects.yaml

Change-Id: I3d4b5b9c6cfce1f8c933b58883854dae1bfd13ae
",git fetch https://review.opendev.org/openstack/project-config refs/changes/44/528444/1 && git format-patch -1 --stdout FETCH_HEAD,['gerrit/projects.yaml'],1,c84060b44ca964159812e15564e27d2f7f51e52e,project-yaml-normalization, - ara, - ara upstream: https://github.com/ghanshyammann/ec2api-tempest-plugin.git upstream: https://github.com/chkumar246/monasca-tempest-plugin.git upstream: https://github.com/chkumar246/trove-tempest-plugin.git upstream: https://github.com/chkumar246/vitrage-tempest-plugin.git,1,5
openstack%2Ftripleo-heat-templates~stable%2Fpike~I48f0711622bfc55054de46b9fb4fd765b7e4df74,openstack/tripleo-heat-templates,stable/pike,I48f0711622bfc55054de46b9fb4fd765b7e4df74,Remove Cinder UID from CephX keyrings' ACLs,MERGED,2017-12-14 14:19:04.000000000,2017-12-16 06:06:08.000000000,2017-12-16 06:06:08.000000000,"[{'_account_id': 3153}, {'_account_id': 19564}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2017-12-14 14:19:04.000000000', 'files': ['docker/services/ceph-ansible/ceph-base.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/d9997b43d1aa0586ec0f3a0474df8476ac341cf5', 'message': ""Remove Cinder UID from CephX keyrings' ACLs\n\nLike for the other OpenStack services, ACLs are managed by the puppet\nmodule.\n\nDepends-On: I0c1bc3d2362c6500b1a515d99f641f8c1468754a\nChange-Id: I48f0711622bfc55054de46b9fb4fd765b7e4df74\nRelated-Bug: #1734134\n(cherry picked from commit f0e5f05b957378fcb9f15e4c1ab232eafff8232a)\n""}]",0,527972,d9997b43d1aa0586ec0f3a0474df8476ac341cf5,8,4,1,6796,,,0,"Remove Cinder UID from CephX keyrings' ACLs

Like for the other OpenStack services, ACLs are managed by the puppet
module.

Depends-On: I0c1bc3d2362c6500b1a515d99f641f8c1468754a
Change-Id: I48f0711622bfc55054de46b9fb4fd765b7e4df74
Related-Bug: #1734134
(cherry picked from commit f0e5f05b957378fcb9f15e4c1ab232eafff8232a)
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/72/527972/1 && git format-patch -1 --stdout FETCH_HEAD,['docker/services/ceph-ansible/ceph-base.yaml'],1,d9997b43d1aa0586ec0f3a0474df8476ac341cf5,bug/1734134,," acls: [""u:165:r""] # uid of cinder user",0,1
openstack%2Fpuppet-tripleo~stable%2Fpike~I0c1bc3d2362c6500b1a515d99f641f8c1468754a,openstack/puppet-tripleo,stable/pike,I0c1bc3d2362c6500b1a515d99f641f8c1468754a,Update cephx keys with ACLs for openstack services.,MERGED,2017-12-14 14:18:34.000000000,2017-12-16 06:03:42.000000000,2017-12-16 06:03:42.000000000,"[{'_account_id': 3153}, {'_account_id': 6796}, {'_account_id': 14985}, {'_account_id': 19564}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2017-12-14 14:18:34.000000000', 'files': ['manifests/profile/base/manila/share.pp', 'metadata.json', 'manifests/profile/base/cinder/volume.pp', 'spec/classes/tripleo_profile_base_gnocchi_api_spec.rb', 'manifests/profile/base/gnocchi/api.pp', 'manifests/profile/base/nova/compute_libvirt_shared.pp', 'spec/classes/tripleo_profile_base_cinder_volume_spec.rb', 'manifests/profile/base/glance/api.pp'], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/3357c38248f1894a9388cb6d8962ded9e216ec02', 'message': 'Update cephx keys with ACLs for openstack services.\n\nThis patch will set file system ACLs on the ceph client keyring.\nThis will help resolve (1) for OSP Ocata and before\n\nChange-Id: I0c1bc3d2362c6500b1a515d99f641f8c1468754a\nPartial-Bug: #1720787\n1: https://bugzilla.redhat.com/show_bug.cgi?id=1462657\n(cherry picked from commit 48c417519f88472d035c3ad6a92edcc2e6039d9b)\n'}]",0,527969,3357c38248f1894a9388cb6d8962ded9e216ec02,19,6,1,6796,,,0,"Update cephx keys with ACLs for openstack services.

This patch will set file system ACLs on the ceph client keyring.
This will help resolve (1) for OSP Ocata and before

Change-Id: I0c1bc3d2362c6500b1a515d99f641f8c1468754a
Partial-Bug: #1720787
1: https://bugzilla.redhat.com/show_bug.cgi?id=1462657
(cherry picked from commit 48c417519f88472d035c3ad6a92edcc2e6039d9b)
",git fetch https://review.opendev.org/openstack/puppet-tripleo refs/changes/69/527969/1 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/profile/base/manila/share.pp', 'metadata.json', 'manifests/profile/base/cinder/volume.pp', 'spec/classes/tripleo_profile_base_gnocchi_api_spec.rb', 'manifests/profile/base/gnocchi/api.pp', 'manifests/profile/base/nova/compute_libvirt_shared.pp', 'spec/classes/tripleo_profile_base_cinder_volume_spec.rb', 'manifests/profile/base/glance/api.pp']",8,3357c38248f1894a9388cb6d8962ded9e216ec02,bug/1720787,"# [*glance_rbd_client_name*] # RBD client naem # (optional) Defaults to hiera('glance::backend::rbd::rbd_store_user') $glance_rbd_client_name = hiera('glance::backend::rbd::rbd_store_user','openstack'), 'rbd': { $backend_store = 'rbd' exec{ ""exec-setfacl-${glance_rbd_client_name}-glance"": path => ['/bin', '/usr/bin'], command => ""setfacl -m u:glance:r-- /etc/ceph/ceph.client.${glance_rbd_client_name}.keyring"", unless => ""getfacl /etc/ceph/ceph.client.${glance_rbd_client_name}.keyring | grep -q user:glance:r--"", } Ceph::Key<| title == ""client.${glance_rbd_client_name}"" |> -> Exec[""exec-setfacl-${glance_rbd_client_name}-glance""] }", 'rbd': { $backend_store = 'rbd' },71,7
openstack%2Ftripleo-common~stable%2Fpike~Idbbb7afd4ab0157c870e1c1e3cd1666fb99651cc,openstack/tripleo-common,stable/pike,Idbbb7afd4ab0157c870e1c1e3cd1666fb99651cc,Don't wait for stack in progress in delete,MERGED,2017-12-15 08:47:26.000000000,2017-12-16 05:51:54.000000000,2017-12-16 05:51:53.000000000,"[{'_account_id': 9712}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2017-12-15 08:47:26.000000000', 'files': ['workbooks/stack.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/8e8360974acf3d297452a2a4fc9998f98a270912', 'message': ""Don't wait for stack in progress in delete\n\nWe don't need to wait for the stack to be in progress when deleting it,\nand it can in fact be a race condition if it disappears too fast. Just\nskip to check if it still exists.\n\nChange-Id: Idbbb7afd4ab0157c870e1c1e3cd1666fb99651cc\n(cherry picked from commit 9178796142da747cf23d72c7a0b6b7c9554e90f4)\n""}]",0,528204,8e8360974acf3d297452a2a4fc9998f98a270912,13,4,1,7385,,,0,"Don't wait for stack in progress in delete

We don't need to wait for the stack to be in progress when deleting it,
and it can in fact be a race condition if it disappears too fast. Just
skip to check if it still exists.

Change-Id: Idbbb7afd4ab0157c870e1c1e3cd1666fb99651cc
(cherry picked from commit 9178796142da747cf23d72c7a0b6b7c9554e90f4)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/04/528204/1 && git format-patch -1 --stdout FETCH_HEAD,['workbooks/stack.yaml'],1,8e8360974acf3d297452a2a4fc9998f98a270912,simplify-delete-stack-stable/pike, on-success: wait_for_stack_does_not_exist, on-success: wait_for_stack_in_progress wait_for_stack_in_progress: workflow: tripleo.stack.v1.wait_for_stack_in_progress stack=<% $.stack %> on-success: wait_for_stack_does_not_exist on-error: wait_for_stack_in_progress_failed wait_for_stack_in_progress_failed: on-success: send_message publish: status: FAILED message: <% task(wait_for_stack_in_progress).result %> ,1,12
openstack%2Fpuppet-tripleo~stable%2Focata~I2ed6e328a9a4915844f699784dd87dc99078fb23,openstack/puppet-tripleo,stable/ocata,I2ed6e328a9a4915844f699784dd87dc99078fb23,gnocchi: ensure upgrade run after swift setup,MERGED,2017-12-13 18:49:31.000000000,2017-12-16 05:38:04.000000000,2017-12-16 05:38:04.000000000,"[{'_account_id': 2813}, {'_account_id': 3153}, {'_account_id': 11897}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2017-12-13 18:49:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/63c51f336fdfe516102236e295f7fc8268f2587d', 'message': 'gnocchi: ensure upgrade run after swift setup\n\nThe orignal fix have create an dependencies on an Class, so\nit does work and fail silencly.\n\nThis changes it to the Anchor.\n\nChange-Id: I2ed6e328a9a4915844f699784dd87dc99078fb23\nCloses-bug: #1724328\n'}, {'number': 2, 'created': '2017-12-13 18:51:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/5b1c962381c37f323b2f51c5cbe78518932cab00', 'message': 'gnocchi: ensure upgrade run after swift setup\n\nThe orignal fix have create an dependencies on an Class, so\nit does work and fail silencly.\n\nThis changes it to the Anchor.\n\nChange-Id: I2ed6e328a9a4915844f699784dd87dc99078fb23\nCloses-bug: #1724328\n'}, {'number': 3, 'created': '2017-12-13 18:57:00.000000000', 'files': ['manifests/profile/base/gnocchi/api.pp'], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/5e1b5cafa6bb7a22f75375266688c86a73b157f2', 'message': 'gnocchi: ensure upgrade run after swift setup\n\nThe orignal fix have create an dependencies on an Class, so\nit does work and fail silencly.\n\nThis changes it to the Anchor.\n\nChange-Id: I2ed6e328a9a4915844f699784dd87dc99078fb23\nCloses-bug: #1724328\n(cherry picked from commit 1c49fbe08d1f764975ce8ef952b055ad25effd65)\n'}]",0,527760,5e1b5cafa6bb7a22f75375266688c86a73b157f2,19,6,3,2813,,,0,"gnocchi: ensure upgrade run after swift setup

The orignal fix have create an dependencies on an Class, so
it does work and fail silencly.

This changes it to the Anchor.

Change-Id: I2ed6e328a9a4915844f699784dd87dc99078fb23
Closes-bug: #1724328
(cherry picked from commit 1c49fbe08d1f764975ce8ef952b055ad25effd65)
",git fetch https://review.opendev.org/openstack/puppet-tripleo refs/changes/60/527760/2 && git format-patch -1 --stdout FETCH_HEAD,['manifests/profile/base/gnocchi/api.pp'],1,63c51f336fdfe516102236e295f7fc8268f2587d,bug/1724328, Anchor<| title == 'swift::service::end' |> ~> Anchor['gnocchi::dbsync::begin'], Anchor<| title == 'swift::service::end' |> ~> Class['Gnocchi::db::sync'],1,1
openstack%2Fmurano-agent~master~I6f8ebfc44ca6940d58b6666cb1bf7c7cb62fbd88,openstack/murano-agent,master,I6f8ebfc44ca6940d58b6666cb1bf7c7cb62fbd88,Updated from global requirements,MERGED,2017-12-15 21:41:21.000000000,2017-12-16 05:17:07.000000000,2017-12-16 05:17:07.000000000,"[{'_account_id': 14107}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 21:41:21.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/murano-agent/commit/d33b5a1a943007a843a28f36f18174c6f86e2d62', 'message': 'Updated from global requirements\n\nChange-Id: I6f8ebfc44ca6940d58b6666cb1bf7c7cb62fbd88\n'}]",0,528411,d33b5a1a943007a843a28f36f18174c6f86e2d62,6,2,1,11131,,,0,"Updated from global requirements

Change-Id: I6f8ebfc44ca6940d58b6666cb1bf7c7cb62fbd88
",git fetch https://review.opendev.org/openstack/murano-agent refs/changes/11/528411/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,d33b5a1a943007a843a28f36f18174c6f86e2d62,openstack/requirements,"oslo.service!=1.28.1,>=1.24.0 # Apache-2.0",oslo.service>=1.24.0 # Apache-2.0,1,1
openstack%2Fmurano-dashboard~master~Ic712120162cbd614af3c74f65224b782b9c6c1ae,openstack/murano-dashboard,master,Ic712120162cbd614af3c74f65224b782b9c6c1ae,Imported Translations from Zanata,MERGED,2017-12-15 07:15:17.000000000,2017-12-16 05:14:26.000000000,2017-12-16 05:14:26.000000000,"[{'_account_id': 6547}, {'_account_id': 14107}, {'_account_id': 22348}, {'_account_id': 23186}]","[{'number': 1, 'created': '2017-12-15 07:15:17.000000000', 'files': ['releasenotes/source/locale/id/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/ja/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/ko_KR/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/ru/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/de/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/zh_CN/LC_MESSAGES/releasenotes.po'], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/5d6a51230dc44f884ed4f4fdb66c13e8e3a7430a', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: Ic712120162cbd614af3c74f65224b782b9c6c1ae\n'}]",0,528174,5d6a51230dc44f884ed4f4fdb66c13e8e3a7430a,11,4,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: Ic712120162cbd614af3c74f65224b782b9c6c1ae
",git fetch https://review.opendev.org/openstack/murano-dashboard refs/changes/74/528174/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/source/locale/id/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/ja/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/ko_KR/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/ru/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/de/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/zh_CN/LC_MESSAGES/releasenotes.po']",7,5d6a51230dc44f884ed4f4fdb66c13e8e3a7430a,zanata/translations,"""POT-Creation-Date: 2017-12-14 15:37+0000\n""msgid ""2.0.0.0b2"" msgstr ""2.0.0.0b2"" msgid ""2.0.0.0b3"" msgstr ""2.0.0.0b3"" msgid ""2.0.0.0rc1"" msgstr ""2.0.0.0rc1"" msgid ""3.0.0.0b1"" msgstr ""3.0.0.0b1"" msgid ""3.0.0.0b2"" msgstr ""3.0.0.0b2"" msgid ""3.0.0.0b3"" msgstr ""3.0.0.0b3版本"" msgid ""3.0.0.0rc1"" msgstr ""3.0.0.0rc1"" msgid """" ""cve-2016-4972 has been addressed. In several places Murano used loaders "" ""inherited directly from yaml.Loader when parsing MuranoPL and UI files from "" ""packages. This is unsafe, because this loader is capable of creating custom "" ""python objects from specifically constructed yaml files. With this change "" ""all yaml loading operations are done using safe loaders instead."" msgstr """" ""cve-2016-4972 已经处理了。在一些地方，当从软件包中解析MuranoPL和UI文件时，"" ""Murano使用了直接继承自yaml.Loader的加载器. 这是不安全的，因为此加载器可以由特"" ""别构造的yaml文件创建自定义的python对象。此修改将所有yaml加载操作都由安全的加"" ""载器代替完成。"" ","""POT-Creation-Date: 2017-12-10 18:43+0000\n""",197,10
openstack%2Fkeystone~master~Ia0ac9a79f9faf618aeafae4621afaf1db62ed1e5,openstack/keystone,master,Ia0ac9a79f9faf618aeafae4621afaf1db62ed1e5,Handle deprecation of inspect.getargspec,MERGED,2017-12-05 19:43:44.000000000,2017-12-16 04:30:47.000000000,2017-12-16 04:30:47.000000000,"[{'_account_id': 2218}, {'_account_id': 5046}, {'_account_id': 15054}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-05 19:43:44.000000000', 'files': ['keystone/common/manager.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/b84927aa899412bb58e0498cc0c867fa207583e1', 'message': ""Handle deprecation of inspect.getargspec\n\nThe getargspec function is deprecated in python3 and in some cases the\ndeprecation warning can actually cause unit test failures[1]. Follow\nSean's example[2] to smartly use the right inspect function and avoid\ndeprecation warnings.\n\n[1] http://logs.openstack.org/47/524747/3/check/openstack-tox-py35/1585cb9/testr_results.html.gz\n[2] https://review.openstack.org/#/c/521979/\n\nChange-Id: Ia0ac9a79f9faf618aeafae4621afaf1db62ed1e5\n""}]",0,525740,b84927aa899412bb58e0498cc0c867fa207583e1,8,4,1,8482,,,0,"Handle deprecation of inspect.getargspec

The getargspec function is deprecated in python3 and in some cases the
deprecation warning can actually cause unit test failures[1]. Follow
Sean's example[2] to smartly use the right inspect function and avoid
deprecation warnings.

[1] http://logs.openstack.org/47/524747/3/check/openstack-tox-py35/1585cb9/testr_results.html.gz
[2] https://review.openstack.org/#/c/521979/

Change-Id: Ia0ac9a79f9faf618aeafae4621afaf1db62ed1e5
",git fetch https://review.opendev.org/openstack/keystone refs/changes/40/525740/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/common/manager.py'],1,b84927aa899412bb58e0498cc0c867fa207583e1,fix-getargspec,"if hasattr(inspect, 'getfullargspec'): getargspec = inspect.getfullargspec else: getargspec = inspect.getargspec __argspec = getargspec(__f)", __argspec = inspect.getargspec(__f),6,1
openstack%2Fneutron~master~Ie97b1ad05e294b5fe763ae8d7319800eb16ea3dc,openstack/neutron,master,Ie97b1ad05e294b5fe763ae8d7319800eb16ea3dc,Remove router_ids argument to auto_schedule_routers(),MERGED,2017-11-10 05:37:09.000000000,2017-12-16 03:48:34.000000000,2017-12-16 03:48:33.000000000,"[{'_account_id': 1131}, {'_account_id': 1653}, {'_account_id': 4694}, {'_account_id': 9732}, {'_account_id': 9845}, {'_account_id': 10385}, {'_account_id': 22348}, {'_account_id': 27346}]","[{'number': 1, 'created': '2017-11-10 05:37:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3306384c00cffea1921244fbb935ecf7df80d822', 'message': 'Remove router_ids argument to auto_schedule_routers()\n\nThe router_ids argument to auto_schedule_routers() is\nunused, and was marked for deprecation in Queens.\n\nChange-Id: Ie97b1ad05e294b5fe763ae8d7319800eb16ea3dc\n'}, {'number': 2, 'created': '2017-12-04 20:37:18.000000000', 'files': ['neutron/tests/functional/scheduler/test_l3_agent_scheduler.py', 'neutron/tests/unit/scheduler/test_l3_agent_scheduler.py', 'neutron/scheduler/l3_agent_scheduler.py', 'neutron/api/rpc/handlers/l3_rpc.py', 'neutron/db/l3_agentschedulers_db.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/d00b7ddec58c5383d28d95b65c22f288eb47f382', 'message': 'Remove router_ids argument to auto_schedule_routers()\n\nThe router_ids argument to auto_schedule_routers() is\nunused, and was marked for deprecation in Queens.\n\nChange-Id: Ie97b1ad05e294b5fe763ae8d7319800eb16ea3dc\n'}]",0,518837,d00b7ddec58c5383d28d95b65c22f288eb47f382,31,8,2,1131,,,0,"Remove router_ids argument to auto_schedule_routers()

The router_ids argument to auto_schedule_routers() is
unused, and was marked for deprecation in Queens.

Change-Id: Ie97b1ad05e294b5fe763ae8d7319800eb16ea3dc
",git fetch https://review.opendev.org/openstack/neutron refs/changes/37/518837/2 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/functional/scheduler/test_l3_agent_scheduler.py', 'neutron/scheduler/l3_agent_scheduler.py', 'neutron/api/rpc/handlers/l3_rpc.py']",3,3306384c00cffea1921244fbb935ecf7df80d822,remove-auto-schedule-routers-router-ids," self.l3plugin.auto_schedule_routers(context, host)"," self.l3plugin.auto_schedule_routers(context, host, router_ids=None)",3,15
openstack%2Fpuppet-ceilometer~master~I60808bc2e8fa652d251b955a6544c2ab9b441cd3,openstack/puppet-ceilometer,master,I60808bc2e8fa652d251b955a6544c2ab9b441cd3,Remove client_package_name in params.pp,MERGED,2017-12-12 00:51:26.000000000,2017-12-16 03:15:59.000000000,2017-12-16 03:15:59.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-12 00:51:26.000000000', 'files': ['manifests/params.pp'], 'web_link': 'https://opendev.org/openstack/puppet-ceilometer/commit/24085e979fe6b73fbfa773c1471c5d23d1925e7d', 'message': 'Remove client_package_name in params.pp\n\nThe ceilometer client has been removed[1].\n\n[1]https://review.openstack.org/#/c/523537/\n\nChange-Id: I60808bc2e8fa652d251b955a6544c2ab9b441cd3\n'}]",0,527278,24085e979fe6b73fbfa773c1471c5d23d1925e7d,11,3,1,9414,,,0,"Remove client_package_name in params.pp

The ceilometer client has been removed[1].

[1]https://review.openstack.org/#/c/523537/

Change-Id: I60808bc2e8fa652d251b955a6544c2ab9b441cd3
",git fetch https://review.opendev.org/openstack/puppet-ceilometer refs/changes/78/527278/1 && git format-patch -1 --stdout FETCH_HEAD,['manifests/params.pp'],1,24085e979fe6b73fbfa773c1471c5d23d1925e7d,remove_client,, $client_package_name = 'python-ceilometerclient',0,1
openstack%2Fpython-tripleoclient~master~Ie736926e6dfd6ac964956255048b5e3e31e30597,openstack/python-tripleoclient,master,Ie736926e6dfd6ac964956255048b5e3e31e30597,Display Horizon URL at the end of a deployment,MERGED,2017-11-06 16:50:32.000000000,2017-12-16 03:11:32.000000000,2017-12-16 03:11:31.000000000,"[{'_account_id': 7065}, {'_account_id': 7509}, {'_account_id': 9712}, {'_account_id': 14985}, {'_account_id': 18575}, {'_account_id': 19554}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2017-11-06 16:50:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/9fb259a5af8c3a5dce50084b076c5dff3712c7e4', 'message': 'Display Horizon URL at the end of a deployment\n\nChange-Id: Ie736926e6dfd6ac964956255048b5e3e31e30597\nDepends-On: I966fc5c4d285f15a8d528aa3105c90fe5a3fbba1\n'}, {'number': 2, 'created': '2017-11-13 12:02:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/8d7ff3e305ef64d11aae44b391ab729c33684801', 'message': 'Display Horizon URL at the end of a deployment\n\nChange-Id: Ie736926e6dfd6ac964956255048b5e3e31e30597\nDepends-On: I966fc5c4d285f15a8d528aa3105c90fe5a3fbba1\n'}, {'number': 3, 'created': '2017-11-20 15:07:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/b3e5e6a1029792deddb05859c390cde25fb379a8', 'message': 'Display Horizon URL at the end of a deployment\n\nChange-Id: Ie736926e6dfd6ac964956255048b5e3e31e30597\nDepends-On: I966fc5c4d285f15a8d528aa3105c90fe5a3fbba1\n'}, {'number': 4, 'created': '2017-11-20 18:21:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/4e579d5b740c1b7976a333e2bafd1d0c60396fa2', 'message': 'Display Horizon URL at the end of a deployment\n\nChange-Id: Ie736926e6dfd6ac964956255048b5e3e31e30597\nDepends-On: I966fc5c4d285f15a8d528aa3105c90fe5a3fbba1\nDepends-On: I81eaf6ae00ee18e1bec4bb39a18a54e90e1ce5d5\n'}, {'number': 5, 'created': '2017-11-28 12:40:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/c2e693bdf49c55a83120fe953b6a2c36565e212b', 'message': 'Display Horizon URL at the end of a deployment\n\nChange-Id: Ie736926e6dfd6ac964956255048b5e3e31e30597\nDepends-On: I966fc5c4d285f15a8d528aa3105c90fe5a3fbba1\n'}, {'number': 6, 'created': '2017-11-28 16:21:27.000000000', 'files': ['tripleoclient/tests/v1/overcloud_deploy/test_overcloud_deploy.py', 'tripleoclient/workflows/deployment.py', 'tripleoclient/v1/overcloud_deploy.py'], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/24cd624e0557123b22f49b0a75b275ee36d8749c', 'message': 'Display Horizon URL at the end of a deployment\n\nChange-Id: Ie736926e6dfd6ac964956255048b5e3e31e30597\nDepends-On: I966fc5c4d285f15a8d528aa3105c90fe5a3fbba1\nDepends-On: I81eaf6ae00ee18e1bec4bb39a18a54e90e1ce5d5\n'}]",2,518030,24cd624e0557123b22f49b0a75b275ee36d8749c,47,8,6,7065,,,0,"Display Horizon URL at the end of a deployment

Change-Id: Ie736926e6dfd6ac964956255048b5e3e31e30597
Depends-On: I966fc5c4d285f15a8d528aa3105c90fe5a3fbba1
Depends-On: I81eaf6ae00ee18e1bec4bb39a18a54e90e1ce5d5
",git fetch https://review.opendev.org/openstack/python-tripleoclient refs/changes/30/518030/4 && git format-patch -1 --stdout FETCH_HEAD,"['tripleoclient/tests/v1/overcloud_deploy/test_overcloud_deploy.py', 'tripleoclient/workflows/deployment.py', 'tripleoclient/v1/overcloud_deploy.py']",3,9fb259a5af8c3a5dce50084b076c5dff3712c7e4,horizon-url," horizon_url = deployment.get_horizon_url( self.clients, stack=stack.stack_name, queue_name=str(uuid.uuid4())) print(""Overcloud Horizon Dashboard URL: {0}"".format(horizon_url))",,74,14
openstack%2Fpython-tripleoclient~master~I9bd9c35208ba4b75590e0d6d793578fcbbe599f8,openstack/python-tripleoclient,master,I9bd9c35208ba4b75590e0d6d793578fcbbe599f8,Undercloud: wire in scheduler_max_attempts,MERGED,2017-12-08 03:26:19.000000000,2017-12-16 03:11:31.000000000,2017-12-16 03:11:30.000000000,"[{'_account_id': 3153}, {'_account_id': 10239}, {'_account_id': 13039}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2017-12-08 03:26:19.000000000', 'files': ['tripleoclient/v1/undercloud_config.py'], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/814a142316c788c984241ef3b3819d022c9d993e', 'message': 'Undercloud: wire in scheduler_max_attempts\n\nChange-Id: I9bd9c35208ba4b75590e0d6d793578fcbbe599f8\nDepends-On: Ib4ea5af26c39e466d0f4b7506a65f020620f2120\n'}]",0,526584,814a142316c788c984241ef3b3819d022c9d993e,14,5,1,360,,,0,"Undercloud: wire in scheduler_max_attempts

Change-Id: I9bd9c35208ba4b75590e0d6d793578fcbbe599f8
Depends-On: Ib4ea5af26c39e466d0f4b7506a65f020620f2120
",git fetch https://review.opendev.org/openstack/python-tripleoclient refs/changes/84/526584/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleoclient/v1/undercloud_config.py'],1,814a142316c788c984241ef3b3819d022c9d993e,nova_scheduler_max_attempts," 'scheduler_max_attempts': 'NovaSchedulerMaxAttempts',",,1,0
openstack%2Ftripleo-heat-templates~master~Ib4ea5af26c39e466d0f4b7506a65f020620f2120,openstack/tripleo-heat-templates,master,Ib4ea5af26c39e466d0f4b7506a65f020620f2120,Add NovaSchedulerMaxAttempts parameter,MERGED,2017-12-08 03:24:56.000000000,2017-12-16 03:11:29.000000000,2017-12-16 03:11:29.000000000,"[{'_account_id': 360}, {'_account_id': 10239}, {'_account_id': 13039}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 23811}]","[{'number': 1, 'created': '2017-12-08 03:24:56.000000000', 'files': ['puppet/services/nova-scheduler.yaml', 'environments/undercloud.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/1ad70e7899ec4c4196393dc733f640ff501e5a13', 'message': 'Add NovaSchedulerMaxAttempts parameter\n\nNeeded by the Undercloud installer. Defaults to 3 to match\nwhat we do in puppet-tripleo today for the Overcloud.\n\nChange-Id: Ib4ea5af26c39e466d0f4b7506a65f020620f2120\n'}]",6,526582,1ad70e7899ec4c4196393dc733f640ff501e5a13,16,7,1,360,,,0,"Add NovaSchedulerMaxAttempts parameter

Needed by the Undercloud installer. Defaults to 3 to match
what we do in puppet-tripleo today for the Overcloud.

Change-Id: Ib4ea5af26c39e466d0f4b7506a65f020620f2120
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/82/526582/1 && git format-patch -1 --stdout FETCH_HEAD,"['puppet/services/nova-scheduler.yaml', 'environments/undercloud.yaml']",2,1ad70e7899ec4c4196393dc733f640ff501e5a13,nova_scheduler_max_attempts, NovaSchedulerMaxAttempts: 30,,10,0
openstack%2Fopenstack-helm~master~I85b39aa0f3c9cf9880bc1d44270e2336d950a35c,openstack/openstack-helm,master,I85b39aa0f3c9cf9880bc1d44270e2336d950a35c,Fix error in doc of install openstack-helm,MERGED,2017-12-15 11:11:40.000000000,2017-12-16 02:45:45.000000000,2017-12-16 02:39:36.000000000,"[{'_account_id': 20466}, {'_account_id': 22348}, {'_account_id': 23928}, {'_account_id': 26201}]","[{'number': 1, 'created': '2017-12-15 11:11:40.000000000', 'files': ['doc/source/install/ext-dns-fqdn.rst', 'doc/source/install/multinode.rst'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/c65961ccfe78d054d3bbac658a5c47542aa3b587', 'message': 'Fix error in doc of install openstack-helm\n\ninstall openstack-helm like doc, but the service of horizon\nis not nodeport.\n\nChange-Id: I85b39aa0f3c9cf9880bc1d44270e2336d950a35c\n'}]",0,528233,c65961ccfe78d054d3bbac658a5c47542aa3b587,10,4,1,22711,,,0,"Fix error in doc of install openstack-helm

install openstack-helm like doc, but the service of horizon
is not nodeport.

Change-Id: I85b39aa0f3c9cf9880bc1d44270e2336d950a35c
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/33/528233/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/install/ext-dns-fqdn.rst', 'doc/source/install/multinode.rst']",2,c65961ccfe78d054d3bbac658a5c47542aa3b587,bug/install-horizon-bug, --set network.node_port.enabled=true, --set network.enable_node_port=true,3,3
openstack%2Fopenstack-ansible~master~I46316c658a3746f91313641e5a26de49a03b2164,openstack/openstack-ansible,master,I46316c658a3746f91313641e5a26de49a03b2164,designate variables for neutron integration,MERGED,2017-10-09 18:33:24.000000000,2017-12-16 02:33:18.000000000,2017-12-16 02:33:17.000000000,"[{'_account_id': 3}, {'_account_id': 538}, {'_account_id': 6816}, {'_account_id': 7353}, {'_account_id': 14805}, {'_account_id': 17068}, {'_account_id': 22348}, {'_account_id': 22582}, {'_account_id': 23163}, {'_account_id': 24468}, {'_account_id': 26405}]","[{'number': 1, 'created': '2017-10-09 18:33:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/329130c9f079e8c1356a377b6acf889ae63ca351', 'message': 'designate variables for neutron integration\n\nThis adds designate and keystone variables needed for designate integration in neutron.\n\nChange-Id: I46316c658a3746f91313641e5a26de49a03b2164\nPartial-Bug: 1687594\n'}, {'number': 2, 'created': '2017-10-10 17:28:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/b7b4e2aeac77a5d5a091911a0a44c2e9aaa8e519', 'message': 'designate variables for neutron integration\n\nThis adds designate and keystone variables needed for designate integration in neutron.\n\nChange-Id: I46316c658a3746f91313641e5a26de49a03b2164\nPartial-Bug: 1687594\n'}, {'number': 3, 'created': '2017-10-10 21:28:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/7cb414c11f55270cccf12f7065a106be37f449ac', 'message': 'designate variables for neutron integration\n\nThis adds designate and keystone variables needed for designate integration in neutron.\n\nChange-Id: I46316c658a3746f91313641e5a26de49a03b2164\nPartial-Bug: 1687594\n'}, {'number': 4, 'created': '2017-10-16 15:59:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/0704072996ec71ea5fff9ef30fdd690bb59dca9f', 'message': 'designate variables for neutron integration\n\nThis adds designate and keystone variables needed for designate integration in neutron.\n\nChange-Id: I46316c658a3746f91313641e5a26de49a03b2164\nPartial-Bug: 1687594\n'}, {'number': 5, 'created': '2017-12-06 11:17:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/794fefcb9652815d2af36daeeffb7faac2128f7a', 'message': 'designate variables for neutron integration\n\nThis adds designate and keystone variables needed for designate integration in neutron.\n\nChange-Id: I46316c658a3746f91313641e5a26de49a03b2164\nPartial-Bug: 1687594\nDepends-On: I82469a1de6a7ebce8ca2f568c1bb8125bbc57965\n'}, {'number': 6, 'created': '2017-12-06 11:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/a8cc7e0bec78e004cbe7ca05eeca2ce0f992e42e', 'message': 'designate variables for neutron integration\n\nThis adds designate and keystone variables needed for\ndesignate integration in neutron.\n\nChange-Id: I46316c658a3746f91313641e5a26de49a03b2164\nPartial-Bug: 1687594\nDepends-On: I82469a1de6a7ebce8ca2f568c1bb8125bbc57965\n'}, {'number': 7, 'created': '2017-12-11 17:06:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/c1e85d2c91b6f6a593f6b7608dcced9815aa4343', 'message': 'designate variables for neutron integration\n\nThis adds designate and keystone variables needed for\ndesignate integration in neutron.\n\nChange-Id: I46316c658a3746f91313641e5a26de49a03b2164\nPartial-Bug: 1687594\nDepends-On: I82469a1de6a7ebce8ca2f568c1bb8125bbc57965\n'}, {'number': 8, 'created': '2017-12-11 17:08:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/8704b7475da8d3d4311c75dfcab87f433d98ee91', 'message': 'designate variables for neutron integration\n\nThis adds designate and keystone variables needed for\ndesignate integration in neutron.\n\nChange-Id: I46316c658a3746f91313641e5a26de49a03b2164\nPartial-Bug: 1687594\nDepends-On: I82469a1de6a7ebce8ca2f568c1bb8125bbc57965\n'}, {'number': 9, 'created': '2017-12-14 12:47:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/0b79160618416c1c114e7728768754f063d65c23', 'message': 'designate variables for neutron integration\n\nThis adds designate and keystone variables needed for\ndesignate integration in neutron.\n\nChange-Id: I46316c658a3746f91313641e5a26de49a03b2164\nPartial-Bug: 1687594\nDepends-On: I82469a1de6a7ebce8ca2f568c1bb8125bbc57965\n'}, {'number': 10, 'created': '2017-12-14 14:25:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/8731531e215d148e0f4ea3d3180c4d0547fabca1', 'message': 'designate variables for neutron integration\n\nThis adds designate and keystone variables needed for\ndesignate integration in neutron.\n\nChange-Id: I46316c658a3746f91313641e5a26de49a03b2164\nCloses-Bug: 1687594\nDepends-On: I82469a1de6a7ebce8ca2f568c1bb8125bbc57965\n'}, {'number': 11, 'created': '2017-12-15 10:59:49.000000000', 'files': ['group_vars/all/designate.yml', 'playbooks/inventory/group_vars/designate_all.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/b8dfac4f689823577cb43692dd0564f303928678', 'message': 'designate variables for neutron integration\n\nThis adds designate and keystone variables needed for\ndesignate integration in neutron.\n\nChange-Id: I46316c658a3746f91313641e5a26de49a03b2164\nCloses-Bug: 1687594\nDepends-On: I82469a1de6a7ebce8ca2f568c1bb8125bbc57965\n'}]",8,510650,b8dfac4f689823577cb43692dd0564f303928678,71,11,11,26405,,,0,"designate variables for neutron integration

This adds designate and keystone variables needed for
designate integration in neutron.

Change-Id: I46316c658a3746f91313641e5a26de49a03b2164
Closes-Bug: 1687594
Depends-On: I82469a1de6a7ebce8ca2f568c1bb8125bbc57965
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/50/510650/10 && git format-patch -1 --stdout FETCH_HEAD,"['group_vars/all/keystone.yml', 'group_vars/all/designate.yml']",2,329130c9f079e8c1356a377b6acf889ae63ca351,bug/1687594,"--- # Copyright 2017, Rackspace US, Inc. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # Referenced by neutron role for dns integration and must remain in 'all' group scoping designate_service_port: 9001 designate_service_proto: http designate_service_user_name: designate designate_service_project_name: service designate_service_project_domain_id: Default designate_service_user_domain_id: Default designate_service_region: ""{{ service_region }}"" designate_service_publicuri_proto: ""{{ openstack_service_publicuri_proto | default(designate_service_proto) }}"" designate_service_publicuri: ""{{ designate_service_publicuri_proto }}://{{ external_lb_vip_address }}:{{ designate_service_port }}"" designate_service_publicurl: ""{{ designate_service_publicuri }}/v2"" ",,33,0
openstack%2Fpython-tripleoclient~master~I872a04f6c7d1c256d126b32303d73bafc6e8fd89,openstack/python-tripleoclient,master,I872a04f6c7d1c256d126b32303d73bafc6e8fd89,Switch to tripleomaster registry,MERGED,2017-12-08 15:55:23.000000000,2017-12-16 02:31:06.000000000,2017-12-16 02:31:06.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2017-12-08 15:55:23.000000000', 'files': ['tripleoclient/v1/undercloud_deploy.py', 'tripleoclient/tests/v1/overcloud_image/test_container_image.py'], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/9f6961b5737430f7badf5b2e3d3196c1703be2ec', 'message': 'Switch to tripleomaster registry\n\nThe tripleoupstream registry is deprecated in favor of tripleomaster,\nwhere images are pushed automatically on rdo promotion.\n\nChange-Id: I872a04f6c7d1c256d126b32303d73bafc6e8fd89\n'}]",0,526714,9f6961b5737430f7badf5b2e3d3196c1703be2ec,9,4,1,13039,,,0,"Switch to tripleomaster registry

The tripleoupstream registry is deprecated in favor of tripleomaster,
where images are pushed automatically on rdo promotion.

Change-Id: I872a04f6c7d1c256d126b32303d73bafc6e8fd89
",git fetch https://review.opendev.org/openstack/python-tripleoclient refs/changes/14/526714/1 && git format-patch -1 --stdout FETCH_HEAD,"['tripleoclient/v1/undercloud_deploy.py', 'tripleoclient/tests/v1/overcloud_image/test_container_image.py']",2,9f6961b5737430f7badf5b2e3d3196c1703be2ec,tripleomaster-registry," 'namespace': 'docker.io/tripleomaster', namespace='docker.io/tripleomaster',"," 'namespace': 'docker.io/tripleoupstream', namespace='docker.io/tripleoupstream',",4,4
openstack%2Fheat~master~I9149a09b534c33f52b2073318e2a3bafdc05e65a,openstack/heat,master,I9149a09b534c33f52b2073318e2a3bafdc05e65a,Imported Translations from Zanata,MERGED,2017-12-13 07:45:28.000000000,2017-12-16 02:30:57.000000000,2017-12-16 02:30:56.000000000,"[{'_account_id': 12404}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-13 07:45:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/4dba7885c197191815f6b811c405f071c1ca9655', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I9149a09b534c33f52b2073318e2a3bafdc05e65a\n'}, {'number': 2, 'created': '2017-12-14 07:37:12.000000000', 'files': ['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po', 'heat/locale/de/LC_MESSAGES/heat.po'], 'web_link': 'https://opendev.org/openstack/heat/commit/2572705698c2b0dc3c005953e6969690d7d16bff', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I9149a09b534c33f52b2073318e2a3bafdc05e65a\n'}]",0,527618,2572705698c2b0dc3c005953e6969690d7d16bff,14,2,2,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: I9149a09b534c33f52b2073318e2a3bafdc05e65a
",git fetch https://review.opendev.org/openstack/heat refs/changes/18/527618/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po', 'heat/locale/de/LC_MESSAGES/heat.po']",2,4dba7885c197191815f6b811c405f071c1ca9655,zanata/translations,"""Project-Id-Version: heat 10.0.0.0b3.dev14\n""""POT-Creation-Date: 2017-12-13 07:37+0000\n""","""Project-Id-Version: heat 10.0.0.dev107\n""""POT-Creation-Date: 2017-10-06 21:39+0000\n""msgid ""The following resource types could not be found: %s"" msgstr ""Die folgenden Ressourcetypen konnten nicht gefunden werden: %s"" #, python-format",40,21
openstack%2Ftempest~master~I78ceb1aaa3e96f6b76fa94aba50527022c3e2d45,openstack/tempest,master,I78ceb1aaa3e96f6b76fa94aba50527022c3e2d45,Add CONF.compute_feature_enabled.volume_backed_live_migration,MERGED,2017-12-14 21:55:53.000000000,2017-12-16 02:24:08.000000000,2017-12-16 02:24:07.000000000,"[{'_account_id': 5689}, {'_account_id': 6167}, {'_account_id': 8556}, {'_account_id': 10385}, {'_account_id': 22348}, {'_account_id': 23186}]","[{'number': 1, 'created': '2017-12-14 21:55:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/77d880a21b2106ddb0ba94b223a386f66e433de1', 'message': 'Add CONF.compute_feature_enabled.volume_backed_live_migration\n\nThe test_volume_backed_live_migration test has been skipped for some\ntime because of a bug [1] which could possibly be resolved with newer\nlibvirt and qemu package versions available in the pike UCA.\n\nThis changes the unconditional skip to a config option controlled skip\nso that we can try re-enabling this test on the master branch for nova\nchanges.\n\n[1] https://bugs.launchpad.net/nova/+bug/1524898\n\nChange-Id: I78ceb1aaa3e96f6b76fa94aba50527022c3e2d45\n'}, {'number': 2, 'created': '2017-12-15 16:54:08.000000000', 'files': ['tempest/api/compute/admin/test_live_migration.py', 'tempest/config.py', 'releasenotes/notes/volume-backed-live-mig-5a38b496ba1ec093.yaml'], 'web_link': 'https://opendev.org/openstack/tempest/commit/334f313220b97761a9bab3976e72f4cef4f53b3a', 'message': 'Add CONF.compute_feature_enabled.volume_backed_live_migration\n\nThe test_volume_backed_live_migration test has been skipped for some\ntime because of a bug [1] which could possibly be resolved with newer\nlibvirt and qemu package versions available in the pike UCA.\n\nThis changes the unconditional skip to a config option controlled skip\nso that we can try re-enabling this test on the master branch for nova\nchanges.\n\n[1] https://bugs.launchpad.net/nova/+bug/1524898\n\nChange-Id: I78ceb1aaa3e96f6b76fa94aba50527022c3e2d45\n'}]",1,528098,334f313220b97761a9bab3976e72f4cef4f53b3a,13,6,2,4690,,,0,"Add CONF.compute_feature_enabled.volume_backed_live_migration

The test_volume_backed_live_migration test has been skipped for some
time because of a bug [1] which could possibly be resolved with newer
libvirt and qemu package versions available in the pike UCA.

This changes the unconditional skip to a config option controlled skip
so that we can try re-enabling this test on the master branch for nova
changes.

[1] https://bugs.launchpad.net/nova/+bug/1524898

Change-Id: I78ceb1aaa3e96f6b76fa94aba50527022c3e2d45
",git fetch https://review.opendev.org/openstack/tempest refs/changes/98/528098/2 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/compute/admin/test_live_migration.py', 'tempest/config.py']",2,77d880a21b2106ddb0ba94b223a386f66e433de1,volume_backed_live_migration," cfg.BoolOpt('volume_backed_live_migration', default=False, help='Does the test environment support volume-backed live ' 'migration?'),",,7,1
openstack%2Ftacker~master~I04cfafd111b1024ad61b8f9ee48bfe0186a97d0d,openstack/tacker,master,I04cfafd111b1024ad61b8f9ee48bfe0186a97d0d,Add support symmetric parameter in VNFFG,MERGED,2017-12-06 03:23:34.000000000,2017-12-16 02:19:10.000000000,2017-12-16 02:19:10.000000000,"[{'_account_id': 2874}, {'_account_id': 18955}, {'_account_id': 19644}, {'_account_id': 20560}, {'_account_id': 22348}, {'_account_id': 25505}, {'_account_id': 26222}]","[{'number': 1, 'created': '2017-12-06 03:23:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/1cb91f61bf8515be89f0b27894ab4426b89ed00f', 'message': ""Add support symmetric parameter\n\nUpdate Symmetric Port Chains by setting 'symmetric=True' as\n'chain_parameters' key and value pair.\n\nChange-Id: I04cfafd111b1024ad61b8f9ee48bfe0186a97d0d\nCloses-Bug: #1736630\n""}, {'number': 2, 'created': '2017-12-06 14:20:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/251a72f3cad7938d9f2f456473547c8da6f9568d', 'message': ""Add support symmetric parameter in VNFFG\n\nThis patch address\n1. Update Symmetric Port Chains by setting 'symmetric=True' as\n'chain_parameters' key and value pair.\n2. Remove 'symmetrical' in flow-classifier functions.\n3. Update the doc and VNFFG samples\n\nChange-Id: I04cfafd111b1024ad61b8f9ee48bfe0186a97d0d\nCloses-Bug: #1736630\n""}, {'number': 3, 'created': '2017-12-06 14:26:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/5051be2f51a1799be19763fd2aa3a35cdd366826', 'message': ""Add support symmetric parameter in VNFFG\n\nThis patch address\n1. Update Symmetric Port Chains by setting 'symmetric=True' as\n'chain_parameters' key and value pair.\n2. Remove 'symmetrical' in flow-classifier functions.\n3. Update the doc and VNFFG samples\n\nChange-Id: I04cfafd111b1024ad61b8f9ee48bfe0186a97d0d\nCloses-Bug: #1736630\n""}, {'number': 4, 'created': '2017-12-06 14:41:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/54bbd467d3baff115bd693895757d4385c1ba02b', 'message': ""Add support symmetric parameter in VNFFG\n\nThis patch address\n1. Update Symmetric Port Chains by setting 'symmetric=True' as\n'chain_parameters' key and value pair.\n2. Remove 'symmetrical' in flow-classifier functions.\n3. Update the doc and VNFFG samples\n\nChange-Id: I04cfafd111b1024ad61b8f9ee48bfe0186a97d0d\nCloses-Bug: #1736630\n""}, {'number': 5, 'created': '2017-12-07 11:30:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/497c44408037ffeaeab6fdfc820118a129434c03', 'message': ""Add support symmetric parameter in VNFFG\n\nThis patch address\n1. Update Symmetric Port Chains by setting 'symmetric=True' as\n'chain_parameters' key and value pair.\n2. Remove 'symmetrical' in flow-classifier functions.\n3. Update the doc and VNFFG samples\n\nChange-Id: I04cfafd111b1024ad61b8f9ee48bfe0186a97d0d\nCloses-Bug: #1736630\n""}, {'number': 6, 'created': '2017-12-07 11:33:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/e0b2ed8d86cff5a175a4e3558378221929fdbcb7', 'message': ""Add support symmetric parameter in VNFFG\n\nThis patch address\n1. Update Symmetric Port Chains by setting 'symmetric=True' as\n'chain_parameters' key and value pair.\n2. Remove 'symmetrical' in flow-classifier functions.\n3. Update the doc and VNFFG samples\n\nChange-Id: I04cfafd111b1024ad61b8f9ee48bfe0186a97d0d\nCloses-Bug: #1736630\n""}, {'number': 7, 'created': '2017-12-12 11:17:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker/commit/2c49367bebc44b8c2ec44ba8ee5eea9da40d73fb', 'message': ""Add support symmetric parameter in VNFFG\n\nThis patch address\n1. Update Symmetric Port Chains by setting 'symmetric=True' as\n'chain_parameters' key and value pair.\n2. Remove 'symmetrical' in flow-classifier functions.\n3. Update the doc and VNFFG samples\n\nChange-Id: I04cfafd111b1024ad61b8f9ee48bfe0186a97d0d\nCloses-Bug: #1736630\n""}, {'number': 8, 'created': '2017-12-13 07:23:45.000000000', 'files': ['samples/tosca-templates/vnffgd/tosca-vnffgd-param-sample.yaml', 'doc/source/user/vnffg_usage_guide.rst', 'tacker/nfvo/drivers/vim/openstack_driver.py', 'samples/tosca-templates/vnffgd/tosca-vnffgd-symmetrical-sample.yaml', 'tacker/nfvo/nfvo_plugin.py', 'tacker/nfvo/drivers/vnffg/abstract_vnffg_driver.py'], 'web_link': 'https://opendev.org/openstack/tacker/commit/a546f58868959ac0be2e4bb67776dfcddd40c289', 'message': ""Add support symmetric parameter in VNFFG\n\nThis patch address\n1. Update Symmetric Port Chains by setting 'symmetric=True' as\n'chain_parameters' key and value pair.\n2. Remove 'symmetrical' in flow-classifier functions.\n3. Update the doc and VNFFG samples\n\nChange-Id: I04cfafd111b1024ad61b8f9ee48bfe0186a97d0d\nCloses-Bug: #1736630\n""}]",18,525863,a546f58868959ac0be2e4bb67776dfcddd40c289,31,7,8,26222,,,0,"Add support symmetric parameter in VNFFG

This patch address
1. Update Symmetric Port Chains by setting 'symmetric=True' as
'chain_parameters' key and value pair.
2. Remove 'symmetrical' in flow-classifier functions.
3. Update the doc and VNFFG samples

Change-Id: I04cfafd111b1024ad61b8f9ee48bfe0186a97d0d
Closes-Bug: #1736630
",git fetch https://review.opendev.org/openstack/tacker refs/changes/63/525863/8 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/user/vnffg_usage_guide.rst', 'tacker/nfvo/drivers/vim/openstack_driver.py']",2,1cb91f61bf8515be89f0b27894ab4426b89ed00f,bug/1736630," if symmetrical: port_chain['symmetric'] = True sfc_pc_params = {} sfc_pc_params['symmetric'] = symmetrical neutronclient_ = NeutronClient(auth_attr) return neutronclient_.port_chain_update(chain_id, sfc_pc_params) def port_chain_update(self, port_chain_id, update_pc): update_pc_dict = {'port_chain': update_pc} return self.client.update_port_chain(port_chain_id, update_pc_dict) "," if symmetrical: LOG.warning(""n-sfc driver does not support symmetrical"") raise NotImplementedError('symmetrical chain not supported') if symmetrical: LOG.warning(""n-sfc driver does not support symmetrical"") raise NotImplementedError('symmetrical chain not supported') LOG.warning(""n-sfc driver does not support sf chain update"") raise NotImplementedError('sf chain update not supported') if symmetrical: LOG.warning(""n-sfc driver does not support symmetrical"") raise NotImplementedError('symmetrical chain not supported') ",11,19
openstack%2Fmonasca-agent~master~I42767eaaf29d872c444a36017eb1972f108176b5,openstack/monasca-agent,master,I42767eaaf29d872c444a36017eb1972f108176b5,Add container memory percentage metric when running in k8s,MERGED,2017-12-13 16:36:51.000000000,2017-12-16 01:40:35.000000000,2017-12-16 01:40:35.000000000,"[{'_account_id': 11809}, {'_account_id': 14273}, {'_account_id': 14517}, {'_account_id': 15027}, {'_account_id': 18179}, {'_account_id': 22157}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-13 16:36:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/ad2787f874208916ea4b801bcc980aa1dbf55155', 'message': 'Add container memory percentage metric when running in k8s\n\nChange-Id: I42767eaaf29d872c444a36017eb1972f108176b5\n'}, {'number': 2, 'created': '2017-12-13 16:46:24.000000000', 'files': ['conf.d/kubernetes.yaml.example', 'docs/Plugins.md', 'monasca_agent/collector/checks_d/kubernetes.py'], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/cc6f1b9ec2a8d178a0b59cffb3063f83b7ffffe5', 'message': 'Add container memory percentage metric when running in k8s\n\nChange-Id: I42767eaaf29d872c444a36017eb1972f108176b5\n'}]",2,527738,cc6f1b9ec2a8d178a0b59cffb3063f83b7ffffe5,13,7,2,15027,,,0,"Add container memory percentage metric when running in k8s

Change-Id: I42767eaaf29d872c444a36017eb1972f108176b5
",git fetch https://review.opendev.org/openstack/monasca-agent refs/changes/38/527738/1 && git format-patch -1 --stdout FETCH_HEAD,"['conf.d/kubernetes.yaml.example', 'docs/Plugins.md', 'monasca_agent/collector/checks_d/kubernetes.py']",3,ad2787f874208916ea4b801bcc980aa1dbf55155,feature/add_memory_percent_metric,"import time self.report_container_mem_percent = init_config.get('report_container_mem_percent', True) memory_limit_map = {} pod_dimensions_map, memory_limit_map) pod_dimensions_map, memory_limit_map) def _process_pods(self, pods, kubernetes_labels, dimensions, container_dimension_map, pod_dimensions_map, memory_limit_map): if self.report_container_metrics or self.report_container_mem_percent: self._report_container_limits(pod_containers, container_dimension_map, name2id, memory_limit_map) def _report_container_limits(self, pod_containers, container_dimension_map, name2id, memory_limit_map): if self.report_container_metrics: self.gauge(""container.cpu.limit"", cpu_value, container_dimensions, hostname=""SUPPRESS"") if self.report_container_metrics: self.gauge(""container.memory.limit_bytes"", memory_in_bytes, container_dimensions, hostname=""SUPPRESS"") if self.report_container_mem_percent: container_key = container_name + "" "" + container_dimensions[""namespace""] if container_key not in memory_limit_map: memory_limit_map[container_key] = memory_in_bytes if self.report_container_metrics: self.gauge(""container.request.cpu"", cpu_value, container_dimensions, hostname=""SUPPRESS"") if self.report_container_metrics: self.gauge(""container.request.memory_bytes"", memory_in_bytes, container_dimensions, hostname=""SUPPRESS"") def _parse_memory(self, memory_data, container_dimensions, pod_key, pod_map, memory_limit_map): if self.report_container_mem_percent and cadvisor_key == ""working_set"": if ""container_name"" in container_dimensions and ""namespace"" in container_dimensions: container_key = container_dimensions[""container_name""] + "" "" + container_dimensions[""namespace""] if container_key not in memory_limit_map: continue memory_limit = memory_limit_map[container_key] memory_usage = metric_value memory_usage_percent = (memory_usage/memory_limit) * 100 self.gauge(""container.mem.usage_percent"", memory_usage_percent, container_dimensions, hostname=""SUPPRESS"") def _process_containers(self, cadvisor_url, dimensions, container_dimension_map, pod_dimension_map, memory_limit_map): pod_metrics, memory_limit_map)"," pod_dimensions_map) pod_dimensions_map) def _process_pods(self, pods, kubernetes_labels, dimensions, container_dimension_map, pod_dimensions_map): if self.report_container_metrics: self._report_container_limits(pod_containers, container_dimension_map, name2id) def _report_container_limits(self, pod_containers, container_dimension_map, name2id): self.gauge(""container.cpu.limit"", cpu_value, container_dimensions, hostname=""SUPPRESS"") self.gauge(""container.memory.limit_bytes"", memory_in_bytes, container_dimensions, hostname=""SUPPRESS"") self.gauge(""container.request.cpu"", cpu_value, container_dimensions, hostname=""SUPPRESS"") self.gauge(""container.request.memory_bytes"", memory_in_bytes, container_dimensions, hostname=""SUPPRESS"") def _parse_memory(self, memory_data, container_dimensions, pod_key, pod_map): def _process_containers(self, cadvisor_url, dimensions, container_dimension_map, pod_dimension_map): pod_metrics)",45,16
openstack%2Fmanila~master~I6c87e77b5a55e1a22112c7fa5d2fdd9f0335b27c,openstack/manila,master,I6c87e77b5a55e1a22112c7fa5d2fdd9f0335b27c,Updated from global requirements,MERGED,2017-12-15 21:36:53.000000000,2017-12-16 01:19:45.000000000,2017-12-16 01:19:45.000000000,"[{'_account_id': 9003}, {'_account_id': 11047}, {'_account_id': 14384}, {'_account_id': 15831}, {'_account_id': 15942}, {'_account_id': 21863}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 25243}]","[{'number': 1, 'created': '2017-12-15 21:36:53.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/manila/commit/ba25aaf543d6835ac14eefcacd92ad1e70503224', 'message': 'Updated from global requirements\n\nChange-Id: I6c87e77b5a55e1a22112c7fa5d2fdd9f0335b27c\n'}]",0,528407,ba25aaf543d6835ac14eefcacd92ad1e70503224,13,9,1,11131,,,0,"Updated from global requirements

Change-Id: I6c87e77b5a55e1a22112c7fa5d2fdd9f0335b27c
",git fetch https://review.opendev.org/openstack/manila refs/changes/07/528407/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,ba25aaf543d6835ac14eefcacd92ad1e70503224,openstack/requirements,"oslo.service!=1.28.1,>=1.24.0 # Apache-2.0",oslo.service>=1.24.0 # Apache-2.0,1,1
openstack%2Fsahara~master~Iffdab388b88b1ce35ce34c684255037d64e8f2af,openstack/sahara,master,Iffdab388b88b1ce35ce34c684255037d64e8f2af,Updated from global requirements,MERGED,2017-12-15 22:16:52.000000000,2017-12-16 00:51:44.000000000,2017-12-16 00:51:44.000000000,"[{'_account_id': 10459}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 22:16:52.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/sahara/commit/a8d3c48672c50286193cbdf0facd8d57c19f42f7', 'message': 'Updated from global requirements\n\nChange-Id: Iffdab388b88b1ce35ce34c684255037d64e8f2af\n'}]",0,528427,a8d3c48672c50286193cbdf0facd8d57c19f42f7,6,2,1,11131,,,0,"Updated from global requirements

Change-Id: Iffdab388b88b1ce35ce34c684255037d64e8f2af
",git fetch https://review.opendev.org/openstack/sahara refs/changes/27/528427/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,a8d3c48672c50286193cbdf0facd8d57c19f42f7,openstack/requirements,"oslo.service!=1.28.1,>=1.24.0 # Apache-2.0",oslo.service>=1.24.0 # Apache-2.0,1,1
openstack%2Finstack-undercloud~stable%2Fpike~I84f68e837a57e895b80fdaecf825e0227be67a37,openstack/instack-undercloud,stable/pike,I84f68e837a57e895b80fdaecf825e0227be67a37,Run nova online data migration before updating packages,MERGED,2017-12-15 14:45:39.000000000,2017-12-16 00:38:50.000000000,2017-12-16 00:38:50.000000000,"[{'_account_id': 6928}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2017-12-15 14:45:39.000000000', 'files': ['instack_undercloud/undercloud.py'], 'web_link': 'https://opendev.org/openstack/instack-undercloud/commit/5004fa5c77ceecc8c43ca9af827ec4786fbad394', 'message': 'Run nova online data migration before updating packages\n\nChange-Id: I84f68e837a57e895b80fdaecf825e0227be67a37\nCloses-bug: 1737175\n(cherry picked from commit e0a678970700785fc2b95c0b3bfe211e600a40d3)\n'}]",0,528287,5004fa5c77ceecc8c43ca9af827ec4786fbad394,12,4,1,23811,,,0,"Run nova online data migration before updating packages

Change-Id: I84f68e837a57e895b80fdaecf825e0227be67a37
Closes-bug: 1737175
(cherry picked from commit e0a678970700785fc2b95c0b3bfe211e600a40d3)
",git fetch https://review.opendev.org/openstack/instack-undercloud refs/changes/87/528287/1 && git format-patch -1 --stdout FETCH_HEAD,['instack_undercloud/undercloud.py'],1,5004fa5c77ceecc8c43ca9af827ec4786fbad394,bug/1737175," # Ensure nova data migrations are complete before upgrading packages LOG.info('Running Nova online data migration') _run_command(['sudo', '-E', '/usr/bin/nova-manage', 'db', 'online_data_migrations']) LOG.info('Nova online data migration completed') "," # We didn't complete the M->N upgrades correctly with a # `nova-manage db online_data_migrations` command before. This # could cause the post-upgrade db sync to fail. Better be safe # than sorry and run it before package upgrade. _run_command(['sudo', '/usr/bin/nova-manage', 'db', 'online_data_migrations'])",6,6
openstack%2Fpuppet-glance~stable%2Focata~I9984bdb416ec9d874345b410557ef42c360232fb,openstack/puppet-glance,stable/ocata,I9984bdb416ec9d874345b410557ef42c360232fb,Add swift_store_large_object_chunk_size parameter,MERGED,2017-12-08 13:56:35.000000000,2017-12-16 00:20:54.000000000,2017-12-16 00:20:54.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-08 13:56:35.000000000', 'files': ['manifests/backend/swift.pp', 'spec/classes/glance_backend_swift_spec.rb', 'releasenotes/notes/add_swift_store_large_object_chunk_size_parameter-98f34404c5a61843.yaml'], 'web_link': 'https://opendev.org/openstack/puppet-glance/commit/965d33582fea996f7cf8f77a4535ce417d4db3c0', 'message': 'Add swift_store_large_object_chunk_size parameter\n\nAdd a new parameter to swift backend to let user set the chunk size when\nglance will start chunking the images before uploading.\n\nChange-Id: I9984bdb416ec9d874345b410557ef42c360232fb\nSigned-off-by: Arnaud Morin <arnaud.morin@corp.ovh.com>\n(cherry picked from commit 2ce1ddfd83361d51228720ff1116020a98b5e949)\n'}]",0,526679,965d33582fea996f7cf8f77a4535ce417d4db3c0,8,3,1,11583,,,0,"Add swift_store_large_object_chunk_size parameter

Add a new parameter to swift backend to let user set the chunk size when
glance will start chunking the images before uploading.

Change-Id: I9984bdb416ec9d874345b410557ef42c360232fb
Signed-off-by: Arnaud Morin <arnaud.morin@corp.ovh.com>
(cherry picked from commit 2ce1ddfd83361d51228720ff1116020a98b5e949)
",git fetch https://review.opendev.org/openstack/puppet-glance refs/changes/79/526679/1 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/backend/swift.pp', 'spec/classes/glance_backend_swift_spec.rb', 'releasenotes/notes/add_swift_store_large_object_chunk_size_parameter-98f34404c5a61843.yaml']",3,965d33582fea996f7cf8f77a4535ce417d4db3c0,dev/arnaud.morin/add_swift_store_large_object_chunk_size_parameter-stable/ocata,"--- features: - Add new option glance::backend::swift::swift_store_large_object_chunk_size The swift_store_large_object_chunk_size is to be used in conjunction with swift_store_large_object_size. While swift_store_large_object_size is the trigger for glance to start chunking images, swift_store_large_object_chunk_size is the size of the chunks. ",,23,1
openstack%2Fnova~master~Ibb90d05d7827a2f78860182e24d62ad41c57e581,openstack/nova,master,Ibb90d05d7827a2f78860182e24d62ad41c57e581,Aggregate ops on ProviderTree,MERGED,2017-12-06 22:49:37.000000000,2017-12-16 00:17:10.000000000,2017-12-16 00:17:10.000000000,"[{'_account_id': 7}, {'_account_id': 9008}, {'_account_id': 9708}, {'_account_id': 9732}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 14070}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15286}, {'_account_id': 15334}, {'_account_id': 16128}, {'_account_id': 16898}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 25625}, {'_account_id': 26515}, {'_account_id': 27272}]","[{'number': 1, 'created': '2017-12-06 22:49:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3dd0a4025c0258eb01a22e0c06f6f1ca0e94bd9a', 'message': 'Aggregate ops on ProviderTree\n\nAdds the following methods on ProviderTree:\n\nin_aggregates: whether a specified provider is a member of *all*\naggregates in a given list\n\nhave_aggregates_changed: whether a new set of aggregates differs from\nthose already registered for a provider\n\nupdate_aggregates: set the list of aggregates with which a provider is\nassociated, returning whether this effected a change\n\nChange-Id: Ibb90d05d7827a2f78860182e24d62ad41c57e581\n'}, {'number': 2, 'created': '2017-12-07 15:23:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/04f97082d5714426128fb489f59da69557c64e9e', 'message': 'Aggregate ops on ProviderTree\n\nAdds the following methods on ProviderTree:\n\nin_aggregates: whether a specified provider is a member of *all*\naggregates in a given list\n\nhave_aggregates_changed: whether a new set of aggregates differs from\nthose already registered for a provider\n\nupdate_aggregates: set the list of aggregates with which a provider is\nassociated, returning whether this effected a change\n\nChange-Id: Ibb90d05d7827a2f78860182e24d62ad41c57e581\n'}, {'number': 3, 'created': '2017-12-07 23:26:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/6cb3a7f9e7019685dcb5a1be8c21b7edbf2c370e', 'message': 'Aggregate ops on ProviderTree\n\nAdds the following methods on ProviderTree:\n\nin_aggregates: whether a specified provider is a member of *all*\naggregates in a given list\n\nhave_aggregates_changed: whether a new set of aggregates differs from\nthose already registered for a provider\n\nupdate_aggregates: set the list of aggregates with which a provider is\nassociated, returning whether this effected a change\n\nChange-Id: Ibb90d05d7827a2f78860182e24d62ad41c57e581\n'}, {'number': 4, 'created': '2017-12-14 16:02:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/cecf3d332f15fa6e055a396c2e339a41890e077f', 'message': 'Aggregate ops on ProviderTree\n\nAdds the following methods on ProviderTree:\n\nin_aggregates: whether a specified provider is a member of *all*\naggregates in a given list\n\nhave_aggregates_changed: whether a new set of aggregates differs from\nthose already registered for a provider\n\nupdate_aggregates: set the list of aggregates with which a provider is\nassociated, returning whether this effected a change\n\nChange-Id: Ibb90d05d7827a2f78860182e24d62ad41c57e581\n'}, {'number': 5, 'created': '2017-12-15 12:16:58.000000000', 'files': ['nova/compute/provider_tree.py', 'nova/tests/unit/compute/test_provider_tree.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/06976f887b13fc96a9e7d0e2142bfb2b8abb52fa', 'message': 'Aggregate ops on ProviderTree\n\nAdds the following methods on ProviderTree:\n\nin_aggregates: whether a specified provider is a member of *all*\naggregates in a given list\n\nhave_aggregates_changed: whether a new set of aggregates differs from\nthose already registered for a provider\n\nupdate_aggregates: set the list of aggregates with which a provider is\nassociated, returning whether this effected a change\n\nChange-Id: Ibb90d05d7827a2f78860182e24d62ad41c57e581\n'}]",9,526216,06976f887b13fc96a9e7d0e2142bfb2b8abb52fa,95,18,5,14070,,,0,"Aggregate ops on ProviderTree

Adds the following methods on ProviderTree:

in_aggregates: whether a specified provider is a member of *all*
aggregates in a given list

have_aggregates_changed: whether a new set of aggregates differs from
those already registered for a provider

update_aggregates: set the list of aggregates with which a provider is
associated, returning whether this effected a change

Change-Id: Ibb90d05d7827a2f78860182e24d62ad41c57e581
",git fetch https://review.opendev.org/openstack/nova refs/changes/16/526216/3 && git format-patch -1 --stdout FETCH_HEAD,"['nova/compute/provider_tree.py', 'nova/tests/unit/compute/test_provider_tree.py']",2,3dd0a4025c0258eb01a22e0c06f6f1ca0e94bd9a,bp/nested-resource-providers," def test_have_aggregates_changed_no_existing_rp(self): cns = self.compute_nodes pt = provider_tree.ProviderTree(cns) self.assertRaises( ValueError, pt.have_aggregates_changed, uuids.non_existing_rp, []) def test_update_aggregates_no_existing_rp(self): cns = self.compute_nodes pt = provider_tree.ProviderTree(cns) self.assertRaises( ValueError, pt.update_aggregates, uuids.non_existing_rp, [], 1) def test_have_aggregates_changed(self): cn = self.compute_node1 cns = self.compute_nodes pt = provider_tree.ProviderTree(cns) rp_gen = 1 aggregates = [ uuids.agg1, uuids.agg2, ] self.assertTrue(pt.have_aggregates_changed(cn.uuid, aggregates)) self.assertTrue(pt.in_aggregates(cn.uuid, [])) self.assertFalse(pt.in_aggregates(cn.uuid, aggregates)) self.assertFalse(pt.in_aggregates(cn.uuid, aggregates[:1])) self.assertTrue(pt.update_aggregates(cn.uuid, aggregates, rp_gen)) self.assertTrue(pt.in_aggregates(cn.uuid, aggregates)) self.assertTrue(pt.in_aggregates(cn.uuid, aggregates[:1])) # Updating with the same aggregates info should return False self.assertFalse(pt.have_aggregates_changed(cn.uuid, aggregates)) # But the generation should get updated rp_gen = 2 self.assertFalse(pt.update_aggregates(cn.uuid, aggregates, rp_gen)) self.assertEqual(rp_gen, pt._find_with_lock(cn.uuid).generation) self.assertTrue(pt.in_aggregates(cn.uuid, aggregates)) self.assertTrue(pt.in_aggregates(cn.uuid, aggregates[:1])) # Make a change to the aggregates list aggregates.append(uuids.agg3) self.assertTrue(pt.have_aggregates_changed(cn.uuid, aggregates)) self.assertFalse(pt.in_aggregates(cn.uuid, aggregates[-1:])) # Update the generation again rp_gen = 3 self.assertTrue(pt.update_aggregates(cn.uuid, aggregates, rp_gen)) self.assertEqual(rp_gen, pt._find_with_lock(cn.uuid).generation) self.assertTrue(pt.in_aggregates(cn.uuid, aggregates[-1:])) self.assertFalse(pt.have_aggregates_changed(cn.uuid, aggregates)) self.assertFalse(pt.update_aggregates(cn.uuid, aggregates, rp_gen)) self.assertTrue(pt.in_aggregates(cn.uuid, aggregates)) self.assertTrue(pt.in_aggregates(cn.uuid, []))",,135,0
openstack%2Fnova~master~Ie85decf067f47c59e42ee4a3acefbe4246891ee1,openstack/nova,master,Ie85decf067f47c59e42ee4a3acefbe4246891ee1,Traits ops on ProviderTree,MERGED,2017-11-20 17:44:29.000000000,2017-12-16 00:11:14.000000000,2017-12-16 00:11:14.000000000,"[{'_account_id': 7}, {'_account_id': 1063}, {'_account_id': 9008}, {'_account_id': 9708}, {'_account_id': 9732}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 14070}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15286}, {'_account_id': 15334}, {'_account_id': 16128}, {'_account_id': 16898}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 25625}, {'_account_id': 26515}, {'_account_id': 27272}]","[{'number': 1, 'created': '2017-11-20 17:44:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/0e3b7ac366d0f4151a459755e97a3911b93eb402', 'message': ""placement: Traits ops on ProviderTree\n\nAdds the following methods on ProviderTree:\n\nhas_traits: whether a specified provider has all of the traits in a\ngiven list\n\nhave_traits_changed: whether a new set of traits differs from what's\nalready set on a provider\n\nupdate_traits: set the list of traits on a provider, returning whether\nthis effected a change\n\nChange-Id: Ie85decf067f47c59e42ee4a3acefbe4246891ee1\n""}, {'number': 2, 'created': '2017-11-20 23:58:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8fba5022ac8db8261a77d588979e1b3517093f60', 'message': ""Traits ops on ProviderTree\n\nAdds the following methods on ProviderTree:\n\nhas_traits: whether a specified provider has all of the traits in a\ngiven list\n\nhave_traits_changed: whether a new set of traits differs from what's\nalready set on a provider\n\nupdate_traits: set the list of traits on a provider, returning whether\nthis effected a change\n\nChange-Id: Ie85decf067f47c59e42ee4a3acefbe4246891ee1\n""}, {'number': 3, 'created': '2017-12-06 22:49:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e6499728c86e03ce2b206ac9589819edaa26f3f9', 'message': ""Traits ops on ProviderTree\n\nAdds the following methods on ProviderTree:\n\nhas_traits: whether a specified provider has all of the traits in a\ngiven list\n\nhave_traits_changed: whether a new set of traits differs from what's\nalready set on a provider\n\nupdate_traits: set the list of traits on a provider, returning whether\nthis effected a change\n\nChange-Id: Ie85decf067f47c59e42ee4a3acefbe4246891ee1\n""}, {'number': 4, 'created': '2017-12-07 15:23:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f627da8f091bc362cadff1b7dba11b2d2a0c5547', 'message': ""Traits ops on ProviderTree\n\nAdds the following methods on ProviderTree:\n\nhas_traits: whether a specified provider has all of the traits in a\ngiven list\n\nhave_traits_changed: whether a new set of traits differs from what's\nalready set on a provider\n\nupdate_traits: set the list of traits on a provider, returning whether\nthis effected a change\n\nChange-Id: Ie85decf067f47c59e42ee4a3acefbe4246891ee1\n""}, {'number': 5, 'created': '2017-12-07 23:26:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/204c591f8c2fa8d745572fcb9bd26c13f65012cc', 'message': ""Traits ops on ProviderTree\n\nAdds the following methods on ProviderTree:\n\nhas_traits: whether a specified provider has all of the traits in a\ngiven list\n\nhave_traits_changed: whether a new set of traits differs from what's\nalready set on a provider\n\nupdate_traits: set the list of traits on a provider, returning whether\nthis effected a change\n\nChange-Id: Ie85decf067f47c59e42ee4a3acefbe4246891ee1\n""}, {'number': 6, 'created': '2017-12-13 17:48:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/296fc0295ff81814ed874107cac3a663ac2f3270', 'message': ""Traits ops on ProviderTree\n\nAdds the following methods on ProviderTree:\n\nhas_traits: whether a specified provider has all of the traits in a\ngiven list\n\nmatches_traits: whether a new set of traits differs from what's already\nset on a provider\n\nupdate_traits: set the list of traits on a provider, returning whether\nthis effected a change\n\nChange-Id: Ie85decf067f47c59e42ee4a3acefbe4246891ee1\n""}, {'number': 7, 'created': '2017-12-14 16:02:20.000000000', 'files': ['nova/compute/provider_tree.py', 'nova/tests/unit/compute/test_provider_tree.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/f029350c8965ba69b6862bd9a7825c05c8afc095', 'message': ""Traits ops on ProviderTree\n\nAdds the following methods on ProviderTree:\n\nhas_traits: whether a specified provider has all of the traits in a\ngiven list\n\nhave_traits_changed: whether a new set of traits differs from what's\nalready set on a provider\n\nupdate_traits: set the list of traits on a provider, returning whether\nthis effected a change\n\nChange-Id: Ie85decf067f47c59e42ee4a3acefbe4246891ee1\n""}]",26,521605,f029350c8965ba69b6862bd9a7825c05c8afc095,118,19,7,14070,,,0,"Traits ops on ProviderTree

Adds the following methods on ProviderTree:

has_traits: whether a specified provider has all of the traits in a
given list

have_traits_changed: whether a new set of traits differs from what's
already set on a provider

update_traits: set the list of traits on a provider, returning whether
this effected a change

Change-Id: Ie85decf067f47c59e42ee4a3acefbe4246891ee1
",git fetch https://review.opendev.org/openstack/nova refs/changes/05/521605/3 && git format-patch -1 --stdout FETCH_HEAD,"['nova/compute/provider_tree.py', 'nova/tests/unit/compute/test_provider_tree.py']",2,0e3b7ac366d0f4151a459755e97a3911b93eb402,bp/nested-resource-providers," def test_have_traits_changed_no_existing_rp(self): cns = self.compute_nodes pt = provider_tree.ProviderTree(cns) self.assertRaises( ValueError, pt.have_traits_changed, uuids.non_existing_rp, []) def test_update_traits_no_existing_rp(self): cns = self.compute_nodes pt = provider_tree.ProviderTree(cns) self.assertRaises( ValueError, pt.update_traits, uuids.non_existing_rp, [], 1) def test_have_traits_changed(self): cn = self.compute_node1 cns = self.compute_nodes pt = provider_tree.ProviderTree(cns) rp_gen = 1 traits = [ ""HW_GPU_API_DIRECT3D_V7_0"", ""HW_NIC_OFFLOAD_SG"", ""HW_CPU_X86_AVX"", ] self.assertTrue(pt.have_traits_changed(cn.uuid, traits)) self.assertTrue(pt.update_traits(cn.uuid, traits, rp_gen)) # Updating with the same traits info should return False self.assertFalse(pt.have_traits_changed(cn.uuid, traits)) # But the generation should get updated rp_gen = 2 self.assertFalse(pt.update_traits(cn.uuid, traits, rp_gen)) self.assertEqual(rp_gen, pt.find(cn.uuid).generation) # Make a change to the traits list traits.append(""HW_GPU_RESOLUTION_W800H600"") self.assertTrue(pt.have_traits_changed(cn.uuid, traits)) # Update the generation again rp_gen = 3 self.assertTrue(pt.update_traits(cn.uuid, traits, rp_gen)) self.assertEqual(rp_gen, pt.find(cn.uuid).generation) self.assertFalse(pt.have_traits_changed(cn.uuid, traits)) self.assertFalse(pt.update_traits(cn.uuid, traits, rp_gen))",,133,6
openstack%2Fpuppet-openstack-integration~master~I2ee3ed7e8e473115d89f5b6c9c8b0c261c35cbe7,openstack/puppet-openstack-integration,master,I2ee3ed7e8e473115d89f5b6c9c8b0c261c35cbe7,Updated from Puppet OpenStack modules constraints,MERGED,2017-12-15 06:03:43.000000000,2017-12-16 00:10:59.000000000,2017-12-16 00:10:59.000000000,"[{'_account_id': 3153}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 06:03:43.000000000', 'files': ['Puppetfile'], 'web_link': 'https://opendev.org/openstack/puppet-openstack-integration/commit/994eaa15052a0f16a5645e3c7af8747096d1d276', 'message': 'Updated from Puppet OpenStack modules constraints\n\nChange-Id: I2ee3ed7e8e473115d89f5b6c9c8b0c261c35cbe7\n'}]",0,528160,994eaa15052a0f16a5645e3c7af8747096d1d276,8,2,1,11131,,,0,"Updated from Puppet OpenStack modules constraints

Change-Id: I2ee3ed7e8e473115d89f5b6c9c8b0c261c35cbe7
",git fetch https://review.opendev.org/openstack/puppet-openstack-integration refs/changes/60/528160/1 && git format-patch -1 --stdout FETCH_HEAD,['Puppetfile'],1,994eaa15052a0f16a5645e3c7af8747096d1d276,openstack/puppet/constraints, :ref => '5.1.0', :ref => '5.0.1',1,1
openstack%2Fopenstack-ansible-haproxy_server~stable%2Focata~I43259dffbe10b2209545de40bd40b21fb785706d,openstack/openstack-ansible-haproxy_server,stable/ocata,I43259dffbe10b2209545de40bd40b21fb785706d,Set python interpreter for HATop download,MERGED,2017-12-15 14:46:07.000000000,2017-12-15 23:59:13.000000000,2017-12-15 23:59:13.000000000,"[{'_account_id': 538}, {'_account_id': 14805}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 14:46:07.000000000', 'files': ['tasks/haproxy_install.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-haproxy_server/commit/61a5fe0bbbffb989211324d8f4747582680839ad', 'message': 'Set python interpreter for HATop download\n\nThe HATop download step uses the python in the ansible virtualenv\nwhich does not have proper SELinux python bindings. This patch ensures\nthat the system python is used.\n\nConflicts:\n>------tests/host_vars/localhost.yml\n\nCloses-Bug: 1738416\nChange-Id: I43259dffbe10b2209545de40bd40b21fb785706d\n(cherry picked from commit 57991a111e9a312ba41b4d8f03193187e12730c4)\n'}]",0,528290,61a5fe0bbbffb989211324d8f4747582680839ad,7,3,1,6816,,,0,"Set python interpreter for HATop download

The HATop download step uses the python in the ansible virtualenv
which does not have proper SELinux python bindings. This patch ensures
that the system python is used.

Conflicts:
>------tests/host_vars/localhost.yml

Closes-Bug: 1738416
Change-Id: I43259dffbe10b2209545de40bd40b21fb785706d
(cherry picked from commit 57991a111e9a312ba41b4d8f03193187e12730c4)
",git fetch https://review.opendev.org/openstack/openstack-ansible-haproxy_server refs/changes/90/528290/1 && git format-patch -1 --stdout FETCH_HEAD,['tasks/haproxy_install.yml'],1,61a5fe0bbbffb989211324d8f4747582680839ad,bug/1738416-stable/ocata," vars: ansible_python_interpreter: ""/usr/bin/python""",,2,0
openstack%2Fopenstack-ansible-haproxy_server~stable%2Fnewton~I43259dffbe10b2209545de40bd40b21fb785706d,openstack/openstack-ansible-haproxy_server,stable/newton,I43259dffbe10b2209545de40bd40b21fb785706d,Set python interpreter for HATop download,MERGED,2017-12-15 14:48:26.000000000,2017-12-15 23:59:12.000000000,2017-12-15 23:59:12.000000000,"[{'_account_id': 538}, {'_account_id': 14805}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-12-15 14:48:26.000000000', 'files': ['tasks/haproxy_install_hatop.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-haproxy_server/commit/ba0e13efafd8b380c48d290ebfa579e2c9753c7b', 'message': 'Set python interpreter for HATop download\n\nThe HATop download step uses the python in the ansible virtualenv\nwhich does not have proper SELinux python bindings. This patch ensures\nthat the system python is used.\n\nConflicts:\n>------tasks/haproxy_install.yml\n>------tests/host_vars/localhost.yml\n\nCloses-Bug: 1738416\nChange-Id: I43259dffbe10b2209545de40bd40b21fb785706d\n(cherry picked from commit 57991a111e9a312ba41b4d8f03193187e12730c4)\n'}]",0,528293,ba0e13efafd8b380c48d290ebfa579e2c9753c7b,7,3,1,6816,,,0,"Set python interpreter for HATop download

The HATop download step uses the python in the ansible virtualenv
which does not have proper SELinux python bindings. This patch ensures
that the system python is used.

Conflicts:
>------tasks/haproxy_install.yml
>------tests/host_vars/localhost.yml

Closes-Bug: 1738416
Change-Id: I43259dffbe10b2209545de40bd40b21fb785706d
(cherry picked from commit 57991a111e9a312ba41b4d8f03193187e12730c4)
",git fetch https://review.opendev.org/openstack/openstack-ansible-haproxy_server refs/changes/93/528293/1 && git format-patch -1 --stdout FETCH_HEAD,['tasks/haproxy_install_hatop.yml'],1,ba0e13efafd8b380c48d290ebfa579e2c9753c7b,bug/1738416," vars: ansible_python_interpreter: ""/usr/bin/python""",,2,0
