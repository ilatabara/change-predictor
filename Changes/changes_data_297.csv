id,project,branch,change_id,subject,status,created,updated,submitted,reviewers,revisions,total_comment_count,number,current_revision,discussion_messages_count,reviewers_count,revisions_count,owner_account_id,owner_name,owner_username,is_owner_bot,commit_message,git_command,changed_files,files_count,commit_id,topic,added_lines,deleted_lines,insertions,deletions
openstack%2Fpuppet-rally~master~I6c972a5bc9ceb6aa22018c5889407581f3e09aa4,openstack/puppet-rally,master,I6c972a5bc9ceb6aa22018c5889407581f3e09aa4,Remove idle_timeout option,MERGED,2020-03-23 06:47:18.000000000,2020-03-23 23:49:07.000000000,2020-03-23 23:47:32.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 06:47:18.000000000', 'files': ['manifests/db.pp', 'releasenotes/notes/remove_idle_timeout_option-a0cdd30b30b8c01a.yaml'], 'web_link': 'https://opendev.org/openstack/puppet-rally/commit/a7884ad157365b865ac65f6bd93ef502d9e27fb2', 'message': 'Remove idle_timeout option\n\nThe idle_timeout parameter has been deprecated for two releases.\nWe can remove it.\n\nChange-Id: I6c972a5bc9ceb6aa22018c5889407581f3e09aa4\n'}]",0,714380,a7884ad157365b865ac65f6bd93ef502d9e27fb2,8,3,1,9414,,,0,"Remove idle_timeout option

The idle_timeout parameter has been deprecated for two releases.
We can remove it.

Change-Id: I6c972a5bc9ceb6aa22018c5889407581f3e09aa4
",git fetch https://review.opendev.org/openstack/puppet-rally refs/changes/80/714380/1 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/db.pp', 'releasenotes/notes/remove_idle_timeout_option-a0cdd30b30b8c01a.yaml']",2,a7884ad157365b865ac65f6bd93ef502d9e27fb2,remove_idle_timeout_option,--- upgrade: - Deprecated idle_timeout option has been removed. ,,4,14
openstack%2Fpuppet-cloudkitty~master~I869d590dce1999f2242d4c82e569c8bd23f0d53e,openstack/puppet-cloudkitty,master,I869d590dce1999f2242d4c82e569c8bd23f0d53e,Remove idle_timeout option,MERGED,2020-03-23 06:47:07.000000000,2020-03-23 23:43:17.000000000,2020-03-23 23:41:23.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 06:47:07.000000000', 'files': ['manifests/db.pp', 'releasenotes/notes/remove_idle_timeout_option-837149cddbb363fe.yaml'], 'web_link': 'https://opendev.org/openstack/puppet-cloudkitty/commit/d3268e989c2d4732be842367cfeacd8e9aac82c1', 'message': 'Remove idle_timeout option\n\nThe idle_timeout parameter has been deprecated for two releases.\nWe can remove it.\n\nChange-Id: I869d590dce1999f2242d4c82e569c8bd23f0d53e\n'}]",0,714361,d3268e989c2d4732be842367cfeacd8e9aac82c1,8,3,1,9414,,,0,"Remove idle_timeout option

The idle_timeout parameter has been deprecated for two releases.
We can remove it.

Change-Id: I869d590dce1999f2242d4c82e569c8bd23f0d53e
",git fetch https://review.opendev.org/openstack/puppet-cloudkitty refs/changes/61/714361/1 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/db.pp', 'releasenotes/notes/remove_idle_timeout_option-837149cddbb363fe.yaml']",2,d3268e989c2d4732be842367cfeacd8e9aac82c1,remove_idle_timeout_option,--- upgrade: - Deprecated idle_timeout option has been removed. ,,4,15
openstack%2Ftripleo-puppet-elements~master~I43567a716b4e5c6a8be0924c2b1ca34e9073c313,openstack/tripleo-puppet-elements,master,I43567a716b4e5c6a8be0924c2b1ca34e9073c313,Cleanup stale interface if exists,ABANDONED,2020-03-11 17:48:21.000000000,2020-03-23 23:37:56.000000000,,"[{'_account_id': 3153}, {'_account_id': 9592}, {'_account_id': 14985}, {'_account_id': 21909}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-11 17:48:21.000000000', 'files': ['elements/overcloud-base/pre-install.d/02-clean-stale-interface'], 'web_link': 'https://opendev.org/openstack/tripleo-puppet-elements/commit/d1538442d3f435594a505d79286d498b50eeea15', 'message': ""Cleanup stale interface if exists\n\nThere is a bug in the CentOS 8 image where the ens3 interface file\nexists. We should clean that up if it exists to prevent issues when\nbooting. We manage the interfaces later with os-net-config so we don't\nwant them to exist in the image.\n\nCloses-Bug: #1866202\nChange-Id: I43567a716b4e5c6a8be0924c2b1ca34e9073c313\n""}]",0,712521,d1538442d3f435594a505d79286d498b50eeea15,11,6,1,9592,,,0,"Cleanup stale interface if exists

There is a bug in the CentOS 8 image where the ens3 interface file
exists. We should clean that up if it exists to prevent issues when
booting. We manage the interfaces later with os-net-config so we don't
want them to exist in the image.

Closes-Bug: #1866202
Change-Id: I43567a716b4e5c6a8be0924c2b1ca34e9073c313
",git fetch https://review.opendev.org/openstack/tripleo-puppet-elements refs/changes/21/712521/1 && git format-patch -1 --stdout FETCH_HEAD,['elements/overcloud-base/pre-install.d/02-clean-stale-interface'],1,d1538442d3f435594a505d79286d498b50eeea15,,#!/bin/bash set -eux set -o pipefail # https://bugs.centos.org/view.php?id=17133 if [ -f /etc/sysconfig/network-scripts/ifcfg-ens3 ]; then rm -f /etc/sysconfig/network-scripts/ifcfg-ens3 fi ,,9,0
openstack%2Fpuppet-glare~master~I5beee515ab6ceb5df7ef54fd35189a5c72ba242f,openstack/puppet-glare,master,I5beee515ab6ceb5df7ef54fd35189a5c72ba242f,Remove idle_timeout option,MERGED,2020-03-23 06:47:10.000000000,2020-03-23 23:37:47.000000000,2020-03-23 23:35:38.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 06:47:10.000000000', 'files': ['releasenotes/notes/remove_idle_timeout_option-a6c46e58cb532aef.yaml', 'manifests/db.pp'], 'web_link': 'https://opendev.org/openstack/puppet-glare/commit/ba292054bd75c0e62732bfc84baf099dfa313f19', 'message': 'Remove idle_timeout option\n\nThe idle_timeout parameter has been deprecated for two releases.\nWe can remove it.\n\nChange-Id: I5beee515ab6ceb5df7ef54fd35189a5c72ba242f\n'}]",0,714368,ba292054bd75c0e62732bfc84baf099dfa313f19,8,3,1,9414,,,0,"Remove idle_timeout option

The idle_timeout parameter has been deprecated for two releases.
We can remove it.

Change-Id: I5beee515ab6ceb5df7ef54fd35189a5c72ba242f
",git fetch https://review.opendev.org/openstack/puppet-glare refs/changes/68/714368/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/remove_idle_timeout_option-a6c46e58cb532aef.yaml', 'manifests/db.pp']",2,ba292054bd75c0e62732bfc84baf099dfa313f19,remove_idle_timeout_option," connection_recycle_time => $database_connection_recycle_time,","# DEPRECATED PARAMETERS # # [*database_idle_timeout*] # Timeout when db connections should be reaped. # Defaults to undef. # # DEPRECATED PARAMETERS $database_idle_timeout = undef, if $database_idle_timeout { warning('The database_idle_timeout parameter is deprecated. Please use \ database_connection_recycle_time instead.') } $database_connection_recycle_time_real = pick($database_idle_timeout, $database_connection_recycle_time) connection_recycle_time => $database_connection_recycle_time_real,",4,15
openstack%2Fpuppet-barbican~master~I41cd0e9a44c5490d9f737381ed45e236c4f65a5f,openstack/puppet-barbican,master,I41cd0e9a44c5490d9f737381ed45e236c4f65a5f,Remove idle_timeout option,MERGED,2020-03-23 06:47:16.000000000,2020-03-23 23:35:40.000000000,2020-03-23 23:33:44.000000000,"[{'_account_id': 3153}, {'_account_id': 16137}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 06:47:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-barbican/commit/3919f1fec5d60a767d00dbf3abee950b0c7890e2', 'message': 'Remove idle_timeout option\n\nThe idle_timeout parameter has been deprecated for two releases.\nWe can remove it.\n\nChange-Id: I41cd0e9a44c5490d9f737381ed45e236c4f65a5f\n'}, {'number': 2, 'created': '2020-03-23 09:01:33.000000000', 'files': ['manifests/db.pp', 'releasenotes/notes/remove_idle_timeout_option-be2fa5de1e3ef0dd.yaml'], 'web_link': 'https://opendev.org/openstack/puppet-barbican/commit/8ffd2a765ba1cb1cbffa495944566994e00984f6', 'message': 'Remove idle_timeout option\n\nThe idle_timeout parameter has been deprecated for two releases.\nWe can remove it.\n\nChange-Id: I41cd0e9a44c5490d9f737381ed45e236c4f65a5f\n'}]",0,714379,8ffd2a765ba1cb1cbffa495944566994e00984f6,10,3,2,9414,,,0,"Remove idle_timeout option

The idle_timeout parameter has been deprecated for two releases.
We can remove it.

Change-Id: I41cd0e9a44c5490d9f737381ed45e236c4f65a5f
",git fetch https://review.opendev.org/openstack/puppet-barbican refs/changes/79/714379/2 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/db.pp', 'releasenotes/notes/remove_idle_timeout_option-be2fa5de1e3ef0dd.yaml']",2,3919f1fec5d60a767d00dbf3abee950b0c7890e2,remove_idle_timeout_option,--- upgrade: - Deprecated idle_timeout option has been removed. ,,4,15
openstack%2Fvalidations-libs~master~I5fbcbab7a402dd1149fa6df718f530a3ee5c6af2,openstack/validations-libs,master,I5fbcbab7a402dd1149fa6df718f530a3ee5c6af2,Add get validation playbook function and cleanup,MERGED,2020-03-23 23:02:03.000000000,2020-03-23 23:34:33.000000000,2020-03-23 23:34:33.000000000,"[{'_account_id': 16515}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 23:02:03.000000000', 'files': ['validations_libs/tests/test_utils.py', 'validations_libs/validation_actions.py', 'validations_libs/utils.py'], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/c98ea9195a5c32b0412a55af8e1c8c2385801f5c', 'message': 'Add get validation playbook function and cleanup\n\nRemove unsed function and add get validation playbook function\n\nChange-Id: I5fbcbab7a402dd1149fa6df718f530a3ee5c6af2\n'}]",0,714552,c98ea9195a5c32b0412a55af8e1c8c2385801f5c,7,2,1,16515,,,0,"Add get validation playbook function and cleanup

Remove unsed function and add get validation playbook function

Change-Id: I5fbcbab7a402dd1149fa6df718f530a3ee5c6af2
",git fetch https://review.opendev.org/openstack/validations-libs refs/changes/52/714552/1 && git format-patch -1 --stdout FETCH_HEAD,"['validations_libs/tests/test_utils.py', 'validations_libs/validation_actions.py', 'validations_libs/utils.py']",3,c98ea9195a5c32b0412a55af8e1c8c2385801f5c,validation/actions/cleanup,"from os import listdir from os.path import isfile, joindef get_validations_playbook(path, validation_id, groups=None): """""" Return a list of validations playbook Can be sorted by Groups """""" if isinstance(groups, six.string_types): groups = [groups] pl = [] for f in listdir(path): pl_path = join(path, f) if isfile(pl_path): if os.path.splitext(f)[0] in validation_id: val = Validation(pl_path) if not groups or set(groups).intersection(val.groups): pl.append(pl_path) return pl ",,31,22
openstack%2Fpuppet-panko~master~I3bfd37d17fd090847612f8a45b2787b1615d8e24,openstack/puppet-panko,master,I3bfd37d17fd090847612f8a45b2787b1615d8e24,Remove idle_timeout option,MERGED,2020-03-23 06:47:16.000000000,2020-03-23 23:28:38.000000000,2020-03-23 23:25:29.000000000,"[{'_account_id': 3153}, {'_account_id': 16137}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 06:47:16.000000000', 'files': ['manifests/db.pp', 'releasenotes/notes/remove_idle_timeout_option-881dee47d7bb281d.yaml'], 'web_link': 'https://opendev.org/openstack/puppet-panko/commit/c7ce5143b98cb1c69489d0f9370e60acab618d7a', 'message': 'Remove idle_timeout option\n\nThe idle_timeout parameter has been deprecated for two releases.\nWe can remove it.\n\nChange-Id: I3bfd37d17fd090847612f8a45b2787b1615d8e24\n'}]",0,714378,c7ce5143b98cb1c69489d0f9370e60acab618d7a,8,3,1,9414,,,0,"Remove idle_timeout option

The idle_timeout parameter has been deprecated for two releases.
We can remove it.

Change-Id: I3bfd37d17fd090847612f8a45b2787b1615d8e24
",git fetch https://review.opendev.org/openstack/puppet-panko refs/changes/78/714378/1 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/db.pp', 'releasenotes/notes/remove_idle_timeout_option-881dee47d7bb281d.yaml']",2,c7ce5143b98cb1c69489d0f9370e60acab618d7a,remove_idle_timeout_option,--- upgrade: - Deprecated idle_timeout option has been removed. ,,4,15
openstack%2Fpuppet-placement~master~I5285bb1497cd7b54dd6365a568fd4d2ec95c0d6f,openstack/puppet-placement,master,I5285bb1497cd7b54dd6365a568fd4d2ec95c0d6f,[ussuri][goal] Cleanup for python 2.7 drop,MERGED,2020-02-16 02:46:17.000000000,2020-03-23 23:15:11.000000000,2020-03-23 23:13:57.000000000,"[{'_account_id': 14985}, {'_account_id': 16137}, {'_account_id': 22348}, {'_account_id': 27621}]","[{'number': 1, 'created': '2020-02-16 02:46:17.000000000', 'files': ['doc/requirements.txt', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/puppet-placement/commit/177bc66b6625c492a9acecf13a164ad8209774f7', 'message': '[ussuri][goal] Cleanup for python 2.7 drop\n\nOpenStack is dropping the py2.7 support in ussuri cycle.\n\npuppet-* repos have to cleanup the requirement and tox\nfor py2.7 drop.\n\nComplete discussion & schedule can be found in\n- http://lists.openstack.org/pipermail/openstack-discuss/2019-October/010142.html\n- https://etherpad.openstack.org/p/drop-python2-support\n\nUssuri Communtiy-wide goal:\nhttps://governance.openstack.org/tc/goals/selected/ussuri/drop-py27.html\n\nChange-Id: I5285bb1497cd7b54dd6365a568fd4d2ec95c0d6f\n'}]",0,707997,177bc66b6625c492a9acecf13a164ad8209774f7,9,4,1,8556,,,0,"[ussuri][goal] Cleanup for python 2.7 drop

OpenStack is dropping the py2.7 support in ussuri cycle.

puppet-* repos have to cleanup the requirement and tox
for py2.7 drop.

Complete discussion & schedule can be found in
- http://lists.openstack.org/pipermail/openstack-discuss/2019-October/010142.html
- https://etherpad.openstack.org/p/drop-python2-support

Ussuri Communtiy-wide goal:
https://governance.openstack.org/tc/goals/selected/ussuri/drop-py27.html

Change-Id: I5285bb1497cd7b54dd6365a568fd4d2ec95c0d6f
",git fetch https://review.opendev.org/openstack/puppet-placement refs/changes/97/707997/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/requirements.txt', 'tox.ini']",2,177bc66b6625c492a9acecf13a164ad8209774f7,drop-py27-support,minversion = 3.1ignore_basepython_conflict = Truebasepython = python3,minversion = 2.0basepython = python3,4,4
openstack%2Fpuppet-heat~master~I4c764f182145bf651ce31f6b500cb5039094d91e,openstack/puppet-heat,master,I4c764f182145bf651ce31f6b500cb5039094d91e,Remove idle_timeout option,MERGED,2020-03-23 06:47:15.000000000,2020-03-23 23:12:24.000000000,2020-03-23 23:10:58.000000000,"[{'_account_id': 3153}, {'_account_id': 16137}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 06:47:15.000000000', 'files': ['releasenotes/notes/remove_idle_timeout_option-bece49d2cb331cdf.yaml', 'manifests/db.pp'], 'web_link': 'https://opendev.org/openstack/puppet-heat/commit/5a6838f7cd6bf44a5c4c0b5a88b1d7634a2affe9', 'message': 'Remove idle_timeout option\n\nThe idle_timeout parameter has been deprecated for two releases.\nWe can remove it.\n\nChange-Id: I4c764f182145bf651ce31f6b500cb5039094d91e\n'}]",0,714377,5a6838f7cd6bf44a5c4c0b5a88b1d7634a2affe9,8,3,1,9414,,,0,"Remove idle_timeout option

The idle_timeout parameter has been deprecated for two releases.
We can remove it.

Change-Id: I4c764f182145bf651ce31f6b500cb5039094d91e
",git fetch https://review.opendev.org/openstack/puppet-heat refs/changes/77/714377/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/remove_idle_timeout_option-bece49d2cb331cdf.yaml', 'manifests/db.pp']",2,5a6838f7cd6bf44a5c4c0b5a88b1d7634a2affe9,remove_idle_timeout_option," $database_connection_recycle_time_real = pick($::heat::database_idle_timeout, $database_connection_recycle_time)","# DEPRECATED PARAMETERS # # [*database_idle_timeout*] # Timeout when db connections should be reaped. # Defaults to undef. # # DEPRECATED PARAMETERS $database_idle_timeout = undef, if $database_idle_timeout { warning('The database_idle_timeout parameter is deprecated. Please use \ database_connection_recycle_time instead.') } $database_connection_recycle_time_real = pick($::heat::database_idle_timeout, $database_idle_timeout, $database_connection_recycle_time)",4,15
openstack%2Ftripleo-common~stable%2Ftrain~I859caa73402b5f6ee6963c2d0073714d39b01cf8,openstack/tripleo-common,stable/train,I859caa73402b5f6ee6963c2d0073714d39b01cf8,Fix path of log file.,MERGED,2020-03-23 15:31:10.000000000,2020-03-23 23:09:02.000000000,2020-03-23 23:07:21.000000000,"[{'_account_id': 14985}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 26343}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-03-23 15:31:10.000000000', 'files': ['workbooks/package_update.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/f7513603d567b7be4420163cfbfe1998c2557f69', 'message': ""Fix path of log file.\n\nIncorrect log file location afer node upgrade failed. The ansible.log\nfile doesn't exists.\n\nChange-Id: I859caa73402b5f6ee6963c2d0073714d39b01cf8\n""}]",0,714476,f7513603d567b7be4420163cfbfe1998c2557f69,10,6,1,31245,,,0,"Fix path of log file.

Incorrect log file location afer node upgrade failed. The ansible.log
file doesn't exists.

Change-Id: I859caa73402b5f6ee6963c2d0073714d39b01cf8
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/76/714476/1 && git format-patch -1 --stdout FETCH_HEAD,['workbooks/package_update.yaml'],1,f7513603d567b7be4420163cfbfe1998c2557f69,train," message: Ansible failed, check log at /var/log/mistral/package_update.log. message: Ansible failed, check log at /var/log/mistral/package_update.log."," message: Ansible failed, check log at <% $.work_dir %>/<% execution().id %>/package_update.log. message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/package_update.log.",2,2
openstack%2Ftripleo-common~stable%2Fstein~I5fb83a13e385ed1a35a6cb735e50df2b5b368164,openstack/tripleo-common,stable/stein,I5fb83a13e385ed1a35a6cb735e50df2b5b368164,Handle race for the already existing layer,MERGED,2020-03-21 17:16:36.000000000,2020-03-23 23:08:57.000000000,2020-03-23 23:07:20.000000000,"[{'_account_id': 3153}, {'_account_id': 6926}, {'_account_id': 11090}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-21 17:16:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/3977aa086f4288971ccf739a4b0bf622a5514654', 'message': 'Handle race for the already existing layer\n\nHard copying a large layer takes time.\nRenaming of a blob may also be not instant for some cases.\nHandle race for the already existing layers by skipping competing\nlayers symlinking or renaming blobs made by concurrent workers\n\nChange-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164\nCloses-bug: #1864953\nSigned-off-by: Bogdan Dobrelya <bdobreli@redhat.com>\n(cherry picked from commit 28b8bf0fe637683123500d15ca20fdd427c2fd7d)\n'}, {'number': 2, 'created': '2020-03-22 16:22:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/02270de5e61609ea97748cbb30b87a6c4f8ba3ef', 'message': 'Handle race for the already existing layer\n\nHard copying a large layer takes time.\nRenaming of a blob may also be not instant for some cases.\nHandle race for the already existing layers by skipping competing\nlayers symlinking or renaming blobs made by concurrent workers\n\nChange-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164\nCloses-bug: #1864953\nSigned-off-by: Bogdan Dobrelya <bdobreli@redhat.com>\n(cherry picked from commit 28b8bf0fe637683123500d15ca20fdd427c2fd7d)\n'}, {'number': 3, 'created': '2020-03-23 13:03:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/16e2593a675bd1ae76fbe73323bd618a72e44acd', 'message': 'Handle race for the already existing layer\n\nHard copying a large layer takes time.\nRenaming of a blob may also be not instant for some cases.\nHandle race for the already existing layers by skipping competing\nlayers symlinking or renaming blobs made by concurrent workers\n\nChange-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164\nCloses-bug: #1864953\nSigned-off-by: Bogdan Dobrelya <bdobreli@redhat.com>\n(cherry picked from commit 28b8bf0fe637683123500d15ca20fdd427c2fd7d)\n'}, {'number': 4, 'created': '2020-03-23 13:23:46.000000000', 'files': ['tripleo_common/image/image_export.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/4af033307ef733ebe7a5f057c18ac38766b33f9e', 'message': 'Handle race for the already existing layer\n\nHard copying a large layer takes time.\nRenaming of a blob may also be not instant for some cases.\nHandle race for the already existing layers by skipping competing\nlayers symlinking or renaming blobs made by concurrent workers\n\nChange-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164\nCloses-bug: #1864953\nSigned-off-by: Bogdan Dobrelya <bdobreli@redhat.com>\n(cherry picked from commit 28b8bf0fe637683123500d15ca20fdd427c2fd7d)\n'}]",0,714271,4af033307ef733ebe7a5f057c18ac38766b33f9e,18,6,4,14985,,,0,"Handle race for the already existing layer

Hard copying a large layer takes time.
Renaming of a blob may also be not instant for some cases.
Handle race for the already existing layers by skipping competing
layers symlinking or renaming blobs made by concurrent workers

Change-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164
Closes-bug: #1864953
Signed-off-by: Bogdan Dobrelya <bdobreli@redhat.com>
(cherry picked from commit 28b8bf0fe637683123500d15ca20fdd427c2fd7d)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/71/714271/2 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_common/image/image_export.py'],1,3977aa086f4288971ccf739a4b0bf622a5514654,bug/1864953,"import errnoimport sixdef skip_if_exists(f): @six.wraps(f) def wrapper(*args, **kwargs): try: return f(*args, **kwargs) except OSError as e: # Handle race for the already existing entity if e.errno == errno.EEXIST: pass else: raise e return wrapper @skip_if_exists os.makedirs(path, 0o775)@skip_if_exists@skip_if_exists"," try: os.makedirs(path, 0o775) except os.error: # Handle race for directory already existing pass",20,5
openstack%2Ftripleo-puppet-elements~master~Idd57bb56d81ba76607246c7a4e4caaa71d1b84e0,openstack/tripleo-puppet-elements,master,Idd57bb56d81ba76607246c7a4e4caaa71d1b84e0,Drop docker packages from overcloud-base,MERGED,2019-02-12 21:47:15.000000000,2020-03-23 23:07:21.000000000,2020-03-23 23:07:21.000000000,"[{'_account_id': 3153}, {'_account_id': 4571}, {'_account_id': 7144}, {'_account_id': 9592}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25402}, {'_account_id': 28543}, {'_account_id': 29222}]","[{'number': 1, 'created': '2019-02-12 21:47:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-puppet-elements/commit/394c09639c1f248b508cdad3f25c92ab9f12bec3', 'message': 'Drop docker packages from overcloud-base\n\nDepends-On: I561c52ce09c66a7f79763c59cd25f15949c054af\n\nChange-Id: Idd57bb56d81ba76607246c7a4e4caaa71d1b84e0\n'}, {'number': 2, 'created': '2019-02-18 13:22:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-puppet-elements/commit/8899041e855259d26a9bf8c04981ef2236534ee6', 'message': 'Drop docker packages from overcloud-base\n\nDepends-On: I561c52ce09c66a7f79763c59cd25f15949c054af\n\nChange-Id: Idd57bb56d81ba76607246c7a4e4caaa71d1b84e0\n'}, {'number': 3, 'created': '2019-05-13 14:19:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-puppet-elements/commit/73db2f61fb7922ad403912a5b2ffd00140c71335', 'message': 'Drop docker packages from overcloud-base\n\nDepends-On: I561c52ce09c66a7f79763c59cd25f15949c054af\n\nChange-Id: Idd57bb56d81ba76607246c7a4e4caaa71d1b84e0\n'}, {'number': 4, 'created': '2019-05-13 14:28:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-puppet-elements/commit/571eb8e165d353a0562835c3a89677f5ab970fd9', 'message': 'Drop docker packages from overcloud-base\n\nDepends-On: I561c52ce09c66a7f79763c59cd25f15949c054af\n\nChange-Id: Idd57bb56d81ba76607246c7a4e4caaa71d1b84e0\n'}, {'number': 5, 'created': '2019-05-20 19:27:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-puppet-elements/commit/e0b9d05f2f7862c201d6fb933709469a23ac0d26', 'message': 'Drop docker packages from overcloud-base\n\nDepends-On: I561c52ce09c66a7f79763c59cd25f15949c054af\n\nChange-Id: Idd57bb56d81ba76607246c7a4e4caaa71d1b84e0\n'}, {'number': 6, 'created': '2020-03-12 20:04:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-puppet-elements/commit/6a0fec821a5750b6339906ae9432da00caf6e913', 'message': 'Drop docker packages from overcloud-base\n\nDepends-On: I561c52ce09c66a7f79763c59cd25f15949c054af\n\nChange-Id: Idd57bb56d81ba76607246c7a4e4caaa71d1b84e0\n'}, {'number': 7, 'created': '2020-03-16 19:34:41.000000000', 'files': ['elements/overcloud-base/install.d/package-installs-overcloud-base', 'elements/overcloud-base/pkg-map'], 'web_link': 'https://opendev.org/openstack/tripleo-puppet-elements/commit/a5ca5d269d605eb232cc81f455f09e8d72d1911c', 'message': 'Drop docker packages from overcloud-base\n\nDepends-On: I561c52ce09c66a7f79763c59cd25f15949c054af\nChange-Id: Idd57bb56d81ba76607246c7a4e4caaa71d1b84e0\n'}]",0,636444,a5ca5d269d605eb232cc81f455f09e8d72d1911c,38,10,7,360,,,0,"Drop docker packages from overcloud-base

Depends-On: I561c52ce09c66a7f79763c59cd25f15949c054af
Change-Id: Idd57bb56d81ba76607246c7a4e4caaa71d1b84e0
",git fetch https://review.opendev.org/openstack/tripleo-puppet-elements refs/changes/44/636444/5 && git format-patch -1 --stdout FETCH_HEAD,"['elements/overcloud-base/install.d/package-installs-overcloud-base', 'elements/overcloud-base/pkg-map']",2,394c09639c1f248b508cdad3f25c92ab9f12bec3,tripleo/drop_docker,," ""docker"": ""docker"", ""python_docker_py"": ""python-docker-py"",",0,4
openstack%2Ftripleo-upgrade~stable%2Ftrain~I443865bc95590a5cffcc28d98913110238e863b6,openstack/tripleo-upgrade,stable/train,I443865bc95590a5cffcc28d98913110238e863b6,Clean up ansible variables.,MERGED,2020-03-19 13:33:05.000000000,2020-03-23 23:07:19.000000000,2020-03-23 23:07:19.000000000,"[{'_account_id': 11090}, {'_account_id': 11166}, {'_account_id': 22348}, {'_account_id': 26343}]","[{'number': 1, 'created': '2020-03-19 13:33:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/e1886f01a91d045458b1d64c4111443d951c90a0', 'message': ""Clean up ansible variables.\n\nThe amount of variables defined inside the\ndefaults/main.yml was piling up. It makes\nsense to provide flexibility via variables,\nbut some of the ones defined in the role\ndidn't change and won't change during its\nuse.\n\nThis patch remove most of the '_template'\nvariables and add the template name in the\nsrc section of the task. Remove also workload variables.\n\nChange-Id: I443865bc95590a5cffcc28d98913110238e863b6\n(cherry picked from commit e1a3cb51a10b72ba92d8ede0828ba5b096cb9846)\n""}, {'number': 2, 'created': '2020-03-19 13:33:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/88fd8b1f0d9fbf9e3162081cfd176706cde9121f', 'message': ""Clean up ansible variables.\n\nThe amount of variables defined inside the\ndefaults/main.yml was piling up. It makes\nsense to provide flexibility via variables,\nbut some of the ones defined in the role\ndidn't change and won't change during its\nuse.\n\nThis patch remove most of the '_template'\nvariables and add the template name in the\nsrc section of the task. Remove also workload variables.\n\nChange-Id: I443865bc95590a5cffcc28d98913110238e863b6\n(cherry picked from commit e1a3cb51a10b72ba92d8ede0828ba5b096cb9846)\n""}, {'number': 3, 'created': '2020-03-19 13:42:16.000000000', 'files': ['tasks/update/create-overcloud-update-scripts.yaml', 'tasks/common/create_l3_agent_connectivity_check_script.yml', 'tasks/upgrade/create-overcloud-upgrade-scripts.yaml', 'tasks/upgrade/overcloud_upgrade_prepare.yml', 'tasks/common/create_workload.yml', 'tasks/upgrade/create-undercloud-upgrade-scripts.yaml', 'defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/bb7cd0a134f7e50f1cbe3c4137ac59edf9d650e0', 'message': ""Clean up ansible variables.\n\nThe amount of variables defined inside the\ndefaults/main.yml was piling up. It makes\nsense to provide flexibility via variables,\nbut some of the ones defined in the role\ndidn't change and won't change during its\nuse.\n\nThis patch remove most of the '_template'\nvariables and add the template name in the\nsrc section of the task. Remove also workload variables.\n\nChange-Id: I443865bc95590a5cffcc28d98913110238e863b6\n(cherry picked from commit e1a3cb51a10b72ba92d8ede0828ba5b096cb9846)\n""}]",0,713876,bb7cd0a134f7e50f1cbe3c4137ac59edf9d650e0,9,4,3,8297,,,0,"Clean up ansible variables.

The amount of variables defined inside the
defaults/main.yml was piling up. It makes
sense to provide flexibility via variables,
but some of the ones defined in the role
didn't change and won't change during its
use.

This patch remove most of the '_template'
variables and add the template name in the
src section of the task. Remove also workload variables.

Change-Id: I443865bc95590a5cffcc28d98913110238e863b6
(cherry picked from commit e1a3cb51a10b72ba92d8ede0828ba5b096cb9846)
",git fetch https://review.opendev.org/openstack/tripleo-upgrade refs/changes/76/713876/3 && git format-patch -1 --stdout FETCH_HEAD,"['tasks/common/create_l3_agent_connectivity_check_script.yml', 'tasks/update/create-overcloud-update-scripts.yaml', 'tasks/upgrade/create-overcloud-upgrade-scripts.yaml', 'tasks/upgrade/overcloud_upgrade_prepare.yml', 'tasks/common/create_workload.yml', 'tasks/upgrade/create-undercloud-upgrade-scripts.yaml', 'defaults/main.yml']",7,e1886f01a91d045458b1d64c4111443d951c90a0,,"# other scripts tripleo_inventory: ""{{ working_dir }}//tripleo-ansible-inventory.yaml""",# note that both UC upgrade and update use the same template undercloud_upgrade_template: undercloud_upgrade.sh.j2 undercloud_update_template: undercloud_upgrade.sh.j2 overcloud_upgrade_prepare_template: overcloud_upgrade_prepare.sh.j2 overcloud_system_upgrade_template: overcloud_system_upgrade.sh.j2 overcloud_upgrade_run_template: overcloud_upgrade_run.sh.j2 overcloud_upgrade_converge_template: overcloud_upgrade_converge.sh.j2 workload_launch_template: workload_launch.sh.j2# overcloud jinja template name overcloud_update_prepare_template: overcloud_update_prepare.sh.j2 overcloud_update_run_template: overcloud_update_run.sh.j2 overcloud_update_converge_template: overcloud_update_converge.sh.j2 overcloud_validate_images_template: validate_docker_images_versions.sh.j2l3_agent_connectivity_check_start_template: l3_agent_start_ping.sh.j2 l3_agent_connectivity_check_stop_template: l3_agent_stop_ping.sh.j2upgrade_init_command_template: upgrade_init_command.yaml.j2,27,36
openstack%2Fvalidations-libs~master~I986fdc93c6aa271121bd06a3d3f972c4b31cef2f,openstack/validations-libs,master,I986fdc93c6aa271121bd06a3d3f972c4b31cef2f,Remove useless function and fix issue in Run command,ABANDONED,2020-03-23 14:09:39.000000000,2020-03-23 23:03:00.000000000,,"[{'_account_id': 11491}, {'_account_id': 22348}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-03-23 14:09:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/8fb1c98f84ff98e31b3cad9d99e1c133c3efe3ea', 'message': 'Remove useless function and fix issue in Run command\n\nChange-Id: I986fdc93c6aa271121bd06a3d3f972c4b31cef2f\n'}, {'number': 2, 'created': '2020-03-23 15:55:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/dd876529134f92f442cb030b12ad45fcac6b07af', 'message': 'Remove useless function and fix issue in Run command\n\nChange-Id: I986fdc93c6aa271121bd06a3d3f972c4b31cef2f\n'}, {'number': 3, 'created': '2020-03-23 22:50:47.000000000', 'files': ['validations_libs/tests/test_utils.py', 'validations_libs/validation_actions.py', 'validations_libs/utils.py'], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/2e5bb7b3f6e91faafd70b29d320ba97344004424', 'message': 'Remove useless function and fix issue in Run command\n\nChange-Id: I986fdc93c6aa271121bd06a3d3f972c4b31cef2f\n'}]",0,714452,2e5bb7b3f6e91faafd70b29d320ba97344004424,9,3,3,16515,,,0,"Remove useless function and fix issue in Run command

Change-Id: I986fdc93c6aa271121bd06a3d3f972c4b31cef2f
",git fetch https://review.opendev.org/openstack/validations-libs refs/changes/52/714452/1 && git format-patch -1 --stdout FETCH_HEAD,"['validations_libs/tests/test_utils.py', 'validations_libs/validation_actions.py', 'validations_libs/utils.py']",3,8fb1c98f84ff98e31b3cad9d99e1c133c3efe3ea,validation/run/improve,"from os import listdir from os.path import isfile, joindef get_validations_playbook(path, validation_id, groups=None): """""" Return a list of validations playbook Can be sorted by Groups """""" if isinstance(groups, six.string_types): groups = [groups] pl = [] for f in listdir(path): pl_path = join(path, f) if isfile(pl_path): if os.path.splitext(f)[0] in validation_id: val = Validation(pl_path) if not groups or set(groups).intersection(val.groups): pl.append(pl_path) return pl ","def get_validations_details(validation): """"""Return validations information"""""" results = parse_all_validations_on_disk(constants.ANSIBLE_VALIDATION_DIR) for r in results: if r['id'] == validation: return r return {} ",31,26
openstack%2Fvalidations-libs~master~I3ec5ac4b02e8e28d06c44c134e7ee27798ed7d8f,openstack/validations-libs,master,I3ec5ac4b02e8e28d06c44c134e7ee27798ed7d8f,Change loops with comprehension list,ABANDONED,2020-03-20 15:18:39.000000000,2020-03-23 23:02:28.000000000,,"[{'_account_id': 11491}, {'_account_id': 16515}, {'_account_id': 22348}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-03-20 15:18:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/1dfbdab9c8ec8d88631247033fbf07c5d3352a28', 'message': 'Add validations show parameters actions\n\nChange-Id: I3ec5ac4b02e8e28d06c44c134e7ee27798ed7d8f\n'}, {'number': 2, 'created': '2020-03-20 15:22:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/af16ea0da27abccea712f92d1c39df625ca2a6ad', 'message': 'Add validations show parameters actions\n\nChange-Id: I3ec5ac4b02e8e28d06c44c134e7ee27798ed7d8f\n'}, {'number': 3, 'created': '2020-03-22 18:08:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/c0624e8f1174cb2c9cafbe33da7f719efb001346', 'message': 'Change loops with comprehension list\n\nChange-Id: I3ec5ac4b02e8e28d06c44c134e7ee27798ed7d8f\n'}, {'number': 4, 'created': '2020-03-23 15:54:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/8ddd67373de3e5a9f23c86ee18c52b5a0c9b0abd', 'message': 'Change loops with comprehension list\n\nChange-Id: I3ec5ac4b02e8e28d06c44c134e7ee27798ed7d8f\n'}, {'number': 5, 'created': '2020-03-23 22:43:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/0ae96c71ce878ad0af9933333039425a7dfdbe44', 'message': 'Change loops with comprehension list\n\nChange-Id: I3ec5ac4b02e8e28d06c44c134e7ee27798ed7d8f\n'}, {'number': 6, 'created': '2020-03-23 22:50:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/4548e5959336961d95f6cf73126d20d0aab76b5a', 'message': 'Change loops with comprehension list\n\nChange-Id: I3ec5ac4b02e8e28d06c44c134e7ee27798ed7d8f\n'}, {'number': 7, 'created': '2020-03-23 22:55:05.000000000', 'files': ['validations_libs/validation_actions.py'], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/1cde83d4d4767c2bd984853d991b928dbd0d0506', 'message': 'Change loops with comprehension list\n\nChange-Id: I3ec5ac4b02e8e28d06c44c134e7ee27798ed7d8f\n'}]",2,714144,1cde83d4d4767c2bd984853d991b928dbd0d0506,20,4,7,16515,,,0,"Change loops with comprehension list

Change-Id: I3ec5ac4b02e8e28d06c44c134e7ee27798ed7d8f
",git fetch https://review.opendev.org/openstack/validations-libs refs/changes/44/714144/5 && git format-patch -1 --stdout FETCH_HEAD,['validations_libs/validation_actions.py'],1,1dfbdab9c8ec8d88631247033fbf07c5d3352a28,validation/run/improve," playbooks = ['{}.yaml'.format(pb) for pb in v_utils.get_validation_group_name_list()] def show_validations_parameters(self, validation, group=None): """"""Return Validations Parameters"""""" validations = v_utils.parse_all_validations_on_disk( constants.ANSIBLE_VALIDATION_DIR) return v_utils.get_validations_parameters({'validations': validations}, validation, group)"," for pb in validation_name: if pb not in v_utils.get_validation_group_name_list(): playbooks.append(pb + '.yaml') else: raise(""Please, use '--group' argument instead of "" ""'--validation' to run validation(s) by their "" ""name(s)."" )",11,8
openstack%2Ftripleo-upgrade~stable%2Ftrain~Icf43e7fe59e49e038257d9d5681fae41ed709faa,openstack/tripleo-upgrade,stable/train,Icf43e7fe59e49e038257d9d5681fae41ed709faa,"Force error when ""launch workload"" task fails",MERGED,2020-02-21 12:08:14.000000000,2020-03-23 23:00:24.000000000,2020-03-23 23:00:23.000000000,"[{'_account_id': 6816}, {'_account_id': 11090}, {'_account_id': 11166}, {'_account_id': 18894}, {'_account_id': 19118}, {'_account_id': 22348}, {'_account_id': 29350}, {'_account_id': 31291}]","[{'number': 1, 'created': '2020-02-21 12:08:14.000000000', 'files': ['tasks/main.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/a98843c5eed51d99b76d7366b3392c1ed5e7ccb0', 'message': 'Force error when ""launch workload"" task fails\n\nWhen tripleo-upgrade uses ""launch workload"" task and it fails the\nexecution of the playbook continues, ignoring the error.\nThe aim of this change is forcing the playbook exit with an error.\n\nChange-Id: Icf43e7fe59e49e038257d9d5681fae41ed709faa\n(cherry picked from commit 106c80b5317875128f33033e1c749b061bf8915d)\n'}]",0,709120,a98843c5eed51d99b76d7366b3392c1ed5e7ccb0,11,8,1,26343,,,0,"Force error when ""launch workload"" task fails

When tripleo-upgrade uses ""launch workload"" task and it fails the
execution of the playbook continues, ignoring the error.
The aim of this change is forcing the playbook exit with an error.

Change-Id: Icf43e7fe59e49e038257d9d5681fae41ed709faa
(cherry picked from commit 106c80b5317875128f33033e1c749b061bf8915d)
",git fetch https://review.opendev.org/openstack/tripleo-upgrade refs/changes/20/709120/1 && git format-patch -1 --stdout FETCH_HEAD,['tasks/main.yml'],1,a98843c5eed51d99b76d7366b3392c1ed5e7ccb0,," shell: | set -o pipefail bash {{ workload_launch_script }} 2>&1 {{ timestamper_cmd }} > workload_launch.log args: chdir: ""{{ working_dir }}"""," command: ""{{ workload_launch_script }}""",5,1
openstack%2Fproject-config~master~Ib9c1ec47024a93571b3c1289ba3ab537417e6830,openstack/project-config,master,Ib9c1ec47024a93571b3c1289ba3ab537417e6830,Apply gerrit jobs directly to jeepyb,MERGED,2020-03-23 21:53:38.000000000,2020-03-23 22:56:45.000000000,2020-03-23 22:56:45.000000000,"[{'_account_id': 1}, {'_account_id': 4146}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 21:53:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/8b7d2032f654118e9c48a42e57e19c63c93c963c', 'message': ""Apply gerrit jobs directly to jeepyb\n\nThere is currently a zuul bug that is attaching runnability to\nproject-template definition and so it's seeing that since this\nproject-template is defined in system-config which is not a config\nproject that jeepyb can't run the upload jobs. That's not what\nwe want to happen.\n\nFor now, work around it by just expanding the template here. Once\nthe zuul bug is fixed, we can revert this change.\n\nChange-Id: Ib9c1ec47024a93571b3c1289ba3ab537417e6830\n""}, {'number': 2, 'created': '2020-03-23 22:00:32.000000000', 'files': ['zuul.d/projects.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/ba3645eb084efbb4f39677f498d3fd23e57191b8', 'message': ""Apply gerrit jobs directly to jeepyb\n\nThere is currently a zuul bug that is attaching runnability to\nproject-template definition and so it's seeing that since this\nproject-template is defined in system-config which is not a config\nproject that jeepyb can't run the upload jobs. That's not what\nwe want to happen.\n\nFor now, work around it by just expanding the template here. Once\nthe zuul bug is fixed, we can revert this change.\n\nChange-Id: Ib9c1ec47024a93571b3c1289ba3ab537417e6830\n""}]",1,714540,ba3645eb084efbb4f39677f498d3fd23e57191b8,9,3,2,2,,,0,"Apply gerrit jobs directly to jeepyb

There is currently a zuul bug that is attaching runnability to
project-template definition and so it's seeing that since this
project-template is defined in system-config which is not a config
project that jeepyb can't run the upload jobs. That's not what
we want to happen.

For now, work around it by just expanding the template here. Once
the zuul bug is fixed, we can revert this change.

Change-Id: Ib9c1ec47024a93571b3c1289ba3ab537417e6830
",git fetch https://review.opendev.org/openstack/project-config refs/changes/40/714540/2 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/projects.yaml'],1,8b7d2032f654118e9c48a42e57e19c63c93c963c,, - system-config-build-image-gerrit-base: dependencies: - name: opendev-buildset-registry - name: system-config-build-image-python-builder soft: true - system-config-build-image-gerrit-2.13: dependencies: - name: opendev-buildset-registry - name: system-config-build-image-python-builder soft: true - name: system-config-build-image-gerrit-base soft: true - system-config-build-image-gerrit-2.15: dependencies: - name: opendev-buildset-registry - name: system-config-build-image-gerrit-base soft: true - system-config-build-image-gerrit-2.16: dependencies: - name: opendev-buildset-registry - name: system-config-build-image-gerrit-base soft: true - system-config-build-image-gerrit-3.0: dependencies: - name: opendev-buildset-registry - name: system-config-build-image-gerrit-base soft: true - system-config-build-image-gerrit-master: voting: false dependencies: - name: opendev-buildset-registry - name: system-config-build-image-gerrit-base soft: true - system-config-upload-image-gerrit-base: dependencies: - name: opendev-buildset-registry - name: system-config-upload-image-python-builder soft: true - system-config-upload-image-gerrit-2.13: dependencies: - name: opendev-buildset-registry - name: system-config-upload-image-python-builder soft: true - name: system-config-upload-image-gerrit-base soft: true - system-config-upload-image-gerrit-2.15:, - system-config-gerrit-images,46,1
openstack%2Ftripleo-ansible~stable%2Ftrain~I5a2270130bdf5b9d781f4d81ec25c6ccf12fdc07,openstack/tripleo-ansible,stable/train,I5a2270130bdf5b9d781f4d81ec25c6ccf12fdc07,tripleo_container_manage: improve logging for failed containers,MERGED,2020-03-23 12:47:51.000000000,2020-03-23 22:45:01.000000000,2020-03-23 22:45:01.000000000,"[{'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-23 12:47:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/768b8718cb5eeacc2407141d21c7c62bef62349c', 'message': ""tripleo_container_manage: improve logging for failed containers\n\nIf a container fails to start after many retries, the default logging of\nthe async_status tasks isn't great and it's hard to figure out what\ncontainer failed to start.\n\nIn this patch, we introduce a new filter that will read the\nasync_results and build a list of containers which failed to start\n(failed to True) or did not finish to start (finished to 0); the\nasync_status ignores errors, but we fail a bit later after building that\nlist.\n\nChange-Id: I5a2270130bdf5b9d781f4d81ec25c6ccf12fdc07\n(cherry picked from commit 8fdf2f276f505dbad80391c7e69b8c26fafbd303)\n""}, {'number': 2, 'created': '2020-03-23 14:10:49.000000000', 'files': ['tripleo_ansible/roles/tripleo-container-manage/tasks/podman/create.yml', 'tripleo_ansible/tests/plugins/filter/test_helpers.py', 'tripleo_ansible/ansible_plugins/filter/helpers.py'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/17e7c59ff7fe5f9b357026ea62920032d15439df', 'message': ""tripleo_container_manage: improve logging for failed containers\n\nIf a container fails to start after many retries, the default logging of\nthe async_status tasks isn't great and it's hard to figure out what\ncontainer failed to start.\n\nIn this patch, we introduce a new filter that will read the\nasync_results and build a list of containers which failed to start\n(failed to True) or did not finish to start (finished to 0); the\nasync_status ignores errors, but we fail a bit later after building that\nlist.\n\nChange-Id: I5a2270130bdf5b9d781f4d81ec25c6ccf12fdc07\n(cherry picked from commit 8fdf2f276f505dbad80391c7e69b8c26fafbd303)\n""}]",0,714430,17e7c59ff7fe5f9b357026ea62920032d15439df,8,3,2,3153,,,0,"tripleo_container_manage: improve logging for failed containers

If a container fails to start after many retries, the default logging of
the async_status tasks isn't great and it's hard to figure out what
container failed to start.

In this patch, we introduce a new filter that will read the
async_results and build a list of containers which failed to start
(failed to True) or did not finish to start (finished to 0); the
async_status ignores errors, but we fail a bit later after building that
list.

Change-Id: I5a2270130bdf5b9d781f4d81ec25c6ccf12fdc07
(cherry picked from commit 8fdf2f276f505dbad80391c7e69b8c26fafbd303)
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/30/714430/2 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_ansible/roles/tripleo-container-manage/tasks/podman/create.yml', 'tripleo_ansible/tests/plugins/filter/test_helpers.py', 'tripleo_ansible/ansible_plugins/filter/helpers.py']",3,768b8718cb5eeacc2407141d21c7c62bef62349c,train/logs," 'get_changed_containers': self.get_changed_containers, 'get_failed_containers': self.get_failed_containers def get_failed_containers(self, async_results): """"""Return a list of containers that failed to start on time. This filter takes in input async results of a podman_container invocation and returns the list of containers that did not finished correctly. """""" failed = [] for item in async_results: if item['failed'] or not item['finished']: for k, v in item['container_data'].items(): failed.append(k) return failed", 'get_changed_containers': self.get_changed_containers,74,1
openstack%2Fpuppet-tacker~master~I83335788f308da40067d66b2d783d27936896a9b,openstack/puppet-tacker,master,I83335788f308da40067d66b2d783d27936896a9b,Remove idle_timeout option,MERGED,2020-03-23 06:47:21.000000000,2020-03-23 22:40:58.000000000,2020-03-23 22:39:31.000000000,"[{'_account_id': 3153}, {'_account_id': 16137}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 06:47:21.000000000', 'files': ['manifests/db.pp', 'releasenotes/notes/remove_idle_timeout_option-9b3ae94dab5435c5.yaml'], 'web_link': 'https://opendev.org/openstack/puppet-tacker/commit/6147d3e4317bb087ddc0a69582d894a4716448af', 'message': 'Remove idle_timeout option\n\nThe idle_timeout parameter has been deprecated for two releases.\nWe can remove it.\n\nChange-Id: I83335788f308da40067d66b2d783d27936896a9b\n'}]",0,714383,6147d3e4317bb087ddc0a69582d894a4716448af,8,3,1,9414,,,0,"Remove idle_timeout option

The idle_timeout parameter has been deprecated for two releases.
We can remove it.

Change-Id: I83335788f308da40067d66b2d783d27936896a9b
",git fetch https://review.opendev.org/openstack/puppet-tacker refs/changes/83/714383/1 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/db.pp', 'releasenotes/notes/remove_idle_timeout_option-9b3ae94dab5435c5.yaml']",2,6147d3e4317bb087ddc0a69582d894a4716448af,remove_idle_timeout_option,--- upgrade: - Deprecated idle_timeout option has been removed. ,,4,15
openstack%2Fvalidations-libs~master~I97c9df5e08c2358612704fcdb78cf24cd6f8e421,openstack/validations-libs,master,I97c9df5e08c2358612704fcdb78cf24cd6f8e421,Implement validation show history command,MERGED,2020-03-22 21:30:07.000000000,2020-03-23 22:34:40.000000000,2020-03-23 22:34:40.000000000,"[{'_account_id': 11491}, {'_account_id': 16515}, {'_account_id': 22348}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-03-22 21:30:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/302b2dc4755b40f9666c19a7973be1043bbd58f1', 'message': 'Implement validation show history command\n\nImplement validation show history command in validation_actions\n\nChange-Id: I97c9df5e08c2358612704fcdb78cf24cd6f8e421\n'}, {'number': 2, 'created': '2020-03-22 22:00:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/f63d8d66c353d7257d6b223f415849270556ab9a', 'message': 'Implement validation show history command\n\nImplement validation show history command in validation_actions\n\nChange-Id: I97c9df5e08c2358612704fcdb78cf24cd6f8e421\n'}, {'number': 3, 'created': '2020-03-22 22:00:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/5e7ddd37ff52a04dc643d16f67edf3703b48b09d', 'message': 'Implement validation show history command\n\nImplement validation show history command in validation_actions\n\nChange-Id: I97c9df5e08c2358612704fcdb78cf24cd6f8e421\n'}, {'number': 4, 'created': '2020-03-23 15:55:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/54eca35715ce62f58470d747f96a2d4088c2477e', 'message': 'Implement validation show history command\n\nImplement validation show history command in validation_actions\n\nChange-Id: I97c9df5e08c2358612704fcdb78cf24cd6f8e421\n'}, {'number': 5, 'created': '2020-03-23 22:16:07.000000000', 'files': ['validations_libs/validation_actions.py', 'validations_libs/validation_logs.py'], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/e367d624452623961def5096445dd66ca7f575e3', 'message': 'Implement validation show history command\n\nImplement validation show history command in validation_actions\n\nChange-Id: I97c9df5e08c2358612704fcdb78cf24cd6f8e421\n'}]",0,714333,e367d624452623961def5096445dd66ca7f575e3,15,4,5,16515,,,0,"Implement validation show history command

Implement validation show history command in validation_actions

Change-Id: I97c9df5e08c2358612704fcdb78cf24cd6f8e421
",git fetch https://review.opendev.org/openstack/validations-libs refs/changes/33/714333/4 && git format-patch -1 --stdout FETCH_HEAD,"['validations_libs/validation_actions.py', 'validations_libs/validation_logs.py']",2,302b2dc4755b40f9666c19a7973be1043bbd58f1,show/history," @property def get_start_time(self): """"""Return Ansible start time"""""" return ', '.join([play['play']['duration'].get('start') for play in self.content['plays']]) def get_logfile_by_validation(self, validation_id): """"""Return logfiles by validation_id"""""" return glob.glob(""{}/*_{}_*"".format(self.logs_path, validation_id)) def get_logfile_content_by_validation(self, validation_id): """"""Return logfiles content by validation_id"""""" log_files = glob.glob(""{}/*_{}_*"".format(self.logs_path, validation_id)) return [self._get_content(l) for l in log_files] ",,35,2
openstack%2Fpuppet-magnum~master~I3621386d22885ecbcb1b1ea4d0afdae46b9cd162,openstack/puppet-magnum,master,I3621386d22885ecbcb1b1ea4d0afdae46b9cd162,Remove idle_timeout option,MERGED,2020-03-23 06:47:22.000000000,2020-03-23 22:31:24.000000000,2020-03-23 22:29:45.000000000,"[{'_account_id': 3153}, {'_account_id': 16137}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 06:47:22.000000000', 'files': ['releasenotes/notes/remove_idle_timeout_option-8a05c8764fdae80d.yaml', 'manifests/db.pp'], 'web_link': 'https://opendev.org/openstack/puppet-magnum/commit/24fd4179c3f0b12ab0690475a7ed4afb9a5b587a', 'message': 'Remove idle_timeout option\n\nThe idle_timeout parameter has been deprecated for two releases.\nWe can remove it.\n\nChange-Id: I3621386d22885ecbcb1b1ea4d0afdae46b9cd162\n'}]",0,714384,24fd4179c3f0b12ab0690475a7ed4afb9a5b587a,8,3,1,9414,,,0,"Remove idle_timeout option

The idle_timeout parameter has been deprecated for two releases.
We can remove it.

Change-Id: I3621386d22885ecbcb1b1ea4d0afdae46b9cd162
",git fetch https://review.opendev.org/openstack/puppet-magnum refs/changes/84/714384/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/remove_idle_timeout_option-8a05c8764fdae80d.yaml', 'manifests/db.pp']",2,24fd4179c3f0b12ab0690475a7ed4afb9a5b587a,remove_idle_timeout_option," connection_recycle_time => $database_connection_recycle_time,","# DEPRECATED PARAMETERS # # [*database_idle_timeout*] # Timeout when db connections should be reaped. # Defaults to undef. # # DEPRECATED PARAMETERS $database_idle_timeout = undef, if $database_idle_timeout { warning('The database_idle_timeout parameter is deprecated. Please use \ database_connection_recycle_time instead.') } $database_connection_recycle_time_real = pick($database_idle_timeout, $database_connection_recycle_time) connection_recycle_time => $database_connection_recycle_time_real,",4,15
openstack%2Fpuppet-octavia~master~I65977cf90f0944cfceaa0c97b12a547079e18871,openstack/puppet-octavia,master,I65977cf90f0944cfceaa0c97b12a547079e18871,Remove idle_timeout option,MERGED,2020-03-23 06:47:28.000000000,2020-03-23 22:29:24.000000000,2020-03-23 22:27:53.000000000,"[{'_account_id': 3153}, {'_account_id': 16137}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 06:47:28.000000000', 'files': ['manifests/db.pp', 'releasenotes/notes/remove_idle_timeout_option-b86daee751340b12.yaml'], 'web_link': 'https://opendev.org/openstack/puppet-octavia/commit/4103b2dc824591494e968895dbf34c0a09d970ae', 'message': 'Remove idle_timeout option\n\nThe idle_timeout parameter has been deprecated for two releases.\nWe can remove it.\n\nChange-Id: I65977cf90f0944cfceaa0c97b12a547079e18871\n'}]",0,714386,4103b2dc824591494e968895dbf34c0a09d970ae,8,3,1,9414,,,0,"Remove idle_timeout option

The idle_timeout parameter has been deprecated for two releases.
We can remove it.

Change-Id: I65977cf90f0944cfceaa0c97b12a547079e18871
",git fetch https://review.opendev.org/openstack/puppet-octavia refs/changes/86/714386/1 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/db.pp', 'releasenotes/notes/remove_idle_timeout_option-b86daee751340b12.yaml']",2,4103b2dc824591494e968895dbf34c0a09d970ae,remove_idle_timeout_option,--- upgrade: - Deprecated idle_timeout option has been removed. ,,4,14
openstack%2Fpuppet-freezer~master~I8dd3443d02949c4001b8f7f7e8a9e9c9ad5a5062,openstack/puppet-freezer,master,I8dd3443d02949c4001b8f7f7e8a9e9c9ad5a5062,Remove idle_timeout option,MERGED,2020-03-23 06:47:26.000000000,2020-03-23 22:25:33.000000000,2020-03-23 22:24:13.000000000,"[{'_account_id': 3153}, {'_account_id': 16137}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 06:47:26.000000000', 'files': ['releasenotes/notes/remove_idle_timeout_option-b9f76d9d8f266e20.yaml', 'manifests/db.pp'], 'web_link': 'https://opendev.org/openstack/puppet-freezer/commit/cc2bf1b6b74fbece00665277601fe756fef3ee3b', 'message': 'Remove idle_timeout option\n\nThe idle_timeout parameter has been deprecated for two releases.\nWe can remove it.\n\nChange-Id: I8dd3443d02949c4001b8f7f7e8a9e9c9ad5a5062\n'}]",0,714385,cc2bf1b6b74fbece00665277601fe756fef3ee3b,8,3,1,9414,,,0,"Remove idle_timeout option

The idle_timeout parameter has been deprecated for two releases.
We can remove it.

Change-Id: I8dd3443d02949c4001b8f7f7e8a9e9c9ad5a5062
",git fetch https://review.opendev.org/openstack/puppet-freezer refs/changes/85/714385/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/remove_idle_timeout_option-b9f76d9d8f266e20.yaml', 'manifests/db.pp']",2,cc2bf1b6b74fbece00665277601fe756fef3ee3b,remove_idle_timeout_option," connection_recycle_time => $database_connection_recycle_time,","# DEPRECATED PARAMETERS # # [*database_idle_timeout*] # Timeout when db connections should be reaped. # Defaults to undef. # # DEPRECATED PARAMETERS $database_idle_timeout = undef, if $database_idle_timeout { warning('The database_idle_timeout parameter is deprecated. Please use \ database_connection_recycle_time instead.') } $database_connection_recycle_time_real = pick($database_idle_timeout, $database_connection_recycle_time) connection_recycle_time => $database_connection_recycle_time_real,",4,15
openstack%2Fpuppet-murano~master~Id6c771b7049845a45a497771c7501ca2a28c568e,openstack/puppet-murano,master,Id6c771b7049845a45a497771c7501ca2a28c568e,Remove idle_timeout option,MERGED,2020-03-23 06:47:32.000000000,2020-03-23 22:21:58.000000000,2020-03-23 22:20:24.000000000,"[{'_account_id': 3153}, {'_account_id': 16137}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 06:47:32.000000000', 'files': ['releasenotes/notes/remove_idle_timeout_option-905104829b81f011.yaml', 'manifests/db.pp'], 'web_link': 'https://opendev.org/openstack/puppet-murano/commit/fea5f45996746491b5609c3c01a7201f40bed1de', 'message': 'Remove idle_timeout option\n\nThe idle_timeout parameter has been deprecated for two releases.\nWe can remove it.\n\nChange-Id: Id6c771b7049845a45a497771c7501ca2a28c568e\n'}]",0,714387,fea5f45996746491b5609c3c01a7201f40bed1de,8,3,1,9414,,,0,"Remove idle_timeout option

The idle_timeout parameter has been deprecated for two releases.
We can remove it.

Change-Id: Id6c771b7049845a45a497771c7501ca2a28c568e
",git fetch https://review.opendev.org/openstack/puppet-murano refs/changes/87/714387/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/remove_idle_timeout_option-905104829b81f011.yaml', 'manifests/db.pp']",2,fea5f45996746491b5609c3c01a7201f40bed1de,remove_idle_timeout_option," $database_connection_recycle_time_real = pick($::murano::database_idle_timeout, $database_connection_recycle_time)","# DEPRECATED PARAMETERS # # [*database_idle_timeout*] # Timeout when db connections should be reaped. # Defaults to undef. # # DEPRECATED PARAMETERS $database_idle_timeout = undef, if $database_idle_timeout { warning('The database_idle_timeout parameter is deprecated. Please use \ database_connection_recycle_time instead.') } $database_connection_recycle_time_real = pick($::murano::database_idle_timeout, $database_idle_timeout, $database_connection_recycle_time)",4,14
openstack%2Fvalidations-libs~master~I743355cef943e43492264d4c4700bebbeab9a37d,openstack/validations-libs,master,I743355cef943e43492264d4c4700bebbeab9a37d,Add validation log object representation,MERGED,2020-03-22 18:05:46.000000000,2020-03-23 22:13:30.000000000,2020-03-23 22:13:30.000000000,"[{'_account_id': 11491}, {'_account_id': 16515}, {'_account_id': 22348}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-03-22 18:05:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/737a9059d3650e5919f97545b629a06266bfa6d8', 'message': 'Add validation log object representation\n\nCreate two classes for Log and Logs in order to get\na simpler representation of the validation Log and\nprovide easy way to manage the log data informations\n\nChange-Id: I743355cef943e43492264d4c4700bebbeab9a37d\n'}, {'number': 2, 'created': '2020-03-23 15:55:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/90bfaa2989c8fbb0921b8b64beb5b50b5d3125db', 'message': 'Add validation log object representation\n\nCreate two classes for Log and Logs in order to get\na simpler representation of the validation Log and\nprovide easy way to manage the log data informations\n\nChange-Id: I743355cef943e43492264d4c4700bebbeab9a37d\n'}, {'number': 3, 'created': '2020-03-23 21:54:21.000000000', 'files': ['validations_libs/tests/test_validations_run.py', 'validations_libs/tests/test_utils.py', 'validations_libs/tests/test_validations_show.py', 'validations_libs/validation_actions.py', 'validations_libs/validation_logs.py', 'validations_libs/utils.py'], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/bcd72301f6e5593ae80328684d89b2b7f64d24b6', 'message': 'Add validation log object representation\n\nCreate two classes for Log and Logs in order to get\na simpler representation of the validation Log and\nprovide easy way to manage the log data informations\n\nChange-Id: I743355cef943e43492264d4c4700bebbeab9a37d\n'}]",0,714322,bcd72301f6e5593ae80328684d89b2b7f64d24b6,14,4,3,16515,,,0,"Add validation log object representation

Create two classes for Log and Logs in order to get
a simpler representation of the validation Log and
provide easy way to manage the log data informations

Change-Id: I743355cef943e43492264d4c4700bebbeab9a37d
",git fetch https://review.opendev.org/openstack/validations-libs refs/changes/22/714322/3 && git format-patch -1 --stdout FETCH_HEAD,"['validations_libs/tests/test_utils.py', 'validations_libs/tests/test_validations_run.py', 'validations_libs/tests/test_validations_show.py', 'validations_libs/validation_actions.py', 'validations_libs/validation_logs.py', 'validations_libs/utils.py']",6,737a9059d3650e5919f97545b629a06266bfa6d8,validation/get/results, groups = [groups],"import jsonimport time group_list = [] group_list.append(groups) groups = group_listdef get_new_validations_logs_on_disk(validations_logs_dir): """"""Return a list of new log execution filenames """""" files = [] for root, dirs, filenames in os.walk(validations_logs_dir): files = [ f for f in filenames if not f.startswith('processed') and os.path.splitext(f)[1] == '.json' ] return files def parse_all_validations_logs_on_disk(uuid_run=None, validation_id=None): results = [] path = constants.VALIDATIONS_LOG_BASEDIR logfile = ""{}/*.json"".format(path) if validation_id: logfile = ""{}/*_{}_*.json"".format(path, validation_id) if uuid_run: logfile = ""{}/*_{}_*.json"".format(path, uuid_run) logfiles_path = glob.glob(logfile) for logfile_path in logfiles_path: with open(logfile_path, 'r') as log: contents = json.load(log) results.append(contents) return results def get_validations_stats(log): """"""Return validations stats from a log file"""""" # Get validation stats total_number = len(log) failed_number = 0 passed_number = 0 last_execution = None dates = [] for l in log: if l.get('validation_output'): failed_number += 1 else: passed_number += 1 date_time = \ l['plays'][0]['play']['duration'].get('start').split('T') date_start = date_time[0] time_start = date_time[1].split('Z')[0] newdate = \ time.strptime(date_start + time_start, '%Y-%m-%d%H:%M:%S.%f') dates.append(newdate) if dates: last_execution = time.strftime('%Y-%m-%d %H:%M:%S', max(dates)) return {""Last execution date"": last_execution, ""Number of execution"": ""Total: {}, Passed: {}, "" ""Failed: {}"".format(total_number, passed_number, failed_number)} ",273,97
openstack%2Fironic-prometheus-exporter~stable%2Ftrain~I5e6221a49c60a18309ca9b5d644d97acb56f7fef,openstack/ironic-prometheus-exporter,stable/train,I5e6221a49c60a18309ca9b5d644d97acb56f7fef,DevStack support and Redfish job,MERGED,2020-03-05 08:53:05.000000000,2020-03-23 22:09:10.000000000,2020-03-23 22:05:39.000000000,"[{'_account_id': 11655}, {'_account_id': 22348}, {'_account_id': 23851}]","[{'number': 1, 'created': '2020-03-05 08:53:05.000000000', 'files': ['bindep.txt', '.zuul.yaml', 'devstack/plugin.sh', 'devstack/settings'], 'web_link': 'https://opendev.org/openstack/ironic-prometheus-exporter/commit/2db9036c024f0184298808e8060c178774ec3af5', 'message': 'DevStack support and Redfish job\n\n- plugin for devstack\n- runs the exporter as a service with gunicorn\n- collect data from virtual baremetal (redfish)\n\nChange-Id: I5e6221a49c60a18309ca9b5d644d97acb56f7fef\n(cherry picked from commit 1765f44faaca43eb9274544c2e6aa33e44635920)\n'}]",0,711402,2db9036c024f0184298808e8060c178774ec3af5,8,3,1,15519,,,0,"DevStack support and Redfish job

- plugin for devstack
- runs the exporter as a service with gunicorn
- collect data from virtual baremetal (redfish)

Change-Id: I5e6221a49c60a18309ca9b5d644d97acb56f7fef
(cherry picked from commit 1765f44faaca43eb9274544c2e6aa33e44635920)
",git fetch https://review.opendev.org/openstack/ironic-prometheus-exporter refs/changes/02/711402/1 && git format-patch -1 --stdout FETCH_HEAD,"['bindep.txt', '.zuul.yaml', 'devstack/plugin.sh', 'devstack/settings']",4,2db9036c024f0184298808e8060c178774ec3af5,685888-stable/train,define_plugin ironic-prometheus-exporter plugin_requires ironic-prometheus-exporter ironic enable_service ironic-prometheus-exporter ,,212,0
openstack%2Ftripleo-ansible~stable%2Ftrain~Ifa494ffdd58772c39808bcaa3d5d37b3802af065,openstack/tripleo-ansible,stable/train,Ifa494ffdd58772c39808bcaa3d5d37b3802af065,Increase number of retries for container create async task,MERGED,2020-03-23 12:47:07.000000000,2020-03-23 22:09:09.000000000,2020-03-23 22:09:09.000000000,"[{'_account_id': 8833}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-23 12:47:07.000000000', 'files': ['tripleo_ansible/roles/tripleo-container-manage/tasks/podman/create.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/875cba87e576f897d641e350b9657777593b3e53', 'message': 'Increase number of retries for container create async task\n\nSeems like some db_sync tasks are taking more time and this\nresults in jobs failing. Though probably not a long\nterm solution, would be good to increase it to reduce the\nlarge number of job failures.\n\nChange-Id: Ifa494ffdd58772c39808bcaa3d5d37b3802af065\nRalated-Bug: #1865473\n(cherry picked from commit d40a353e0474f291ef5f77d3486ea590390201de)\n'}]",0,714429,875cba87e576f897d641e350b9657777593b3e53,7,4,1,3153,,,0,"Increase number of retries for container create async task

Seems like some db_sync tasks are taking more time and this
results in jobs failing. Though probably not a long
term solution, would be good to increase it to reduce the
large number of job failures.

Change-Id: Ifa494ffdd58772c39808bcaa3d5d37b3802af065
Ralated-Bug: #1865473
(cherry picked from commit d40a353e0474f291ef5f77d3486ea590390201de)
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/29/714429/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_ansible/roles/tripleo-container-manage/tasks/podman/create.yml'],1,875cba87e576f897d641e350b9657777593b3e53,train/timeout, retries: 60, retries: 30,1,1
openstack%2Fironic-python-agent~master~I00a7b88289c9a0a65aabc8b1e32b63f0f787a65f,openstack/ironic-python-agent,master,I00a7b88289c9a0a65aabc8b1e32b63f0f787a65f,Explicitly set ramdisk type,MERGED,2020-03-23 10:56:35.000000000,2020-03-23 22:05:06.000000000,2020-03-23 22:01:49.000000000,"[{'_account_id': 11655}, {'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-23 10:56:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/1efe8c038f94823954b3e2bfdeda56120d7ea3aa', 'message': ""Explicitly set ramdisk type\n\nThe ironic-lib-base job is based on ironic-base job which\nwill default to dib.\nTo prevent breaking the ironic-lib CI we set the ramdisk\ntype to tinyipa for now and we'll convert the jobs here later.\n\nChange-Id: I00a7b88289c9a0a65aabc8b1e32b63f0f787a65f\n""}, {'number': 2, 'created': '2020-03-23 10:57:34.000000000', 'files': ['zuul.d/ironic-python-agent-jobs.yaml'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/476931d71905dd8ed2e7e04982e0ac4fda4113e7', 'message': ""Explicitly set ramdisk type\n\nThe ironic-ipa-base job is based on ironic-base job which\nwill default to dib.\nTo prevent breaking the ironic-ipa CI we set the ramdisk\ntype to tinyipa for now and we'll convert the jobs here later.\n\nChange-Id: I00a7b88289c9a0a65aabc8b1e32b63f0f787a65f\n""}]",0,714419,476931d71905dd8ed2e7e04982e0ac4fda4113e7,10,4,2,23851,,,0,"Explicitly set ramdisk type

The ironic-ipa-base job is based on ironic-base job which
will default to dib.
To prevent breaking the ironic-ipa CI we set the ramdisk
type to tinyipa for now and we'll convert the jobs here later.

Change-Id: I00a7b88289c9a0a65aabc8b1e32b63f0f787a65f
",git fetch https://review.opendev.org/openstack/ironic-python-agent refs/changes/19/714419/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/ironic-python-agent-jobs.yaml'],1,1efe8c038f94823954b3e2bfdeda56120d7ea3aa,set-ramdisk-type, IRONIC_RAMDISK_TYPE: tinyipa,,1,0
openstack%2Fpuppet-tripleo~master~I85049de9960586a1069aa750c8d727c6e37cec73,openstack/puppet-tripleo,master,I85049de9960586a1069aa750c8d727c6e37cec73,Add Octavia OVN Provider configuration (1 of 2),MERGED,2020-02-04 16:18:52.000000000,2020-03-23 22:04:58.000000000,2020-03-23 22:03:30.000000000,"[{'_account_id': 1131}, {'_account_id': 3153}, {'_account_id': 6469}, {'_account_id': 6681}, {'_account_id': 6926}, {'_account_id': 8655}, {'_account_id': 11082}, {'_account_id': 11628}, {'_account_id': 11952}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 23804}]","[{'number': 1, 'created': '2020-02-04 16:18:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/00d9f7c1853c59264dda1d53e5ac80caf4d05e23', 'message': ""octavia: Use SSL protocol when TLS is enabled\n\nOctavia's API connectivity to OVN DB host is already capable\nof enabling TLS. This change uses that information to switch\nfrom TCP to SSL protocol in these cases.\n\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nCloses-Bug: #1861886\n""}, {'number': 2, 'created': '2020-02-04 20:18:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/9d3c029c6c06a7bd4720c43805ae58221f965a18', 'message': ""octavia: Use SSL protocol when TLS is enabled\n\nOctavia's API connectivity to OVN DB host is already capable\nof enabling TLS. This change uses that information to switch\nfrom TCP to SSL protocol in these cases.\n\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nCloses-Bug: #1861886\n""}, {'number': 3, 'created': '2020-02-05 21:32:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/506eaf646f7e5f2349be086969a4a846d796a30d', 'message': 'octavia: Use SSL protocol when TLS is enabled\n\nThis patch enhances Octavia config, so it can connect to\nOVN_Northbound DB using SSL.\n\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nCloses-Bug: #1861886\n'}, {'number': 4, 'created': '2020-02-05 21:34:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/2624d89c6b50cff9eb322dcfb2ba5f062245b787', 'message': 'octavia: Use SSL protocol when TLS is enabled\n\nThis patch enhances Octavia config, so it can connect to\nOVN_Northbound DB using SSL.\n\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nCloses-Bug: #1861886\n'}, {'number': 5, 'created': '2020-02-12 17:40:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/359727b153260512e2b361096c273389c807eab0', 'message': 'octavia: Use SSL protocol when TLS is enabled\n\nThis patch enhances Octavia config, so it can connect to\nOVN_Northbound DB using SSL.\n\nDepends-On: https://review.opendev.org/#/c/707434/\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nCloses-Bug: #1861886\n'}, {'number': 6, 'created': '2020-02-14 13:21:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/66b23ec83c985b881c937ccb2d8d54948ef895cf', 'message': 'OVN Octavia Driver: Use SSL/TLS protocol when TLS is enabled\n\nThis patch enhances OVN Octavia config, so it can connect to\nOVN_Northbound DB using SSL/TLS.\n\nDepends-On: https://review.opendev.org/#/c/707434/\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nCloses-Bug: #1861886\n'}, {'number': 7, 'created': '2020-02-25 21:07:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/71bda608041654cf8e3d3636144b9e4e10847b0b', 'message': 'Octavia using OVN Driver: Make protocol configurable\n\nThis patch enhances Octavia config for OVN driver, so\nit can connect to OVN_Northbound DB using SSL/TLS.\n\nDepends-On: https://review.opendev.org/#/c/709618/\nCo-Authored-By: Brent Eagles <beagles@redhat.com>\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nCloses-Bug: #1861886\n'}, {'number': 8, 'created': '2020-03-03 03:17:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/1c670724feac319466775b759d7affd53f6b4b46', 'message': ""Add Octavia OVN Provider configuration\n\nThis patch enhances Octavia's OVN driver config, so it can connect to\nOVN_Northbound DB using TLS.\n\nDepends-On: https://review.opendev.org/#/c/709618/\nCo-Authored-By: Brent Eagles <beagles@redhat.com>\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nRelated-Bug: #1861886\n""}, {'number': 9, 'created': '2020-03-03 10:46:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/f1a8413fc4cfdb311deff58c97a46ef210c7dc59', 'message': ""Add Octavia OVN Provider configuration\n\nThis patch enhances Octavia's OVN driver config, so it can connect to\nOVN_Northbound DB using TLS.\n\nDepends-On: https://review.opendev.org/#/c/709618/\nCo-Authored-By: Brent Eagles <beagles@redhat.com>\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nRelated-Bug: #1861886\n""}, {'number': 10, 'created': '2020-03-03 15:18:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/e390303cba3ebdd39bb56ea77e02abd5954db906', 'message': ""Add Octavia OVN Provider configuration\n\nThis patch enhances Octavia's OVN driver config, so it can connect to\nOVN_Northbound DB using TLS.\n\nDepends-On: https://review.opendev.org/#/c/709618/\nCo-Authored-By: Brent Eagles <beagles@redhat.com>\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nRelated-Bug: #1861886\n""}, {'number': 11, 'created': '2020-03-03 18:55:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/f01352504d55ab5f4e7f135b0131f31b9903013a', 'message': ""Add Octavia OVN Provider configuration\n\nThis patch enhances Octavia's OVN driver config, so it can connect to\nOVN_Northbound DB using TLS.\n\nDepends-On: https://review.opendev.org/#/c/709618/\nCo-Authored-By: Brent Eagles <beagles@redhat.com>\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nRelated-Bug: #1861886\n""}, {'number': 12, 'created': '2020-03-03 19:45:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/a93162175845f183a8192607abb46d83783fa7de', 'message': ""Add Octavia OVN Provider configuration\n\nThis patch enhances Octavia's OVN driver config, so it can connect to\nOVN_Northbound DB using TLS.\n\nDepends-On: https://review.opendev.org/#/c/709618/\nCo-Authored-By: Brent Eagles <beagles@redhat.com>\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nRelated-Bug: #1861886\n""}, {'number': 13, 'created': '2020-03-04 20:49:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/4e0b95e5545498a73a4141d95462f60663d703e9', 'message': ""Add Octavia OVN Provider configuration\n\nThis patch enhances Octavia's OVN driver config, so it can connect to\nOVN_Northbound DB using TLS.\n\nDepends-On: https://review.opendev.org/#/c/711333/\nCo-Authored-By: Brent Eagles <beagles@redhat.com>\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nRelated-Bug: #1861886\n""}, {'number': 14, 'created': '2020-03-16 17:48:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/605c7ea6a7de8cc142b3c3575b1ba85ab5bfb8ef', 'message': ""Add Octavia OVN Provider configuration\n\nThis patch enhances Octavia's OVN driver config, so it can connect to\nOVN_Northbound DB using TLS.\n\nDepends-On: https://review.opendev.org/#/c/711333/\nCo-Authored-By: Brent Eagles <beagles@redhat.com>\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nRelated-Bug: #1861886\n""}, {'number': 15, 'created': '2020-03-18 10:19:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/fc8e14f6d227cf10ba9e1a34548b3fc695496b1e', 'message': ""Add Octavia OVN Provider configuration\n\nThis patch enhances Octavia's OVN driver config, so it can connect to\nOVN_Northbound DB using TLS.\n\nDepends-On: https://review.opendev.org/#/c/711333/\nCo-Authored-By: Brent Eagles <beagles@redhat.com>\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nRelated-Bug: #1861886\n""}, {'number': 16, 'created': '2020-03-18 10:22:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/dde1d120da72f93fd8ddffaeb559ce80d5c7bd82', 'message': ""Add Octavia OVN Provider configuration\n\nThis patch enhances Octavia's OVN driver config, so it can connect to\nOVN_Northbound DB using TLS.\n\nDepends-On: https://review.opendev.org/#/c/711557/\nCo-Authored-By: Brent Eagles <beagles@redhat.com>\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nRelated-Bug: #1861886\n""}, {'number': 17, 'created': '2020-03-18 13:19:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/9a30cadf69dc979299e549791535a402be61141f', 'message': ""Add Octavia OVN Provider configuration\n\nThis patch enhances Octavia's OVN driver config, so it can connect to\nOVN_Northbound DB using TLS.\n\nDepends-On: https://review.opendev.org/#/c/711333/\nDepends-On: https://review.opendev.org/#/c/711557/\n\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nRelated-Bug: #1861886\n""}, {'number': 18, 'created': '2020-03-18 18:30:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/d36a0b7761b23b0c1cad5128279d5279e557e29d', 'message': ""Add Octavia OVN Provider configuration (1 of 2)\n\nThis is part 1 of 2, where ovn provider info located in\ntripleo::profile::base::octavia::api will move\nto newly created octavia::provider::ovn.\nBut that has to be split into 2 parts to avoid breaking the\nCI until the THT+pupple-tripleo changes merges [1].\n\n[1]: https://review.opendev.org/#/q/topic:bug/1861886+(status:open+OR+status:merged)\n\nThis patch enhances Octavia's OVN driver config, so it can connect to\nOVN_Northbound DB using TLS.\n\nDepends-On: https://review.opendev.org/#/c/711333/\n\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nRelated-Bug: #1861886\n""}, {'number': 19, 'created': '2020-03-18 18:35:20.000000000', 'files': ['manifests/certmonger/ovn_octavia.pp', 'spec/classes/tripleo_profile_base_octavia_provider_ovn_spec.rb', 'releasenotes/notes/add-octavia-provider-ovn-6734aa08af4772e4.yaml', 'manifests/profile/base/octavia/provider/ovn.pp', 'manifests/profile/base/certmonger_user.pp'], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/c68aa2e140890478156aa197a4217bd362e913ab', 'message': ""Add Octavia OVN Provider configuration (1 of 2)\n\nThis is part 1 of 2, where ovn provider info located in\ntripleo::profile::base::octavia::api will move\nto newly created octavia::provider::ovn.\nBut that has to be split into 2 parts to avoid breaking the\nCI until the THT+pupple-tripleo changes merges [1].\n\n[1]: https://review.opendev.org/#/q/topic:bug/1861886+(status:open+OR+status:merged)\n\nThis patch enhances Octavia's OVN driver config, so it can connect to\nOVN_Northbound DB using TLS.\n\nDepends-On: https://review.opendev.org/#/c/711333/\n\nChange-Id: I85049de9960586a1069aa750c8d727c6e37cec73\nRelated-Bug: #1861886\n""}]",16,705728,c68aa2e140890478156aa197a4217bd362e913ab,92,12,19,11952,,,0,"Add Octavia OVN Provider configuration (1 of 2)

This is part 1 of 2, where ovn provider info located in
tripleo::profile::base::octavia::api will move
to newly created octavia::provider::ovn.
But that has to be split into 2 parts to avoid breaking the
CI until the THT+pupple-tripleo changes merges [1].

[1]: https://review.opendev.org/#/q/topic:bug/1861886+(status:open+OR+status:merged)

This patch enhances Octavia's OVN driver config, so it can connect to
OVN_Northbound DB using TLS.

Depends-On: https://review.opendev.org/#/c/711333/

Change-Id: I85049de9960586a1069aa750c8d727c6e37cec73
Related-Bug: #1861886
",git fetch https://review.opendev.org/openstack/puppet-tripleo refs/changes/28/705728/10 && git format-patch -1 --stdout FETCH_HEAD,['manifests/profile/base/octavia/api.pp'],1,00d9f7c1853c59264dda1d53e5ac80caf4d05e23,bug/1861886," if $enable_internal_tls { $ovn_nb_connection = join(['ssl', normalize_ip_for_uri($ovn_db_host), ""${ovn_nb_port}""], ':') } else { $ovn_nb_connection = join(['tcp', normalize_ip_for_uri($ovn_db_host), ""${ovn_nb_port}""], ':') }"," $ovn_nb_connection = join(['tcp', normalize_ip_for_uri($ovn_db_host), ""${ovn_nb_port}""], ':')",5,1
openstack%2Fsushy-cli~master~I53c62e5d78b5bded8f506ab0b62384bd7f3fdf45,openstack/sushy-cli,master,I53c62e5d78b5bded8f506ab0b62384bd7f3fdf45,Add `--insecure` and `--tls-certificates` options,MERGED,2020-03-10 11:57:49.000000000,2020-03-23 21:59:47.000000000,2020-03-23 21:56:55.000000000,"[{'_account_id': 15519}, {'_account_id': 22348}, {'_account_id': 23851}, {'_account_id': 26340}, {'_account_id': 31182}]","[{'number': 1, 'created': '2020-03-10 11:57:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/95e5bb581fcd2ed925bdc1fb3f1340fc7729f024', 'message': 'Add `--insecure` and `--tls-certificates` options\n\nThese options are essential for lab deployments where\nsecurity always gets in the way.\n\nChange-Id: I53c62e5d78b5bded8f506ab0b62384bd7f3fdf45\n'}, {'number': 2, 'created': '2020-03-11 11:12:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/c844719d3ad487baca74796d6ced740f9801c09c', 'message': 'Add `--insecure` and `--tls-certificates` options\n\nThese options allow working around invalid or self-signed\nTLS certificates sometimes found in the BMCs.\n\nChange-Id: I53c62e5d78b5bded8f506ab0b62384bd7f3fdf45\n'}, {'number': 3, 'created': '2020-03-11 13:05:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/980592f3a6b19a3b082daf9c7e54a0082c6854cb', 'message': 'Add `--insecure` and `--tls-certificates` options\n\nThese options allow working around invalid or self-signed\nTLS certificates sometimes found in the BMCs.\n\nChange-Id: I53c62e5d78b5bded8f506ab0b62384bd7f3fdf45\n'}, {'number': 4, 'created': '2020-03-11 14:02:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/5f23421343d7d1c8d5201dd34b324146b4c84f79', 'message': 'Add `--insecure` and `--tls-certificates` options\n\nThese options allow working around invalid or self-signed\nTLS certificates sometimes found in the BMCs.\n\nChange-Id: I53c62e5d78b5bded8f506ab0b62384bd7f3fdf45\n'}, {'number': 5, 'created': '2020-03-11 14:12:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/7d18cb1fd73e072259c21b86d5ed7a05d28d13b8', 'message': 'Add `--insecure` and `--tls-certificates` options\n\nThese options allow working around invalid or self-signed\nTLS certificates sometimes found in the BMCs.\n\nChange-Id: I53c62e5d78b5bded8f506ab0b62384bd7f3fdf45\n'}, {'number': 6, 'created': '2020-03-11 16:58:17.000000000', 'files': ['requirements.txt', 'releasenotes/notes/add-tls-options-3331d369f5d101b7.yaml', 'sushycli/tests/unit/cmd/test_sushycli.py', 'sushycli/base.py', 'lower-constraints.txt'], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/c93d49aa95a6973bd4fca34a8a839310a109eb4b', 'message': 'Add `--insecure` and `--tls-certificates` options\n\nThese options allow working around invalid or self-signed\nTLS certificates sometimes found in the BMCs.\n\nChange-Id: I53c62e5d78b5bded8f506ab0b62384bd7f3fdf45\n'}]",0,712058,c93d49aa95a6973bd4fca34a8a839310a109eb4b,19,5,6,26340,,,0,"Add `--insecure` and `--tls-certificates` options

These options allow working around invalid or self-signed
TLS certificates sometimes found in the BMCs.

Change-Id: I53c62e5d78b5bded8f506ab0b62384bd7f3fdf45
",git fetch https://review.opendev.org/openstack/sushy-cli refs/changes/58/712058/1 && git format-patch -1 --stdout FETCH_HEAD,['sushycli/base.py'],1,95e5bb581fcd2ed925bdc1fb3f1340fc7729f024,09-add-tls-options," parser.add_argument( '--insecure', action='store_true', help='Do not verify server TLS certificate') parser.add_argument( '--tls-certificates', metavar='<FILE|DIR>', help='Path to a CA bundle or a directory containing trusted ' 'TLS certificates.') args.service_endpoint, username=args.username, password=args.password, verify=args.tls_certificates or not args.insecure)"," args.service_endpoint, username=args.username, password=args.password)",12,2
openstack%2Fsushy-cli~master~If78c227a3f19067293bd67d5929af50d3245c611,openstack/sushy-cli,master,If78c227a3f19067293bd67d5929af50d3245c611,"Add systems, managers and chassis listing commands",MERGED,2020-02-07 10:15:58.000000000,2020-03-23 21:58:24.000000000,2020-03-23 21:56:47.000000000,"[{'_account_id': 15519}, {'_account_id': 22348}, {'_account_id': 23851}, {'_account_id': 26340}]","[{'number': 1, 'created': '2020-02-07 10:15:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/0e1b580a0dd6bbafdda8b90b7b661d203da87433', 'message': 'Add systems, managers and chassis listing commands\n\nTo fish anything out of Redfish BMC, the user needs to discover\nsystems/managers/chassis entry points.\n\nThe commands introduced with this commit are:\n\n  sushycli system list\n  sushycli manager list\n  sushycli chassis list\n\nThe output of these commands includes item ID which could\nbe fed back to other sushycli commands for addressing the\nentity of interest.\n\nChange-Id: If78c227a3f19067293bd67d5929af50d3245c611\nStory: 2006608\nTask: 36776\n'}, {'number': 2, 'created': '2020-02-07 11:18:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/939a22eaa6b1269b4a973638798f4c9da9a927ab', 'message': 'Add systems, managers and chassis listing commands\n\nTo fish anything out of Redfish BMC, the user needs to discover\nsystems/managers/chassis entry points.\n\nThe commands introduced with this commit are:\n\n  sushycli system list\n  sushycli manager list\n  sushycli chassis list\n\nThe output of these commands includes item ID which could\nbe fed back to other sushycli commands for addressing the\nentity of interest.\n\nChange-Id: If78c227a3f19067293bd67d5929af50d3245c611\nStory: 2006608\nTask: 36776\n'}, {'number': 3, 'created': '2020-02-10 14:47:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/c03114b31a8fab70891172efe93d9c5637df5805', 'message': 'Add systems, managers and chassis listing commands\n\nTo fish anything out of Redfish BMC, the user needs to discover\nsystems/managers/chassis entry points.\n\nThe commands introduced with this commit are:\n\n  sushycli system list\n  sushycli manager list\n  sushycli chassis list\n\nThe output of these commands includes item ID which could\nbe fed back to other sushycli commands for addressing the\nentity of interest.\n\nChange-Id: If78c227a3f19067293bd67d5929af50d3245c611\nStory: 2006608\nTask: 36776\n'}, {'number': 4, 'created': '2020-02-11 11:27:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/29a7ae7f13d0fba6f8e8f9419efdb2ae993e530d', 'message': 'Add systems, managers and chassis listing commands\n\nTo fish anything out of Redfish BMC, the user needs to discover\nsystems/managers/chassis entry points.\n\nThe commands introduced with this commit are:\n\n  sushycli system list\n  sushycli manager list\n  sushycli chassis list\n\nThe output of these commands includes item ID which could\nbe fed back to other sushycli commands for addressing the\nentity of interest.\n\nChange-Id: If78c227a3f19067293bd67d5929af50d3245c611\nStory: 2006608\nTask: 36776\n'}, {'number': 5, 'created': '2020-02-11 11:35:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/d1614b7f0da54a7510e4796337cb4f65fa1ac504', 'message': 'Add systems, managers and chassis listing commands\n\nTo fish anything out of Redfish BMC, the user needs to discover\nsystems/managers/chassis entry points.\n\nThe commands introduced with this commit are:\n\n  sushycli system list\n  sushycli manager list\n  sushycli chassis list\n\nThe output of these commands includes item ID which could\nbe fed back to other sushycli commands for addressing the\nentity of interest.\n\nChange-Id: If78c227a3f19067293bd67d5929af50d3245c611\nStory: 2006608\nTask: 36776\n'}, {'number': 6, 'created': '2020-02-13 05:50:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/20191d03ebf64fe919b68c6a2a49c9d3dbc9c35d', 'message': 'Add systems, managers and chassis listing commands\n\nTo fish anything out of Redfish BMC, the user needs to discover\nsystems/managers/chassis entry points.\n\nThe commands introduced with this commit are:\n\n  sushycli system list\n  sushycli manager list\n  sushycli chassis list\n\nThe output of these commands includes item ID which could\nbe fed back to other sushycli commands for addressing the\nentity of interest.\n\nChange-Id: If78c227a3f19067293bd67d5929af50d3245c611\nStory: 2006608\nTask: 36776\n'}, {'number': 7, 'created': '2020-02-13 07:27:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/31186093b743146884c2cc34c7e3236edb785b26', 'message': 'Add systems, managers and chassis listing commands\n\nTo fish anything out of Redfish BMC, the user needs to discover\nsystems/managers/chassis entry points.\n\nThe commands introduced with this commit are:\n\n  sushycli system list\n  sushycli manager list\n  sushycli chassis list\n\nThe output of these commands includes item ID which could\nbe fed back to other sushycli commands for addressing the\nentity of interest.\n\nChange-Id: If78c227a3f19067293bd67d5929af50d3245c611\nStory: 2006608\nTask: 36776\n'}, {'number': 8, 'created': '2020-02-17 10:52:59.000000000', 'files': ['sushycli/manager_list.py', 'sushycli/chassis_list.py', 'sushycli/tests/unit/cmd/test_sushycli.py', 'sushycli/system_list.py', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/b2ce4d60ad773555b14baf65af0463705ccec0d8', 'message': 'Add systems, managers and chassis listing commands\n\nTo fish anything out of Redfish BMC, the user needs to discover\nsystems/managers/chassis entry points.\n\nThe commands introduced with this commit are:\n\n  sushycli system list\n  sushycli manager list\n  sushycli chassis list\n\nThe output of these commands includes item ID which could\nbe fed back to other sushycli commands for addressing the\nentity of interest.\n\nChange-Id: If78c227a3f19067293bd67d5929af50d3245c611\nStory: 2006608\nTask: 36776\n'}]",0,706461,b2ce4d60ad773555b14baf65af0463705ccec0d8,23,4,8,26340,,,0,"Add systems, managers and chassis listing commands

To fish anything out of Redfish BMC, the user needs to discover
systems/managers/chassis entry points.

The commands introduced with this commit are:

  sushycli system list
  sushycli manager list
  sushycli chassis list

The output of these commands includes item ID which could
be fed back to other sushycli commands for addressing the
entity of interest.

Change-Id: If78c227a3f19067293bd67d5929af50d3245c611
Story: 2006608
Task: 36776
",git fetch https://review.opendev.org/openstack/sushy-cli refs/changes/61/706461/8 && git format-patch -1 --stdout FETCH_HEAD,"['sushycli/manager_list.py', 'sushycli/chassis_list.py', 'sushycli/tests/unit/cmd/test_sushycli.py', 'setup.cfg', 'sushycli/system_list.py']",5,0e1b580a0dd6bbafdda8b90b7b661d203da87433,07-add-system-manager-chassis-list,"# -*- coding: utf-8 -*- # Copyright 2010-2020 OpenStack Foundation # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from sushycli import base class SystemList(base.BaseLister): """"""List available systems"""""" def get_parser(self, prog_name): """"""List systems command parser. :param prog_name: name of the cliff command being executed :returns: an `argparse.ArgumentParser` instance """""" parser = super(SystemList, self).get_parser(prog_name) return parser def take_action(self, args): """"""List systems command action. :param args: a namespace of command-line option-value pairs that come from the user :returns: CLI process exit code """""" root = super(SystemList, self).take_action(args) systems = root.get_system_collection() rows = [] for system in systems.get_members(): sys_inst = root.get_system(system.path) rows.append( [sys_inst.name, sys_inst.uuid, system.path] ) columns = [ 'Name', 'Identity', 'System ID' ] return columns, rows ",,284,0
openstack%2Fsushy-cli~master~If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a,openstack/sushy-cli,master,If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a,Add system boot device/mode commands,MERGED,2020-02-06 21:48:20.000000000,2020-03-23 21:56:51.000000000,2020-03-23 21:56:51.000000000,"[{'_account_id': 22348}, {'_account_id': 23851}, {'_account_id': 26340}, {'_account_id': 31182}]","[{'number': 1, 'created': '2020-02-06 21:48:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/f5a966df38a4f7bd621bcb5d09cf4599438ce17a', 'message': 'Add system boot commands\n\nsushycli system boot show # shows system boot allowed devices and boot mode\nsushycli system boot set  # set system boot resource\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 2, 'created': '2020-02-06 21:58:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/4be2edc940b9e7fde3395c1a6036f9425b5e6b85', 'message': 'Add system boot commands\n\nsushycli system boot show # shows system boot allowed devices and boot mode\nsushycli system boot set  # set system boot resource\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 3, 'created': '2020-02-06 22:36:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/93090865849cae22d5ad3a16aef1d82e2aedc457', 'message': 'Add system boot commands\n\nsushycli system boot show # shows system boot allowed devices and boot mode\nsushycli system boot set  # set system boot resource\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 4, 'created': '2020-02-06 22:43:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/58f58a1aa0c57600e8fa1e05705b76d2432d22bc', 'message': 'Add system boot commands\n\nsushycli system boot show # shows system boot allowed devices and boot mode\nsushycli system boot set  # set system boot resource\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 5, 'created': '2020-02-07 23:40:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/028be6e12cfb1ae4982437906d545ec8a5b5ba61', 'message': 'Add system boot commands\n\nsushycli system boot show # shows system boot allowed devices and boot mode\nsushycli system boot set  # set system boot resource\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 6, 'created': '2020-02-09 23:32:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/26728d2b02faced322a15c31927706e617961dbb', 'message': 'Add system boot commands\n\nsushycli system boot show # shows system boot allowed devices and boot mode\nsushycli system boot set  # set system boot resource\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 7, 'created': '2020-02-10 14:54:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/0bc1cfc2e534d238482b4186df0e69dc7f57fcc3', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 8, 'created': '2020-02-10 17:58:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/e11430be700ac0fdb4cae9c64e90ae3ca938943a', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 9, 'created': '2020-02-10 19:23:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/02d3438a775ad81d665cc0ea39316b02830ba67b', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 10, 'created': '2020-02-11 11:35:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/359fdf42a70ea925d0790d3da7c62de93bf3e455', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 11, 'created': '2020-02-11 21:51:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/adf04ed9c538c3448f1e7de2062982ae8ac7b0e8', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 12, 'created': '2020-02-11 21:54:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/b1e8a052a61bfe27c070f6298069c896aa31d0f2', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 13, 'created': '2020-02-12 12:36:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/fd82f1b2bca431a8efa4894f15af398a49746332', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 14, 'created': '2020-02-12 12:36:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/63c5387e2d7ccdb6ed90cb9afb3be0e9ec8d3611', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 15, 'created': '2020-02-12 14:19:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/59d210c7fd9a7648e0429cda4db666938973d523', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 16, 'created': '2020-02-12 14:25:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/7b3d5ad346a8d50748419e9c8a83d130bd12b31e', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 17, 'created': '2020-02-12 16:21:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/e54836355210d0e40d51e65f746aaa7482fd1cb1', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 18, 'created': '2020-02-13 06:08:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/bdec16dfbcdb396abf74d27fbf37ae03ca921574', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 19, 'created': '2020-02-13 07:27:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/a7bd5d58a73fcb9aaf4dc2b6c9ee17b223482898', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}, {'number': 20, 'created': '2020-02-17 10:53:45.000000000', 'files': ['sushycli/tests/unit/cmd/test_sushycli.py', 'sushycli/system_boot.py', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/65780acbf752f5d0d8bd4d2202b7fe8aff8e83dd', 'message': 'Add system boot device/mode commands\n\nThe new commands are:\n\n  # shows allowed boot devices and boot mode\n  sushycli system boot show\n\n  # set system boot device/boot mode\n  sushycli system boot set ...\n\nChange-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a\nStory: 2006608\nTask: 36776\n'}]",55,706392,65780acbf752f5d0d8bd4d2202b7fe8aff8e83dd,56,4,20,31182,,,0,"Add system boot device/mode commands

The new commands are:

  # shows allowed boot devices and boot mode
  sushycli system boot show

  # set system boot device/boot mode
  sushycli system boot set ...

Change-Id: If3e6dedacf3b6e51e04c27489420ecd0ab7c0f4a
Story: 2006608
Task: 36776
",git fetch https://review.opendev.org/openstack/sushy-cli refs/changes/92/706392/20 && git format-patch -1 --stdout FETCH_HEAD,"['sushycli/tests/unit/cmd/test_system_boot.py', 'sushycli/system_boot.py', 'setup.cfg']",3,f5a966df38a4f7bd621bcb5d09cf4599438ce17a,08-add-system-boot-management,sushycli = system_power_show = sushycli.system_power:SystemPowerShow system_power_set = sushycli.system_power:SystemPowerSet version_show = sushycli.version:VersionShow system_boot_show = sushycli.system_boot:SystemBootShow system_boot_set = sushycli.system_boot:SystemBootSet,[bdist_wheel] universal = 1,189,2
openstack%2Fpython-openstackclient~master~I918c8befc51236cc33d96a5c88fb6eafdd143e9c,openstack/python-openstackclient,master,I918c8befc51236cc33d96a5c88fb6eafdd143e9c,"Fix network segment range ""_get_ranges"" function",MERGED,2020-02-26 13:22:59.000000000,2020-03-23 21:43:33.000000000,2020-03-23 21:41:45.000000000,"[{'_account_id': 2}, {'_account_id': 841}, {'_account_id': 11904}, {'_account_id': 11975}, {'_account_id': 13252}, {'_account_id': 15334}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-02-26 13:22:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/4de1d5d78b40a04ffe9a3d16d5588794b2e2b7cd', 'message': 'Fix network segment range ""_get_ranges"" function\n\nThis function should return an ordered set of ranges based on an\nunordered list of numbers (int or str).\n\nChange-Id: I918c8befc51236cc33d96a5c88fb6eafdd143e9c\nCloses-Bug: #1864837\n'}, {'number': 2, 'created': '2020-02-26 13:23:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/3ee4148fd71ac9d03fe5af1cafb8e476b6e28334', 'message': 'Fix network segment range ""_get_ranges"" function\n\nThis function should return an ordered set of ranges based on an\nunordered list of numbers (int or str).\n\nChange-Id: I918c8befc51236cc33d96a5c88fb6eafdd143e9c\nCloses-Bug: #1864837\n'}, {'number': 3, 'created': '2020-02-27 13:32:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/130b08080ff5ffa9433ec69ceed4ea62362dbe03', 'message': 'Fix network segment range ""_get_ranges"" function\n\nThis function should return an ordered set of ranges based on an\nunordered list of numbers (int or str).\n\nChange-Id: I918c8befc51236cc33d96a5c88fb6eafdd143e9c\nCloses-Bug: #2007341\n'}, {'number': 4, 'created': '2020-03-23 14:17:36.000000000', 'files': ['openstackclient/tests/unit/network/v2/test_network_segment_range.py', 'openstackclient/network/v2/network_segment_range.py'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/27da238da2f9a8b35082d06624313a844cb6cc6f', 'message': 'Fix network segment range ""_get_ranges"" function\n\nThis function should return an ordered set of ranges based on an\nunordered list of numbers (int or str).\n\nChange-Id: I918c8befc51236cc33d96a5c88fb6eafdd143e9c\nStory: 2007341\nTask: 38878\n'}]",3,710031,27da238da2f9a8b35082d06624313a844cb6cc6f,23,8,4,16688,,,0,"Fix network segment range ""_get_ranges"" function

This function should return an ordered set of ranges based on an
unordered list of numbers (int or str).

Change-Id: I918c8befc51236cc33d96a5c88fb6eafdd143e9c
Story: 2007341
Task: 38878
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/31/710031/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstackclient/tests/unit/network/v2/test_network_segment_range.py', 'openstackclient/network/v2/network_segment_range.py']",2,4de1d5d78b40a04ffe9a3d16d5588794b2e2b7cd,bug/2007341, item = sorted([int(i) for i in item])," item = [int(i) if isinstance(i, six.string_types) else i for i in item]",15,1
openstack%2Fbifrost~master~Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219,openstack/bifrost,master,Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219,Fix dhcp job,MERGED,2020-03-11 10:10:24.000000000,2020-03-23 21:25:46.000000000,2020-03-23 21:22:52.000000000,"[{'_account_id': 11655}, {'_account_id': 14826}, {'_account_id': 15519}, {'_account_id': 22348}, {'_account_id': 23851}]","[{'number': 1, 'created': '2020-03-11 10:10:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/7eab1c4f1a7c4314478b26a52b953e582cf2f981', 'message': '[WIP] Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 2, 'created': '2020-03-11 11:15:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/25739a512df08bdee695a72d9aa86e263b8be61a', 'message': '[WIP] Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 3, 'created': '2020-03-11 17:04:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/6581fa9259bf9bd6519d62ab4b51fdd624d65ec6', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 4, 'created': '2020-03-11 17:50:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/e9d65cd60274eb89bc49a0a336323ec9b8601aa8', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 5, 'created': '2020-03-12 07:59:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/546cc17a75703fab12492a8d588cfae4fa45d6e5', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 6, 'created': '2020-03-13 14:20:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/6174579a983440b6ae6989a6bee82f3e5ad7bc10', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 7, 'created': '2020-03-17 07:06:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/e7e0e03a8c6de736114e7b36801f3a5f7da6fafa', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 8, 'created': '2020-03-17 09:52:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/d350da596888808fc486bbd0a2b5251cebcc1d5e', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 9, 'created': '2020-03-17 09:53:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/ac43b51e8c6fee5a21435b4bc1361dbcbf44d479', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 10, 'created': '2020-03-17 14:21:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/07a3bd99b924d3ba211dfaa2b74f46b69446b498', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 11, 'created': '2020-03-18 11:15:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/4f5312fbae530a21ed04e9286cbae4d5f0791428', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 12, 'created': '2020-03-18 14:04:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/ae644cbf6e3beb6a681bfa30e97a833ceb721238', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 13, 'created': '2020-03-18 14:04:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/1f00bcf38275216a762fdd9dd2ae3fbed44e308c', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 14, 'created': '2020-03-18 16:35:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/4a23c358b572151fc3d0e9d57a4a5e7131f82e78', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}, {'number': 15, 'created': '2020-03-20 09:07:24.000000000', 'files': ['playbooks/roles/bifrost-deploy-nodes-dynamic/tasks/main.yml', 'playbooks/roles/bifrost-test-dhcp/files/test-dhcp.py', 'scripts/test-bifrost.sh', 'playbooks/roles/bifrost-test-dhcp/tasks/main.yml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/a47c553e0559ec3ec392dfd2e83c6a12ecaba369', 'message': 'Fix dhcp job\n\nChange-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219\n'}]",6,712304,a47c553e0559ec3ec392dfd2e83c6a12ecaba369,48,5,15,23851,,,0,"Fix dhcp job

Change-Id: Ibb1d32ae8b69e9ad47db55d455fb3d7f0cfc4219
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/04/712304/1 && git format-patch -1 --stdout FETCH_HEAD,['scripts/test-bifrost.sh'],1,7eab1c4f1a7c4314478b26a52b953e582cf2f981,fix-dhcp," VM_MEMORY_SIZE=""2048"""," VM_MEMORY_SIZE=""1024""",1,1
openstack%2Frpm-packaging~stable%2Ftrain~I4e30553bd4f425de07a5d56939ef6030247127d4,openstack/rpm-packaging,stable/train,I4e30553bd4f425de07a5d56939ef6030247127d4,openstackclient: Add missing obsoletes,MERGED,2020-03-23 17:24:56.000000000,2020-03-23 21:08:55.000000000,2020-03-23 21:08:55.000000000,"[{'_account_id': 6593}, {'_account_id': 8482}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-23 17:24:56.000000000', 'files': ['openstack/python-openstackclient/python-openstackclient.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/6de64fb75957e586211fd6c1b3c4e1aace26b14e', 'message': 'openstackclient: Add missing obsoletes\n\nWe need to uninstall the python2.x versions that are no longer\nbeing built for being able to update in a rolling release distribution.\n\nChange-Id: I4e30553bd4f425de07a5d56939ef6030247127d4\n'}]",0,714499,6de64fb75957e586211fd6c1b3c4e1aace26b14e,9,5,1,6593,,,0,"openstackclient: Add missing obsoletes

We need to uninstall the python2.x versions that are no longer
being built for being able to update in a rolling release distribution.

Change-Id: I4e30553bd4f425de07a5d56939ef6030247127d4
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/99/714499/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/python-openstackclient/python-openstackclient.spec.j2'],1,6de64fb75957e586211fd6c1b3c4e1aace26b14e,,%if 0%{?suse_version} Obsoletes: {{ py2name(py_versions='py2') }} < 4.0.0 %endif,,3,0
openstack%2Frpm-packaging~master~I4e30553bd4f425de07a5d56939ef6030247127d4,openstack/rpm-packaging,master,I4e30553bd4f425de07a5d56939ef6030247127d4,openstackclient: Add missing obsoletes,MERGED,2020-03-23 17:23:59.000000000,2020-03-23 21:06:10.000000000,2020-03-23 21:06:10.000000000,"[{'_account_id': 6593}, {'_account_id': 8482}, {'_account_id': 13294}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-23 17:23:59.000000000', 'files': ['openstack/python-openstackclient/python-openstackclient.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/d888e94f58786a6ba1c93ff2eeac750934860e6e', 'message': 'openstackclient: Add missing obsoletes\n\nWe need to uninstall the python2.x versions that are no longer\nbeing built for being able to update in a rolling release distribution.\n\nChange-Id: I4e30553bd4f425de07a5d56939ef6030247127d4\n'}]",0,714498,d888e94f58786a6ba1c93ff2eeac750934860e6e,11,6,1,6593,,,0,"openstackclient: Add missing obsoletes

We need to uninstall the python2.x versions that are no longer
being built for being able to update in a rolling release distribution.

Change-Id: I4e30553bd4f425de07a5d56939ef6030247127d4
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/98/714498/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/python-openstackclient/python-openstackclient.spec.j2'],1,d888e94f58786a6ba1c93ff2eeac750934860e6e,,%if 0%{?suse_version} Obsoletes: {{ py2name(py_versions='py2') }} < 4.0.0 %endif,,3,0
openstack%2Fproject-config~master~I10bb5e8d171bbc84baa28678889708c32833d2bf,openstack/project-config,master,I10bb5e8d171bbc84baa28678889708c32833d2bf,Linaro US:  Add a 16GB RAM label for bionic,MERGED,2020-03-23 03:07:59.000000000,2020-03-23 20:58:00.000000000,2020-03-23 20:58:00.000000000,"[{'_account_id': 4146}, {'_account_id': 6547}, {'_account_id': 7118}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 03:07:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/b08b7e5064a27b1b980a1dec8e4249df8f89d3ee', 'message': 'Linaro US: Change bionic job to 16G RAM\n\nChange-Id: I10bb5e8d171bbc84baa28678889708c32833d2bf\nSigned-off-by: Kevin Zhao <kevin.zhao@linaro.org>\n'}, {'number': 2, 'created': '2020-03-23 05:04:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/82f4444c8ad479bf19c5741a03af742a6a986aca', 'message': 'Linaro US: Change bionic job to 16G RAM\n\nChange-Id: I10bb5e8d171bbc84baa28678889708c32833d2bf\nSigned-off-by: Kevin Zhao <kevin.zhao@linaro.org>\n'}, {'number': 3, 'created': '2020-03-23 05:41:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/6cabe434b9a0f472ceb0c464b07aae91624ba018', 'message': 'Linaro US: Change bionic job to 16G RAM\n\nChange-Id: I10bb5e8d171bbc84baa28678889708c32833d2bf\nSigned-off-by: Kevin Zhao <kevin.zhao@linaro.org>\n'}, {'number': 4, 'created': '2020-03-23 06:03:43.000000000', 'files': ['nodepool/nl03.openstack.org.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/5ec9683cc4229a4167cca158a4165cadb822f038', 'message': 'Linaro US:  Add a 16GB RAM label for bionic\n\nDevStack test need a large RAM.\nIn 8G VM we meet several weird issue.\n\nChange-Id: I10bb5e8d171bbc84baa28678889708c32833d2bf\nSigned-off-by: Kevin Zhao <kevin.zhao@linaro.org>\n'}]",0,714346,5ec9683cc4229a4167cca158a4165cadb822f038,15,4,4,22076,,,0,"Linaro US:  Add a 16GB RAM label for bionic

DevStack test need a large RAM.
In 8G VM we meet several weird issue.

Change-Id: I10bb5e8d171bbc84baa28678889708c32833d2bf
Signed-off-by: Kevin Zhao <kevin.zhao@linaro.org>
",git fetch https://review.opendev.org/openstack/project-config refs/changes/46/714346/1 && git format-patch -1 --stdout FETCH_HEAD,['nodepool/nl03.openstack.org.yaml'],1,b08b7e5064a27b1b980a1dec8e4249df8f89d3ee,, min-ram: 16000 flavor-name: 'm1.xxlarge', min-ram: 8000 flavor-name: 'os.large',2,2
openstack%2Fpython-openstackclient~master~Iefbf0f13cd349b05de910f95b9467877cb53e46b,openstack/python-openstackclient,master,Iefbf0f13cd349b05de910f95b9467877cb53e46b,Remove trailing newline from dockerhub secret,MERGED,2020-03-23 17:55:35.000000000,2020-03-23 20:52:52.000000000,2020-03-23 20:47:20.000000000,"[{'_account_id': 2}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 17:55:35.000000000', 'files': ['.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/9d96a13a97f0da0ae178433140e8aee8d834c6a1', 'message': 'Remove trailing newline from dockerhub secret\n\nWhen doing encrypt_secret, one should use echo -n not just echo.\n\nChange-Id: Iefbf0f13cd349b05de910f95b9467877cb53e46b\n'}]",0,714502,9d96a13a97f0da0ae178433140e8aee8d834c6a1,7,2,1,2,,,0,"Remove trailing newline from dockerhub secret

When doing encrypt_secret, one should use echo -n not just echo.

Change-Id: Iefbf0f13cd349b05de910f95b9467877cb53e46b
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/02/714502/1 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.yaml'],1,9d96a13a97f0da0ae178433140e8aee8d834c6a1,command_exception, - LbIZjJiVstRVXMpoLQ3+/JcNB6lKVUWJXXo5+Outf+PKAaO7mNnv8XLiFMKnJ6ftopLyu hWbX9rA+NddvplLQkf1xxkh7QBBU8PToLr58quI2SENUclt4tpjxbZfZu451kFSNJvNvR E58cHHpfJZpyRnS2htXmN/Qy24gbV2w7CQxSZD2YhlcrerD8uQ8rWEnlY1wcJEaEGomtS ZTGxsdK2TsZC2cd4b7TG7+xbl2i+hjADzwSQAgUzlLlwuG71667+IWk4SOZ7OycJTv9NN ZTak8+CGfiMKdmsxZ1Z8uD7DC+RIklDjMWyly6zuhWzfhOmsmU0CesR50moodRUvbK79p NZM8u0hBex5cl2EpUEwJL/FSPJXUhDMPoMoTZT/SAuXf25R9eZ9JGrKsIAlmVhpl8ifoE 8TpPyvIHGS3YelTQjhqOX0wGb9T4ZauQCcI5Ajzy9NuCTyD9xxme9OX1zz7gMACRnVHvz q7U7Ue90MnmGH6E2SgKjIZhyzy9Efwb7JUvH1Zb3hlrjCjEhwi9MV5FnABTEeXyYwE10s 3o/KZg2zvdWkVG6x0dEkjpoQaNuaB7T2Na7Sm421n/z3LCzhiQGuTUjENnL6cMEtuA6Pp BfI5+Qlg7HMwkBXNB73EPfWHzbCR3VNrzGYTy9FvhGud0/cXsuBXgps4WH63ic=, - X666jS/g43U4ykLzuQSNhl9XOVpKViT1bMQb/YJgTHj8AzigONu3+4BrxItS7mqwp9AWp YtNHZ0Dju3piNw07ulnsOAYHpBcz+zFK5UzVMXzkHk5vu1W0WWYLVayU4HJKliAwPmNii J3itt196sQ6IhIe/Hamcm715rP9cQd50w32lvAV4RNXoc4Gq7ZzOIzyA80UEl6HY7jiR9 WYFI4PoCrmNxKKV8YAaH1OlJW71WF0O+77+oostKwFG35bxIniUl1ZinxOgjC2va8j5nT KKgVgQdjwjuMOXEA6iiLewAo39P+nL/81AheDGQNex6x1oJuVRklQmiznbQI3knTOeJIh ncRJuE9RfS0xuR5J45WOdKWVjIYpMkTXF+5Kn/pEtNelA2ADzaloGidNxEOvOD74vVf68 mtZ1kffQi6d7yq57m0ZJ8WwTEo5g8Z+tmS40U2vdZ0vN5c/keGJisKbOpwavloP9owize U9EgIAMIT6LE/fJWPGGugMR57z7gnCNiq970crTjWvsP1yFGvo/FQiUpplv5ni6C+s9jq 1rba4Ya0uxQa2r5zjFXNJ52zGoRVlpjSsfpEtvCmTzfF5p0fa2aSyDs8ZOjaiDCVSmTPn SE9rutyH0XO7CP9RFetxirfgSQ8ZDpUmXpfKaTGa0glIfjmmf4yyaaJRKOkubg=,10,10
openstack%2Fironic~master~I6fa1989d79006c2bfb621a40afcf0003b6e8b7d9,openstack/ironic,master,I6fa1989d79006c2bfb621a40afcf0003b6e8b7d9,Follow-up releasenote use_secrets,MERGED,2020-03-23 13:03:38.000000000,2020-03-23 20:41:09.000000000,2020-03-23 16:22:53.000000000,"[{'_account_id': 19339}, {'_account_id': 22348}, {'_account_id': 23851}, {'_account_id': 24828}]","[{'number': 1, 'created': '2020-03-23 13:03:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/98fafcde0138ef7615f91bc3129ad5ba72ce377d', 'message': 'Follow-up releasenote use_secrets\n\nFollow-up to update the release note from\nI90c3c94112d093e2309414b9902f58d31d925ad3\n\nChange-Id: I6fa1989d79006c2bfb621a40afcf0003b6e8b7d9\n'}, {'number': 2, 'created': '2020-03-23 15:00:05.000000000', 'files': ['releasenotes/notes/use_secrets_to_generate_token-55af0f43e5a80b9e.yaml'], 'web_link': 'https://opendev.org/openstack/ironic/commit/f359d184107f132f63f303bb9d3a5d891220053e', 'message': 'Follow-up releasenote use_secrets\n\nFollow-up to update the release note from\nI90c3c94112d093e2309414b9902f58d31d925ad3\n\nChange-Id: I6fa1989d79006c2bfb621a40afcf0003b6e8b7d9\n'}]",1,714435,f359d184107f132f63f303bb9d3a5d891220053e,12,4,2,15519,,,0,"Follow-up releasenote use_secrets

Follow-up to update the release note from
I90c3c94112d093e2309414b9902f58d31d925ad3

Change-Id: I6fa1989d79006c2bfb621a40afcf0003b6e8b7d9
",git fetch https://review.opendev.org/openstack/ironic refs/changes/35/714435/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/notes/use_secrets_to_generate_token-55af0f43e5a80b9e.yaml'],1,98fafcde0138ef7615f91bc3129ad5ba72ce377d,follow_up_use_secrets, the secrets module to be in compliance with the FIPS 140-2 standard., the secrets module to be in compliance with the FIPS 140-2. fixes: - | The secret token that is used for IPA verification will be generated using the secrets module.,1,5
openstack%2Foctavia~master~I7169b34d93bc8a265fc74fedcbba67e980285a7e,openstack/octavia,master,I7169b34d93bc8a265fc74fedcbba67e980285a7e,Allow AZ to override valid_vip_networks config,MERGED,2019-12-18 00:48:29.000000000,2020-03-23 20:32:42.000000000,2020-03-23 20:28:58.000000000,"[{'_account_id': 1131}, {'_account_id': 6469}, {'_account_id': 7249}, {'_account_id': 10273}, {'_account_id': 11628}, {'_account_id': 22348}]","[{'number': 1, 'created': '2019-12-18 00:48:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a524fc3f8a211a44e9ca6a24c2fed4c4ad1a6e92', 'message': 'Allow AZ to override valid_vip_networks config\n\nDifferent AZs may have access to different vip networks.\n\nChange-Id: I7169b34d93bc8a265fc74fedcbba67e980285a7e\n'}, {'number': 2, 'created': '2019-12-19 01:59:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f5ee617bd761ea51b6920fa2082b89be2e7bf40d', 'message': 'Allow AZ to override valid_vip_networks config\n\nDifferent AZs may have access to different vip networks.\n\nChange-Id: I7169b34d93bc8a265fc74fedcbba67e980285a7e\n'}, {'number': 3, 'created': '2020-01-24 21:26:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cf3878342dc0b3e1321b1155d635fa88fe2444c6', 'message': 'Allow AZ to override valid_vip_networks config\n\nDifferent AZs may have access to different vip networks.\n\nChange-Id: I7169b34d93bc8a265fc74fedcbba67e980285a7e\n'}, {'number': 4, 'created': '2020-02-03 19:22:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ae16754c0786cc6742686dd55ac6eaae43812d13', 'message': 'Allow AZ to override valid_vip_networks config\n\nDifferent AZs may have access to different vip networks.\n\nChange-Id: I7169b34d93bc8a265fc74fedcbba67e980285a7e\n'}, {'number': 5, 'created': '2020-02-25 01:30:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/08ee17d667155b497470f889f48ef073cb58b9db', 'message': 'Allow AZ to override valid_vip_networks config\n\nDifferent AZs may have access to different vip networks.\n\nChange-Id: I7169b34d93bc8a265fc74fedcbba67e980285a7e\n'}, {'number': 6, 'created': '2020-02-25 01:30:56.000000000', 'files': ['octavia/api/v2/controllers/load_balancer.py', 'doc/source/admin/flavors.rst', 'octavia/common/constants.py', 'octavia/tests/unit/api/drivers/amphora_driver/v2/test_amphora_driver.py', 'doc/source/contributor/guides/providers.rst', 'octavia/api/drivers/amphora_driver/v2/driver.py', 'octavia/common/validate.py', 'releasenotes/notes/availability-zones-can-override-valid-vip-networks-5566aa4769c158dc.yaml', 'octavia/api/drivers/amphora_driver/availability_zone_schema.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/4a360bfda70639bc62731c326f8cbeee6ed2cd10', 'message': 'Allow AZ to override valid_vip_networks config\n\nDifferent AZs may have access to different vip networks.\n\nChange-Id: I7169b34d93bc8a265fc74fedcbba67e980285a7e\n'}]",4,699521,4a360bfda70639bc62731c326f8cbeee6ed2cd10,24,6,6,10273,,,0,"Allow AZ to override valid_vip_networks config

Different AZs may have access to different vip networks.

Change-Id: I7169b34d93bc8a265fc74fedcbba67e980285a7e
",git fetch https://review.opendev.org/openstack/octavia refs/changes/21/699521/1 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/api/v2/controllers/load_balancer.py', 'octavia/common/constants.py', 'octavia/api/drivers/amphora_driver/v2/driver.py', 'octavia/api/drivers/amphora_driver/availability_zone_schema.py', 'octavia/common/validate.py']",5,a524fc3f8a211a44e9ca6a24c2fed4c4ad1a6e92,az-tweaks,"def network_allowed_by_config(network_id, valid_networks=None): if CONF.networking.valid_vip_networks and not valid_networks: valid_networks = CONF.networking.valid_vip_networks if valid_networks: valid_networks = map(str.lower, valid_networks)","def network_allowed_by_config(network_id): if CONF.networking.valid_vip_networks: valid_networks = map(str.lower, CONF.networking.valid_vip_networks)",31,4
openstack%2Frally~master~Ifcf9f3a33fe956cff6953aa5e2bdd43c0e18f027,openstack/rally,master,Ifcf9f3a33fe956cff6953aa5e2bdd43c0e18f027,Support cinder api v3,ABANDONED,2017-12-05 09:45:26.000000000,2020-03-23 20:32:14.000000000,,"[{'_account_id': 9545}, {'_account_id': 14817}, {'_account_id': 21528}, {'_account_id': 22348}, {'_account_id': 25072}, {'_account_id': 26015}]","[{'number': 1, 'created': '2017-12-05 09:45:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5b6d81a122c4431dc01908519e4f12e903f347fe', 'message': 'Support cinder api v3\n\nCinder already support api version 3, but rally is still not use v3 to\ntest, this patch is to make rally can test with v3 for cinder\n\nChange-Id: Ifcf9f3a33fe956cff6953aa5e2bdd43c0e18f027\n'}, {'number': 2, 'created': '2018-05-07 05:45:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b95db38d6372747cd1025b3889a595ee96e70f5e', 'message': 'Support cinder api v3\n\nCinder already support api version 3, but rally is still not use v3 to\ntest, this patch is to make rally can test with v3 for cinder\n\nChange-Id: Ifcf9f3a33fe956cff6953aa5e2bdd43c0e18f027\n'}, {'number': 3, 'created': '2018-05-07 07:13:42.000000000', 'files': ['rally/plugins/openstack/services/storage/cinder_v3.py', 'rally/consts.py', 'tests/unit/plugins/openstack/services/storage/test_cinder_v3.py', 'rally/plugins/openstack/osclients.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/b8d9ac8d28a4cbe0d75af28e0516385db33abbbd', 'message': 'Support cinder api v3\n\nCinder already support api version 3, but rally is still not use v3 to\ntest, this patch is to make rally can test with v3 for cinder\n\nChange-Id: Ifcf9f3a33fe956cff6953aa5e2bdd43c0e18f027\n'}]",3,525513,b8d9ac8d28a4cbe0d75af28e0516385db33abbbd,16,6,3,25072,,,0,"Support cinder api v3

Cinder already support api version 3, but rally is still not use v3 to
test, this patch is to make rally can test with v3 for cinder

Change-Id: Ifcf9f3a33fe956cff6953aa5e2bdd43c0e18f027
",git fetch https://review.opendev.org/openstack/rally refs/changes/13/525513/3 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/openstack/services/storage/cinder_v3.py', 'rally/consts.py', 'tests/unit/plugins/openstack/services/storage/test_cinder_v3.py', 'rally/plugins/openstack/osclients.py']",4,5b6d81a122c4431dc01908519e4f12e903f347fe,cinder_v3," supported_versions=[""1"", ""2"", ""3""])"," supported_versions=[""1"", ""2""])",804,1
openstack%2Foctavia~master~Ia930e17c76cd601ac005de10fb03231a19f1a776,openstack/octavia,master,Ia930e17c76cd601ac005de10fb03231a19f1a776,Network Delta calculations should respect AZs,MERGED,2020-01-31 07:31:21.000000000,2020-03-23 20:30:53.000000000,2020-03-23 20:28:57.000000000,"[{'_account_id': 1131}, {'_account_id': 6469}, {'_account_id': 10273}, {'_account_id': 11628}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-01-31 07:31:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/492816e5676dde027b6f35d0efce948431fc8bf7', 'message': ""Network Delta calculations should respect AZs\n\nThe network delta calculations were all based on the static configured\namp_boot_network_list which is not correct if it's overridden by the AZ.\n\nChange-Id: Ia930e17c76cd601ac005de10fb03231a19f1a776\n""}, {'number': 2, 'created': '2020-01-31 20:14:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2f05458a5580675ae824b39bb9a0d1f860c60d58', 'message': ""Network Delta calculations should respect AZs\n\nThe network delta calculations were all based on the static configured\namp_boot_network_list which is not correct if it's overridden by the AZ.\n\nChange-Id: Ia930e17c76cd601ac005de10fb03231a19f1a776\n""}, {'number': 3, 'created': '2020-02-25 01:30:12.000000000', 'files': ['octavia/tests/unit/controller/worker/v2/tasks/test_network_tasks.py', 'octavia/tests/unit/controller/worker/v2/test_controller_worker.py', 'octavia/controller/worker/v1/flows/load_balancer_flows.py', 'octavia/controller/worker/v1/flows/member_flows.py', 'octavia/controller/worker/v2/flows/member_flows.py', 'octavia/tests/unit/controller/worker/v2/flows/test_member_flows.py', 'octavia/controller/worker/v2/flows/load_balancer_flows.py', 'octavia/tests/unit/controller/worker/v1/test_controller_worker.py', 'octavia/controller/worker/v1/controller_worker.py', 'octavia/controller/worker/v1/flows/amphora_flows.py', 'octavia/controller/worker/v2/controller_worker.py', 'octavia/tests/unit/controller/worker/v1/tasks/test_network_tasks.py', 'octavia/controller/worker/v2/tasks/network_tasks.py', 'octavia/controller/worker/v1/tasks/network_tasks.py', 'octavia/tests/unit/controller/worker/v1/flows/test_member_flows.py', 'octavia/controller/worker/v2/flows/amphora_flows.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/741397f1a9e0c5feda90f85b752a30ed7c23faf1', 'message': ""Network Delta calculations should respect AZs\n\nThe network delta calculations were all based on the static configured\namp_boot_network_list which is not correct if it's overridden by the AZ.\n\nChange-Id: Ia930e17c76cd601ac005de10fb03231a19f1a776\n""}]",1,705165,741397f1a9e0c5feda90f85b752a30ed7c23faf1,16,5,3,10273,,,0,"Network Delta calculations should respect AZs

The network delta calculations were all based on the static configured
amp_boot_network_list which is not correct if it's overridden by the AZ.

Change-Id: Ia930e17c76cd601ac005de10fb03231a19f1a776
",git fetch https://review.opendev.org/openstack/octavia refs/changes/65/705165/3 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/tests/unit/controller/worker/v2/tasks/test_network_tasks.py', 'octavia/tests/unit/controller/worker/v2/test_controller_worker.py', 'octavia/controller/worker/v1/flows/load_balancer_flows.py', 'octavia/controller/worker/v1/flows/member_flows.py', 'octavia/controller/worker/v2/flows/member_flows.py', 'octavia/tests/unit/controller/worker/v2/flows/test_member_flows.py', 'octavia/controller/worker/v2/flows/load_balancer_flows.py', 'octavia/tests/unit/controller/worker/v1/test_controller_worker.py', 'octavia/controller/worker/v1/controller_worker.py', 'octavia/controller/worker/v1/flows/amphora_flows.py', 'octavia/controller/worker/v2/controller_worker.py', 'octavia/tests/unit/controller/worker/v1/tasks/test_network_tasks.py', 'octavia/controller/worker/v2/tasks/network_tasks.py', 'octavia/controller/worker/v1/tasks/network_tasks.py', 'octavia/tests/unit/controller/worker/v1/flows/test_member_flows.py', 'octavia/controller/worker/v2/flows/amphora_flows.py']",16,492816e5676dde027b6f35d0efce948431fc8bf7,az-tweaks," requires=(constants.LOADBALANCER, constants.AMPHORA, constants.AVAILABILITY_ZONE),"," requires=(constants.LOADBALANCER, constants.AMPHORA),",128,67
openstack%2Fopenstack-ansible-os_tempest~master~I6ce85678381d6d75e04a3de1a4f15d34cee7858a,openstack/openstack-ansible-os_tempest,master,I6ce85678381d6d75e04a3de1a4f15d34cee7858a,"Missing document start ""---""",MERGED,2020-03-13 09:26:54.000000000,2020-03-23 20:27:00.000000000,2020-03-23 20:23:12.000000000,"[{'_account_id': 22348}, {'_account_id': 23317}, {'_account_id': 25023}, {'_account_id': 29865}]","[{'number': 1, 'created': '2020-03-13 09:26:54.000000000', 'files': ['zuul.d/jobs.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_tempest/commit/442f3a0eb5c13438be74bb9644d29e6e511c9d3e', 'message': 'Missing document start ""---""\n\nChange-Id: I6ce85678381d6d75e04a3de1a4f15d34cee7858a\n'}]",0,712879,442f3a0eb5c13438be74bb9644d29e6e511c9d3e,10,4,1,23317,,,0,"Missing document start ""---""

Change-Id: I6ce85678381d6d75e04a3de1a4f15d34cee7858a
",git fetch https://review.opendev.org/openstack/openstack-ansible-os_tempest refs/changes/79/712879/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/jobs.yaml'],1,442f3a0eb5c13438be74bb9644d29e6e511c9d3e,fix,---,,1,0
openstack%2Fxstatic-angular-lrdragndrop~master~Ifd17bc8e39bf61d12835fec889f0ea7945b0234d,openstack/xstatic-angular-lrdragndrop,master,Ifd17bc8e39bf61d12835fec889f0ea7945b0234d,add the project and bug report url in README.rst,ABANDONED,2019-06-18 09:05:42.000000000,2020-03-23 20:26:17.000000000,,"[{'_account_id': 841}, {'_account_id': 22348}]","[{'number': 1, 'created': '2019-06-18 09:05:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/xstatic-angular-lrdragndrop/commit/e6053a78a7c15b872ce68233b00292fb547eb8ab', 'message': 'add the project and bug report url in README.rst\n\nChange-Id: Ifd17bc8e39bf61d12835fec889f0ea7945b0234d\n'}, {'number': 2, 'created': '2019-08-27 07:25:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/xstatic-angular-lrdragndrop/commit/1e60ae994add8259cadfd5588d2824a1cf82ac0d', 'message': 'add the project and bug report url in README.rst\n\nChange-Id: Ifd17bc8e39bf61d12835fec889f0ea7945b0234d\n'}, {'number': 3, 'created': '2019-08-27 07:28:22.000000000', 'files': ['README.txt'], 'web_link': 'https://opendev.org/openstack/xstatic-angular-lrdragndrop/commit/d720aa3e658b9dee44d927762ec2ca1b72d81aa6', 'message': 'add the project and bug report url in README.rst\n\nChange-Id: Ifd17bc8e39bf61d12835fec889f0ea7945b0234d\n'}]",2,665919,d720aa3e658b9dee44d927762ec2ca1b72d81aa6,13,2,3,30455,,,0,"add the project and bug report url in README.rst

Change-Id: Ifd17bc8e39bf61d12835fec889f0ea7945b0234d
",git fetch https://review.opendev.org/openstack/xstatic-angular-lrdragndrop refs/changes/19/665919/1 && git format-patch -1 --stdout FETCH_HEAD,['README.txt'],1,e6053a78a7c15b872ce68233b00292fb547eb8ab,, The project home is at https://launchpad.net/xstatic-angular-irdragndrop The bugs is at: https://bugs.launchpad.net/xstatic-angular-irdragndrop,,7,0
openstack%2Fopenstack-ansible-os_tempest~master~I69587a455cb875e5e1f1a509a87082ceeab52d91,openstack/openstack-ansible-os_tempest,master,I69587a455cb875e5e1f1a509a87082ceeab52d91,Use current stackviz tarball,MERGED,2020-02-26 08:27:58.000000000,2020-03-23 19:54:58.000000000,2020-03-23 19:53:27.000000000,"[{'_account_id': 1004}, {'_account_id': 6547}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25023}, {'_account_id': 25600}]","[{'number': 1, 'created': '2020-02-26 08:27:58.000000000', 'files': ['defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_tempest/commit/35f80371c2fc33e0e6c3f3968d6fbd017d8a72e7', 'message': 'Use current stackviz tarball\n\nhttps://tarballs.openstack.org/package-stackviz-element/ has not been\nupdated since 2017, use the current location for the stackviz tarball.\n\nChange-Id: I69587a455cb875e5e1f1a509a87082ceeab52d91\n'}]",0,709977,35f80371c2fc33e0e6c3f3968d6fbd017d8a72e7,14,6,1,6547,,,0,"Use current stackviz tarball

https://tarballs.openstack.org/package-stackviz-element/ has not been
updated since 2017, use the current location for the stackviz tarball.

Change-Id: I69587a455cb875e5e1f1a509a87082ceeab52d91
",git fetch https://review.opendev.org/openstack/openstack-ansible-os_tempest refs/changes/77/709977/1 && git format-patch -1 --stdout FETCH_HEAD,['defaults/main.yml'],1,35f80371c2fc33e0e6c3f3968d6fbd017d8a72e7,fix-stackviz,"stackviz_tarball: ""https://tarballs.opendev.org/openstack/stackviz/dist/stackviz-latest.tar.gz""","stackviz_tarball: ""https://tarballs.openstack.org/package-stackviz-element/stackviz-latest.tar.gz""",1,1
openstack%2Fnova~master~I167ce1b5c3220c7eb1b3d55cc4a8ea10df9df796,openstack/nova,master,I167ce1b5c3220c7eb1b3d55cc4a8ea10df9df796,Merge qos related renos for Ussuri,MERGED,2020-02-10 09:34:31.000000000,2020-03-23 19:41:22.000000000,2020-03-23 19:38:08.000000000,"[{'_account_id': 9008}, {'_account_id': 9708}, {'_account_id': 14070}, {'_account_id': 14384}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26458}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-02-10 09:34:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c5cb650fb831ec97b750f2e41e315d3672df7980', 'message': 'Merge qos related renos for Ussuri\n\nAs now all three remaining server move operation is supported with qos\nports the three separate reno for Ussuri is merged into a single one.\n\nChange-Id: I167ce1b5c3220c7eb1b3d55cc4a8ea10df9df796\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 2, 'created': '2020-02-11 08:49:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/0815c95322b62915d62b5ed0ee0a8a9917ddd019', 'message': 'Merge qos related renos for Ussuri\n\nAs now all three remaining server move operation is supported with qos\nports the three separate reno for Ussuri is merged into a single one.\n\nChange-Id: I167ce1b5c3220c7eb1b3d55cc4a8ea10df9df796\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 3, 'created': '2020-02-11 12:27:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/bd3c6b8012566642fbef8a81cb404471b1018e8c', 'message': 'Merge qos related renos for Ussuri\n\nAs now all three remaining server move operation is supported with qos\nports the three separate reno for Ussuri is merged into a single one.\n\nChange-Id: I167ce1b5c3220c7eb1b3d55cc4a8ea10df9df796\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 4, 'created': '2020-02-19 17:12:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/5677b8736c3bb3b1601b1bb9e0d22b963b140313', 'message': 'Merge qos related renos for Ussuri\n\nAs now all three remaining server move operation is supported with qos\nports the three separate reno for Ussuri is merged into a single one.\n\nChange-Id: I167ce1b5c3220c7eb1b3d55cc4a8ea10df9df796\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 5, 'created': '2020-02-21 15:31:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/29f84ee7508cf48df04fe189e3bb7df87554792c', 'message': 'Merge qos related renos for Ussuri\n\nAs now all three remaining server move operation is supported with qos\nports the three separate reno for Ussuri is merged into a single one.\n\nChange-Id: I167ce1b5c3220c7eb1b3d55cc4a8ea10df9df796\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 6, 'created': '2020-03-18 14:40:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1c92a1f41131988b7fadddc56bb86017d36c2c09', 'message': 'Merge qos related renos for Ussuri\n\nAs now all three remaining server move operation is supported with qos\nports the three separate reno for Ussuri is merged into a single one.\n\nChange-Id: I167ce1b5c3220c7eb1b3d55cc4a8ea10df9df796\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 7, 'created': '2020-03-18 18:13:51.000000000', 'files': ['releasenotes/notes/support-live-migrating-servers-with-neutron-ports-with-resource-request-cf9a21dacb9c5ece.yaml', 'releasenotes/notes/support-unshelving-servers-with-neutron-ports-with-resource-request-d91a282fe56c7489.yaml', 'releasenotes/notes/support-server-move-operations-with-neutron-ports-with-resource-request-c41598d0e4aef37b.yaml', 'releasenotes/notes/support-evacuting-servers-with-neutron-ports-with-resource-request-04cf8c721cbc376f.yaml'], 'web_link': 'https://opendev.org/openstack/nova/commit/24113ba0156414b38613e8d52fe8363e8edbaaa8', 'message': 'Merge qos related renos for Ussuri\n\nAs now all three remaining server move operation is supported with qos\nports the three separate reno for Ussuri is merged into a single one.\n\nChange-Id: I167ce1b5c3220c7eb1b3d55cc4a8ea10df9df796\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}]",5,706766,24113ba0156414b38613e8d52fe8363e8edbaaa8,63,11,7,9708,,,0,"Merge qos related renos for Ussuri

As now all three remaining server move operation is supported with qos
ports the three separate reno for Ussuri is merged into a single one.

Change-Id: I167ce1b5c3220c7eb1b3d55cc4a8ea10df9df796
blueprint: support-move-ops-with-qos-ports-ussuri
",git fetch https://review.opendev.org/openstack/nova refs/changes/66/706766/3 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/support-live-migrating-servers-with-neutron-ports-with-resource-request-cf9a21dacb9c5ece.yaml', 'releasenotes/notes/support-unshelving-servers-with-neutron-ports-with-resource-request-d91a282fe56c7489.yaml', 'releasenotes/notes/support-server-move-operations-with-neutron-ports-with-resource-request-c41598d0e4aef37b.yaml', 'releasenotes/notes/support-evacuting-servers-with-neutron-ports-with-resource-request-04cf8c721cbc376f.yaml']",4,c5cb650fb831ec97b750f2e41e315d3672df7980,bp/support-move-ops-with-qos-ports-ussuri,,"--- features: - | The server ``evacute`` action API now supports servers with neutron ports having resource requests, e.g. ports that have QoS minimum bandwidth rules attached. ",6,18
openstack%2Fnova~master~Iaf70ee41f1bfb1a4964da3f59cd3a0b4b5e20d36,openstack/nova,master,Iaf70ee41f1bfb1a4964da3f59cd3a0b4b5e20d36,Enable unshelve with qos ports,MERGED,2020-02-03 16:13:02.000000000,2020-03-23 19:29:36.000000000,2020-03-23 19:24:42.000000000,"[{'_account_id': 4393}, {'_account_id': 9008}, {'_account_id': 9708}, {'_account_id': 10118}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26458}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-02-03 16:13:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/14b807ba8be1bf717ae31bc1ffe17bae94d0789c', 'message': 'Enable unshelve with qos ports\n\nPrevious patches in the blueprint implemented the support for unshelve\nwith qos ports and added functional test coverage for the\nvarious scenarios. So this patch removes the API check\nthat rejected such operation and document the new feature.\n\nAs this was the last unsupported move operation this patch also cleans\nup the master switch stub function.\n\nChange-Id: Iaf70ee41f1bfb1a4964da3f59cd3a0b4b5e20d36\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 2, 'created': '2020-02-10 09:32:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/4ba0d2101cec0814bab3cd501eacf01becb9116f', 'message': 'Enable unshelve with qos ports\n\nPrevious patches in the blueprint implemented the support for unshelve\nwith qos ports and added functional test coverage for the\nvarious scenarios. So this patch removes the API check\nthat rejected such operation and document the new feature.\n\nAs this was the last unsupported move operation this patch also cleans\nup the master switch stub function.\n\nChange-Id: Iaf70ee41f1bfb1a4964da3f59cd3a0b4b5e20d36\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 3, 'created': '2020-02-11 12:25:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/29f4eda1ea93581eb76d4d43b16f7d35a41e57f7', 'message': 'Enable unshelve with qos ports\n\nPrevious patches in the blueprint implemented the support for unshelve\nwith qos ports and added functional test coverage for the\nvarious scenarios. So this patch removes the API check\nthat rejected such operation and document the new feature.\n\nAs this was the last unsupported move operation this patch also cleans\nup the master switch stub function.\n\nChange-Id: Iaf70ee41f1bfb1a4964da3f59cd3a0b4b5e20d36\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 4, 'created': '2020-02-21 15:29:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/09cfa07a5da19f8d2f8fdeba30968d7d7d8bc1f1', 'message': 'Enable unshelve with qos ports\n\nPrevious patches in the blueprint implemented the support for unshelve\nwith qos ports and added functional test coverage for the\nvarious scenarios. So this patch removes the API check\nthat rejected such operation and document the new feature.\n\nAs this was the last unsupported move operation this patch also cleans\nup the master switch stub function.\n\nChange-Id: Iaf70ee41f1bfb1a4964da3f59cd3a0b4b5e20d36\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 5, 'created': '2020-03-18 10:06:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/7e50e6ead950904f095e989a28be77b71d86963d', 'message': 'Enable unshelve with qos ports\n\nPrevious patches in the blueprint implemented the support for unshelve\nwith qos ports and added functional test coverage for the\nvarious scenarios. So this patch removes the API check\nthat rejected such operation and document the new feature.\n\nAs this was the last unsupported move operation this patch also cleans\nup the master switch stub function.\n\nChange-Id: Iaf70ee41f1bfb1a4964da3f59cd3a0b4b5e20d36\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 6, 'created': '2020-03-18 14:38:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/448b24867a1a30c8952b6f127420f8d100502727', 'message': 'Enable unshelve with qos ports\n\nPrevious patches in the blueprint implemented the support for unshelve\nwith qos ports and added functional test coverage for the\nvarious scenarios. So this patch changes the API check\nthat rejected such operation to check for the service version and therefore\nconditionally enable the feature.\n\nChange-Id: Iaf70ee41f1bfb1a4964da3f59cd3a0b4b5e20d36\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 7, 'created': '2020-03-18 18:10:50.000000000', 'files': ['nova/api/openstack/compute/shelve.py', 'releasenotes/notes/support-unshelving-servers-with-neutron-ports-with-resource-request-d91a282fe56c7489.yaml', 'api-guide/source/port_with_resource_request.rst', 'nova/api/openstack/common.py', 'nova/tests/functional/test_servers.py', 'nova/tests/unit/api/openstack/compute/test_server_actions.py', 'nova/tests/unit/api/openstack/compute/test_migrate_server.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/a4c3260f56c9189378a1406869eea75e2d023fb2', 'message': 'Enable unshelve with qos ports\n\nPrevious patches in the blueprint implemented the support for unshelve\nwith qos ports and added functional test coverage for the\nvarious scenarios. So this patch changes the API check\nthat rejected such operation to check for the service version and therefore\nconditionally enable the feature.\n\nChange-Id: Iaf70ee41f1bfb1a4964da3f59cd3a0b4b5e20d36\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}]",9,705475,a4c3260f56c9189378a1406869eea75e2d023fb2,113,13,7,9708,,,0,"Enable unshelve with qos ports

Previous patches in the blueprint implemented the support for unshelve
with qos ports and added functional test coverage for the
various scenarios. So this patch changes the API check
that rejected such operation to check for the service version and therefore
conditionally enable the feature.

Change-Id: Iaf70ee41f1bfb1a4964da3f59cd3a0b4b5e20d36
blueprint: support-move-ops-with-qos-ports-ussuri
",git fetch https://review.opendev.org/openstack/nova refs/changes/75/705475/7 && git format-patch -1 --stdout FETCH_HEAD,"['nova/api/openstack/compute/shelve.py', 'releasenotes/notes/support-unshelving-servers-with-neutron-ports-with-resource-request-d91a282fe56c7489.yaml', 'api-guide/source/port_with_resource_request.rst', 'nova/api/openstack/common.py', 'nova/tests/functional/test_servers.py', 'nova/tests/unit/api/openstack/compute/test_server_actions.py', 'nova/tests/unit/api/openstack/compute/test_migrate_server.py']",7,14b807ba8be1bf717ae31bc1ffe17bae94d0789c,bp/support-move-ops-with-qos-ports-ussuri," self, mock_has_res_req, mock_get_service):"," @mock.patch('nova.api.openstack.common.' 'supports_port_resource_request_during_move', return_value=True) self, mock_has_res_req, mock_get_service, mock_support):",12,127
openstack%2Fkeystone~master~I205e8bbf9a4579b16177f57e29e363f4205a2b48,openstack/keystone,master,I205e8bbf9a4579b16177f57e29e363f4205a2b48,Add openstack_groups to assertion,MERGED,2018-08-02 11:03:08.000000000,2020-03-23 19:28:18.000000000,2020-03-23 19:24:38.000000000,"[{'_account_id': 2218}, {'_account_id': 5046}, {'_account_id': 8482}, {'_account_id': 8871}, {'_account_id': 15054}, {'_account_id': 16465}, {'_account_id': 21420}, {'_account_id': 22348}, {'_account_id': 27621}]","[{'number': 1, 'created': '2018-08-02 11:03:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/b6e0f35545622511cc87b92ec56674e079638f64', 'message': 'Add openstack_user_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_user_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 2, 'created': '2018-08-02 14:11:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/28e890369beb90e407ad6fdcf655900014c391ac', 'message': 'Add openstack_user_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_user_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 3, 'created': '2018-08-03 09:29:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/bc7b60ee66e03292ed2c6f60e888315d47f6cc14', 'message': 'Add openstack_user_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_user_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 4, 'created': '2018-08-03 11:18:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/0ca72eb9877c4869f317de6e609368a2dba40bea', 'message': 'Add openstack_user_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_user_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 5, 'created': '2018-08-06 06:34:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/c321af46277085f8a8c961f2df8a760529a1f9ed', 'message': 'Add openstack_user_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_user_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 6, 'created': '2018-08-06 06:50:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/4beccb1d032a46a208e464a6b470a003182dfc1b', 'message': 'Add openstack_user_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_user_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 7, 'created': '2018-08-06 09:06:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/c4e8f804e50c439249a21b538f1b900d8ecbd83b', 'message': 'Add openstack_user_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_user_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 8, 'created': '2018-08-06 10:29:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/dc0867114f0b63ae525b46b66285d53c8a9dffc3', 'message': 'Add openstack_user_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_user_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 9, 'created': '2018-11-19 07:56:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/d346d426333dfcae09d63f9ec3fafb3248b7ba4e', 'message': 'Add openstack_user_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_user_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 10, 'created': '2018-11-19 08:44:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/08bf47c92d22a6ff3afddd666418dfb5f8551d1e', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 11, 'created': '2018-11-21 01:00:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/ef69fcc91a7d90c000fcca26b63126df27a8e7e4', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 12, 'created': '2018-12-21 07:15:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/46c070e08845c41474610feac3dedb201ba16f09', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 13, 'created': '2018-12-21 11:32:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/9b0c6cade4621121d75c2de6a6784890b68e69a9', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 14, 'created': '2018-12-24 06:03:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/63e1c7f98ce82c83c02619543c5b14a9b643bcf9', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 15, 'created': '2018-12-27 06:43:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/c1bb5f22bafa03a5bcffe2f2b73b287fc64be44c', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 16, 'created': '2018-12-27 07:24:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/2ee36ef21d2432150c8ef88ae3f03a8d906715cd', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 17, 'created': '2019-01-07 11:20:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/32fd95cd58d35d6d72bca8f0b233e5cdcdc76053', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 18, 'created': '2019-01-08 15:05:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/4db761db87943cf913bfa41e107a7a17dc658190', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 19, 'created': '2019-01-09 03:55:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/4c7ed7bc731243b9740e188b03719c43ab09ac9a', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 20, 'created': '2019-01-09 09:36:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/2a295c48ea80c015be86a967195cb19d2f45ead2', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 21, 'created': '2019-01-09 15:07:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/5caec760a4f7b4ba5ab96b30bbcc746e50eb65c0', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 22, 'created': '2019-01-11 06:39:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/2e426052c755812a4522961659ad49494322839c', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 23, 'created': '2019-01-15 10:46:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/9eed0faf3d181e3ade4eb5ffd201021630b39306', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 24, 'created': '2019-01-16 09:53:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/59258326ff73333e7c43cc87449fe250bac8254d', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 25, 'created': '2019-01-16 16:22:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/466203cd6665000ea2d92e325bdff49064ba6dc2', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 26, 'created': '2019-01-17 07:18:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/7e9890e698655bb91848bed4ef3acab036ee9112', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 27, 'created': '2019-01-31 06:23:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/7c424032c483675275adeae5b7d15a309574d5ee', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 28, 'created': '2019-01-31 09:40:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/1ee8523031ac8f4ea18c7b59697740516f513718', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 29, 'created': '2019-01-31 09:52:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/3bce19eea5c3930692e771bc8dceb1e2fa6a6acd', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 30, 'created': '2019-02-06 10:49:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/575cfd946284e173ecdbf7628fef550c7dd5316c', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 31, 'created': '2019-02-28 10:22:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/cb052ce30d85e8391996a7767c15ee47de17c040', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 32, 'created': '2019-02-28 10:49:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/1c4231ff4fa72a0ea1262c1543791231b2b22150', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 33, 'created': '2019-07-29 06:59:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/10611d32db9f39c6de3bef4e1faa4cf655cf9d85', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 34, 'created': '2019-07-30 05:15:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/a4aaab37d48eb47268b1139ed61c3ac39893661d', 'message': '[WIP] Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 35, 'created': '2019-12-25 14:34:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/ae9222188c9e9b6605bac4237c332e2492852558', 'message': '[WIP] Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 36, 'created': '2019-12-26 11:26:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/07c3a9e85bd3586887b85061721d631b31340a0e', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 37, 'created': '2020-01-17 11:01:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/31ac1477fd67021e5771fd52aa9534a362016a09', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 38, 'created': '2020-01-21 09:24:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/6b21f9d1ca28589c39805b79f6404b02eea1f635', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 39, 'created': '2020-01-22 11:34:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/38650989da0062cb2c73838e3e584045158410b7', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 40, 'created': '2020-01-27 07:20:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/6d7bdac2aecd653ff9068c36f5809bad2bd13745', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 41, 'created': '2020-02-14 10:34:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/afa5fa3339df13b07c0fc74d2bca28509ada0e35', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 42, 'created': '2020-02-17 08:19:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/7744c88648e8ff3f5c6f73de4ff4aa7195290893', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 43, 'created': '2020-02-17 09:56:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/4f80b164a2b89b3c4bbafdc2b0fa64cdbdaa539d', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 44, 'created': '2020-02-20 08:48:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/801577dca3f121c54022830fbfc652b8fd32a1f8', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 45, 'created': '2020-02-20 11:15:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/3dd00a31335537070737d59eeea7a661792f6d99', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 46, 'created': '2020-02-27 16:45:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/748bfb2d74201a5203466d29cfd4e56d71796d2d', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 47, 'created': '2020-03-04 07:24:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/de2928d75102b00210e82aea994957845e99a345', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 48, 'created': '2020-03-04 10:05:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/0115f58b3d92d2d5f042ea6a0001459b73b2f7f1', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 49, 'created': '2020-03-04 10:06:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/2e2780a7fcd3d73de44fcc0da660c86547c90a3d', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 50, 'created': '2020-03-19 11:56:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/902553ee5bb501d7ae695e8e5daecd8bb64d1682', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}, {'number': 51, 'created': '2020-03-19 14:45:30.000000000', 'files': ['doc/source/admin/federation/shibboleth.inc', 'releasenotes/notes/bug-1641625-fe463874dc5edb10.yaml', 'keystone/federation/utils.py', 'keystone/tests/unit/test_v3_federation.py', 'doc/source/admin/federation/mapping_combinations.rst', 'keystone/tests/unit/mapping_fixtures.py', 'devstack/files/federation/attribute-map.xml', 'keystone/api/_shared/saml.py', 'keystone/tests/unit/saml2/signed_saml2_assertion.xml', 'keystone/tests/unit/contrib/federation/test_utils.py', 'keystone/federation/idp.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/dda426b61a18590a81c5b3af281eb0c410756692', 'message': 'Add openstack_groups to assertion\n\nCurrently, a keystone IdP does not provide the\ngroups to which user belong when generating SAML\nassertions.This patch adds an additional attribute\ncalled ""openstack_groups"" in the assertion.\n\nChange-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48\nCloses-Bug: #1641625\n'}]",98,588211,dda426b61a18590a81c5b3af281eb0c410756692,193,9,51,27621,,,0,"Add openstack_groups to assertion

Currently, a keystone IdP does not provide the
groups to which user belong when generating SAML
assertions.This patch adds an additional attribute
called ""openstack_groups"" in the assertion.

Change-Id: I205e8bbf9a4579b16177f57e29e363f4205a2b48
Closes-Bug: #1641625
",git fetch https://review.opendev.org/openstack/keystone refs/changes/11/588211/47 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/tests/unit/test_v3_federation.py', 'keystone/tests/unit/saml2/signed_saml2_assertion.xml', 'keystone/federation/idp.py', 'keystone/federation/controllers.py']",4,b6e0f35545622511cc87b92ec56674e079638f64,bug/1641625," user_group_ids = [] for user_group in token.group_ids: user_group_ids.append(user_group['id']) role_names, project, project_domain_name, user_group_ids)"," role_names, project, project_domain_name)",60,12
openstack%2Fnetworking-ovn~stable%2Ftrain~I3d3c9ced93a0f8e1d5565f37ad2d9626650ca5d0,openstack/networking-ovn,stable/train,I3d3c9ced93a0f8e1d5565f37ad2d9626650ca5d0,DNM: More logs to reproduce bug 1728282,ABANDONED,2020-03-09 17:42:44.000000000,2020-03-23 19:16:51.000000000,,"[{'_account_id': 11952}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-09 17:42:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/dc0489009abb73a3d38c67a4958ff5009d1d1613', 'message': 'DNM: More logs to reproduce bug 1728282\n\nPlease ignore\n\nChange-Id: I3d3c9ced93a0f8e1d5565f37ad2d9626650ca5d0\n'}, {'number': 2, 'created': '2020-03-10 22:14:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/97e493cc61041cf32949403114d80d6394209527', 'message': 'DNM: More logs to reproduce bug 1728282\n\nPlease ignore\n\nChange-Id: I3d3c9ced93a0f8e1d5565f37ad2d9626650ca5d0\n'}, {'number': 3, 'created': '2020-03-11 15:03:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/3afa7f0db21afe3ff39d976dd40017b8ee6dfe6b', 'message': 'DNM: More logs to reproduce bug 1728282\n\nPlease ignore\n\nChange-Id: I3d3c9ced93a0f8e1d5565f37ad2d9626650ca5d0\n'}, {'number': 4, 'created': '2020-03-11 15:08:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/277f4f12915b1cd05262013d70ba451c20a4ed8c', 'message': 'DNM: More logs to reproduce bug 1728282\n\nPlease ignore\n\nChange-Id: I3d3c9ced93a0f8e1d5565f37ad2d9626650ca5d0\n'}, {'number': 5, 'created': '2020-03-11 15:11:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/c0817bb08ab804d1b52124c4dba226ab69dc4dfb', 'message': 'DNM: More logs to reproduce bug 1728282\n\nPlease ignore\n\nChange-Id: I3d3c9ced93a0f8e1d5565f37ad2d9626650ca5d0\n'}, {'number': 6, 'created': '2020-03-17 14:32:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/189dd7b9a986d42a609b813e8e3624c6a40b3a0f', 'message': 'DNM: More logs to reproduce bug 1728282\n\nPlease ignore\n\nChange-Id: I3d3c9ced93a0f8e1d5565f37ad2d9626650ca5d0\n'}, {'number': 7, 'created': '2020-03-18 15:21:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/5c8786fc18391652031726f6b6efdb20dc0e3b9e', 'message': 'DNM: More logs to reproduce bug 1728282\n\nPlease ignore\n\nChange-Id: I3d3c9ced93a0f8e1d5565f37ad2d9626650ca5d0\n'}, {'number': 8, 'created': '2020-03-18 15:24:20.000000000', 'files': ['networking_ovn/ovsdb/ovsdb_monitor.py', 'zuul.d/project.yaml', 'zuul.d/networking-ovn-jobs.yaml'], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/10aad5b971deb4444bda56aff09cef7aff604c58', 'message': 'DNM: More logs to reproduce bug 1728282\n\nPlease ignore\n\nDepends-On: https://review.opendev.org/#/c/713675/\n\nChange-Id: I3d3c9ced93a0f8e1d5565f37ad2d9626650ca5d0\n'}]",0,711959,10aad5b971deb4444bda56aff09cef7aff604c58,18,2,8,11952,,,0,"DNM: More logs to reproduce bug 1728282

Please ignore

Depends-On: https://review.opendev.org/#/c/713675/

Change-Id: I3d3c9ced93a0f8e1d5565f37ad2d9626650ca5d0
",git fetch https://review.opendev.org/openstack/networking-ovn refs/changes/59/711959/5 && git format-patch -1 --stdout FETCH_HEAD,['networking_ovn/ovsdb/ovsdb_monitor.py'],1,dc0489009abb73a3d38c67a4958ff5009d1d1613,," try: LOG.info(""XXX flaviof PortBindingChassisEvent looking chassis %(chassis)s row %(row)s"", {'chassis': chassis, 'row': idlutils.row2str(row)}) LOG.info(""XXX flaviof PortBindingChassisEvent looking chassis %(chassis)s datapath %(datapath)s"", {'chassis': chassis, 'datapath': row.datapath}) router = row.datapath.external_ids.get('name', '').replace( 'neutron-', '') host = chassis[0].hostname LOG.info(""Router %(router)s is bound to host %(host)s"", {'router': router, 'host': host}) except Exception: LOG.exception('XXX flaviof PortBindingChassisEvent Exception') return"," router = row.datapath.external_ids.get('name', '').replace( 'neutron-', '') host = chassis[0].hostname LOG.info(""Router %(router)s is bound to host %(host)s"", {'router': router, 'host': host})",14,5
openstack%2Fovsdbapp~stable%2Ftrain~I59409f8ec1d8a59f94818abd3e3065c824df7b93,openstack/ovsdbapp,stable/train,I59409f8ec1d8a59f94818abd3e3065c824df7b93,WIP Remove eventhandler thread,ABANDONED,2020-03-18 15:19:21.000000000,2020-03-23 19:16:12.000000000,,"[{'_account_id': 5756}, {'_account_id': 11952}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-18 15:19:21.000000000', 'files': ['ovsdbapp/backend/ovs_idl/transaction.py', 'ovsdbapp/event.py', 'ovsdbapp/backend/ovs_idl/connection.py', 'ovsdbapp/backend/ovs_idl/__init__.py', 'ovsdbapp/tests/functional/schema/ovn_southbound/test_impl_idl.py'], 'web_link': 'https://opendev.org/openstack/ovsdbapp/commit/d65001f4a3ff2bbf7ac74fc994acd49a454e764d', 'message': 'WIP Remove eventhandler thread\n\nFor now, just looking at how tests react to this.\n\nWe call Idl.run() from the Connection thread. Idl.run() asserts\n""not self.txn"" and within that run() call is where notify() is\ncalled. RowEventHandler\'s notify is called which queues the event\nto be processed in it\'s own processing thread. This means that\nwhen we get around to calling the event\'s run() in the other thread,\nwe could be processing a transaction, which could change what values\nwe access on the Row objects.\n\nThis removes the RowEventHandler thread which means that\nnotifications are handled in the Connection thread. Since some\nevents themselves create new transactions in their run() methods,\nwe can\'t run those inside Idl.run() without making Idl.run()\nrecursive and causing issues. So the notification is still queued\nlike it was in the previous version, and after we call Idl.run(),\nwe call RowEventHandler\'s notify_run() to dequeue the notification\nand process it.\n\nChange-Id: I59409f8ec1d8a59f94818abd3e3065c824df7b93\n(cherry picked from commit 1f5e751f441143f1486c2bc1b7d4b38e10938bc0)\n'}]",0,713675,d65001f4a3ff2bbf7ac74fc994acd49a454e764d,4,3,1,11952,,,0,"WIP Remove eventhandler thread

For now, just looking at how tests react to this.

We call Idl.run() from the Connection thread. Idl.run() asserts
""not self.txn"" and within that run() call is where notify() is
called. RowEventHandler's notify is called which queues the event
to be processed in it's own processing thread. This means that
when we get around to calling the event's run() in the other thread,
we could be processing a transaction, which could change what values
we access on the Row objects.

This removes the RowEventHandler thread which means that
notifications are handled in the Connection thread. Since some
events themselves create new transactions in their run() methods,
we can't run those inside Idl.run() without making Idl.run()
recursive and causing issues. So the notification is still queued
like it was in the previous version, and after we call Idl.run(),
we call RowEventHandler's notify_run() to dequeue the notification
and process it.

Change-Id: I59409f8ec1d8a59f94818abd3e3065c824df7b93
(cherry picked from commit 1f5e751f441143f1486c2bc1b7d4b38e10938bc0)
",git fetch https://review.opendev.org/openstack/ovsdbapp refs/changes/75/713675/1 && git format-patch -1 --stdout FETCH_HEAD,"['ovsdbapp/backend/ovs_idl/transaction.py', 'ovsdbapp/event.py', 'ovsdbapp/backend/ovs_idl/connection.py', 'ovsdbapp/backend/ovs_idl/__init__.py', 'ovsdbapp/tests/functional/schema/ovn_southbound/test_impl_idl.py']",5,d65001f4a3ff2bbf7ac74fc994acd49a454e764d,, self.api.notify_handler.watch_event(row_event),from ovsdbapp import event as ovsdb_event self.handler = ovsdb_event.RowEventHandler() self.api.idl.notify = self.handler.notify self.handler.watch_event(row_event),29,34
openstack%2Fnova~master~I22fb253616443766ef21692735b80faa284fa26d,openstack/nova,master,I22fb253616443766ef21692735b80faa284fa26d,Make serialize_args handle exception messages safely,MERGED,2020-03-12 14:35:48.000000000,2020-03-23 19:08:25.000000000,2020-03-23 19:04:52.000000000,"[{'_account_id': 4393}, {'_account_id': 6873}, {'_account_id': 9008}, {'_account_id': 9708}, {'_account_id': 10118}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26458}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-03-12 14:35:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/2dfa7f9d1ea3cfc485e030147768329bdd6254d8', 'message': 'Make serialize_args handle exception messages safely\n\nWe have recently move towards handling exceptions that may be viewed\nby API users in a way that results in either the exc.format_message()\ntext, or just the exception class name (without details) in order to\navoid leaking sensitive information out of the API. This makes the\nserialize_args decorator behave that way. Note that because for a\nremotable method, serialize_args gets called twice (once on the remote\nside before the call and again on the local side at replay), which\nmeans we need to gate that message-or-class-name behavior on whether\nor not we are seeing a true Exception object.\n\nChange-Id: I22fb253616443766ef21692735b80faa284fa26d\n'}, {'number': 2, 'created': '2020-03-17 09:20:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/2956f9538c4516f4368ddf0ee682f29ea13595e9', 'message': 'Make serialize_args handle exception messages safely\n\nWe have recently move towards handling exceptions that may be viewed\nby API users in a way that results in either the exc.format_message()\ntext, or just the exception class name (without details) in order to\navoid leaking sensitive information out of the API. This makes the\nserialize_args decorator behave that way. Note that because for a\nremotable method, serialize_args gets called twice (once on the remote\nside before the call and again on the local side at replay), which\nmeans we need to gate that message-or-class-name behavior on whether\nor not we are seeing a true Exception object.\n\nChange-Id: I22fb253616443766ef21692735b80faa284fa26d\n'}, {'number': 3, 'created': '2020-03-21 06:08:11.000000000', 'files': ['nova/objects/base.py', 'nova/tests/unit/objects/test_objects.py', 'nova/tests/unit/objects/test_instance_action.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/a9346e60333856ceb96acdcdd15a482b6255bb2d', 'message': 'Make serialize_args handle exception messages safely\n\nWe have recently move towards handling exceptions that may be viewed\nby API users in a way that results in either the exc.format_message()\ntext, or just the exception class name (without details) in order to\navoid leaking sensitive information out of the API. This makes the\nserialize_args decorator behave that way. Note that because for a\nremotable method, serialize_args gets called twice (once on the remote\nside before the call and again on the local side at replay), which\nmeans we need to gate that message-or-class-name behavior on whether\nor not we are seeing a true Exception object.\n\nChange-Id: I22fb253616443766ef21692735b80faa284fa26d\n'}]",8,712697,a9346e60333856ceb96acdcdd15a482b6255bb2d,40,14,3,4393,,,0,"Make serialize_args handle exception messages safely

We have recently move towards handling exceptions that may be viewed
by API users in a way that results in either the exc.format_message()
text, or just the exception class name (without details) in order to
avoid leaking sensitive information out of the API. This makes the
serialize_args decorator behave that way. Note that because for a
remotable method, serialize_args gets called twice (once on the remote
side before the call and again on the local side at replay), which
means we need to gate that message-or-class-name behavior on whether
or not we are seeing a true Exception object.

Change-Id: I22fb253616443766ef21692735b80faa284fa26d
",git fetch https://review.opendev.org/openstack/nova refs/changes/97/712697/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/objects/base.py', 'nova/tests/unit/objects/test_objects.py', 'nova/tests/unit/objects/test_instance_action.py']",3,2dfa7f9d1ea3cfc485e030147768329bdd6254d8,bp/action-event-fault-details," exc = exception.NotFound() exc_val=exc, exc_val=exc.format_message(),"," exc_val=mock.sentinel.exc_val, exc_val=str(mock.sentinel.exc_val),",33,6
openstack%2Fproject-config~master~Ib09c74314b705655e70a02fcc07316d0d080d533,openstack/project-config,master,Ib09c74314b705655e70a02fcc07316d0d080d533,Run gerrit image jobs on jeepyb patches,MERGED,2020-03-23 18:32:46.000000000,2020-03-23 19:02:31.000000000,2020-03-23 19:02:31.000000000,"[{'_account_id': 1}, {'_account_id': 4146}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 18:32:46.000000000', 'files': ['zuul.d/projects.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/bb51a1affc63cbb6ce7c77dfaec8be9d664ca2bf', 'message': ""Run gerrit image jobs on jeepyb patches\n\nWe're building gerrit container images with jeepyb built in. This\nmeans we should trigger new images to be built when we land jeepyb\npatches.\n\nOnce we have a manage-projects functional test in system-config\nwe can add that here too.\n\nWhile we're touching it - remove the pep8 job, we can\nset that in the project's .zuul.yaml.\n\nChange-Id: Ib09c74314b705655e70a02fcc07316d0d080d533\n""}]",0,714516,bb51a1affc63cbb6ce7c77dfaec8be9d664ca2bf,7,3,1,2,,,0,"Run gerrit image jobs on jeepyb patches

We're building gerrit container images with jeepyb built in. This
means we should trigger new images to be built when we land jeepyb
patches.

Once we have a manage-projects functional test in system-config
we can add that here too.

While we're touching it - remove the pep8 job, we can
set that in the project's .zuul.yaml.

Change-Id: Ib09c74314b705655e70a02fcc07316d0d080d533
",git fetch https://review.opendev.org/openstack/project-config refs/changes/16/714516/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/projects.yaml'],1,bb51a1affc63cbb6ce7c77dfaec8be9d664ca2bf,, - system-config-gerrit-images - opendev-buildset-registry - opendev-buildset-registry, - openstack-tox-pep8 - openstack-tox-pep8,3,2
openstack%2Freleases~master~I94755dd6a115b5541b87e04bb60b20f0068f5166,openstack/releases,master,I94755dd6a115b5541b87e04bb60b20f0068f5166,pymod2pkg: release 0.23,MERGED,2020-03-23 14:51:35.000000000,2020-03-23 18:55:40.000000000,2020-03-23 18:55:40.000000000,"[{'_account_id': 11904}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 14:51:35.000000000', 'files': ['deliverables/_independent/pymod2pkg.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/4537e6143b3afca8c95c356dc2dd12a5e2fb4da6', 'message': 'pymod2pkg: release 0.23\n\nThis drops python 2.x support, adds python 3.8 support and\nmakes it work with openSUSE tumbleweed again.\n\nChange-Id: I94755dd6a115b5541b87e04bb60b20f0068f5166\n'}]",0,714466,4537e6143b3afca8c95c356dc2dd12a5e2fb4da6,7,2,1,6593,,,0,"pymod2pkg: release 0.23

This drops python 2.x support, adds python 3.8 support and
makes it work with openSUSE tumbleweed again.

Change-Id: I94755dd6a115b5541b87e04bb60b20f0068f5166
",git fetch https://review.opendev.org/openstack/releases refs/changes/66/714466/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/_independent/pymod2pkg.yaml'],1,4537e6143b3afca8c95c356dc2dd12a5e2fb4da6,, - version: 0.23.0 projects: - repo: openstack/pymod2pkg hash: 037724dd88ba3a89e02ab040fc39ab2d93a117cd,,4,0
openstack%2Freleases~master~I2cc4dfb3ffa26eac45c14afa944ec826e25e8f50,openstack/releases,master,I2cc4dfb3ffa26eac45c14afa944ec826e25e8f50,zun: ussuri cycle highlights,MERGED,2020-03-22 23:14:29.000000000,2020-03-23 18:55:39.000000000,2020-03-23 18:55:39.000000000,"[{'_account_id': 308}, {'_account_id': 1736}, {'_account_id': 11904}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-03-22 23:14:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/82f359508e5a385e0b88a396a1f49b4f2bc3aa45', 'message': 'zun: ussuri cycle highlights\n\nChange-Id: I2cc4dfb3ffa26eac45c14afa944ec826e25e8f50\n'}, {'number': 2, 'created': '2020-03-23 09:49:18.000000000', 'files': ['deliverables/ussuri/zun.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/ab69e1dced66b8bbaec8cce516c77f7802ae9c97', 'message': 'zun: ussuri cycle highlights\n\nChange-Id: I2cc4dfb3ffa26eac45c14afa944ec826e25e8f50\n'}]",0,714338,ab69e1dced66b8bbaec8cce516c77f7802ae9c97,12,5,2,11536,,,0,"zun: ussuri cycle highlights

Change-Id: I2cc4dfb3ffa26eac45c14afa944ec826e25e8f50
",git fetch https://review.opendev.org/openstack/releases refs/changes/38/714338/2 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/ussuri/zun.yaml'],1,82f359508e5a385e0b88a396a1f49b4f2bc3aa45,,"cycle-highlights: - Starting from this release, Zun adds support for CRI-compatible runtime. Zun uses CRI runtime to realize the concept of capsule (pod). As a result, users can use Zun API to create pods in Kata container via a CRI runtime.",,5,0
openstack%2Freleases~master~I77ed584610268bf25d089931643cd196f6ded5b9,openstack/releases,master,I77ed584610268bf25d089931643cd196f6ded5b9,Liaison: Add hongbin's other email,MERGED,2020-03-23 09:48:40.000000000,2020-03-23 18:50:19.000000000,2020-03-23 18:50:19.000000000,"[{'_account_id': 1736}, {'_account_id': 11904}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-03-23 09:48:40.000000000', 'files': ['data/release_liaisons.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/c0bddbfe479b7d239c2e38a907e4b53389988ff5', 'message': ""Liaison: Add hongbin's other email\n\nHongbin has two Gerrit accounts (gmail.com and huawei.com),\nbut seems to use the gmail.com account more in Gerrit. Add\nit to the liaison file so that PTL-approval works.\n\nChange-Id: I77ed584610268bf25d089931643cd196f6ded5b9\n""}]",0,714406,c0bddbfe479b7d239c2e38a907e4b53389988ff5,8,4,1,308,,,0,"Liaison: Add hongbin's other email

Hongbin has two Gerrit accounts (gmail.com and huawei.com),
but seems to use the gmail.com account more in Gerrit. Add
it to the liaison file so that PTL-approval works.

Change-Id: I77ed584610268bf25d089931643cd196f6ded5b9
",git fetch https://review.opendev.org/openstack/releases refs/changes/06/714406/1 && git format-patch -1 --stdout FETCH_HEAD,['data/release_liaisons.yaml'],1,c0bddbfe479b7d239c2e38a907e4b53389988ff5,liaison-fix, - name: 'Hongbin Lu' irc: hongbin email: hongbin034@gmail.com,,3,0
openstack%2Fcyborg-specs~master~Iea0759e4615da2445f48842f3ba6b82e8bcc9899,openstack/cyborg-specs,master,Iea0759e4615da2445f48842f3ba6b82e8bcc9899,Change specs link and remove invalid setting,MERGED,2020-03-09 08:53:09.000000000,2020-03-23 18:45:07.000000000,2020-03-23 18:42:13.000000000,"[{'_account_id': 21672}, {'_account_id': 22348}, {'_account_id': 24872}, {'_account_id': 25738}, {'_account_id': 26458}, {'_account_id': 28748}]","[{'number': 1, 'created': '2020-03-09 08:53:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg-specs/commit/3aadedb813b4741358fa5114db7732916cb120d9', 'message': 'Chang specs link and remove invalid setting\n\nChange-Id: Iea0759e4615da2445f48842f3ba6b82e8bcc9899\n'}, {'number': 2, 'created': '2020-03-09 08:53:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg-specs/commit/0ba8237c0942048f34e1edb5e96406eb2d9d7ee5', 'message': 'Change specs link and remove invalid setting\n\nChange-Id: Iea0759e4615da2445f48842f3ba6b82e8bcc9899\n'}, {'number': 3, 'created': '2020-03-09 09:36:55.000000000', 'files': ['setup.cfg'], 'web_link': 'https://opendev.org/openstack/cyborg-specs/commit/00215f7bf0729fc4c7f47e6786256ff929eb64fc', 'message': 'Change specs link and remove invalid setting\n\nChange-Id: Iea0759e4615da2445f48842f3ba6b82e8bcc9899\n'}]",12,711877,00215f7bf0729fc4c7f47e6786256ff929eb64fc,18,6,3,26458,,,0,"Change specs link and remove invalid setting

Change-Id: Iea0759e4615da2445f48842f3ba6b82e8bcc9899
",git fetch https://review.opendev.org/openstack/cyborg-specs refs/changes/77/711877/2 && git format-patch -1 --stdout FETCH_HEAD,['setup.cfg'],1,3aadedb813b4741358fa5114db7732916cb120d9,,home-page = http://specs.openstack.org/openstack/nova-specs/[wheel] universal = 1,home-page = http://www.openstack.org/[files] packages = specs [build_sphinx] all-files = 1 source-dir = doc/source build-dir = doc/build [upload_sphinx] upload-dir = doc/build/html [compile_catalog] directory = specs/locale domain = specs [update_catalog] domain = specs output_dir = specs/locale input_file = specs/locale/specs.pot [extract_messages] keywords = _ gettext ngettext l_ lazy_gettext mapping_file = babel.cfg output_file = specs/locale/specs.pot,3,26
openstack%2Fpython-tripleoclient~master~Icdc6c22a4e9fde316ef1700a4ee484a06c9dfd7d,openstack/python-tripleoclient,master,Icdc6c22a4e9fde316ef1700a4ee484a06c9dfd7d,Use ansible playbooks for plan creation/update,MERGED,2020-03-12 08:07:31.000000000,2020-03-23 18:38:10.000000000,2020-03-23 18:34:11.000000000,"[{'_account_id': 3153}, {'_account_id': 7353}, {'_account_id': 8833}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25877}]","[{'number': 1, 'created': '2020-03-12 08:07:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/9f85ad185003f08694aa7aba86aa84b8ffc9a8dc', 'message': 'WIP Use ansible playbook for plan creation\n\nDepends-On: https://review.opendev.org/712605\nChange-Id: Icdc6c22a4e9fde316ef1700a4ee484a06c9dfd7d\n'}, {'number': 2, 'created': '2020-03-13 10:36:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/8cf12242f5d837000e352cf6380fc9cce0ecd157', 'message': 'WIP Use ansible playbook for plan creation/update\n\nDepends-On: https://review.opendev.org/712605\nDepends-On: https://review.opendev.org/712899\nChange-Id: Icdc6c22a4e9fde316ef1700a4ee484a06c9dfd7d\n'}, {'number': 3, 'created': '2020-03-19 03:14:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/a6cc113632a0ca16ba846ce29fe48df498ae1a0e', 'message': 'WIP Use ansible playbook for plan creation/update\n\nDepends-On: https://review.opendev.org/712605\nDepends-On: https://review.opendev.org/712899\nChange-Id: Icdc6c22a4e9fde316ef1700a4ee484a06c9dfd7d\n'}, {'number': 4, 'created': '2020-03-20 07:08:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/19fc0ac5849f6015d1f3257978f34c1ef0a05093', 'message': 'Use ansible playbooks for plan creation/update\n\nThis removes mistral usage from plan create and\nupdate cli commands by calling the new playbooks.\n\nDepends-On: https://review.opendev.org/712605\nDepends-On: https://review.opendev.org/712899\nChange-Id: Icdc6c22a4e9fde316ef1700a4ee484a06c9dfd7d\n'}, {'number': 5, 'created': '2020-03-22 04:38:52.000000000', 'files': ['tripleoclient/tests/v1/overcloud_deploy/test_overcloud_deploy.py', 'tripleoclient/tests/v1/test_overcloud_plan.py', 'tripleoclient/workflows/plan_management.py', 'tripleoclient/tests/workflows/test_plan_management.py', 'tripleoclient/v1/overcloud_plan.py'], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/4a55e5b9c2ce97143998b8010daeca590080e9c2', 'message': 'Use ansible playbooks for plan creation/update\n\nThis removes mistral usage from plan create and\nupdate cli commands by calling the new playbooks.\n\nDepends-On: https://review.opendev.org/712605\nDepends-On: https://review.opendev.org/712899\nChange-Id: Icdc6c22a4e9fde316ef1700a4ee484a06c9dfd7d\n'}]",0,712621,4a55e5b9c2ce97143998b8010daeca590080e9c2,28,6,5,8833,,,0,"Use ansible playbooks for plan creation/update

This removes mistral usage from plan create and
update cli commands by calling the new playbooks.

Depends-On: https://review.opendev.org/712605
Depends-On: https://review.opendev.org/712899
Change-Id: Icdc6c22a4e9fde316ef1700a4ee484a06c9dfd7d
",git fetch https://review.opendev.org/openstack/python-tripleoclient refs/changes/21/712621/1 && git format-patch -1 --stdout FETCH_HEAD,"['tripleoclient/tests/v1/overcloud_deploy/test_overcloud_deploy.py', 'tripleoclient/tests/v1/test_overcloud_plan.py', 'tripleoclient/workflows/plan_management.py', 'tripleoclient/tests/workflows/test_plan_management.py', 'tripleoclient/v1/overcloud_plan.py']",5,9f85ad185003f08694aa7aba86aa84b8ffc9a8dc,mistral_to_ansible," use_default_templates=use_default_templates, validate_stack=False)", use_default_templates=use_default_templates),34,7
openstack%2Fpaunch~stable%2Fstein~I3c6d0670e11d035287d12f4207489a13e0891943,openstack/paunch,stable/stein,I3c6d0670e11d035287d12f4207489a13e0891943,Cleanup containers in the same loop as they are created,MERGED,2020-03-05 10:37:32.000000000,2020-03-23 18:37:46.000000000,2020-03-23 18:34:09.000000000,"[{'_account_id': 3153}, {'_account_id': 6926}, {'_account_id': 8297}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 26343}]","[{'number': 1, 'created': '2020-03-05 10:37:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/paunch/commit/4a465c4e76dee5b949501c009b42cff13b03cc92', 'message': ""Cleanup containers in the same loop as they are created\n\nSplit delete_missing_and_updated() into 2 methods:\n\n  * delete_missing(), that will remove all containers installed on the\n    host but missing from the given config. This runs outside of the\n    loop, once.\n  * delete_updated(), that will remove a container installed on the host\n    (if present), that is part of the config, if config_data changed or\n    didn't exist. It runs within the create loop, so the downtime\n    between a container removal and creation should be shorter than\n    before.\n  * make delete_missing(), delete_updated() and rename_containers()\n    returning True, if any container has been touched by either. Use\n    that flag in order to keep the container_names contents always\n    actual.\n  * in order to make that cached container_names working and saving off\n    extra podman ps/inspect calls, rework it to return a list instead\n    of an iterator. There is no huge lists of containers, iterators buy\n    us nothing here, while podman CLI calls are the more expensive\n    thing and we optimize the latter instead.\n\n(cherry picked from commit eb3d3b75bb8bf300fb52390610a6a53ddbfaa48e)\nCo-Authored-By: Bogdan Dobrelya <bdobreli@redhat.com>\nChange-Id: I3c6d0670e11d035287d12f4207489a13e0891943\nCloses-Bug: #1862954\n""}, {'number': 2, 'created': '2020-03-05 11:07:46.000000000', 'files': ['paunch/runner.py', 'paunch/tests/test_builder_base.py', 'paunch/builder/base.py'], 'web_link': 'https://opendev.org/openstack/paunch/commit/fab6bd7c18ab447f9b7beca75fb00398534e0359', 'message': ""Cleanup containers in the same loop as they are created\n\nSplit delete_missing_and_updated() into 2 methods:\n\n  * delete_missing(), that will remove all containers installed on the\n    host but missing from the given config. This runs outside of the\n    loop, once.\n  * delete_updated(), that will remove a container installed on the host\n    (if present), that is part of the config, if config_data changed or\n    didn't exist. It runs within the create loop, so the downtime\n    between a container removal and creation should be shorter than\n    before.\n  * make delete_missing(), delete_updated() and rename_containers()\n    returning True, if any container has been touched by either. Use\n    that flag in order to keep the container_names contents always\n    actual.\n  * in order to make that cached container_names working and saving off\n    extra podman ps/inspect calls, rework it to return a list instead\n    of an iterator. There is no huge lists of containers, iterators buy\n    us nothing here, while podman CLI calls are the more expensive\n    thing and we optimize the latter instead.\n\n(cherry picked from commit eb3d3b75bb8bf300fb52390610a6a53ddbfaa48e)\nCo-Authored-By: Bogdan Dobrelya <bdobreli@redhat.com>\nChange-Id: I3c6d0670e11d035287d12f4207489a13e0891943\nCloses-Bug: #1862954\n""}]",0,711426,fab6bd7c18ab447f9b7beca75fb00398534e0359,15,6,2,6926,,,0,"Cleanup containers in the same loop as they are created

Split delete_missing_and_updated() into 2 methods:

  * delete_missing(), that will remove all containers installed on the
    host but missing from the given config. This runs outside of the
    loop, once.
  * delete_updated(), that will remove a container installed on the host
    (if present), that is part of the config, if config_data changed or
    didn't exist. It runs within the create loop, so the downtime
    between a container removal and creation should be shorter than
    before.
  * make delete_missing(), delete_updated() and rename_containers()
    returning True, if any container has been touched by either. Use
    that flag in order to keep the container_names contents always
    actual.
  * in order to make that cached container_names working and saving off
    extra podman ps/inspect calls, rework it to return a list instead
    of an iterator. There is no huge lists of containers, iterators buy
    us nothing here, while podman CLI calls are the more expensive
    thing and we optimize the latter instead.

(cherry picked from commit eb3d3b75bb8bf300fb52390610a6a53ddbfaa48e)
Co-Authored-By: Bogdan Dobrelya <bdobreli@redhat.com>
Change-Id: I3c6d0670e11d035287d12f4207489a13e0891943
Closes-Bug: #1862954
",git fetch https://review.opendev.org/openstack/paunch refs/changes/26/711426/1 && git format-patch -1 --stdout FETCH_HEAD,"['paunch/runner.py', 'paunch/builder/base.py', 'paunch/tests/test_builder_base.py']",3,4a465c4e76dee5b949501c009b42cff13b03cc92,," @mock.patch(""paunch.builder.base.BaseBuilder.delete_updated"", return_value=False) def test_apply(self, mock_delete_updated, mock_cpu): # container_names for delete_missing (twice by managed_by) ('', '', 0), ('''five five six six two two three-12345678 three''', '', 0), ('', '', 0), # stop five ('', '', 0), # rm five ('', '', 0), # stop six ('', '', 0), # rm six # container_names for rename_containers ('three-12345678 three', '', 0), ('', '', 0), # rename three # desired/container_names to be refreshed after delete/rename ('three three', '', 0), # renamed three already exists ('a\nb\nc', '', 0) # exec four # container_names for delete_missing # rm containers missing in config mock.call(['docker', 'stop', 'five'], mock.ANY), mock.call(['docker', 'rm', 'five'], mock.ANY), mock.call(['docker', 'stop', 'six'], mock.ANY), mock.call(['docker', 'rm', 'six'], mock.ANY), # container_names for rename # rename three from an ephemeral to the static name mock.call(['docker', 'rename', 'three-12345678', 'three'], mock.ANY), # container_names to be refreshed after delete/rename @mock.patch(""paunch.runner.BaseRunner.container_names"") @mock.patch(""paunch.runner.BaseRunner.discover_container_name"", return_value='one') def test_apply_idempotency(self, mock_dname, mock_cnames, mock_cpu): config = { # running with the same config and given an ephemeral name # not running yet # running, but with a different config 'one_ls': { 'command': ['one', 'ls', '-l', '/'] } # five is running but is not managed by us } # represents the state before and after renaming/removing things mock_cnames.side_effect = ( # delete_missing [['five', 'five'], ['one-12345678', 'one'], ['three', 'three']], # rename_containers [['one-12345678', 'one']], # refresh container_names/desired after del/rename [['one', 'one'], ['three', 'three']], # refresh container_names/desired after delete_updated [['one', 'one']] ) ('', '', 0), # ps for rename one # inspect one ('{""start_order"": 0, ""image"": ""centos:7""}', '', 0), # inspect three ('{""start_order"": 42, ""image"": ""centos:7""}', '', 0), # stop three, changed config data ('', '', 0), # rm three, changed config data ('', '', 0), ('Created three-12345678', '', 0), ('a\nb\nc', '', 0) # exec one 'Created three-12345678', # rename one from an ephemeral to the static name mock.call(['docker', 'rename', 'one-12345678', 'one'], mock.ANY), # check the renamed one, config hasn't changed 'one'], mock.ANY, False), # don't run one, its already running # rm three, changed config mock.call(['docker', 'inspect', '--type', 'container', '--format', '{{index .Config.Labels ""config_data""}}', 'three'], mock.ANY, False), mock.call(['docker', 'stop', 'three'], mock.ANY), mock.call(['docker', 'rm', 'three'], mock.ANY), # run three mock.call( ['docker', 'run', '--name', 'three-12345678', '--label', 'config_id=foo', '--label', 'container_name=three', '--label', 'managed_by=tester', '--label', 'config_data=%s' % json.dumps(config['three']), '--detach=true', '--cpuset-cpus=0,1,2,3', 'centos:7'], mock.ANY ), # FIXME(bogdando): shall exec ls in the renamed one! # Why discover_container_name is never called to get it as c_name? ['docker', 'exec', 'one-12345678', 'ls', '-l', '/'], mock.ANY"," def test_apply(self, mock_cpu): ('', '', 0), # ps for delete_missing_and_updated container_names ('', '', 0), # ps2 for delete_missing_and_updated container_names ('', '', 0), # ps for after delete_missing_and_updated renames ('', '', 0), # ps2 for after delete_missing_and_updated renames ('', '', 0), # ps to only create containers which don't exist ('', '', 0), # ps2 to only create containers which don't exist ('Created three-12345678', '', 0), ('a\nb\nc', '', 0) 'Created three-12345678', # ps for delete_missing_and_updated container_names # ps2 for delete_missing_and_updated container_names # ps for after delete_missing_and_updated renames # ps2 for after delete_missing_and_updated renames mock.call( ['docker', 'ps', '-a', '--filter', 'label=managed_by=paunch', '--format', '{{.Names}} {{.Label ""container_name""}}'], mock.ANY ), # ps to only create containers which don't exist # ps2 to only create containers which don't exist mock.call( ['docker', 'ps', '-a', '--filter', 'label=managed_by=paunch', '--filter', 'label=config_id=foo', '--format', '{{.Names}} {{.Label ""container_name""}}'], mock.ANY ), # run three mock.call( ['docker', 'run', '--name', 'three-12345678', '--label', 'config_id=foo', '--label', 'container_name=three', '--label', 'managed_by=tester', '--label', 'config_data=%s' % json.dumps(config['three']), '--detach=true', '--cpuset-cpus=0,1,2,3', 'centos:6'], mock.ANY ), def test_apply_idempotency(self, mock_cpu): config = { # not running yet # running, but with a different config # running with the same config 'four_ls': { 'command': ['four', 'ls', '-l', '/'] } } # ps for delete_missing_and_updated container_names ('''five five six six two-12345678 two three-12345678 three''', '', 0), # stop six ('', '', 0), # rm six ('', '', 0), # inspect two ('{""start_order"": 1, ""image"": ""centos:6""}', '', 0), # stop two, changed config data ('', '', 0), # rm two, changed config data ('', '', 0), # inspect three ('{""start_order"": 2, ""image"": ""centos:7""}', '', 0), # ps for after delete_missing_and_updated renames ('', '', 0), # ps2 for after delete_missing_and_updated renames ('', '', 0), # ps to only create containers which don't exist ('three-12345678 three', '', 0), ('Created one-12345678', '', 0), ('a\nb\nc', '', 0) 'Created one-12345678', # ps for delete_missing_and_updated container_names mock.call( ['docker', 'ps', '-a', '--filter', 'label=managed_by=tester', '--filter', 'label=config_id=foo', '--format', '{{.Names}} {{.Label ""container_name""}}'], mock.ANY ), mock.call(['docker', 'stop', 'six'], mock.ANY), mock.call(['docker', 'rm', 'six'], mock.ANY), # rm two, changed config 'two-12345678'], mock.ANY, False), mock.call(['docker', 'stop', 'two-12345678'], mock.ANY), mock.call(['docker', 'rm', 'two-12345678'], mock.ANY), # check three, config hasn't changed mock.call(['docker', 'inspect', '--type', 'container', '--format', '{{index .Config.Labels ""config_data""}}', 'three-12345678'], mock.ANY, False), # ps for after delete_missing_and_updated renames mock.call( ['docker', 'ps', '-a', '--filter', 'label=managed_by=tester', '--format', '{{.Names}} {{.Label ""container_name""}}'], mock.ANY ), # ps2 for after delete_missing_and_updated renames mock.call( ['docker', 'ps', '-a', '--filter', 'label=managed_by=paunch', '--format', '{{.Names}} {{.Label ""container_name""}}'], mock.ANY ), # ps to only create containers which don't exist mock.call( ['docker', 'ps', '-a', '--filter', 'label=managed_by=tester', '--filter', 'label=config_id=foo', '--format', '{{.Names}} {{.Label ""container_name""}}'], mock.ANY ), # run one mock.call( ['docker', 'run', '--name', 'one-12345678', '--label', 'config_id=foo', '--label', 'container_name=one', '--label', 'managed_by=tester', '--label', 'config_data=%s' % json.dumps(config['one']), '--detach=true', '--cpuset-cpus=0,1,2,3', 'centos:7'], mock.ANY ), # don't run three, its already running # execute within four ['docker', 'exec', 'four-12345678', 'ls', '-l', '/'], mock.ANY",151,154
openstack%2Ftripleo-common~stable%2Fstein~I2fc2072f0ab8b761a2b8079f746c3b91cacc8733,openstack/tripleo-common,stable/stein,I2fc2072f0ab8b761a2b8079f746c3b91cacc8733,Refactor registry request actions,MERGED,2020-03-20 01:24:14.000000000,2020-03-23 18:36:45.000000000,2020-03-23 18:34:13.000000000,"[{'_account_id': 3153}, {'_account_id': 6816}, {'_account_id': 11090}, {'_account_id': 11166}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-20 01:24:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/d2cea108ec3586485ad30ff823d96843d7a1ce32', 'message': 'Refactor registry request actions\n\nIn order to handle re-authentication at request time rather than\nletting tenancity trigger it, this change creates a session helper class\nthat wraps the various get/patch/push/put actions that we use in the\nimage uploader to perform a single re-autentication action when we get a\n401 from a registry.  Previously we would let the reauthentication occur\nwhen tenancity would retry various functions.  There are several\nfunctions which perform muiltple requests that could exceed the TTL for\nauthentication tokens. This leads to failures that could have been\nprevented if we re-auth at the time of the request rather than retrying\nthe entire action.\n\nChange-Id: I2fc2072f0ab8b761a2b8079f746c3b91cacc8733\nRelated-Bug: #1867981\n(cherry picked from commit 19dba3f385139ff1e37a9561d62b5f4d9eccedac)\n(cherry picked from commit 4ee02060ee9e742abc912c6508d0d9714aa13a72)\n'}, {'number': 2, 'created': '2020-03-20 13:37:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/7a27427be24e9224f9a6610fd40d2b3132c9f304', 'message': 'Refactor registry request actions\n\nIn order to handle re-authentication at request time rather than\nletting tenancity trigger it, this change creates a session helper class\nthat wraps the various get/patch/push/put actions that we use in the\nimage uploader to perform a single re-autentication action when we get a\n401 from a registry.  Previously we would let the reauthentication occur\nwhen tenancity would retry various functions.  There are several\nfunctions which perform muiltple requests that could exceed the TTL for\nauthentication tokens. This leads to failures that could have been\nprevented if we re-auth at the time of the request rather than retrying\nthe entire action.\n\nChange-Id: I2fc2072f0ab8b761a2b8079f746c3b91cacc8733\nRelated-Bug: #1867981\n(cherry picked from commit 19dba3f385139ff1e37a9561d62b5f4d9eccedac)\n(cherry picked from commit 4ee02060ee9e742abc912c6508d0d9714aa13a72)\n'}, {'number': 3, 'created': '2020-03-20 17:01:37.000000000', 'files': ['tripleo_common/tests/image/test_image_uploader.py', 'tripleo_common/image/image_uploader.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/b995380f604c3b5e839a6a49caaa3d0e100fb47a', 'message': 'Refactor registry request actions\n\nIn order to handle re-authentication at request time rather than\nletting tenancity trigger it, this change creates a session helper class\nthat wraps the various get/patch/push/put actions that we use in the\nimage uploader to perform a single re-autentication action when we get a\n401 from a registry.  Previously we would let the reauthentication occur\nwhen tenancity would retry various functions.  There are several\nfunctions which perform muiltple requests that could exceed the TTL for\nauthentication tokens. This leads to failures that could have been\nprevented if we re-auth at the time of the request rather than retrying\nthe entire action.\n\nChange-Id: I2fc2072f0ab8b761a2b8079f746c3b91cacc8733\nRelated-Bug: #1867981\n(cherry picked from commit 19dba3f385139ff1e37a9561d62b5f4d9eccedac)\n(cherry picked from commit 4ee02060ee9e742abc912c6508d0d9714aa13a72)\n'}]",0,714014,b995380f604c3b5e839a6a49caaa3d0e100fb47a,21,7,3,3153,,,0,"Refactor registry request actions

In order to handle re-authentication at request time rather than
letting tenancity trigger it, this change creates a session helper class
that wraps the various get/patch/push/put actions that we use in the
image uploader to perform a single re-autentication action when we get a
401 from a registry.  Previously we would let the reauthentication occur
when tenancity would retry various functions.  There are several
functions which perform muiltple requests that could exceed the TTL for
authentication tokens. This leads to failures that could have been
prevented if we re-auth at the time of the request rather than retrying
the entire action.

Change-Id: I2fc2072f0ab8b761a2b8079f746c3b91cacc8733
Related-Bug: #1867981
(cherry picked from commit 19dba3f385139ff1e37a9561d62b5f4d9eccedac)
(cherry picked from commit 4ee02060ee9e742abc912c6508d0d9714aa13a72)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/14/714014/3 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_common/tests/image/test_image_uploader.py', 'tripleo_common/image/image_uploader.py']",2,d2cea108ec3586485ad30ff823d96843d7a1ce32,bug/1867981,"class RegistrySessionHelper(object): """""" Class with various registry session helpers This class contains a bunch of static methods to be used when making session requests against a container registry. The methods are primarily used to handle authentication/reauthentication for the requests against registries that require auth. """""" @staticmethod def check_status(session, request, allow_reauth=True): """""" Check request status and trigger reauth This function can be used to check if we need to perform authentication for a container registry request because we've gotten a 401. """""" hash_request_id = hashlib.sha1(str(request.url).encode()) request_id = hash_request_id.hexdigest() text = getattr(request, 'text', 'unknown') reason = getattr(request, 'reason', 'unknown') status_code = getattr(request, 'status_code', None) headers = getattr(request, 'headers', {}) session_headers = getattr(session, 'headers', {}) if status_code >= 300: LOG.info( 'Non-2xx: id {}, status {}, reason {}, text {}'.format( request_id, status_code, reason, text ) ) if status_code == 401: LOG.warning( 'Failure: id {}, status {}, reason {} text {}'.format( request_id, status_code, reason, text ) ) LOG.debug( 'Request headers after 401: id {}, headers {}'.format( request_id, headers ) ) LOG.debug( 'Session headers after 401: id {}, headers {}'.format( request_id, session_headers ) ) www_auth = headers.get( 'www-authenticate', headers.get( 'Www-Authenticate' ) ) if www_auth: error = None # Handle docker.io shenanigans. docker.io will return 401 # for 403 and 404 but provide an error string. Other registries # like registry.redhat.io and quay.io do not do this. So if # we find an error string, check to see if we should reauth. do_reauth = allow_reauth if 'error=' in www_auth: error = re.search('error=""(.*?)""', www_auth).group(1) LOG.warning( 'Error detected in auth headers: error {}'.format( error ) ) do_reauth = (error == 'invalid_token' and allow_reauth) if do_reauth: if hasattr(session, 'reauthenticate'): reauth = int(session.headers.get('_TripleOReAuth', 0)) reauth += 1 session.headers['_TripleOReAuth'] = str(reauth) LOG.warning( 'Re-authenticating: id {}, count {}'.format( request_id, reauth ) ) session.reauthenticate(**session.auth_args) request.raise_for_status() @staticmethod def _action(action, request_session, *args, **kwargs): """""" Perform a session action and retry if auth fails This function dynamically performs a specific type of call using the provided session (get, patch, post, etc). It will attempt a single re-authentication if the initial request fails with a 401. """""" _action = getattr(request_session, action) try: req = _action(*args, **kwargs) RegistrySessionHelper.check_status(session=request_session, request=req) except requests.exceptions.HTTPError as e: if e.response.status_code == 401: req = _action(*args, **kwargs) RegistrySessionHelper.check_status(session=request_session, request=req) else: raise return req @staticmethod def get(request_session, *args, **kwargs): """""" Perform a get and retry if auth fails This function is designed to be used when we perform a get to an authenticated source. This function will attempt a single re-authentication request if the first one fails. """""" return RegistrySessionHelper._action('get', request_session, *args, **kwargs) @staticmethod def patch(request_session, *args, **kwargs): """""" Perform a patch and retry if auth fails This function is designed to be used when we perform a path to an authenticated source. This function will attempt a single re-authentication request if the first one fails. """""" return RegistrySessionHelper._action('patch', request_session, *args, **kwargs) @staticmethod def post(request_session, *args, **kwargs): """""" Perform a post and retry if auth fails This function is designed to be used when we perform a post to an authenticated source. This function will attempt a single re-authentication request if the first one fails. """""" return RegistrySessionHelper._action('post', request_session, *args, **kwargs) @staticmethod def put(request_session, *args, **kwargs): """""" Perform a put and retry if auth fails This function is designed to be used when we perform a put to an authenticated source. This function will attempt a single re-authentication request if the first one fails. """""" return RegistrySessionHelper._action('put', request_session, *args, **kwargs) try: manifest_r = RegistrySessionHelper.get( session, manifest_url, headers=manifest_headers, timeout=30 ) except requests.exceptions.HTTPError as e: if e.response.status_code in (403, 404): raise ImageNotFoundException('Not found image: %s' % image_url.geturl()) else: raise tags_r = RegistrySessionHelper.get(session, tags_url, timeout=30) config_r = RegistrySessionHelper.get( session, config_url, headers=config_headers, timeout=30 ) r = RegistrySessionHelper.post(session, url, data=data, timeout=30) try: RegistrySessionHelper.post( session, upload_req_url, timeout=30 ) except requests.exceptions.HTTPError as e: if e.response.status_code in (501, 403, 404, 405): cls.export_registries.add(image_url.netloc) return True else: raise try: r = RegistrySessionHelper.get( session, url, headers=manifest_headers, timeout=30 ) except requests.exceptions.HTTPError as e: if e.response.status_code in (403, 404): raise ImageNotFoundException('Not found image: %s' % url) else: raise r = RegistrySessionHelper.post( session, upload_req_url, timeout=30 ) RegistrySessionHelper.check_status(session=session, request=blob_req) r = RegistrySessionHelper.get( source_session, source_config_url, timeout=30 ) r = RegistrySessionHelper.put( target_session, try: r = RegistrySessionHelper.put( target_session, manifest_url, timeout=30, data=manifest_str.encode('utf-8'), headers={ 'Content-Type': manifest_type } ) except requests.exceptions.HTTPError as e: if e.response.status_code == 400: LOG.error(cls._get_response_text(r)) raise ImageUploaderException('Pushing manifest failed') else: raise upload_resp = RegistrySessionHelper.patch( session, upload_resp = RegistrySessionHelper.put( session,"," @staticmethod def check_status(session, request, allow_reauth=True): hash_request_id = hashlib.sha1(str(request.url).encode()) request_id = hash_request_id.hexdigest() text = getattr(request, 'text', 'unknown') reason = getattr(request, 'reason', 'unknown') status_code = getattr(request, 'status_code', None) headers = getattr(request, 'headers', {}) session_headers = getattr(session, 'headers', {}) if status_code >= 300: LOG.info( 'Non-2xx: id {}, status {}, reason {}, text {}'.format( request_id, status_code, reason, text ) ) if status_code == 401: LOG.warning( 'Failure: id {}, status {}, reason {} text {}'.format( request_id, status_code, reason, text ) ) LOG.debug( 'Request headers after 401: id {}, headers {}'.format( request_id, headers ) ) LOG.debug( 'Session headers after 401: id {}, headers {}'.format( request_id, session_headers ) ) www_auth = headers.get( 'www-authenticate', headers.get( 'Www-Authenticate' ) ) if www_auth: error = None # Handle docker.io shenanigans. docker.io will return 401 # for 403 and 404 but provide an error string. Other registries # like registry.redhat.io and quay.io do not do this. So if # we find an error string, check to see if we should reauth. do_reauth = allow_reauth if 'error=' in www_auth: error = re.search('error=""(.*?)""', www_auth).group(1) LOG.warning( 'Error detected in auth headers: error {}'.format( error ) ) do_reauth = (error == 'invalid_token' and allow_reauth) if do_reauth: if hasattr(session, 'reauthenticate'): reauth = int(session.headers.get('_TripleOReAuth', 0)) reauth += 1 session.headers['_TripleOReAuth'] = str(reauth) LOG.warning( 'Re-authenticating: id {}, count {}'.format( request_id, reauth ) ) session.reauthenticate(**session.auth_args) request.raise_for_status() manifest_r = session.get(manifest_url, headers=manifest_headers, timeout=30) if manifest_r.status_code in (403, 404): raise ImageNotFoundException('Not found image: %s' % image_url.geturl()) cls.check_status(session=session, request=manifest_r) tags_r = session.get(tags_url, timeout=30) cls.check_status(session=session, request=tags_r) config_r = session.get(config_url, headers=config_headers, timeout=30) cls.check_status(session=session, request=config_r) r = session.post(url, data=data, timeout=30) cls.check_status(session=session, request=r) r = session.post(upload_req_url, timeout=30) if r.status_code in (501, 403, 404, 405): cls.export_registries.add(image_url.netloc) return True cls.check_status(session=session, request=r) r = session.get(url, headers=manifest_headers, timeout=30) if r.status_code in (403, 404): raise ImageNotFoundException('Not found image: %s' % url) cls.check_status(session=session, request=r) r = session.post(upload_req_url, timeout=30) cls.check_status(session=session, request=r) cls.check_status(session=session, request=blob_req) # Because the image layer fetching can exceed the auth # token lifetime, we may have a bad token here and don't want # to retry all of the layer fetching to just fetch the config # data. Let's try a single retry here (as check_status with # reauth by default). try: r = source_session.get(source_config_url, timeout=30) cls.check_status( session=source_session, request=r ) except requests.exceptions.HTTPError as e: LOG.debug('[%s] Config fetch failed, retrying: %s' % (image, source_config_url)) if e.response.status_code == 401: # check_status should have reauthed so try on more # time and raise again if we still have problems. r = source_session.get(source_config_url, timeout=30) cls.check_status( session=source_session, request=r ) else: raise r = target_session.put( cls.check_status(session=target_session, request=r) r = target_session.put( manifest_url, timeout=30, data=manifest_str.encode('utf-8'), headers={ 'Content-Type': manifest_type } ) if r.status_code == 400: LOG.error(cls._get_response_text(r)) raise ImageUploaderException('Pushing manifest failed') cls.check_status(session=target_session, request=r) upload_resp = session.patch( cls.check_status(session=session, request=upload_resp) upload_resp = session.put( cls.check_status(session=session, request=upload_resp)",400,189
openstack%2Fpython-tripleoclient~master~I799c7cad76fd03ee45d3624feb5a7210f9ce8bf5,openstack/python-tripleoclient,master,I799c7cad76fd03ee45d3624feb5a7210f9ce8bf5,Don't encode to utf8 data read from logfile,MERGED,2020-03-22 03:20:44.000000000,2020-03-23 18:36:02.000000000,2020-03-23 18:34:10.000000000,"[{'_account_id': 3153}, {'_account_id': 7353}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-22 03:20:44.000000000', 'files': ['tripleoclient/v1/tripleo_validator.py'], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/39a9e8aa15dc4853289b01e442f5e0dfaa57d1a4', 'message': ""Don't encode to utf8 data read from logfile\n\nIn python3 all strings are stored as unicode in an instance of the str type.\nEncoded strings on the other hand are represented as binary data in the form\nof instances of the bytes type.\n\nIn https://review.opendev.org/714280 we changed to not write in binary mode.\n\nStory: 2007449\n\nChange-Id: I799c7cad76fd03ee45d3624feb5a7210f9ce8bf5\n""}]",0,714281,39a9e8aa15dc4853289b01e442f5e0dfaa57d1a4,9,4,1,8833,,,0,"Don't encode to utf8 data read from logfile

In python3 all strings are stored as unicode in an instance of the str type.
Encoded strings on the other hand are represented as binary data in the form
of instances of the bytes type.

In https://review.opendev.org/714280 we changed to not write in binary mode.

Story: 2007449

Change-Id: I799c7cad76fd03ee45d3624feb5a7210f9ce8bf5
",git fetch https://review.opendev.org/openstack/python-tripleoclient refs/changes/81/714281/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleoclient/v1/tripleo_validator.py'],1,39a9e8aa15dc4853289b01e442f5e0dfaa57d1a4,," host = [x for x in i['play'].get('host').split(', ')] for ht in list(contents['stats'].keys()):"," host = [ x.encode('utf-8') for x in i['play'].get('host').split(', ') ] for h in list(contents['stats'].keys()): ht = h.encode('utf-8')",2,6
openstack%2Fopenstack-ansible-ops~master~I0e93bb48207eda662a97864c363a4de3378d2440,openstack/openstack-ansible-ops,master,I0e93bb48207eda662a97864c363a4de3378d2440,"Missing document start ""---""",MERGED,2020-03-13 08:49:53.000000000,2020-03-23 18:28:29.000000000,2020-03-23 18:23:43.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 29865}]","[{'number': 1, 'created': '2020-03-13 08:49:53.000000000', 'files': ['zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-ops/commit/573c82e9195d673753d1fdcdb1d2717e73b3e3f8', 'message': 'Missing document start ""---""\n\nChange-Id: I0e93bb48207eda662a97864c363a4de3378d2440\n'}]",0,712863,573c82e9195d673753d1fdcdb1d2717e73b3e3f8,8,3,1,23317,,,0,"Missing document start ""---""

Change-Id: I0e93bb48207eda662a97864c363a4de3378d2440
",git fetch https://review.opendev.org/openstack/openstack-ansible-ops refs/changes/63/712863/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/project.yaml'],1,573c82e9195d673753d1fdcdb1d2717e73b3e3f8,fix,---,,1,0
openstack%2Fsushy-cli~master~Id112292788946ad3dd308d9d1943e5ed89e96af5,openstack/sushy-cli,master,Id112292788946ad3dd308d9d1943e5ed89e96af5,Add system/manager/chassis inventory show command,MERGED,2020-02-05 16:22:42.000000000,2020-03-23 17:59:10.000000000,2020-03-23 17:57:44.000000000,"[{'_account_id': 10239}, {'_account_id': 15519}, {'_account_id': 22348}, {'_account_id': 23851}, {'_account_id': 26340}]","[{'number': 1, 'created': '2020-02-05 16:22:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/5fcb412a6267cbbade77d9fd6947d9f034852814', 'message': 'Add `sushycli system inventory show` command\n\nThis new command reports system inventory-related items\nsuch as system manufacturer, SKU, serial number etc.\n\nChange-Id: Id112292788946ad3dd308d9d1943e5ed89e96af5\nStory: 2006608\nTask: 36776\n'}, {'number': 2, 'created': '2020-02-07 11:09:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/1d485750c07163562c5b8526e460c07c1cd60b0a', 'message': 'Add system/manager/chassis inventory show command\n\nAdded three new commands to show inventory information:\n\n  sushycli system inventory show\n  sushycli manager inventory show\n  sushycli chassis inventory show\n\nThese new command report inventory-related items\nsuch as system manufacturer, SKU, serial number etc.\n\nChange-Id: Id112292788946ad3dd308d9d1943e5ed89e96af5\nStory: 2006608\nTask: 36776\n'}, {'number': 3, 'created': '2020-02-07 11:17:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/77bb24e91b77726b50bf49a66fc6a0750378bd25', 'message': 'Add system/manager/chassis inventory show command\n\nAdded three new commands to show inventory information:\n\n  sushycli system inventory show\n  sushycli manager inventory show\n  sushycli chassis inventory show\n\nThese new command report inventory-related items\nsuch as system manufacturer, SKU, serial number etc.\n\nChange-Id: Id112292788946ad3dd308d9d1943e5ed89e96af5\nStory: 2006608\nTask: 36776\n'}, {'number': 4, 'created': '2020-02-10 14:47:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/de19c76e84bee23156ae54f1facd93dbb44acf89', 'message': 'Add system/manager/chassis inventory show command\n\nAdded three new commands to show inventory information:\n\n  sushycli system inventory show\n  sushycli manager inventory show\n  sushycli chassis inventory show\n\nThese new command report inventory-related items\nsuch as system manufacturer, SKU, serial number etc.\n\nChange-Id: Id112292788946ad3dd308d9d1943e5ed89e96af5\nStory: 2006608\nTask: 36776\n'}, {'number': 5, 'created': '2020-02-11 10:05:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/7cd6ab3b29aad2563d1dfcff3533b82f23a3e977', 'message': 'Add system/manager/chassis inventory show command\n\nAdded three new commands to show inventory information:\n\n  sushycli system inventory show\n  sushycli manager inventory show\n  sushycli chassis inventory show\n\nThese new command report inventory-related items\nsuch as system manufacturer, SKU, serial number etc.\n\nChange-Id: Id112292788946ad3dd308d9d1943e5ed89e96af5\nStory: 2006608\nTask: 36776\n'}, {'number': 6, 'created': '2020-02-11 11:00:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/dfe42d5242ab4821a33d46f8fba5e0a9d0a4f818', 'message': 'Add system/manager/chassis inventory show command\n\nAdded three new commands to show inventory information:\n\n  sushycli system inventory show\n  sushycli manager inventory show\n  sushycli chassis inventory show\n\nThese new command report inventory-related items\nsuch as system manufacturer, SKU, serial number etc.\n\nDepends-On: https://review.opendev.org/#/c/706602\nChange-Id: Id112292788946ad3dd308d9d1943e5ed89e96af5\nStory: 2006608\nTask: 36776\n'}, {'number': 7, 'created': '2020-02-11 11:13:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/c1631e4b36188edb0907b57caa046972a72325e2', 'message': 'Add system/manager/chassis inventory show command\n\nAdded three new commands to show inventory information:\n\n  sushycli system inventory show\n  sushycli manager inventory show\n  sushycli chassis inventory show\n\nThese new command report inventory-related items\nsuch as system manufacturer, SKU, serial number etc.\n\nDepends-On: https://review.opendev.org/#/c/706602\nChange-Id: Id112292788946ad3dd308d9d1943e5ed89e96af5\nStory: 2006608\nTask: 36776\n'}, {'number': 8, 'created': '2020-02-13 05:38:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/1b57b30916a8119d42d5e7c6935cd7924ca5bc04', 'message': 'Add system/manager/chassis inventory show command\n\nAdded three new commands to show inventory information:\n\n  sushycli system inventory show\n  sushycli manager inventory show\n  sushycli chassis inventory show\n\nThese new command report inventory-related items\nsuch as system manufacturer, SKU, serial number etc.\n\nAlso, `--system-id` option made optional to leverage\nnew sushy feature of automatic ComputerSystem\nselection. Bumped sushy dependency to >= 3.1.0.\n\nDepends-On: https://review.opendev.org/#/c/706602\nChange-Id: Id112292788946ad3dd308d9d1943e5ed89e96af5\nStory: 2006608\nTask: 36776\n'}, {'number': 9, 'created': '2020-02-13 07:27:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/d46b2c8e4b31f25d74b03361161243e61ccbd39c', 'message': 'Add system/manager/chassis inventory show command\n\nAdded three new commands to show inventory information:\n\n  sushycli system inventory show\n  sushycli manager inventory show\n  sushycli chassis inventory show\n\nThese new command report inventory-related items\nsuch as system manufacturer, SKU, serial number etc.\n\nAlso, `--system-id` option made optional to leverage\nnew sushy feature of automatic ComputerSystem\nselection. Bumped sushy dependency to >= 3.1.0.\n\nDepends-On: https://review.opendev.org/#/c/706602\nChange-Id: Id112292788946ad3dd308d9d1943e5ed89e96af5\nStory: 2006608\nTask: 36776\n'}, {'number': 10, 'created': '2020-02-17 10:50:48.000000000', 'files': ['sushycli/base_manager.py', 'lower-constraints.txt', 'sushycli/system_inventory.py', 'sushycli/chassis_inventory.py', 'requirements.txt', 'sushycli/manager_inventory.py', 'sushycli/base_chassis.py', 'sushycli/tests/unit/cmd/test_sushycli.py', 'sushycli/base.py', 'sushycli/system_power.py', 'sushycli/utils.py', 'setup.cfg', 'sushycli/base_system.py'], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/e0d870989defe0fdeca31353f0b1d622728e2e6c', 'message': 'Add system/manager/chassis inventory show command\n\nAdded three new commands to show inventory information:\n\n  sushycli system inventory show\n  sushycli manager inventory show\n  sushycli chassis inventory show\n\nThese new command report inventory-related items\nsuch as system manufacturer, SKU, serial number etc.\n\nAlso, `--system-id` option made optional to leverage\nnew sushy feature of automatic ComputerSystem\nselection. Bumped sushy dependency to >= 3.1.0.\n\nDepends-On: https://review.opendev.org/#/c/706602\nChange-Id: Id112292788946ad3dd308d9d1943e5ed89e96af5\nStory: 2006608\nTask: 36776\n'}]",2,706005,e0d870989defe0fdeca31353f0b1d622728e2e6c,31,5,10,26340,,,0,"Add system/manager/chassis inventory show command

Added three new commands to show inventory information:

  sushycli system inventory show
  sushycli manager inventory show
  sushycli chassis inventory show

These new command report inventory-related items
such as system manufacturer, SKU, serial number etc.

Also, `--system-id` option made optional to leverage
new sushy feature of automatic ComputerSystem
selection. Bumped sushy dependency to >= 3.1.0.

Depends-On: https://review.opendev.org/#/c/706602
Change-Id: Id112292788946ad3dd308d9d1943e5ed89e96af5
Story: 2006608
Task: 36776
",git fetch https://review.opendev.org/openstack/sushy-cli refs/changes/05/706005/2 && git format-patch -1 --stdout FETCH_HEAD,"['sushycli/tests/unit/cmd/test_sushycli.py', 'setup.cfg', 'sushycli/system_inventory.py']",3,5fcb412a6267cbbade77d9fd6947d9f034852814,06-add-inventory-show,"# -*- coding: utf-8 -*- # Copyright 2010-2020 OpenStack Foundation # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from sushycli import base SYSTEM_ID_HELP = ( 'The canonical path to the ComputerSystem resource that the driver ' 'will interact with. It should include the root service, version and ' 'the unique resource path to a ComputerSystem. For example: ' '/redfish/v1/Systems/1' ) class SystemInventoryShow(base.BaseLister): """"""Show machine general information"""""" def get_parser(self, prog_name): """"""System information command parser. :param prog_name: name of the cliff command being executed :returns: an `argparse.ArgumentParser` instance """""" parser = super(SystemInventoryShow, self).get_parser(prog_name) parser.add_argument( '--system-id', required=True, help=SYSTEM_ID_HELP) return parser @staticmethod def _get_column(sys_inst, attr): attr = attr.lower().replace(' ', '_') value = getattr(sys_inst, attr) if isinstance(value, list): value = ', '.join(str(x) for x in value) return value def take_action(self, args): """"""Show system information command action. :param args: a namespace of command-line option-value pairs that come from the user :returns: CLI process exit code """""" root = super(SystemInventoryShow, self).take_action(args) sys_inst = root.get_system(args.system_id) columns = [ 'Identity', 'Name', 'Description', 'Manufacturer', 'Part Number', 'Serial Number', 'SKU', 'Asset Tag', 'OEM Vendors' ] return columns, [[self._get_column(sys_inst, column) for column in columns]] ",,114,0
openstack%2Fcinder~stable%2Fqueens~I068f1022263b2ad0d9c7f749f92d7780e8e3e23c,openstack/cinder,stable/queens,I068f1022263b2ad0d9c7f749f92d7780e8e3e23c,Solidfire: set verify_ssl before _create_cluster_reference,ABANDONED,2020-03-23 14:12:29.000000000,2020-03-23 17:55:08.000000000,,"[{'_account_id': 9732}, {'_account_id': 24921}, {'_account_id': 26537}]","[{'number': 1, 'created': '2020-03-23 14:12:29.000000000', 'files': ['cinder/volume/drivers/solidfire.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/5d6438d66beaa46e23e73ec2e514bb43f7ecc510', 'message': 'Solidfire: set verify_ssl before _create_cluster_reference\n\nThe backport to Queens of\n84b024b solidfire: Enable SSL with requests\n\nadded self.verify_ssl which is consumed in _issue_api_request().\n\nUnfortunately, __init__ calls _create_cluster_reference(),\nwhich calls _issue_api_request() before self.verify_ssl has\nbeen created.  This results in an exception being thrown.\n\nThis bug only appears to exist in the Queens backport.\n\nCloses-Bug: #1868565\nChange-Id: I068f1022263b2ad0d9c7f749f92d7780e8e3e23c\n'}]",0,714455,5d6438d66beaa46e23e73ec2e514bb43f7ecc510,7,3,1,4523,,,0,"Solidfire: set verify_ssl before _create_cluster_reference

The backport to Queens of
84b024b solidfire: Enable SSL with requests

added self.verify_ssl which is consumed in _issue_api_request().

Unfortunately, __init__ calls _create_cluster_reference(),
which calls _issue_api_request() before self.verify_ssl has
been created.  This results in an exception being thrown.

This bug only appears to exist in the Queens backport.

Closes-Bug: #1868565
Change-Id: I068f1022263b2ad0d9c7f749f92d7780e8e3e23c
",git fetch https://review.opendev.org/openstack/cinder refs/changes/55/714455/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/drivers/solidfire.py'],1,5d6438d66beaa46e23e73ec2e514bb43f7ecc510,, self.verify_ssl = self.configuration.driver_ssl_cert_verify, self.verify_ssl = self.configuration.driver_ssl_cert_verify,1,1
openstack%2Fmanila-tempest-plugin~master~I886a6d0c377f677fa12b837f67301f1a6ed94072,openstack/manila-tempest-plugin,master,I886a6d0c377f677fa12b837f67301f1a6ed94072,Skip invalid member share-export tasks,ABANDONED,2020-03-01 20:32:41.000000000,2020-03-23 17:33:44.000000000,,"[{'_account_id': 9003}, {'_account_id': 16643}, {'_account_id': 22348}, {'_account_id': 25243}]","[{'number': 1, 'created': '2020-03-01 20:32:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-tempest-plugin/commit/f976cc18f99c3b62f2b9c1aa7d1778d0a89ed36a', 'message': 'Skip invalid member share-export tests\n\nThese tests are passing only because we fail to enforce\npolicy forbidding listing share exports belonging to\nanother user.\n\nRelated-bug: #1655427\nChange-Id: I886a6d0c377f677fa12b837f67301f1a6ed94072\n'}, {'number': 2, 'created': '2020-03-01 20:42:47.000000000', 'files': ['manila_tempest_tests/tests/api/admin/test_export_locations.py'], 'web_link': 'https://opendev.org/openstack/manila-tempest-plugin/commit/8521aef448154285702f671a9b7c6a2de12e0a02', 'message': 'Skip invalid member share-export tasks\n\nRelated-bug: #1655427\nChange-Id: I886a6d0c377f677fa12b837f67301f1a6ed94072\n'}]",0,710660,8521aef448154285702f671a9b7c6a2de12e0a02,11,4,2,9003,,,0,"Skip invalid member share-export tasks

Related-bug: #1655427
Change-Id: I886a6d0c377f677fa12b837f67301f1a6ed94072
",git fetch https://review.opendev.org/openstack/manila-tempest-plugin refs/changes/60/710660/2 && git format-patch -1 --stdout FETCH_HEAD,['manila_tempest_tests/tests/api/admin/test_export_locations.py'],1,f976cc18f99c3b62f2b9c1aa7d1778d0a89ed36a,bug/1655427,"import testtools @testtools.skip(""Bug 1655427"") @testtools.skip(""Bug 1655427"")",,3,0
openstack%2Fmanila-tempest-plugin~master~I12a1df36e8d928c54c03ed644dd60557f349ddb3,openstack/manila-tempest-plugin,master,I12a1df36e8d928c54c03ed644dd60557f349ddb3,Fix export locations tests,MERGED,2020-03-20 18:13:56.000000000,2020-03-23 17:25:26.000000000,2020-03-23 17:25:26.000000000,"[{'_account_id': 8556}, {'_account_id': 9003}, {'_account_id': 22348}, {'_account_id': 25243}, {'_account_id': 29632}, {'_account_id': 30002}]","[{'number': 1, 'created': '2020-03-20 18:13:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-tempest-plugin/commit/0823b72d2199a5eae59fd565a07eb55dd0155df2', 'message': 'DNM - export locations test fixes\n\nChange-Id: I12a1df36e8d928c54c03ed644dd60557f349ddb3\n'}, {'number': 2, 'created': '2020-03-20 21:34:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-tempest-plugin/commit/70000458d09e30fce0c67e3f6643cb501085dad8', 'message': 'Fix export locations tests\n\nThe tempest tests\n- api.admin.test_export_locations.ExportLocationsTest#test_list_share_export_locations_by_member\n- api.admin.test_export_locations.ExportLocationsTest#test_get_share_export_location_by_member\n\nwere both written with the assumption that a user within a\nproject creates a share, and the share\'s export locations\nare available to other users in the project. In this specific\ncontext, the user creating the share has an ""admin"" role -\nbut that is just circumstantial. Any user with the ability\nto create shares in a project can do so, and expect that\nthose shares are accessible to other users by virtue of default\npolicy.\n\nHowever, tempest test projects each have only one user,\nand the private share in both these test cases is created\nwithin the original user\'s project, and is not supposed to\nbe accessible across projects. This behavior is called out in\nLP bug #1654598.\n\nSo, enhance the test infra to create a user within the same\nproject applying roles specified within tempest.conf and test\naccessibility with such a user.\n\nOnce bug #1654598 has been resolved, we can use the existing\ntest behavior of testing access via a different project as\nnegative test cases (See: https://review.opendev.org/710661/)\n\nRelated-Bug: #1654598\nPartial-Bug: #1655427\nChange-Id: I12a1df36e8d928c54c03ed644dd60557f349ddb3\nSigned-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>\n'}, {'number': 3, 'created': '2020-03-20 22:47:39.000000000', 'files': ['manila_tempest_tests/tests/api/base.py', 'manila_tempest_tests/tests/api/admin/test_export_locations.py'], 'web_link': 'https://opendev.org/openstack/manila-tempest-plugin/commit/c678e21b5e0d0aec2d014a40ceadcd573bac08f9', 'message': 'Fix export locations tests\n\nThe tempest tests\n- api.admin.test_export_locations.ExportLocationsTest#test_list_share_export_locations_by_member\n- api.admin.test_export_locations.ExportLocationsTest#test_get_share_export_location_by_member\n\nwere both written with the assumption that a user within a\nproject creates a share, and the share\'s export locations\nare available to other users in the project. In this specific\ncontext, the user creating the share has an ""admin"" role -\nbut that is just circumstantial. Any user with the ability\nto create shares in a project can do so, and expect that\nthose shares are accessible to other users by virtue of default\npolicy.\n\nHowever, tempest test projects each have only one user,\nand the private share in both these test cases is created\nwithin the original user\'s project, and is not supposed to\nbe accessible across projects. This behavior is called out in\nLP bug #1654598.\n\nSo, enhance the test infra to create a user within the same\nproject applying roles specified within tempest.conf and test\naccessibility with such a user.\n\nOnce bug #1654598 has been resolved, we can use the existing\ntest behavior of testing access via a different project as\nnegative test cases (See: https://review.opendev.org/710661/)\n\nRelated-Bug: #1654598\nPartial-Bug: #1655427\nChange-Id: I12a1df36e8d928c54c03ed644dd60557f349ddb3\nSigned-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>\n'}]",1,714181,c678e21b5e0d0aec2d014a40ceadcd573bac08f9,13,6,3,16643,,,0,"Fix export locations tests

The tempest tests
- api.admin.test_export_locations.ExportLocationsTest#test_list_share_export_locations_by_member
- api.admin.test_export_locations.ExportLocationsTest#test_get_share_export_location_by_member

were both written with the assumption that a user within a
project creates a share, and the share's export locations
are available to other users in the project. In this specific
context, the user creating the share has an ""admin"" role -
but that is just circumstantial. Any user with the ability
to create shares in a project can do so, and expect that
those shares are accessible to other users by virtue of default
policy.

However, tempest test projects each have only one user,
and the private share in both these test cases is created
within the original user's project, and is not supposed to
be accessible across projects. This behavior is called out in
LP bug #1654598.

So, enhance the test infra to create a user within the same
project applying roles specified within tempest.conf and test
accessibility with such a user.

Once bug #1654598 has been resolved, we can use the existing
test behavior of testing access via a different project as
negative test cases (See: https://review.opendev.org/710661/)

Related-Bug: #1654598
Partial-Bug: #1655427
Change-Id: I12a1df36e8d928c54c03ed644dd60557f349ddb3
Signed-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>
",git fetch https://review.opendev.org/openstack/manila-tempest-plugin refs/changes/81/714181/2 && git format-patch -1 --stdout FETCH_HEAD,"['manila_tempest_tests/tests/api/base.py', 'manila_tempest_tests/tests/api/admin/test_export_locations.py']",2,0823b72d2199a5eae59fd565a07eb55dd0155df2,bug/1654598, cls.member_client = cls.admin_project_regular_user_client[ 'shares_v2_client'], cls.member_client = cls.shares_v2_client,56,1
openstack%2Fpython-openstackclient~master~I1befae9622fc1ef72cd77cfd5792aad3fa231a6a,openstack/python-openstackclient,master,I1befae9622fc1ef72cd77cfd5792aad3fa231a6a,Change dockerhub password,MERGED,2020-03-23 13:09:18.000000000,2020-03-23 17:19:50.000000000,2020-03-23 17:15:37.000000000,"[{'_account_id': 10239}, {'_account_id': 11904}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 13:09:18.000000000', 'files': ['.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/42abde330eb830aff7e1b63919df2d53e607ce4d', 'message': 'Change dockerhub password\n\nChanged it dockerhub side.\n\nChange-Id: I1befae9622fc1ef72cd77cfd5792aad3fa231a6a\n'}]",0,714436,42abde330eb830aff7e1b63919df2d53e607ce4d,9,4,1,2,,,0,"Change dockerhub password

Changed it dockerhub side.

Change-Id: I1befae9622fc1ef72cd77cfd5792aad3fa231a6a
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/36/714436/1 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.yaml'],1,42abde330eb830aff7e1b63919df2d53e607ce4d,command_exception, - X666jS/g43U4ykLzuQSNhl9XOVpKViT1bMQb/YJgTHj8AzigONu3+4BrxItS7mqwp9AWp YtNHZ0Dju3piNw07ulnsOAYHpBcz+zFK5UzVMXzkHk5vu1W0WWYLVayU4HJKliAwPmNii J3itt196sQ6IhIe/Hamcm715rP9cQd50w32lvAV4RNXoc4Gq7ZzOIzyA80UEl6HY7jiR9 WYFI4PoCrmNxKKV8YAaH1OlJW71WF0O+77+oostKwFG35bxIniUl1ZinxOgjC2va8j5nT KKgVgQdjwjuMOXEA6iiLewAo39P+nL/81AheDGQNex6x1oJuVRklQmiznbQI3knTOeJIh ncRJuE9RfS0xuR5J45WOdKWVjIYpMkTXF+5Kn/pEtNelA2ADzaloGidNxEOvOD74vVf68 mtZ1kffQi6d7yq57m0ZJ8WwTEo5g8Z+tmS40U2vdZ0vN5c/keGJisKbOpwavloP9owize U9EgIAMIT6LE/fJWPGGugMR57z7gnCNiq970crTjWvsP1yFGvo/FQiUpplv5ni6C+s9jq 1rba4Ya0uxQa2r5zjFXNJ52zGoRVlpjSsfpEtvCmTzfF5p0fa2aSyDs8ZOjaiDCVSmTPn SE9rutyH0XO7CP9RFetxirfgSQ8ZDpUmXpfKaTGa0glIfjmmf4yyaaJRKOkubg=, - n764ECFMGlEna6S5ezNyvW5nmq8IZCBts/7QRzdo2tWLQMp/mNFoaQensd797Ra3NLaS7 NzCFJwGQvgWF6hJJIUfnf3h2+RecCwHahLN4r95RtjhAltARzHSZVDCeNXhiRJqSDS4Qc kmXR2NTNfz8kkWUhnWNVjaBhYdMk0LnqZjQCxiCaR+eNdmeecWlSuJXg0Uz6vObLNfGHO KxV3RiGc4J0AATYpYFEpC/SyPbBk0pJv6JWJb7nNIe0CEVW/7hkCfA6o3hQ5PNAswn5ZP sp/L8NdoRQe/fEWOm/9K2lZqQehEj6SKsk6jkx3Wiy5stqFcGfafrxWcfQoQWpKHY5TeP R9U0jJy+ipnhfnm0flBIBt9XHYykrTuFwp5QVdRhRRQDwg5RZBX+VmaBeSQlS2Z0oJmCX PXFQmUDfnoU5go0BALlXDdy1sYE+SrQH4Eydw+hgf2oDFh+EkdhXMFburxnU8B7t4ey14 EM1W4BdOBUgeI4fa/92BP6ipgUFvcJu19FYTdg4v7NZ/ApnwZnZ5KC4eYlDaKNQiPQUmW pFJrnxxYXeDgmiXij8mCCgo8KEGvPCKHAghZ14iBCaWqvniLXuOSkFI1gYU+llg4i2jAp ts3GfQqBe8jfROGPMexVuonqZxZBxvWmIgDsAaqAeJCtykS1xeIiAMtA8rNl40=,10,10
openstack%2Freleases~master~Icddb5be224931b45637003f6b83ccc3b87db0f71,openstack/releases,master,Icddb5be224931b45637003f6b83ccc3b87db0f71,Release horizon 18.1.0 (ussuri),MERGED,2020-03-23 08:46:23.000000000,2020-03-23 17:17:45.000000000,2020-03-23 17:17:45.000000000,"[{'_account_id': 841}, {'_account_id': 1736}, {'_account_id': 8313}, {'_account_id': 11904}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-03-23 08:46:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/eb83c1eff6a709a7f9b534f60b95df458aaaabb7', 'message': 'Release horizon 18.1.0 (ussuri)\n\nSince the last release, horizon changed dependencies from pyScss and\ndjango-pyscss to pyScss2 & django-pyscss2 as those projects are no more\nmaintained and that causes issues on py37 and above on various projects\nthat depends on horizon.\n\nChange-Id: Icddb5be224931b45637003f6b83ccc3b87db0f71\n'}, {'number': 2, 'created': '2020-03-23 15:28:53.000000000', 'files': ['deliverables/ussuri/horizon.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/21e3eeb8224d8cb92551ac607c46ad4234ca884b', 'message': 'Release horizon 18.1.0 (ussuri)\n\nSince the last release, horizon changed dependencies from pyScss and\ndjango-pyscss to pyScss2 & django-pyscss2 as those projects are no more\nmaintained and that causes issues on py37 and above on various projects\nthat depends on horizon.\n\nChange-Id: Icddb5be224931b45637003f6b83ccc3b87db0f71\n'}]",2,714398,21e3eeb8224d8cb92551ac607c46ad4234ca884b,16,6,2,8313,,,0,"Release horizon 18.1.0 (ussuri)

Since the last release, horizon changed dependencies from pyScss and
django-pyscss to pyScss2 & django-pyscss2 as those projects are no more
maintained and that causes issues on py37 and above on various projects
that depends on horizon.

Change-Id: Icddb5be224931b45637003f6b83ccc3b87db0f71
",git fetch https://review.opendev.org/openstack/releases refs/changes/98/714398/2 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/ussuri/horizon.yaml'],1,eb83c1eff6a709a7f9b534f60b95df458aaaabb7,horizon-ussuri, - version: 18.1.0 projects: - repo: openstack/horizon hash: 7acb68b5c2d4d8e355c80bab861c3cd18e799773,,4,0
openstack%2Fproject-config~master~I02b8c5ab3034f27d32a194eeb7a5259fe8069920,openstack/project-config,master,I02b8c5ab3034f27d32a194eeb7a5259fe8069920,Clean up infra gerritbot irc channel configs,MERGED,2019-04-04 17:27:37.000000000,2020-03-23 17:13:48.000000000,2020-03-23 17:13:48.000000000,"[{'_account_id': 4146}, {'_account_id': 6316}, {'_account_id': 6547}, {'_account_id': 13252}, {'_account_id': 22348}]","[{'number': 1, 'created': '2019-04-04 17:27:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/341b3d74faf46b5b45d35454c1f8ee88dd569380', 'message': ""Simplify and clean up infra gerritbot irc channel configs\n\nRemove projects we don't want to see events for like hacking and\nos-performance-tools. Switch to using regexes for project names in\nopenstack-infra/ and to get events for all branch names. This should\ngive us better more relevant events and simplify the config at the same\ntime.\n\nChange-Id: I02b8c5ab3034f27d32a194eeb7a5259fe8069920\n""}, {'number': 2, 'created': '2019-04-05 07:45:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/4471c2ff42ca3ba0e90d7f87f99bd8921204d740', 'message': ""Simplify and clean up infra gerritbot irc channel configs\n\nRemove projects we don't want to see events for like hacking and\nos-performance-tools. Switch to using regexes for project names in\nopenstack-infra/ and to get events for all branch names. This should\ngive us better more relevant events and simplify the config at the same\ntime.\n\nChange-Id: I02b8c5ab3034f27d32a194eeb7a5259fe8069920\n""}, {'number': 3, 'created': '2020-03-20 09:22:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/020b80e6e1a9aa2f50e406fee4ef41d99c99635d', 'message': ""Clean up infra gerritbot irc channel configs\n\nRemove projects we don't want to see events for like hacking and\nos-performance-tools. Switch to using regexes for project names in\nopenstack-infra and opendev to get events for all branch names.\n\nThis should give us better more relevant events and simplify the\nconfig at the same time.\n\nChange-Id: I02b8c5ab3034f27d32a194eeb7a5259fe8069920\n""}, {'number': 4, 'created': '2020-03-20 09:32:30.000000000', 'files': ['gerritbot/channels.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/c2422ac86fb248f455c01045d028c9f4bc0e30e1', 'message': ""Clean up infra gerritbot irc channel configs\n\nRemove projects we don't want to see events for like hacking and\nos-performance-tools. Switch to using regexes for project names in\nopenstack-infra and opendev to get events for all branch names.\n\nThis should give us better more relevant events and simplify the\nconfig at the same time.\n\nChange-Id: I02b8c5ab3034f27d32a194eeb7a5259fe8069920\n""}]",0,650083,c2422ac86fb248f455c01045d028c9f4bc0e30e1,17,5,4,4146,,,0,"Clean up infra gerritbot irc channel configs

Remove projects we don't want to see events for like hacking and
os-performance-tools. Switch to using regexes for project names in
openstack-infra and opendev to get events for all branch names.

This should give us better more relevant events and simplify the
config at the same time.

Change-Id: I02b8c5ab3034f27d32a194eeb7a5259fe8069920
",git fetch https://review.opendev.org/openstack/project-config refs/changes/83/650083/4 && git format-patch -1 --stdout FETCH_HEAD,['gerritbot/channels.yaml'],1,341b3d74faf46b5b45d35454c1f8ee88dd569380,cleanup-infra-channel-projects, - ^openstack-infra/.* - ^.* - ^openstack-infra/.* - ^.*, - openstack-dev/cookiecutter - openstack-dev/hacking - openstack-dev/openstack-nose - openstack-dev/specs-cookiecutter - openstack-infra/afsmon - openstack-infra/ansible-build-image - openstack-infra/ansible-role-puppet - openstack-infra/askbot-theme - openstack-infra/bindep - openstack-infra/bugdaystats - openstack-infra/ciwatch - openstack-infra/devstack-gate - openstack-infra/elastic-recheck - openstack-infra/gear - openstack-infra/gearman-plugin - openstack-infra/germqtt - openstack-infra/gerrit - openstack-infra/gerritbot - openstack-infra/gerritlib - openstack-infra/git-restack - openstack-infra/git-review - openstack-infra/gitdm - openstack-infra/glean - openstack-infra/grafyaml - openstack-infra/groups - openstack-infra/groups-static-pages - openstack-infra/infra-ansible - openstack-infra/infra-manual - openstack-infra/infra-specs - openstack-infra/irc-meetings - openstack-infra/jeepyb - openstack-infra/lodgeit - openstack-infra/log-classify - openstack-infra/log_processor - openstack-infra/logstash-filters - openstack-infra/lpmqtt - openstack-infra/meetbot - openstack-infra/mqtt_statsd - openstack-infra/nodepool - openstack-infra/nose-html-output - openstack-infra/odsreg - openstack-infra/opendev-website - openstack-infra/openstack-zuul-jobs - openstack-infra/openstack-zuul-roles - openstack-infra/openstackid - openstack-infra/openstackid-resources - openstack-infra/openstackweb - openstack-infra/os-loganalyze - openstack-infra/project-config - openstack-infra/project-config-example - openstack-infra/publications - openstack-infra/puppet-accessbot - openstack-infra/puppet-ansible - openstack-infra/puppet-apparmor - openstack-infra/puppet-askbot - openstack-infra/puppet-asterisk - openstack-infra/puppet-bandersnatch - openstack-infra/puppet-bugdaystats - openstack-infra/puppet-bup - openstack-infra/puppet-cgit - openstack-infra/puppet-ciwatch - openstack-infra/puppet-dashboard - openstack-infra/puppet-diskimage_builder - openstack-infra/puppet-drupal - openstack-infra/puppet-elastic_recheck - openstack-infra/puppet-elasticsearch - openstack-infra/puppet-ethercalc - openstack-infra/puppet-etherpad_lite - openstack-infra/puppet-exim - openstack-infra/puppet-germqtt - openstack-infra/puppet-gerrit - openstack-infra/puppet-gerritbot - openstack-infra/puppet-github - openstack-infra/puppet-grafyaml - openstack-infra/puppet-graphite - openstack-infra/puppet-haveged - openstack-infra/puppet-hound - openstack-infra/puppet-httpd - openstack-infra/puppet-infra-cookiecutter - openstack-infra/puppet-infracloud - openstack-infra/puppet-ipsilon - openstack-infra/puppet-iptables - openstack-infra/puppet-jeepyb - openstack-infra/puppet-jenkins - openstack-infra/puppet-kerberos - openstack-infra/puppet-kibana - openstack-infra/puppet-lodgeit - openstack-infra/puppet-log_processor - openstack-infra/puppet-logrotate - openstack-infra/puppet-logstash - openstack-infra/puppet-lpmqtt - openstack-infra/puppet-mailman - openstack-infra/puppet-mediawiki - openstack-infra/puppet-meetbot - openstack-infra/puppet-mosquitto - openstack-infra/puppet-mqtt_statsd - openstack-infra/puppet-mysql_backup - openstack-infra/puppet-nodepool - openstack-infra/puppet-openafs - openstack-infra/puppet-openstack_health - openstack-infra/puppet-openstack_infra_spec_helper - openstack-infra/puppet-openstackci - openstack-infra/puppet-openstackid - openstack-infra/puppet-os_client_config - openstack-infra/puppet-packagekit - openstack-infra/puppet-pgsql_backup - openstack-infra/puppet-phabricator - openstack-infra/puppet-pip - openstack-infra/puppet-planet - openstack-infra/puppet-project_config - openstack-infra/puppet-ptgbot - openstack-infra/puppet-puppet - openstack-infra/puppet-redis - openstack-infra/puppet-refstack - openstack-infra/puppet-reviewday - openstack-infra/puppet-simpleproxy - openstack-infra/puppet-snmpd - openstack-infra/puppet-ssh - openstack-infra/puppet-ssl_cert_check - openstack-infra/puppet-statusbot - openstack-infra/puppet-storyboard - openstack-infra/puppet-subunit2sql - openstack-infra/puppet-sudoers - openstack-infra/puppet-tmpreaper - openstack-infra/puppet-translation_checksite - openstack-infra/puppet-ulimit - openstack-infra/puppet-unattended_upgrades - openstack-infra/puppet-unbound - openstack-infra/puppet-user - openstack-infra/puppet-vcsrepo - openstack-infra/puppet-yum - openstack-infra/puppet-zanata - openstack-infra/puppet-zuul - openstack-infra/python-storyboardclient - openstack-infra/reviewday - openstack-infra/reviewstats - openstack-infra/statusbot - openstack-infra/storyboard - openstack-infra/storyboard-webclient - openstack-infra/subunit2sql - openstack-infra/system-config - openstack-infra/yaml2ical - openstack-infra/zmq-event-publisher - openstack-infra/zone-opendev.org - openstack-infra/zone-zuul-ci.org - openstack-infra/zuul - openstack-infra/zuul-base-jobs - openstack-infra/zuul-jobs - openstack-infra/zuul-preview - openstack-infra/zuul-sphinx - openstack-infra/zuul-website - openstack-infra/zuul-website-media - openstack/os-performance-tools - openstack/os-testr - master - debian/sid #zuul-packaging repo - feature/development #askbot-theme repo - feature/openid #openstackid repo - feature/gearman-zk-shim # nodepool - openstack/2.10.2 #gerrit repo - results #gitdm repo # Branches for publications repo - ci-automation - devstack-tutorial - gearman-plugin - overview - processing-ci-log-events - stackforge - sysadmin-codereview - template - using-your-own - zuul - openstack-infra/afsmon - openstack-infra/ansible-build-image - openstack-infra/ansible-role-puppet - openstack-infra/askbot-theme - openstack-infra/bindep - openstack-infra/bugdaystats - openstack-infra/ciwatch - openstack-infra/devstack-gate - openstack-infra/elastic-recheck - openstack-infra/gear - openstack-infra/gearman-plugin - openstack-infra/germqtt - openstack-infra/gerrit - openstack-infra/gerritbot - openstack-infra/gerritlib - openstack-infra/git-restack - openstack-infra/git-review - openstack-infra/gitdm - openstack-infra/glean - openstack-infra/grafyaml - openstack-infra/groups - openstack-infra/groups-static-pages - openstack-infra/infra-ansible - openstack-infra/infra-manual - openstack-infra/infra-specs - openstack-infra/irc-meetings - openstack-infra/jeepyb - openstack-infra/lodgeit - openstack-infra/log-classify - openstack-infra/log_processor - openstack-infra/logstash-filters - openstack-infra/lpmqtt - openstack-infra/meetbot - openstack-infra/mqtt_statsd - openstack-infra/nodepool - openstack-infra/nose-html-output - openstack-infra/odsreg - openstack-infra/opendev-website - openstack-infra/openstack-zuul-jobs - openstack-infra/openstack-zuul-roles - openstack-infra/openstackid - openstack-infra/openstackid-resources - openstack-infra/openstackweb - openstack-infra/os-loganalyze - openstack-infra/project-config - openstack-infra/project-config-example - openstack-infra/publications - openstack-infra/puppet-accessbot - openstack-infra/puppet-ansible - openstack-infra/puppet-apparmor - openstack-infra/puppet-askbot - openstack-infra/puppet-asterisk - openstack-infra/puppet-bandersnatch - openstack-infra/puppet-bugdaystats - openstack-infra/puppet-bup - openstack-infra/puppet-cgit - openstack-infra/puppet-ciwatch - openstack-infra/puppet-dashboard - openstack-infra/puppet-diskimage_builder - openstack-infra/puppet-drupal - openstack-infra/puppet-elastic_recheck - openstack-infra/puppet-elasticsearch - openstack-infra/puppet-ethercalc - openstack-infra/puppet-etherpad_lite - openstack-infra/puppet-exim - openstack-infra/puppet-germqtt - openstack-infra/puppet-gerrit - openstack-infra/puppet-gerritbot - openstack-infra/puppet-github - openstack-infra/puppet-grafyaml - openstack-infra/puppet-graphite - openstack-infra/puppet-haveged - openstack-infra/puppet-hound - openstack-infra/puppet-httpd - openstack-infra/puppet-infra-cookiecutter - openstack-infra/puppet-infracloud - openstack-infra/puppet-ipsilon - openstack-infra/puppet-iptables - openstack-infra/puppet-jeepyb - openstack-infra/puppet-jenkins - openstack-infra/puppet-kerberos - openstack-infra/puppet-kibana - openstack-infra/puppet-lodgeit - openstack-infra/puppet-log_processor - openstack-infra/puppet-logrotate - openstack-infra/puppet-logstash - openstack-infra/puppet-lpmqtt - openstack-infra/puppet-mailman - openstack-infra/puppet-mediawiki - openstack-infra/puppet-meetbot - openstack-infra/puppet-mosquitto - openstack-infra/puppet-mqtt_statsd - openstack-infra/puppet-mysql_backup - openstack-infra/puppet-nodepool - openstack-infra/puppet-openafs - openstack-infra/puppet-openstack_health - openstack-infra/puppet-openstack_infra_spec_helper - openstack-infra/puppet-openstackci - openstack-infra/puppet-openstackid - openstack-infra/puppet-os_client_config - openstack-infra/puppet-packagekit - openstack-infra/puppet-pgsql_backup - openstack-infra/puppet-phabricator - openstack-infra/puppet-pip - openstack-infra/puppet-planet - openstack-infra/puppet-project_config - openstack-infra/puppet-ptgbot - openstack-infra/puppet-puppet - openstack-infra/puppet-redis - openstack-infra/puppet-refstack - openstack-infra/puppet-reviewday - openstack-infra/puppet-simpleproxy - openstack-infra/puppet-snmpd - openstack-infra/puppet-ssh - openstack-infra/puppet-ssl_cert_check - openstack-infra/puppet-statusbot - openstack-infra/puppet-storyboard - openstack-infra/puppet-subunit2sql - openstack-infra/puppet-sudoers - openstack-infra/puppet-tmpreaper - openstack-infra/puppet-translation_checksite - openstack-infra/puppet-ulimit - openstack-infra/puppet-unattended_upgrades - openstack-infra/puppet-unbound - openstack-infra/puppet-user - openstack-infra/puppet-vcsrepo - openstack-infra/puppet-yum - openstack-infra/puppet-zanata - openstack-infra/puppet-zuul - openstack-infra/python-storyboardclient - openstack-infra/reviewday - openstack-infra/reviewstats - openstack-infra/statusbot - openstack-infra/storyboard - openstack-infra/storyboard-webclient - openstack-infra/subunit2sql - openstack-infra/system-config - openstack-infra/yaml2ical - openstack-infra/zmq-event-publisher - openstack-infra/zone-opendev.org - openstack-infra/zone-zuul-ci.org - openstack-infra/zuul - openstack-infra/zuul-base-jobs - openstack-infra/zuul-jobs - openstack-infra/zuul-preview - openstack-infra/zuul-sphinx - openstack-infra/zuul-website - openstack-infra/zuul-website-media - master - debian/sid #zuul-packaging repo - feature/development #askbot-theme repo - feature/openid #openstackid repo - feature/gearman-zk-shim # nodepool - openstack/2.10.2 #gerrit repo - results #gitdm repo # Branches for publications repo - ci-automation - devstack-tutorial - gearman-plugin - overview - processing-ci-log-events - stackforge - sysadmin-codereview - template - using-your-own - zuul,4,338
openstack%2Freleases~master~I7823bd1683ba8e6a0397d04df324486613258d04,openstack/releases,master,I7823bd1683ba8e6a0397d04df324486613258d04,[kolla] Transition Rocky to EM,MERGED,2020-02-26 01:00:38.000000000,2020-03-23 16:54:12.000000000,2020-03-23 16:54:12.000000000,"[{'_account_id': 11904}, {'_account_id': 14826}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-02-26 01:00:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/63823ea0fba6b9c986a64a4cae90d619f2388fd6', 'message': '[kolla] Transition Rocky to EM\n\nThis transition the rocky branch to extended maintenance.\nChanges for bugfixes and things the team deems important are\nstill encouraged, but there will no longer be official releases\noff of the branch.\n\nPlease +1 if the team is ready for us to proceed with this\ntransition, or -1 if there are any final backports currently in\nflight that we should wait for. For the latter case, please\nupdate the patch with the new commit hash after doing a final\nrelease to get those changes out so we know to proceed with the\ntransition.\n\nChange-Id: I7823bd1683ba8e6a0397d04df324486613258d04\n'}, {'number': 2, 'created': '2020-03-23 16:25:04.000000000', 'files': ['deliverables/rocky/kolla-ansible.yaml', 'deliverables/rocky/kolla.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/9286c12bca6f5eefb2876c512eee9679cdf4b374', 'message': '[kolla] Transition Rocky to EM\n\nThis transition the rocky branch to extended maintenance.\nChanges for bugfixes and things the team deems important are\nstill encouraged, but there will no longer be official releases\noff of the branch.\n\nPlease +1 if the team is ready for us to proceed with this\ntransition, or -1 if there are any final backports currently in\nflight that we should wait for. For the latter case, please\nupdate the patch with the new commit hash after doing a final\nrelease to get those changes out so we know to proceed with the\ntransition.\n\nChange-Id: I7823bd1683ba8e6a0397d04df324486613258d04\n'}]",0,709893,9286c12bca6f5eefb2876c512eee9679cdf4b374,15,3,2,11904,,,0,"[kolla] Transition Rocky to EM

This transition the rocky branch to extended maintenance.
Changes for bugfixes and things the team deems important are
still encouraged, but there will no longer be official releases
off of the branch.

Please +1 if the team is ready for us to proceed with this
transition, or -1 if there are any final backports currently in
flight that we should wait for. For the latter case, please
update the patch with the new commit hash after doing a final
release to get those changes out so we know to proceed with the
transition.

Change-Id: I7823bd1683ba8e6a0397d04df324486613258d04
",git fetch https://review.opendev.org/openstack/releases refs/changes/93/709893/2 && git format-patch -1 --stdout FETCH_HEAD,"['deliverables/rocky/kolla-ansible.yaml', 'deliverables/rocky/kolla.yaml']",2,63823ea0fba6b9c986a64a4cae90d619f2388fd6,rocky-em, - version: rocky-em projects: - repo: openstack/kolla hash: e1c0cb16e9e6a6522587a65c093c20d1f3e41b54,,8,0
openstack%2Fcharm-ceph-osd~master~I1067a8cdd1e2b706db07f194eca6fb2efeccb817,openstack/charm-ceph-osd,master,I1067a8cdd1e2b706db07f194eca6fb2efeccb817,Maintain OSD state on upgrade,MERGED,2020-03-17 13:56:10.000000000,2020-03-23 16:46:46.000000000,2020-03-23 16:26:53.000000000,"[{'_account_id': 935}, {'_account_id': 11805}, {'_account_id': 20634}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 13:56:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ceph-osd/commit/db8af0166bb86e3c66d8b718899862db45608818', 'message': ""Maintain OSD state on upgrade\n\nEnsure each OSD reaches its pre-restart state before proceeding\nafter restart. This prevents the charm from finalizing the upgrade\nprior to OSDs recovering after upgrade. For example, if the state\nis 'active' prior to restart, then it must reach 'active' after\nrestart, at which point the upgrade will be allowed to complete.\n\nChange-Id: I1067a8cdd1e2b706db07f194eca6fb2efeccb817\nCloses-Bug: #1821028\n""}, {'number': 2, 'created': '2020-03-18 14:54:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ceph-osd/commit/9431c288d55375ef048f94f8d4eaea716e2f2129', 'message': ""Maintain OSD state on upgrade\n\nEnsure each OSD reaches its pre-restart state before proceeding\nafter restart. This prevents the charm from finalizing the upgrade\nprior to OSDs recovering after upgrade. For example, if the state\nis 'active' prior to restart, then it must reach 'active' after\nrestart, at which point the upgrade will be allowed to complete.\n\nChange-Id: I1067a8cdd1e2b706db07f194eca6fb2efeccb817\nCloses-Bug: #1821028\n""}, {'number': 3, 'created': '2020-03-18 15:01:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ceph-osd/commit/c10823bb3793f19653beac5fec68494ee0095771', 'message': ""Maintain OSD state on upgrade\n\nSync charms.ceph\n\nEnsure each OSD reaches its pre-restart state before proceeding\nafter restart. This prevents the charm from finalizing the upgrade\nprior to OSDs recovering after upgrade. For example, if the state\nis 'active' prior to restart, then it must reach 'active' after\nrestart, at which point the upgrade will be allowed to complete.\n\nChange-Id: I1067a8cdd1e2b706db07f194eca6fb2efeccb817\nCloses-Bug: #1821028\n""}, {'number': 4, 'created': '2020-03-19 02:08:03.000000000', 'files': ['lib/charms_ceph/utils.py', 'lib/charms_ceph/broker.py'], 'web_link': 'https://opendev.org/openstack/charm-ceph-osd/commit/cb0f757f185565bcec1179087be037d596e9885a', 'message': ""Maintain OSD state on upgrade\n\nSync charms.ceph\n\nEnsure each OSD reaches its pre-restart state before proceeding\nafter restart. This prevents the charm from finalizing the upgrade\nprior to OSDs recovering after upgrade. For example, if the state\nis 'active' prior to restart, then it must reach 'active' after\nrestart, at which point the upgrade will be allowed to complete.\n\nChange-Id: I1067a8cdd1e2b706db07f194eca6fb2efeccb817\nDepends-On: https://review.opendev.org/#/c/713743/\nCloses-Bug: #1821028\n""}]",2,713449,cb0f757f185565bcec1179087be037d596e9885a,20,5,4,11805,,,0,"Maintain OSD state on upgrade

Sync charms.ceph

Ensure each OSD reaches its pre-restart state before proceeding
after restart. This prevents the charm from finalizing the upgrade
prior to OSDs recovering after upgrade. For example, if the state
is 'active' prior to restart, then it must reach 'active' after
restart, at which point the upgrade will be allowed to complete.

Change-Id: I1067a8cdd1e2b706db07f194eca6fb2efeccb817
Depends-On: https://review.opendev.org/#/c/713743/
Closes-Bug: #1821028
",git fetch https://review.opendev.org/openstack/charm-ceph-osd refs/changes/49/713449/3 && git format-patch -1 --stdout FETCH_HEAD,['lib/charms_ceph/utils.py'],1,db8af0166bb86e3c66d8b718899862db45608818,bug/1821028,"from contextlib import contextmanager # TODO(coreycb): Need to maintain osd state for restarts here toodef get_osd_state(osd_goal_state=None): while True: for local_id in get_local_osd_ids(): asok = ""/var/run/ceph/ceph-osd.{}.asok"".format(local_id) cmd = [ 'ceph', 'daemon', asok, 'status' ] result = json.loads(str(subprocess .check_output(cmd) .decode('UTF-8'))) # if no goal state requested then just return current state if not osd_goal_state: return result['state'] # otherwise loop until state matches goal state if result['state'] == osd_goal_state: return result['state'] @contextmanager def maintain_osd_state(): osd_state = get_osd_state() try: yield finally: await get_osd_state(osd_goal_state=osd_state) with maintain_osd_state(osd_num): stop_osd(osd_num) disable_osd(osd_num) update_owner(osd_dir) enable_osd(osd_num)", stop_osd(osd_num) disable_osd(osd_num) update_owner(osd_dir) enable_osd(osd_num),37,4
openstack%2Frpm-packaging~master~I2e04fda3f60f418a5b8da89a4839a900ab3cfcf8,openstack/rpm-packaging,master,I2e04fda3f60f418a5b8da89a4839a900ab3cfcf8,Update ironic-lib to 4.1.0,MERGED,2020-03-19 14:34:43.000000000,2020-03-23 16:41:22.000000000,2020-03-23 16:41:22.000000000,"[{'_account_id': 6593}, {'_account_id': 7102}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 14:34:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/e86b7204903687fe80ca84f9d43a5f350ddad513', 'message': 'Update ironic-lib to 4.1.0\n\nChange-Id: I2e04fda3f60f418a5b8da89a4839a900ab3cfcf8\n'}, {'number': 2, 'created': '2020-03-21 11:02:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/b87ca33ecb561d64552221a9579a80202bcc448f', 'message': 'Update ironic-lib to 4.1.0\n\nChange-Id: I2e04fda3f60f418a5b8da89a4839a900ab3cfcf8\n'}, {'number': 3, 'created': '2020-03-23 09:11:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/6bc736726d75e412fa68285a89075af94b876274', 'message': 'Update ironic-lib to 4.1.0\n\nChange-Id: I2e04fda3f60f418a5b8da89a4839a900ab3cfcf8\n'}, {'number': 4, 'created': '2020-03-23 09:11:26.000000000', 'files': ['openstack/ironic-lib/ironic-lib.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/a021373f1b0d87cbe882238c529d138fcae5e99b', 'message': 'Update ironic-lib to 4.1.0\n\nChange-Id: I2e04fda3f60f418a5b8da89a4839a900ab3cfcf8\n'}]",0,713893,a021373f1b0d87cbe882238c529d138fcae5e99b,19,5,4,23851,,,0,"Update ironic-lib to 4.1.0

Change-Id: I2e04fda3f60f418a5b8da89a4839a900ab3cfcf8
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/93/713893/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/ironic-lib/ironic-lib.spec.j2'],1,e86b7204903687fe80ca84f9d43a5f350ddad513,ironic-lib-ver,{% set upstream_version = upstream_version('4.1.0') %}install -p -m 640 etc/ironic/rootwrap.d/*.filters %{buildroot}/%{_sysconfdir}/ironic/rootwrap.d/%dir %{_sysconfdir}/ironic/rootwrap.d %{_sysconfdir}/ironic/rootwrap.d/*.filters,{% set upstream_version = upstream_version('2.21.0') %}install -p -m 640 etc/rootwrap.d/*.filters %{buildroot}/%{_sysconfdir}/ironic/rootwrap.d%{_sysconfdir}/ironic/rootwrap.d,4,3
openstack%2Fhorizon~master~I50a50f75a1e70ccc183f598099f0e2f0fff8985a,openstack/horizon,master,I50a50f75a1e70ccc183f598099f0e2f0fff8985a,Tell reno to ignore the kilo branch,MERGED,2020-03-23 12:57:55.000000000,2020-03-23 16:38:54.000000000,2020-03-23 16:37:08.000000000,"[{'_account_id': 841}, {'_account_id': 1736}, {'_account_id': 8313}, {'_account_id': 11904}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-03-23 12:57:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/c2fc8415677ff2ec9d0802781b30d623816a982d', 'message': ""Tell reno to ignore the kilo branch\n\nWhen reno 3.x runs under setuptools, it scans all of the branches it can\nfind, including any that look like they're closed and have an -eol tag.\n\nThis patch is a copy of one similar in cinder repo:\nhttps://review.opendev.org/707495\n\nChange-Id: I50a50f75a1e70ccc183f598099f0e2f0fff8985a\n""}, {'number': 2, 'created': '2020-03-23 13:18:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/becb21a1d73a8e23d4a186b33e7b1510a24fbfc4', 'message': ""Tell reno to ignore the kilo branch\n\nWhen reno 3.x runs under setuptools, it scans all of the branches it can\nfind, including any that look like they're closed and have an -eol tag.\n\nThis patch is a copy of one similar in cinder repo:\nhttps://review.opendev.org/707495\n\nChange-Id: I50a50f75a1e70ccc183f598099f0e2f0fff8985a\n""}, {'number': 3, 'created': '2020-03-23 13:18:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/710a28f23511c82984ffc4f77d953502b85c17fe', 'message': ""Tell reno to ignore the kilo branch\n\nWhen reno 3.x runs under setuptools, it scans all of the branches it can\nfind, including any that look like they're closed and have an -eol tag.\n\nThis patch is a copy of one similar in cinder repo:\nhttps://review.opendev.org/707495\n\nChange-Id: I50a50f75a1e70ccc183f598099f0e2f0fff8985a\n""}, {'number': 4, 'created': '2020-03-23 13:19:46.000000000', 'files': ['.gitignore', 'reno.yaml'], 'web_link': 'https://opendev.org/openstack/horizon/commit/571eb23c79c8a231832cd6478447369320c2cac0', 'message': ""Tell reno to ignore the kilo branch\n\nWhen reno 3.x runs under setuptools, it scans all of the branches it can\nfind, including any that look like they're closed and have an -eol tag.\n\nThis patch is a copy of one similar in cinder repo:\nhttps://review.opendev.org/707495\n\nChange-Id: I50a50f75a1e70ccc183f598099f0e2f0fff8985a\n""}]",6,714433,571eb23c79c8a231832cd6478447369320c2cac0,18,6,4,8313,,,0,"Tell reno to ignore the kilo branch

When reno 3.x runs under setuptools, it scans all of the branches it can
find, including any that look like they're closed and have an -eol tag.

This patch is a copy of one similar in cinder repo:
https://review.opendev.org/707495

Change-Id: I50a50f75a1e70ccc183f598099f0e2f0fff8985a
",git fetch https://review.opendev.org/openstack/horizon refs/changes/33/714433/4 && git format-patch -1 --stdout FETCH_HEAD,"['.gitignore', 'reno.yaml']",2,c2fc8415677ff2ec9d0802781b30d623816a982d,ignore-kilo-in-release-notes,"--- # Ignore the kilo-eol tag because that branch does not work with reno # and contains no release notes. closed_branch_tag_re: ""(.+)(?<!kilo)-eol"" ",,10,0
openstack%2Freleases~master~I20722633fedf28a31320867cd7ab9732297c91a8,openstack/releases,master,I20722633fedf28a31320867cd7ab9732297c91a8,Update openstackdocstheme 2.0.1,MERGED,2020-03-23 13:38:03.000000000,2020-03-23 16:23:46.000000000,2020-03-23 16:23:46.000000000,"[{'_account_id': 308}, {'_account_id': 6928}, {'_account_id': 11904}, {'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-03-23 13:38:03.000000000', 'files': ['deliverables/_independent/openstackdocstheme.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/48cba5a70050ebc576e8f6eb2294a7b2a64fb063', 'message': 'Update openstackdocstheme 2.0.1\n\nMainly update links since infra-manual was updated.\n\nChange-Id: I20722633fedf28a31320867cd7ab9732297c91a8\n'}]",0,714442,48cba5a70050ebc576e8f6eb2294a7b2a64fb063,13,6,1,6547,,,0,"Update openstackdocstheme 2.0.1

Mainly update links since infra-manual was updated.

Change-Id: I20722633fedf28a31320867cd7ab9732297c91a8
",git fetch https://review.opendev.org/openstack/releases refs/changes/42/714442/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/_independent/openstackdocstheme.yaml'],1,48cba5a70050ebc576e8f6eb2294a7b2a64fb063,docstheme, - projects: - hash: 403c9bca39fe2562311a4268457080cce36f7b90 repo: openstack/openstackdocstheme version: 2.0.1,,4,0
openstack%2Fopenstack-helm-infra~master~I501de225e1226991db9c263cedf38397cda7b51f,openstack/openstack-helm-infra,master,I501de225e1226991db9c263cedf38397cda7b51f,Fix Kibana Selenium tests,MERGED,2020-03-11 16:01:50.000000000,2020-03-23 16:19:39.000000000,2020-03-23 16:18:10.000000000,"[{'_account_id': 7769}, {'_account_id': 8898}, {'_account_id': 12281}, {'_account_id': 17591}, {'_account_id': 20466}, {'_account_id': 21420}, {'_account_id': 22348}, {'_account_id': 22477}, {'_account_id': 23928}, {'_account_id': 30777}]","[{'number': 1, 'created': '2020-03-11 16:01:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/49f70ef096811a85e8a8c0424ca1187ecff8a86a', 'message': 'Fix Kibana Selenium tests\n\nXPath to expected element was changed after\nKibana upgrade, this commit changes XPath\naccording chnges in new Kibana.\n\nChange-Id: I501de225e1226991db9c263cedf38397cda7b51f\n'}, {'number': 2, 'created': '2020-03-12 09:49:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/ec1c5a39fc6dcf9f2f66997cdfc381120eb08891', 'message': 'Fix Kibana Selenium tests\n\nXPath to expected element was changed after\nKibana upgrade, this commit changes XPath\naccording chnges in new Kibana.\n\nChange-Id: I501de225e1226991db9c263cedf38397cda7b51f\n'}, {'number': 3, 'created': '2020-03-17 15:12:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/2e29aaa24a8e6ae8a41d03c4fea9ba8004d8c178', 'message': 'Fix Kibana Selenium tests\n\nXPath to expected element was changed after\nKibana upgrade, this commit changes XPath\naccording chnges in new Kibana.\n\nChange-Id: I501de225e1226991db9c263cedf38397cda7b51f\n'}, {'number': 4, 'created': '2020-03-17 15:33:44.000000000', 'files': ['tools/gate/selenium/kibanaSelenium.py', 'tools/deployment/common/kibana-selenium.sh'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/49b50d632bceb2e0ca18e1d7f9ace335148fe562', 'message': 'Fix Kibana Selenium tests\n\nXPath to expected element was changed after\nKibana upgrade, this commit changes XPath\naccording chnges in new Kibana.\n\nChange-Id: I501de225e1226991db9c263cedf38397cda7b51f\n'}]",0,712488,49b50d632bceb2e0ca18e1d7f9ace335148fe562,13,10,4,28735,,,0,"Fix Kibana Selenium tests

XPath to expected element was changed after
Kibana upgrade, this commit changes XPath
according chnges in new Kibana.

Change-Id: I501de225e1226991db9c263cedf38397cda7b51f
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/88/712488/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/gate/selenium/kibanaSelenium.py'],1,49f70ef096811a85e8a8c0424ca1187ecff8a86a,," (By.XPATH, '//*[@id=""kibana-body""]/div[2]/div/div/div/div[3]' 'discover-app/main/div[2]/div[2]/div/div[2]/section[2]'"," (By.XPATH, '//*[@id=""kibana-body""]/div[1]/div/div/div[3]/' 'discover-app/div/div[2]/div[2]/div/div[2]/div[2]/'",2,2
openstack%2Ftripleo-ansible~master~I3e1a3ec911ec8b0219e315fe43ec342ce3e11334,openstack/tripleo-ansible,master,I3e1a3ec911ec8b0219e315fe43ec342ce3e11334,Remove duplicate mock requirement from test-reqs,MERGED,2020-03-09 22:54:01.000000000,2020-03-23 16:16:05.000000000,2020-03-23 16:11:48.000000000,"[{'_account_id': 7353}, {'_account_id': 10969}, {'_account_id': 22348}, {'_account_id': 24162}]","[{'number': 1, 'created': '2020-03-09 22:54:01.000000000', 'files': ['test-requirements.txt'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/0867c0d5a48f1327be2f572054438936d2c83d4c', 'message': 'Remove duplicate mock requirement from test-reqs\n\nmock presents in molecule-requirements and pip fails to install\nfrom both files complaining about double requirement\nChange-Id: I3e1a3ec911ec8b0219e315fe43ec342ce3e11334\n'}]",0,711997,0867c0d5a48f1327be2f572054438936d2c83d4c,11,4,1,10969,,,0,"Remove duplicate mock requirement from test-reqs

mock presents in molecule-requirements and pip fails to install
from both files complaining about double requirement
Change-Id: I3e1a3ec911ec8b0219e315fe43ec342ce3e11334
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/97/711997/1 && git format-patch -1 --stdout FETCH_HEAD,['test-requirements.txt'],1,0867c0d5a48f1327be2f572054438936d2c83d4c,,,mock>=2.0.0 # BSD,0,1
openstack%2Ftripleo-ansible~master~I42a65e379981fe7d7879ae5061eafe32845887b7,openstack/tripleo-ansible,master,I42a65e379981fe7d7879ae5061eafe32845887b7,Add deploy plan playbook,MERGED,2020-03-18 12:37:50.000000000,2020-03-23 16:14:22.000000000,2020-03-23 16:11:47.000000000,"[{'_account_id': 3153}, {'_account_id': 7353}, {'_account_id': 8833}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-18 12:37:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/49d99d22f30c48f593d06d3db548bc2abef8933b', 'message': 'WIP Add deploy plan playbook\n\nNeeds some more work..\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 2, 'created': '2020-03-18 15:23:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/2dffcdc191b96268e4d593d9a0b45c9f5ecac9de', 'message': 'WIP Add deploy plan playbook\n\nNeeds some more work..\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 3, 'created': '2020-03-19 02:25:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/a52e04dec4735e8b152342e056a9403f393b687d', 'message': 'Add deploy plan playbook\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 4, 'created': '2020-03-19 05:33:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/951106f65d6cd1ba1ac46e09b7db9f70f86fb697', 'message': 'Add deploy plan playbook\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 5, 'created': '2020-03-19 19:35:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/5ec4ac8d83481d33b13b0d3fb440b4819c6f4caf', 'message': 'Add deploy plan playbook\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 6, 'created': '2020-03-19 21:52:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/531f0343a8619ad66108a8542ddb2e3ac98d1505', 'message': 'Add deploy plan playbook\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 7, 'created': '2020-03-20 02:52:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/6c79e9a5b7357edc922381dde1a00e816f14a2a2', 'message': 'Add deploy plan playbook\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 8, 'created': '2020-03-20 05:21:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/5e6c825c90e6f5f3b313ef21873834fe644005ca', 'message': 'Add deploy plan playbook\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 9, 'created': '2020-03-20 07:13:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/168f1fd60d24a4c3da0f2e116a60e56bab59d0e6', 'message': 'Add deploy plan playbook\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 10, 'created': '2020-03-20 10:49:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/430e64a86dfe2cb5c3fc57821c42e086694e6fd1', 'message': 'Add deploy plan playbook\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 11, 'created': '2020-03-20 13:47:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/2326d5b6d4e485e4f7b5d7d56c588752dcc91d1b', 'message': 'Add deploy plan playbook\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 12, 'created': '2020-03-20 13:53:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/4012372b431c7175933a3f7f62106cde84e381e3', 'message': 'Add deploy plan playbook\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 13, 'created': '2020-03-22 06:51:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/371b41c69c347eddabea214c07596238bd40530f', 'message': 'Add deploy plan playbook\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}, {'number': 14, 'created': '2020-03-22 07:13:13.000000000', 'files': ['tripleo_ansible/playbooks/cli-deploy-deployment-plan.yaml', 'tripleo_ansible/ansible_plugins/modules/tripleo_plan_deploy.py', 'doc/source/modules/modules-tripleo_plan_deploy.rst'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/91a2b34b1385ced942fcb6edc90cf567594e92e5', 'message': 'Add deploy plan playbook\n\nChange-Id: I42a65e379981fe7d7879ae5061eafe32845887b7\nDepends-On: https://review.opendev.org/713580\n'}]",5,713641,91a2b34b1385ced942fcb6edc90cf567594e92e5,43,5,14,8833,,,0,"Add deploy plan playbook

Change-Id: I42a65e379981fe7d7879ae5061eafe32845887b7
Depends-On: https://review.opendev.org/713580
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/41/713641/9 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_ansible/playbooks/cli-deploy-deployment-plan.yaml', 'tripleo_ansible/ansible_plugins/modules/tripleo_plan_deploy.py', 'doc/source/modules/modules-tripleo_plan_deploy.rst']",3,49d99d22f30c48f593d06d3db548bc2abef8933b,mistral_to_ansible,============================ Module - tripleo_plan_deploy ============================ This module provides for the following ansible plugin: * tripleo_plan_deploy .. ansibleautoplugin:: :module: tripleo_ansible/ansible_plugins/modules/tripleo_plan_deploy.py :documentation: true :examples: true ,,247,0
openstack%2Fproject-config~master~I3138bda523533bbbdc354d50c57179dca60d6d9c,openstack/project-config,master,I3138bda523533bbbdc354d50c57179dca60d6d9c,Cache CirrOS 0.5.1 for AArch64 too,MERGED,2020-03-23 15:49:50.000000000,2020-03-23 16:12:39.000000000,2020-03-23 16:12:38.000000000,"[{'_account_id': 4146}, {'_account_id': 6547}, {'_account_id': 22348}, {'_account_id': 24072}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-23 15:49:50.000000000', 'files': ['nodepool/elements/cache-devstack/source-repository-images'], 'web_link': 'https://opendev.org/openstack/project-config/commit/624eec932b9eda2f797695f77836664d660d52d5', 'message': ""Cache CirrOS 0.5.1 for AArch64 too\n\nOpenStack is getting tested on AArch64 CI nodes.\nLet's cache CirrOS for it as well.\n\nChange-Id: I3138bda523533bbbdc354d50c57179dca60d6d9c\n""}]",0,714481,624eec932b9eda2f797695f77836664d660d52d5,10,5,1,30491,,,0,"Cache CirrOS 0.5.1 for AArch64 too

OpenStack is getting tested on AArch64 CI nodes.
Let's cache CirrOS for it as well.

Change-Id: I3138bda523533bbbdc354d50c57179dca60d6d9c
",git fetch https://review.opendev.org/openstack/project-config refs/changes/81/714481/1 && git format-patch -1 --stdout FETCH_HEAD,['nodepool/elements/cache-devstack/source-repository-images'],1,624eec932b9eda2f797695f77836664d660d52d5,cache-cirros-0.5.1-aarch64,cirros-0.5.1-aarch64-disk.img file /opt/cache/files/cirros-0.5.1-aarch64-disk.img https://download.cirros-cloud.net/0.5.1/cirros-0.5.1-aarch64-disk.img,,1,0
openstack%2Fproject-config~master~I4e82a955b59dfdcebbcd1f1687466e546781d28b,openstack/project-config,master,I4e82a955b59dfdcebbcd1f1687466e546781d28b,Cache CirrOS 0.5.1,MERGED,2020-03-23 15:29:48.000000000,2020-03-23 16:12:37.000000000,2020-03-23 16:12:37.000000000,"[{'_account_id': 4146}, {'_account_id': 6547}, {'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 24072}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-23 15:29:48.000000000', 'files': ['nodepool/elements/cache-devstack/source-repository-images'], 'web_link': 'https://opendev.org/openstack/project-config/commit/dc685864310c2b7d008c46ec9f62ea0496989e8e', 'message': ""Cache CirrOS 0.5.1\n\nWe are moving towards CirrOS 0.5.1 usage in CI.\nDevStack's patch is pending [1] and Kolla Ansible merged [2].\n\n[1] https://review.opendev.org/711182\n[2] https://review.opendev.org/711492\n\nChange-Id: I4e82a955b59dfdcebbcd1f1687466e546781d28b\n""}]",0,714475,dc685864310c2b7d008c46ec9f62ea0496989e8e,11,6,1,30491,,,0,"Cache CirrOS 0.5.1

We are moving towards CirrOS 0.5.1 usage in CI.
DevStack's patch is pending [1] and Kolla Ansible merged [2].

[1] https://review.opendev.org/711182
[2] https://review.opendev.org/711492

Change-Id: I4e82a955b59dfdcebbcd1f1687466e546781d28b
",git fetch https://review.opendev.org/openstack/project-config refs/changes/75/714475/1 && git format-patch -1 --stdout FETCH_HEAD,['nodepool/elements/cache-devstack/source-repository-images'],1,dc685864310c2b7d008c46ec9f62ea0496989e8e,cache-cirros-0.5.1,cirros-0.5.1-x86_64-disk.img file /opt/cache/files/cirros-0.5.1-x86_64-disk.img https://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img,,1,0
openstack%2Frpm-packaging~stable%2Frocky~Id3a6d3044c12ea1f25041fde606478989dba03fd,openstack/rpm-packaging,stable/rocky,Id3a6d3044c12ea1f25041fde606478989dba03fd,Update to Rocky EM releases,MERGED,2020-03-18 12:23:34.000000000,2020-03-23 16:06:10.000000000,2020-03-23 16:06:10.000000000,"[{'_account_id': 6593}, {'_account_id': 7102}, {'_account_id': 13294}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-18 12:23:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/87c87ed3013addd94a38f797814399200e9cdbdd', 'message': 'Update to Rocky EM releases\n\nChange-Id: Id3a6d3044c12ea1f25041fde606478989dba03fd\n'}, {'number': 2, 'created': '2020-03-18 14:16:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/a30530c598433607b1d404f2b0967cca9baaa935', 'message': ""Update to Rocky EM releases\n\nIncludes a patch to python-keystoneclient to make\ntests work again in 2020 which wasn't released before Rocky\nwas closed for further releases.\n\nChange-Id: Id3a6d3044c12ea1f25041fde606478989dba03fd\n""}, {'number': 3, 'created': '2020-03-19 10:57:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/0f016afc10465b678e216f823c30510910c030cc', 'message': 'Update to Rocky EM releases\n\nChange-Id: Id3a6d3044c12ea1f25041fde606478989dba03fd\n'}, {'number': 4, 'created': '2020-03-20 19:45:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/3d5fd6b28ab81407896d13377fdd49c8df51864b', 'message': 'Update to Rocky EM releases\n\nChange-Id: Id3a6d3044c12ea1f25041fde606478989dba03fd\n'}, {'number': 5, 'created': '2020-03-21 10:23:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/5ca481806efb53014ca0be401a3d6f810a394c3a', 'message': 'Update to Rocky EM releases\n\nChange-Id: Id3a6d3044c12ea1f25041fde606478989dba03fd\n'}, {'number': 6, 'created': '2020-03-23 10:44:32.000000000', 'files': ['openstack/ironic-lib/ironic-lib.spec.j2', 'openstack/oslo.rootwrap/0001-Run-rootwrap-with-lower-fd-ulimit-by-default.patch', 'openstack/os-brick/os-brick.spec.j2', 'openstack/oslo.rootwrap/oslo.rootwrap.spec.j2', 'openstack/keystoneauth1/keystoneauth1.spec.j2', 'openstack/keystonemiddleware/keystonemiddleware.spec.j2', 'openstack/oslo.config/oslo.config.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/139f8c622060049ad6e7e78776530309e9f623fe', 'message': 'Update to Rocky EM releases\n\nChange-Id: Id3a6d3044c12ea1f25041fde606478989dba03fd\n'}]",1,713637,139f8c622060049ad6e7e78776530309e9f623fe,39,6,6,6593,,,0,"Update to Rocky EM releases

Change-Id: Id3a6d3044c12ea1f25041fde606478989dba03fd
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/37/713637/2 && git format-patch -1 --stdout FETCH_HEAD,"['openstack/ironic-lib/ironic-lib.spec.j2', 'openstack/os-brick/os-brick.spec.j2', 'openstack/python-watcherclient/python-watcherclient.spec.j2', 'openstack/python-octaviaclient/python-octaviaclient.spec.j2', 'openstack/python-openstackclient/python-openstackclient.spec.j2', 'openstack/osprofiler/osprofiler.spec.j2', 'openstack/python-cinderclient/python-cinderclient.spec.j2', 'openstack/python-novaclient/python-novaclient.spec.j2', 'openstack/oslo.rootwrap/oslo.rootwrap.spec.j2', 'openstack/python-swiftclient/python-swiftclient.spec.j2', 'openstack/keystoneauth1/keystoneauth1.spec.j2', 'openstack/keystonemiddleware/keystonemiddleware.spec.j2', 'openstack/sushy/sushy.spec.j2', 'openstack/oslo.config/oslo.config.spec.j2', 'openstack/ovsdbapp/ovsdbapp.spec.j2', 'openstack/python-ironicclient/python-ironicclient.spec.j2']",16,87c87ed3013addd94a38f797814399200e9cdbdd,,{% set upstream_version = upstream_version('2.5.4') %},{% set upstream_version = upstream_version('2.5.3') %},16,16
openstack%2Fbifrost~master~I63e64f4967a5264e0898f27bee4d2371271f02cd,openstack/bifrost,master,I63e64f4967a5264e0898f27bee4d2371271f02cd,Don't install python openwsman package,MERGED,2020-03-23 14:06:16.000000000,2020-03-23 15:55:53.000000000,2020-03-23 15:52:06.000000000,"[{'_account_id': 10239}, {'_account_id': 15519}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 14:06:16.000000000', 'files': ['playbooks/roles/bifrost-ironic-install/defaults/required_defaults_RedHat_family.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_Ubuntu_18.04.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_Debian_family.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_Suse_family.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_Fedora_26.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_Fedora_25.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_Fedora_27.yml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/5c63d4ab2b6c2e852418c90e629c44dd605ec5ea', 'message': ""Don't install python openwsman package\n\nChange-Id: I63e64f4967a5264e0898f27bee4d2371271f02cd\n""}]",0,714451,5c63d4ab2b6c2e852418c90e629c44dd605ec5ea,8,3,1,14826,,,0,"Don't install python openwsman package

Change-Id: I63e64f4967a5264e0898f27bee4d2371271f02cd
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/51/714451/1 && git format-patch -1 --stdout FETCH_HEAD,"['playbooks/roles/bifrost-ironic-install/defaults/required_defaults_RedHat_family.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_Ubuntu_18.04.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_Debian_family.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_Suse_family.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_Fedora_26.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_Fedora_25.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_Fedora_27.yml']",7,5c63d4ab2b6c2e852418c90e629c44dd605ec5ea,,, - openwsman-python,0,7
openstack%2Fpython-tripleoclient~master~Iadcfcd97e5f3d65de4179a706bc30f9c5e3aae41,openstack/python-tripleoclient,master,Iadcfcd97e5f3d65de4179a706bc30f9c5e3aae41,Fix order of parameters to isinstance(),MERGED,2020-03-22 02:53:43.000000000,2020-03-23 15:48:57.000000000,2020-03-23 15:47:10.000000000,"[{'_account_id': 3153}, {'_account_id': 11491}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-22 02:53:43.000000000', 'files': ['tripleoclient/utils.py'], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/e7693eb1aef49f027b0305ccee8253e235e78abb', 'message': 'Fix order of parameters to isinstance()\n\nRegression from https://review.opendev.org/704521\n\nChange-Id: Iadcfcd97e5f3d65de4179a706bc30f9c5e3aae41\nCloses-Bug: #1868416\n'}]",0,714279,e7693eb1aef49f027b0305ccee8253e235e78abb,10,4,1,8833,,,0,"Fix order of parameters to isinstance()

Regression from https://review.opendev.org/704521

Change-Id: Iadcfcd97e5f3d65de4179a706bc30f9c5e3aae41
Closes-Bug: #1868416
",git fetch https://review.opendev.org/openstack/python-tripleoclient refs/changes/79/714279/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleoclient/utils.py'],1,e7693eb1aef49f027b0305ccee8253e235e78abb,," if groups and isinstance(groups, dict):"," if groups and isinstance(dict, groups):",1,1
openstack%2Foslo.serialization~master~I834b2ebba94d5dfca121ed06ff5ba42336d1b97c,openstack/oslo.serialization,master,I834b2ebba94d5dfca121ed06ff5ba42336d1b97c,Add releasenote to deprecate the yamlutils module,MERGED,2020-02-18 15:28:39.000000000,2020-03-23 15:48:07.000000000,2020-03-23 15:45:58.000000000,"[{'_account_id': 6928}, {'_account_id': 11904}, {'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 28522}, {'_account_id': 31245}]","[{'number': 1, 'created': '2020-02-18 15:28:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.serialization/commit/c6a4cccc5017f962d5c4a79a8581f4b32a024085', 'message': 'Add releasenote to deprecate the yamlutils module\n\nWe missed to inform customers by adding release note, these changes do\nthat.\n\nChange-Id: I834b2ebba94d5dfca121ed06ff5ba42336d1b97c\n'}, {'number': 2, 'created': '2020-02-21 09:31:47.000000000', 'files': ['releasenotes/notes/deprecate-yamlutils-module-96eee55f7ae57382.yaml'], 'web_link': 'https://opendev.org/openstack/oslo.serialization/commit/f0c28ee96aea2628f3e79f82567bc5d527e365f4', 'message': 'Add releasenote to deprecate the yamlutils module\n\nWe missed to inform customers by adding release note, these changes do\nthat.\n\nChange-Id: I834b2ebba94d5dfca121ed06ff5ba42336d1b97c\n'}]",4,708417,f0c28ee96aea2628f3e79f82567bc5d527e365f4,13,6,2,28522,,,0,"Add releasenote to deprecate the yamlutils module

We missed to inform customers by adding release note, these changes do
that.

Change-Id: I834b2ebba94d5dfca121ed06ff5ba42336d1b97c
",git fetch https://review.opendev.org/openstack/oslo.serialization refs/changes/17/708417/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/notes/deprecate-yamlutils-module-96eee55f7ae57382.yaml'],1,c6a4cccc5017f962d5c4a79a8581f4b32a024085,reno-yamlutils,--- deprecations: - | The yamlutils module of oslo.serialization is deprecated in Ussuri and support will be dropped during the V cycle. ,,5,0
openstack%2Freleases~master~I699c1a3a2424df0ff6bf549e919218ed7628b3e1,openstack/releases,master,I699c1a3a2424df0ff6bf549e919218ed7628b3e1,Release kolla 7.1.1 and kolla-ansible 7.2.1 for Rocky,MERGED,2020-03-23 14:17:52.000000000,2020-03-23 15:47:09.000000000,2020-03-23 15:47:08.000000000,"[{'_account_id': 11904}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-03-23 14:17:52.000000000', 'files': ['deliverables/rocky/kolla-ansible.yaml', 'deliverables/rocky/kolla.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/e768897687c8ff919f904d73a7af350728c5c7de', 'message': 'Release kolla 7.1.1 and kolla-ansible 7.2.1 for Rocky\n\nChange-Id: I699c1a3a2424df0ff6bf549e919218ed7628b3e1\n'}]",0,714457,e768897687c8ff919f904d73a7af350728c5c7de,8,3,1,14826,,,0,"Release kolla 7.1.1 and kolla-ansible 7.2.1 for Rocky

Change-Id: I699c1a3a2424df0ff6bf549e919218ed7628b3e1
",git fetch https://review.opendev.org/openstack/releases refs/changes/57/714457/1 && git format-patch -1 --stdout FETCH_HEAD,"['deliverables/rocky/kolla-ansible.yaml', 'deliverables/rocky/kolla.yaml']",2,e768897687c8ff919f904d73a7af350728c5c7de,, - projects: - repo: openstack/kolla hash: 703dedbb7f34c2b7923847943ba88e08b6b69809 version: 7.1.1,,8,0
openstack%2Fsushy-cli~master~Ia2f7b4fe62a8a10b993843acdf31566fc5d02b94,openstack/sushy-cli,master,Ia2f7b4fe62a8a10b993843acdf31566fc5d02b94,"Update commands to generate docs, renos and pdf",MERGED,2020-02-10 13:34:10.000000000,2020-03-23 15:42:53.000000000,2020-03-23 15:40:15.000000000,"[{'_account_id': 10239}, {'_account_id': 15519}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-02-10 13:34:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/62bf50dfeb1a16ec9a4206df14938e1753cd4776', 'message': 'Update commands to generate docs, renos and pdf\n\nAlso add some forgotten stuff to gitignore.\n\nChange-Id: Ia2f7b4fe62a8a10b993843acdf31566fc5d02b94\n'}, {'number': 2, 'created': '2020-02-10 13:34:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/07454ae7e259a628d706dc556a64e7848799a81f', 'message': 'Update commands to generate docs, renos and pdf\n\nAlso add some forgotten stuff to gitignore.\n\nChange-Id: Ia2f7b4fe62a8a10b993843acdf31566fc5d02b94\n'}, {'number': 3, 'created': '2020-02-10 13:39:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/09b52683515c6924d3e6fc7645d3eb2fa6c438de', 'message': 'Update commands to generate docs, renos and pdf\n\nAlso add some forgotten stuff to gitignore.\n\nChange-Id: Ia2f7b4fe62a8a10b993843acdf31566fc5d02b94\n'}, {'number': 4, 'created': '2020-02-18 10:13:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/fe6c11a0f65cf14e38aed00003cfb1d796288f51', 'message': 'Update commands to generate docs, renos and pdf\n\nAlso add some forgotten stuff to gitignore.\n\nChange-Id: Ia2f7b4fe62a8a10b993843acdf31566fc5d02b94\n'}, {'number': 5, 'created': '2020-02-18 15:29:15.000000000', 'files': ['.gitignore', 'test-requirements.txt', 'doc/requirements.txt', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/sushy-cli/commit/206d33baed5ee06b92b4199c23ccf5938e75ec93', 'message': 'Update commands to generate docs, renos and pdf\n\nAlso add some forgotten stuff to gitignore.\n\nChange-Id: Ia2f7b4fe62a8a10b993843acdf31566fc5d02b94\n'}]",2,706826,206d33baed5ee06b92b4199c23ccf5938e75ec93,18,3,5,23851,,,0,"Update commands to generate docs, renos and pdf

Also add some forgotten stuff to gitignore.

Change-Id: Ia2f7b4fe62a8a10b993843acdf31566fc5d02b94
",git fetch https://review.opendev.org/openstack/sushy-cli refs/changes/26/706826/3 && git format-patch -1 --stdout FETCH_HEAD,"['.gitignore', 'test-requirements.txt', 'doc/requirements.txt', 'tox.ini']",4,62bf50dfeb1a16ec9a4206df14938e1753cd4776,modern-docs-command,deps = -c{env:UPPER_CONSTRAINTS_FILE:https://releases.openstack.org/constraints/upper/master} -r{toxinidir}/requirements.txt -r{toxinidir}/doc/requirements.txt commands = sphinx-build -b html -W doc/source doc/build/htmldeps = {[testenv:docs]deps} commands = sphinx-build -W -b latex doc/source doc/build/pdf make -C doc/build/pdfdeps = -c{env:UPPER_CONSTRAINTS_FILE:https://releases.openstack.org/constraints/upper/master} -r{toxinidir}/requirements.txt -r{toxinidir}/doc/requirements.txt,commands = python setup.py build_sphinxcommands = sphinx-build -b latex doc/source doc/build/pdf make -C doc/build/pdf,26,8
openstack%2Freleases~master~Ic15e15f14d3668cfd5f586bef2ede5bd1a46b15a,openstack/releases,master,Ic15e15f14d3668cfd5f586bef2ede5bd1a46b15a,Add Cycle Highlights to U Release Schedule,MERGED,2020-03-19 17:18:50.000000000,2020-03-23 15:32:14.000000000,2020-03-23 15:32:14.000000000,"[{'_account_id': 308}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-03-19 17:18:50.000000000', 'files': ['doc/source/ussuri/schedule.rst', 'doc/source/ussuri/schedule.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/5f68d5f0b88ebc95a9c9a138b6311b5a0309d261', 'message': 'Add Cycle Highlights to U Release Schedule\n\nChange-Id: Ic15e15f14d3668cfd5f586bef2ede5bd1a46b15a\n'}]",0,713940,5f68d5f0b88ebc95a9c9a138b6311b5a0309d261,7,3,1,16708,,,0,"Add Cycle Highlights to U Release Schedule

Change-Id: Ic15e15f14d3668cfd5f586bef2ede5bd1a46b15a
",git fetch https://review.opendev.org/openstack/releases refs/changes/40/713940/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/ussuri/schedule.rst', 'doc/source/ussuri/schedule.yaml']",2,5f68d5f0b88ebc95a9c9a138b6311b5a0309d261,add_cycle_highlights_to_u_schedule, - u_cycle-highlights,,14,0
openstack%2Fpython-manilaclient~master~I10cb6ea800908ebbe48eae7ba8c18680249ff1d2,openstack/python-manilaclient,master,I10cb6ea800908ebbe48eae7ba8c18680249ff1d2,Implement OSC share type commands,MERGED,2020-01-06 14:04:31.000000000,2020-03-23 15:24:35.000000000,2020-03-23 15:22:05.000000000,"[{'_account_id': 5575}, {'_account_id': 6413}, {'_account_id': 18058}, {'_account_id': 22348}, {'_account_id': 29632}, {'_account_id': 31213}]","[{'number': 1, 'created': '2020-01-06 14:04:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/7a3495a7fd1af0c762a842324fa4d5f33aee68e9', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 2, 'created': '2020-01-13 09:23:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/dcaafb96f0815ff8ac735e5113e3b0988dd2bca0', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 3, 'created': '2020-01-13 21:24:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/4b75e60a576459a84a81377047d007507b150fe6', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 4, 'created': '2020-01-13 21:40:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/cb36dbc8676aad5c29981aea4aff181d704215b1', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 5, 'created': '2020-01-15 10:55:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/c882ca612e7717b1f95c344cf1707ce724dc09c1', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 6, 'created': '2020-01-16 08:53:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/57f10b86e07e06352eb29b9078461a9adc69b21d', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 7, 'created': '2020-01-16 19:52:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/aae2a1fabe2a3a226d7c3d21f77921d5cb319298', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 8, 'created': '2020-01-19 15:29:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/2acfed268f4795a0189deeb40a3ee94061350cee', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 9, 'created': '2020-01-20 19:36:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/d5f9594a58dfc87430c76c74da92af61a0c1ba39', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 10, 'created': '2020-01-20 19:59:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/683e847288783de6a7309bb8acebc07b972892a7', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 11, 'created': '2020-01-21 15:02:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/aedb8e81ad8192aed82941aef3f8b6db3a66a367', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 12, 'created': '2020-01-30 15:01:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/8a8d3682bbd16a82da9c3f2fd26adfa429c2eb19', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 13, 'created': '2020-01-30 19:30:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/1bc3e57893c72b3e915a3bcf7d6f177b991e0aa8', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 14, 'created': '2020-02-06 16:24:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/b7a87401b6dd568bc3407a239494233797ff28ad', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 15, 'created': '2020-02-24 08:02:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/b3b30f09521eac5dc684d62b89efff4e3c7cadde', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 16, 'created': '2020-02-25 15:47:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/03371846731ea2ca4546533fd52ef139fade1101', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 17, 'created': '2020-02-28 09:27:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/4ce86b71df8cf287711c9b5a887bab5a544c1787', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 18, 'created': '2020-03-03 09:23:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/8d54ceee4906e1d8cee6d9a5c2eb790b25cc58a1', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}, {'number': 19, 'created': '2020-03-09 09:19:11.000000000', 'files': ['manilaclient/tests/unit/osc/v2/test_share_type.py', 'manilaclient/osc/utils.py', 'manilaclient/tests/unit/osc/v2/fakes.py', 'manilaclient/tests/unit/osc/v2/test_share_type_access.py', 'manilaclient/common/constants.py', 'manilaclient/osc/v2/share_types.py', 'setup.cfg', 'manilaclient/osc/v2/share_type_access.py'], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/e3652b9c1c36c825d07dce741802930ad0ec1a12', 'message': 'Implement OSC share type commands\n\nIn this patch we add openstack commands for:\nshare type create\nshare type delete\nshare type set\nshare type unset\nshare type list\nshare type show\nshare type access create\nshare type access list\nshare type access delete\n\nThese commands can be used to replace all “manila type-” commands.\n“openstack share type set” combines “manila type-key” with\n“manila type-update” commands and can be used to set name, description,\nvisibility and extra specs.\n\nChange-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2\nPartially-implements: bp openstack-client-support\n'}]",13,701229,e3652b9c1c36c825d07dce741802930ad0ec1a12,67,6,19,31213,,,0,"Implement OSC share type commands

In this patch we add openstack commands for:
share type create
share type delete
share type set
share type unset
share type list
share type show
share type access create
share type access list
share type access delete

These commands can be used to replace all “manila type-” commands.
“openstack share type set” combines “manila type-key” with
“manila type-update” commands and can be used to set name, description,
visibility and extra specs.

Change-Id: I10cb6ea800908ebbe48eae7ba8c18680249ff1d2
Partially-implements: bp openstack-client-support
",git fetch https://review.opendev.org/openstack/python-manilaclient refs/changes/29/701229/6 && git format-patch -1 --stdout FETCH_HEAD,"['manilaclient/tests/osc/unit/v2/test_share_type_access.py', 'manilaclient/tests/osc/unit/v2/test_share_type.py', 'manilaclient/osc/v2/share_types.py', 'setup.cfg', 'manilaclient/osc/v2/share_type_access.py', 'manilaclient/tests/osc/unit/v2/fakes.py']",6,7a3495a7fd1af0c762a842324fa4d5f33aee68e9,bp/openstack-client-support," self.share_types = mock.Mock() self.share_type_access = mock.Mock() def create_one_sharetype(attrs=None, methods=None): methods = methods or {} methods=methods, @staticmethod def create_share_types(attrs=None, count=2): """"""Create multiple fake share types. :param Dictionary attrs: A dictionary with all attributes :param Integer count: The number of share types to be faked :return: A list of FakeResource objects """""" share_types = [] for n in range(0, count): share_types.append(FakeShareType.create_one_sharetype(attrs)) return share_types @staticmethod def get_share_types(share_types=None, count=2): """"""Get an iterable MagicMock object with a list of faked types. If types list is provided, then initialize the Mock object with the list. Otherwise create one. :param List types: A list of FakeResource objects faking types :param Integer count: The number of types to be faked :return An iterable Mock object with side_effect set to a list of faked types """""" if share_types is None: share_types = FakeShareType.create_share_types(count) return mock.Mock(side_effect=share_types)", def create_one_sharetype(attrs=None):,1195,1
openstack%2Fbifrost~master~Id0192853a6e3d9be5f89be57eb8990cc5a5a9411,openstack/bifrost,master,Id0192853a6e3d9be5f89be57eb8990cc5a5a9411,[DNM] Test run inspector after ironic-api,ABANDONED,2020-03-18 10:10:58.000000000,2020-03-23 15:23:37.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2020-03-18 10:10:58.000000000', 'files': ['playbooks/roles/bifrost-ironic-install/tasks/start.yml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/178a7e9155d3d6e55f94fde5456ed5232844ed15', 'message': '[DNM] Test run inspector after ironic-api\n\nChange-Id: Id0192853a6e3d9be5f89be57eb8990cc5a5a9411\nDepdens-On: https://review.opendev.org/672179\n'}]",0,713608,178a7e9155d3d6e55f94fde5456ed5232844ed15,3,1,1,23851,,,0,"[DNM] Test run inspector after ironic-api

Change-Id: Id0192853a6e3d9be5f89be57eb8990cc5a5a9411
Depdens-On: https://review.opendev.org/672179
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/08/713608/1 && git format-patch -1 --stdout FETCH_HEAD,['playbooks/roles/bifrost-ironic-install/tasks/start.yml'],1,178a7e9155d3d6e55f94fde5456ed5232844ed15,inspector-after-ironic-api,"- name: ""start ironic-inspector"" include: inspector_start.yml when: enable_inspector | bool == true ","- name: ""start ironic-inspector"" include: inspector_start.yml when: enable_inspector | bool == true ",4,4
openstack%2Fvirtualbmc~master~I784e7cd9e0545a591150dc21a8232bd85672c21c,openstack/virtualbmc,master,I784e7cd9e0545a591150dc21a8232bd85672c21c,Fix stop command by using default SIGTERM handler in children,MERGED,2020-03-13 17:28:39.000000000,2020-03-23 15:05:50.000000000,2020-03-23 15:03:52.000000000,"[{'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 23851}, {'_account_id': 26340}]","[{'number': 1, 'created': '2020-03-13 17:28:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/virtualbmc/commit/2846dda3268d5635a8b305fbddd950752471805d', 'message': 'Fix stop command by killing children\n\nThe daemon child processes do not respond to a SIGTERM. This means that\nthey stay running indefinitely. Switch to a SIGKILL to ensure the child\nis killed.\n\nChange-Id: I784e7cd9e0545a591150dc21a8232bd85672c21c\nStory: 2003534\nTask: 24819\n'}, {'number': 2, 'created': '2020-03-20 11:15:00.000000000', 'files': ['virtualbmc/manager.py'], 'web_link': 'https://opendev.org/openstack/virtualbmc/commit/3449d74b4eb13019246ab507029fee3bc78a2ff9', 'message': 'Fix stop command by using default SIGTERM handler in children\n\nThe daemon child processes do not respond to a SIGTERM. This means that\nthey stay running indefinitely. This is because the manager process\ninstalls a SIGTERM handler to propagate the signal to children, and this\nhandler is inherited by the children.\n\nReturn children to the default handler for SIGTERM.\n\nChange-Id: I784e7cd9e0545a591150dc21a8232bd85672c21c\nStory: 2003534\nTask: 24819\n'}]",0,713038,3449d74b4eb13019246ab507029fee3bc78a2ff9,15,4,2,14826,,,0,"Fix stop command by using default SIGTERM handler in children

The daemon child processes do not respond to a SIGTERM. This means that
they stay running indefinitely. This is because the manager process
installs a SIGTERM handler to propagate the signal to children, and this
handler is inherited by the children.

Return children to the default handler for SIGTERM.

Change-Id: I784e7cd9e0545a591150dc21a8232bd85672c21c
Story: 2003534
Task: 24819
",git fetch https://review.opendev.org/openstack/virtualbmc refs/changes/38/713038/2 && git format-patch -1 --stdout FETCH_HEAD,['virtualbmc/manager.py'],1,2846dda3268d5635a8b305fbddd950752471805d,,"import signal # NOTE(mgoddard): The child processes do not respond to # a SIGTERM. Go straight in with a SIGKILL. os.kill(instance.pid, signal.SIGKILL)", instance.terminate(),4,1
openstack%2Freleases~master~I42bac529509dc1c1e9a7db352c5a616ac3927c8e,openstack/releases,master,I42bac529509dc1c1e9a7db352c5a616ac3927c8e,Release Rally 3.0.0,MERGED,2020-03-23 14:14:39.000000000,2020-03-23 14:57:17.000000000,2020-03-23 14:57:17.000000000,"[{'_account_id': 11904}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 14:14:39.000000000', 'files': ['deliverables/_independent/rally.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/2f33d6b8542b86b02ba118b985389d2d8142751b', 'message': 'Release Rally 3.0.0\n\nChange-Id: I42bac529509dc1c1e9a7db352c5a616ac3927c8e\n'}]",0,714456,2f33d6b8542b86b02ba118b985389d2d8142751b,7,2,1,9545,,,0,"Release Rally 3.0.0

Change-Id: I42bac529509dc1c1e9a7db352c5a616ac3927c8e
",git fetch https://review.opendev.org/openstack/releases refs/changes/56/714456/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/_independent/rally.yaml'],1,2f33d6b8542b86b02ba118b985389d2d8142751b,rally_3.0.0, - version: 3.0.0 projects: - repo: openstack/rally hash: 6b86848fc8517ca465cd0caf916168bd312f0a9b,,4,0
openstack%2Fpuppet-gnocchi~master~I6fb80bc63a9f7b370f9349681327a50d705d7514,openstack/puppet-gnocchi,master,I6fb80bc63a9f7b370f9349681327a50d705d7514,Deprecate gnocchi::database_connection,MERGED,2020-03-18 13:50:40.000000000,2020-03-23 14:55:12.000000000,2020-03-23 14:52:25.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 16137}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-18 13:50:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-gnocchi/commit/4a5d0ad16da92b266325faaf1a0a3b0c128ae7d9', 'message': ""Deprecate gnocchi::database_connection\n\nSince gnocchi::db class was introcued[1], we have 2 parameters,\ngnocchi::database_connection and gnocchi::db::database_connection,\nto configure the same database_connection parameter.\n\nLet's deprecate the one in gnocchi class, so that we can have\nonly one parameter to set the one parameter.\n\n[1] https://github.com/openstack/puppet-gnocchi/commit/4d64bd45a79005a170b2407a5053356f2d5e09fb\nChange-Id: I6fb80bc63a9f7b370f9349681327a50d705d7514\n""}, {'number': 2, 'created': '2020-03-18 13:51:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-gnocchi/commit/47fef2a7e5db59e6f2e271d2d06f36270d4486e6', 'message': ""Deprecate gnocchi::database_connection\n\nSince gnocchi::db class was introcued[1], we have 2 parameters,\ngnocchi::database_connection and gnocchi::db::database_connection,\nto configure the same database_connection parameter.\n\nLet's deprecate the one in gnocchi class, so that we can have\nonly one parameter to set the one parameter.\n\n[1] https://github.com/openstack/puppet-gnocchi/commit/4d64bd45a79005a170b2407a5053356f2d5e09fb\nChange-Id: I6fb80bc63a9f7b370f9349681327a50d705d7514\n""}, {'number': 3, 'created': '2020-03-18 13:52:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-gnocchi/commit/b5a205629c4e46f12e704dfeaa3029b5be9da839', 'message': ""Deprecate gnocchi::database_connection\n\nSince gnocchi::db class was introcued[1], we have 2 parameters,\ngnocchi::database_connection and gnocchi::db::database_connection,\nto configure the same database_connection parameter.\n\nLet's deprecate the one in gnocchi class, so that we can have\nonly one parameter to set the one parameter.\n\n[1] https://github.com/openstack/puppet-gnocchi/commit/4d64bd45a79005a170b2407a5053356f2d5e09fb\nChange-Id: I6fb80bc63a9f7b370f9349681327a50d705d7514\n""}, {'number': 4, 'created': '2020-03-22 13:03:39.000000000', 'files': ['manifests/init.pp', 'releasenotes/notes/deprecate-gnocchi-database_connection-65d47012e2bd0579.yaml'], 'web_link': 'https://opendev.org/openstack/puppet-gnocchi/commit/b15098e93bd6a3ee0a01f01c8038812303d4b4ab', 'message': ""Deprecate gnocchi::database_connection\n\nSince gnocchi::db class was introcued[1], we have 2 parameters,\ngnocchi::database_connection and gnocchi::db::database_connection,\nto configure the same database_connection parameter.\n\nLet's deprecate the one in gnocchi class, so that we can have\nonly one parameter to set the one parameter.\n\n[1] https://github.com/openstack/puppet-gnocchi/commit/4d64bd45a79005a170b2407a5053356f2d5e09fb\nChange-Id: I6fb80bc63a9f7b370f9349681327a50d705d7514\n""}]",0,713656,b15098e93bd6a3ee0a01f01c8038812303d4b4ab,16,4,4,9816,,,0,"Deprecate gnocchi::database_connection

Since gnocchi::db class was introcued[1], we have 2 parameters,
gnocchi::database_connection and gnocchi::db::database_connection,
to configure the same database_connection parameter.

Let's deprecate the one in gnocchi class, so that we can have
only one parameter to set the one parameter.

[1] https://github.com/openstack/puppet-gnocchi/commit/4d64bd45a79005a170b2407a5053356f2d5e09fb
Change-Id: I6fb80bc63a9f7b370f9349681327a50d705d7514
",git fetch https://review.opendev.org/openstack/puppet-gnocchi refs/changes/56/713656/4 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/init.pp', 'releasenotes/notes/deprecate-gnocchi-database_connection-65d47012e2bd0579.yaml']",2,4a5d0ad16da92b266325faaf1a0a3b0c128ae7d9,database_connection,--- deprecations: - | The gnocchi::database_connection was deperecated. Use the gnocchi::db::dabase_connection available parameter instead. ,,18,5
openstack%2Freleases~master~I73ded29fdbf1c5c6e0fa82d48b5fd5fadc401689,openstack/releases,master,I73ded29fdbf1c5c6e0fa82d48b5fd5fadc401689,Release neutron-tempest-plugin 1.0.0,MERGED,2020-03-23 09:44:21.000000000,2020-03-23 14:48:39.000000000,2020-03-23 14:48:39.000000000,"[{'_account_id': 11904}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 09:44:21.000000000', 'files': ['deliverables/ussuri/neutron-tempest-plugin.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/105d95a3db5df9e1b03506cf8f6c6cea0d83dc1d', 'message': 'Release neutron-tempest-plugin 1.0.0\n\nThis new version has dropped support for Python 2 and because\nof that there is major version bump.\nIt also has some new test cases and bug fixes which may be useful e.g.\nin downstream testing.\n\nChange-Id: I73ded29fdbf1c5c6e0fa82d48b5fd5fadc401689\n'}]",0,714405,105d95a3db5df9e1b03506cf8f6c6cea0d83dc1d,7,2,1,11975,,,0,"Release neutron-tempest-plugin 1.0.0

This new version has dropped support for Python 2 and because
of that there is major version bump.
It also has some new test cases and bug fixes which may be useful e.g.
in downstream testing.

Change-Id: I73ded29fdbf1c5c6e0fa82d48b5fd5fadc401689
",git fetch https://review.opendev.org/openstack/releases refs/changes/05/714405/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/ussuri/neutron-tempest-plugin.yaml'],1,105d95a3db5df9e1b03506cf8f6c6cea0d83dc1d,release-neutron-tempest-plugin, - projects: - hash: 22d7cc950ba228ec00383b1ee6447c64ea713cb0 repo: openstack/neutron-tempest-plugin version: 1.0.0,,4,0
openstack%2Fheat-translator~master~Ifdb287c84dfb19d25b519a2e23f1b82ccc3f4088,openstack/heat-translator,master,Ifdb287c84dfb19d25b519a2e23f1b82ccc3f4088,ETSI-NFV SOL 001 translation: ScalingPolicy,MERGED,2019-11-28 04:09:56.000000000,2020-03-23 14:44:34.000000000,2020-03-23 14:41:41.000000000,"[{'_account_id': 16511}, {'_account_id': 18955}, {'_account_id': 22348}, {'_account_id': 25701}, {'_account_id': 26588}]","[{'number': 1, 'created': '2019-11-28 04:09:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/787ccec881de4c1b848ae75fdc27aba8131b3373', 'message': 'ETSI-NFV SOL 001 translation: ScalingPolicy\n\nCurrently heat-translator supports translation of TOSCA Simple Profile\nfor YAML[1] and TOSCA Simple Profile for NFV[2] only.\nThis commit enables to translation of the follwoing type defined in\nETSI NFV-SOL 001[3].\n- tosca.policies.nfv.InstantiationLevels\n- tosca.policies.nfv.ScalingAspects\n- tosca.policies.nfv.VduInitialDelta\n- tosca.policies.nfv.VduInstantiationLevels\n- tosca.policies.nfv.VduScalingAspectDeltas\n- tosca.policies.nfv.VirtualLinkInstantiationLevels\n\n[1] http://docs.oasis-open.org/tosca/tosca-nfv/v1.0/tosca-nfv-v1.0.html\n[2] http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html\n[3] https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/001/02.06.01_60/gs_NFV-SOL001v020601p.pdf\n\nChange-Id: Ifdb287c84dfb19d25b519a2e23f1b82ccc3f4088\nStory: 2006372\nTask: 37622\n'}, {'number': 2, 'created': '2019-11-29 02:08:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/6c4c6f497c610375a31de4dbd8afacb3026e491a', 'message': 'ETSI-NFV SOL 001 translation: ScalingPolicy\n\nCurrently heat-translator supports translation of TOSCA Simple Profile\nfor YAML[1] and TOSCA Simple Profile for NFV[2] only.\nThis commit enables to translation of the follwoing type defined in\nETSI NFV-SOL 001[3].\n- tosca.policies.nfv.InstantiationLevels\n- tosca.policies.nfv.ScalingAspects\n- tosca.policies.nfv.VduInitialDelta\n- tosca.policies.nfv.VduInstantiationLevels\n- tosca.policies.nfv.VduScalingAspectDeltas\n- tosca.policies.nfv.VirtualLinkInstantiationLevels\n\n[1] http://docs.oasis-open.org/tosca/tosca-nfv/v1.0/tosca-nfv-v1.0.html\n[2] http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html\n[3] https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/001/02.06.01_60/gs_NFV-SOL001v020601p.pdf\n\nChange-Id: Ifdb287c84dfb19d25b519a2e23f1b82ccc3f4088\nStory: 2006372\nTask: 37622\n'}, {'number': 3, 'created': '2019-12-04 04:08:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/0d6f0d47aec8cb37288faf2b15d095052c2de335', 'message': 'WIP:ETSI-NFV SOL 001 translation: ScalingPolicy\n\nCurrently heat-translator supports translation of TOSCA Simple Profile\nfor YAML[1] and TOSCA Simple Profile for NFV[2] only.\nThis commit enables to translation of the follwoing type defined in\nETSI NFV-SOL 001[3].\n- tosca.policies.nfv.InstantiationLevels\n- tosca.policies.nfv.ScalingAspects\n- tosca.policies.nfv.VduInitialDelta\n- tosca.policies.nfv.VduInstantiationLevels\n- tosca.policies.nfv.VduScalingAspectDeltas\n- tosca.policies.nfv.VirtualLinkInstantiationLevels\n\n[1] http://docs.oasis-open.org/tosca/tosca-nfv/v1.0/tosca-nfv-v1.0.html\n[2] http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html\n[3] https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/001/02.06.01_60/gs_NFV-SOL001v020601p.pdf\n\nChange-Id: Ifdb287c84dfb19d25b519a2e23f1b82ccc3f4088\nStory: 2006372\nTask: 37622\n'}, {'number': 4, 'created': '2019-12-06 09:10:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/4c78e0d1573cdc7694f134e1386baa50e3064c17', 'message': 'ETSI-NFV SOL 001 translation: ScalingPolicy\n\nCurrently heat-translator supports translation of TOSCA Simple Profile\nfor YAML[1] and TOSCA Simple Profile for NFV[2] only.\nThis commit enables to translation of the follwoing type defined in\nETSI NFV-SOL 001[3].\n- tosca.policies.nfv.InstantiationLevels\n- tosca.policies.nfv.ScalingAspects\n- tosca.policies.nfv.VduInitialDelta\n- tosca.policies.nfv.VduInstantiationLevels\n- tosca.policies.nfv.VduScalingAspectDeltas\n- tosca.policies.nfv.VirtualLinkInstantiationLevels\n\n[1] http://docs.oasis-open.org/tosca/tosca-nfv/v1.0/tosca-nfv-v1.0.html\n[2] http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html\n[3] https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/001/02.06.01_60/gs_NFV-SOL001v020601p.pdf\n\nChange-Id: Ifdb287c84dfb19d25b519a2e23f1b82ccc3f4088\nStory: 2006372\nTask: 37622\n'}, {'number': 5, 'created': '2019-12-06 10:10:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/8b7de0c6a62695dd87f7697e36d9b43e80a91a5f', 'message': 'ETSI-NFV SOL 001 translation: ScalingPolicy\n\nCurrently heat-translator supports translation of TOSCA Simple Profile\nfor YAML[1] and TOSCA Simple Profile for NFV[2] only.\nThis commit enables to translation of the follwoing type defined in\nETSI NFV-SOL 001[3].\n- tosca.policies.nfv.InstantiationLevels\n- tosca.policies.nfv.ScalingAspects\n- tosca.policies.nfv.VduInitialDelta\n- tosca.policies.nfv.VduInstantiationLevels\n- tosca.policies.nfv.VduScalingAspectDeltas\n- tosca.policies.nfv.VirtualLinkInstantiationLevels\n\n[1] http://docs.oasis-open.org/tosca/tosca-nfv/v1.0/tosca-nfv-v1.0.html\n[2] http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html\n[3] https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/001/02.06.01_60/gs_NFV-SOL001v020601p.pdf\n\nChange-Id: Ifdb287c84dfb19d25b519a2e23f1b82ccc3f4088\nStory: 2006372\nTask: 37622\n'}, {'number': 6, 'created': '2019-12-09 10:41:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/fc15ab71dc3343aa389d2df32edf77def42e2154', 'message': 'ETSI-NFV SOL 001 translation: ScalingPolicy\n\nCurrently heat-translator supports translation of TOSCA Simple Profile\nfor YAML[1] and TOSCA Simple Profile for NFV[2] only.\nThis commit enables to translation of the follwoing type defined in\nETSI NFV-SOL 001[3].\n- tosca.policies.nfv.InstantiationLevels\n- tosca.policies.nfv.ScalingAspects\n- tosca.policies.nfv.VduInitialDelta\n- tosca.policies.nfv.VduInstantiationLevels\n- tosca.policies.nfv.VduScalingAspectDeltas\n- tosca.policies.nfv.VirtualLinkInstantiationLevels\n\n[1] http://docs.oasis-open.org/tosca/tosca-nfv/v1.0/tosca-nfv-v1.0.html\n[2] http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html\n[3] https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/001/02.06.01_60/gs_NFV-SOL001v020601p.pdf\n\nChange-Id: Ifdb287c84dfb19d25b519a2e23f1b82ccc3f4088\nStory: 2006372\nTask: 37622\n'}, {'number': 7, 'created': '2019-12-20 01:02:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/89cc49a981879aab92e6a9d25a2aaf3671e3d647', 'message': 'ETSI-NFV SOL 001 translation: ScalingPolicy\n\nCurrently heat-translator supports translation of TOSCA Simple Profile\nfor YAML[1] and TOSCA Simple Profile for NFV[2] only.\nThis commit enables to translation of the follwoing type defined in\nETSI NFV-SOL 001[3].\n- tosca.policies.nfv.InstantiationLevels\n- tosca.policies.nfv.ScalingAspects\n- tosca.policies.nfv.VduInitialDelta\n- tosca.policies.nfv.VduInstantiationLevels\n- tosca.policies.nfv.VduScalingAspectDeltas\n- tosca.policies.nfv.VirtualLinkInstantiationLevels\n\n[1] http://docs.oasis-open.org/tosca/tosca-nfv/v1.0/tosca-nfv-v1.0.html\n[2] http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html\n[3] https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/001/02.06.01_60/gs_NFV-SOL001v020601p.pdf\n\nChange-Id: Ifdb287c84dfb19d25b519a2e23f1b82ccc3f4088\nStory: 2006372\nTask: 37622\n'}, {'number': 8, 'created': '2020-01-22 03:36:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/efeb46f1b88f692a16f7108631e104be64cfb91c', 'message': 'ETSI-NFV SOL 001 translation: ScalingPolicy\n\nCurrently heat-translator supports translation of TOSCA Simple Profile\nfor YAML[1] and TOSCA Simple Profile for NFV[2] only.\nThis commit enables to translation of the follwoing type defined in\nETSI NFV-SOL 001[3].\n- tosca.policies.nfv.InstantiationLevels\n- tosca.policies.nfv.ScalingAspects\n- tosca.policies.nfv.VduInitialDelta\n- tosca.policies.nfv.VduInstantiationLevels\n- tosca.policies.nfv.VduScalingAspectDeltas\n- tosca.policies.nfv.VirtualLinkInstantiationLevels\n\n[1] http://docs.oasis-open.org/tosca/tosca-nfv/v1.0/tosca-nfv-v1.0.html\n[2] http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html\n[3] https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/001/02.06.01_60/gs_NFV-SOL001v020601p.pdf\n\nChange-Id: Ifdb287c84dfb19d25b519a2e23f1b82ccc3f4088\nStory: 2006372\nTask: 37622\n'}, {'number': 9, 'created': '2020-01-24 04:54:58.000000000', 'files': ['translator/tests/test_etsi_tosca_hot_translation.py', 'translator/tests/data/hot_output/etsi_nfv/scaling_non_target_vdu_in_initial_delta/hot_nfv_scaling_non_target_vdu_in_initial_delta.yaml', 'translator/hot/tosca/tests/test_tosca_autoscaling.py', 'translator/tests/data/hot_output/etsi_nfv/vdu_cp_vl_with_mixed_scaling/hot_nfv_vdu_cp_vl_with_mixed_scaling.yaml', 'translator/hot/tosca/etsi_nfv/tosca_policies_nfv_vduscalingaspectdeltas.py', 'translator/hot/tosca/etsi_nfv/tosca_policies_nfv_virtuallinkinstantiationlevels.py', 'translator/tests/data/hot_output/nfv/SP_res.yaml', 'translator/tests/data/hot_output/reservation/SP_RSV_res.yaml', 'translator/tests/data/etsi_nfv/tosca_nfv_vdu_cp_with_scaling_multi_aspects.yaml', 'translator/tests/data/hot_output/monitoring/asg_res.yaml', 'translator/hot/translate_node_templates.py', 'translator/hot/tosca/etsi_nfv/tosca_policies_nfv_instantiationlevels.py', 'translator/hot/tosca/etsi_nfv/tosca_policies_nfv_scalingaspects.py', 'translator/tests/data/hot_output/etsi_nfv/scaling_non_deltas_in_aspect_delta/hot_nfv_scaling_non_deltas_in_aspect_delta.yaml', 'translator/tests/data/hot_output/etsi_nfv/scaling_non_deltas_in_aspect_delta/worker_instance.hot.yaml', 'translator/tests/data/etsi_nfv/tosca_nfv_scaling_non_target_vdu_in_aspect_delta.yaml', 'translator/tests/data/hot_output/etsi_nfv/scaling_non_target_vdu_in_aspect_delta/hot_nfv_scaling_non_target_vdu_in_aspect_delta.yaml', 'translator/tests/data/etsi_nfv/tosca_nfv_scaling_non_target_vdu_in_initial_delta.yaml', 'translator/hot/tosca/etsi_nfv/tosca_policies_nfv_vduinstantiationlevels.py', 'translator/tests/test_translate_node_template.py', 'translator/tests/data/hot_output/nfv/SP1_res.yaml', 'translator/hot/tosca/etsi_nfv/tosca_policies_nfv_vduinitialdelta.py', 'translator/tests/data/hot_output/etsi_nfv/vdu_cp_with_scaling_multi_aspects/worker_instance1.hot.yaml', 'translator/hot/tosca/etsi_nfv/scalingaspect/tosca_policies_nfv_scalingaspect.py', 'translator/hot/tosca/etsi_nfv/scalingaspect/__init__.py', 'translator/tests/data/hot_output/etsi_nfv/vdu_cp_vl_with_mixed_scaling/worker_instance.hot.yaml', 'translator/tests/data/hot_output/etsi_nfv/vnf_vdu_cp_vl_blockstorage_with_scaling/hot_nfv_vnf_vdu_cp_vl_blockstorage_with_scaling.yaml', 'translator/hot/tosca/tosca_policies_scaling.py', 'translator/hot/syntax/hot_resource.py', 'translator/tests/data/hot_output/etsi_nfv/vnf_vdu_cp_vl_blockstorage_with_scaling/worker_instance.hot.yaml', 'translator/tests/data/etsi_nfv/tosca_nfv_scaling_non_deltas_in_aspect_delta.yaml', 'translator/tests/data/hot_output/etsi_nfv/scaling_non_target_vdu_in_aspect_delta/worker_instance.hot.yaml', 'translator/tests/data/hot_output/autoscaling/asg_res.yaml', 'translator/tests/data/hot_output/etsi_nfv/vdu_cp_with_scaling_multi_aspects/hot_nfv_vdu_cp_with_scaling_multi_aspects.yaml', 'translator/tests/data/etsi_nfv/tosca_nfv_vdu_cp_vl_with_mixed_scaling.yaml', 'translator/tests/data/etsi_nfv/tosca_nfv_vnf_vdu_cp_vl_blockstorage_with_scaling.yaml', 'translator/tests/data/hot_output/etsi_nfv/scaling_non_target_vdu_in_initial_delta/worker_instance.hot.yaml', 'translator/tests/data/hot_output/etsi_nfv/vdu_cp_with_scaling_multi_aspects/worker_instance2.hot.yaml'], 'web_link': 'https://opendev.org/openstack/heat-translator/commit/292248be2c9ff16f318fd124c0c1cba512616ed3', 'message': 'ETSI-NFV SOL 001 translation: ScalingPolicy\n\nCurrently heat-translator supports translation of TOSCA Simple Profile\nfor YAML[1] and TOSCA Simple Profile for NFV[2] only.\nThis commit enables to translation of the follwoing type defined in\nETSI NFV-SOL 001[3].\n- tosca.policies.nfv.InstantiationLevels\n- tosca.policies.nfv.ScalingAspects\n- tosca.policies.nfv.VduInitialDelta\n- tosca.policies.nfv.VduInstantiationLevels\n- tosca.policies.nfv.VduScalingAspectDeltas\n- tosca.policies.nfv.VirtualLinkInstantiationLevels\n\n[1] http://docs.oasis-open.org/tosca/tosca-nfv/v1.0/tosca-nfv-v1.0.html\n[2] http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html\n[3] https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/001/02.06.01_60/gs_NFV-SOL001v020601p.pdf\n\nChange-Id: Ifdb287c84dfb19d25b519a2e23f1b82ccc3f4088\nStory: 2006372\nTask: 37622\n'}]",0,696446,292248be2c9ff16f318fd124c0c1cba512616ed3,26,5,9,31072,,,0,"ETSI-NFV SOL 001 translation: ScalingPolicy

Currently heat-translator supports translation of TOSCA Simple Profile
for YAML[1] and TOSCA Simple Profile for NFV[2] only.
This commit enables to translation of the follwoing type defined in
ETSI NFV-SOL 001[3].
- tosca.policies.nfv.InstantiationLevels
- tosca.policies.nfv.ScalingAspects
- tosca.policies.nfv.VduInitialDelta
- tosca.policies.nfv.VduInstantiationLevels
- tosca.policies.nfv.VduScalingAspectDeltas
- tosca.policies.nfv.VirtualLinkInstantiationLevels

[1] http://docs.oasis-open.org/tosca/tosca-nfv/v1.0/tosca-nfv-v1.0.html
[2] http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/TOSCA-Simple-Profile-YAML-v1.0.html
[3] https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/001/02.06.01_60/gs_NFV-SOL001v020601p.pdf

Change-Id: Ifdb287c84dfb19d25b519a2e23f1b82ccc3f4088
Story: 2006372
Task: 37622
",git fetch https://review.opendev.org/openstack/heat-translator refs/changes/46/696446/1 && git format-patch -1 --stdout FETCH_HEAD,"['translator/tests/test_etsi_tosca_hot_translation.py', 'translator/tests/data/hot_output/etsi_nfv/scaling_non_target_vdu_in_initial_delta/hot_nfv_scaling_non_target_vdu_in_initial_delta.yaml', 'translator/hot/tosca/tests/test_tosca_autoscaling.py', 'translator/tests/data/hot_output/etsi_nfv/vdu_cp_vl_with_mixed_scaling/hot_nfv_vdu_cp_vl_with_mixed_scaling.yaml', 'translator/hot/tosca/etsi_nfv/tosca_policies_nfv_vduscalingaspectdeltas.py', 'translator/hot/tosca/etsi_nfv/tosca_policies_nfv_virtuallinkinstantiationlevels.py', 'translator/tests/data/hot_output/nfv/SP_res.yaml', 'translator/tests/data/hot_output/reservation/SP_RSV_res.yaml', 'translator/tests/data/etsi_nfv/tosca_nfv_vdu_cp_with_scaling_multi_aspects.yaml', 'translator/tests/data/hot_output/monitoring/asg_res.yaml', 'translator/hot/translate_node_templates.py', 'translator/hot/tosca/etsi_nfv/tosca_policies_nfv_instantiationlevels.py', 'translator/hot/tosca/etsi_nfv/tosca_policies_nfv_scalingaspects.py', 'translator/tests/data/hot_output/etsi_nfv/scaling_non_deltas_in_aspect_delta/hot_nfv_scaling_non_deltas_in_aspect_delta.yaml', 'translator/tests/data/hot_output/etsi_nfv/scaling_non_deltas_in_aspect_delta/worker_instance.hot.yaml', 'translator/tests/data/etsi_nfv/tosca_nfv_scaling_non_target_vdu_in_aspect_delta.yaml', 'translator/tests/data/hot_output/etsi_nfv/scaling_non_target_vdu_in_aspect_delta/hot_nfv_scaling_non_target_vdu_in_aspect_delta.yaml', 'translator/tests/data/etsi_nfv/tosca_nfv_scaling_non_target_vdu_in_initial_delta.yaml', 'translator/hot/tosca/etsi_nfv/tosca_policies_nfv_vduinstantiationlevels.py', 'translator/tests/test_translate_node_template.py', 'translator/tests/data/hot_output/nfv/SP1_res.yaml', 'translator/hot/tosca/etsi_nfv/tosca_policies_nfv_vduinitialdelta.py', 'translator/tests/data/hot_output/etsi_nfv/vdu_cp_with_scaling_multi_aspects/worker_instance1.hot.yaml', 'translator/hot/tosca/etsi_nfv/scalingaspect/tosca_policies_nfv_scalingaspect.py', 'translator/hot/tosca/etsi_nfv/scalingaspect/__init__.py', 'translator/tests/data/hot_output/etsi_nfv/vdu_cp_vl_with_mixed_scaling/worker_instance.hot.yaml', 'translator/tests/data/hot_output/etsi_nfv/vnf_vdu_cp_vl_blockstorage_with_scaling/hot_nfv_vnf_vdu_cp_vl_blockstorage_with_scaling.yaml', 'translator/hot/tosca/tosca_policies_scaling.py', 'translator/hot/syntax/hot_resource.py', 'translator/tests/data/hot_output/etsi_nfv/vnf_vdu_cp_vl_blockstorage_with_scaling/worker_instance.hot.yaml', 'translator/tests/data/etsi_nfv/tosca_nfv_scaling_non_deltas_in_aspect_delta.yaml', 'translator/tests/data/hot_output/etsi_nfv/scaling_non_target_vdu_in_aspect_delta/worker_instance.hot.yaml', 'translator/tests/data/hot_output/autoscaling/asg_res.yaml', 'translator/tests/data/hot_output/etsi_nfv/vdu_cp_with_scaling_multi_aspects/hot_nfv_vdu_cp_with_scaling_multi_aspects.yaml', 'translator/tests/data/etsi_nfv/tosca_nfv_vdu_cp_vl_with_mixed_scaling.yaml', 'translator/tests/data/etsi_nfv/tosca_nfv_vnf_vdu_cp_vl_blockstorage_with_scaling.yaml', 'translator/tests/data/hot_output/etsi_nfv/scaling_non_target_vdu_in_initial_delta/worker_instance.hot.yaml', 'translator/tests/data/hot_output/etsi_nfv/vdu_cp_with_scaling_multi_aspects/worker_instance2.hot.yaml']",38,787ccec881de4c1b848ae75fdc27aba8131b3373,etsi_nfv-sol001,﻿heat_template_version: 2013-05-23 description: Scaling template parameters: vdu2_flavor_id: type: string resources: CP2: type: OS::Neutron::Port properties: network: REPLACE_TO_EXTERNAL_VL binding:vnic_type: direct-physical VDU2: type: OS::Nova::Server properties: flavor: { get_param: vdu2_flavor_id } name: VDU2 networks: - port: { get_resource: CP2 } ,,2076,70
openstack%2Fproject-config~master~I13fa07754e38281c63dcf0eceaa4c3b3c2715618,openstack/project-config,master,I13fa07754e38281c63dcf0eceaa4c3b3c2715618,check-release-approval: handle no-review case,MERGED,2020-03-20 10:27:21.000000000,2020-03-23 14:44:30.000000000,2020-03-23 14:44:30.000000000,"[{'_account_id': 308}, {'_account_id': 4146}, {'_account_id': 6547}, {'_account_id': 11904}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 10:27:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/bd00c3e7ba0806acc21905c7a57874b436b19a1d', 'message': ""check-release-approval: handle no-review case\n\nWhen no review is posted yet, Gerrit returns simplified 'labels'\ndata. In particular it's missing the ['labels']['Code-Review]['all']\ndictionary, which we assumed would always be present.\n\nWe should only add approvers from the reviews if the 'all' key is\nprovided, and otherwise just work from the owner email.\n\nRemove all changes pushed to investigate the issue: use a narrow\nquery again (rather than the /detail call) and no longer catch the\nexception.\n\nChange-Id: I13fa07754e38281c63dcf0eceaa4c3b3c2715618\n""}, {'number': 2, 'created': '2020-03-20 10:38:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/df3175eb4e11fc2a667d57d5bdc5a8f76ad0adbe', 'message': ""check-release-approval: handle no-review case\n\nWhen no review is posted yet, Gerrit returns simplified 'labels'\ndata. In particular it's missing the ['labels']['Code-Review]['all']\ndictionary, which we assumed would always be present.\n\nWe should only add approvers from the reviews if the 'all' key is\nprovided, and otherwise just work from the owner email.\n\nRemove all changes pushed to investigate the issue: use a narrow\nquery again (rather than the /detail call) and no longer catch the\nexception.\n\nChange-Id: I13fa07754e38281c63dcf0eceaa4c3b3c2715618\n""}, {'number': 3, 'created': '2020-03-20 10:40:14.000000000', 'files': ['roles/check-release-approval/files/check_approval.py'], 'web_link': 'https://opendev.org/openstack/project-config/commit/2ce6a551ac24d70a72b49bff573e40425b7fd663', 'message': ""check-release-approval: handle no-review case\n\nWhen no review is posted yet, Gerrit returns simplified 'labels'\ndata. In particular it's missing the ['labels']['Code-Review]['all']\ndictionary, which we assumed would always be present.\n\nWe should only add approvers from the reviews if the 'all' key is\nprovided, and otherwise just work from the owner email.\n\nRemove all changes pushed to investigate the issue: use a narrow\nquery again (rather than the /detail call) and no longer catch the\nexception.\n\nChange-Id: I13fa07754e38281c63dcf0eceaa4c3b3c2715618\n""}]",1,714066,2ce6a551ac24d70a72b49bff573e40425b7fd663,13,5,3,308,,,0,"check-release-approval: handle no-review case

When no review is posted yet, Gerrit returns simplified 'labels'
data. In particular it's missing the ['labels']['Code-Review]['all']
dictionary, which we assumed would always be present.

We should only add approvers from the reviews if the 'all' key is
provided, and otherwise just work from the owner email.

Remove all changes pushed to investigate the issue: use a narrow
query again (rather than the /detail call) and no longer catch the
exception.

Change-Id: I13fa07754e38281c63dcf0eceaa4c3b3c2715618
",git fetch https://review.opendev.org/openstack/project-config refs/changes/66/714066/1 && git format-patch -1 --stdout FETCH_HEAD,['roles/check-release-approval/files/check_approval.py'],1,bd00c3e7ba0806acc21905c7a57874b436b19a1d,fix-check-approval, self.load_from_gerrit(args.changeid) call = 'changes/%s' % changeid + \ '?o=CURRENT_REVISION&o=CURRENT_FILES&o=DETAILED_LABELS' + \ '&o=DETAILED_ACCOUNTS' # Extract approvers from JSON data. Approvers include change owner # and anyone who voted Code-Review+1. NB: Gerrit does not fill # labels.CodeReview.all unless there is a vote already self.approvers = [decoded['owner']['email']] if 'all' in decoded['labels']['Code-Review']: self.approvers.extend( [i['email'] for i in decoded['labels']['Code-Review']['all'] if i['value'] > 0] ) # Extract list of modified deliberables files from JSON data," try: self.load_from_gerrit(args.changeid) except KeyError: LOG.warning( '\ndata from gerrit is missing required keys:\n\n%s\n', json.dumps(self.raw_data, indent=2)) raise call = 'changes/%s/detail' % changeid + \ '?o=CURRENT_REVISION&o=CURRENT_FILES' # Instantiate object with retrieved data self.raw_data = decoded self.approvers = [i['email'] for i in decoded['labels']['Code-Review']['all'] if i['value'] > 0] self.approvers.append(decoded['owner']['email'])",16,16
openstack%2Fbifrost~stable%2Ftrain~I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5,openstack/bifrost,stable/train,I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5,CentOS 8 support (Train),MERGED,2020-03-17 18:23:03.000000000,2020-03-23 14:43:03.000000000,2020-03-21 13:25:18.000000000,"[{'_account_id': 10239}, {'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 23851}]","[{'number': 1, 'created': '2020-03-17 18:23:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/b2b647a2e75daeda11f8a4596da7fb0520382c7f', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 2, 'created': '2020-03-17 18:31:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/df2263965a48029ecc0829594e7b8fafdc6df818', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 3, 'created': '2020-03-17 18:51:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/bd4ce50feed4cf173c55a1d2501b03d55d4d493c', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 4, 'created': '2020-03-17 19:27:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/d668412d5907a8c848ac2b69f069b4dedb6c99be', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 5, 'created': '2020-03-18 10:09:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/de1de47232bc009faeb969190ab84eb12354e6ed', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 6, 'created': '2020-03-18 10:50:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/452e46d00fd1a610d7478e34feeb85721295eb62', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 7, 'created': '2020-03-18 11:29:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/9507ac36b0bb03e11d924c60e2ce5f571896a00b', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 8, 'created': '2020-03-18 11:50:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/c6ea08109aa320361ab02c38f081b2984b284afa', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 9, 'created': '2020-03-18 12:44:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/cc8c8e470e56d3e5adf0bf76dd66719e670bf90d', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 10, 'created': '2020-03-18 13:07:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/e3781eaf43c1a69664cdcb0b2243421b9a8c220f', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 11, 'created': '2020-03-18 13:10:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/99c7a179deacca2aaae2b71299559053add781cb', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 12, 'created': '2020-03-18 16:46:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/49ea409fffb36e8aa74a88f146df89f1f633fae6', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 13, 'created': '2020-03-18 17:22:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/ba55da3291a499dc1dd366f3e2b530f35a812e14', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 14, 'created': '2020-03-18 19:46:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/54c78e52b9da02959a1580a6a84f0293d1049ee7', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 15, 'created': '2020-03-19 11:00:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/e4e84a905eba969129a4e018635f12fe6534842a', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 16, 'created': '2020-03-19 12:47:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/16faaee4fe6fcc566a1043f5fd7555fcc45b3f25', 'message': 'WIP: CentOS 8 support (Train)\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n'}, {'number': 17, 'created': '2020-03-19 13:58:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/9860dce5ffcf81bfc848cfe2406678469df341f7', 'message': ""CentOS 8 support (Train)\n\nThis is a selective backport of the CentOS 8 support from the master\nbranch. It is a combination of parts of various commits, with\nmodifications to continue to support CentOS 7 and Python 2.\n\n* Make bindep.txt CentOS version specific\n* kvm_stats util has been moved to kernel-tools, the package\n  qemu-kvm-tools doesn't exist anymore\n* qemu-system-x86 doesn't exist anymore, therefore we run\n  /usr/libexec/qemu-kvm instead of\n  /bin/qemu-system-x86_64\n* installing package trousers to add tss user and group needed by\n  libvirt\n* Stop setting CPU mode - allow libvirt to select. host-model was\n  failing on libvirt 4.5.0 on CentOS 8 due to\n  https://bugzilla.redhat.com/show_bug.cgi?id=1804224\n* Stop using unversioned Python executable - use ansible_python\n* Modify required packages for CentOS 8\n* Use Ansible 2.8 in CI\n* Switch from yum to dnf where necessary\n* Use a virtualenv for CentOS 8 CI\n* Add two CentOS 8 jobs\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n""}, {'number': 18, 'created': '2020-03-19 17:10:27.000000000', 'files': ['playbooks/roles/bifrost-create-vm-nodes/tasks/prepare_libvirt.yml', 'playbooks/roles/bifrost-ironic-install/tasks/main.yml', 'scripts/collect-test-info.sh', 'playbooks/roles/bifrost-create-vm-nodes/defaults/required_defaults_CentOS_8.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_CentOS_8.yml', 'playbooks/roles/bifrost-create-vm-nodes/defaults/main.yml', 'playbooks/roles/bifrost-create-vm-nodes/templates/testvm.xml.j2', 'zuul.d/bifrost-jobs.yaml', 'scripts/test-bifrost.sh', 'playbooks/roles/bifrost-keystone-install/tasks/main.yml', 'playbooks/roles/bifrost-create-vm-nodes/tasks/main.yml', 'bindep.txt', 'scripts/install-deps.sh', 'playbooks/roles/bifrost-ironic-install/tasks/install.yml', 'zuul.d/project.yaml', 'playbooks/roles/bifrost-keystone-install/tasks/pip_install.yml', 'scripts/env-setup.sh', 'playbooks/roles/bifrost-keystone-install/defaults/required_defaults_CentOS_8.yml', 'playbooks/roles/bifrost-ironic-install/tasks/pip_install.yml', 'playbooks/roles/bifrost-test-dhcp/tasks/main.yml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/d1ddbdf37b2c43dd86bb5aa40bf1b5dd247b33f8', 'message': ""CentOS 8 support (Train)\n\nThis is a selective backport of the CentOS 8 support from the master\nbranch. It is a combination of parts of various commits, with\nmodifications to continue to support CentOS 7 and Python 2.\n\n* Make bindep.txt CentOS version specific\n* kvm_stats util has been moved to kernel-tools, the package\n  qemu-kvm-tools doesn't exist anymore\n* qemu-system-x86 doesn't exist anymore, therefore we run\n  /usr/libexec/qemu-kvm instead of\n  /bin/qemu-system-x86_64\n* installing package trousers to add tss user and group needed by\n  libvirt\n* Stop setting CPU mode - allow libvirt to select. host-model was\n  failing on libvirt 4.5.0 on CentOS 8 due to\n  https://bugzilla.redhat.com/show_bug.cgi?id=1804224\n* Stop using unversioned Python executable - use ansible_python\n* Modify required packages for CentOS 8\n* Use Ansible 2.8 in CI\n* Switch from yum to dnf where necessary\n* Use a virtualenv for CentOS 8 CI\n* Add two CentOS 8 jobs\n\nChange-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5\n""}]",11,713508,d1ddbdf37b2c43dd86bb5aa40bf1b5dd247b33f8,46,4,18,14826,,,0,"CentOS 8 support (Train)

This is a selective backport of the CentOS 8 support from the master
branch. It is a combination of parts of various commits, with
modifications to continue to support CentOS 7 and Python 2.

* Make bindep.txt CentOS version specific
* kvm_stats util has been moved to kernel-tools, the package
  qemu-kvm-tools doesn't exist anymore
* qemu-system-x86 doesn't exist anymore, therefore we run
  /usr/libexec/qemu-kvm instead of
  /bin/qemu-system-x86_64
* installing package trousers to add tss user and group needed by
  libvirt
* Stop setting CPU mode - allow libvirt to select. host-model was
  failing on libvirt 4.5.0 on CentOS 8 due to
  https://bugzilla.redhat.com/show_bug.cgi?id=1804224
* Stop using unversioned Python executable - use ansible_python
* Modify required packages for CentOS 8
* Use Ansible 2.8 in CI
* Switch from yum to dnf where necessary
* Use a virtualenv for CentOS 8 CI
* Add two CentOS 8 jobs

Change-Id: I9101a7d9c378b7fec9e6a94ff6e86e7d604680d5
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/08/713508/8 && git format-patch -1 --stdout FETCH_HEAD,"['playbooks/roles/bifrost-create-vm-nodes/tasks/prepare_libvirt.yml', 'playbooks/roles/bifrost-ironic-install/tasks/main.yml', 'scripts/collect-test-info.sh', 'playbooks/roles/bifrost-create-vm-nodes/defaults/required_defaults_CentOS_8.yml', 'playbooks/roles/bifrost-ironic-install/defaults/required_defaults_CentOS_8.yml', 'playbooks/roles/bifrost-create-vm-nodes/defaults/main.yml', 'playbooks/roles/bifrost-create-vm-nodes/templates/testvm.xml.j2', 'zuul.d/bifrost-jobs.yaml', 'scripts/test-bifrost.sh', 'playbooks/roles/bifrost-keystone-install/tasks/main.yml', 'playbooks/roles/bifrost-create-vm-nodes/tasks/main.yml', 'bindep.txt', 'scripts/install-deps.sh', 'playbooks/roles/bifrost-ironic-install/tasks/install.yml', 'zuul.d/project.yaml', 'playbooks/roles/bifrost-keystone-install/tasks/pip_install.yml', 'scripts/env-setup.sh', 'playbooks/roles/bifrost-keystone-install/defaults/required_defaults_CentOS_8.yml', 'playbooks/roles/bifrost-ironic-install/tasks/pip_install.yml']",19,b2b647a2e75daeda11f8a4596da7fb0520382c7f,centos8-train," venv_command: ""{{ ansible_python.executable }} -m virtualenv"" when: enable_venv executable: ""/usr/bin/pip{{ ansible_python.version.major }}"" until: pip_package_install_done is succeeded when: (source_install is not defined or source_install | bool == false ) and enable_venv | default(false) | bool == false pip{{ ansible_python.version.major}} install -r {{ sourcedir }}/requirements.txt until: pip_package_install_done is succeeded when: source_install is defined and source_install | default(true) | bool command: pip{{ ansible_python.version.major }} install {{ sourcedir }} {{ extra_args | default('') }}"," venv_command: ""{{ hostvars[inventory_hostname].ansible_python.executable }} -m virtualenv"" when: enable_venv|bool until: pip_package_install_done|succeeded when: (source_install is not defined or source_install == false) and not enable_venv|bool pip install -r {{ sourcedir }}/requirements.txt until: pip_package_install_done|succeeded when: source_install is defined and (source_install | bool == true) command: pip install {{ sourcedir }} {{ extra_args | default('') }}",247,93
openstack%2Fkolla-ansible~stable%2Ftrain~I5344abe93f682418a0f61c894d64b360bc129592,openstack/kolla-ansible,stable/train,I5344abe93f682418a0f61c894d64b360bc129592,CI: CentOS 8: Enable Masakari job and periodics,MERGED,2020-03-20 09:38:02.000000000,2020-03-23 14:36:53.000000000,2020-03-23 14:31:11.000000000,"[{'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 24072}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-20 09:38:02.000000000', 'files': ['zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/d2efead12364737ee0aee88c46743afeecbcf859', 'message': 'CI: CentOS 8: Enable Masakari job and periodics\n\nDepends-On: https://review.opendev.org/714055\n\nChange-Id: I5344abe93f682418a0f61c894d64b360bc129592\nPartially-Implements: blueprint centos-rhel-8\n(cherry picked from commit 368ad387d0cdbeb464d1f72d6037998a2189077f)\n'}]",0,714059,d2efead12364737ee0aee88c46743afeecbcf859,12,4,1,14826,,,0,"CI: CentOS 8: Enable Masakari job and periodics

Depends-On: https://review.opendev.org/714055

Change-Id: I5344abe93f682418a0f61c894d64b360bc129592
Partially-Implements: blueprint centos-rhel-8
(cherry picked from commit 368ad387d0cdbeb464d1f72d6037998a2189077f)
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/59/714059/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/project.yaml'],1,d2efead12364737ee0aee88c46743afeecbcf859,bp/centos-rhel-8, - kolla-ansible-centos8-source-masakari: files: - ^ansible/roles/masakari/ - ^tests/test-masakari.sh - ^tests/test-dashboard.sh - kolla-ansible-centos8-source-zun - kolla-ansible-centos8-source-masakari - kolla-ansible-centos8-source-ironic - kolla-ansible-centos8-source-upgrade - kolla-ansible-centos8-source-mariadb, # FIXME(mgoddard): Masakari CentOS 8 job. # - kolla-ansible-centos8-source-masakari: # files: # - ^ansible/roles/masakari/ # - ^tests/test-masakari.sh # - ^tests/test-dashboard.sh,10,6
openstack%2Fkolla~stable%2Ftrain~I1c786eda399eca4464c9d68ac040c2965350f775,openstack/kolla,stable/train,I1c786eda399eca4464c9d68ac040c2965350f775,CentOS 8: Enable hacluster and masakari images,MERGED,2020-03-20 09:17:50.000000000,2020-03-23 14:36:48.000000000,2020-03-23 14:31:09.000000000,"[{'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 24072}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-20 09:17:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/bea99bdeb24bbe3a590d299eef357dd75599f5fc', 'message': ""CentOS 8: Enable hacluster and masakari images\n\nCentOS 8.1 came with HA repo. So we can use it.\n\nhacluster-pcs image is disabled as we lack 'crmsh' package for it.\n\nPartially-Implements: blueprint centos-rhel-8\n\nCo-Authored-By: Mark Goddard <mark@stackhpc.com>\nChange-Id: I1c786eda399eca4464c9d68ac040c2965350f775\n(cherry picked from commit 92224ccb518c90d7c269158d074d811aee2f05f0)\n""}, {'number': 2, 'created': '2020-03-20 09:52:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/43edcbac85211d91268d2e3baa13e0b1f2bc0065', 'message': ""CentOS 8: Enable hacluster and masakari images\n\nCentOS 8.1 came with HA repo. So we can use it.\n\nhacluster-pcs image is disabled as we lack 'crmsh' package for it.\n\nPartially-Implements: blueprint centos-rhel-8\n\nCo-Authored-By: Mark Goddard <mark@stackhpc.com>\nChange-Id: I1c786eda399eca4464c9d68ac040c2965350f775\n(cherry picked from commit 92224ccb518c90d7c269158d074d811aee2f05f0)\n""}, {'number': 3, 'created': '2020-03-20 10:45:44.000000000', 'files': ['kolla/image/build.py', 'docker/base/Dockerfile.j2'], 'web_link': 'https://opendev.org/openstack/kolla/commit/ad7a0f0dcf594fce8c92915b0040ec18eb024d50', 'message': ""CentOS 8: Enable hacluster and masakari images\n\nCentOS 8.1 came with HA repo. So we can use it.\n\nhacluster-pcs image is disabled as we lack 'crmsh' package for it.\n\nPartially-Implements: blueprint centos-rhel-8\n\nCo-Authored-By: Mark Goddard <mark@stackhpc.com>\nChange-Id: I1c786eda399eca4464c9d68ac040c2965350f775\n(cherry picked from commit 92224ccb518c90d7c269158d074d811aee2f05f0)\n""}]",3,714055,ad7a0f0dcf594fce8c92915b0040ec18eb024d50,26,4,3,14826,,,0,"CentOS 8: Enable hacluster and masakari images

CentOS 8.1 came with HA repo. So we can use it.

hacluster-pcs image is disabled as we lack 'crmsh' package for it.

Partially-Implements: blueprint centos-rhel-8

Co-Authored-By: Mark Goddard <mark@stackhpc.com>
Change-Id: I1c786eda399eca4464c9d68ac040c2965350f775
(cherry picked from commit 92224ccb518c90d7c269158d074d811aee2f05f0)
",git fetch https://review.opendev.org/openstack/kolla refs/changes/55/714055/2 && git format-patch -1 --stdout FETCH_HEAD,"['kolla/image/build.py', 'docker/base/Dockerfile.j2']",2,bea99bdeb24bbe3a590d299eef357dd75599f5fc,bp/centos-rhel-8, 'HighAvailability',,2,3
openstack%2Frally~master~I9264395985876530fe0eb94dc5d3cae2d3fc9ace,openstack/rally,master,I9264395985876530fe0eb94dc5d3cae2d3fc9ace,[plugins] Add proper debug message in case of multiple match,ABANDONED,2018-06-13 16:02:01.000000000,2020-03-23 14:36:25.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2018-06-13 16:02:01.000000000', 'files': ['rally/common/plugin/plugin.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/34fca0fa99cf19f5a0d221643eebaa59f74a0c95', 'message': ""[plugins] Add proper debug message in case of multiple match\n\nIn case of obtaining plugin by the name without selecting platform, we\nhave a hack to use default platfrom in case of multiple match found.\n\nTo do not hide this magic, let's print debug message with that info.\n\nChange-Id: I9264395985876530fe0eb94dc5d3cae2d3fc9ace\n""}]",0,575155,34fca0fa99cf19f5a0d221643eebaa59f74a0c95,3,1,1,9545,,,0,"[plugins] Add proper debug message in case of multiple match

In case of obtaining plugin by the name without selecting platform, we
have a hack to use default platfrom in case of multiple match found.

To do not hide this magic, let's print debug message with that info.

Change-Id: I9264395985876530fe0eb94dc5d3cae2d3fc9ace
",git fetch https://review.opendev.org/openstack/rally refs/changes/55/575155/1 && git format-patch -1 --stdout FETCH_HEAD,['rally/common/plugin/plugin.py'],1,34fca0fa99cf19f5a0d221643eebaa59f74a0c95,plugin_warning,"from rally.common import loggingLOG = logging.getLogger(__name__) LOG.debug( ""Multiple plugins found for '%s': '%s'. The default one "" ""will be used."" % (name, [p for p in results if p.get_fullname()]))",,8,0
openstack%2Frally~master~Ic83b5f862ebd9e207a8d072ad74596275d561db9,openstack/rally,master,Ic83b5f862ebd9e207a8d072ad74596275d561db9,Move generate_random_path helper to proper place,ABANDONED,2018-07-26 10:44:27.000000000,2020-03-23 14:35:33.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2018-07-26 10:44:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/974dc8c3ef0c5b290279595ec687e7128bca4b44', 'message': 'Move generate_random_path helper to proper place\n\nrally.common.utils contains utils for different stuff. Its name is tto\nbroad and we have a bunch of other modules called ""utils"". Name\ncollision produces chaos and aliases.\nIt would be nice if we can fix this issue and put all helpers to proper\nmodules.\n\nChange-Id: Ic83b5f862ebd9e207a8d072ad74596275d561db9\n'}, {'number': 2, 'created': '2018-07-26 10:51:53.000000000', 'files': ['rally/plugins/common/verification/testr.py', 'tests/unit/common/test_utils.py', 'tests/unit/utils/test_fileutils.py', 'tests/unit/plugins/common/verification/test_testr.py', 'rally/utils/fileutils.py', 'rally/common/utils.py', 'CHANGELOG.rst'], 'web_link': 'https://opendev.org/openstack/rally/commit/cfd3add24ac539843ab0a3d86fdef74363f31b08', 'message': 'Move generate_random_path helper to proper place\n\nrally.common.utils contains utils for different stuff. Its name is tto\nbroad and we have a bunch of other modules called ""utils"". Name\ncollision produces chaos and aliases.\nIt would be nice if we can fix this issue and put all helpers to proper\nmodules.\n\nChange-Id: Ic83b5f862ebd9e207a8d072ad74596275d561db9\n'}]",0,586046,cfd3add24ac539843ab0a3d86fdef74363f31b08,4,1,2,9545,,,0,"Move generate_random_path helper to proper place

rally.common.utils contains utils for different stuff. Its name is tto
broad and we have a bunch of other modules called ""utils"". Name
collision produces chaos and aliases.
It would be nice if we can fix this issue and put all helpers to proper
modules.

Change-Id: Ic83b5f862ebd9e207a8d072ad74596275d561db9
",git fetch https://review.opendev.org/openstack/rally refs/changes/46/586046/1 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/common/verification/testr.py', 'tests/functional/test_cli_task.py', 'tests/unit/common/test_utils.py', 'tests/unit/utils/test_fileutils.py', 'tests/unit/plugins/common/verification/test_testr.py', 'rally/utils/fileutils.py', 'rally/common/utils.py', 'CHANGELOG.rst']",8,974dc8c3ef0c5b290279595ec687e7128bca4b44,functionalBug,Deprecated ~~~~~~~~~~ * Help method for generating random paths moved from ``rally.common.utils.generate_random_path`` to ``rally.utils.fileutils.generate_random_path``. The old location is deprecated. ,,97,38
openstack%2Frally~master~I0ba656080e5ac75e8de888fd2c833213125b59ea,openstack/rally,master,I0ba656080e5ac75e8de888fd2c833213125b59ea,[docs] refactor task component docs,ABANDONED,2017-03-21 11:58:48.000000000,2020-03-23 14:34:47.000000000,,"[{'_account_id': 13340}, {'_account_id': 14817}]","[{'number': 1, 'created': '2017-03-21 11:58:48.000000000', 'files': ['doc/source/task/howto/scenario_plugin.rst', 'doc/source/task/howto/sla_plugin.rst', 'doc/source/task/howto/hook_and_trigger_plugins.rst', 'doc/source/task/reports.rst', 'doc/source/task/cli_reference.rst', 'doc/source/plugins/index.rst', 'doc/source/task/howto/context_plugin.rst', 'doc/source/task/howto/index.rst', 'doc/source/task/howto/runner_plugin.rst', 'doc/source/task/index.rst'], 'web_link': 'https://opendev.org/openstack/rally/commit/6861b19190203a62256fef1d8eee6ab13b070c50', 'message': '[docs] refactor task component docs\n\nChange-Id: I0ba656080e5ac75e8de888fd2c833213125b59ea\n'}]",0,448027,6861b19190203a62256fef1d8eee6ab13b070c50,7,2,1,9545,,,0,"[docs] refactor task component docs

Change-Id: I0ba656080e5ac75e8de888fd2c833213125b59ea
",git fetch https://review.opendev.org/openstack/rally refs/changes/27/448027/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/task/howto/scenario_plugin.rst', 'doc/source/task/howto/sla_plugin.rst', 'doc/source/task/howto/hook_and_trigger_plugins.rst', 'doc/source/task/reports.rst', 'doc/source/plugins/index.rst', 'doc/source/task/cli_reference.rst', 'doc/source/task/howto/context_plugin.rst', 'doc/source/task/howto/index.rst', 'doc/source/task/howto/runner_plugin.rst', 'doc/source/task/index.rst']",10,6861b19190203a62256fef1d8eee6ab13b070c50,docs_task,".. Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Task Component is the most powerful thing in Rally. It allows to use simple test scenario (i.e boot server) and generate whatever you want workload with it (smoke run, a big real constant or RPC (requests-per-second) load, etc). .. toctree:: :maxdepth: 2 :glob: reports cli_reference howto/* ",".. _task-component:This section describes Rally Task Component (including feature presented since Rally v0.5.0, allowing to analyze statistics trends for the given tasks). .. contents:: :depth: 2 :local: HTML Reports ============ HTML reports provide comprehensive analysis. Data is structured and displayed interactively, with charts and tables. Task Report ----------- Get the whole information about task workloads results, in pretty and convenient format! .. image:: ../images/Report-Collage.png Generate report for single task, using task UUID ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Having a finished task, generate report with command: .. code-block:: shell $ rally task report <task-uuid> --out <report-file> Example: .. code-block:: shell $ rally task report 6f63d9ec-eecd-4696-8e9c-2ba065c68535 --out report.html Generate report for single task, using JSON file ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Report can be generated from a task results JSON file. This file can be generated with command *rally task results*: .. code-block:: shell $ rally task results 6f63d9ec-eecd-4696-8e9c-2ba065c68535 > results.json $ rally task report results.json --out report.html Generate report for many tasks ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Report can be generated from many tasks. All workloads from specified tasks results will be composed into an entire report. To generate report, use *--tasks* argument with specified list of tasks UUIDs and/or tasks results JSON files. Example: .. code-block:: shell $ rally task report --tasks 6f63d9ec-eecd-4696-8e9c-2ba065c68535 20ae7e95-7395-4be4-aec2-b89220adee60 a5737eba-a204-43d6-a262-d5ea4b0065da results.json another_results.json --out report.html Task Overview ~~~~~~~~~~~~~ This is a table with brief summary of all workloads results. All columns are sortable and clickable. .. image:: ../images/Report-Task-Overview.png Load duration +++++++++++++ Time from first iteration start to last iteration end. In other words, this is a time of all workload iterations execution. Full duration +++++++++++++ This time includes iterations time (`Load duration <#load-duration>`_) plus time taken by another actions related to the task, mostly Contexts execution time. Iterations ++++++++++ How many times the workload has run. This comes from the value of *runner.times* in task input file. Failures ++++++++ Number of failed iterations. Failure means that there was an Exception raised. Success (SLA) +++++++++++++ This is a boolean result of workload SLA. See `Service-level agreement explanation <#id2>`_ below. Input file ~~~~~~~~~~ This shows JSON which can be used to run a task with exactly the same workloads list and configuration. This is not an exact copy (neither concatenation) of actually used input files (in command *rally task start*), however this is exactly what is needed to run workloads given in the report. .. image:: ../images/Report-Task-Input-file.png Tab «Overview» ~~~~~~~~~~~~~~ Service-level agreement +++++++++++++++++++++++ `SLA`_ results appear in task report only if *""sla""* section is defined in task input file. For example, having this in task input file: .. code-block:: json ""sla"": { ""performance_degradation"": { ""max_degradation"": 50 }, ""max_seconds_per_iteration"": 1.0, ""failure_rate"": { ""max"": 0 }, ""outliers"": { ""max"": 1, ""min_iterations"": 10, ""sigmas"": 10 }, ""max_avg_duration"": 0.5 } will result SLA section similar to the following: .. image:: ../images/Report-Task-SLA.png What if workload has no ""sla"" configuration in input file? ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ If *""sla""* section is missed in input file, then block *Service-level agreement* is not displayed and its result is assumed to be always passed (no matter how many failures occurred). Total durations +++++++++++++++ There is a durations analysis, which is represented by statistics table and duration StackedArea chart. .. image:: ../images/Report-Task-Total-durations.png Table with statistics data ^^^^^^^^^^^^^^^^^^^^^^^^^^ **Action** Name of the workload metric that has some duration saved. This is either an atomic action name or *Total* which points to workload `load duration <#load-duration>`_. **Min (sec)** `Minimal`_ duration value **Median (sec)** `Median`_ duration value **90%ile (sec)** `Percentile`_ for 90% durations **95%ile (sec)** `Percentile`_ for 95% durations **Max (sec)** `Maximal`_ duration value **Avg (sec)** `Average`_ duration value **Success** Percent of successful runs. This is how many percent of this action runs (number of runs is given in *Count* column) were successful. **Count** Number of actually run atomic actions. This can differ from `iterations count <#iterations>`_ because some atomic actions do not start if some exception is raised before in the workload runtime (for example in previous atomic action). StackedArea with durations per iteration ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ This chart shows `load_duration <#load-duration>`_ and `idle_duration <#id5>`_ values per iteration. If there is only one iteration, then chart is useless so it is hidden. Idle duration ^^^^^^^^^^^^^ Sometimes workload does nothing for some reason (waiting for something or just making a dummy load). This is achieved by calling *time.sleep()* and spent time is called *idle duration*. Load Profile ++++++++++++ `Load profile`_ chart shows number of iterations running in parallel for each workload moment: .. image:: ../images/Report-Task-Load-profile.png Distribution ++++++++++++ Pie chart shows percent of successful and failed `iterations <#iterations>`_. Histogram shows durations distribution with the following `methods`_ (selected in dropdown list): **Square Root Choice**, **Sturges Formula**, **Rise Rule** .. image:: ../images/Report-Task-Distribution.png Tab «Details» ~~~~~~~~~~~~~ Atomic Action Durations +++++++++++++++++++++++ There is a StackedArea chart that shows atomic actions durations per iteration. If there is only one iteration, then chart is useless so it is hidden. .. image:: ../images/Report-Task-Actions-durations.png Distribution ++++++++++++ `Distribution <#distribution>`_ for atomic actions durations Tab «Scenario Data» ~~~~~~~~~~~~~~~~~~~ This tab only appears if workload provides some custom output via method *Scenario.add_output()*. Aggregated ++++++++++ This shows charts with data aggregated from all iterations. This means that each X axis point represents an iteration, so each iteration provided some values that are aggregated into charts or tables. .. image:: ../images/Report-Task-Scenario-Data-Aggregated.png Per iteration +++++++++++++ Each iteration can create its own, complete charts and tables. .. image:: ../images/Report-Task-Scenario-Data-Per-iteration.png Tab «Failures» ++++++++++++++ Complete information about exceptions raised during the workload run **Iteration** Number of iteration where exception is occurred **Exception type** Type of raised Exception subclass **Exception message** Message delivered by the exception Click on a row expands it with exception traceback. .. image:: ../images/Report-Task-Failures.png Tab «Input Task» ~~~~~~~~~~~~~~~~ This shows JSON for input file which can be used to run current workload. .. image:: ../images/Report-Task-Subtask-configuration.png Trends Report ------------- If same workload is run several times, some results of these runs can be compared. Compared metrics are ssuccess rate (percent of successful iterations) and statistics for durations. How to generate trends report ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Use command *rally task trends* with given tasks UUIDs and/or tasks results JSON files and the name of desired output file. Example: .. code-block:: shell $ rally task trends --tasks 6f63d9ec-eecd-4696-8e9c-2ba065c68535 a5737eba-a204-43d6-a262-d5ea4b0065da --out trends.html What is an order of workload runs? ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Workload run number in shown on charts X axis, the order of runs is exactly as it comes from tasks data in the moment of report generation. Trends overview ~~~~~~~~~~~~~~~ .. image:: ../images/Report-Trends-Overview.png If workload has been actually run only once ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ That is obvious that it is not possible to have trend for a single value. There should be at least two workload runs to make results comparison possible. So in this case there is only a help message displayed. .. image:: ../images/Report-Trends-single-run.png Tab «Total» ~~~~~~~~~~~ Total durations +++++++++++++++ Shows workload `load_duration <#load-duration>`_ statistics trends. Total success rate ++++++++++++++++++ Shows trends for percent of successful iterations .. image:: ../images/Report-Trends-Total.png Tab «Atomic actions» ++++++++++++++++++++ Statistics trends for atomic actions durations. Charts are same as for total durations. .. image:: ../images/Report-Trends-Atomic-actions.png Tab «Configuration» +++++++++++++++++++ Here is a configuration JSON for current workload. .. image:: ../images/Report-Trends-Configuration.png CLI References ============== For more information regarding Rally Task Component CLI please proceed to `CLI reference <../cli/cli_reference.html#category-task>`_ .. references: .. _SLA: https://en.wikipedia.org/wiki/Service-level_agreement .. _Minimal: https://en.wikipedia.org/wiki/Maxima_and_minima .. _Median: https://en.wikipedia.org/wiki/Median .. _Percentile: https://en.wikipedia.org/wiki/Percentile .. _Maximal: https://en.wikipedia.org/wiki/Maxima_and_minima .. _Average: https://en.wikipedia.org/wiki/Average .. _Load profile: https://en.wikipedia.org/wiki/Load_profile .. _methods: https://en.wikipedia.org/wiki/Histogram",427,376
openstack%2Frally~master~I0f5fc8bd43970c8f7fa3e46e40fc023b1ca7a0b2,openstack/rally,master,I0f5fc8bd43970c8f7fa3e46e40fc023b1ca7a0b2,[ci][cli] Install dependencies for specific db drivers,ABANDONED,2018-02-26 08:24:30.000000000,2020-03-23 14:34:33.000000000,,"[{'_account_id': 14817}, {'_account_id': 22348}]","[{'number': 1, 'created': '2018-02-26 08:24:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/093a0f5205a05c259faaa95759250af65e7b1c69', 'message': '[ci][cli] Install dependencies for specific db drivers\n\nWe are launching `tox -ecli` job on different environments (sqlite,\nmysql, postgres). As soon as we turned off site-packages for this job,\ndb drivers should be installed manually.\n\nThis patch adds a script which parces rally configuration file and\ninstalls required packages\n\nChange-Id: I0f5fc8bd43970c8f7fa3e46e40fc023b1ca7a0b2\n'}, {'number': 2, 'created': '2018-02-26 10:51:39.000000000', 'files': ['tests/check_samples/utils.py', 'tests/ci/pip_install.sh', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/rally/commit/7eea49f78d0ae8671dfebb22de9472568eaf93ae', 'message': '[ci][cli] Install dependencies for specific db drivers\n\nWe are launching `tox -ecli` job on different environments (sqlite,\nmysql, postgres). As soon as we turned off site-packages for this job,\ndb drivers should be installed manually.\n\nThis patch adds a script which parces rally configuration file and\ninstalls required packages\n\nChange-Id: I0f5fc8bd43970c8f7fa3e46e40fc023b1ca7a0b2\n'}]",0,547935,7eea49f78d0ae8671dfebb22de9472568eaf93ae,7,2,2,9545,,,0,"[ci][cli] Install dependencies for specific db drivers

We are launching `tox -ecli` job on different environments (sqlite,
mysql, postgres). As soon as we turned off site-packages for this job,
db drivers should be installed manually.

This patch adds a script which parces rally configuration file and
installs required packages

Change-Id: I0f5fc8bd43970c8f7fa3e46e40fc023b1ca7a0b2
",git fetch https://review.opendev.org/openstack/rally refs/changes/35/547935/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/ci/pip_install.sh', 'tox.ini']",2,093a0f5205a05c259faaa95759250af65e7b1c69,cli_tests,install_command = {toxinidir}/tests/ci/pip_install.sh -c ./upper-constraints.txt -U {opts} {packages},,15,0
openstack%2Frally~stable%2F0.10~Ibcb949ad98323f770bf20f144ad66b4a47c00e22,openstack/rally,stable/0.10,Ibcb949ad98323f770bf20f144ad66b4a47c00e22,[verify] Add support for sTestr,ABANDONED,2018-04-03 11:45:04.000000000,2020-03-23 14:33:47.000000000,,"[{'_account_id': 22348}, {'_account_id': 26025}]","[{'number': 1, 'created': '2018-04-03 11:45:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b783dc64d1e1974669738a91ab10fb4fd82fbc80', 'message': '[verify] Add support for sTestr\n\nTempest team switched from `testr` to `stestr` tool. `stestr` is\nmodified version of original `testr` and has the similar interface.\nThe support of a new tool can be easily integrated in the currect\nTestrRunner\n\nChange-Id: Ibcb949ad98323f770bf20f144ad66b4a47c00e22\n(cherry picked from commit dbf0329303b9e2137c040c66c8db0c54907d80e2)\n'}, {'number': 2, 'created': '2018-04-03 12:44:49.000000000', 'files': ['tests/ci/osresources.py', 'rally/plugins/common/verification/testr.py', 'tests/unit/plugins/common/verification/test_testr.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/ed854514ccc43b8b92863d58a5319a045c64dd75', 'message': '[verify] Add support for sTestr\n\nTempest team switched from `testr` to `stestr` tool. `stestr` is\nmodified version of original `testr` and has the similar interface.\nThe support of a new tool can be easily integrated in the currect\nTestrRunner\n\nChange-Id: Ibcb949ad98323f770bf20f144ad66b4a47c00e22\n(cherry picked from commit dbf0329303b9e2137c040c66c8db0c54907d80e2)\n'}]",1,558484,ed854514ccc43b8b92863d58a5319a045c64dd75,6,2,2,9545,,,0,"[verify] Add support for sTestr

Tempest team switched from `testr` to `stestr` tool. `stestr` is
modified version of original `testr` and has the similar interface.
The support of a new tool can be easily integrated in the currect
TestrRunner

Change-Id: Ibcb949ad98323f770bf20f144ad66b4a47c00e22
(cherry picked from commit dbf0329303b9e2137c040c66c8db0c54907d80e2)
",git fetch https://review.opendev.org/openstack/rally refs/changes/84/558484/2 && git format-patch -1 --stdout FETCH_HEAD,"['rally/plugins/common/verification/testr.py', 'tests/unit/plugins/common/verification/test_testr.py']",2,b783dc64d1e1974669738a91ab10fb4fd82fbc80,fix_tempest-stable/0.10," def assertEqualCmd(self, expected, actual, msg="""", stestr=False): cmd = [""stestr"" if stestr else ""testr"", ""run"", ""--subunit""] def test_setup_with_concurrency_stestr(self): self.verifier.manager._use_testr = False # default behaviour cfg = {""verifier"": self.verifier} ctx = testr.TestrContext(cfg) ctx.setup() self.assertEqualCmd([], cfg[""testr_cmd""], stestr=True) cfg = {""verifier"": self.verifier, ""run_args"": {""concurrency"": 0}} ctx = testr.TestrContext(cfg) ctx.setup() self.assertEqualCmd([], cfg[""testr_cmd""], stestr=True) # serial mode cfg = {""verifier"": self.verifier, ""run_args"": {""concurrency"": 1}} ctx = testr.TestrContext(cfg) ctx.setup() self.assertEqualCmd([""--serial""], cfg[""testr_cmd""], stestr=True) # parallel mode cfg = {""verifier"": self.verifier, ""run_args"": {""concurrency"": 2}} ctx = testr.TestrContext(cfg) ctx.setup() self.assertEqualCmd([""--concurrency"", ""2""], cfg[""testr_cmd""], stestr=True) def test_list_tests_via_stestr(self, mock_check_output): launcher._use_testr = False self.assertEqual([""tests.FooTestCase.test_something"", ""tests.BarTestCase.test_another[id=123]"", ""tests.FooTestCase.test_another[id=a2-213,smoke]""], launcher.list_tests()) mock_check_output.assert_called_once_with( [""stestr"", ""list"", """"], cwd=launcher.repo_dir, env=launcher.environ, debug_output=False) @mock.patch(""%s.utils.check_output"" % PATH) def test_list_tests_via_testr(self, mock_check_output): mock_check_output.return_value = ( ""logging message\n"" # should be ignored ""one more useless data\n"" # should be ignored ""tests.FooTestCase.test_something\n"" # valid ""tests.FooTestCase.test_another[\n"" # invalid ""tests.BarTestCase.test_another[id=123]\n"" # valid ""tests.FooTestCase.test_another[id=a2-213,smoke]\n"" # valid ) verifier = mock.Mock() launcher = testr.TestrLauncher(verifier) launcher._use_testr = True mock_exists.assert_called_once_with( os.path.join(launcher.repo_dir, "".testr"")) mock_exists.reset_mock() # case #3: initializing stestr without errors launcher._use_testr = False launcher._init_testr() mock_check_output.assert_called_once_with( [""stestr"", ""init""], cwd=launcher.repo_dir, env=launcher.environ) self.assertFalse(mock_exists.called) self.assertFalse(mock_rmtree.called) mock_check_output.reset_mock() # case #4: initializing testr with error [""stestr"", ""init""], cwd=launcher.repo_dir, env=launcher.environ)"," def assertEqualCmd(self, expected, actual, msg=""""): cmd = [""testr"", ""run"", ""--subunit""] def test_list_tests(self, mock_check_output): # case #3: initializing testr with error [""testr"", ""init""], cwd=launcher.repo_dir, env=launcher.environ)",101,12
openstack%2Frally~master~I22551a1f054c8ae5d9c7a9c2ff057d7275b9c76e,openstack/rally,master,I22551a1f054c8ae5d9c7a9c2ff057d7275b9c76e,Get rid of jsonschema for task format validation,ABANDONED,2017-08-18 17:46:36.000000000,2020-03-23 14:33:07.000000000,,"[{'_account_id': 6172}, {'_account_id': 14817}]","[{'number': 1, 'created': '2017-08-18 17:46:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ecfe5a363b28ccc6db136a6aa56f2e18ba10a07d', 'message': 'Get rid of jsonschema for task format validation\n\njsonschema is very powerful tool, but the error messages are not\nuser-friendly in case of complex schemas.\nTask file can be quite big, so it is realy easy to make a mistake\nsomewhere and good error messages can save the situation.\n\nThis patch switches the validation of Task Format V2 (which is out top\npriority) to the row python validation.\n\nNOTE: several lines in unit tests contains `# noqa` comment to turn off\n      the check of self.assertRaises do not accept Exception as s first\n      argument.\n\nChange-Id: I22551a1f054c8ae5d9c7a9c2ff057d7275b9c76e\n'}, {'number': 2, 'created': '2017-08-21 12:39:01.000000000', 'files': ['rally/task/engine.py', 'tests/unit/task/test_engine.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/16e3d82e02809eb5872bbd4e4f2ec116edae8cc1', 'message': 'Get rid of jsonschema for task format validation\n\njsonschema is very powerful tool, but the error messages are not\nuser-friendly in case of complex schemas.\nTask file can be quite big, so it is realy easy to make a mistake\nsomewhere and good error messages can save the situation.\n\nThis patch switches the validation of Task Format V2 (which is out top\npriority) to the row python validation.\n\nNOTE: several lines in unit tests contains `# noqa` comment to turn off\n      the check of self.assertRaises do not accept Exception as s first\n      argument.\n\nChange-Id: I22551a1f054c8ae5d9c7a9c2ff057d7275b9c76e\n'}]",0,495392,16e3d82e02809eb5872bbd4e4f2ec116edae8cc1,10,2,2,9545,,,0,"Get rid of jsonschema for task format validation

jsonschema is very powerful tool, but the error messages are not
user-friendly in case of complex schemas.
Task file can be quite big, so it is realy easy to make a mistake
somewhere and good error messages can save the situation.

This patch switches the validation of Task Format V2 (which is out top
priority) to the row python validation.

NOTE: several lines in unit tests contains `# noqa` comment to turn off
      the check of self.assertRaises do not accept Exception as s first
      argument.

Change-Id: I22551a1f054c8ae5d9c7a9c2ff057d7275b9c76e
",git fetch https://review.opendev.org/openstack/rally refs/changes/92/495392/1 && git format-patch -1 --stdout FETCH_HEAD,"['rally/task/engine.py', 'tests/unit/task/test_engine.py']",2,ecfe5a363b28ccc6db136a6aa56f2e18ba10a07d,task_results,"import copy @mock.patch(""rally.task.engine.TaskConfig._validate_task"") def test__validate_task_format_v1(self, mock_validate, mock__validate_task): mock_validate.assert_called_once_with( config, engine.TaskConfig.CONFIG_SCHEMA_V1) self.assertFalse(mock__validate_task.called) @mock.patch(""rally.task.engine.TaskConfig._validate_subtask"") def test__validate_task(self, mock_validate, mock__validate_subtask): subtask = mock.MagicMock() # min required config config = {""version"": 2, ""title"": ""foo"", ""subtasks"": [subtask]} mock__validate_subtask.assert_called_once_with(1, subtask) # all possible options config = {""version"": 2, ""title"": ""foo"", ""subtasks"": [subtask], ""tags"": [""tag1"", ""tag2""], ""description"": ""foo""} engine.TaskConfig(config) # negative case: wrong version config = {""version"": 3, ""title"": ""foo"", ""subtasks"": [subtask]} e = self.assertRaises(Exception, engine.TaskConfig, config) # noqa self.assertEqual(""Unsupported version of the Task config."", str(e)) config[""version""] = 2 # negative case: missed required keys for key in (""title"", ""subtasks""): cfg = copy.copy(config) cfg.pop(key) e = self.assertRaises(Exception, engine.TaskConfig, cfg) # noqa self.assertEqual(""The following properties are required for the "" ""Task, but missed: %s."" % key, str(e)) # negative case: redundant keys config[""foo""] = ""bar"" e = self.assertRaises(Exception, engine.TaskConfig, config) # noqa self.assertEqual(""The following properties of the Task are unexpected "" ""(redundant): foo."", str(e)) config.pop(""foo"") # negative case: wrong type for key, etype in [(""title"", ""string""), (""description"", ""string""), (""tags"", ""list(array)"")]: cfg = copy.copy(config) cfg[key] = True e = self.assertRaises(Exception, engine.TaskConfig, cfg) # noqa self.assertEqual( ""The property '%s' of the Task should be a %s, but "" ""'<type 'bool'>' is found."" % (key, etype), str(e)) # negative case: tags are in the wrong format config[""tags""] = [""tag1"", False] e = self.assertRaises(Exception, engine.TaskConfig, config) # noqa self.assertEqual(""One or several tags of the Task are in the wrong "" ""format (a string is expected)."", str(e)) config.pop(""tags"") # negative case: a list of subtasks are empty or not a list msg = ""The Task should contain a list of subtasks to execute."" config[""subtasks""] = [] e = self.assertRaises(Exception, engine.TaskConfig, config) # noqa self.assertEqual(msg, str(e)) config[""subtasks""] = {} e = self.assertRaises(Exception, engine.TaskConfig, config) # noqa self.assertEqual(msg, str(e)) # During all these checks the jsonschema's validate should not be # called since it is hardcoded for Task V1 format. self.assertFalse(mock_validate.called) @mock.patch(""rally.task.engine.TaskConfig._validate_workload"") def test__validate_subtask(self, mock__validate_workload): # mock all redundant checks :) class TaskConfig(engine.TaskConfig): def __init__(self): pass task = TaskConfig() # min required config workload = mock.Mock() config = {""title"": ""TestCase"", ""workloads"": [workload]} task._validate_subtask(2, config) mock__validate_workload.assert_called_once_with(1, 2, workload) # max allowed options config = {""title"": ""Testcase"", ""description"": ""Foo"", ""tags"": [""tag1"", ""tag2""], ""workloads"": [workload]} task._validate_subtask(2, config) # negative case: missed required keys for key in (""title"", ""workloads""): cfg = copy.copy(config) cfg.pop(key) e = self.assertRaises(Exception, task._validate_subtask, 2, cfg) # noqa self.assertEqual(""The following properties are required for the "" ""SubTask #2, but missed: %s."" % key, str(e)) # negative case: redundant keys config[""foo""] = ""bar"" e = self.assertRaises(Exception, task._validate_subtask, 2, config) # noqa self.assertEqual(""The following properties of the SubTask #2 are "" ""unexpected (redundant): foo."", str(e)) config.pop(""foo"") # negative case: wrong type for key, etype in [(""title"", ""string""), (""description"", ""string""), (""tags"", ""list(array)"")]: cfg = copy.copy(config) cfg[key] = True e = self.assertRaises(Exception, task._validate_subtask, 2, cfg) # noqa self.assertEqual( ""The property '%s' of the SubTask #2 should be a %s, but "" ""'<type 'bool'>' is found."" % (key, etype), str(e)) # negative case: tags are in the wrong format config[""tags""] = [""tag1"", False] e = self.assertRaises(Exception, task._validate_subtask, 3, config) # noqa self.assertEqual(""One or several tags of the SubTask #3 are in the "" ""wrong format (a string is expected)."", str(e)) config.pop(""tags"") # negative case: a list of workloads are empty or not a list msg = ""The SubTask #2 should contain a list of workloads to execute."" config[""workloads""] = [] e = self.assertRaises(Exception, task._validate_subtask, 2, config) # noqa self.assertEqual(msg, str(e)) config[""workloads""] = {} e = self.assertRaises(Exception, task._validate_subtask, 2, config) # noqa self.assertEqual(msg, str(e)) def test__validate_workload(self): # mock all redundant checks :) class TaskConfig(engine.TaskConfig): def __init__(self): pass task = TaskConfig() # min required config config = {""name"": ""Foo.bar"", ""runner"": {""type"": ""some""}} task._validate_workload(2, 3, config) # max allowed options config = {""name"": ""Foo.bar"", ""description"": ""do not use jsonschema for validating data "" ""from the end user, it prints too "" ""complicated error messages."", ""runner"": {""type"": ""some""}, ""args"": {""key1"": ""value1"", ""key2"": ""value2""}, ""context"": {""users"": {}}, ""sla"": {""some"": ""thing""}, ""hooks"": [{""name"": ""xxx"", ""args"": {}, ""trigger"": {""name"": ""yyy"", ""args"": {}}}]} task._validate_workload(2, 3, config) # negative case: missed required keys for key in (""name"", ""runner""): cfg = copy.copy(config) cfg.pop(key) e = self.assertRaises(Exception, task._validate_workload, 2, 3, cfg) # noqa self.assertEqual(""The following properties are required for the "" ""Workload #2 (SubTask #3), but missed: %s."" % key, str(e)) # negative case: redundant keys config[""foo""] = ""bar"" e = self.assertRaises(Exception, task._validate_workload, 2, 3, config) # noqa self.assertEqual(""The following properties of the Workload #2 "" ""(SubTask #3) are unexpected (redundant): foo."", str(e)) config.pop(""foo"") # negative case: wrong type for key, etype in [(""name"", ""string""), (""description"", ""string""), (""context"", ""dictionary""), (""args"", ""dictionary""), (""runner"", ""dictionary""), (""sla"", ""dictionary""), (""hooks"", ""list(array)"")]: cfg = copy.copy(config) cfg[key] = True e = self.assertRaises(Exception, task._validate_workload, 2, 3, cfg) # noqa self.assertEqual( ""The property '%s' of the Workload #2 (SubTask #3) should be a"" "" %s, but '<type 'bool'>' is found."" % (key, etype), str(e)) # negative case: missed runner's type config[""runner""] = {""args"": []} e = self.assertRaises(Exception, task._validate_workload, 2, 3, config) # noqa self.assertEqual(""The type of runner is missed in the Workload #2 "" ""(SubTask #3)."", str(e)) config[""runner""] = {""type"": ""some""} # negative case: wrong hook config config[""hooks""] = [{""foo"": ""bar""}] e = self.assertRaises(Exception, task._validate_workload, 2, 3, config) # noqa self.assertIn(""Validation of the hook #1 (Workload #2; SubTask #3) "" ""failed due to: Additional properties are not allowed"" "" ('foo' was unexpected)"", str(e)) @mock.patch(""rally.task.engine.TaskConfig._validate_task_format_v1"") def test__adopt_task_format_v1(self, mock__validate_task_format_v1):"," def test_validate_json(self, mock_validate): mock_validate.assert_has_calls([ mock.call(config, engine.TaskConfig.CONFIG_SCHEMA_V1)]) def test_validate_json_v2(self, mock_validate): config = {""version"": 2, ""subtasks"": []} mock_validate.assert_has_calls([ mock.call(config, engine.TaskConfig.CONFIG_SCHEMA_V2)]) @mock.patch(""rally.task.engine.TaskConfig._get_version"") @mock.patch(""rally.task.engine.TaskConfig._validate_json"") def test_validate_version(self, mock_task_config__validate_json, mock_task_config__get_version): mock_task_config__get_version.return_value = 1 engine.TaskConfig(mock.MagicMock()) @mock.patch(""rally.task.engine.TaskConfig._get_version"") @mock.patch(""rally.task.engine.TaskConfig._validate_json"") def test_validate_version_wrong_version( self, mock_task_config__validate_json, mock_task_config__get_version): mock_task_config__get_version.return_value = ""wrong"" self.assertRaises(exceptions.InvalidTaskException, engine.TaskConfig, mock.MagicMock) @mock.patch(""rally.task.engine.TaskConfig._get_version"") @mock.patch(""rally.task.engine.TaskConfig._validate_json"") def test__adopt_task_format_v1( self, mock_task_config__validate_json, mock_task_config__get_version): mock_task_config__get_version.return_value = 1",398,118
openstack%2Ftripleo-common~stable%2Ftrain~I59f89730e6ed52e4ca54dbd19fcbf6aea0506450,openstack/tripleo-common,stable/train,I59f89730e6ed52e4ca54dbd19fcbf6aea0506450,Fix path of log file.,MERGED,2020-03-19 09:46:05.000000000,2020-03-23 14:32:54.000000000,2020-03-20 23:14:53.000000000,"[{'_account_id': 8297}, {'_account_id': 14985}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 26343}, {'_account_id': 27141}, {'_account_id': 28522}, {'_account_id': 31245}]","[{'number': 1, 'created': '2020-03-19 09:46:05.000000000', 'files': ['workbooks/package_update.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/92879675cd83d73b42f273b040722367f9bad580', 'message': ""Fix path of log file.\n\nIncorrect log file location afer node upgrade failed. The ansible.log\nfile doesn't exists.\n\nChange-Id: I59f89730e6ed52e4ca54dbd19fcbf6aea0506450\n""}]",1,713810,92879675cd83d73b42f273b040722367f9bad580,19,9,1,31245,,,0,"Fix path of log file.

Incorrect log file location afer node upgrade failed. The ansible.log
file doesn't exists.

Change-Id: I59f89730e6ed52e4ca54dbd19fcbf6aea0506450
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/10/713810/1 && git format-patch -1 --stdout FETCH_HEAD,['workbooks/package_update.yaml'],1,92879675cd83d73b42f273b040722367f9bad580,train," message: Ansible failed, check log at <% $.work_dir %>/<% execution().id %>/package_update.log. message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/package_update.log."," message: Ansible failed, check log at <% $.work_dir %>/<% execution().id %>/ansible.log. message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.",2,2
openstack%2Frally~master~If6f94e3805afbb735cd16530f68e4004f5bea850,openstack/rally,master,If6f94e3805afbb735cd16530f68e4004f5bea850,[Docs] several improvements,ABANDONED,2016-01-01 12:54:41.000000000,2020-03-23 14:32:29.000000000,,[{'_account_id': 14817}],"[{'number': 1, 'created': '2016-01-01 12:54:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b54a797159c783cb310907aca774f46050bdbd0b', 'message': '[Docs] several improvements\n\n - changes py26 to py34 in `Contribute to Rally` page\n - add substitution for |rally| => **Rally** to Overview\n   TODO: share this substitutuion accross all pages\n\nChange-Id: If6f94e3805afbb735cd16530f68e4004f5bea850\n'}, {'number': 2, 'created': '2016-01-04 10:52:53.000000000', 'files': ['doc/source/overview.rst', 'doc/source/contribute.rst', 'tests/README.rst'], 'web_link': 'https://opendev.org/openstack/rally/commit/58089b87aa62237e2e3daa2b064dbe75f28a0f5c', 'message': '[Docs] several improvements\n\n - changes py26 to py34 in `Contribute to Rally` page\n - add substitution for |rally| => **Rally** to Overview\n   TODO: share this substitutuion accross all pages\n\nChange-Id: If6f94e3805afbb735cd16530f68e4004f5bea850\n'}]",0,262860,58089b87aa62237e2e3daa2b064dbe75f28a0f5c,9,1,2,9545,,,0,"[Docs] several improvements

 - changes py26 to py34 in `Contribute to Rally` page
 - add substitution for |rally| => **Rally** to Overview
   TODO: share this substitutuion accross all pages

Change-Id: If6f94e3805afbb735cd16530f68e4004f5bea850
",git fetch https://review.opendev.org/openstack/rally refs/changes/60/262860/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/overview.rst', 'doc/source/contribute.rst']",2,b54a797159c783cb310907aca774f46050bdbd0b,docs,"To run py27, py34 or pep8 only: #NOTE: <name> is one of py27, py34 or pep8","To run py26, py27 or pep8 only: #NOTE: <name> is one of py26, py27 or pep8",20,18
openstack%2Fkayobe~master~I0e88683f775769c1a80879685b0e7a2983599b08,openstack/kayobe,master,I0e88683f775769c1a80879685b0e7a2983599b08,CentOS 8: Enable overcloud upgrade job,MERGED,2020-03-12 15:58:00.000000000,2020-03-23 14:31:12.000000000,2020-03-23 14:24:54.000000000,"[{'_account_id': 14826}, {'_account_id': 15197}, {'_account_id': 17669}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-12 15:58:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/07a365552d3ade45b909b1ca1f2e79eadebb665d', 'message': 'CentOS 8: Enable upgrade jobs\n\nDepends-On: https://review.opendev.org/711067\n\nChange-Id: I0e88683f775769c1a80879685b0e7a2983599b08\nStory: 2006574\nTask: 39047\n'}, {'number': 2, 'created': '2020-03-13 10:42:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/bc094b27bf3d9c0cc12afa090f0cd52fc6376e35', 'message': 'CentOS 8: Enable overcloud upgrade job\n\nMissing bifrost image for Train, so no seed upgrade job yet.\n\nDepends-On: https://review.opendev.org/711067\n\nChange-Id: I0e88683f775769c1a80879685b0e7a2983599b08\nStory: 2006574\nTask: 39047\n'}, {'number': 3, 'created': '2020-03-23 10:33:15.000000000', 'files': ['playbooks/kayobe-overcloud-upgrade-base/overrides.yml.j2', 'zuul.d/project.yaml', 'playbooks/kayobe-overcloud-upgrade-base/pre.yml'], 'web_link': 'https://opendev.org/openstack/kayobe/commit/a28fd66b6e0eee1e79f07f8b2ecab8853958ef47', 'message': 'CentOS 8: Enable overcloud upgrade job\n\nMissing bifrost image for Train, so no seed upgrade job yet.\n\nDepends-On: https://review.opendev.org/711067\n\nChange-Id: I0e88683f775769c1a80879685b0e7a2983599b08\nStory: 2006574\nTask: 39047\n'}]",0,712723,a28fd66b6e0eee1e79f07f8b2ecab8853958ef47,24,4,3,14826,,,0,"CentOS 8: Enable overcloud upgrade job

Missing bifrost image for Train, so no seed upgrade job yet.

Depends-On: https://review.opendev.org/711067

Change-Id: I0e88683f775769c1a80879685b0e7a2983599b08
Story: 2006574
Task: 39047
",git fetch https://review.opendev.org/openstack/kayobe refs/changes/23/712723/3 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/project.yaml'],1,07a365552d3ade45b909b1ca1f2e79eadebb665d,, - kayobe-overcloud-upgrade-centos8 - kayobe-seed-upgrade-centos8 - kayobe-overcloud-upgrade-centos8 - kayobe-seed-upgrade-centos, # TODO(mgoddard): Enable upgrade jobs when CentOS 8 support in Train. # - kayobe-overcloud-upgrade-centos8 # TODO(mgoddard): Enable upgrade jobs when CentOS 8 support in Train. # - kayobe-seed-upgrade-centos8 # TODO(mgoddard): Enable upgrade jobs when CentOS 8 support in Train. # - kayobe-overcloud-upgrade-centos8 # TODO(mgoddard): Enable upgrade jobs when CentOS 8 support in Train. # - kayobe-seed-upgrade-centos,4,8
openstack%2Fopenstack-helm-infra~master~Ib2e16e08ec20d24924c14fe80927d8180ede06d0,openstack/openstack-helm-infra,master,Ib2e16e08ec20d24924c14fe80927d8180ede06d0,Fix Grafana Selenium tests,MERGED,2020-03-06 12:43:44.000000000,2020-03-23 14:27:06.000000000,2020-03-23 14:22:46.000000000,"[{'_account_id': 7769}, {'_account_id': 8898}, {'_account_id': 12281}, {'_account_id': 17591}, {'_account_id': 20466}, {'_account_id': 21420}, {'_account_id': 22348}, {'_account_id': 22477}, {'_account_id': 23928}, {'_account_id': 30777}]","[{'number': 1, 'created': '2020-03-06 12:43:44.000000000', 'files': ['tools/gate/selenium/grafanaSelenium.py'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/0d2f62a3ed17b33ad222cca4466e0039665861a2', 'message': 'Fix Grafana Selenium tests\n\nSince grafana values_overrides were added we need\nto align Selenium tests as well.\n\nChange-Id: Ib2e16e08ec20d24924c14fe80927d8180ede06d0\n'}]",0,711614,0d2f62a3ed17b33ad222cca4466e0039665861a2,8,10,1,28735,,,0,"Fix Grafana Selenium tests

Since grafana values_overrides were added we need
to align Selenium tests as well.

Change-Id: Ib2e16e08ec20d24924c14fe80927d8180ede06d0
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/14/711614/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/gate/selenium/grafanaSelenium.py'],1,0d2f62a3ed17b33ad222cca4466e0039665861a2,, st.click_link_by_name('OSH Home'), st.click_link_by_name('Home'),1,1
openstack%2Fcinder~stable%2Fqueens~Ib64c86fa5e3791dfa5386b58b468a91f1a190de6,openstack/cinder,stable/queens,Ib64c86fa5e3791dfa5386b58b468a91f1a190de6,solidfire: Enable SSL with requests,MERGED,2019-08-07 16:29:00.000000000,2020-03-23 14:23:34.000000000,2019-10-07 15:03:49.000000000,"[{'_account_id': 24}, {'_account_id': 5314}, {'_account_id': 7198}, {'_account_id': 10118}, {'_account_id': 12369}, {'_account_id': 19933}, {'_account_id': 21767}, {'_account_id': 21863}, {'_account_id': 21976}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23613}, {'_account_id': 24921}, {'_account_id': 26077}, {'_account_id': 26537}, {'_account_id': 29716}]","[{'number': 1, 'created': '2019-08-07 16:29:00.000000000', 'files': ['cinder/volume/drivers/solidfire.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/84b024bbcf446e1294850df23aab899aac5c83c6', 'message': 'solidfire: Enable SSL with requests\n\nSSL requests are not being verified when contacting\nthe REST API. Use the driver_ssl_cert_verify\nconfig option to turn on or off. Defaults to False.\n\nChange-Id: Ib64c86fa5e3791dfa5386b58b468a91f1a190de6\nSigned-off-by: Chuck Short <chucks@redhat.com>\n(cherry picked from commit 61eeb626e67942f77f41e8589c2dd1f9a6c75345)\n'}]",0,675146,84b024bbcf446e1294850df23aab899aac5c83c6,22,16,1,6593,,,0,"solidfire: Enable SSL with requests

SSL requests are not being verified when contacting
the REST API. Use the driver_ssl_cert_verify
config option to turn on or off. Defaults to False.

Change-Id: Ib64c86fa5e3791dfa5386b58b468a91f1a190de6
Signed-off-by: Chuck Short <chucks@redhat.com>
(cherry picked from commit 61eeb626e67942f77f41e8589c2dd1f9a6c75345)
",git fetch https://review.opendev.org/openstack/cinder refs/changes/46/675146/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/drivers/solidfire.py'],1,84b024bbcf446e1294850df23aab899aac5c83c6,," self.verify_ssl = self.configuration.driver_ssl_cert_verify verify=self.verify_ssl,"," verify=False,",2,1
openstack%2Fwhitebox-tempest-plugin~master~Ic64d49761fe57cff3c1ecbd5f00ae608f53ae247,openstack/whitebox-tempest-plugin,master,Ic64d49761fe57cff3c1ecbd5f00ae608f53ae247,Add tests for virtio scsi disks,MERGED,2019-12-18 20:13:54.000000000,2020-03-23 14:23:32.000000000,2020-03-23 14:23:32.000000000,"[{'_account_id': 7020}, {'_account_id': 8864}, {'_account_id': 10135}, {'_account_id': 11604}, {'_account_id': 22348}, {'_account_id': 27478}, {'_account_id': 31033}]","[{'number': 1, 'created': '2019-12-18 20:13:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/aa3d738f3df6721355c4830e984659d1b6c5dd40', 'message': 'Add tests for virtio scsi disks\n\nThis commit adds testcases to validate behavior when booting instances\nwith additional virtio scsi disks or when booting guests with multiple\nscsi disks. Test class is VirtioSCSIDisk which contains to two test\nmethods, test_boot_with_multiple_disks and\ntest_attach_multiple_scsi_disks. The test_boot testcase verifies an\ninstance can be deployed with seven instances attached to the same\ncontroller. The test_attach method verifies attaching seven instances\nto an active instance works successfully. The total volumes created are\ngoverned by the class variable disks_to_create. VirtioSCSIDisk also has\nmultiple helper methods that parse instance xml gather total disk count,\ntotal scsi disk count, and scsi controller information.\n\nChange-Id: Ic64d49761fe57cff3c1ecbd5f00ae608f53ae247\n'}, {'number': 2, 'created': '2019-12-18 21:18:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/8efd220ef4317ce154a6622436c2c5c3930b4a57', 'message': 'Add tests for virtio scsi disks\n\nThis commit adds testcases to validate behavior when booting instances\nwith additional virtio scsi disks or when booting guests with multiple\nscsi disks. Test class is VirtioSCSIDisk which contains to two test\nmethods, test_boot_with_multiple_disks and\ntest_attach_multiple_scsi_disks. The test_boot testcase verifies an\ninstance can be deployed with seven instances attached to the same\ncontroller. The test_attach method verifies attaching seven instances\nto an active instance works successfully. The total volumes created are\ngoverned by the class variable disks_to_create. VirtioSCSIDisk also has\nmultiple helper methods that parse instance xml gather total disk count,\ntotal scsi disk count, and scsi controller information.\n\nChange-Id: Ic64d49761fe57cff3c1ecbd5f00ae608f53ae247\n'}, {'number': 3, 'created': '2019-12-18 23:19:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/59b8f7930084130f836adf9d53ab0965932ece12', 'message': 'Add tests for virtio scsi disks\n\nThis commit adds testcases to validate behavior when booting instances\nwith additional virtio scsi disks or when booting guests with multiple\nscsi disks. Test class is VirtioSCSIDisk which contains to two test\nmethods, test_boot_with_multiple_disks and\ntest_attach_multiple_scsi_disks. The test_boot testcase verifies an\ninstance can be deployed with seven instances attached to the same\ncontroller. The test_attach method verifies attaching seven instances\nto an active instance works successfully. The total volumes created are\ngoverned by the class variable disks_to_create. VirtioSCSIDisk also has\nmultiple helper methods that parse instance xml gather total disk count,\ntotal scsi disk count, and scsi controller information.\n\nChange-Id: Ic64d49761fe57cff3c1ecbd5f00ae608f53ae247\n'}, {'number': 4, 'created': '2019-12-18 23:25:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/c4f3ac10c4dfe2f2bf592f36e228facff9c7a712', 'message': 'Add tests for virtio scsi disks\n\nThis commit adds testcases to validate behavior when attaching multiple\nscsi disks to an active instance or when deploying a guest with multiple\nscsi disks. Test class is VirtioSCSIDisk which contains to two test\nmethods, test_boot_with_multiple_disks and\ntest_attach_multiple_scsi_disks. The test_boot testcase verifies an\ninstance can be deployed with seven instances attached to the same\ncontroller. The test_attach method verifies attaching seven instances to\nan active instance works successfully. The total volumes created are\ngoverned by the class variable disks_to_create. VirtioSCSIDisk also has\nmultiple helper methods that parse instance xml gather total disk count,\ntotal scsi disk count, and scsi controller information.\n\nChange-Id: Ic64d49761fe57cff3c1ecbd5f00ae608f53ae247\n'}, {'number': 5, 'created': '2020-03-20 14:09:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/80adc8c0ca1b2d54b9e567695dd363c4e30d66e8', 'message': 'Add tests for virtio scsi disks\n\nThis commit adds testcases to validate behavior when attaching multiple\nscsi disks to an active instance or when deploying a guest with multiple\nscsi disks. Test class is VirtioSCSIDisk which contains to two test\nmethods, test_boot_with_multiple_disks and\ntest_attach_multiple_scsi_disks. The test_boot testcase verifies an\ninstance can be deployed with seven volumes attached to the same\ncontroller. The test_attach method verifies attaching seven instances to\nan active instance works successfully. The total volumes created are\ngoverned by the class variable disks_to_create. To validate RFE [1] and\nensure BZ [2] are operating correctly the value for disks_to_create is\nset to seven. VirtioSCSIDisk also has multiple helper methods that parse\ninstance xml gather total disk count, total scsi disk count, and scsi\ncontroller information.\n\nAlso added the config parameter available_cinder_storage to whitebox\ngroup in config.py. The two test methods use that parameter as a skip\ncheck to validate the deployment has enough storage to create all of the\nnecessary volumes.\n\n[1] https://bugzilla.redhat.com/show_bug.cgi?id=1269577\n[2] https://bugzilla.redhat.com/show_bug.cgi?id=1741782\n\nChange-Id: Ic64d49761fe57cff3c1ecbd5f00ae608f53ae247\n'}, {'number': 6, 'created': '2020-03-20 14:33:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/c9d8d4d2e0f7b63333aee0d02a71624cda3ceb41', 'message': 'Add tests for virtio scsi disks\n\nThis commit adds testcases to validate behavior when attaching multiple\nscsi disks to an active instance or when deploying a guest with multiple\nscsi disks. Test class is VirtioSCSIDisk which contains to two test\nmethods, test_boot_with_multiple_disks and\ntest_attach_multiple_scsi_disks. The test_boot testcase verifies an\ninstance can be deployed with seven volumes attached to the same\ncontroller. The test_attach method verifies attaching seven instances to\nan active instance works successfully. The total volumes created are\ngoverned by the class variable disks_to_create. To validate RFE [1] and\nensure BZ [2] are operating correctly the value for disks_to_create is\nset to seven. VirtioSCSIDisk also has multiple helper methods that parse\ninstance xml gather total disk count, total scsi disk count, and scsi\ncontroller information.\n\nAlso added the config parameter available_cinder_storage to whitebox\ngroup in config.py. The two test methods use that parameter as a skip\ncheck to validate the deployment has enough storage to create all of the\nnecessary volumes.\n\n[1] https://bugzilla.redhat.com/show_bug.cgi?id=1269577\n[2] https://bugzilla.redhat.com/show_bug.cgi?id=1741782\n\nChange-Id: Ic64d49761fe57cff3c1ecbd5f00ae608f53ae247\n'}, {'number': 7, 'created': '2020-03-20 16:59:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/45c78cc8f0713fca4573e83bbfadf542a70a751c', 'message': 'Add tests for virtio scsi disks\n\nThis commit adds testcases to validate behavior when attaching multiple\nscsi disks to an active instance or when deploying a guest with multiple\nscsi disks. Test class is VirtioSCSIDisk which contains to two test\nmethods, test_boot_with_multiple_disks and\ntest_attach_multiple_scsi_disks. The test_boot testcase verifies an\ninstance can be deployed with seven volumes attached to the same\ncontroller. The test_attach method verifies attaching seven volumes to\nan active instance works successfully. The total volumes created are\ngoverned by the class variable disks_to_create. To validate RFE [1] and\nensure BZ [2] are operating correctly the value for disks_to_create is\nset to seven. VirtioSCSIDisk also has multiple helper methods that parse\ninstance xml gather total disk count, total scsi disk count, and scsi\ncontroller information.\n\nAlso added the config parameter available_cinder_storage to the whitebox\ngroup in config.py. The two test methods use that parameter as a skip\ncheck to validate the deployment has enough storage to create all of the\nnecessary volumes.\n\nTo work with zuul deployments the devstack/plugin.sh and\ndevstack/settings where updated with a new parameter\nWHITEBOX_AVAILABLE_CINDER_STORAGE to pass to the tempest.cfg. The\ncurrent value passed to the parameter is 24, based on default value\npassed to VOLUME_BACKING_FILE_SIZE in stackrc [3].\n\n[1] https://bugzilla.redhat.com/show_bug.cgi?id=1269577\n[2] https://bugzilla.redhat.com/show_bug.cgi?id=1741782\n[3] https://github.com/openstack/devstack/blob/master/stackrc#L779\n\nChange-Id: Ic64d49761fe57cff3c1ecbd5f00ae608f53ae247\n'}, {'number': 8, 'created': '2020-03-20 21:08:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/23b05ff7a8236fc4d32fad13c95d1629637366fe', 'message': 'Add tests for virtio scsi disks\n\nThis commit adds testcases to validate behavior when attaching or\ndeploying a guest with multiple scsi disks beyond six. The reason it\nneeds to be six or more is due to the verification of Nova Bug #1686116\n[1]. The Test class created is VirtioSCSIDisk which contains to two test\nmethods, test_boot_with_multiple_disks and\ntest_attach_multiple_scsi_disks. The test_boot testcase verifies an\ninstance can be deployed with seven volumes attached to the same\ncontroller. The test_attach method attachs disks to an instance until it\nhas more than six virtio-scsi disks. The total volumes created are\ngoverned by the class variable disks_to_create. VirtioSCSIDisk also has\nmultiple helper methods that parse instance xml, gather total disk count,\ntotal scsi disk count, and scsi controller information.\n\nAlso added the config parameter available_cinder_storage to the whitebox\ngroup in config.py. The two test methods use that parameter as a skip\ncheck to validate the deployment has enough storage to create all of the\nnecessary volumes.\n\nTo work with zuul deployments the devstack/plugin.sh and\ndevstack/settings where updated with a new parameter\nWHITEBOX_AVAILABLE_CINDER_STORAGE to pass to the tempest.cfg. The\ncurrent value passed to the parameter is 24, based on default value\npassed to VOLUME_BACKING_FILE_SIZE in stackrc.\n\n[1] https://bugs.launchpad.net/nova/+bug/1686116\n[2] https://github.com/openstack/devstack/blob/master/stackrc#L779\n\nChange-Id: Ic64d49761fe57cff3c1ecbd5f00ae608f53ae247\n'}, {'number': 9, 'created': '2020-03-23 12:04:45.000000000', 'files': ['whitebox_tempest_plugin/api/compute/test_virtio_scsi_attach.py', 'devstack/plugin.sh', 'whitebox_tempest_plugin/config.py', 'devstack/settings'], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/6b455f905401521b4f4848cfc5789fdda156d33b', 'message': 'Add tests for virtio scsi disks\n\nThis commit adds testcases to validate behavior when attaching or\ndeploying a guest with multiple scsi disks beyond six. The reason it\nneeds to be six or more is due to the verification of Nova Bug #1686116\n[1]. The Test class created is VirtioSCSIDisk which contains to two test\nmethods, test_boot_with_multiple_disks and\ntest_attach_multiple_scsi_disks. The test_boot testcase verifies an\ninstance can be deployed with seven volumes attached to the same\ncontroller. The test_attach method attachs disks to an instance until it\nhas more than six virtio-scsi disks. The total volumes created are\ngoverned by the class variable disks_to_create. VirtioSCSIDisk also has\nmultiple helper methods that parse instance xml, gather total disk count,\ntotal scsi disk count, and scsi controller information.\n\nAlso added the config parameter available_cinder_storage to the whitebox\ngroup in config.py. The two test methods use that parameter as a skip\ncheck to validate the deployment has enough storage to create all of the\nnecessary volumes.\n\nTo work with zuul deployments the devstack/plugin.sh and\ndevstack/settings where updated with a new parameter\nWHITEBOX_AVAILABLE_CINDER_STORAGE to pass to the tempest.cfg. The\ncurrent value passed to the parameter is 24, based on default value\npassed to VOLUME_BACKING_FILE_SIZE in stackrc.\n\n[1] https://bugs.launchpad.net/nova/+bug/1686116\n[2] https://github.com/openstack/devstack/blob/master/stackrc#L779\n\nChange-Id: Ic64d49761fe57cff3c1ecbd5f00ae608f53ae247\n'}]",66,699864,6b455f905401521b4f4848cfc5789fdda156d33b,31,7,9,31033,,,0,"Add tests for virtio scsi disks

This commit adds testcases to validate behavior when attaching or
deploying a guest with multiple scsi disks beyond six. The reason it
needs to be six or more is due to the verification of Nova Bug #1686116
[1]. The Test class created is VirtioSCSIDisk which contains to two test
methods, test_boot_with_multiple_disks and
test_attach_multiple_scsi_disks. The test_boot testcase verifies an
instance can be deployed with seven volumes attached to the same
controller. The test_attach method attachs disks to an instance until it
has more than six virtio-scsi disks. The total volumes created are
governed by the class variable disks_to_create. VirtioSCSIDisk also has
multiple helper methods that parse instance xml, gather total disk count,
total scsi disk count, and scsi controller information.

Also added the config parameter available_cinder_storage to the whitebox
group in config.py. The two test methods use that parameter as a skip
check to validate the deployment has enough storage to create all of the
necessary volumes.

To work with zuul deployments the devstack/plugin.sh and
devstack/settings where updated with a new parameter
WHITEBOX_AVAILABLE_CINDER_STORAGE to pass to the tempest.cfg. The
current value passed to the parameter is 24, based on default value
passed to VOLUME_BACKING_FILE_SIZE in stackrc.

[1] https://bugs.launchpad.net/nova/+bug/1686116
[2] https://github.com/openstack/devstack/blob/master/stackrc#L779

Change-Id: Ic64d49761fe57cff3c1ecbd5f00ae608f53ae247
",git fetch https://review.opendev.org/openstack/whitebox-tempest-plugin refs/changes/64/699864/9 && git format-patch -1 --stdout FETCH_HEAD,['whitebox_tempest_plugin/api/compute/test_virtio_scsi_attach.py'],1,aa3d738f3df6721355c4830e984659d1b6c5dd40,virtio_scsi,"# Copyright 2015 Intel Corporation # Copyright 2018 Red Hat Inc. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """""" Tests for CPU pinning and CPU thread pinning policies. Based on tests for the Intel NFV CI. For more information, refer to: - https://wiki.openstack.org/wiki/ThirdPartySystems/Intel_NFV_CI - https://github.com/openstack/intel-nfv-ci-tests """""" from tempest import config from whitebox_tempest_plugin.api.compute import base from oslo_log import log as logging CONF = config.CONF LOG = logging.getLogger(__name__) # TODO(jparker) Add checks for 1GB size and LVM backend class VirtioSCSIDisk(base.BaseWhiteboxComputeTest): # NOTE: This test creates seven 1G volumes. To run it successfully, # ensure that the backing file for the volume group that Cinder uses # has space for at least seven 1G volumes! disks_to_create = 7 def get_attached_disks(self, server_id): """"""Returns all disk devices attached to the server """""" root = self.get_server_xml(server_id) disks = root.findall(""./devices/disk"") return disks def get_scsi_disks(self, server_id, index): """"""Returns all scsi disks attached to a specific disk controller for the server """""" all_disks = self.get_attached_disks(server_id) scsi_disks = [disk for disk in all_disks if disk.find(""target[@bus='scsi']"") is not None and disk.find(""address[@controller='{}']"".format(index)) is not None] return scsi_disks def get_scsi_disk_controller(self, server_id): """"""Returns all scsi disk controllers for the server """""" root = self.get_server_xml(server_id) disk_cntrl = root.findall(""./devices/controller[@type='scsi']"" ""[@model='virtio-scsi']"") return disk_cntrl def get_created_vol_ids(self): """"""Get the ids of every volume created for the test """""" vol_ids = [vol['id'] for vol in self.volumes_client.list_volumes()['volumes']] return vol_ids def get_all_serial_ids(self, disks): """"""Create a list of serial ids from a list of disks """""" serial_ids = [disk.find('serial').text for disk in disks if getattr(disk.find('serial'), 'text', None) is not None] return serial_ids def test_boot_with_multiple_disks(self): """"""Using block device mapping boot an instance with multiple volumes. Volume count is determined by instance variable disks_to_create. When the guest boots, it should only have one disk controller and all disks created should be present in guest xml. """""" bdm = [] for idx in range(self.disks_to_create): boot_dict = {} if idx == 0: boot_dict['uuid'] = CONF.compute.image_ref boot_dict['source_type'] = 'image' else: boot_dict['source_type'] = 'blank' boot_dict.update({ 'destination_type': 'volume', 'volume_size': 1, 'boot_index': idx, 'disk_bus': 'scsi', 'delete_on_termination': True }) bdm.append(boot_dict) flavor = self.create_flavor() server = self.create_test_server(flavor=flavor['id'], block_device_mapping_v2=bdm) disk_ctrl = self.get_scsi_disk_controller(server_id=server['id']) self.assertEqual(len(disk_ctrl), 1, ""One and only one SCSI Disk controller should have"" "" been created but instead"" "" found: {} controllers"".format(len(disk_ctrl))) cntrl_index = disk_ctrl[0].attrib['index'] scsi_disks = self.get_scsi_disks(server_id=server['id'], index=cntrl_index) self.assertEqual(len(scsi_disks), self.disks_to_create, ""Expected {} scsi disks on the domain but"" "" found {}"".format(self.disks_to_create, len(scsi_disks))) vol_ids = self.get_created_vol_ids() vol_ids.sort() serial_ids = self.get_all_serial_ids(scsi_disks) serial_ids.sort() self.assertEqual(vol_ids, serial_ids, ""Created vol ids do not align with serial ids"" "" found on the domain"") def test_attach_multiple_scsi_disks(self): """"""After booting an instance attach multiple virtio-scsi disks to the guest. Validate that all volumes attach correctly to the instance. """""" expected_disks = self.disks_to_create + 1 img_id = self.copy_default_image(hw_scsi_model='virtio-scsi', hw_disk_bus='scsi') flavor = self.create_flavor() server = self.create_test_server(flavor=flavor['id'], image_id=img_id) vol_ids = [] for _ in range(self.disks_to_create): volume = self.create_volume() vol_ids.append(volume['id']) self.attach_volume(server, volume) vol_ids.sort() disk_ctrl = self.get_scsi_disk_controller(server_id=server['id']) self.assertEqual(len(disk_ctrl), 1, ""One and only one SCSI Disk controller should have"" "" been created but instead"" "" found: {} controllers"".format(len(disk_ctrl))) cntrl_index = disk_ctrl[0].attrib['index'] scsi_disks = self.get_scsi_disks(server_id=server['id'], index=cntrl_index) self.assertEqual(len(scsi_disks), expected_disks, ""Expected {} disks but only"" "" found {}"".format(expected_disks, len(scsi_disks))) serial_ids = self.get_all_serial_ids(scsi_disks) serial_ids.sort() self.assertEqual(vol_ids, serial_ids, ""Created vol ids do not align with serial ids"" "" found on the domain"") ",,170,0
openstack%2Fopenstack-manuals~master~Ibf7c9e3f6b62c0bc686738db4ed270eca7b74f3b,openstack/openstack-manuals,master,Ibf7c9e3f6b62c0bc686738db4ed270eca7b74f3b,[glossary] Update Victoria description,MERGED,2020-03-20 05:54:37.000000000,2020-03-23 14:22:20.000000000,2020-03-23 13:35:50.000000000,"[{'_account_id': 6547}, {'_account_id': 10607}, {'_account_id': 15334}, {'_account_id': 16708}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 05:54:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/3cf39e57d0f0d10fa50ea31a6e54a35934198e94', 'message': '[glossary] Update Victoria description\n\nThe summit is cancelled, so update description.\n\nChange-Id: Ibf7c9e3f6b62c0bc686738db4ed270eca7b74f3b\n'}, {'number': 2, 'created': '2020-03-20 20:35:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/3b1bce4c00c68586227b031d34eee4e48767c61a', 'message': '[glossary] Update Victoria description\n\nThe in-person event is cancelled, so update description.\n\nChange-Id: Ibf7c9e3f6b62c0bc686738db4ed270eca7b74f3b\n'}, {'number': 3, 'created': '2020-03-23 09:59:40.000000000', 'files': ['doc/common/glossary.rst'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/13a8cbfdcd384ce581679be9862a3f96a169381d', 'message': '[glossary] Update Victoria description\n\nThe in-person event is cancelled, so update description.\n\nChange-Id: Ibf7c9e3f6b62c0bc686738db4ed270eca7b74f3b\n'}]",3,714029,13a8cbfdcd384ce581679be9862a3f96a169381d,15,5,3,6547,,,0,"[glossary] Update Victoria description

The in-person event is cancelled, so update description.

Change-Id: Ibf7c9e3f6b62c0bc686738db4ed270eca7b74f3b
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/29/714029/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/common/glossary.rst'],1,3cf39e57d0f0d10fa50ea31a6e54a35934198e94,covid-19," OpenStack Infrastructure Summit was planned to take place in Vancouver, British Columbia, Canada. The summit was cancelled due to COVID-19. The release is named after Victoria, the capital city of British Columbia."," OpenStack Infrastructure Summit took place in Vancouver, British Columbia, Canada. The release is named after Victoria, the capital city of British Columbia.",4,3
openstack%2Fcinder~stable%2Frocky~Ic616bbcced22db6eb8c8946dec98aefd84b16c31,openstack/cinder,stable/rocky,Ic616bbcced22db6eb8c8946dec98aefd84b16c31,NetApp SolidFire: Fix NetApp SolidFire SSL option,MERGED,2018-09-05 12:47:56.000000000,2020-03-23 14:18:42.000000000,2018-09-12 23:53:07.000000000,"[{'_account_id': 24}, {'_account_id': 4523}, {'_account_id': 10058}, {'_account_id': 10118}, {'_account_id': 11611}, {'_account_id': 11904}, {'_account_id': 12369}, {'_account_id': 13144}, {'_account_id': 15670}, {'_account_id': 21863}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23613}, {'_account_id': 24236}, {'_account_id': 25243}, {'_account_id': 26077}, {'_account_id': 26537}, {'_account_id': 27615}, {'_account_id': 28801}]","[{'number': 1, 'created': '2018-09-05 12:47:56.000000000', 'files': ['cinder/volume/drivers/solidfire.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/ee974f98a19fd6fa625f1e74dc021fb7bcf1692e', 'message': 'NetApp SolidFire: Fix NetApp SolidFire SSL option\n\nThe driver.verify_ssl atribute is being defined before its usage in\nthe _create_cluster_reference() causing the driver initialization\nto fail.\n\n(cherry picked from commit 9089982ef14482a132ef4da578ab0df22908a044)\n\nChange-Id: Ic616bbcced22db6eb8c8946dec98aefd84b16c31\n'}]",0,600029,ee974f98a19fd6fa625f1e74dc021fb7bcf1692e,24,19,1,24407,,,0,"NetApp SolidFire: Fix NetApp SolidFire SSL option

The driver.verify_ssl atribute is being defined before its usage in
the _create_cluster_reference() causing the driver initialization
to fail.

(cherry picked from commit 9089982ef14482a132ef4da578ab0df22908a044)

Change-Id: Ic616bbcced22db6eb8c8946dec98aefd84b16c31
",git fetch https://review.opendev.org/openstack/cinder refs/changes/29/600029/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/drivers/solidfire.py'],1,ee974f98a19fd6fa625f1e74dc021fb7bcf1692e,bug/1788458, self.verify_ssl = self.configuration.driver_ssl_cert_verify, self.verify_ssl = self.configuration.driver_ssl_cert_verify,1,1
openstack%2Fnova~master~I605005e97b58a9dc037f426c7c313e8dfb081e4d,openstack/nova,master,I605005e97b58a9dc037f426c7c313e8dfb081e4d,Bump python-subunit minimum to 1.4.0,MERGED,2020-03-18 10:21:15.000000000,2020-03-23 14:14:24.000000000,2020-03-23 14:09:58.000000000,"[{'_account_id': 7166}, {'_account_id': 9708}, {'_account_id': 10118}, {'_account_id': 10135}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-03-18 10:21:15.000000000', 'files': ['lower-constraints.txt'], 'web_link': 'https://opendev.org/openstack/nova/commit/a2f30f9d7cad1c025592c1087ff0cba6dec9a309', 'message': ""Bump python-subunit minimum to 1.4.0\n\nThis contains the fix required to avoid the 'subunit parser error'\nerrors we've been seeing for some time now.\n\nChange-Id: I605005e97b58a9dc037f426c7c313e8dfb081e4d\nSigned-off-by: Stephen Finucane <stephenfin@redhat.com>\nCloses-Bug: #1813147\n""}]",1,713611,a2f30f9d7cad1c025592c1087ff0cba6dec9a309,31,12,1,15334,,,0,"Bump python-subunit minimum to 1.4.0

This contains the fix required to avoid the 'subunit parser error'
errors we've been seeing for some time now.

Change-Id: I605005e97b58a9dc037f426c7c313e8dfb081e4d
Signed-off-by: Stephen Finucane <stephenfin@redhat.com>
Closes-Bug: #1813147
",git fetch https://review.opendev.org/openstack/nova refs/changes/11/713611/1 && git format-patch -1 --stdout FETCH_HEAD,['lower-constraints.txt'],1,a2f30f9d7cad1c025592c1087ff0cba6dec9a309,bug/1813147,python-subunit==1.4.0,python-subunit==1.2.0,1,1
openstack%2Foperations-guide~master~If72f3a2da3614b5af6da32662bad0f4b9808d60f,openstack/operations-guide,master,If72f3a2da3614b5af6da32662bad0f4b9808d60f,Updated from openstack-manuals,MERGED,2020-03-23 13:47:37.000000000,2020-03-23 14:13:00.000000000,2020-03-23 14:09:45.000000000,"[{'_account_id': 11904}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 13:47:37.000000000', 'files': ['doc/source/common/glossary.rst'], 'web_link': 'https://opendev.org/openstack/operations-guide/commit/bfe26773195990f46baf6d869fbd0fc941d07e09', 'message': 'Updated from openstack-manuals\n\nChange-Id: If72f3a2da3614b5af6da32662bad0f4b9808d60f\n'}]",0,714446,bfe26773195990f46baf6d869fbd0fc941d07e09,7,2,1,11131,,,0,"Updated from openstack-manuals

Change-Id: If72f3a2da3614b5af6da32662bad0f4b9808d60f
",git fetch https://review.opendev.org/openstack/operations-guide refs/changes/46/714446/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/common/glossary.rst'],1,bfe26773195990f46baf6d869fbd0fc941d07e09,openstack/openstack-manuals," OpenDev + PTG was planned to take place in Vancouver, British Columbia, Canada. The release is named after Victoria, the capital city of British Columbia. The in-person event was cancelled due to COVID-19. The event is being virtualized instead."," OpenStack Infrastructure Summit took place in Vancouver, British Columbia, Canada. The release is named after Victoria, the capital city of British Columbia.",6,3
openstack%2Fnova~master~I5a07d8ec6661f338e5dc12d83e0835f1cbe80b2b,openstack/nova,master,I5a07d8ec6661f338e5dc12d83e0835f1cbe80b2b,Support for nova-manage placement heal_allocations --cell,ABANDONED,2020-03-23 14:11:44.000000000,2020-03-23 14:12:30.000000000,,"[{'_account_id': 10135}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-03-23 14:11:44.000000000', 'files': ['nova/tests/unit/cmd/test_manage.py', 'gate/test_evacuate.sh', 'nova/cmd/manage.py', 'releasenotes/notes/register-allocation-per-cell-9177b3e2161a632c.yaml'], 'web_link': 'https://opendev.org/openstack/nova/commit/e364c0c3d2ba251c6d8412cb953f845dfcefd9bb', 'message': 'Support for nova-manage placement heal_allocations --cell\n\nChange-Id: I5a07d8ec6661f338e5dc12d83e0835f1cbe80b2b\n'}]",0,714454,e364c0c3d2ba251c6d8412cb953f845dfcefd9bb,3,2,1,31733,,,0,"Support for nova-manage placement heal_allocations --cell

Change-Id: I5a07d8ec6661f338e5dc12d83e0835f1cbe80b2b
",git fetch https://review.opendev.org/openstack/nova refs/changes/54/714454/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/cmd/test_manage.py', 'gate/test_evacuate.sh', 'nova/cmd/manage.py', 'releasenotes/notes/register-allocation-per-cell-9177b3e2161a632c.yaml']",4,e364c0c3d2ba251c6d8412cb953f845dfcefd9bb,bug/1868531,--- features: - Add ``--cell`` options to the ``nova-manage placement heal_allocations`` command. We can use this option to provide support for registration per cell. ,,25,3
openstack%2Fopenstack-ansible-os_tacker~master~I6f7532d84ba90a7fbd12ae8bdfef077b7632eed4,openstack/openstack-ansible-os_tacker,master,I6f7532d84ba90a7fbd12ae8bdfef077b7632eed4,"Missing document start ""---""",MERGED,2020-03-13 09:00:36.000000000,2020-03-23 14:11:41.000000000,2020-03-23 14:07:27.000000000,"[{'_account_id': 22348}, {'_account_id': 23317}, {'_account_id': 25023}, {'_account_id': 29865}]","[{'number': 1, 'created': '2020-03-13 09:00:36.000000000', 'files': ['zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_tacker/commit/c7dcfb411c6952ad8da15bd4ffd1dc8c9a4d28bc', 'message': 'Missing document start ""---""\n\nChange-Id: I6f7532d84ba90a7fbd12ae8bdfef077b7632eed4\n'}]",0,712869,c7dcfb411c6952ad8da15bd4ffd1dc8c9a4d28bc,10,4,1,23317,,,0,"Missing document start ""---""

Change-Id: I6f7532d84ba90a7fbd12ae8bdfef077b7632eed4
",git fetch https://review.opendev.org/openstack/openstack-ansible-os_tacker refs/changes/69/712869/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/project.yaml'],1,c7dcfb411c6952ad8da15bd4ffd1dc8c9a4d28bc,fix,---,,1,0
openstack%2Frally~master~I9ef8d235b8c1e3bd1a5330cb5fe9b99967bbe961,openstack/rally,master,I9ef8d235b8c1e3bd1a5330cb5fe9b99967bbe961,Propose Rally 3.0.0,MERGED,2020-03-22 20:19:27.000000000,2020-03-23 14:11:13.000000000,2020-03-23 14:11:13.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 20:19:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/15fa18388ff3c6472932a2a6fbcdbeaa0ee7a4a7', 'message': 'Propose Rally 3.0.0\n\nChange-Id: I9ef8d235b8c1e3bd1a5330cb5fe9b99967bbe961\n'}, {'number': 2, 'created': '2020-03-22 20:42:48.000000000', 'files': ['Dockerfile', 'DOCKER_README.md', 'CHANGELOG.rst'], 'web_link': 'https://opendev.org/openstack/rally/commit/89662fffe8f77657a0052ae201f659372a8f7c31', 'message': 'Propose Rally 3.0.0\n\nChange-Id: I9ef8d235b8c1e3bd1a5330cb5fe9b99967bbe961\n'}]",0,714330,89662fffe8f77657a0052ae201f659372a8f7c31,8,2,2,9545,,,0,"Propose Rally 3.0.0

Change-Id: I9ef8d235b8c1e3bd1a5330cb5fe9b99967bbe961
",git fetch https://review.opendev.org/openstack/rally refs/changes/30/714330/1 && git format-patch -1 --stdout FETCH_HEAD,"['Dockerfile', 'DOCKER_README.md', 'CHANGELOG.rst']",3,15fa18388ff3c6472932a2a6fbcdbeaa0ee7a4a7,docker_push,[3.0.0] - 2020-03-23 --------------------,[unreleased] ------------,10,10
openstack%2Fopenstack-ansible-os_mistral~master~I193f02ed623214d85ab68f658026305b8fa431f2,openstack/openstack-ansible-os_mistral,master,I193f02ed623214d85ab68f658026305b8fa431f2,"Missing document start ""---""",MERGED,2020-03-13 09:13:43.000000000,2020-03-23 14:09:45.000000000,2020-03-23 14:04:58.000000000,"[{'_account_id': 22348}, {'_account_id': 23317}, {'_account_id': 25023}, {'_account_id': 29865}]","[{'number': 1, 'created': '2020-03-13 09:13:43.000000000', 'files': ['zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_mistral/commit/c9bd8cad76fe2054fadd91cebefe7bdd5650c46c', 'message': 'Missing document start ""---""\n\nChange-Id: I193f02ed623214d85ab68f658026305b8fa431f2\n'}]",0,712875,c9bd8cad76fe2054fadd91cebefe7bdd5650c46c,10,4,1,23317,,,0,"Missing document start ""---""

Change-Id: I193f02ed623214d85ab68f658026305b8fa431f2
",git fetch https://review.opendev.org/openstack/openstack-ansible-os_mistral refs/changes/75/712875/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/project.yaml'],1,c9bd8cad76fe2054fadd91cebefe7bdd5650c46c,fix,---,,1,0
openstack%2Fsecurity-doc~master~Ie7959fee4b10b1de083085e970b3c7a8070f3fad,openstack/security-doc,master,Ie7959fee4b10b1de083085e970b3c7a8070f3fad,Updated from openstack-manuals,MERGED,2020-03-23 13:47:26.000000000,2020-03-23 14:07:16.000000000,2020-03-23 14:04:34.000000000,"[{'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 13:47:26.000000000', 'files': ['common/glossary.rst'], 'web_link': 'https://opendev.org/openstack/security-doc/commit/6599959a0102a77e5500a0bb712cac7b291d2fed', 'message': 'Updated from openstack-manuals\n\nChange-Id: Ie7959fee4b10b1de083085e970b3c7a8070f3fad\n'}]",0,714444,6599959a0102a77e5500a0bb712cac7b291d2fed,7,2,1,11131,,,0,"Updated from openstack-manuals

Change-Id: Ie7959fee4b10b1de083085e970b3c7a8070f3fad
",git fetch https://review.opendev.org/openstack/security-doc refs/changes/44/714444/1 && git format-patch -1 --stdout FETCH_HEAD,['common/glossary.rst'],1,6599959a0102a77e5500a0bb712cac7b291d2fed,openstack/openstack-manuals," OpenDev + PTG was planned to take place in Vancouver, British Columbia, Canada. The release is named after Victoria, the capital city of British Columbia. The in-person event was cancelled due to COVID-19. The event is being virtualized instead."," OpenStack Infrastructure Summit took place in Vancouver, British Columbia, Canada. The release is named after Victoria, the capital city of British Columbia.",6,3
openstack%2Fopenstack-manuals~master~I73e3c07e16fc7ef0157f47ffd510a0cc68cee060,openstack/openstack-manuals,master,I73e3c07e16fc7ef0157f47ffd510a0cc68cee060,Redirect infra-manual,MERGED,2020-03-19 16:38:41.000000000,2020-03-23 13:57:34.000000000,2020-03-23 13:34:24.000000000,"[{'_account_id': 6547}, {'_account_id': 10607}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 16:38:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/d5400abae3cf229452fd719f397e2b4f9cc37cde', 'message': 'Redirect infra-manual\n\nThe infra-manual now lives on docs.opendev.org, update links and add a\nredirect.\n\nDepends-On: https://review.opendev.org/713925\nDepends-On: https://review.opendev.org/713929\nChange-Id: I73e3c07e16fc7ef0157f47ffd510a0cc68cee060\n'}, {'number': 2, 'created': '2020-03-20 16:38:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/42017ecc0fc21b6bc708d0089c902affbe7beca1', 'message': 'Redirect infra-manual\n\nThe infra-manual now lives on docs.opendev.org, update links and add a\nredirect.\n\nDepends-On: https://review.opendev.org/713925\nDepends-On: https://review.opendev.org/713929\nChange-Id: I73e3c07e16fc7ef0157f47ffd510a0cc68cee060\n'}, {'number': 3, 'created': '2020-03-20 20:48:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/69c7ede39cfb62ef0662d1cf11e22750179c7b8f', 'message': 'Redirect infra-manual\n\nThe infra-manual now lives on docs.opendev.org, update links and add a\nredirect.\n\nNew location is: https://docs.opendev.org/opendev/infra-manual/latest/\n\nDepends-On: https://review.opendev.org/713925\nDepends-On: https://review.opendev.org/713929\nChange-Id: I73e3c07e16fc7ef0157f47ffd510a0cc68cee060\n'}, {'number': 4, 'created': '2020-03-21 08:17:40.000000000', 'files': ['www/.htaccess', 'CONTRIBUTING.rst', 'doc/doc-contrib-guide/source/docs-review.rst', 'doc/doc-contrib-guide/source/doc-tools/contributing.rst', 'www/templates/footer.tmpl', 'doc/doc-contrib-guide/source/docs-builds.rst', 'www/redirect-tests.txt', 'doc/doc-contrib-guide/source/quickstart/first-timers.rst'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/c67079cd61ca9264b2f503f6514faa4cc6066a29', 'message': 'Redirect infra-manual\n\nThe infra-manual now lives on docs.opendev.org, update links and add a\nredirect.\n\nNew location is: https://docs.opendev.org/opendev/infra-manual/latest/\n\nFor the Contribute footer, point to Contributors Guide.\n\nDepends-On: https://review.opendev.org/713925\nDepends-On: https://review.opendev.org/713929\nChange-Id: I73e3c07e16fc7ef0157f47ffd510a0cc68cee060\n'}]",0,713930,c67079cd61ca9264b2f503f6514faa4cc6066a29,20,4,4,6547,,,0,"Redirect infra-manual

The infra-manual now lives on docs.opendev.org, update links and add a
redirect.

New location is: https://docs.opendev.org/opendev/infra-manual/latest/

For the Contribute footer, point to Contributors Guide.

Depends-On: https://review.opendev.org/713925
Depends-On: https://review.opendev.org/713929
Change-Id: I73e3c07e16fc7ef0157f47ffd510a0cc68cee060
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/30/713930/4 && git format-patch -1 --stdout FETCH_HEAD,"['www/.htaccess', 'CONTRIBUTING.rst', 'doc/doc-contrib-guide/source/docs-review.rst', 'doc/doc-contrib-guide/source/doc-tools/contributing.rst', 'doc/doc-contrib-guide/source/docs-builds.rst', 'www/redirect-tests.txt', 'doc/doc-contrib-guide/source/quickstart/first-timers.rst']",7,d5400abae3cf229452fd719f397e2b4f9cc37cde,infra-manual, <https://docs.opendev.org/opendev/infra-manual/developers.html#understanding-changes-and-patch-sets>`_. .. seealso:: * `Automated Testing <https://docs.opendev.org/opendev/infra-manual/developers.html#automated-testing>`_<https://docs.opendev.org/opendev/infra-manual/developers.html#accessing-gerrit-over-https>`_.. _`Sign the appropriate Individual Contributor License Agreement`: https://docs.opendev.org/opendev/infra-manual/developers.html#sign-the-appropriate-individual-contributor-license-agreement .. _`Installing git-review`: https://docs.opendev.org/opendev/infra-manual/developers.html#install-the-git-review-utility.. _`Development Workflow`: https://docs.opendev.org/opendev/infra-manual/developers.html#development-workflow.. _`Starting Work on a New Project`: https://docs.opendev.org/opendev/infra-manual/developers.html#starting-work-on-a-new-project .. _`Starting a Change`: https://docs.opendev.org/opendev/infra-manual/developers.html#starting-a-change .. _`Committing a change`: https://docs.opendev.org/opendev/infra-manual/developers.html#committing-a-change .. _`Submitting a Change for Review`: https://docs.opendev.org/opendev/infra-manual/developers.html#submitting-a-change-for-review .. _`Updating a Change`: https://docs.opendev.org/opendev/infra-manual/developers.html#updating-a-change, <https://docs.openstack.org/infra/manual/developers.html#understanding-changes-and-patch-sets>`_. .. seealso:: * `Automated Testing <https://docs.openstack.org/infra/manual/developers.html#automated-testing>`_<https://docs.openstack.org/infra/manual/developers.html#accessing-gerrit-over-https>`_.. _`Sign the appropriate Individual Contributor License Agreement`: https://docs.openstack.org/infra/manual/developers.html#sign-the-appropriate-individual-contributor-license-agreement .. _`Installing git-review`: https://docs.openstack.org/infra/manual/developers.html#install-the-git-review-utility.. _`Development Workflow`: https://docs.openstack.org/infra/manual/developers.html#development-workflow.. _`Starting Work on a New Project`: https://docs.openstack.org/infra/manual/developers.html#starting-work-on-a-new-project .. _`Starting a Change`: https://docs.openstack.org/infra/manual/developers.html#starting-a-change .. _`Committing a change`: https://docs.openstack.org/infra/manual/developers.html#committing-a-change .. _`Submitting a Change for Review`: https://docs.openstack.org/infra/manual/developers.html#submitting-a-change-for-review .. _`Updating a Change`: https://docs.openstack.org/infra/manual/developers.html#updating-a-change,32,22
openstack%2Fopenstackdocstheme~master~I9794b46fa1fb9a7a72d4db69e9742c203a2a66b6,openstack/openstackdocstheme,master,I9794b46fa1fb9a7a72d4db69e9742c203a2a66b6,Update several links,MERGED,2020-03-21 08:14:36.000000000,2020-03-23 13:38:00.000000000,2020-03-23 13:36:19.000000000,"[{'_account_id': 11904}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-21 08:14:36.000000000', 'files': ['CONTRIBUTING.rst', 'openstackdocstheme/theme/openstackdocs/script_search.html', 'openstackdocstheme/theme/openstackdocs/layout.html', 'openstackdocstheme/theme/openstackdocs/footer.html', 'openstackdocstheme/theme/openstackdocs/static/js/docs.js', 'openstackdocstheme/theme/openstackdocs/license_cc.html', 'openstackdocstheme/theme/openstackdocs/sidebartoc_menu.html', 'openstackdocstheme/theme/starlingxdocs/footer.html', 'openstackdocstheme/theme/openstackdocs/header.html', 'openstackdocstheme/theme/starlingxdocs/sidebartoc_menu.html'], 'web_link': 'https://opendev.org/openstack/openstackdocstheme/commit/403c9bca39fe2562311a4268457080cce36f7b90', 'message': 'Update several links\n\nLink to Contributor Guide instead of Infra Manual since that is now the\nOpenDev Manual.\n\nUpdate broken security link.\n\nUpdate links to use https everywhere.\n\nUpdate CONTRIBUTING.rst with current cookiecutter template.\n\nChange-Id: I9794b46fa1fb9a7a72d4db69e9742c203a2a66b6\n'}]",0,714249,403c9bca39fe2562311a4268457080cce36f7b90,8,3,1,6547,,,0,"Update several links

Link to Contributor Guide instead of Infra Manual since that is now the
OpenDev Manual.

Update broken security link.

Update links to use https everywhere.

Update CONTRIBUTING.rst with current cookiecutter template.

Change-Id: I9794b46fa1fb9a7a72d4db69e9742c203a2a66b6
",git fetch https://review.opendev.org/openstack/openstackdocstheme refs/changes/49/714249/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'openstackdocstheme/theme/openstackdocs/script_search.html', 'openstackdocstheme/theme/openstackdocs/layout.html', 'openstackdocstheme/theme/openstackdocs/footer.html', 'openstackdocstheme/theme/openstackdocs/license_cc.html', 'openstackdocstheme/theme/openstackdocs/static/js/docs.js', 'openstackdocstheme/theme/openstackdocs/sidebartoc_menu.html', 'openstackdocstheme/theme/starlingxdocs/footer.html', 'openstackdocstheme/theme/openstackdocs/header.html', 'openstackdocstheme/theme/starlingxdocs/sidebartoc_menu.html']",10,403c9bca39fe2562311a4268457080cce36f7b90,infra-manual," <li role=""presentation""><a role=""menuitem"" tabindex=""-1"" href=""https://docs.starlingx.io"">Install Guides</a></li> <li role=""presentation""><a role=""menuitem"" tabindex=""-1"" href=""https://docs.starlingx.io/api-ref/"">API Guides</a></li>"," <li role=""presentation""><a role=""menuitem"" tabindex=""-1"" href=""http://docs.starlingx.io"">Install Guides</a></li> <li role=""presentation""><a role=""menuitem"" tabindex=""-1"" href=""http://docs.starlingx.io/api-ref/"">API Guides</a></li>",55,57
openstack%2Fcontributor-guide~master~I1730ee6ff4ad835671b2dc361831bf7b98c3d2c8,openstack/contributor-guide,master,I1730ee6ff4ad835671b2dc361831bf7b98c3d2c8,Update infra-manual location,MERGED,2020-03-21 11:43:44.000000000,2020-03-23 13:37:36.000000000,2020-03-23 13:35:32.000000000,"[{'_account_id': 10607}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-21 11:43:44.000000000', 'files': ['doc/source/code-and-documentation/quick-start.rst', 'doc/source/organizations/index.rst', 'doc/source/code-and-documentation/using-gerrit.rst'], 'web_link': 'https://opendev.org/openstack/contributor-guide/commit/94e487039be9d132027c0d3daeede5c2a67456ad', 'message': 'Update infra-manual location\n\nThe infra-manual now lives on docs.opendev.org, update links.\n\nNew location is: https://docs.opendev.org/opendev/infra-manual/latest\n\nChange-Id: I1730ee6ff4ad835671b2dc361831bf7b98c3d2c8\n'}]",0,714253,94e487039be9d132027c0d3daeede5c2a67456ad,8,3,1,6547,,,0,"Update infra-manual location

The infra-manual now lives on docs.opendev.org, update links.

New location is: https://docs.opendev.org/opendev/infra-manual/latest

Change-Id: I1730ee6ff4ad835671b2dc361831bf7b98c3d2c8
",git fetch https://review.opendev.org/openstack/contributor-guide refs/changes/53/714253/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/code-and-documentation/quick-start.rst', 'doc/source/organizations/index.rst', 'doc/source/code-and-documentation/using-gerrit.rst']",3,94e487039be9d132027c0d3daeede5c2a67456ad,infra-manual," clone, create a patch and push the change, directions can be `found here <https://docs.opendev.org/opendev/infra-manual/latest/developers.html#starting-work-on-a-new-project>`_"," clone, create a patch and push the change, directions can be `found here <https://docs.openstack.org/infra/manual/developers.html#starting-work-on-a-new-project>`_",3,3
openstack%2Fcookiecutter~master~Iff1e4d49bb88b84e96be097bbfd8a5cba5932df1,openstack/cookiecutter,master,Iff1e4d49bb88b84e96be097bbfd8a5cba5932df1,Update CONTRIBUTING,MERGED,2020-03-20 20:54:24.000000000,2020-03-23 13:27:26.000000000,2020-03-23 13:27:26.000000000,"[{'_account_id': 11904}, {'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 24162}]","[{'number': 1, 'created': '2020-03-20 20:54:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookiecutter/commit/102bc4a9a6d4bf2866baf83fbaa4c1beff6635d4', 'message': 'Update infra-manual location\n\nThe infra-manual now lives on docs.opendev.org, update links.\n\nNew location is: https://docs.opendev.org/opendev/infra-manual/latest\n\nChange-Id: Iff1e4d49bb88b84e96be097bbfd8a5cba5932df1\n'}, {'number': 2, 'created': '2020-03-20 20:58:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookiecutter/commit/e1c43b983fb5f3d029027272cc1f3b317c7c9c18', 'message': 'Update infra-manual location\n\nThe infra-manual now lives on docs.opendev.org, update links.\n\nNew location is: https://docs.opendev.org/opendev/infra-manual/latest\n\nChange-Id: Iff1e4d49bb88b84e96be097bbfd8a5cba5932df1\n'}, {'number': 3, 'created': '2020-03-21 06:14:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookiecutter/commit/8b89b2d6ce6d3e36f6a916bbcff30c863e398ce2', 'message': 'Update CONTRIBUTING\n\nSync {{cookiecutter.repo_name}}/CONTRIBUTING.rst with CONTRIBUTING.rst,\nwe should use the same content that we push everybody else to use.\n\nChange-Id: Iff1e4d49bb88b84e96be097bbfd8a5cba5932df1\n'}, {'number': 4, 'created': '2020-03-21 07:54:59.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/cookiecutter/commit/02e6702c9fe50b0328eed52972f47f8c46b46640', 'message': 'Update CONTRIBUTING\n\nSync {{cookiecutter.repo_name}}/CONTRIBUTING.rst with CONTRIBUTING.rst,\nwe should use the same content that we push everybody else to use.\n\nChange-Id: Iff1e4d49bb88b84e96be097bbfd8a5cba5932df1\n'}]",0,714226,02e6702c9fe50b0328eed52972f47f8c46b46640,14,4,4,6547,,,0,"Update CONTRIBUTING

Sync {{cookiecutter.repo_name}}/CONTRIBUTING.rst with CONTRIBUTING.rst,
we should use the same content that we push everybody else to use.

Change-Id: Iff1e4d49bb88b84e96be097bbfd8a5cba5932df1
",git fetch https://review.opendev.org/openstack/cookiecutter refs/changes/26/714226/4 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,102bc4a9a6d4bf2866baf83fbaa4c1beff6635d4,infra-manual, https://docs.opendev.org/opendev/infra-manual/developers.html https://docs.opendev.org/opendev/infra-manual/developers.html#development-workflow, https://docs.openstack.org/infra/manual/developers.html https://docs.openstack.org/infra/manual/developers.html#development-workflow,2,2
openstack%2Fopenstack-ansible-os_cloudkitty~master~I845c94a9abcdce86aef59085370db3a966430c81,openstack/openstack-ansible-os_cloudkitty,master,I845c94a9abcdce86aef59085370db3a966430c81,"Missing document start ""---""",MERGED,2020-03-13 09:34:05.000000000,2020-03-23 13:06:03.000000000,2020-03-23 13:02:56.000000000,"[{'_account_id': 22348}, {'_account_id': 23317}, {'_account_id': 25023}, {'_account_id': 29865}]","[{'number': 1, 'created': '2020-03-13 09:34:05.000000000', 'files': ['zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_cloudkitty/commit/639f13d56f4e9b49655175eff9b5a282f3738a2a', 'message': 'Missing document start ""---""\n\nChange-Id: I845c94a9abcdce86aef59085370db3a966430c81\n'}]",0,712882,639f13d56f4e9b49655175eff9b5a282f3738a2a,10,4,1,23317,,,0,"Missing document start ""---""

Change-Id: I845c94a9abcdce86aef59085370db3a966430c81
",git fetch https://review.opendev.org/openstack/openstack-ansible-os_cloudkitty refs/changes/82/712882/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/project.yaml'],1,639f13d56f4e9b49655175eff9b5a282f3738a2a,fix,---,,1,0
openstack%2Fpymod2pkg~master~I40768af80ff376866b1070026a93fd3f4bb1c2e4,openstack/pymod2pkg,master,I40768af80ff376866b1070026a93fd3f4bb1c2e4,Switch away from deprecated distribution lookup,MERGED,2020-03-20 20:00:48.000000000,2020-03-23 12:40:28.000000000,2020-03-23 12:38:24.000000000,"[{'_account_id': 7102}, {'_account_id': 13294}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 20:00:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/pymod2pkg/commit/af81721444eadde04657a2c02cd9dee30b8e5e07', 'message': 'Switch away from deprecated distribution lookup\n\nPython 3.8 removed the linux_distribution support, we need\nto switch to the distro package.\n\nChange-Id: I40768af80ff376866b1070026a93fd3f4bb1c2e4\n'}, {'number': 2, 'created': '2020-03-20 20:18:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/pymod2pkg/commit/523e038bc46e8bcf5ecf76acb5ee7f474b52fd84', 'message': 'Switch away from deprecated distribution lookup\n\nPython 3.8 removed the linux_distribution support, we need\nto switch to the distro package.\n\nTo pass testing, we need to update to a newer flake8 version.\nAlso update setuptools annotation since Python 2.x support\nhas been dropped.\n\nChange-Id: I40768af80ff376866b1070026a93fd3f4bb1c2e4\n'}, {'number': 3, 'created': '2020-03-23 10:54:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/pymod2pkg/commit/ccc91512f190b01f55372b27e5c549903bc2b100', 'message': 'Switch away from deprecated distribution lookup\n\nPython 3.8 removed the linux_distribution support, we need\nto switch to the distro package.\n\nTo pass testing, we need to update to a newer flake8 version.\nAlso update setuptools annotation since Python 2.x support\nhas been dropped.\n\nChange-Id: I40768af80ff376866b1070026a93fd3f4bb1c2e4\n'}, {'number': 4, 'created': '2020-03-23 11:37:21.000000000', 'files': ['requirements.txt', 'test-requirements.txt', 'pymod2pkg/__init__.py', 'setup.cfg', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/pymod2pkg/commit/037724dd88ba3a89e02ab040fc39ab2d93a117cd', 'message': 'Switch away from deprecated distribution lookup\n\nPython 3.8 removed the linux_distribution support, we need\nto switch to the distro package.\n\nTo pass testing, we need to update to a newer flake8 version.\nAlso update setuptools annotation since Python 2.x support\nhas been dropped.\n\nChange-Id: I40768af80ff376866b1070026a93fd3f4bb1c2e4\n'}]",1,714205,037724dd88ba3a89e02ab040fc39ab2d93a117cd,16,3,4,6593,,,0,"Switch away from deprecated distribution lookup

Python 3.8 removed the linux_distribution support, we need
to switch to the distro package.

To pass testing, we need to update to a newer flake8 version.
Also update setuptools annotation since Python 2.x support
has been dropped.

Change-Id: I40768af80ff376866b1070026a93fd3f4bb1c2e4
",git fetch https://review.opendev.org/openstack/pymod2pkg refs/changes/05/714205/4 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'pymod2pkg/__init__.py']",2,af81721444eadde04657a2c02cd9dee30b8e5e07,,import distro `distro.LinuxDistribution().like().partition(' ')[0]` default=distro.LinuxDistribution().like().partition(' ')[0]), `platform.linux_distribution()[0]` default=platform.linux_distribution()[0]),4,2
openstack%2Fopenstack-ansible~stable%2Ftrain~Id3fae8f94f99327fb9c6486df9cdf3aee5fd9ef8,openstack/openstack-ansible,stable/train,Id3fae8f94f99327fb9c6486df9cdf3aee5fd9ef8,[doc] Add placement definition to config samples,MERGED,2020-03-14 12:31:43.000000000,2020-03-23 12:32:22.000000000,2020-03-23 12:29:41.000000000,"[{'_account_id': 22348}, {'_account_id': 28619}, {'_account_id': 29865}]","[{'number': 1, 'created': '2020-03-14 12:31:43.000000000', 'files': ['etc/openstack_deploy/openstack_user_config.yml.prod-ceph.example', 'etc/openstack_deploy/user_variables.yml.prod-ceph.example', 'etc/openstack_deploy/openstack_user_config.yml.prod.example', 'etc/openstack_deploy/openstack_user_config.yml.provnet-group.example'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/b2d67351d01d665c2518b27767482e2cc0e947ce', 'message': '[doc] Add placement definition to config samples\n\nChange-Id: Id3fae8f94f99327fb9c6486df9cdf3aee5fd9ef8\n(cherry picked from commit 9d2a3c07ebd0cf334b79a06a83fc7124bfdb0f32)\n'}]",0,713088,b2d67351d01d665c2518b27767482e2cc0e947ce,8,3,1,25023,,,0,"[doc] Add placement definition to config samples

Change-Id: Id3fae8f94f99327fb9c6486df9cdf3aee5fd9ef8
(cherry picked from commit 9d2a3c07ebd0cf334b79a06a83fc7124bfdb0f32)
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/88/713088/1 && git format-patch -1 --stdout FETCH_HEAD,"['etc/openstack_deploy/openstack_user_config.yml.prod-ceph.example', 'etc/openstack_deploy/user_variables.yml.prod-ceph.example', 'etc/openstack_deploy/openstack_user_config.yml.prod.example', 'etc/openstack_deploy/openstack_user_config.yml.provnet-group.example']",4,b2d67351d01d665c2518b27767482e2cc0e947ce,,# placement placement-infra_hosts: infra1: ip: 172.29.236.11 infra2: ip: 172.29.236.12 infra3: ip: 172.29.236.13 ,,21,1
openstack%2Fironic~master~I3a36f58b12487e18a6898aef6b077d4221f8a5b8,openstack/ironic,master,I3a36f58b12487e18a6898aef6b077d4221f8a5b8,Add indicators REST API endpoints,MERGED,2019-04-11 13:27:07.000000000,2020-03-23 12:32:08.000000000,2020-03-23 12:29:49.000000000,"[{'_account_id': 4571}, {'_account_id': 10118}, {'_account_id': 10239}, {'_account_id': 11655}, {'_account_id': 14629}, {'_account_id': 19003}, {'_account_id': 19339}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 23851}, {'_account_id': 26340}, {'_account_id': 28429}, {'_account_id': 30100}]","[{'number': 1, 'created': '2019-04-11 13:27:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/066ea1879230c2b8db252ccf7531417c6b767d15', 'message': 'WIP: Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators/supported` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>\n  to retrieve all indicators and their states for the hardware component.\n* PUT /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>`\n  change state of the desired indicators of the component.\n\nStill WIP because of missing unit tests, the rest should be more or\nless in place.\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 2, 'created': '2019-04-11 16:24:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d742887003639291812ad67625fef01df886538e', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators/supported` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>\n  to retrieve all indicators and their states for the hardware component.\n* PUT /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>`\n  change state of the desired indicators of the component.\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 3, 'created': '2019-04-15 14:02:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a44fcec58d77ff08fbabb3dc0e6630e592354d13', 'message': 'Add indicator management to redfish hw type\n\nImplements necessary indicator management calls to redfish driver\nto actually read/set system/chassis and drive LEDs through Redfish.\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30471\n'}, {'number': 4, 'created': '2019-04-15 21:31:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8b3d77d2fb66b61cdd296f63d8688c9ca88990c1', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>\n  to retrieve all indicators and their states for the hardware component.\n* PUT /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>`\n  change state of the desired indicators of the component.\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 5, 'created': '2019-04-16 18:57:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/65bed5b504f7f6321e0da86cdc6751e19dc85a71', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>\n  to retrieve all indicators and their states for the hardware component.\n* PUT /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>`\n  change state of the desired indicators of the component.\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 6, 'created': '2019-06-06 10:37:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/62a50606159a0d32a8bc257c35412d73f45ff5dd', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>\n  to retrieve all indicators and their states for the hardware component.\n* PUT /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>`\n  change state of the desired indicators of the component.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 7, 'created': '2019-06-07 14:54:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f24412d2420e003d6218cc75604f315c9b2e98ae', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>\n  to retrieve all indicators and their states for the hardware component.\n* PUT /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>`\n  change state of the desired indicators of the component.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 8, 'created': '2019-06-07 15:21:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8f62726043c0e08038434e817ff522267739d4b5', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>\n  to retrieve all indicators and their states for the hardware component.\n* PUT /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>`\n  change state of the desired indicators of the component.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 9, 'created': '2019-06-07 16:42:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/81b8081da7eb47e9ea20d7e9fda38715d33ce83b', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>\n  to retrieve all indicators and their states for the hardware component.\n* PUT /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>`\n  change state of the desired indicators of the component.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 10, 'created': '2019-06-10 13:49:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6fb488ea657584cbfe77ec013b88139b1c1b64a8', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>\n  to retrieve all indicators and their states for the hardware component.\n* PUT /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>`\n  change state of the desired indicators of the component.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 11, 'created': '2019-06-11 12:14:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/393c0831a8c5c9c81b4f22a2f1e548f64053e506', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>\n  to retrieve all indicators and their states for the hardware component.\n* PUT /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>`\n  change state of the desired indicators of the component.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 12, 'created': '2019-06-12 17:42:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7532e243f8f6d238b4b36e7796533816489f3f82', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>\n  to retrieve all indicators and their states for the hardware component.\n* PUT /v1/nodes/<node_ident>/management/indicators/<component>/<indicator_ident>`\n  change state of the desired indicators of the component.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 13, 'created': '2019-06-12 20:38:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/333077a15b40329cd53d841b5b42e041c9b6bff0', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 14, 'created': '2019-06-19 15:36:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/aa39dd987c361f2c874d754afe03e05fc52481fb', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 15, 'created': '2019-08-26 10:08:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5f93429d17ce30435366c63b798fefe457bdf352', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 16, 'created': '2019-09-23 12:51:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/008466faf0c5fe2bc2d3a4a49784f688a42f9074', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 17, 'created': '2019-10-12 09:25:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d2ff1bcad937f22782f87ac8f3b2446b31799773', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 18, 'created': '2019-11-04 11:43:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/875671392ea5127927c3a1c219d909e3b9c1f4cc', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 19, 'created': '2019-11-07 11:05:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/715dc6e7397763a7ebd96a851a72512b2242b9c2', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 20, 'created': '2019-11-07 15:59:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/da4b87f93b614a9dcbee1f84563a7bba8fe09021', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 21, 'created': '2019-11-08 16:48:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/63dfcc1741bfdfadfd4f35f52c7f718171ffed3f', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 22, 'created': '2019-11-21 11:39:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/4814fedb7ae4a569701cc8ef7d069de12e86b94b', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 23, 'created': '2020-01-02 15:02:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bdd4e1fb0a214012331fee7b3a2fa3aedc021cdc', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 24, 'created': '2020-02-07 00:12:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/5a2bccee9b2588caece8bc44eb7ff6fda1218553', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 25, 'created': '2020-02-07 00:28:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2e72f08dbbacdcd5d682753feb6e0d2843a4f274', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 26, 'created': '2020-02-07 19:20:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/804f3ae991c2b0c9704b4624a65c2c89584dad95', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 27, 'created': '2020-02-10 22:47:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/e6ba4d61ae951e71fd08c5ca13717872e115d435', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 28, 'created': '2020-02-11 00:22:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/31b1ae661c263a84089047c08001f047224c3790', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 29, 'created': '2020-02-20 23:48:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7cf7bbf9272607cf10eb8fc2814a25b2827d12e2', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 30, 'created': '2020-03-07 15:21:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/ab9279dd8fbaa62e4dc6d6b0bbd855ede97e5e84', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 31, 'created': '2020-03-11 20:58:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7c8f64b5a2a194ae3bc25464d6b9421f5e4f9afa', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 32, 'created': '2020-03-17 14:13:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/dfa80e9ee8602c79002627d02e4db8b603986f84', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}, {'number': 33, 'created': '2020-03-21 18:45:01.000000000', 'files': ['ironic/conductor/manager.py', 'ironic/tests/unit/conductor/test_rpcapi.py', 'ironic/api/controllers/v1/versions.py', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/common/policy.py', 'ironic/common/release_mappings.py', 'ironic/tests/unit/api/controllers/v1/test_node.py', 'doc/source/contributor/webapi-version-history.rst', 'ironic/conductor/rpcapi.py', 'releasenotes/notes/add-indicator-api-8c816b3828e6b43b.yaml', 'ironic/api/controllers/v1/node.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/263fd021b23889b139a7971d93a18268c6b0be71', 'message': 'Add indicators REST API endpoints\n\nAdded REST API endpoints for indicator management:\n\n* GET /v1/nodes/<node_ident>/management/indicators` to list all\n  available indicators names for each of the hardware component.\n* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>\n  to retrieve the state of given indicator.\n* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`\n  change state of the desired indicator.\n\nThis implementation slightly deviates from the original spec in\npart of having component name in the URL - this implementation\nflattens component out.\n\nThe spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst\n\nChange-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8\nStory: 2005342\nTask: 30291\n'}]",32,651785,263fd021b23889b139a7971d93a18268c6b0be71,187,13,33,26340,,,0,"Add indicators REST API endpoints

Added REST API endpoints for indicator management:

* GET /v1/nodes/<node_ident>/management/indicators` to list all
  available indicators names for each of the hardware component.
* GET /v1/nodes/<node_ident>/management/indicators/<indicator_ident>
  to retrieve the state of given indicator.
* PUT /v1/nodes/<node_ident>/management/indicators/<indicator_ident>`
  change state of the desired indicator.

This implementation slightly deviates from the original spec in
part of having component name in the URL - this implementation
flattens component out.

The spec: https://review.opendev.org/#/c/655685/7/specs/approved/expose-hardware-indicators.rst

Change-Id: I3a36f58b12487e18a6898aef6b077d4221f8a5b8
Story: 2005342
Task: 30291
",git fetch https://review.opendev.org/openstack/ironic refs/changes/85/651785/28 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/conductor/manager.py', 'ironic/api/controllers/v1/versions.py', 'ironic/common/policy.py', 'ironic/common/release_mappings.py', 'doc/source/contributor/webapi-version-history.rst', 'ironic/api/controllers/v1/node.py', 'ironic/conductor/rpcapi.py', 'releasenotes/notes/add-indicator-api-8c816b3828e6b43b.yaml']",8,066ea1879230c2b8db252ccf7531417c6b767d15,21-add-led-mgmt-api,"--- features: - | Adds REST API endpoints for indicator management. Three new endpoints, for listing, reading and setting the indicators, reside under the ``/v1/nodes/<node_ident>/management/indicators`` location. ",,306,4
openstack%2Fkolla-ansible~stable%2Frocky~I4181654c88554c81940f0d079cf1d64326cdec79,openstack/kolla-ansible,stable/rocky,I4181654c88554c81940f0d079cf1d64326cdec79,CI: install tox,MERGED,2020-03-18 10:38:22.000000000,2020-03-23 12:12:30.000000000,2020-03-23 12:10:56.000000000,"[{'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 22629}]","[{'number': 1, 'created': '2020-03-18 10:38:22.000000000', 'files': ['tests/run.yml'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/dded06f8d8a2df0219f28817902f9287ae5b2045', 'message': 'CI: install tox\n\ntox will be removed from the base image. Install it before that happens.\n\nThis change is made in a simple way that can be easily backported.\n\nDepends-On: https://review.opendev.org/713620\n\nChange-Id: I4181654c88554c81940f0d079cf1d64326cdec79\n(cherry picked from commit dd1ebf20caf97fa53675ef299513fa1c2a2da757)\n'}]",0,713621,dded06f8d8a2df0219f28817902f9287ae5b2045,8,3,1,14826,,,0,"CI: install tox

tox will be removed from the base image. Install it before that happens.

This change is made in a simple way that can be easily backported.

Depends-On: https://review.opendev.org/713620

Change-Id: I4181654c88554c81940f0d079cf1d64326cdec79
(cherry picked from commit dd1ebf20caf97fa53675ef299513fa1c2a2da757)
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/21/713621/1 && git format-patch -1 --stdout FETCH_HEAD,['tests/run.yml'],1,dded06f8d8a2df0219f28817902f9287ae5b2045,, - name: Ensure tox is installed pip: name: tox when: need_build_image become: true ,,6,0
openstack%2Fkolla~stable%2Ftrain~I2a24c87aa44c1e03ba76a2fb0bd71caf54eb4211,openstack/kolla,stable/train,I2a24c87aa44c1e03ba76a2fb0bd71caf54eb4211,CI: Install tox,MERGED,2020-03-18 10:32:13.000000000,2020-03-23 12:12:30.000000000,2020-03-23 12:10:54.000000000,"[{'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 24072}]","[{'number': 1, 'created': '2020-03-18 10:32:13.000000000', 'files': ['tests/playbooks/pre.yml'], 'web_link': 'https://opendev.org/openstack/kolla/commit/18a00d5a6352d8fb74d726989d11e1dfb3f2bf8f', 'message': 'CI: Install tox\n\ntox will be removed from the base image. Install it before that happens.\n\nThis change is made in a simple way that can be easily backported.\n\nChange-Id: I2a24c87aa44c1e03ba76a2fb0bd71caf54eb4211\n(cherry picked from commit a81413fc52b0eaf1eefc3302ba425ac9b93d5a4d)\n'}]",0,713616,18a00d5a6352d8fb74d726989d11e1dfb3f2bf8f,8,3,1,14826,,,0,"CI: Install tox

tox will be removed from the base image. Install it before that happens.

This change is made in a simple way that can be easily backported.

Change-Id: I2a24c87aa44c1e03ba76a2fb0bd71caf54eb4211
(cherry picked from commit a81413fc52b0eaf1eefc3302ba425ac9b93d5a4d)
",git fetch https://review.opendev.org/openstack/kolla refs/changes/16/713616/1 && git format-patch -1 --stdout FETCH_HEAD,['tests/playbooks/pre.yml'],1,18a00d5a6352d8fb74d726989d11e1dfb3f2bf8f,, - name: Ensure tox is installed pip: name: tox become: true,,5,0
openstack%2Fkolla-ansible~stable%2Fstein~I4181654c88554c81940f0d079cf1d64326cdec79,openstack/kolla-ansible,stable/stein,I4181654c88554c81940f0d079cf1d64326cdec79,CI: install tox,MERGED,2020-03-18 10:33:02.000000000,2020-03-23 12:12:29.000000000,2020-03-23 12:10:55.000000000,"[{'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 22629}]","[{'number': 1, 'created': '2020-03-18 10:33:02.000000000', 'files': ['tests/run.yml'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/47915af85101179bf4b1aa6a8283b6a509f04dea', 'message': 'CI: install tox\n\ntox will be removed from the base image. Install it before that happens.\n\nThis change is made in a simple way that can be easily backported.\n\nDepends-On: https://review.opendev.org/713618\n\nChange-Id: I4181654c88554c81940f0d079cf1d64326cdec79\n(cherry picked from commit dd1ebf20caf97fa53675ef299513fa1c2a2da757)\n'}]",0,713619,47915af85101179bf4b1aa6a8283b6a509f04dea,8,3,1,14826,,,0,"CI: install tox

tox will be removed from the base image. Install it before that happens.

This change is made in a simple way that can be easily backported.

Depends-On: https://review.opendev.org/713618

Change-Id: I4181654c88554c81940f0d079cf1d64326cdec79
(cherry picked from commit dd1ebf20caf97fa53675ef299513fa1c2a2da757)
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/19/713619/1 && git format-patch -1 --stdout FETCH_HEAD,['tests/run.yml'],1,47915af85101179bf4b1aa6a8283b6a509f04dea,, - name: Ensure tox is installed pip: name: tox when: need_build_image become: true ,,6,0
openstack%2Fcinder~master~I45f3f9ab1b6f931fcda9a64ae71bb479b250fbe3,openstack/cinder,master,I45f3f9ab1b6f931fcda9a64ae71bb479b250fbe3,Compact DB migrations to Queens,MERGED,2019-05-03 04:19:06.000000000,2020-03-23 12:10:06.000000000,2019-05-09 20:35:12.000000000,"[{'_account_id': 5997}, {'_account_id': 9008}, {'_account_id': 9732}, {'_account_id': 10118}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 13144}, {'_account_id': 14384}, {'_account_id': 15386}, {'_account_id': 15670}, {'_account_id': 15941}, {'_account_id': 16834}, {'_account_id': 16897}, {'_account_id': 18120}, {'_account_id': 18883}, {'_account_id': 19933}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23613}, {'_account_id': 24921}, {'_account_id': 25243}, {'_account_id': 25678}, {'_account_id': 26077}, {'_account_id': 26537}, {'_account_id': 27615}, {'_account_id': 29716}]","[{'number': 1, 'created': '2019-05-03 04:19:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/26a3312338b2237d2857f3a1b62ec329aa86e5a2', 'message': 'Compact DB migrations to Queens\n\nThis compacts all database migrations up to Queens into one initial\nschema to remove the need to apply every database change along the way.\n\nChange-Id: I45f3f9ab1b6f931fcda9a64ae71bb479b250fbe3\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}, {'number': 2, 'created': '2019-05-03 14:45:29.000000000', 'files': ['cinder/db/sqlalchemy/migrate_repo/versions/121_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/118_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/120_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/109_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/108_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/113_add_reservation_deleted_uuid_index.py', 'cinder/db/sqlalchemy/migrate_repo/versions/111_add_project_resource_idx_for_quota_usage.py', 'cinder/db/sqlalchemy/migrate_repo/versions/098_message_add_expire_at_index.py', 'cinder/db/sqlalchemy/migrate_repo/versions/100_add_foreign_key_indexes.py', 'cinder/db/migration.py', 'cinder/db/sqlalchemy/migrate_repo/versions/112_add_uuid_to_services_table.py', 'cinder/db/sqlalchemy/migrate_repo/versions/104_change_size_of_project_id.py', 'cinder/db/sqlalchemy/migrate_repo/versions/102_add_replication_status_to_groups_table.py', 'cinder/db/sqlalchemy/migrate_repo/versions/107_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/123_cinder_init.py', 'cinder/db/sqlalchemy/migrate_repo/versions/105_add_backup_metadata.py', 'cinder/db/sqlalchemy/migrate_repo/versions/123_add_transfer_no_snapshots.py', 'cinder/db/sqlalchemy/migrate_repo/versions/103_message_action_detail_column.py', 'cinder/db/sqlalchemy/migrate_repo/versions/122_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/099_add_connection_info_to_attachment.py', 'cinder/db/sqlalchemy/migrate_repo/versions/115_add_shared_targets_to_volumes.py', 'cinder/db/sqlalchemy/migrate_repo/versions/119_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/101_fix_replication_status_default_upgrade.sql', 'cinder/db/sqlalchemy/migrate_repo/versions/117_add_encryption_key_id_to_backups.py', 'cinder/db/sqlalchemy/migrate_repo/versions/106_placeholder.py', 'releasenotes/notes/db-schema-from-queens-de5025a780ff1d30.yaml', 'cinder/db/sqlalchemy/migrate_repo/versions/110_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/114_add_service_uuid_fk_to_volumes.py', 'cinder/db/sqlalchemy/migrate_repo/versions/116_add_volume_attachment_connector.py', 'cinder/tests/unit/db/test_migrations.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/883e78537592e9764e86329153d5e52922473092', 'message': 'Compact DB migrations to Queens\n\nThis compacts all database migrations up to Queens into one initial\nschema to remove the need to apply every database change along the way.\n\nChange-Id: I45f3f9ab1b6f931fcda9a64ae71bb479b250fbe3\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}]",0,656918,883e78537592e9764e86329153d5e52922473092,61,32,2,11904,,,0,"Compact DB migrations to Queens

This compacts all database migrations up to Queens into one initial
schema to remove the need to apply every database change along the way.

Change-Id: I45f3f9ab1b6f931fcda9a64ae71bb479b250fbe3
Signed-off-by: Sean McGinnis <sean.mcginnis@gmail.com>
",git fetch https://review.opendev.org/openstack/cinder refs/changes/18/656918/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/db/sqlalchemy/migrate_repo/versions/121_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/118_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/120_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/109_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/108_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/113_add_reservation_deleted_uuid_index.py', 'cinder/db/sqlalchemy/migrate_repo/versions/111_add_project_resource_idx_for_quota_usage.py', 'cinder/db/sqlalchemy/migrate_repo/versions/098_message_add_expire_at_index.py', 'cinder/db/sqlalchemy/migrate_repo/versions/100_add_foreign_key_indexes.py', 'cinder/db/migration.py', 'cinder/db/sqlalchemy/migrate_repo/versions/112_add_uuid_to_services_table.py', 'cinder/db/sqlalchemy/migrate_repo/versions/104_change_size_of_project_id.py', 'cinder/db/sqlalchemy/migrate_repo/versions/102_add_replication_status_to_groups_table.py', 'cinder/db/sqlalchemy/migrate_repo/versions/107_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/123_cinder_init.py', 'cinder/db/sqlalchemy/migrate_repo/versions/105_add_backup_metadata.py', 'cinder/db/sqlalchemy/migrate_repo/versions/123_add_transfer_no_snapshots.py', 'cinder/db/sqlalchemy/migrate_repo/versions/103_message_action_detail_column.py', 'cinder/db/sqlalchemy/migrate_repo/versions/122_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/099_add_connection_info_to_attachment.py', 'cinder/db/sqlalchemy/migrate_repo/versions/115_add_shared_targets_to_volumes.py', 'cinder/db/sqlalchemy/migrate_repo/versions/119_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/101_fix_replication_status_default_upgrade.sql', 'cinder/db/sqlalchemy/migrate_repo/versions/117_add_encryption_key_id_to_backups.py', 'cinder/db/sqlalchemy/migrate_repo/versions/106_placeholder.py', 'releasenotes/notes/db-schema-from-queens-de5025a780ff1d30.yaml', 'cinder/db/sqlalchemy/migrate_repo/versions/110_placeholder.py', 'cinder/db/sqlalchemy/migrate_repo/versions/114_add_service_uuid_fk_to_volumes.py', 'cinder/db/sqlalchemy/migrate_repo/versions/116_add_volume_attachment_connector.py', 'cinder/tests/unit/db/test_migrations.py']",30,26a3312338b2237d2857f3a1b62ec329aa86e5a2,compact_db,," # NOTE : 104 modifies size of messages.project_id to 255. # This should be safe according to documentation. 104, def _check_098(self, engine, data): self.assertTrue(engine.dialect.has_table(engine.connect(), ""messages"")) ids = self.get_indexed_columns(engine, 'messages') self.assertTrue('expires_at' in ids) def _check_099(self, engine, data): self.assertTrue(engine.dialect.has_table(engine.connect(), ""volume_attachment"")) attachment = db_utils.get_table(engine, 'volume_attachment') self.assertIsInstance(attachment.c.connection_info.type, self.TEXT_TYPE) def _pre_upgrade_101(self, engine): """"""Add data to test the SQL migration."""""" types_table = db_utils.get_table(engine, 'volume_types') for i in range(1, 5): types_table.insert().execute({'id': str(i)}) specs_table = db_utils.get_table(engine, 'volume_type_extra_specs') specs = [ {'volume_type_id': '1', 'key': 'key', 'value': '<is> False'}, {'volume_type_id': '2', 'key': 'replication_enabled', 'value': '<is> False'}, {'volume_type_id': '3', 'key': 'replication_enabled', 'value': '<is> True', 'deleted': True}, {'volume_type_id': '3', 'key': 'key', 'value': '<is> True'}, {'volume_type_id': '4', 'key': 'replication_enabled', 'value': '<is> True'}, {'volume_type_id': '4', 'key': 'key', 'value': '<is> True'}, ] for spec in specs: specs_table.insert().execute(spec) volumes_table = db_utils.get_table(engine, 'volumes') volumes = [ {'id': '1', 'replication_status': 'disabled', 'volume_type_id': None}, {'id': '2', 'replication_status': 'disabled', 'volume_type_id': ''}, {'id': '3', 'replication_status': 'disabled', 'volume_type_id': '1'}, {'id': '4', 'replication_status': 'disabled', 'volume_type_id': '2'}, {'id': '5', 'replication_status': 'disabled', 'volume_type_id': '2'}, {'id': '6', 'replication_status': 'disabled', 'volume_type_id': '3'}, {'id': '7', 'replication_status': 'error', 'volume_type_id': '4'}, {'id': '8', 'deleted': True, 'replication_status': 'disabled', 'volume_type_id': '4'}, {'id': '9', 'replication_status': 'disabled', 'deleted': None, 'volume_type_id': '4'}, {'id': '10', 'replication_status': 'disabled', 'deleted': False, 'volume_type_id': '4'}, ] for volume in volumes: volumes_table.insert().execute(volume) # Only the last volume should be changed to enabled expected = {v['id']: v['replication_status'] for v in volumes} expected['9'] = 'enabled' expected['10'] = 'enabled' return expected def _check_101(self, engine, data): # Get existing volumes after the migration volumes_table = db_utils.get_table(engine, 'volumes') volumes = volumes_table.select().execute() # Check that the replication_status is the one we expect according to # _pre_upgrade_098 for volume in volumes: self.assertEqual(data[volume.id], volume.replication_status, 'id %s' % volume.id) def _check_102(self, engine, data): """"""Test adding replication_status to groups table."""""" groups = db_utils.get_table(engine, 'groups') self.assertIsInstance(groups.c.replication_status.type, self.VARCHAR_TYPE) def _check_103(self, engine, data): self.assertTrue(engine.dialect.has_table(engine.connect(), ""messages"")) attachment = db_utils.get_table(engine, 'messages') self.assertIsInstance(attachment.c.detail_id.type, self.VARCHAR_TYPE) self.assertIsInstance(attachment.c.action_id.type, self.VARCHAR_TYPE) def _check_104(self, engine, data): messages = db_utils.get_table(engine, 'messages') self.assertEqual(255, messages.c.project_id.type.length) def _check_105(self, engine, data): self.assertTrue(engine.dialect.has_table(engine.connect(), ""backup_metadata"")) backup_metadata = db_utils.get_table(engine, 'backup_metadata') self.assertIsInstance(backup_metadata.c.created_at.type, self.TIME_TYPE) self.assertIsInstance(backup_metadata.c.updated_at.type, self.TIME_TYPE) self.assertIsInstance(backup_metadata.c.deleted_at.type, self.TIME_TYPE) self.assertIsInstance(backup_metadata.c.deleted.type, self.BOOL_TYPE) self.assertIsInstance(backup_metadata.c.id.type, self.INTEGER_TYPE) self.assertIsInstance(backup_metadata.c.key.type, self.VARCHAR_TYPE) self.assertIsInstance(backup_metadata.c.value.type, self.VARCHAR_TYPE) self.assertIsInstance(backup_metadata.c.backup_id.type, self.VARCHAR_TYPE) f_keys = self.get_foreign_key_columns(engine, 'backup_metadata') self.assertEqual({'backup_id'}, f_keys) def _check_111(self, engine, data): self.assertTrue(db_utils.index_exists_on_columns( engine, 'quota_usages', ['project_id', 'resource'])) def _check_112(self, engine, data): services = db_utils.get_table(engine, 'services') self.assertIsInstance(services.c.uuid.type, self.VARCHAR_TYPE) def _check_113(self, engine, data): """"""Test that adding reservations index works correctly."""""" reservations = db_utils.get_table(engine, 'reservations') index_columns = [] for idx in reservations.indexes: if idx.name == 'reservations_deleted_uuid_idx': index_columns = idx.columns.keys() break self.assertEqual(sorted(['deleted', 'uuid']), sorted(index_columns)) def _check_114(self, engine, data): volumes = db_utils.get_table(engine, 'volumes') self.assertIsInstance(volumes.c.service_uuid.type, self.VARCHAR_TYPE) index_columns = [] for idx in volumes.indexes: if idx.name == 'volumes_service_uuid_idx': index_columns = idx.columns.keys() break self.assertEqual(sorted(['deleted', 'service_uuid']), sorted(index_columns)) def _check_115(self, engine, data): volumes = db_utils.get_table(engine, 'volumes') self.assertIsInstance(volumes.c.shared_targets.type, self.BOOL_TYPE) def _check_116(self, engine, data): volume_attachment = db_utils.get_table(engine, 'volume_attachment') self.assertIn('connector', volume_attachment.c) def _check_123(self, engine, data): volume_transfer = db_utils.get_table(engine, 'transfers') self.assertIn('no_snapshots', volume_transfer.c) ",79,913
openstack%2Frpm-packaging~stable%2Frocky~Ibdec283e4e254bd7b6621ebd146fda9b6ad2a72a,openstack/rpm-packaging,stable/rocky,Ibdec283e4e254bd7b6621ebd146fda9b6ad2a72a,Update to Rocky EM releases (Part II),MERGED,2020-03-19 10:57:51.000000000,2020-03-23 12:04:51.000000000,2020-03-23 12:04:51.000000000,"[{'_account_id': 7102}, {'_account_id': 13294}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 10:57:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/bc23c0e20d039589abd69bf9cbce5cb20d7a0ad6', 'message': 'Update to Rocky EM releases (Part II)\n\nChange-Id: Ibdec283e4e254bd7b6621ebd146fda9b6ad2a72a\n'}, {'number': 2, 'created': '2020-03-20 19:45:00.000000000', 'files': ['openstack/sushy/sushy.spec.j2', 'openstack/osprofiler/osprofiler.spec.j2', 'openstack/ovsdbapp/ovsdbapp.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/145d3eaaf939d1dcb46e8a3e4c40d3a94dff006a', 'message': 'Update to Rocky EM releases (Part II)\n\nChange-Id: Ibdec283e4e254bd7b6621ebd146fda9b6ad2a72a\n'}]",0,713834,145d3eaaf939d1dcb46e8a3e4c40d3a94dff006a,13,5,2,6593,,,0,"Update to Rocky EM releases (Part II)

Change-Id: Ibdec283e4e254bd7b6621ebd146fda9b6ad2a72a
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/34/713834/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack/sushy/sushy.spec.j2', 'openstack/osprofiler/osprofiler.spec.j2', 'openstack/ovsdbapp/ovsdbapp.spec.j2']",3,bc23c0e20d039589abd69bf9cbce5cb20d7a0ad6,,{% set upstream_version = upstream_version('0.12.5') %},{% set upstream_version = upstream_version('0.12.4') %},3,3
openstack%2Fproject-config~master~I52144afd37f31c8f42e124f0c152503a70ec59ef,openstack/project-config,master,I52144afd37f31c8f42e124f0c152503a70ec59ef,Add #openvswitch channel to accessbot config,NEW,2020-03-20 16:43:06.000000000,2020-03-23 12:01:28.000000000,,"[{'_account_id': 6547}, {'_account_id': 13252}, {'_account_id': 22348}, {'_account_id': 23804}]","[{'number': 1, 'created': '2020-03-20 16:43:06.000000000', 'files': ['accessbot/channels.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/9ec8e561b1f02f3aeda3c1338aec421fb2c7df8a', 'message': 'Add #openvswitch channel to accessbot config\n\nThe #openvswitch channel is making use of OpenDev\'s ""openstack""\nmeetbot for meeting minutes and channel logging, so needs to be\nincluded in our accessbot configuration as well.\n\nChange-Id: I52144afd37f31c8f42e124f0c152503a70ec59ef\n'}]",0,714174,9ec8e561b1f02f3aeda3c1338aec421fb2c7df8a,7,4,1,5263,,,0,"Add #openvswitch channel to accessbot config

The #openvswitch channel is making use of OpenDev's ""openstack""
meetbot for meeting minutes and channel logging, so needs to be
included in our accessbot configuration as well.

Change-Id: I52144afd37f31c8f42e124f0c152503a70ec59ef
",git fetch https://review.opendev.org/openstack/project-config refs/changes/74/714174/1 && git format-patch -1 --stdout FETCH_HEAD,['accessbot/channels.yaml'],1,9ec8e561b1f02f3aeda3c1338aec421fb2c7df8a,openvswitch-irc, - name: openvswitch,,1,0
openstack%2Fkolla-ansible~master~Ibca24836f19b3cbf6166fa39a3702883938feda8,openstack/kolla-ansible,master,Ibca24836f19b3cbf6166fa39a3702883938feda8,cirros: upgrade to 0.5.1,MERGED,2020-03-04 09:12:10.000000000,2020-03-23 11:42:55.000000000,2020-03-23 11:41:18.000000000,"[{'_account_id': 167}, {'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 22629}, {'_account_id': 24072}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-04 09:12:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/7f3e6902979bbf23eb7b8c5d5cf1c976e7164c08', 'message': 'cirros: upgrade to 0.5.0\n\nWe released CirrOS 0.5.0 yesterday. Time to move then.\n\nChange-Id: Ibca24836f19b3cbf6166fa39a3702883938feda8\n'}, {'number': 2, 'created': '2020-03-07 09:27:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/f4ec6be3e78156e98320f9bd39aebe5f82029f0f', 'message': 'cirros: upgrade to 0.5.1\n\nWe released CirrOS 0.5.1. Time to move then.\n\nChange-Id: Ibca24836f19b3cbf6166fa39a3702883938feda8\n'}, {'number': 3, 'created': '2020-03-07 09:28:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/b0f3eef53da492b539ae9f2c17e71e6b730d5809', 'message': 'cirros: upgrade to 0.5.1\n\nWe released CirrOS 0.5.1. Time to move then.\n\nChange-Id: Ibca24836f19b3cbf6166fa39a3702883938feda8\n'}, {'number': 4, 'created': '2020-03-21 15:40:10.000000000', 'files': ['tools/init-runonce', 'ansible/roles/tempest/defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/9bde29a30b7df6da8724e52f8bd970bd177d754c', 'message': 'cirros: upgrade to 0.5.1\n\nWe released CirrOS 0.5.1. Time to move then.\n\nChange-Id: Ibca24836f19b3cbf6166fa39a3702883938feda8\n'}]",0,711182,9bde29a30b7df6da8724e52f8bd970bd177d754c,23,6,4,24072,,,0,"cirros: upgrade to 0.5.1

We released CirrOS 0.5.1. Time to move then.

Change-Id: Ibca24836f19b3cbf6166fa39a3702883938feda8
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/82/711182/2 && git format-patch -1 --stdout FETCH_HEAD,"['tools/init-runonce', 'ansible/roles/tempest/defaults/main.yml']",2,7f3e6902979bbf23eb7b8c5d5cf1c976e7164c08,,"image_url: ""https://github.com/cirros-dev/cirros/releases/download/0.5.0/cirros-0.5.0-x86_64-disk.img""","image_url: ""http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img""",3,3
openstack%2Fbifrost~stable%2Ftrain~I7268825cc1e6c63525565c6313b798b7680edc14,openstack/bifrost,stable/train,I7268825cc1e6c63525565c6313b798b7680edc14,Add release note for CentOS 8 and Ansible 2.8,MERGED,2020-03-23 11:04:30.000000000,2020-03-23 11:27:16.000000000,2020-03-23 11:25:32.000000000,"[{'_account_id': 10239}, {'_account_id': 22348}, {'_account_id': 23851}]","[{'number': 1, 'created': '2020-03-23 11:04:30.000000000', 'files': ['releasenotes/notes/centos8-fbeb6b3fa93ea384.yaml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/172bad1c3bb481b550c89d81f8a4308f7cef44b2', 'message': 'Add release note for CentOS 8 and Ansible 2.8\n\nChange-Id: I7268825cc1e6c63525565c6313b798b7680edc14\n'}]",0,714420,172bad1c3bb481b550c89d81f8a4308f7cef44b2,8,3,1,14826,,,0,"Add release note for CentOS 8 and Ansible 2.8

Change-Id: I7268825cc1e6c63525565c6313b798b7680edc14
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/20/714420/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/notes/centos8-fbeb6b3fa93ea384.yaml'],1,172bad1c3bb481b550c89d81f8a4308f7cef44b2,,"--- features: - | Adds support for CentOS 8. Support for CentOS 7 will be dropped in Ussuri. upgrade: - | The default version of Ansible becomes 2.8, replacing version 2.6 that is EOL. This version guarantees full bug fix and security patches and has a better support for Python 3.x. ",,9,0
openstack%2Fcinder~stable%2Frocky~I942d9d8b3976232ae1cf82b698fb27285fbef13a,openstack/cinder,stable/rocky,I942d9d8b3976232ae1cf82b698fb27285fbef13a,ChunkedBackupDriver: Freeing memory on restore,MERGED,2020-03-05 15:31:05.000000000,2020-03-23 11:25:13.000000000,2020-03-23 11:25:13.000000000,"[{'_account_id': 1736}, {'_account_id': 4523}, {'_account_id': 5997}, {'_account_id': 9535}, {'_account_id': 9732}, {'_account_id': 10118}, {'_account_id': 10459}, {'_account_id': 11611}, {'_account_id': 11904}, {'_account_id': 12369}, {'_account_id': 18883}, {'_account_id': 19933}, {'_account_id': 21863}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23613}, {'_account_id': 24230}, {'_account_id': 24921}, {'_account_id': 25243}, {'_account_id': 26077}, {'_account_id': 26537}, {'_account_id': 28801}, {'_account_id': 29716}, {'_account_id': 30590}]","[{'number': 1, 'created': '2020-03-05 15:31:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/64fddd4efac048f5d220b8a51ea0121af13c27cb', 'message': 'ChunkedBackupDriver: Freeing memory on restore\n\nWhen restoring many backups, the backup service uses a lot of memory and\nwhen we do many concurrent restores the backup service ends up getting\nkilled becase the system runs out of memory.\n\nThis patch reduces the ref count to the data as soon as possible to let\nPython garbage collect it instead of hogging it for the whole chunk\nrestore.\n\nRelated-Bug: #1865011\nChange-Id: I942d9d8b3976232ae1cf82b698fb27285fbef13a\n(cherry picked from commit 69462315bc8416bd02ebd6085f0cfc141cfd8877)\n'}, {'number': 2, 'created': '2020-03-10 15:39:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/eb70443cdb99a66579bd11a3ba40b06de87c4e17', 'message': 'ChunkedBackupDriver: Freeing memory on restore\n\nWhen restoring many backups, the backup service uses a lot of memory and\nwhen we do many concurrent restores the backup service ends up getting\nkilled becase the system runs out of memory.\n\nThis patch reduces the ref count to the data as soon as possible to let\nPython garbage collect it instead of hogging it for the whole chunk\nrestore.\n\nRelated-Bug: #1865011\nChange-Id: I942d9d8b3976232ae1cf82b698fb27285fbef13a\n(cherry picked from commit 69462315bc8416bd02ebd6085f0cfc141cfd8877)\n(cherry picked from commit 94db15dd3b86c7da92ceaa4b8c2d948b2b30c65b)\n'}, {'number': 3, 'created': '2020-03-10 19:58:08.000000000', 'files': ['cinder/backup/chunkeddriver.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/20b6b7477daae43907f38337aef15a5f47a92283', 'message': 'ChunkedBackupDriver: Freeing memory on restore\n\nWhen restoring many backups, the backup service uses a lot of memory and\nwhen we do many concurrent restores the backup service ends up getting\nkilled becase the system runs out of memory.\n\nThis patch reduces the ref count to the data as soon as possible to let\nPython garbage collect it instead of hogging it for the whole chunk\nrestore.\n\nRelated-Bug: #1865011\nChange-Id: I942d9d8b3976232ae1cf82b698fb27285fbef13a\n(cherry picked from commit 69462315bc8416bd02ebd6085f0cfc141cfd8877)\n(cherry picked from commit 94db15dd3b86c7da92ceaa4b8c2d948b2b30c65b)\n(cherry picked from commit f4aa81424fff8718bd432cc5a7aae357fe028023)\n'}]",1,711481,20b6b7477daae43907f38337aef15a5f47a92283,69,25,3,7130,,,0,"ChunkedBackupDriver: Freeing memory on restore

When restoring many backups, the backup service uses a lot of memory and
when we do many concurrent restores the backup service ends up getting
killed becase the system runs out of memory.

This patch reduces the ref count to the data as soon as possible to let
Python garbage collect it instead of hogging it for the whole chunk
restore.

Related-Bug: #1865011
Change-Id: I942d9d8b3976232ae1cf82b698fb27285fbef13a
(cherry picked from commit 69462315bc8416bd02ebd6085f0cfc141cfd8877)
(cherry picked from commit 94db15dd3b86c7da92ceaa4b8c2d948b2b30c65b)
(cherry picked from commit f4aa81424fff8718bd432cc5a7aae357fe028023)
",git fetch https://review.opendev.org/openstack/cinder refs/changes/81/711481/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/backup/chunkeddriver.py'],1,64fddd4efac048f5d220b8a51ea0121af13c27cb,bug/1865011-stable/rocky, body = None # Allow Python to free it decompressed = None # Allow Python to free it body = None # Allow Python to free it,,3,0
openstack%2Fcharm-nova-cloud-controller~stable%2F20.02~I2ee633ba0b445025a789a77e62950cd572636c6c,openstack/charm-nova-cloud-controller,stable/20.02,I2ee633ba0b445025a789a77e62950cd572636c6c,Fix action replay for clear-knownhost-cache,MERGED,2020-03-18 20:00:16.000000000,2020-03-23 11:00:39.000000000,2020-03-23 11:00:39.000000000,"[{'_account_id': 13686}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-18 20:00:16.000000000', 'files': ['actions/actions.py', 'unit_tests/test_actions.py'], 'web_link': 'https://opendev.org/openstack/charm-nova-cloud-controller/commit/aa8670b427bb6f7dd6391919f47b083fdbb78325', 'message': 'Fix action replay for clear-knownhost-cache\n\nThe return key was illegal in Juju, so this patchset makes\nit legal.\n\nChange-Id: I2ee633ba0b445025a789a77e62950cd572636c6c\nPartial-Bug: #1860743\n(cherry picked from commit 67dfc8d882995b11707b42f4758f4d9c303eb7b6)\n'}]",0,713728,aa8670b427bb6f7dd6391919f47b083fdbb78325,9,4,1,20870,,,0,"Fix action replay for clear-knownhost-cache

The return key was illegal in Juju, so this patchset makes
it legal.

Change-Id: I2ee633ba0b445025a789a77e62950cd572636c6c
Partial-Bug: #1860743
(cherry picked from commit 67dfc8d882995b11707b42f4758f4d9c303eb7b6)
",git fetch https://review.opendev.org/openstack/charm-nova-cloud-controller refs/changes/28/713728/1 && git format-patch -1 --stdout FETCH_HEAD,"['actions/actions.py', 'unit_tests/test_actions.py']",2,aa8670b427bb6f7dd6391919f47b083fdbb78325,bug/1860743-stable/20.02," ""units-updated"": [{'aservice/2': '10.0.0.2'}] ""units-updated"": [ ""units-updated"": ["," ""Units updated"": [{'aservice/2': '10.0.0.2'}] ""Units updated"": [ ""Units updated"": [",4,4
openstack%2Ftripleo-quickstart-extras~master~I8f56ca5597a29533ab744387c502f9515e53b485,openstack/tripleo-quickstart-extras,master,I8f56ca5597a29533ab744387c502f9515e53b485,Use current stackviz tarball,MERGED,2020-02-26 08:31:00.000000000,2020-03-23 10:51:31.000000000,2020-03-23 10:51:30.000000000,"[{'_account_id': 9592}, {'_account_id': 14985}, {'_account_id': 20172}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-02-26 08:31:00.000000000', 'files': ['roles/validate-tempest/defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart-extras/commit/06787d57570c75e0e83fdabbe450394330dc5e43', 'message': 'Use current stackviz tarball\n\nhttps://tarballs.openstack.org/package-stackviz-element/ has not been\nupdated since 2017, use the current location for the stackviz tarball.\n\nChange-Id: I8f56ca5597a29533ab744387c502f9515e53b485\n'}]",0,709979,06787d57570c75e0e83fdabbe450394330dc5e43,8,5,1,6547,,,0,"Use current stackviz tarball

https://tarballs.openstack.org/package-stackviz-element/ has not been
updated since 2017, use the current location for the stackviz tarball.

Change-Id: I8f56ca5597a29533ab744387c502f9515e53b485
",git fetch https://review.opendev.org/openstack/tripleo-quickstart-extras refs/changes/79/709979/1 && git format-patch -1 --stdout FETCH_HEAD,['roles/validate-tempest/defaults/main.yml'],1,06787d57570c75e0e83fdabbe450394330dc5e43,fix-stackviz,"stackviz_tarball: ""https://tarballs.opendev.org/openstack/stackviz/dist/stackviz-latest.tar.gz""","stackviz_tarball: ""https://tarballs.openstack.org/package-stackviz-element/stackviz-latest.tar.gz""",1,1
openstack%2Fopenstack-ansible-ops~master~I3e31373be3af65c8a2b54ad32c8ea24c72ab5a41,openstack/openstack-ansible-ops,master,I3e31373be3af65c8a2b54ad32c8ea24c72ab5a41,Fix typo elasticserch to elasticsearch,MERGED,2020-03-20 14:55:41.000000000,2020-03-23 10:39:42.000000000,2020-03-23 10:37:54.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 29865}, {'_account_id': 31749}]","[{'number': 1, 'created': '2020-03-20 14:55:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-ops/commit/b7c2e1e99c1ce45e00ab153007da56888e590fa9', 'message': 'Fix typo elasticserch to elasticsearch\n\nChange-Id: I3e31373be3af65c8a2b54ad32c8ea24c72ab5a41\n'}, {'number': 2, 'created': '2020-03-20 16:23:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-ops/commit/e707f8392334caf4042ffcf990f428d27ab75edb', 'message': 'Fix typo elasticserch to elasticsearch\n\nChange-Id: I3e31373be3af65c8a2b54ad32c8ea24c72ab5a41\n'}, {'number': 3, 'created': '2020-03-23 08:47:25.000000000', 'files': ['elk_metrics_7x/roles/elasticsearch/templates/elasticsearch.yml.j2', 'elk_metrics_7x/tests/testLayout.yml', 'elk_metrics_7x/roles/elastic_data_hosts/vars/data-node-variables.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-ops/commit/8388a54f52519c128100e9aeb36ead942b18b2c8', 'message': 'Fix typo elasticserch to elasticsearch\n\nChange-Id: I3e31373be3af65c8a2b54ad32c8ea24c72ab5a41\n'}]",0,714134,8388a54f52519c128100e9aeb36ead942b18b2c8,14,4,3,31749,,,0,"Fix typo elasticserch to elasticsearch

Change-Id: I3e31373be3af65c8a2b54ad32c8ea24c72ab5a41
",git fetch https://review.opendev.org/openstack/openstack-ansible-ops refs/changes/34/714134/2 && git format-patch -1 --stdout FETCH_HEAD,"['elk_metrics_6x/roles/elastic_data_hosts/vars/data-node-variables.yml', 'elk_metrics_6x/tests/testLayout.yml', 'elk_metrics_7x/roles/elasticsearch/templates/elasticsearch.yml.j2', 'elk_metrics_6x/roles/elasticsearch/templates/elasticsearch.yml.j2', 'elk_metrics_7x/tests/testLayout.yml', 'elk_metrics_7x/roles/elastic_data_hosts/vars/data-node-variables.yml']",6,b7c2e1e99c1ce45e00ab153007da56888e590fa9,correct-elasticsearch-typo,elasticsearch_interface_speed: |-,elasticserch_interface_speed: |-,7,7
openstack%2Fopenstack-manuals~master~Idee902381e40504f3186d02ff07939ef259b92d5,openstack/openstack-manuals,master,Idee902381e40504f3186d02ff07939ef259b92d5,Remove dead repos,MERGED,2020-03-20 16:38:06.000000000,2020-03-23 10:34:33.000000000,2020-03-23 10:05:48.000000000,"[{'_account_id': 10607}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 16:38:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/26417cddf80ffac1db560b5a9ce07135dfef3fc2', 'message': 'Remove dead repos\n\npynotedb and subunit2sql are not part of OpenStack anymore, remove links\nfor latest from index pages.\n\nChange-Id: Idee902381e40504f3186d02ff07939ef259b92d5\n'}, {'number': 2, 'created': '2020-03-20 20:47:54.000000000', 'files': ['www/project-data/latest.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/dc0f953b4f4c7932670960e90250c8f3b2477b86', 'message': 'Remove dead repos\n\npynotedb and subunit2sql are not part of OpenStack anymore, remove links\nfor latest from index pages.\n\nChange-Id: Idee902381e40504f3186d02ff07939ef259b92d5\n'}]",0,714172,dc0f953b4f4c7932670960e90250c8f3b2477b86,10,2,2,6547,,,0,"Remove dead repos

pynotedb and subunit2sql are not part of OpenStack anymore, remove links
for latest from index pages.

Change-Id: Idee902381e40504f3186d02ff07939ef259b92d5
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/72/714172/1 && git format-patch -1 --stdout FETCH_HEAD,['www/project-data/latest.yaml'],1,26417cddf80ffac1db560b5a9ce07135dfef3fc2,infra-manual,,- name: pynotedb service: Python parser for notedb type: tool - name: subunit2sql service: Tooling for converting subunit streams into a SQL DB type: tool ,0,8
openstack%2Fnova~master~Ia8dbe8ba61ec6d1b8498918a53a103a6eff4d488,openstack/nova,master,Ia8dbe8ba61ec6d1b8498918a53a103a6eff4d488,Rename 'nova.network.security_group.neutron_driver' -> 'nova.network.security_group',ABANDONED,2019-11-29 17:18:03.000000000,2020-03-23 10:32:51.000000000,,[],"[{'number': 1, 'created': '2019-11-29 17:18:03.000000000', 'files': ['nova/api/openstack/compute/security_groups.py', 'nova/api/openstack/compute/views/servers.py', 'nova/tests/unit/network/test_security_group.py', 'nova/network/security_group.py', 'nova/api/metadata/base.py', 'nova/tests/unit/api/openstack/fakes.py', 'nova/network/security_group/security_group_base.py', 'nova/tests/unit/api/openstack/compute/test_security_groups.py', 'nova/compute/api.py', 'nova/tests/functional/api_sample_tests/test_security_groups.py', 'nova/network/security_group/__init__.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/c9982944b0317d9ea0703bfc108a967e46f96b2f', 'message': ""Rename 'nova.network.security_group.neutron_driver' -> 'nova.network.security_group'\n\nAnother layer of complexity removed.\n\nChange-Id: Ia8dbe8ba61ec6d1b8498918a53a103a6eff4d488\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n""}]",0,696744,c9982944b0317d9ea0703bfc108a967e46f96b2f,2,0,1,15334,,,0,"Rename 'nova.network.security_group.neutron_driver' -> 'nova.network.security_group'

Another layer of complexity removed.

Change-Id: Ia8dbe8ba61ec6d1b8498918a53a103a6eff4d488
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/nova refs/changes/44/696744/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/api/openstack/compute/security_groups.py', 'nova/api/openstack/compute/views/servers.py', 'nova/network/security_group.py', 'nova/tests/unit/network/test_security_group.py', 'nova/api/metadata/base.py', 'nova/tests/unit/api/openstack/fakes.py', 'nova/network/security_group/security_group_base.py', 'nova/tests/unit/api/openstack/compute/test_security_groups.py', 'nova/compute/api.py', 'nova/tests/functional/api_sample_tests/test_security_groups.py', 'nova/network/security_group/__init__.py']",11,c9982944b0317d9ea0703bfc108a967e46f96b2f,bp/remove-nova-network-ussuri,,,233,309
openstack%2Fnova~master~I329f0fd589a4b2e0426485f09f6782f94275cc07,openstack/nova,master,I329f0fd589a4b2e0426485f09f6782f94275cc07,Rename 'nova.network.neutronv2' -> 'nova.network',ABANDONED,2019-11-29 17:18:03.000000000,2020-03-23 10:32:32.000000000,,[],"[{'number': 1, 'created': '2019-11-29 17:18:03.000000000', 'files': ['nova/tests/unit/virt/vmwareapi/stubs.py', 'nova/network/constants.py', 'nova/tests/unit/notifications/test_base.py', 'nova/tests/unit/network/test_api.py', 'nova/api/metadata/handler.py', 'nova/api/openstack/common.py', 'nova/tests/functional/api_sample_tests/test_networks.py', 'nova/api/openstack/compute/migrate_server.py', 'nova/tests/functional/test_metadata.py', 'nova/notifications/base.py', 'nova/tests/functional/api_sample_tests/test_floating_ip_pools.py', 'nova/network/api.py', 'nova/tests/unit/cmd/test_manage.py', 'nova/tests/unit/compute/test_shelve.py', 'nova/compute/manager.py', 'nova/tests/unit/api/openstack/compute/test_migrate_server.py', 'nova/tests/unit/network/security_group/test_neutron_driver.py', 'nova/api/openstack/compute/tenant_networks.py', 'nova/tests/unit/test_fixtures.py', 'nova/tests/unit/compute/test_compute.py', 'nova/cmd/manage.py', 'nova/conductor/tasks/live_migrate.py', 'nova/network/__init__.py', 'nova/tests/unit/test_profiler.py', 'nova/virt/vmwareapi/vmops.py', 'nova/tests/functional/test_nova_manage.py', 'nova/tests/unit/compute/test_compute_mgr.py', 'nova/tests/functional/api_sample_tests/test_servers.py', 'nova/api/metadata/base.py', 'nova/tests/functional/test_servers.py', 'nova/api/openstack/compute/shelve.py', 'nova/tests/unit/conductor/test_conductor.py', 'nova/api/openstack/compute/evacuate.py', 'nova/api/openstack/compute/floating_ip_pools.py', 'nova/tests/unit/api/openstack/compute/test_server_actions.py', 'nova/tests/functional/api_sample_tests/test_attach_interfaces.py', 'nova/tests/unit/test_metadata.py', 'nova/network/base_api.py', 'nova/tests/unit/api/openstack/compute/test_networks.py', 'nova/tests/unit/api/openstack/fakes.py', 'nova/api/openstack/compute/floating_ips.py', 'nova/tests/unit/api/openstack/compute/test_security_groups.py', 'nova/compute/api.py', 'nova/conductor/tasks/cross_cell_migrate.py', 'nova/tests/unit/api/openstack/test_common.py', 'nova/network/neutronv2/__init__.py', 'nova/network/security_group/neutron_driver.py', 'nova/api/openstack/compute/attach_interfaces.py', 'nova/tests/unit/compute/test_compute_api.py', 'nova/tests/fixtures.py', 'nova/api/openstack/compute/schemas/servers.py', 'nova/api/openstack/compute/networks.py', 'nova/conductor/manager.py', 'nova/tests/unit/api/openstack/compute/test_evacuate.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/5f6ece1fa3456927e35f4b36261881fadec99c5c', 'message': ""Rename 'nova.network.neutronv2' -> 'nova.network'\n\nThere is only one true networking API and it's name is neutron.\n\nChange-Id: I329f0fd589a4b2e0426485f09f6782f94275cc07\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n""}]",0,696743,5f6ece1fa3456927e35f4b36261881fadec99c5c,2,0,1,15334,,,0,"Rename 'nova.network.neutronv2' -> 'nova.network'

There is only one true networking API and it's name is neutron.

Change-Id: I329f0fd589a4b2e0426485f09f6782f94275cc07
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/nova refs/changes/43/696743/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/virt/vmwareapi/stubs.py', 'nova/network/constants.py', 'nova/tests/unit/notifications/test_base.py', 'nova/tests/unit/network/test_api.py', 'nova/api/metadata/handler.py', 'nova/api/openstack/common.py', 'nova/tests/functional/api_sample_tests/test_networks.py', 'nova/api/openstack/compute/migrate_server.py', 'nova/tests/functional/test_metadata.py', 'nova/notifications/base.py', 'nova/tests/functional/api_sample_tests/test_floating_ip_pools.py', 'nova/network/api.py', 'nova/tests/unit/cmd/test_manage.py', 'nova/tests/unit/compute/test_shelve.py', 'nova/compute/manager.py', 'nova/tests/unit/api/openstack/compute/test_migrate_server.py', 'nova/tests/unit/network/security_group/test_neutron_driver.py', 'nova/api/openstack/compute/tenant_networks.py', 'nova/tests/unit/test_fixtures.py', 'nova/tests/unit/compute/test_compute.py', 'nova/cmd/manage.py', 'nova/conductor/tasks/live_migrate.py', 'nova/network/__init__.py', 'nova/tests/unit/test_profiler.py', 'nova/virt/vmwareapi/vmops.py', 'nova/tests/functional/test_nova_manage.py', 'nova/tests/unit/compute/test_compute_mgr.py', 'nova/tests/functional/api_sample_tests/test_servers.py', 'nova/api/metadata/base.py', 'nova/tests/functional/test_servers.py', 'nova/api/openstack/compute/shelve.py', 'nova/tests/unit/conductor/test_conductor.py', 'nova/api/openstack/compute/evacuate.py', 'nova/api/openstack/compute/floating_ip_pools.py', 'nova/tests/unit/api/openstack/compute/test_server_actions.py', 'nova/tests/functional/api_sample_tests/test_attach_interfaces.py', 'nova/tests/unit/test_metadata.py', 'nova/network/base_api.py', 'nova/tests/unit/api/openstack/compute/test_networks.py', 'nova/tests/unit/api/openstack/fakes.py', 'nova/api/openstack/compute/floating_ips.py', 'nova/tests/unit/api/openstack/compute/test_security_groups.py', 'nova/compute/api.py', 'nova/conductor/tasks/cross_cell_migrate.py', 'nova/tests/unit/api/openstack/test_common.py', 'nova/network/neutronv2/__init__.py', 'nova/network/security_group/neutron_driver.py', 'nova/api/openstack/compute/attach_interfaces.py', 'nova/tests/unit/compute/test_compute_api.py', 'nova/tests/fixtures.py', 'nova/api/openstack/compute/schemas/servers.py', 'nova/api/openstack/compute/networks.py', 'nova/conductor/manager.py', 'nova/tests/unit/api/openstack/compute/test_evacuate.py']",54,5f6ece1fa3456927e35f4b36261881fadec99c5c,bp/remove-nova-network-ussuri, fixtures.MockPatch('nova.network.api.API.list_ports')).mock fixtures.MockPatch('nova.network.api.API.list_ports')).mock, fixtures.MockPatch( 'nova.network.neutronv2.api.API.list_ports')).mock fixtures.MockPatch( 'nova.network.neutronv2.api.API.list_ports')).mock,320,666
openstack%2Fnova~master~I08aa4d581720b0f6cd1dabccc98dac210d1a1663,openstack/nova,master,I08aa4d581720b0f6cd1dabccc98dac210d1a1663,nova-net: Remove dependency on nova-net from fake cache,ABANDONED,2019-11-29 17:18:03.000000000,2020-03-23 10:32:19.000000000,,[],"[{'number': 1, 'created': '2019-11-29 17:18:03.000000000', 'files': ['nova/tests/unit/fake_network.py', 'nova/tests/unit/virt/test_netutils.py', 'nova/tests/unit/virt/xenapi/test_xenapi.py', 'nova/tests/unit/test_notifications.py', 'nova/tests/unit/api/openstack/compute/test_floating_ips.py', 'nova/tests/unit/network/test_manager.py', 'nova/tests/unit/virt/libvirt/test_driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/7aa584fc054d1d0d98fb5aefc8a04336efdec86b', 'message': ""nova-net: Remove dependency on nova-net from fake cache\n\nThere's more to be done here around making the cached objects look like\nsomething that would be generated by neutron, but this is good enough\nfor now.\n\nChange-Id: I08aa4d581720b0f6cd1dabccc98dac210d1a1663\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n""}]",0,696742,7aa584fc054d1d0d98fb5aefc8a04336efdec86b,2,0,1,15334,,,0,"nova-net: Remove dependency on nova-net from fake cache

There's more to be done here around making the cached objects look like
something that would be generated by neutron, but this is good enough
for now.

Change-Id: I08aa4d581720b0f6cd1dabccc98dac210d1a1663
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/nova refs/changes/42/696742/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/fake_network.py', 'nova/tests/unit/virt/test_netutils.py', 'nova/tests/unit/virt/xenapi/test_xenapi.py', 'nova/tests/unit/test_notifications.py', 'nova/tests/unit/api/openstack/compute/test_floating_ips.py', 'nova/tests/unit/network/test_manager.py', 'nova/tests/unit/virt/libvirt/test_driver.py']",7,7aa584fc054d1d0d98fb5aefc8a04336efdec86b,bp/remove-nova-network-ussuri," network_info = _fake_network_info(self) _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), # TODO(stephenfin): Fix this _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self), _fake_network_info(self)) self.context, ins_ref, _fake_network_info(self)) self.context, ins_ref, _fake_network_info(self)) fake_net = _fake_network_info(self) fake_net = _fake_network_info(self) self.context, ins_ref, _fake_network_info(self)) self.context, ins_ref, _fake_network_info(self)) network_info = _fake_network_info(self) network_info = _fake_network_info(self) network_info = _fake_network_info(self) vif = _fake_network_info(self)[0] network_info = _fake_network_info(self) network_info = _fake_network_info(self)"," network_info = _fake_network_info(self, 4) _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1), _fake_network_info(self, 1)) self.context, ins_ref, _fake_network_info(self, 1)) self.context, ins_ref, _fake_network_info(self, 1)) fake_net = _fake_network_info(self, 1) fake_net = _fake_network_info(self, 1) self.context, ins_ref, _fake_network_info(self, 1)) self.context, ins_ref, _fake_network_info(self, 1)) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1) vif = _fake_network_info(self, 1)[0] network_info = _fake_network_info(self, 1) network_info = _fake_network_info(self, 1)",167,217
openstack%2Fnova~master~I4350b6f3ca3fd7de1bc6db3e14fe261f2746b002,openstack/nova,master,I4350b6f3ca3fd7de1bc6db3e14fe261f2746b002,nova-net: Remove 'MetadataManager',ABANDONED,2019-11-29 17:18:03.000000000,2020-03-23 10:28:00.000000000,,[],"[{'number': 1, 'created': '2019-11-29 17:18:03.000000000', 'files': ['nova/service.py', 'nova/api/manager.py', 'nova/tests/unit/test_profiler.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/71b1051ecb7e203f6436870a28749f9c007a03ed', 'message': ""nova-net: Remove 'MetadataManager'\n\nThis was only applying some iptables rules when running under\nnova-network. Since that's no longer possible, we don't need to keep the\nmanager around. So don't!\n\nChange-Id: I4350b6f3ca3fd7de1bc6db3e14fe261f2746b002\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n""}]",0,696741,71b1051ecb7e203f6436870a28749f9c007a03ed,2,0,1,15334,,,0,"nova-net: Remove 'MetadataManager'

This was only applying some iptables rules when running under
nova-network. Since that's no longer possible, we don't need to keep the
manager around. So don't!

Change-Id: I4350b6f3ca3fd7de1bc6db3e14fe261f2746b002
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/nova refs/changes/41/696741/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/service.py', 'nova/api/manager.py', 'nova/tests/unit/test_profiler.py']",3,71b1051ecb7e203f6436870a28749f9c007a03ed,bp/remove-nova-network-ussuri,," 'nova.api.manager.MetadataManager',",0,29
openstack%2Fcinder~master~I6b420aec9ec417e2446c1c4253098cebbdcc820e,openstack/cinder,master,I6b420aec9ec417e2446c1c4253098cebbdcc820e,RBD: Fix volume delete error for missed parent,NEW,2020-02-23 09:31:13.000000000,2020-03-23 10:26:18.000000000,,"[{'_account_id': 4523}, {'_account_id': 8037}, {'_account_id': 9008}, {'_account_id': 9236}, {'_account_id': 9732}, {'_account_id': 10118}, {'_account_id': 11611}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 12988}, {'_account_id': 14384}, {'_account_id': 15386}, {'_account_id': 15670}, {'_account_id': 15831}, {'_account_id': 15941}, {'_account_id': 18120}, {'_account_id': 18883}, {'_account_id': 19933}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 21976}, {'_account_id': 22126}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23613}, {'_account_id': 24815}, {'_account_id': 24921}, {'_account_id': 25243}, {'_account_id': 25678}, {'_account_id': 26077}, {'_account_id': 26537}, {'_account_id': 28801}, {'_account_id': 29705}, {'_account_id': 29716}, {'_account_id': 30688}, {'_account_id': 31416}]","[{'number': 1, 'created': '2020-02-23 09:31:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/52f295246f78170ec7aac04d25e4dec0b5360ed9', 'message': ""RBD: Fix volume delete error caused by\n\nTry to get renamed parent deleted in Cinder,\nif the parent of the volume in delete_volume\nflow is NotFound.\n\nFix volume is deleted error because parent volume\nis deleted parallely in the volume deleting flows.\nVolume may be deleted error caused by parent volume\nrename to end with '.deleted'.\n\nChange-Id: I6b420aec9ec417e2446c1c4253098cebbdcc820e\nCloses-Bug: #1641518\n""}, {'number': 2, 'created': '2020-02-23 09:40:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6e3c3e31842fa7b83153d7f15b53ff206ec76981', 'message': ""RBD: Fix volume delete error for missed parent\n\nTry to get renamed parent deleted in Cinder,\nif the parent of the volume in delete_volume\nflow is NotFound.\n\nFix volume is deleted error because parent volume\nis deleted parallely in the volume deleting flows.\nVolume may be deleted error caused by parent volume\nrename to end with '.deleted'.\n\nChange-Id: I6b420aec9ec417e2446c1c4253098cebbdcc820e\nCloses-Bug: #1641518\n""}, {'number': 3, 'created': '2020-02-24 01:15:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/234c1d237425fd77c2363879461af2570d96803a', 'message': ""RBD: Fix volume delete error for missed parent\n\nTry to get renamed parent deleted in Cinder,\nif the parent of the volume in delete_volume\nflow is NotFound.\n\nFix volume is deleted error because parent volume\nis deleted parallely in the volume deleting flows.\nVolume may be deleted error caused by parent volume\nrename to end with '.deleted'.\n\nChange-Id: I6b420aec9ec417e2446c1c4253098cebbdcc820e\nCloses-Bug: #1641518\n""}, {'number': 4, 'created': '2020-02-25 10:05:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9722027a67dc1dce2ab5190a230de08db2422469', 'message': ""RBD: Fix volume delete error for missed parent\n\nTry to get renamed parent deleted in Cinder,\nif the parent of the volume in delete_volume\nflow is NotFound.\n\nFix volume is deleted error because parent volume\nis deleted parallely in the volume deleting flows.\nVolume may be deleted error caused by parent volume\nrename to end with '.deleted'.\n\nChange-Id: I6b420aec9ec417e2446c1c4253098cebbdcc820e\nCloses-Bug: #1641518\n""}, {'number': 5, 'created': '2020-02-28 03:57:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/7e3ec632a6a8786d346df604919543268ad0b581', 'message': ""RBD: Fix volume delete error for missed parent\n\nTry to get renamed parent deleted in Cinder,\nif the parent of the volume in delete_volume\nflow is NotFound.\n\nFix volume is deleted error because parent volume\nis deleted parallely in the volume deleting flows.\nVolume may be deleted error caused by parent volume\nrename to end with '.deleted'.\n\nChange-Id: I6b420aec9ec417e2446c1c4253098cebbdcc820e\nCloses-Bug: #1641518\n""}, {'number': 6, 'created': '2020-03-19 02:56:58.000000000', 'files': ['cinder/volume/drivers/rbd.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/3ec68c6c3d93dec77d0aa0f3329c89e3674b6838', 'message': ""RBD: Fix volume delete error for missed parent\n\nTry to get renamed parent deleted in Cinder,\nif the parent of the volume in delete_volume\nflow is NotFound.\n\nFix volume is deleted error because parent volume\nis deleted parallely in the volume deleting flows.\nVolume may be deleted error caused by parent volume\nrename to end with '.deleted'.\n\nChange-Id: I6b420aec9ec417e2446c1c4253098cebbdcc820e\nCloses-Bug: #1641518\n""}]",0,709342,3ec68c6c3d93dec77d0aa0f3329c89e3674b6838,151,39,6,8037,,,0,"RBD: Fix volume delete error for missed parent

Try to get renamed parent deleted in Cinder,
if the parent of the volume in delete_volume
flow is NotFound.

Fix volume is deleted error because parent volume
is deleted parallely in the volume deleting flows.
Volume may be deleted error caused by parent volume
rename to end with '.deleted'.

Change-Id: I6b420aec9ec417e2446c1c4253098cebbdcc820e
Closes-Bug: #1641518
",git fetch https://review.opendev.org/openstack/cinder refs/changes/42/709342/6 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/drivers/rbd.py'],1,52f295246f78170ec7aac04d25e4dec0b5360ed9,bug/1641518," try: parent_rbd = self.rbd.Image(client.ioctx, parent_name) except self.rbd.ImageNotFound: if not parent_name.endswith('.deleted'): # If parent has been deleted in Cinder, # try to check renamed parent. parent_name += '.deleted' parent_rbd = self.rbd.Image(client.ioctx, parent_name) else: LOG.warning('Volume %s have been deleted.', parent_name) return"," parent_rbd = self.rbd.Image(client.ioctx, parent_name)",11,1
openstack%2Fkolla~master~Ic82c59a5e78078b4fea10df9d30b35da14cad922,openstack/kolla,master,Ic82c59a5e78078b4fea10df9d30b35da14cad922,Zun: add zun-cni-daemon image,MERGED,2020-02-17 23:07:04.000000000,2020-03-23 10:18:28.000000000,2020-03-23 10:16:40.000000000,"[{'_account_id': 167}, {'_account_id': 7488}, {'_account_id': 11536}, {'_account_id': 14826}, {'_account_id': 19316}, {'_account_id': 22165}, {'_account_id': 22348}, {'_account_id': 22629}, {'_account_id': 23181}, {'_account_id': 23717}, {'_account_id': 24072}, {'_account_id': 30491}, {'_account_id': 30523}]","[{'number': 1, 'created': '2020-02-17 23:07:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/91d570a4143e064f21fd7f937b80f1e17615c526', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 2, 'created': '2020-02-17 23:07:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/8f79957bc81d428fe0d341fb60d8b5b1f9c6a406', 'message': '[WIP] Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 3, 'created': '2020-02-18 03:12:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/eef537a8766d6b2d4e50c4ec11fe486b8856b5c0', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 4, 'created': '2020-02-20 02:13:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/36dc5697b7d51e5ca824fa3ef2b30c1aab845708', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 5, 'created': '2020-02-23 17:15:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/1eebac32942b402a21eca5d69959fb095f6fc052', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 6, 'created': '2020-02-24 03:43:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/7dbb2509164c9423feb03bffb2773f44e6ec6ac0', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 7, 'created': '2020-02-24 04:30:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/6000c582ce42e5b067c235acf94c6940d7c1647a', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 8, 'created': '2020-02-24 05:02:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/d8052f5aba26335db300d000a3587e026aca440d', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 9, 'created': '2020-02-26 03:48:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/43a8fe9bc1355652dba564625f1a67e778e89d30', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 10, 'created': '2020-02-29 23:25:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/19932f1468b928e9053707aa6dc25c977b0b7db3', 'message': '[WIP] Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 11, 'created': '2020-03-01 20:33:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/872c7e4b94d9e8d52774a91c97b35cbddb1444ad', 'message': '[WIP] Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 12, 'created': '2020-03-01 20:53:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/8e7336d86eb9a059d426adb3c45c1799f1e82bd5', 'message': '[WIP] Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 13, 'created': '2020-03-01 20:56:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/5ba52749fdc0c02426a8f2f0523ba5f86bc0f43c', 'message': '[WIP] Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 14, 'created': '2020-03-01 23:05:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/425396d1795648410e87753ce5b07d3c975d8ca0', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\nThis image is based on the zun base image and includes additional\npackages such as openvswitch.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 15, 'created': '2020-03-05 02:41:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/9b059e1e8315bb64f5eb7718239db4c01849d4f7', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\nThis image is based on the zun base image and includes additional\npackages such as openvswitch.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 16, 'created': '2020-03-13 03:59:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/23de65933b0be6a6db201467b7655209037dbc92', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\nThis image is based on the zun base image and includes additional\npackages such as openvswitch.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 17, 'created': '2020-03-18 02:36:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla/commit/5cb29b145ff953b51e3d861e92d1e53a103f0e8f', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\nThis image is based on the zun base image and includes additional\npackages such as openvswitch.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}, {'number': 18, 'created': '2020-03-21 16:44:25.000000000', 'files': ['releasenotes/notes/add-zun-cni-daemon-image-842bf6d7ba804dda.yaml', 'docker/zun/zun-cni-daemon/Dockerfile.j2', 'docker/zun/zun-cni-daemon/zun_sudoers'], 'web_link': 'https://opendev.org/openstack/kolla/commit/39820c4abb83c55e66e7760c5844754fc07e4599', 'message': 'Zun: add zun-cni-daemon image\n\nZun-cni-daemon is a new process for implementing CNI plugin for Zun.\nIt will be used by CRI runtime to connect podsandbox to neutron.\nThis image is based on the zun base image and includes additional\npackages such as openvswitch.\n\nNeeded-By: https://review.opendev.org/#/c/708213/\nChange-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922\n'}]",10,708273,39820c4abb83c55e66e7760c5844754fc07e4599,91,13,18,11536,,,0,"Zun: add zun-cni-daemon image

Zun-cni-daemon is a new process for implementing CNI plugin for Zun.
It will be used by CRI runtime to connect podsandbox to neutron.
This image is based on the zun base image and includes additional
packages such as openvswitch.

Needed-By: https://review.opendev.org/#/c/708213/
Change-Id: Ic82c59a5e78078b4fea10df9d30b35da14cad922
",git fetch https://review.opendev.org/openstack/kolla refs/changes/73/708273/15 && git format-patch -1 --stdout FETCH_HEAD,"['docker/zun/zun-cni-daemon/Dockerfile.j2', 'docker/zun/zun-cni-daemon/zun_sudoers']",2,91d570a4143e064f21fd7f937b80f1e17615c526,,zun ALL=(root) NOPASSWD: /var/lib/kolla/venv/bin/zun-rootwrap /etc/zun/rootwrap.conf * ,,22,0
openstack%2Fnova~master~Ia7c317e373e4037495d379d06eda19a71412d409,openstack/nova,master,Ia7c317e373e4037495d379d06eda19a71412d409,nova-live-migration: Only stop n-cpu and q-agt during evacuation testing,MERGED,2020-03-20 09:24:00.000000000,2020-03-23 10:18:18.000000000,2020-03-23 10:15:20.000000000,"[{'_account_id': 9008}, {'_account_id': 9708}, {'_account_id': 10118}, {'_account_id': 10135}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-03-20 09:24:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/08dc05d197f04946da01e86919c43d4dca13b1f0', 'message': 'nova-live-migration: Only stop n-cpu and q-agt during evacuation testing\n\nI8af2ad741ca08c3d88efb9aa817c4d1470491a23 started to correctly fence the\nsubnode ahead of evacuation testing but missed that c-vol and g-api\nwhere also running on the host. As a result the BFV evacuation test will\nfail if the volume being used is created on the c-vol backend hosted on\nthe subnode.\n\nThis change now avoids this by limiting the services stopped ahead of\nthe evacuation on the subnode to n-cpu and q-agt.\n\nChange-Id: Ia7c317e373e4037495d379d06eda19a71412d409\nCloses-Bug: #1868234\n'}, {'number': 2, 'created': '2020-03-21 17:08:47.000000000', 'files': ['gate/test_evacuate.sh'], 'web_link': 'https://opendev.org/openstack/nova/commit/1e16b3184d4e298c454ede7c56040f6d70276a0c', 'message': 'nova-live-migration: Only stop n-cpu and q-agt during evacuation testing\n\nI8af2ad741ca08c3d88efb9aa817c4d1470491a23 started to correctly fence the\nsubnode ahead of evacuation testing but missed that c-vol and g-api\nwhere also running on the host. As a result the BFV evacuation test will\nfail if the volume being used is created on the c-vol backend hosted on\nthe subnode.\n\nThis change now avoids this by limiting the services stopped ahead of\nthe evacuation on the subnode to n-cpu and q-agt.\n\nChange-Id: Ia7c317e373e4037495d379d06eda19a71412d409\nCloses-Bug: #1868234\n'}]",2,714057,1e16b3184d4e298c454ede7c56040f6d70276a0c,38,12,2,10135,,,0,"nova-live-migration: Only stop n-cpu and q-agt during evacuation testing

I8af2ad741ca08c3d88efb9aa817c4d1470491a23 started to correctly fence the
subnode ahead of evacuation testing but missed that c-vol and g-api
where also running on the host. As a result the BFV evacuation test will
fail if the volume being used is created on the c-vol backend hosted on
the subnode.

This change now avoids this by limiting the services stopped ahead of
the evacuation on the subnode to n-cpu and q-agt.

Change-Id: Ia7c317e373e4037495d379d06eda19a71412d409
Closes-Bug: #1868234
",git fetch https://review.opendev.org/openstack/nova refs/changes/57/714057/2 && git format-patch -1 --stdout FETCH_HEAD,['gate/test_evacuate.sh'],1,08dc05d197f04946da01e86919c43d4dca13b1f0,bug/1868234,"echo ""Stopping n-cpu, q-agt and guest domains on subnode"" $ANSIBLE subnodes --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""systemctl stop devstack@n-cpu devstack@q-agt""","echo ""Stopping all services and guest domains on subnode"" $ANSIBLE subnodes --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""systemctl stop devstack@*""",2,2
openstack%2Fcharm-mysql-innodb-cluster~master~I349cd154760f157a755cb9c29b07862fbb64793f,openstack/charm-mysql-innodb-cluster,master,I349cd154760f157a755cb9c29b07862fbb64793f,Use mysql-shell python,MERGED,2020-03-19 21:22:00.000000000,2020-03-23 10:17:09.000000000,2020-03-23 10:17:09.000000000,"[{'_account_id': 20648}, {'_account_id': 20805}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 21:22:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-mysql-innodb-cluster/commit/b96d677ae78e6e8be5957572fb7763ad1c8bab16', 'message': 'Use mysql-shell python\n\nCurrently the mysql-shell snap in the snap store is deb based and\ndefaults to JavaScript.\n\nThis change enables the use of the python interpreter for msyql-shell.\nIt also allows for a resource version of the mysql-shell snap for use\nduring development cycles.\n\nChange-Id: I349cd154760f157a755cb9c29b07862fbb64793f\n'}, {'number': 2, 'created': '2020-03-20 23:04:58.000000000', 'files': ['src/tests/bundles/eoan.yaml', 'test-requirements.txt', 'unit_tests/test_lib_charm_openstack_mysql_innodb_cluster.py', 'unit_tests/__init__.py', 'src/lib/charm/openstack/mysql_innodb_cluster.py', 'src/metadata.yaml', 'src/reactive/mysql_innodb_cluster_handlers.py', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/charm-mysql-innodb-cluster/commit/d7ce147e3744a21d62631cfb2f5bcefeff705a5c', 'message': 'Use mysql-shell python\n\nCurrently the mysql-shell snap in the snap store is deb based and\ndefaults to JavaScript.\n\nThis change enables the use of the python interpreter for msyql-shell.\nIt also allows for a resource version of the mysql-shell snap for use\nduring development cycles.\n\nChange-Id: I349cd154760f157a755cb9c29b07862fbb64793f\n'}]",3,713984,d7ce147e3744a21d62631cfb2f5bcefeff705a5c,15,4,2,20805,,,0,"Use mysql-shell python

Currently the mysql-shell snap in the snap store is deb based and
defaults to JavaScript.

This change enables the use of the python interpreter for msyql-shell.
It also allows for a resource version of the mysql-shell snap for use
during development cycles.

Change-Id: I349cd154760f157a755cb9c29b07862fbb64793f
",git fetch https://review.opendev.org/openstack/charm-mysql-innodb-cluster refs/changes/84/713984/2 && git format-patch -1 --stdout FETCH_HEAD,"['src/tests/bundles/eoan.yaml', 'test-requirements.txt', 'unit_tests/test_lib_charm_openstack_mysql_innodb_cluster.py', 'unit_tests/__init__.py', 'src/lib/charm/openstack/mysql_innodb_cluster.py', 'src/metadata.yaml', 'src/reactive/mysql_innodb_cluster_handlers.py', 'tox.ini']",8,b96d677ae78e6e8be5957572fb7763ad1c8bab16,new-mysql-shell,[testenv:py38] basepython = python3.8 deps = -r{toxinidir}/test-requirements.txt commands = stestr run --slowest {posargs} ,,164,138
openstack%2Frpm-packaging~stable%2Ftrain~I85b40ed73d73f0b0744638f66149fab9c3d3253e,openstack/rpm-packaging,stable/train,I85b40ed73d73f0b0744638f66149fab9c3d3253e,Add obsoletes on the no-longer provided python2 package variants,MERGED,2020-03-20 20:50:21.000000000,2020-03-23 10:00:47.000000000,2020-03-23 10:00:46.000000000,"[{'_account_id': 6593}, {'_account_id': 7102}, {'_account_id': 13294}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-20 20:50:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/ed11e07394060b564dbeac638d5b253a359d8a92', 'message': 'Add obsoletes on the no-longer provided python2 package variants\n\nWe need to uninstall the python2.x versions that are no longer\nbeing built for being able to update in a rolling release distribution.\n\nChange-Id: I85b40ed73d73f0b0744638f66149fab9c3d3253e\n'}, {'number': 2, 'created': '2020-03-23 08:06:09.000000000', 'files': ['openstack/python-heatclient/python-heatclient.spec.j2', 'openstack/oslo.concurrency/oslo.concurrency.spec.j2', 'openstack/python-barbicanclient/python-barbicanclient.spec.j2', 'openstack/openstacksdk/openstacksdk.spec.j2', 'openstack/oslo.privsep/oslo.privsep.spec.j2', 'openstack/python-magnumclient/python-magnumclient.spec.j2', 'openstack/osprofiler/osprofiler.spec.j2', 'openstack/python-manilaclient/python-manilaclient.spec.j2', 'openstack/python-monascaclient/python-monascaclient.spec.j2', 'openstack/python-cinderclient/python-cinderclient.spec.j2', 'openstack/python-neutronclient/python-neutronclient.spec.j2', 'openstack/python-novaclient/python-novaclient.spec.j2', 'openstack/pyghmi/pyghmi.spec.j2', 'openstack/python-glanceclient/python-glanceclient.spec.j2', 'openstack/python-swiftclient/python-swiftclient.spec.j2', 'openstack/oslo.messaging/oslo.messaging.spec.j2', 'openstack/oslo.log/oslo.log.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/25323b819b15ab6f21eea6aa470d999b99fc9bb0', 'message': 'Add obsoletes on the no-longer provided python2 package variants\n\nWe need to uninstall the python2.x versions that are no longer\nbeing built for being able to update in a rolling release distribution.\n\nChange-Id: I85b40ed73d73f0b0744638f66149fab9c3d3253e\n'}]",0,714225,25323b819b15ab6f21eea6aa470d999b99fc9bb0,13,6,2,6593,,,0,"Add obsoletes on the no-longer provided python2 package variants

We need to uninstall the python2.x versions that are no longer
being built for being able to update in a rolling release distribution.

Change-Id: I85b40ed73d73f0b0744638f66149fab9c3d3253e
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/25/714225/2 && git format-patch -1 --stdout FETCH_HEAD,"['openstack/python-heatclient/python-heatclient.spec.j2', 'openstack/oslo.concurrency/oslo.concurrency.spec.j2', 'openstack/python-barbicanclient/python-barbicanclient.spec.j2', 'openstack/openstacksdk/openstacksdk.spec.j2', 'openstack/oslo.privsep/oslo.privsep.spec.j2', 'openstack/python-magnumclient/python-magnumclient.spec.j2', 'openstack/osprofiler/osprofiler.spec.j2', 'openstack/python-manilaclient/python-manilaclient.spec.j2', 'openstack/python-monascaclient/python-monascaclient.spec.j2', 'openstack/python-cinderclient/python-cinderclient.spec.j2', 'openstack/python-neutronclient/python-neutronclient.spec.j2', 'openstack/python-novaclient/python-novaclient.spec.j2', 'openstack/pyghmi/pyghmi.spec.j2', 'openstack/python-glanceclient/python-glanceclient.spec.j2', 'openstack/python-swiftclient/python-swiftclient.spec.j2', 'openstack/oslo.messaging/oslo.messaging.spec.j2', 'openstack/oslo.log/oslo.log.spec.j2']",17,ed11e07394060b564dbeac638d5b253a359d8a92,,%if 0%{?suse_version} Obsoletes: {{ py2name(py_versions='py2') }} < 4.0.0 %endif,,54,3
openstack%2Frpm-packaging~master~I85b40ed73d73f0b0744638f66149fab9c3d3253e,openstack/rpm-packaging,master,I85b40ed73d73f0b0744638f66149fab9c3d3253e,Add obsoletes on the no-longer provided python2 package variants,MERGED,2020-03-20 20:50:04.000000000,2020-03-23 10:00:35.000000000,2020-03-23 10:00:35.000000000,"[{'_account_id': 6593}, {'_account_id': 7102}, {'_account_id': 13294}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-20 20:50:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/1d4ccd8cf8258f975ede45429126b8c6fa72d51e', 'message': 'Add obsoletes on the no-longer provided python2 package variants\n\nWe need to uninstall the python2.x versions that are no longer\nbeing built for being able to update in a rolling release distribution.\n\nChange-Id: I85b40ed73d73f0b0744638f66149fab9c3d3253e\n'}, {'number': 2, 'created': '2020-03-21 10:59:12.000000000', 'files': ['openstack/python-heatclient/python-heatclient.spec.j2', 'openstack/oslo.concurrency/oslo.concurrency.spec.j2', 'openstack/python-barbicanclient/python-barbicanclient.spec.j2', 'openstack/openstacksdk/openstacksdk.spec.j2', 'openstack/oslo.privsep/oslo.privsep.spec.j2', 'openstack/python-magnumclient/python-magnumclient.spec.j2', 'openstack/osprofiler/osprofiler.spec.j2', 'openstack/python-manilaclient/python-manilaclient.spec.j2', 'openstack/python-monascaclient/python-monascaclient.spec.j2', 'openstack/python-cinderclient/python-cinderclient.spec.j2', 'openstack/python-neutronclient/python-neutronclient.spec.j2', 'openstack/python-novaclient/python-novaclient.spec.j2', 'openstack/pyghmi/pyghmi.spec.j2', 'openstack/python-glanceclient/python-glanceclient.spec.j2', 'openstack/python-swiftclient/python-swiftclient.spec.j2', 'openstack/oslo.messaging/oslo.messaging.spec.j2', 'openstack/oslo.log/oslo.log.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/889f7445fa3591b7a033d0d608ed8f03de7255b7', 'message': 'Add obsoletes on the no-longer provided python2 package variants\n\nWe need to uninstall the python2.x versions that are no longer\nbeing built for being able to update in a rolling release distribution.\n\nChange-Id: I85b40ed73d73f0b0744638f66149fab9c3d3253e\n'}]",12,714224,889f7445fa3591b7a033d0d608ed8f03de7255b7,17,6,2,6593,,,0,"Add obsoletes on the no-longer provided python2 package variants

We need to uninstall the python2.x versions that are no longer
being built for being able to update in a rolling release distribution.

Change-Id: I85b40ed73d73f0b0744638f66149fab9c3d3253e
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/24/714224/2 && git format-patch -1 --stdout FETCH_HEAD,"['openstack/python-heatclient/python-heatclient.spec.j2', 'openstack/oslo.concurrency/oslo.concurrency.spec.j2', 'openstack/python-barbicanclient/python-barbicanclient.spec.j2', 'openstack/openstacksdk/openstacksdk.spec.j2', 'openstack/oslo.privsep/oslo.privsep.spec.j2', 'openstack/python-magnumclient/python-magnumclient.spec.j2', 'openstack/osprofiler/osprofiler.spec.j2', 'openstack/python-manilaclient/python-manilaclient.spec.j2', 'openstack/python-monascaclient/python-monascaclient.spec.j2', 'openstack/python-cinderclient/python-cinderclient.spec.j2', 'openstack/python-neutronclient/python-neutronclient.spec.j2', 'openstack/python-novaclient/python-novaclient.spec.j2', 'openstack/pyghmi/pyghmi.spec.j2', 'openstack/python-glanceclient/python-glanceclient.spec.j2', 'openstack/python-swiftclient/python-swiftclient.spec.j2', 'openstack/oslo.messaging/oslo.messaging.spec.j2', 'openstack/oslo.log/oslo.log.spec.j2']",17,1d4ccd8cf8258f975ede45429126b8c6fa72d51e,,%if 0%{?suse_version} Obsoletes: {{ py2name(py_versions='py2') }} < 4.0.0 %endif,,54,3
openstack%2Fapi-site~master~If828ad2c894f5b532b96bb9ef1cc95e0f7c7cb5e,openstack/api-site,master,If828ad2c894f5b532b96bb9ef1cc95e0f7c7cb5e,Update infra-manual location,MERGED,2020-03-20 20:56:08.000000000,2020-03-23 09:59:48.000000000,2020-03-23 09:58:10.000000000,"[{'_account_id': 10607}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 20:56:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/api-site/commit/89ff367a98a42c331d804d5d494dc657a151dc01', 'message': 'Update infra-manual location\n\nThe infra-manual now lives on docs.opendev.org, update links.\n\nNew location is: https://docs.opendev.org/opendev/infra-manual/latest\n\nChange-Id: If828ad2c894f5b532b96bb9ef1cc95e0f7c7cb5e\n'}, {'number': 2, 'created': '2020-03-21 14:20:08.000000000', 'files': ['www/templates/footer.tmpl', 'README.rst'], 'web_link': 'https://opendev.org/openstack/api-site/commit/8de4ff83401b1d1b24f6642b49fa05b7a2dc8d15', 'message': 'Update infra-manual location\n\nThe infra-manual now lives on docs.opendev.org, update links.\n\nNew location is: https://docs.opendev.org/opendev/infra-manual/latest\n\nUpdate footer to point to Contributors guide.\n\nChange-Id: If828ad2c894f5b532b96bb9ef1cc95e0f7c7cb5e\n'}]",0,714227,8de4ff83401b1d1b24f6642b49fa05b7a2dc8d15,9,2,2,6547,,,0,"Update infra-manual location

The infra-manual now lives on docs.opendev.org, update links.

New location is: https://docs.opendev.org/opendev/infra-manual/latest

Update footer to point to Contributors guide.

Change-Id: If828ad2c894f5b532b96bb9ef1cc95e0f7c7cb5e
",git fetch https://review.opendev.org/openstack/api-site refs/changes/27/714227/2 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,89ff367a98a42c331d804d5d494dc657a151dc01,infra-manual,<https://docs.opendev.org/opendev/infra-manual/developers.html#development-workflow>`_.,<https://docs.openstack.org/infra/manual/developers.html#development-workflow>`_.,1,1
openstack%2Fproject-team-guide~master~Icbda007a00b4ca0bac1ccf00a093d89685db3f87,openstack/project-team-guide,master,Icbda007a00b4ca0bac1ccf00a093d89685db3f87,Update infra-manual location,MERGED,2020-03-21 15:55:25.000000000,2020-03-23 09:57:55.000000000,2020-03-23 09:56:24.000000000,"[{'_account_id': 308}, {'_account_id': 10607}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-21 15:55:25.000000000', 'files': ['doc/source/repository.rst', 'doc/source/preface.rst', 'doc/source/release-management.rst', 'doc/source/testing.rst'], 'web_link': 'https://opendev.org/openstack/project-team-guide/commit/7f9b39fadfb8a8f129cb0f32032e0bae8b82ca55', 'message': 'Update infra-manual location\n\nThe infra-manual now lives on docs.opendev.org, update links.\n\nNew location is: https://docs.opendev.org/opendev/infra-manual/latest\n\nChange-Id: Icbda007a00b4ca0bac1ccf00a093d89685db3f87\n'}]",0,714266,7f9b39fadfb8a8f129cb0f32032e0bae8b82ca55,8,3,1,6547,,,0,"Update infra-manual location

The infra-manual now lives on docs.opendev.org, update links.

New location is: https://docs.opendev.org/opendev/infra-manual/latest

Change-Id: Icbda007a00b4ca0bac1ccf00a093d89685db3f87
",git fetch https://review.opendev.org/openstack/project-team-guide refs/changes/66/714266/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/repository.rst', 'doc/source/preface.rst', 'doc/source/release-management.rst', 'doc/source/testing.rst']",4,7f9b39fadfb8a8f129cb0f32032e0bae8b82ca55,infra-manual,"including how to configure and use it, see the `OpenDev Manual <https://docs.opendev.org/opendev/infra-manual/latest/>`_.<https://docs.opendev.org/opendev/infra-manual/latest/developers.html#automated-testing>`_.","including how to configure and use it, see the `Infrastructure User Manual <http://docs.openstack.org/infra/manual/>`_.<http://docs.openstack.org/infra/manual/developers.html#automated-testing>`_.",7,7
openstack%2Frpm-packaging~master~I65f786648135a33a554233e8c9b726a88de8f4be,openstack/rpm-packaging,master,I65f786648135a33a554233e8c9b726a88de8f4be,cyborg: update buildrequires,MERGED,2020-03-23 08:51:36.000000000,2020-03-23 09:57:49.000000000,2020-03-23 09:57:49.000000000,"[{'_account_id': 7102}, {'_account_id': 13294}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-23 08:51:36.000000000', 'files': ['openstack/cyborg/cyborg.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/0c51800cb86bda9897d2b23275a5112c999fa5d0', 'message': 'cyborg: update buildrequires\n\nChange-Id: I65f786648135a33a554233e8c9b726a88de8f4be\n'}]",0,714401,0c51800cb86bda9897d2b23275a5112c999fa5d0,9,5,1,6593,,,0,"cyborg: update buildrequires

Change-Id: I65f786648135a33a554233e8c9b726a88de8f4be
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/01/714401/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/cyborg/cyborg.spec.j2'],1,0c51800cb86bda9897d2b23275a5112c999fa5d0,ironic-lib-ver,BuildRequires: {{ py3('sphinxcontrib-svg2pdfconverter') }},,1,0
openstack%2Fproject-team-guide~master~I85202add85b3bc9ceafb7d9dccfe4d7713043254,openstack/project-team-guide,master,I85202add85b3bc9ceafb7d9dccfe4d7713043254,Clarify how to transition to End of Life,MERGED,2020-03-20 15:20:21.000000000,2020-03-23 09:49:48.000000000,2020-03-23 09:48:23.000000000,"[{'_account_id': 308}, {'_account_id': 6547}, {'_account_id': 11904}, {'_account_id': 17685}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 15:20:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-team-guide/commit/8edb54eb19cb03fbf2503946b38dfd8b613bd802', 'message': 'Clarify how to transition to End of Life\n\nClearly state that in case of a project is Unmaintained for 6 months or\nits Team decides to move the project to End of Life, then a patch\nneeds to be proposed in the release repository that tags the project in\nthat branch as $series-eol.\n\nChange-Id: I85202add85b3bc9ceafb7d9dccfe4d7713043254\n'}, {'number': 2, 'created': '2020-03-20 15:47:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-team-guide/commit/a0f523a0298910eb426b6f68ab9695fe628f1d39', 'message': 'Clarify how to transition to End of Life\n\nClearly state that in case of a project is Unmaintained for 6 months or\nits Team decides to move the project to End of Life, then a patch\nneeds to be proposed in the release repository that tags the project in\nthat branch as $series-eol.\n\nChange-Id: I85202add85b3bc9ceafb7d9dccfe4d7713043254\n'}, {'number': 3, 'created': '2020-03-20 16:12:56.000000000', 'files': ['doc/source/stable-branches.rst'], 'web_link': 'https://opendev.org/openstack/project-team-guide/commit/6ae60dadf6e07d46b80086ab412e0de1831c5473', 'message': 'Clarify how to transition to End of Life\n\nClearly state that in case of a project is Unmaintained for 6 months or\nits Team decides to move the project to End of Life, then a patch\nneeds to be proposed in the release repository that tags the project in\nthat branch as $series-eol.\n\nChange-Id: I85202add85b3bc9ceafb7d9dccfe4d7713043254\n'}]",8,714146,6ae60dadf6e07d46b80086ab412e0de1831c5473,17,5,3,17685,,,0,"Clarify how to transition to End of Life

Clearly state that in case of a project is Unmaintained for 6 months or
its Team decides to move the project to End of Life, then a patch
needs to be proposed in the release repository that tags the project in
that branch as $series-eol.

Change-Id: I85202add85b3bc9ceafb7d9dccfe4d7713043254
",git fetch https://review.opendev.org/openstack/project-team-guide refs/changes/46/714146/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/stable-branches.rst'],1,8edb54eb19cb03fbf2503946b38dfd8b613bd802,,"After a project/branch exceeds the time allocation as `Unmaintained`_, or a team decides to explicitly end support for a branch, itTo initiate this transition either the PTL of the given project or other stable maintainer should propose a patch against the given project/repository. (For example, see: https://review.opendev.org/#/c/677478/) ","After a project/branch exceeds the time allocation as `Unmaintained`, or a team decides to explicitly end support for a branch, it",6,2
openstack%2Fmetalsmith~master~I3b5d8d8883fd40f32e0ec4dd6fa982e2c1f78507,openstack/metalsmith,master,I3b5d8d8883fd40f32e0ec4dd6fa982e2c1f78507,[DNM] Testing CI,ABANDONED,2020-03-09 15:29:40.000000000,2020-03-23 09:44:35.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2020-03-09 15:29:40.000000000', 'files': ['metalsmith/exceptions.py'], 'web_link': 'https://opendev.org/openstack/metalsmith/commit/18a73be5cca7141a4ff045f8a65270dc4872aecd', 'message': '[DNM] Testing CI\n\nChange-Id: I3b5d8d8883fd40f32e0ec4dd6fa982e2c1f78507\n'}]",0,711935,18a73be5cca7141a4ff045f8a65270dc4872aecd,3,1,1,23851,,,0,"[DNM] Testing CI

Change-Id: I3b5d8d8883fd40f32e0ec4dd6fa982e2c1f78507
",git fetch https://review.opendev.org/openstack/metalsmith refs/changes/35/711935/1 && git format-patch -1 --stdout FETCH_HEAD,['metalsmith/exceptions.py'],1,18a73be5cca7141a4ff045f8a65270dc4872aecd,test-ci, # This is a test,,2,0
openstack%2Frpm-packaging~stable%2Ftrain~Icf53d24f5e3252d29b8788d3edbbfe6900c7e561,openstack/rpm-packaging,stable/train,Icf53d24f5e3252d29b8788d3edbbfe6900c7e561,openstackclient: Drop update-alternatives,MERGED,2020-03-23 07:29:39.000000000,2020-03-23 09:23:29.000000000,2020-03-23 09:23:29.000000000,"[{'_account_id': 6593}, {'_account_id': 13294}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-23 07:29:39.000000000', 'files': ['openstack/python-openstackclient/python-openstackclient.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/6eab2820650058f42d02fc51801795159b182c12', 'message': 'openstackclient: Drop update-alternatives\n\nNo longer needed and currently the package is broken due to\nnon-available alternatives.\n\nChange-Id: Icf53d24f5e3252d29b8788d3edbbfe6900c7e561\n(cherry picked from commit 535c1dbbd0e01e2d2d0902f4bc1a84fb7dcc3733)\n'}]",0,714389,6eab2820650058f42d02fc51801795159b182c12,9,4,1,7102,,,0,"openstackclient: Drop update-alternatives

No longer needed and currently the package is broken due to
non-available alternatives.

Change-Id: Icf53d24f5e3252d29b8788d3edbbfe6900c7e561
(cherry picked from commit 535c1dbbd0e01e2d2d0902f4bc1a84fb7dcc3733)
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/89/714389/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/python-openstackclient/python-openstackclient.spec.j2'],1,6eab2820650058f42d02fc51801795159b182c12,,%{_bindir}/openstack %{_mandir}/man1/openstack.1.gz,"%if 0%{?suse_version} Requires(post): update-alternatives Requires(postun): update-alternatives %else # on RDO, update-alternatives is in chkconfig Requires(post): chkconfig Requires(postun): chkconfig %endif%python_clone -a %{buildroot}%{_bindir}/openstack %python_clone -a %{buildroot}%{_mandir}/man1/openstack.1 %post %{python_install_alternative openstack openstack.1} %postun %python_uninstall_alternative openstack%python_alternative %{_bindir}/openstack* %python_alternative %{_mandir}/man1/openstack*.1",2,18
openstack%2Fkeystone~master~I72f278b7da59096f71f0e59f0fb1f70f93265aa4,openstack/keystone,master,I72f278b7da59096f71f0e59f0fb1f70f93265aa4,remove oslo-concurrency from requirements,MERGED,2020-03-17 12:39:38.000000000,2020-03-23 09:19:34.000000000,2020-03-23 09:17:32.000000000,"[{'_account_id': 15054}, {'_account_id': 21420}, {'_account_id': 22348}, {'_account_id': 27621}]","[{'number': 1, 'created': '2020-03-17 12:39:38.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/keystone/commit/e5bab15a091526b91e24ca50dfc759f448c2dc14', 'message': 'remove oslo-concurrency from requirements\n\nmany years ago when eventlet support was dropped\nthe usage of osl_concurrency was also removed.\ncommit was here I963d94bbd188dbb6eba68623a42c5bc3f2289da4\ndropping requirement on it since it is not used\n\nChange-Id: I72f278b7da59096f71f0e59f0fb1f70f93265aa4\n'}]",0,713425,e5bab15a091526b91e24ca50dfc759f448c2dc14,9,4,1,16282,,,0,"remove oslo-concurrency from requirements

many years ago when eventlet support was dropped
the usage of osl_concurrency was also removed.
commit was here I963d94bbd188dbb6eba68623a42c5bc3f2289da4
dropping requirement on it since it is not used

Change-Id: I72f278b7da59096f71f0e59f0fb1f70f93265aa4
",git fetch https://review.opendev.org/openstack/keystone refs/changes/25/713425/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,e5bab15a091526b91e24ca50dfc759f448c2dc14,,,oslo.concurrency>=3.26.0 # Apache-2.0,0,1
openstack%2Fcharm-nova-cloud-controller~master~I0ccb4366b03557b316da1507015112c7f378176e,openstack/charm-nova-cloud-controller,master,I0ccb4366b03557b316da1507015112c7f378176e,Enable hardware offload support,MERGED,2019-10-03 13:46:12.000000000,2020-03-23 09:04:10.000000000,2020-03-23 09:04:09.000000000,"[{'_account_id': 935}, {'_account_id': 13686}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2019-10-03 13:46:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-nova-cloud-controller/commit/0d635e193671b35d8f2751c6d25e1f30bdaac9ac', 'message': 'Enable hardware offload support\n\nEnable PCI Passthrough Filter if neutron-api charm has enabled\nsupport for hardware offloading.\n\nChange-Id: I0ccb4366b03557b316da1507015112c7f378176e\nDepends-On: I1f59012ad2d16af18ca310906f6c6b537bb7ec72\n'}, {'number': 2, 'created': '2019-11-01 13:23:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-nova-cloud-controller/commit/b505b2a28383a91a87bbdbc8f355fbbc136136bb', 'message': 'Enable hardware offload support\n\nEnable PCI Passthrough Filter if neutron-api charm has enabled\nsupport for hardware offloading.\n\nChange-Id: I0ccb4366b03557b316da1507015112c7f378176e\nDepends-On: I1f59012ad2d16af18ca310906f6c6b537bb7ec72\n'}, {'number': 3, 'created': '2020-03-18 10:31:16.000000000', 'files': ['hooks/nova_cc_context.py', 'unit_tests/test_nova_cc_contexts.py'], 'web_link': 'https://opendev.org/openstack/charm-nova-cloud-controller/commit/24531f5c2888eb73f0b1f0cbd319354a51a7d365', 'message': 'Enable hardware offload support\n\nEnable PCI Passthrough Filter if neutron-api charm has enabled\nsupport for hardware offloading.\n\nChange-Id: I0ccb4366b03557b316da1507015112c7f378176e\nDepends-On: I1f59012ad2d16af18ca310906f6c6b537bb7ec72\n'}]",0,686406,24531f5c2888eb73f0b1f0cbd319354a51a7d365,19,4,3,935,,,0,"Enable hardware offload support

Enable PCI Passthrough Filter if neutron-api charm has enabled
support for hardware offloading.

Change-Id: I0ccb4366b03557b316da1507015112c7f378176e
Depends-On: I1f59012ad2d16af18ca310906f6c6b537bb7ec72
",git fetch https://review.opendev.org/openstack/charm-nova-cloud-controller refs/changes/06/686406/1 && git format-patch -1 --stdout FETCH_HEAD,['hooks/nova_cc_context.py'],1,0d635e193671b35d8f2751c6d25e1f30bdaac9ac,mlnx-hardware-offload," if (rdata.get('enable-sriov', '').lower() == 'true' or rdata.get('enable-hardware-offload', '').lower() == 'true'):"," if rdata.get('enable-sriov', '').lower() == 'true':",3,1
openstack%2Fpython-designateclient~master~I9582d623727e5853637812083033a348b71551ce,openstack/python-designateclient,master,I9582d623727e5853637812083033a348b71551ce,Use unittest.mock instead of third party mock,MERGED,2020-03-13 16:41:21.000000000,2020-03-23 08:33:51.000000000,2020-03-23 08:32:23.000000000,"[{'_account_id': 13252}, {'_account_id': 22348}, {'_account_id': 22623}]","[{'number': 1, 'created': '2020-03-13 16:41:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-designateclient/commit/a68cb7c37b7923982ab89b7074a0e8d424ee6614', 'message': 'Use unittest.mock instead of third party mock\n\nNow that we are py36 or later, we can use the standard library\nunittest.mock module instead of the third party mock lib.\n\nChange-Id: I9582d623727e5853637812083033a348b71551ce\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}, {'number': 2, 'created': '2020-03-13 18:30:14.000000000', 'files': ['designateclient/tests/test_utils.py', 'test-requirements.txt', 'lower-constraints.txt'], 'web_link': 'https://opendev.org/openstack/python-designateclient/commit/b9bcae014013834bc72dc5a490eeae4876eda764', 'message': 'Use unittest.mock instead of third party mock\n\nNow that we are py36 or later, we can use the standard library\nunittest.mock module instead of the third party mock lib.\n\nChange-Id: I9582d623727e5853637812083033a348b71551ce\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}]",0,713021,b9bcae014013834bc72dc5a490eeae4876eda764,10,3,2,11904,,,0,"Use unittest.mock instead of third party mock

Now that we are py36 or later, we can use the standard library
unittest.mock module instead of the third party mock lib.

Change-Id: I9582d623727e5853637812083033a348b71551ce
Signed-off-by: Sean McGinnis <sean.mcginnis@gmail.com>
",git fetch https://review.opendev.org/openstack/python-designateclient refs/changes/21/713021/2 && git format-patch -1 --stdout FETCH_HEAD,"['designateclient/tests/test_utils.py', 'test-requirements.txt', 'lower-constraints.txt']",3,a68cb7c37b7923982ab89b7074a0e8d424ee6614,unittest.mock,,mock==2.0.0,1,5
openstack%2Fpython-designateclient~master~I0c525eb48270494e8171042e56e14867c53e5af3,openstack/python-designateclient,master,I0c525eb48270494e8171042e56e14867c53e5af3,Drop py27 support,MERGED,2020-03-13 18:29:41.000000000,2020-03-23 08:22:39.000000000,2020-03-23 08:19:58.000000000,"[{'_account_id': 8099}, {'_account_id': 8556}, {'_account_id': 13252}, {'_account_id': 22348}, {'_account_id': 22623}, {'_account_id': 27621}]","[{'number': 1, 'created': '2020-03-13 18:29:41.000000000', 'files': ['.zuul.yaml', 'setup.cfg', 'tox.ini', 'releasenotes/notes/drop-py2-c4e50d006fa4446c.yaml'], 'web_link': 'https://opendev.org/openstack/python-designateclient/commit/1c8e5e801365055a2a7c37b57dbfd1e0ea7c8716', 'message': 'Drop py27 support\n\nPer the Ussuri cycle goal, this drops py27 testing and support.\n\nhttps://governance.openstack.org/tc/goals/selected/ussuri/drop-py27.html\n\nChange-Id: I0c525eb48270494e8171042e56e14867c53e5af3\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}]",0,713044,1c8e5e801365055a2a7c37b57dbfd1e0ea7c8716,12,6,1,11904,,,0,"Drop py27 support

Per the Ussuri cycle goal, this drops py27 testing and support.

https://governance.openstack.org/tc/goals/selected/ussuri/drop-py27.html

Change-Id: I0c525eb48270494e8171042e56e14867c53e5af3
Signed-off-by: Sean McGinnis <sean.mcginnis@gmail.com>
",git fetch https://review.opendev.org/openstack/python-designateclient refs/changes/44/713044/1 && git format-patch -1 --stdout FETCH_HEAD,"['.zuul.yaml', 'setup.cfg', 'tox.ini', 'releasenotes/notes/drop-py2-c4e50d006fa4446c.yaml']",4,1c8e5e801365055a2a7c37b57dbfd1e0ea7c8716,drop-py27-support,--- upgrade: - | Python 2.7 support has been dropped. The last release of python-designateclient to support Python 2.7 is 3.1. The minimum version of Python now supported is Python 3.6. ,,21,13
openstack%2Fkuryr-kubernetes~master~Idd5d5b5499b09ee980008177f87144892568d8c8,openstack/kuryr-kubernetes,master,Idd5d5b5499b09ee980008177f87144892568d8c8,Remove excess physnet to device mapping,ABANDONED,2020-03-20 07:08:08.000000000,2020-03-23 08:15:35.000000000,,"[{'_account_id': 11600}, {'_account_id': 13692}, {'_account_id': 14570}, {'_account_id': 22348}, {'_account_id': 23567}, {'_account_id': 24604}, {'_account_id': 28082}, {'_account_id': 29615}]","[{'number': 1, 'created': '2020-03-20 07:08:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr-kubernetes/commit/3ce5fee3043d90a399d6767c31c403b39cc8ba2e', 'message': 'Remove excess physnet to device mapping\n\nSince sriov binding driver uses pod resource\nservice and compute particular virtual function\nwhich was returned by pod resource service, there\nis no need to have physycal_device_mapping.\n\nChange-Id: Idd5d5b5499b09ee980008177f87144892568d8c8\nSigned-off-by: Danil Golov <d.golov@samsung.com>\n'}, {'number': 2, 'created': '2020-03-20 11:07:01.000000000', 'files': ['kuryr_kubernetes/tests/unit/cni/test_binding.py', 'kuryr_kubernetes/cni/binding/sriov.py', 'kuryr_kubernetes/config.py'], 'web_link': 'https://opendev.org/openstack/kuryr-kubernetes/commit/934eb90a1a7d2cd65a0628827bed47773f1f770f', 'message': 'Remove excess physnet to device mapping\n\nSince sriov binding driver uses pod resource\nservice and compute particular virtual function\nwhich was returned by pod resource service, there\nis no need to have physycal_device_mapping.\n\nChange-Id: Idd5d5b5499b09ee980008177f87144892568d8c8\nSigned-off-by: Danil Golov <d.golov@samsung.com>\n'}]",0,714039,934eb90a1a7d2cd65a0628827bed47773f1f770f,10,8,2,24604,,,0,"Remove excess physnet to device mapping

Since sriov binding driver uses pod resource
service and compute particular virtual function
which was returned by pod resource service, there
is no need to have physycal_device_mapping.

Change-Id: Idd5d5b5499b09ee980008177f87144892568d8c8
Signed-off-by: Danil Golov <d.golov@samsung.com>
",git fetch https://review.opendev.org/openstack/kuryr-kubernetes refs/changes/39/714039/2 && git format-patch -1 --stdout FETCH_HEAD,"['kuryr_kubernetes/tests/unit/cni/test_binding.py', 'kuryr_kubernetes/cni/binding/sriov.py', 'kuryr_kubernetes/config.py']",3,3ce5fee3043d90a399d6767c31c403b39cc8ba2e,,," cfg.ListOpt('physical_device_mappings', default=DEFAULT_DEVICE_MAPPINGS, help=_(""Comma-separated list of "" ""<physical_network>:<network_device> tuples mapping "" ""physical network names to the agent's node-specific "" ""physical network device interfaces of SR-IOV physical "" ""function to be used for VLAN networks."")),",2,36
openstack%2Fvalidations-common~master~Id151dc6ca801e032630a1c1b9c1570fba234ac9a,openstack/validations-common,master,Id151dc6ca801e032630a1c1b9c1570fba234ac9a,Don't write playbook stats in binary mode,MERGED,2020-03-23 07:13:34.000000000,2020-03-23 08:14:10.000000000,2020-03-23 08:14:10.000000000,"[{'_account_id': 8833}, {'_account_id': 16515}, {'_account_id': 22348}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-03-23 07:13:34.000000000', 'files': ['validations_common/callback_plugins/validation_json.py'], 'web_link': 'https://opendev.org/openstack/validations-common/commit/e102a0dc6d05d25681c00de6c20f937230efeb73', 'message': ""Don't write playbook stats in binary mode\n\nDon't write the json string in binary mode.\n\nChange-Id: Id151dc6ca801e032630a1c1b9c1570fba234ac9a\nCo-Authored-By: Rabi Mishra <ramishra@redhat.com>\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n""}]",0,714388,e102a0dc6d05d25681c00de6c20f937230efeb73,7,4,1,11491,,,0,"Don't write playbook stats in binary mode

Don't write the json string in binary mode.

Change-Id: Id151dc6ca801e032630a1c1b9c1570fba234ac9a
Co-Authored-By: Rabi Mishra <ramishra@redhat.com>
Signed-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>
",git fetch https://review.opendev.org/openstack/validations-common refs/changes/88/714388/1 && git format-patch -1 --stdout FETCH_HEAD,['validations_common/callback_plugins/validation_json.py'],1,e102a0dc6d05d25681c00de6c20f937230efeb73,callback_binary_mode," with open(log_file, 'w') as js:"," with open(log_file, 'wb') as js:",1,1
openstack%2Foctavia-dashboard~master~I42d616a8882c69f55667c38b5624c20e63f68a67,openstack/octavia-dashboard,master,I42d616a8882c69f55667c38b5624c20e63f68a67,Update openstackdocstheme,ABANDONED,2019-09-14 05:48:06.000000000,2020-03-23 08:04:17.000000000,,"[{'_account_id': 6547}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2019-09-14 05:48:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia-dashboard/commit/46db790d61bf0347c6428d28f37482b28314e87a', 'message': 'WIP: Update openstackdocstheme\n\nUpdate building of releasenotes and docs to use current settings for\nopenstackdocstheme.\n\nThis will show also the version number for HTML, to work around a bug in\nopenstackdocstheme (TBD: file bug).\n\nChange-Id: I42d616a8882c69f55667c38b5624c20e63f68a67\nWIP: To not conflict with PDF work and since it removes all other jobs.\n'}, {'number': 2, 'created': '2019-09-14 07:30:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia-dashboard/commit/6cc208361b86d676d852c00beeb8355283585d77', 'message': 'Update openstackdocstheme\n\nUpdate building of releasenotes and docs to use current settings for\nopenstackdocstheme.\n\nThis will show also the version number for HTML, to work around a bug in\nopenstackdocstheme or sphinx. The programatic setting is not needed.\n\nRelated-Bug: #1843976\nChange-Id: I42d616a8882c69f55667c38b5624c20e63f68a67\n'}, {'number': 3, 'created': '2019-09-14 07:51:41.000000000', 'files': ['doc/source/conf.py', 'releasenotes/source/conf.py'], 'web_link': 'https://opendev.org/openstack/octavia-dashboard/commit/0b35d06f1bb70ba5a2c5715f1a384914a9e00daa', 'message': 'Update openstackdocstheme\n\nUpdate building of releasenotes and docs to use current settings for\nopenstackdocstheme.\n\nThis will show also the version number for HTML, to work around a bug in\nopenstackdocstheme or sphinx. The programatic setting is not needed.\n\nRelated-Bug: #1843976\nChange-Id: I42d616a8882c69f55667c38b5624c20e63f68a67\n'}]",0,682193,0b35d06f1bb70ba5a2c5715f1a384914a9e00daa,8,3,3,6547,,,0,"Update openstackdocstheme

Update building of releasenotes and docs to use current settings for
openstackdocstheme.

This will show also the version number for HTML, to work around a bug in
openstackdocstheme or sphinx. The programatic setting is not needed.

Related-Bug: #1843976
Change-Id: I42d616a8882c69f55667c38b5624c20e63f68a67
",git fetch https://review.opendev.org/openstack/octavia-dashboard refs/changes/93/682193/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/conf.py', 'zuul.d/projects.yaml', 'releasenotes/source/conf.py']",3,46db790d61bf0347c6428d28f37482b28314e87a,build-pdf-docs,use_storyboard = True,bug_project = '909' bug_tag = 'doc',8,12
openstack%2Ftraining-labs~stable%2Frocky~I3718ff3f3a2a876125e81fea9cc82912e62dfd4a,openstack/training-labs,stable/rocky,I3718ff3f3a2a876125e81fea9cc82912e62dfd4a,Update Ubuntu LTS ISO to 18.04.4,MERGED,2020-03-23 05:33:50.000000000,2020-03-23 07:54:51.000000000,2020-03-23 07:54:51.000000000,"[{'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 05:33:50.000000000', 'files': ['labs/stacktrain/distros/ubuntu_18_04_server_amd64.py'], 'web_link': 'https://opendev.org/openstack/training-labs/commit/c7c36f5200f496dc89d5df462643180e3bea694b', 'message': 'Update Ubuntu LTS ISO to 18.04.4\n\nChange-Id: I3718ff3f3a2a876125e81fea9cc82912e62dfd4a\nbackport: train stein rocky\n(cherry picked from commit cb79feb85846329cc4fed5bd69bbff0a40749205)\n'}]",0,714355,c7c36f5200f496dc89d5df462643180e3bea694b,6,2,1,11109,,,0,"Update Ubuntu LTS ISO to 18.04.4

Change-Id: I3718ff3f3a2a876125e81fea9cc82912e62dfd4a
backport: train stein rocky
(cherry picked from commit cb79feb85846329cc4fed5bd69bbff0a40749205)
",git fetch https://review.opendev.org/openstack/training-labs refs/changes/55/714355/1 && git format-patch -1 --stdout FETCH_HEAD,['labs/stacktrain/distros/ubuntu_18_04_server_amd64.py'],1,c7c36f5200f496dc89d5df462643180e3bea694b,update_lts-stable/stein-stable/rocky," ""ubuntu-18.04.4-server-amd64.iso"") self.md5 = ""d5bc5c59c24191bb45dd85fc6a420b34"""," ""ubuntu-18.04.3-server-amd64.iso"") self.md5 = ""cb7cd5a0c94899a04a536441c8b6d2bf""",2,2
openstack%2Fvalidations-common~master~I3255940d11e7474386722437cbaa864cce912afa,openstack/validations-common,master,I3255940d11e7474386722437cbaa864cce912afa,Use ansible runner uuid as playbook id if exist,MERGED,2020-03-22 14:23:38.000000000,2020-03-23 07:37:02.000000000,2020-03-23 07:37:02.000000000,"[{'_account_id': 11491}, {'_account_id': 22348}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-03-22 14:23:38.000000000', 'files': ['validations_common/callback_plugins/validation_json.py'], 'web_link': 'https://opendev.org/openstack/validations-common/commit/6d83507aab3e7597321ca0a1c5e5faa06694c616', 'message': 'Use ansible runner uuid as playbook id if exist\n\nIf present, we should use uuid exported by ansible runner\nfor the playbook id.\n\nChange-Id: I3255940d11e7474386722437cbaa864cce912afa\n'}]",0,714303,6d83507aab3e7597321ca0a1c5e5faa06694c616,6,3,1,16515,,,0,"Use ansible runner uuid as playbook id if exist

If present, we should use uuid exported by ansible runner
for the playbook id.

Change-Id: I3255940d11e7474386722437cbaa864cce912afa
",git fetch https://review.opendev.org/openstack/validations-common refs/changes/03/714303/1 && git format-patch -1 --stdout FETCH_HEAD,['validations_common/callback_plugins/validation_json.py'],1,6d83507aab3e7597321ca0a1c5e5faa06694c616,play/uuid," 'id': (os.getenv('ANSIBLE_UUID') if os.getenv('ANSIBLE_UUID') else str(play._uuid)),"," 'id': str(play._uuid),",2,1
openstack%2Frpm-packaging~master~Icf53d24f5e3252d29b8788d3edbbfe6900c7e561,openstack/rpm-packaging,master,Icf53d24f5e3252d29b8788d3edbbfe6900c7e561,openstackclient: Drop update-alternatives,MERGED,2020-02-27 07:32:14.000000000,2020-03-23 07:29:39.000000000,2020-02-27 09:19:26.000000000,"[{'_account_id': 6593}, {'_account_id': 13294}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-02-27 07:32:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/931f94bcd12effa5c406a3a579d511c1045985db', 'message': 'openstackclient: Drop update-alternatives\n\nNo longer needed and currently the package is broken due to\nnon-available alternatives.\n\nChange-Id: Icf53d24f5e3252d29b8788d3edbbfe6900c7e561\n'}, {'number': 2, 'created': '2020-02-27 07:59:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/21cbfeb6e5c6ac2848959ad62f3ae45c93bae335', 'message': 'openstackclient: Drop update-alternatives\n\nNo longer needed and currently the package is broken due to\nnon-available alternatives.\n\nChange-Id: Icf53d24f5e3252d29b8788d3edbbfe6900c7e561\n'}, {'number': 3, 'created': '2020-02-27 08:36:12.000000000', 'files': ['openstack/python-openstackclient/python-openstackclient.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/535c1dbbd0e01e2d2d0902f4bc1a84fb7dcc3733', 'message': 'openstackclient: Drop update-alternatives\n\nNo longer needed and currently the package is broken due to\nnon-available alternatives.\n\nChange-Id: Icf53d24f5e3252d29b8788d3edbbfe6900c7e561\n'}]",0,710192,535c1dbbd0e01e2d2d0902f4bc1a84fb7dcc3733,20,5,3,7102,,,0,"openstackclient: Drop update-alternatives

No longer needed and currently the package is broken due to
non-available alternatives.

Change-Id: Icf53d24f5e3252d29b8788d3edbbfe6900c7e561
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/92/710192/2 && git format-patch -1 --stdout FETCH_HEAD,['openstack/python-openstackclient/python-openstackclient.spec.j2'],1,931f94bcd12effa5c406a3a579d511c1045985db,,%{_bindir}/openstack %{_mandir}/man1/openstack*.1,"%if 0%{?suse_version} Requires(post): update-alternatives Requires(postun): update-alternatives %else # on RDO, update-alternatives is in chkconfig Requires(post): chkconfig Requires(postun): chkconfig %endif%python_clone -a %{buildroot}%{_bindir}/openstack %python_clone -a %{buildroot}%{_mandir}/man1/openstack.1 %post %{python_install_alternative openstack openstack.1} %postun %python_uninstall_alternative openstack%python_alternative %{_bindir}/openstack* %python_alternative %{_mandir}/man1/openstack*.1",2,18
openstack%2Ftripleo-common~stable%2Ftrain~I469b4f51c57e1651f42064b0134aa3916af72d80,openstack/tripleo-common,stable/train,I469b4f51c57e1651f42064b0134aa3916af72d80,Add libcgroup-tools to libvirt container,MERGED,2020-03-20 08:12:19.000000000,2020-03-23 07:29:31.000000000,2020-03-21 04:05:50.000000000,"[{'_account_id': 360}, {'_account_id': 7144}, {'_account_id': 11082}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25877}]","[{'number': 1, 'created': '2020-03-20 08:12:19.000000000', 'files': ['container-images/tripleo_kolla_template_overrides.j2'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/3ae13ab4132e8084ebcf5772b1b6e57b386e8a09', 'message': 'Add libcgroup-tools to libvirt container\n\nTo reuse the tripleo containers when running libvirtd container\non OCP we need to run it via a wrapper script and create our own\ncgroups and run libvirtd using cgexec to break out the default\nOCP cgroup of the pod. Otherwise if the libvirtd pod gets\ndeleted/recreated all VMs get shutdown as OCP cleans up the\nresources owned by the pod.\n\nChange-Id: I469b4f51c57e1651f42064b0134aa3916af72d80\n(cherry picked from commit dde8702afe1e867c27097c3acbdfbb1078905d73)\n'}]",2,714044,3ae13ab4132e8084ebcf5772b1b6e57b386e8a09,10,7,1,17216,,,0,"Add libcgroup-tools to libvirt container

To reuse the tripleo containers when running libvirtd container
on OCP we need to run it via a wrapper script and create our own
cgroups and run libvirtd using cgexec to break out the default
OCP cgroup of the pod. Otherwise if the libvirtd pod gets
deleted/recreated all VMs get shutdown as OCP cleans up the
resources owned by the pod.

Change-Id: I469b4f51c57e1651f42064b0134aa3916af72d80
(cherry picked from commit dde8702afe1e867c27097c3acbdfbb1078905d73)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/44/714044/1 && git format-patch -1 --stdout FETCH_HEAD,['container-images/tripleo_kolla_template_overrides.j2'],1,3ae13ab4132e8084ebcf5772b1b6e57b386e8a09,libcgroup-tools-stable/train," 'libcgroup-tools',",,1,0
openstack%2Fmasakari~master~Ie16eede633e6c5706c9d68d525503b6ab77ea10c,openstack/masakari,master,Ie16eede633e6c5706c9d68d525503b6ab77ea10c,Add ignore_basepython_conflict in tox.ini,MERGED,2020-01-23 13:37:19.000000000,2020-03-23 06:44:43.000000000,2020-03-23 06:42:29.000000000,"[{'_account_id': 1011}, {'_account_id': 20182}, {'_account_id': 22348}, {'_account_id': 25267}]","[{'number': 1, 'created': '2020-01-23 13:37:19.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/masakari/commit/c596c966e96f53c6f35be10a147e2ba6fddb6f7e', 'message': 'Add ignore_basepython_conflict in tox.ini\n\nignore_basepython_conflict = True is to avoid the\npython version conflict for basepython defined in base\nenv.\n\nBumping the tox minversion to 3.1.1 which has this flag available.\n\nChange-Id: Ie16eede633e6c5706c9d68d525503b6ab77ea10c\n'}]",0,703990,c596c966e96f53c6f35be10a147e2ba6fddb6f7e,10,4,1,8556,,,0,"Add ignore_basepython_conflict in tox.ini

ignore_basepython_conflict = True is to avoid the
python version conflict for basepython defined in base
env.

Bumping the tox minversion to 3.1.1 which has this flag available.

Change-Id: Ie16eede633e6c5706c9d68d525503b6ab77ea10c
",git fetch https://review.opendev.org/openstack/masakari refs/changes/90/703990/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,c596c966e96f53c6f35be10a147e2ba6fddb6f7e,tox-up,minversion = 3.1.1ignore_basepython_conflict = True,minversion = 2.0,2,1
openstack%2Ftraining-labs~master~I0ce042ade45cc3591a1f68d2b5ef8f8da1166a70,openstack/training-labs,master,I0ce042ade45cc3591a1f68d2b5ef8f8da1166a70,Updates order of neutron network creation,ABANDONED,2019-02-23 08:33:14.000000000,2020-03-23 06:41:13.000000000,,"[{'_account_id': 10068}, {'_account_id': 11109}, {'_account_id': 22348}]","[{'number': 1, 'created': '2019-02-23 08:33:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/training-labs/commit/3154b1ec4ae6152aabea45519d2516bcf947cf06', 'message': 'Updates order of neutron network creation\n\nMoving public and private network and relted router creation to\ncontroller node provisioning. This avoids possible failures and race\nconditions when run from the compute node instead.\n\nChange-Id: I0ce042ade45cc3591a1f68d2b5ef8f8da1166a70\n'}, {'number': 2, 'created': '2019-02-23 08:40:23.000000000', 'files': ['labs/osbash/config/scripts.ubuntu_cluster'], 'web_link': 'https://opendev.org/openstack/training-labs/commit/da1302cddaccb71df2b5c20bdb2e4ab6fafda882', 'message': 'Updates order of neutron network creation\n\nMoving public and private network and relted router creation to\ncontroller node provisioning. This avoids possible failures and race\nconditions when run from the compute node instead.\n\nChange-Id: I0ce042ade45cc3591a1f68d2b5ef8f8da1166a70\n'}]",0,638799,da1302cddaccb71df2b5c20bdb2e4ab6fafda882,7,3,2,29966,,,0,"Updates order of neutron network creation

Moving public and private network and relted router creation to
controller node provisioning. This avoids possible failures and race
conditions when run from the compute node instead.

Change-Id: I0ce042ade45cc3591a1f68d2b5ef8f8da1166a70
",git fetch https://review.opendev.org/openstack/training-labs refs/changes/99/638799/1 && git format-patch -1 --stdout FETCH_HEAD,['labs/osbash/config/scripts.ubuntu_cluster'],1,3154b1ec4ae6152aabea45519d2516bcf947cf06,fix_network_creation,cmd queue config_public_network.sh cmd queue config_private_network.sh ,cmd queue config_public_network.sh cmd queue config_private_network.sh,3,2
openstack%2Ftraining-labs~master~I7111bfb968aa7651f1f0ad9a1153c5a63fc2d291,openstack/training-labs,master,I7111bfb968aa7651f1f0ad9a1153c5a63fc2d291,Add python 3.6 unit test job,ABANDONED,2018-11-06 08:58:44.000000000,2020-03-23 06:40:17.000000000,,"[{'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2018-11-06 08:58:44.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/training-labs/commit/6f521cccd1a9d35e74fa3403bdcec4ed8a64d5a2', 'message': 'Add python 3.6 unit test job\n\nThis is a mechanically generated patch to add a unit test job running\nunder Python 3.6 as part of the python3-first goal.\n\nSee the python3-first goal document for details:\nhttps://governance.openstack.org/tc/goals/stein/python3-first.html\n\nChange-Id: I7111bfb968aa7651f1f0ad9a1153c5a63fc2d291\n'}]",3,615786,6f521cccd1a9d35e74fa3403bdcec4ed8a64d5a2,4,2,1,28956,,,0,"Add python 3.6 unit test job

This is a mechanically generated patch to add a unit test job running
under Python 3.6 as part of the python3-first goal.

See the python3-first goal document for details:
https://governance.openstack.org/tc/goals/stein/python3-first.html

Change-Id: I7111bfb968aa7651f1f0ad9a1153c5a63fc2d291
",git fetch https://review.opendev.org/openstack/training-labs refs/changes/86/615786/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,6f521cccd1a9d35e74fa3403bdcec4ed8a64d5a2,python3-first,"envlist = py36,pep8,bash8","envlist = pep8,bash8",1,1
openstack%2Ftraining-labs~stable%2Fstein~I3718ff3f3a2a876125e81fea9cc82912e62dfd4a,openstack/training-labs,stable/stein,I3718ff3f3a2a876125e81fea9cc82912e62dfd4a,Update Ubuntu LTS ISO to 18.04.4,MERGED,2020-03-23 05:33:31.000000000,2020-03-23 06:36:20.000000000,2020-03-23 06:36:20.000000000,"[{'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-23 05:33:31.000000000', 'files': ['labs/stacktrain/distros/ubuntu_18_04_server_amd64.py'], 'web_link': 'https://opendev.org/openstack/training-labs/commit/83f1df0a0c1f411264185f0c8b790428a850ec9a', 'message': 'Update Ubuntu LTS ISO to 18.04.4\n\nChange-Id: I3718ff3f3a2a876125e81fea9cc82912e62dfd4a\nbackport: train stein rocky\n(cherry picked from commit cb79feb85846329cc4fed5bd69bbff0a40749205)\n'}]",0,714353,83f1df0a0c1f411264185f0c8b790428a850ec9a,7,2,1,11109,,,0,"Update Ubuntu LTS ISO to 18.04.4

Change-Id: I3718ff3f3a2a876125e81fea9cc82912e62dfd4a
backport: train stein rocky
(cherry picked from commit cb79feb85846329cc4fed5bd69bbff0a40749205)
",git fetch https://review.opendev.org/openstack/training-labs refs/changes/53/714353/1 && git format-patch -1 --stdout FETCH_HEAD,['labs/stacktrain/distros/ubuntu_18_04_server_amd64.py'],1,83f1df0a0c1f411264185f0c8b790428a850ec9a,update_lts-stable/stein," ""ubuntu-18.04.4-server-amd64.iso"") self.md5 = ""d5bc5c59c24191bb45dd85fc6a420b34"""," ""ubuntu-18.04.3-server-amd64.iso"") self.md5 = ""cb7cd5a0c94899a04a536441c8b6d2bf""",2,2
openstack%2Fpuppet-keystone~stable%2Ftrain~I507d1b736dbbb147c67b9d399c033703b432b16d,openstack/puppet-keystone,stable/train,I507d1b736dbbb147c67b9d399c033703b432b16d,Update ldap-backend options,MERGED,2020-03-19 14:56:36.000000000,2020-03-23 06:35:07.000000000,2020-03-21 20:11:40.000000000,"[{'_account_id': 3153}, {'_account_id': 5046}, {'_account_id': 9954}, {'_account_id': 14985}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 14:56:36.000000000', 'files': ['manifests/ldap_backend.pp', 'spec/acceptance/keystone_wsgi_apache_spec.rb'], 'web_link': 'https://opendev.org/openstack/puppet-keystone/commit/e59b90669832d5bc20da2da0c0ac5ed9c16e656f', 'message': ""Update ldap-backend options\n\nSeveral of the options defined in ldap_backend.pp were using duplicate\nvalues already specified in keystone/conf/ldap.py. Instead of\nduplicating the same value, we can set them to undefined and just let\nthe default values from keystone come through.\n\nThis commit also updates the values of use_pool and use_auth_pool to\nTrue so they're consistent with the default values in keystone.\n\nCo-Authored-By: Dave Wilde <dwilde@redhat.com>\n\nChange-Id: I507d1b736dbbb147c67b9d399c033703b432b16d\n(cherry picked from commit 1081ac51db7f560e5856809480fb98c215747476)\n""}]",0,713898,e59b90669832d5bc20da2da0c0ac5ed9c16e656f,14,5,1,5046,,,0,"Update ldap-backend options

Several of the options defined in ldap_backend.pp were using duplicate
values already specified in keystone/conf/ldap.py. Instead of
duplicating the same value, we can set them to undefined and just let
the default values from keystone come through.

This commit also updates the values of use_pool and use_auth_pool to
True so they're consistent with the default values in keystone.

Co-Authored-By: Dave Wilde <dwilde@redhat.com>

Change-Id: I507d1b736dbbb147c67b9d399c033703b432b16d
(cherry picked from commit 1081ac51db7f560e5856809480fb98c215747476)
",git fetch https://review.opendev.org/openstack/puppet-keystone refs/changes/98/713898/1 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/ldap_backend.pp', 'spec/acceptance/keystone_wsgi_apache_spec.rb']",2,e59b90669832d5bc20da2da0c0ac5ed9c16e656f,,,use_pool=False pool_retry_delay=0.1auth_pool_size=100 auth_pool_connection_lifetime=60pool_connection_timeout=-1 use_auth_pool=False pool_connection_lifetime=600 pool_size=10 pool_retry_max=3pool_retry_delay=0.1use_pool=False pool_retry_max=3 pool_size=10 auth_pool_size=100 auth_pool_connection_lifetime=60 use_auth_pool=False pool_connection_lifetime=600 pool_connection_timeout=-1,18,36
openstack%2Fmasakari~master~Ib6216099bee97c0c54e847df8608829e9c2f4776,openstack/masakari,master,Ib6216099bee97c0c54e847df8608829e9c2f4776,Drop use of SQLAlchemy-Utils,MERGED,2020-03-10 14:18:47.000000000,2020-03-23 06:14:42.000000000,2020-03-23 06:11:30.000000000,"[{'_account_id': 1011}, {'_account_id': 22348}, {'_account_id': 25267}]","[{'number': 1, 'created': '2020-03-10 14:18:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/masakari/commit/8fe3e203d5ac369378ebf9741baf56e8f4fa4b64', 'message': 'Drop use of SQLAlchemy-Utils\n\nSQLAlchemy Utils is not used by Masakari, drop in preference to\nstraight vanilla SQLAlchemy.\n\nChange-Id: Ib6216099bee97c0c54e847df8608829e9c2f4776\n'}, {'number': 2, 'created': '2020-03-10 16:14:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/masakari/commit/c46b34eab05232a9d0c878cd979bda774f74adcb', 'message': 'Drop use of SQLAlchemy-Utils\n\nSQLAlchemy Utils is not used by Masakari, drop in preference to\nstraight vanilla SQLAlchemy.\n\nChange-Id: Ib6216099bee97c0c54e847df8608829e9c2f4776\n'}, {'number': 3, 'created': '2020-03-11 09:16:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/masakari/commit/c5ee2525a4ef126559d9b95337fd8dd59aca08cd', 'message': 'Drop use of SQLAlchemy-Utils\n\nSQLAlchemy Utils is not used by Masakari, drop in preference to\nstraight vanilla SQLAlchemy.\n\nChange-Id: Ib6216099bee97c0c54e847df8608829e9c2f4776\n'}, {'number': 4, 'created': '2020-03-16 12:04:26.000000000', 'files': ['requirements.txt', 'lower-constraints.txt'], 'web_link': 'https://opendev.org/openstack/masakari/commit/cd52fb1bcfe3aa295f641301f734ccba1c331288', 'message': 'Drop use of SQLAlchemy-Utils\n\nSQLAlchemy Utils is not used by Masakari, drop in preference to\nstraight vanilla SQLAlchemy.\n\nChange-Id: Ib6216099bee97c0c54e847df8608829e9c2f4776\n'}]",0,712097,cd52fb1bcfe3aa295f641301f734ccba1c331288,17,3,4,935,,,0,"Drop use of SQLAlchemy-Utils

SQLAlchemy Utils is not used by Masakari, drop in preference to
straight vanilla SQLAlchemy.

Change-Id: Ib6216099bee97c0c54e847df8608829e9c2f4776
",git fetch https://review.opendev.org/openstack/masakari refs/changes/97/712097/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,8fe3e203d5ac369378ebf9741baf56e8f4fa4b64,drop-sqla-utils,SQLAlchemy>=1.2.19 # MIT,SQLAlchemy-Utils>=0.33.10 # Apache-2.0,1,1
openstack%2Fmasakari~master~Ief87c65e240bb55b2c7d0712450a672421543ded,openstack/masakari,master,Ief87c65e240bb55b2c7d0712450a672421543ded,"There are minor problems with the parameter format, func prototype definition likes: (self, msg, *args, **kwargs)",NEW,2020-01-20 09:29:30.000000000,2020-03-23 06:13:52.000000000,,"[{'_account_id': 1011}, {'_account_id': 22348}, {'_account_id': 25267}, {'_account_id': 29071}]","[{'number': 1, 'created': '2020-01-20 09:29:30.000000000', 'files': ['masakari/ha/api.py'], 'web_link': 'https://opendev.org/openstack/masakari/commit/7161cc928396d1131c61308410d525ebf1f8f6d5', 'message': 'There are minor problems with the parameter format,\nfunc prototype definition likes: (self, msg, *args, **kwargs)\n\nChange-Id: Ief87c65e240bb55b2c7d0712450a672421543ded\n'}]",3,703347,7161cc928396d1131c61308410d525ebf1f8f6d5,5,4,1,30744,,,0,"There are minor problems with the parameter format,
func prototype definition likes: (self, msg, *args, **kwargs)

Change-Id: Ief87c65e240bb55b2c7d0712450a672421543ded
",git fetch https://review.opendev.org/openstack/masakari refs/changes/47/703347/1 && git format-patch -1 --stdout FETCH_HEAD,['masakari/ha/api.py'],1,7161cc928396d1131c61308410d525ebf1f8f6d5,changes-in-LOG-parameter," LOG.debug(""Fetching failover segment by uuid %s"", segment_uuid) LOG.debug(""Fetching host by uuid %s"", host_uuid) LOG.debug(""Fetching notification by uuid %s"", notification_uuid) LOG.debug(""Failed to fetch notification by uuid %s"", notification_uuid)"," LOG.debug(""Fetching failover segment by "" ""UUID"", segment_uuid=segment_uuid) LOG.debug(""Fetching host by "" ""UUID"", host_uuid=host_uuid) LOG.debug(""Fetching notification by "" ""UUID"", notification_uuid=notification_uuid) LOG.debug(""Failed to fetch notification by "" ""uuid %s"", notification_uuid)",5,8
openstack%2Fmasakari-monitors~master~Id27862284d34f6585881da39c0385cda91959b93,openstack/masakari-monitors,master,Id27862284d34f6585881da39c0385cda91959b93,[ussuri][goal] Drop python 2.7 support and testing,MERGED,2019-11-15 14:23:07.000000000,2020-03-23 06:03:55.000000000,2020-03-23 06:01:31.000000000,"[{'_account_id': 1011}, {'_account_id': 2394}, {'_account_id': 3085}, {'_account_id': 8556}, {'_account_id': 8716}, {'_account_id': 8988}, {'_account_id': 12950}, {'_account_id': 20182}, {'_account_id': 22348}, {'_account_id': 25267}]","[{'number': 1, 'created': '2019-11-15 14:23:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/masakari-monitors/commit/9ede8cf85e7b31706fa05bed078ec587d2591682', 'message': '[ussuri][goal] Drop python 2.7 support and testing\n\nOpenStack is dropping the py2.7 support in ussuri cycle.\n\nmasakari-monitors is ready with python 3 and ok to drop the\npython 2.7 support.\n\nComplete discussion & schedule can be found in\n- http://lists.openstack.org/pipermail/openstack-discuss/2019-October/010142.html\n- https://etherpad.openstack.org/p/drop-python2-support\n\nUssuri Communtiy-wide goal:\nhttps://governance.openstack.org/tc/goals/selected/ussuri/drop-py27.html\n\nDepends-On: https://review.opendev.org/#/c/693631/\nChange-Id: Id27862284d34f6585881da39c0385cda91959b93\n'}, {'number': 2, 'created': '2019-11-16 17:04:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/masakari-monitors/commit/c9a679ee16472694c46d6a9a1b04b5eac4a8d91e', 'message': '[ussuri][goal] Drop python 2.7 support and testing\n\nOpenStack is dropping the py2.7 support in ussuri cycle.\n\nmasakari-monitors is ready with python 3 and ok to drop the\npython 2.7 support.\n\nComplete discussion & schedule can be found in\n- http://lists.openstack.org/pipermail/openstack-discuss/2019-October/010142.html\n- https://etherpad.openstack.org/p/drop-python2-support\n\nUssuri Communtiy-wide goal:\nhttps://governance.openstack.org/tc/goals/selected/ussuri/drop-py27.html\n\nDepends-On: https://review.opendev.org/#/c/693631/\nChange-Id: Id27862284d34f6585881da39c0385cda91959b93\n'}, {'number': 3, 'created': '2020-01-23 17:29:08.000000000', 'files': ['.zuul.yaml', 'releasenotes/notes/drop-py-2-7-b28de816eac45468.yaml', 'setup.cfg', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/masakari-monitors/commit/8711c071c5919bd4fd95cfb41962883f2e9365ad', 'message': '[ussuri][goal] Drop python 2.7 support and testing\n\nOpenStack is dropping the py2.7 support in ussuri cycle.\n\nmasakari-monitors is ready with python 3 and ok to drop the\npython 2.7 support.\n\nComplete discussion & schedule can be found in\n- http://lists.openstack.org/pipermail/openstack-discuss/2019-October/010142.html\n- https://etherpad.openstack.org/p/drop-python2-support\n\nUssuri Communtiy-wide goal:\nhttps://governance.openstack.org/tc/goals/selected/ussuri/drop-py27.html\n\nDepends-On: https://review.opendev.org/#/c/693631/\nChange-Id: Id27862284d34f6585881da39c0385cda91959b93\n'}]",2,694552,8711c071c5919bd4fd95cfb41962883f2e9365ad,14,10,3,8556,,,0,"[ussuri][goal] Drop python 2.7 support and testing

OpenStack is dropping the py2.7 support in ussuri cycle.

masakari-monitors is ready with python 3 and ok to drop the
python 2.7 support.

Complete discussion & schedule can be found in
- http://lists.openstack.org/pipermail/openstack-discuss/2019-October/010142.html
- https://etherpad.openstack.org/p/drop-python2-support

Ussuri Communtiy-wide goal:
https://governance.openstack.org/tc/goals/selected/ussuri/drop-py27.html

Depends-On: https://review.opendev.org/#/c/693631/
Change-Id: Id27862284d34f6585881da39c0385cda91959b93
",git fetch https://review.opendev.org/openstack/masakari-monitors refs/changes/52/694552/2 && git format-patch -1 --stdout FETCH_HEAD,"['.zuul.yaml', 'releasenotes/notes/drop-py-2-7-b28de816eac45468.yaml', 'setup.cfg', 'tox.ini']",4,9ede8cf85e7b31706fa05bed078ec587d2591682,drop-py27-support,"envlist = pep8,py36,py37","envlist = pep8,py27,py36,py37[testenv:py27] commands = {[testenv]commands} stestr run {posargs} ",8,10
openstack%2Fmasakari~master~I92b6ee8ea5afdb16367a5b6d939e65e2fa48190d,openstack/masakari,master,I92b6ee8ea5afdb16367a5b6d939e65e2fa48190d,Fix constraints URL enforcement for lower-constraints,MERGED,2019-09-27 19:09:27.000000000,2020-03-23 06:00:51.000000000,2020-03-23 05:59:05.000000000,"[{'_account_id': 935}, {'_account_id': 1011}, {'_account_id': 20191}, {'_account_id': 22348}, {'_account_id': 25267}, {'_account_id': 28522}]","[{'number': 1, 'created': '2019-09-27 19:09:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/masakari/commit/94f48d05937fec29cd1ffaf9461130c214fd87ec', 'message': 'Fix constraints URL enforcement for lower-constraints\n\nDue to the upper-constraints argument being hard coded in the\ninstall_command, even jobs like lower-constraints will end up getting\nthe latest upper-constraints installed.\n\nThe correct way to handle the constraints is to separate it out into\ndeps. This allows the l-c job to properly set what constraints to use.\n\nAlso fixes constraints URL to the preferred static path.\n\nChange-Id: I92b6ee8ea5afdb16367a5b6d939e65e2fa48190d\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}, {'number': 2, 'created': '2019-12-10 22:06:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/masakari/commit/4bf89c6c0057669916279335abcc7c1565f62124', 'message': 'Fix constraints URL enforcement for lower-constraints\n\nDue to the upper-constraints argument being hard coded in the\ninstall_command, even jobs like lower-constraints will end up getting\nthe latest upper-constraints installed.\n\nThe correct way to handle the constraints is to separate it out into\ndeps. This allows the l-c job to properly set what constraints to use.\n\nAlso fixes constraints URL to the preferred static path.\n\nChange-Id: I92b6ee8ea5afdb16367a5b6d939e65e2fa48190d\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}, {'number': 3, 'created': '2019-12-16 17:24:18.000000000', 'files': ['test-requirements.txt', 'lower-constraints.txt', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/masakari/commit/75388fdd27574a97b0ac49a298c0e916e0f19834', 'message': 'Fix constraints URL enforcement for lower-constraints\n\nDue to the upper-constraints argument being hard coded in the\ninstall_command, even jobs like lower-constraints will end up getting\nthe latest upper-constraints installed.\n\nThe correct way to handle the constraints is to separate it out into\ndeps. This allows the l-c job to properly set what constraints to use.\n\nAlso fixes constraints URL to the preferred static path.\n\nChange-Id: I92b6ee8ea5afdb16367a5b6d939e65e2fa48190d\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}]",1,685464,75388fdd27574a97b0ac49a298c0e916e0f19834,18,6,3,11904,,,0,"Fix constraints URL enforcement for lower-constraints

Due to the upper-constraints argument being hard coded in the
install_command, even jobs like lower-constraints will end up getting
the latest upper-constraints installed.

The correct way to handle the constraints is to separate it out into
deps. This allows the l-c job to properly set what constraints to use.

Also fixes constraints URL to the preferred static path.

Change-Id: I92b6ee8ea5afdb16367a5b6d939e65e2fa48190d
Signed-off-by: Sean McGinnis <sean.mcginnis@gmail.com>
",git fetch https://review.opendev.org/openstack/masakari refs/changes/64/685464/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,94f48d05937fec29cd1ffaf9461130c214fd87ec,,install_command = pip install {opts} {packages}deps = -c{env:UPPER_CONSTRAINTS_FILE:https://releases.openstack.org/constraints/upper/master} -r{toxinidir}/requirements.txt -r{toxinidir}/test-requirements.txt,install_command = pip install -c{env:UPPER_CONSTRAINTS_FILE:https://opendev.org/openstack/requirements/raw/branch/master/upper-constraints.txt} {opts} {packages}deps = -r{toxinidir}/test-requirements.txt,5,2
openstack%2Fmasakari~master~I031feb46aee4e33fab0f96d033a32198a742214e,openstack/masakari,master,I031feb46aee4e33fab0f96d033a32198a742214e,Sync Sphinx requirement,MERGED,2019-08-01 09:06:38.000000000,2020-03-23 05:59:28.000000000,2020-03-23 05:58:13.000000000,"[{'_account_id': 1011}, {'_account_id': 2394}, {'_account_id': 8716}, {'_account_id': 8988}, {'_account_id': 11904}, {'_account_id': 12950}, {'_account_id': 18955}, {'_account_id': 22348}, {'_account_id': 25267}]","[{'number': 1, 'created': '2019-08-01 09:06:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/masakari/commit/389fff40d44e3e8deb47263876441add0ec3e1c8', 'message': 'Sync Sphinx requirement\n\nSync sphinx dependency with global requirements. It caps python 2 since\nsphinx 2.0 no longer supports Python 2.7.\n\nChange-Id: I031feb46aee4e33fab0f96d033a32198a742214e\n'}, {'number': 2, 'created': '2019-12-16 17:24:18.000000000', 'files': ['doc/requirements.txt'], 'web_link': 'https://opendev.org/openstack/masakari/commit/a6abc4a355e218e33e5ea51a83f46de69f149d40', 'message': 'Sync Sphinx requirement\n\nSync sphinx dependency with global requirements. It caps python 2 since\nsphinx 2.0 no longer supports Python 2.7.\n\nChange-Id: I031feb46aee4e33fab0f96d033a32198a742214e\n'}]",0,673965,a6abc4a355e218e33e5ea51a83f46de69f149d40,16,9,2,27822,,,0,"Sync Sphinx requirement

Sync sphinx dependency with global requirements. It caps python 2 since
sphinx 2.0 no longer supports Python 2.7.

Change-Id: I031feb46aee4e33fab0f96d033a32198a742214e
",git fetch https://review.opendev.org/openstack/masakari refs/changes/65/673965/2 && git format-patch -1 --stdout FETCH_HEAD,['doc/requirements.txt'],1,389fff40d44e3e8deb47263876441add0ec3e1c8,sphinx,"sphinx!=1.6.6,!=1.6.7,>=1.6.5,<2.0.0;python_version=='2.7' # BSD sphinx!=1.6.6,!=1.6.7,!=2.1.0,>=1.6.5;python_version>='3.4' # BSD","sphinx!=1.6.6,!=1.6.7,>=1.6.2 # BSD",2,1
openstack%2Ftraining-labs~master~I3718ff3f3a2a876125e81fea9cc82912e62dfd4a,openstack/training-labs,master,I3718ff3f3a2a876125e81fea9cc82912e62dfd4a,Update Ubuntu LTS ISO to 18.04.4,MERGED,2020-03-22 18:32:19.000000000,2020-03-23 05:33:31.000000000,2020-03-22 19:23:19.000000000,"[{'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 18:32:19.000000000', 'files': ['labs/stacktrain/distros/ubuntu_18_04_server_amd64.py'], 'web_link': 'https://opendev.org/openstack/training-labs/commit/cb79feb85846329cc4fed5bd69bbff0a40749205', 'message': 'Update Ubuntu LTS ISO to 18.04.4\n\nChange-Id: I3718ff3f3a2a876125e81fea9cc82912e62dfd4a\nbackport: train stein rocky\n'}]",0,714326,cb79feb85846329cc4fed5bd69bbff0a40749205,7,2,1,11109,,,0,"Update Ubuntu LTS ISO to 18.04.4

Change-Id: I3718ff3f3a2a876125e81fea9cc82912e62dfd4a
backport: train stein rocky
",git fetch https://review.opendev.org/openstack/training-labs refs/changes/26/714326/1 && git format-patch -1 --stdout FETCH_HEAD,['labs/stacktrain/distros/ubuntu_18_04_server_amd64.py'],1,cb79feb85846329cc4fed5bd69bbff0a40749205,update_lts," ""ubuntu-18.04.4-server-amd64.iso"") self.md5 = ""d5bc5c59c24191bb45dd85fc6a420b34"""," ""ubuntu-18.04.3-server-amd64.iso"") self.md5 = ""cb7cd5a0c94899a04a536441c8b6d2bf""",2,2
openstack%2Fhorizon~master~Ic9c2ee8991d5b8390dc832fb21471d90b765c9bd,openstack/horizon,master,Ic9c2ee8991d5b8390dc832fb21471d90b765c9bd,Imported Translations from Zanata,MERGED,2020-03-16 07:35:48.000000000,2020-03-23 05:20:20.000000000,2020-03-23 05:18:46.000000000,"[{'_account_id': 22348}, {'_account_id': 29313}]","[{'number': 1, 'created': '2020-03-16 07:35:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/a08f225e540c777ce140dabbb2400c6f8634d87a', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: Ic9c2ee8991d5b8390dc832fb21471d90b765c9bd\n'}, {'number': 2, 'created': '2020-03-21 07:33:43.000000000', 'files': ['releasenotes/source/locale/id/LC_MESSAGES/releasenotes.po', 'releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'], 'web_link': 'https://opendev.org/openstack/horizon/commit/7acb68b5c2d4d8e355c80bab861c3cd18e799773', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: Ic9c2ee8991d5b8390dc832fb21471d90b765c9bd\n'}]",0,713168,7acb68b5c2d4d8e355c80bab861c3cd18e799773,10,2,2,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: Ic9c2ee8991d5b8390dc832fb21471d90b765c9bd
",git fetch https://review.opendev.org/openstack/horizon refs/changes/68/713168/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'],1,a08f225e540c777ce140dabbb2400c6f8634d87a,zanata/translations,"""POT-Creation-Date: 2020-03-11 18:30+0000\n""""PO-Revision-Date: 2020-03-15 10:21+0000\n""msgid ""18.0.0-61"" msgstr ""18.0.0-61""","""POT-Creation-Date: 2020-03-10 22:29+0000\n""""PO-Revision-Date: 2020-03-11 02:25+0000\n""msgid ""18.0.0-60"" msgstr ""18.0.0-60""",4,4
openstack%2Frpm-packaging~master~I3f93b91c4cfcb42f6964461db89d4fb7299ae57c,openstack/rpm-packaging,master,I3f93b91c4cfcb42f6964461db89d4fb7299ae57c,"Revert ""sahara: add patch to fix documentation build""",MERGED,2020-03-21 13:04:02.000000000,2020-03-23 05:14:28.000000000,2020-03-23 05:14:28.000000000,"[{'_account_id': 7102}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-21 13:04:02.000000000', 'files': ['openstack/sahara/sahara.spec.j2', 'openstack/sahara/0001-Fix-syntax-error-in-image-widths.patch'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/732ee51baa4d928f9ce7b5ba817c03413eabab65', 'message': 'Revert ""sahara: add patch to fix documentation build""\n\nThis was now merged upstream:\n\nhttps://review.opendev.org/#/c/710658/\n\nThis reverts commit 0294246fe28d0ff9d62de6cd316e5b021fef8ecf.\n\nChange-Id: I3f93b91c4cfcb42f6964461db89d4fb7299ae57c\n'}]",0,714255,732ee51baa4d928f9ce7b5ba817c03413eabab65,8,4,1,6593,,,0,"Revert ""sahara: add patch to fix documentation build""

This was now merged upstream:

https://review.opendev.org/#/c/710658/

This reverts commit 0294246fe28d0ff9d62de6cd316e5b021fef8ecf.

Change-Id: I3f93b91c4cfcb42f6964461db89d4fb7299ae57c
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/55/714255/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack/sahara/sahara.spec.j2', 'openstack/sahara/0001-Fix-syntax-error-in-image-widths.patch']",2,732ee51baa4d928f9ce7b5ba817c03413eabab65,ironic-lib-ver,,"From 0825bddef65c11928ae6d37a2a7702e2d9435143 Mon Sep 17 00:00:00 2001 From: Dirk Mueller <dirk@dmllr.de> Date: Sun, 1 Mar 2020 20:54:43 +0100 Subject: [PATCH] Fix syntax error in image widths I get when building sahara this error: File ""/usr/lib/python3.6/site-packages/sphinx/writers/html5.py"", line 548, in visit_image atts['width'] = int(atts['width']) * scale ValueError: invalid literal for int() with base 10: '800px' As it turns out, the default is pixels anyway, so we don't need to add a type suffix. For scale to work, we need to specify both width and height for images, otherwise sphinx reports an error. Change-Id: If0f8383e37a8b1185a8a08b24c79b313fdcb498c --- doc/source/intro/architecture.rst | 5 +++-- doc/source/intro/overview.rst | 5 +++-- 2 files changed, 6 insertions(+), 4 deletions(-) diff --git a/doc/source/intro/architecture.rst b/doc/source/intro/architecture.rst index 608bede3..6ebd2c44 100644 --- a/doc/source/intro/architecture.rst +++ b/doc/source/intro/architecture.rst @@ -2,8 +2,9 @@ Architecture ============ .. image:: ../images/sahara-architecture.svg - :width: 800 px - :scale: 100 % + :width: 960 + :height: 635 + :scale: 83 % :align: left diff --git a/doc/source/intro/overview.rst b/doc/source/intro/overview.rst index 1560b417..6f121541 100644 --- a/doc/source/intro/overview.rst +++ b/doc/source/intro/overview.rst @@ -75,8 +75,9 @@ The sahara product communicates with the following OpenStack services: like passwords and private keys in a secure storage. .. image:: ../images/openstack-interop.png - :width: 800 px - :scale: 99 % + :width: 960 + :height: 720 + :scale: 83 % :align: left General Workflow -- 2.25.1 ",0,58
openstack%2Fpython-tripleoclient~master~Ide8cba32a0ea14086714094291f1a8866b669491,openstack/python-tripleoclient,master,Ide8cba32a0ea14086714094291f1a8866b669491,Get validation groups info from groups.yaml file,MERGED,2020-01-28 09:55:30.000000000,2020-03-23 04:52:28.000000000,2020-02-29 02:41:36.000000000,"[{'_account_id': 3153}, {'_account_id': 7353}, {'_account_id': 14985}, {'_account_id': 16515}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-01-28 09:55:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/0f861be453e99d816e7533b0863881ed16af1049', 'message': 'WIP Groups from groups.yaml file\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n(cherry picked from commit bb712a1229213e10a3082c81dfdcba6b9cbb2d04)\n'}, {'number': 2, 'created': '2020-01-30 15:25:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/e6a5a4d20e2bee68c96c19d19c126abe6ad95fe6', 'message': 'WIP Groups from groups.yaml file\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n(cherry picked from commit bb712a1229213e10a3082c81dfdcba6b9cbb2d04)\n'}, {'number': 3, 'created': '2020-02-03 12:53:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/c62536a1338c308cf07e2d21612bf0d2d661e3f9', 'message': 'WIP Groups from groups.yaml file\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n(cherry picked from commit bb712a1229213e10a3082c81dfdcba6b9cbb2d04)\n'}, {'number': 4, 'created': '2020-02-03 12:55:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/b35c2d8b6f6e639d302318229e0f86c25e769f86', 'message': 'WIP Groups from groups.yaml file\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n(cherry picked from commit bb712a1229213e10a3082c81dfdcba6b9cbb2d04)\n'}, {'number': 5, 'created': '2020-02-04 10:20:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/3e12d772c7766ffe548557684d2d1c7241347354', 'message': ""Get validation groups info from groups.yaml file\n\nThis patch makes the VALIDATION_GROUPS list constants dynamic by getting\nthem from the 'groups.yaml' file. This file comes from\ntripleo-validations and is present on the Undercloud at the location [1].\n\n[1] => /usr/share/openstack-tripleo-validations\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n(cherry picked from commit bb712a1229213e10a3082c81dfdcba6b9cbb2d04)\n""}, {'number': 6, 'created': '2020-02-05 11:02:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/4f13149a65186e387b58f4fd1aa93761af25f616', 'message': ""Get validation groups info from groups.yaml file\n\nThis patch makes the VALIDATION_GROUPS list constants dynamic by getting\nthem from the 'groups.yaml' file. This file comes from\ntripleo-validations and is present on the Undercloud at the location [1].\n\n[1] => /usr/share/openstack-tripleo-validations\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n(cherry picked from commit bb712a1229213e10a3082c81dfdcba6b9cbb2d04)\n""}, {'number': 7, 'created': '2020-02-05 13:52:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/e0c84e07d9bae78ab87bfa50ff12d38a8d7be1bc', 'message': ""Get validation groups info from groups.yaml file\n\nThis patch makes the VALIDATION_GROUPS list constants dynamic by getting\nthem from the 'groups.yaml' file. This file comes from\ntripleo-validations and is present on the Undercloud at the location [1].\n\n[1] => /usr/share/openstack-tripleo-validations\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n(cherry picked from commit bb712a1229213e10a3082c81dfdcba6b9cbb2d04)\n""}, {'number': 8, 'created': '2020-02-07 15:00:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/05806c0b3a314d60e99b2f2fafa9565974d0699b', 'message': ""Get validation groups info from groups.yaml file\n\nThis patch makes the VALIDATION_GROUPS list constants dynamic by getting\nthem from the 'groups.yaml' file. This file comes from\ntripleo-validations and is present on the Undercloud at the location [1].\n\n[1] => /usr/share/openstack-tripleo-validations\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n(cherry picked from commit bb712a1229213e10a3082c81dfdcba6b9cbb2d04)\n""}, {'number': 9, 'created': '2020-02-12 06:33:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/8a3473fe05308f7908e1b892049456d5d22de8dd', 'message': ""Get validation groups info from groups.yaml file\n\nThis patch makes the VALIDATION_GROUPS list constants dynamic by getting\nthem from the 'groups.yaml' file. This file comes from\ntripleo-validations and is present on the Undercloud at the location [1].\n\n[1] => /usr/share/openstack-tripleo-validations\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n(cherry picked from commit bb712a1229213e10a3082c81dfdcba6b9cbb2d04)\n""}, {'number': 10, 'created': '2020-02-24 09:59:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/836210e162d6238fc9a57b91df43951b643fe980', 'message': ""Get validation groups info from groups.yaml file\n\nThis patch makes the VALIDATION_GROUPS list constants dynamic by getting\nthem from the 'groups.yaml' file. This file comes from\ntripleo-validations and is present on the Undercloud at the location [1].\n\n[1] => /usr/share/openstack-tripleo-validations\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n(cherry picked from commit bb712a1229213e10a3082c81dfdcba6b9cbb2d04)\n""}, {'number': 11, 'created': '2020-02-26 09:55:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/4221a844daa160aa94267a12490a0302f42461ef', 'message': ""Get validation groups info from groups.yaml file\n\nThis patch makes the VALIDATION_GROUPS list constants dynamic by getting\nthem from the 'groups.yaml' file. This file comes from\ntripleo-validations and is present on the Undercloud at the location [1].\n\n[1] => /usr/share/openstack-tripleo-validations\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n(cherry picked from commit bb712a1229213e10a3082c81dfdcba6b9cbb2d04)\n""}, {'number': 12, 'created': '2020-02-26 16:27:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/48ae45d52303c130108bc9ff986296cfe369f3c9', 'message': ""Get validation groups info from groups.yaml file\n\nThis patch makes the VALIDATION_GROUPS list constants dynamic by getting\nthem from the 'groups.yaml' file. This file comes from\ntripleo-validations and is present on the Undercloud at the location [1].\n\n[1] => /usr/share/openstack-tripleo-validations\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n(cherry picked from commit bb712a1229213e10a3082c81dfdcba6b9cbb2d04)\n""}, {'number': 13, 'created': '2020-02-27 09:26:08.000000000', 'files': ['tripleoclient/tests/v1/tripleo/test_tripleo_validator.py', 'tripleoclient/constants.py', 'tripleoclient/utils.py', 'tripleoclient/v1/tripleo_validator.py'], 'web_link': 'https://opendev.org/openstack/python-tripleoclient/commit/afb9887a408dc7ce4b178f730ea17f7c77f120b9', 'message': ""Get validation groups info from groups.yaml file\n\nThis patch makes the VALIDATION_GROUPS list constants dynamic by getting\nthem from the 'groups.yaml' file. This file comes from\ntripleo-validations and is present on the Undercloud at the location [1].\n\n[1] => /usr/share/openstack-tripleo-validations\n\nChange-Id: Ide8cba32a0ea14086714094291f1a8866b669491\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n""}]",2,704521,afb9887a408dc7ce4b178f730ea17f7c77f120b9,53,7,13,11491,,,0,"Get validation groups info from groups.yaml file

This patch makes the VALIDATION_GROUPS list constants dynamic by getting
them from the 'groups.yaml' file. This file comes from
tripleo-validations and is present on the Undercloud at the location [1].

[1] => /usr/share/openstack-tripleo-validations

Change-Id: Ide8cba32a0ea14086714094291f1a8866b669491
Signed-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>
",git fetch https://review.opendev.org/openstack/python-tripleoclient refs/changes/21/704521/2 && git format-patch -1 --stdout FETCH_HEAD,['tripleoclient/utils.py'],1,0f861be453e99d816e7533b0863881ed16af1049,validations_output_logging,"def parse_all_validation_groups_on_disk(groups_file_path=None, raw_contents=False): if raw_contents: return contents",def parse_all_validation_groups_on_disk(groups_file_path=None):,4,1
openstack%2Fcinder~master~Ic8f07aa67a39b911318d0f7515d6a66e9febd44f,openstack/cinder,master,Ic8f07aa67a39b911318d0f7515d6a66e9febd44f,Do not render apidoc for cmds,ABANDONED,2020-03-22 15:24:21.000000000,2020-03-23 03:01:40.000000000,,"[{'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 14384}, {'_account_id': 15386}, {'_account_id': 15941}, {'_account_id': 18883}, {'_account_id': 21884}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 24921}, {'_account_id': 25678}, {'_account_id': 26537}, {'_account_id': 29705}, {'_account_id': 29716}]","[{'number': 1, 'created': '2020-03-22 15:24:21.000000000', 'files': ['doc/source/conf.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/9398aea1f16176fc6fc748b2e4b0ba65f7930aba', 'message': 'Do not render apidoc for cmds\n\nLooks like apidoc failed because register_cli_opt\nused by some command line entries, block them all.\n\nChange-Id: Ic8f07aa67a39b911318d0f7515d6a66e9febd44f\n'}]",0,714313,9398aea1f16176fc6fc748b2e4b0ba65f7930aba,17,14,1,29071,,,0,"Do not render apidoc for cmds

Looks like apidoc failed because register_cli_opt
used by some command line entries, block them all.

Change-Id: Ic8f07aa67a39b911318d0f7515d6a66e9febd44f
",git fetch https://review.opendev.org/openstack/cinder refs/changes/13/714313/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/conf.py'],1,9398aea1f16176fc6fc748b2e4b0ba65f7930aba,block-cmd-apidoc," 'cmd/*', 'cmd',",,2,0
openstack%2Fcinder~master~I0857cecd7d8ab0ee7e3e9bd6e15f4987ede4d653,openstack/cinder,master,I0857cecd7d8ab0ee7e3e9bd6e15f4987ede4d653,Remove logging on Swift backup obj writer,MERGED,2018-01-24 16:06:33.000000000,2020-03-23 02:12:24.000000000,2018-01-24 20:50:19.000000000,"[{'_account_id': 1736}, {'_account_id': 6873}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12176}, {'_account_id': 12822}, {'_account_id': 14208}, {'_account_id': 14532}, {'_account_id': 15670}, {'_account_id': 15831}, {'_account_id': 15941}, {'_account_id': 16897}, {'_account_id': 16898}, {'_account_id': 18120}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 21976}, {'_account_id': 22126}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23083}, {'_account_id': 23613}, {'_account_id': 24230}, {'_account_id': 24502}, {'_account_id': 24815}, {'_account_id': 24863}, {'_account_id': 25243}, {'_account_id': 25678}, {'_account_id': 25837}, {'_account_id': 26537}]","[{'number': 1, 'created': '2018-01-24 16:06:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1c47222a68e8d7beb77b836ccd235cf98b7c7eb8', 'message': 'Remove logging on Swift backup obj writer\n\nSince change I1f1d9c0d6e3f04f1ecd5ef7c5d813005ee116409 we are running\nparts of the backups on native threads, which due to an eventlet bug [1]\nhave bad interactions with greenthreads, so we have to avoid any logging\nwhen executing code in a native thread.\n\nThis patch removes the MD5 logging on the SwiftObjectWriter close\nmethod.\n\n[1] https://github.com/eventlet/eventlet/issues/432\n\nChange-Id: I0857cecd7d8ab0ee7e3e9bd6e15f4987ede4d653\n'}, {'number': 2, 'created': '2018-01-24 16:25:15.000000000', 'files': ['cinder/backup/drivers/swift.py', 'cinder/backup/chunkeddriver.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/c6cb84bd63135d62ccd7ed9b23b245c41e9f105c', 'message': 'Remove logging on Swift backup obj writer\n\nSince change I1f1d9c0d6e3f04f1ecd5ef7c5d813005ee116409 we are running\nparts of the backups on native threads, which due to an eventlet bug [1]\nhave bad interactions with greenthreads, so we have to avoid any logging\nwhen executing code in a native thread.\n\nThis patch removes the MD5 logging on the SwiftObjectWriter close\nmethod and adds comments and docstring referring to this limitation.\n\n[1] https://github.com/eventlet/eventlet/issues/432\n\nCloses-Bug: #1745168\nChange-Id: I0857cecd7d8ab0ee7e3e9bd6e15f4987ede4d653\n'}]",1,537437,c6cb84bd63135d62ccd7ed9b23b245c41e9f105c,44,33,2,9535,,,0,"Remove logging on Swift backup obj writer

Since change I1f1d9c0d6e3f04f1ecd5ef7c5d813005ee116409 we are running
parts of the backups on native threads, which due to an eventlet bug [1]
have bad interactions with greenthreads, so we have to avoid any logging
when executing code in a native thread.

This patch removes the MD5 logging on the SwiftObjectWriter close
method and adds comments and docstring referring to this limitation.

[1] https://github.com/eventlet/eventlet/issues/432

Closes-Bug: #1745168
Change-Id: I0857cecd7d8ab0ee7e3e9bd6e15f4987ede4d653
",git fetch https://review.opendev.org/openstack/cinder refs/changes/37/537437/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/backup/drivers/swift.py'],1,1c47222a68e8d7beb77b836ccd235cf98b7c7eb8,bug/1745168,," LOG.debug('swift MD5 for %(object_name)s: %(etag)s', {'object_name': self.object_name, 'etag': etag, }) LOG.debug('backup MD5 for %(object_name)s: %(md5)s', {'object_name': self.object_name, 'md5': md5})",0,4
openstack%2Fcinder~master~Ifb65b8008f30bc9cc4b6cd9b867a726ec4ed4707,openstack/cinder,master,Ifb65b8008f30bc9cc4b6cd9b867a726ec4ed4707,Improve ChunkedBackupDriver hashlib calls,MERGED,2018-02-05 19:04:35.000000000,2020-03-23 02:09:06.000000000,2018-03-06 17:27:15.000000000,"[{'_account_id': 1736}, {'_account_id': 2243}, {'_account_id': 4523}, {'_account_id': 5997}, {'_account_id': 9008}, {'_account_id': 9535}, {'_account_id': 10118}, {'_account_id': 10622}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 13144}, {'_account_id': 13628}, {'_account_id': 14208}, {'_account_id': 14384}, {'_account_id': 14624}, {'_account_id': 15386}, {'_account_id': 15670}, {'_account_id': 15941}, {'_account_id': 16422}, {'_account_id': 16897}, {'_account_id': 16898}, {'_account_id': 18120}, {'_account_id': 18883}, {'_account_id': 19933}, {'_account_id': 21728}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 21976}, {'_account_id': 22126}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23613}, {'_account_id': 24230}, {'_account_id': 24236}, {'_account_id': 24502}, {'_account_id': 24814}, {'_account_id': 24815}, {'_account_id': 24863}, {'_account_id': 25243}, {'_account_id': 25677}, {'_account_id': 25678}, {'_account_id': 25837}, {'_account_id': 26537}, {'_account_id': 27248}]","[{'number': 1, 'created': '2018-02-05 19:04:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6ca7362bf59c55f59d68bc23f308c97c48d5a9c1', 'message': 'Improve ChunkedBackupDriver hashlib calls\n\nCurrently we have 2 hashlib calls withing the ChunkedBackupDriver, one\nto calculate the MD5 of the chunk and another to calculate the SHA256 of\nthe blocks within each chunk.\n\nThis patch improve interactions between cinder and the hashlib library\nmethod calls by making sure MD5 and SHA256 related calls are execute in\na native thread to improve context switching responsiveness within\neventlet.\n\nThe MD5 of a 1GB chunk could take around 4 seconds, so the overhead of\ncreating a native thread is acceptable, and for the SHA256 instead of\ncreating a thread for each call we create a single thread to do the\ncalculations of all the blocks, thus making it cost effective.\n\nCurrent code slices the data into blocks, which means that the data is being\ncopied, but this has now been switched to a memoryview object to take advantage\nof the buffer protocol so copying of data is no longer necesary.\n\nChange-Id: Ifb65b8008f30bc9cc4b6cd9b867a726ec4ed4707\n'}, {'number': 2, 'created': '2018-02-06 16:59:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/56398aefa3e69fdc53dd37d910285344b7aafd0c', 'message': 'Improve ChunkedBackupDriver hashlib calls\n\nCurrently we have 2 hashlib calls withing the ChunkedBackupDriver, one\nto calculate the MD5 of the chunk and another to calculate the SHA256 of\nthe blocks within each chunk.\n\nThis patch improve interactions between cinder and the hashlib library\nmethod calls by making sure MD5 and SHA256 related calls are execute in\na native thread to improve context switching responsiveness within\neventlet.\n\nThe MD5 of a 1GB chunk could take around 4 seconds, so the overhead of\ncreating a native thread is acceptable, and for the SHA256 instead of\ncreating a thread for each call we create a single thread to do the\ncalculations of all the blocks, thus making it cost effective.\n\nCurrent code slices the data into blocks, which means that the data is being\ncopied, but this has now been switched to a memoryview object to take advantage\nof the buffer protocol so copying of data is no longer necesary.\n\nChange-Id: Ifb65b8008f30bc9cc4b6cd9b867a726ec4ed4707\n'}, {'number': 3, 'created': '2018-02-22 15:52:33.000000000', 'files': ['cinder/backup/chunkeddriver.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/671b02b504a45953cf225a210c361729d480dd66', 'message': 'Improve ChunkedBackupDriver hashlib calls\n\nCurrently we have 2 hashlib calls withing the ChunkedBackupDriver, one\nto calculate the MD5 of the chunk and another to calculate the SHA256 of\nthe blocks within each chunk.\n\nThis patch improve interactions between cinder and the hashlib library\nmethod calls by making sure MD5 and SHA256 related calls are execute in\na native thread to improve context switching responsiveness within\neventlet.\n\nThe MD5 of a 1GB chunk could take around 4 seconds, so the overhead of\ncreating a native thread is acceptable, and for the SHA256 instead of\ncreating a thread for each call we create a single thread to do the\ncalculations of all the blocks, thus making it cost effective.\n\nCurrent code slices the data into blocks, which means that the data is being\ncopied, but this has now been switched to a memoryview object to take advantage\nof the buffer protocol so copying of data is no longer necesary.\n\nChange-Id: Ifb65b8008f30bc9cc4b6cd9b867a726ec4ed4707\n'}]",2,540973,671b02b504a45953cf225a210c361729d480dd66,119,48,3,9535,,,0,"Improve ChunkedBackupDriver hashlib calls

Currently we have 2 hashlib calls withing the ChunkedBackupDriver, one
to calculate the MD5 of the chunk and another to calculate the SHA256 of
the blocks within each chunk.

This patch improve interactions between cinder and the hashlib library
method calls by making sure MD5 and SHA256 related calls are execute in
a native thread to improve context switching responsiveness within
eventlet.

The MD5 of a 1GB chunk could take around 4 seconds, so the overhead of
creating a native thread is acceptable, and for the SHA256 instead of
creating a thread for each call we create a single thread to do the
calculations of all the blocks, thus making it cost effective.

Current code slices the data into blocks, which means that the data is being
copied, but this has now been switched to a memoryview object to take advantage
of the buffer protocol so copying of data is no longer necesary.

Change-Id: Ifb65b8008f30bc9cc4b6cd9b867a726ec4ed4707
",git fetch https://review.opendev.org/openstack/cinder refs/changes/73/540973/2 && git format-patch -1 --stdout FETCH_HEAD,['cinder/backup/chunkeddriver.py'],1,6ca7362bf59c55f59d68bc23f308c97c48d5a9c1,improve/backup-haslib," md5 = eventlet.tpool.execute(hashlib.md5, data).hexdigest() def _calculate_sha(self, data): """"""Calculate SHA256 of a data chunk. This method cannot log anything as it is called on a native thread. """""" # NOTE(geguileo): Using memoryview to avoid data copying when slicing # for the sha256 call. chunk = memoryview(data) shalist = [] off = 0 datalen = len(chunk) while off < datalen: chunk_end = min(datalen, off + self.sha_block_size_bytes) block = chunk[off:chunk_end] sha = hashlib.sha256(block).hexdigest() shalist.append(sha) off += self.sha_block_size_bytes return shalist shalist = eventlet.tpool.execute(self._calculate_sha, data)", md5 = hashlib.md5(data).hexdigest() shalist = [] off = 0 datalen = len(data) while off < datalen: chunk_start = off chunk_end = chunk_start + self.sha_block_size_bytes if chunk_end > datalen: chunk_end = datalen chunk = data[chunk_start:chunk_end] sha = hashlib.sha256(chunk).hexdigest() shalist.append(sha) off += self.sha_block_size_bytes,21,13
openstack%2Ftripleo-quickstart~master~Ie47c4cd63205dcbf458b794b0d4ba0da96e417b0,openstack/tripleo-quickstart,master,Ie47c4cd63205dcbf458b794b0d4ba0da96e417b0,Run py2 in case of CentOS7 on OVB,MERGED,2020-03-22 13:27:54.000000000,2020-03-23 01:15:47.000000000,2020-03-23 01:14:36.000000000,"[{'_account_id': 8367}, {'_account_id': 9592}, {'_account_id': 10969}, {'_account_id': 18846}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-22 13:27:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart/commit/1d48001d8c25a930cc5aebfe6c52400e6d3e5141', 'message': 'Run py2 in case of CentOS7 on OVB\n\nRun script with python2 even if python3 is installed on CentOS7,\nbecause all python modules are of version py2.\n\nChange-Id: Ie47c4cd63205dcbf458b794b0d4ba0da96e417b0\nCloses-Bug: #1868439\n'}, {'number': 2, 'created': '2020-03-22 13:28:13.000000000', 'files': ['roles/tripleo-inventory/tasks/inventory.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart/commit/16beb60b8f6f6dc7233dd3a8e2e839ec48ee8a64', 'message': 'Run py2 in case of CentOS7 on OVB\n\nRun script with python2 even if python3 is installed on CentOS7,\nbecause all python modules are of version py2.\n\nChange-Id: Ie47c4cd63205dcbf458b794b0d4ba0da96e417b0\nCloses-Bug: #1868439\n'}]",0,714299,16beb60b8f6f6dc7233dd3a8e2e839ec48ee8a64,21,6,2,10969,,,0,"Run py2 in case of CentOS7 on OVB

Run script with python2 even if python3 is installed on CentOS7,
because all python modules are of version py2.

Change-Id: Ie47c4cd63205dcbf458b794b0d4ba0da96e417b0
Closes-Bug: #1868439
",git fetch https://review.opendev.org/openstack/tripleo-quickstart refs/changes/99/714299/2 && git format-patch -1 --stdout FETCH_HEAD,['roles/tripleo-inventory/tasks/inventory.yml'],1,1d48001d8c25a930cc5aebfe6c52400e6d3e5141,fix1868439, {%- if ansible_distribution_major_version|int == 7 -%} python {{ working_dir }}/get-overcloud-nodes.py {% else %} {% endif %},,4,0
openstack%2Fmanila~master~I595c5df77b14bc95f1130c2a1a1dea2244503951,openstack/manila,master,I595c5df77b14bc95f1130c2a1a1dea2244503951,Another solution about bug:1655427,ABANDONED,2020-03-17 07:03:24.000000000,2020-03-23 01:10:38.000000000,,"[{'_account_id': 15386}, {'_account_id': 15831}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 07:03:24.000000000', 'files': ['manila/api/v2/share_export_locations.py', 'manila/db/api.py', 'manila/db/sqlalchemy/api.py', 'manila/api/v2/share_instance_export_locations.py'], 'web_link': 'https://opendev.org/openstack/manila/commit/4180567fb99880ecdb96b8b4e8c3281627ef988c', 'message': 'Another solution about bug:1655427\n\nChange-Id: I595c5df77b14bc95f1130c2a1a1dea2244503951\n'}]",0,713376,4180567fb99880ecdb96b8b4e8c3281627ef988c,9,5,1,30407,,,0,"Another solution about bug:1655427

Change-Id: I595c5df77b14bc95f1130c2a1a1dea2244503951
",git fetch https://review.opendev.org/openstack/manila refs/changes/76/713376/1 && git format-patch -1 --stdout FETCH_HEAD,"['manila/api/v2/share_export_locations.py', 'manila/db/api.py', 'manila/db/sqlalchemy/api.py', 'manila/api/v2/share_instance_export_locations.py']",4,4180567fb99880ecdb96b8b4e8c3281627ef988c,bug/1655427," db_api.share_instance_get(context, share_instance_id, project_only=True)"," db_api.share_instance_get(context, share_instance_id)",21,12
openstack%2Fcyborg~master~I70100a71114e1d75833e6c783086325baa0e3fd2,openstack/cyborg,master,I70100a71114e1d75833e6c783086325baa0e3fd2,Generate PDF documentation,MERGED,2019-10-03 08:00:54.000000000,2020-03-23 00:51:42.000000000,2020-03-23 00:50:18.000000000,"[{'_account_id': 841}, {'_account_id': 11878}, {'_account_id': 14107}, {'_account_id': 14131}, {'_account_id': 21672}, {'_account_id': 22306}, {'_account_id': 22348}, {'_account_id': 23168}, {'_account_id': 25738}, {'_account_id': 26458}, {'_account_id': 26490}, {'_account_id': 27458}, {'_account_id': 28748}, {'_account_id': 30395}, {'_account_id': 30759}]","[{'number': 1, 'created': '2019-10-03 08:00:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/21ce241e29e756fd15fecc3e6c250a538ab1385d', 'message': 'PDF documentation build\n\ndoc/source/conf.py are modified a bit\nto fit both HTML and PDF docs.\n\nChange-Id: I70100a71114e1d75833e6c783086325baa0e3fd2\n'}, {'number': 2, 'created': '2019-10-03 08:53:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/e3516d7bb3a05ea744ca0da80ef6919ee3083c8e', 'message': 'Generate PDF documentation\n\ntox.ini and doc/source/conf.py are modified a bit\nto fit both HTML and PDF docs.\n\nChange-Id: I70100a71114e1d75833e6c783086325baa0e3fd2\n'}, {'number': 3, 'created': '2019-10-12 08:22:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/794bde2a2e86c48716773d2d5557fd360c2984a1', 'message': 'Generate PDF documentation\n\n- Add \'pdf-docs\' tox env\n- Skip sample configuration files in PDF doc due to LaTeX error\n- Specify openany in extraclassoptions to skip blank pages\n  (oneside is also specified to use the same page layout\n   for even and odd pages)\n- tocdepth is set to 2 for better PDF TOC\n- printindex and makeindex are set to empty in latex_elements\n  to avoid an empty ""Index"" section in PDF\n\nChange-Id: I70100a71114e1d75833e6c783086325baa0e3fd2\n'}, {'number': 4, 'created': '2020-03-21 06:41:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/e3c6b267cd684f625d45cf126b7a1df8166744c3', 'message': 'Generate PDF documentation\n\n- Add \'pdf-docs\' tox env\n- Skip sample configuration files in PDF doc due to LaTeX error\n- Specify openany in extraclassoptions to skip blank pages\n  (oneside is also specified to use the same page layout\n   for even and odd pages)\n- tocdepth is set to 2 for better PDF TOC\n- printindex and makeindex are set to empty in latex_elements\n  to avoid an empty ""Index"" section in PDF\n\nChange-Id: I70100a71114e1d75833e6c783086325baa0e3fd2\n'}, {'number': 5, 'created': '2020-03-21 07:53:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/4e8a1ff11db24158686c773400dfe9c2aa5a7724', 'message': 'Generate PDF documentation\n\n- Add a new pdf-docs environment to enable PDF build.\n- maxlistdepth=10 in latex_elements is needed to handle\n  deeper levels of nesting.\n- Specify openany in extraclassoptions to skip blank pages\n  (oneside is also specified to use the same page layout\n   for even and odd pages)\n- tocdepth is set to 2 for better PDF TOC\n- printindex and makeindex are set to empty in latex_elements\n  to avoid an empty ""Index"" section in PDF\n- Sample config/policy files are skipped in the PDF document\n  as inline sample files cause LaTeX error [1] and direct links\n  in PDF doc is discouraged. Note that sample-config and sample-policy\n  need to be excluded to avoid the LaTeX error [1] and\n  :orphan: is specified in those files.\n\n[1] https://github.com/sphinx-doc/sphinx/issues/3099\n\nChange-Id: I70100a71114e1d75833e6c783086325baa0e3fd2\n'}, {'number': 6, 'created': '2020-03-21 08:14:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/93108098a20606ca2214f201c7fde82778b8606d', 'message': 'Generate PDF documentation\n\n- Add a new pdf-docs environment to enable PDF build.\n- sphinxcontrib-svg2pdfconverter is used to handle SVG properly.\n- maxlistdepth=10 in latex_elements is needed to handle\n  deeper levels of nesting.\n- Specify openany in extraclassoptions to skip blank pages\n  (oneside is also specified to use the same page layout\n   for even and odd pages)\n- tocdepth is set to 2 for better PDF TOC\n- printindex and makeindex are set to empty in latex_elements\n  to avoid an empty ""Index"" section in PDF\n- Sample config/policy files are skipped in the PDF document\n  as inline sample files cause LaTeX error [1] and direct links\n  in PDF doc is discouraged. Note that sample-config and sample-policy\n  need to be excluded to avoid the LaTeX error [1] and\n  :orphan: is specified in those files.\n\n[1] https://github.com/sphinx-doc/sphinx/issues/3099\n\nChange-Id: I70100a71114e1d75833e6c783086325baa0e3fd2\n'}, {'number': 7, 'created': '2020-03-21 14:44:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/c20fd36dcde42c01679b131c58b38d2947d900f5', 'message': 'Generate PDF documentation\n\n- Add a new pdf-docs environment to enable PDF build.\n- sphinxcontrib-svg2pdfconverter is used to handle SVG properly.\n- maxlistdepth=10 in latex_elements is needed to handle\n  deeper levels of nesting.\n- Specify openany in extraclassoptions to skip blank pages\n  (oneside is also specified to use the same page layout\n   for even and odd pages)\n- tocdepth is set to 2 for better PDF TOC\n- printindex and makeindex are set to empty in latex_elements\n  to avoid an empty ""Index"" section in PDF\n- Sample config/policy files are skipped in the PDF document\n  as inline sample files cause LaTeX error [1] and direct links\n  in PDF doc is discouraged. Note that sample-config and sample-policy\n  need to be excluded to avoid the LaTeX error [1] and\n  :orphan: is specified in those files.\n\n[1] https://github.com/sphinx-doc/sphinx/issues/3099\n\nChange-Id: I70100a71114e1d75833e6c783086325baa0e3fd2\n'}, {'number': 8, 'created': '2020-03-21 15:03:03.000000000', 'files': ['doc/source/conf.py', 'doc/source/configuration/sample_config.rst', 'doc/requirements.txt', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/cyborg/commit/31d23b73159e542d8791602afec991b4d3990cdf', 'message': 'Generate PDF documentation\n\n- Add a new pdf-docs environment to enable PDF build.\n- sphinxcontrib-svg2pdfconverter is used to handle SVG properly.\n- maxlistdepth=10 in latex_elements is needed to handle\n  deeper levels of nesting.\n- Specify openany in extraclassoptions to skip blank pages\n  (oneside is also specified to use the same page layout\n   for even and odd pages)\n- tocdepth is set to 2 for better PDF TOC\n- printindex and makeindex are set to empty in latex_elements\n  to avoid an empty ""Index"" section in PDF\n- Sample config/policy files are skipped in the PDF document\n  as inline sample files cause LaTeX error [1] and direct links\n  in PDF doc is discouraged. Note that sample-config and sample-policy\n  need to be excluded to avoid the LaTeX error [1] and\n  :orphan: is specified in those files.\n\n[1] https://github.com/sphinx-doc/sphinx/issues/3099\n\nChange-Id: I70100a71114e1d75833e6c783086325baa0e3fd2\n'}]",7,686344,31d23b73159e542d8791602afec991b4d3990cdf,35,15,8,30395,,,0,"Generate PDF documentation

- Add a new pdf-docs environment to enable PDF build.
- sphinxcontrib-svg2pdfconverter is used to handle SVG properly.
- maxlistdepth=10 in latex_elements is needed to handle
  deeper levels of nesting.
- Specify openany in extraclassoptions to skip blank pages
  (oneside is also specified to use the same page layout
   for even and odd pages)
- tocdepth is set to 2 for better PDF TOC
- printindex and makeindex are set to empty in latex_elements
  to avoid an empty ""Index"" section in PDF
- Sample config/policy files are skipped in the PDF document
  as inline sample files cause LaTeX error [1] and direct links
  in PDF doc is discouraged. Note that sample-config and sample-policy
  need to be excluded to avoid the LaTeX error [1] and
  :orphan: is specified in those files.

[1] https://github.com/sphinx-doc/sphinx/issues/3099

Change-Id: I70100a71114e1d75833e6c783086325baa0e3fd2
",git fetch https://review.opendev.org/openstack/cyborg refs/changes/44/686344/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/conf.py'],1,21ce241e29e756fd15fecc3e6c250a538ab1385d,build-pdf-docs," 'doc-cyborg.tex', u'OpenStack Foundation', 'howto'),"," '%s.tex' % project, u'OpenStack Foundation', 'manual'),",2,2
openstack%2Fpython-zunclient~master~Ie370f352c9f996c4f1953787b1d5b24bd723c6cc,openstack/python-zunclient,master,Ie370f352c9f996c4f1953787b1d5b24bd723c6cc,Add support for requested host,MERGED,2020-03-14 22:21:04.000000000,2020-03-22 23:23:57.000000000,2020-03-22 23:22:42.000000000,"[{'_account_id': 11536}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-14 22:21:04.000000000', 'files': ['zunclient/v1/containers_shell.py', 'zunclient/osc/v1/containers.py', 'zunclient/api_versions.py', 'zunclient/v1/containers.py'], 'web_link': 'https://opendev.org/openstack/python-zunclient/commit/3c40b5c0933627df2f210635c242c010df308494', 'message': 'Add support for requested host\n\nDepends-On: https://review.opendev.org/#/c/711864/\nChange-Id: Ie370f352c9f996c4f1953787b1d5b24bd723c6cc\n'}]",0,713114,3c40b5c0933627df2f210635c242c010df308494,9,2,1,11536,,,0,"Add support for requested host

Depends-On: https://review.opendev.org/#/c/711864/
Change-Id: Ie370f352c9f996c4f1953787b1d5b24bd723c6cc
",git fetch https://review.opendev.org/openstack/python-zunclient refs/changes/14/713114/1 && git format-patch -1 --stdout FETCH_HEAD,"['zunclient/v1/containers_shell.py', 'zunclient/osc/v1/containers.py', 'zunclient/api_versions.py', 'zunclient/v1/containers.py']",4,3c40b5c0933627df2f210635c242c010df308494,," 'exposed_ports', 'healthcheck', 'registry', 'tty', 'host']"," 'exposed_ports', 'healthcheck', 'registry', 'tty']",27,2
openstack%2Fironic~master~I90c3c94112d093e2309414b9902f58d31d925ad3,openstack/ironic,master,I90c3c94112d093e2309414b9902f58d31d925ad3,Do not use random to generate token,MERGED,2020-03-18 16:06:28.000000000,2020-03-22 22:42:26.000000000,2020-03-22 22:40:49.000000000,"[{'_account_id': 10239}, {'_account_id': 11655}, {'_account_id': 14629}, {'_account_id': 15519}, {'_account_id': 19339}, {'_account_id': 22348}, {'_account_id': 23851}, {'_account_id': 24828}]","[{'number': 1, 'created': '2020-03-18 16:06:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/dea5cd324b1339d0e5fec073266dd767df40d981', 'message': ""Do not use random to generate token\n\nTo avoid problems with FIPS 140=2 let's generate\nthe token using the secrets module instead of random.\n\nChange-Id: I90c3c94112d093e2309414b9902f58d31d925ad3\n""}, {'number': 2, 'created': '2020-03-19 12:15:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/30c940db41245ba41e2aaeb3a1b41bd3e4b64ac0', 'message': ""Do not use random to generate token\n\nTo avoid problems with FIPS 140=2 let's generate\nthe token using the secrets module instead of random.\n\nChange-Id: I90c3c94112d093e2309414b9902f58d31d925ad3\n""}, {'number': 3, 'created': '2020-03-19 15:04:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/60d4e7f12897243cf96f32f0e9ef64d8ac5504a7', 'message': ""Do not use random to generate token\n\nTo avoid problems with FIPS 140=2 let's generate\nthe token using the secrets module instead of random.\n\nChange-Id: I90c3c94112d093e2309414b9902f58d31d925ad3\nStory: 2007444\nTask: 39104\n""}, {'number': 4, 'created': '2020-03-19 17:58:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bd3600acff829546108ed64d2a8ecdcfbac87b58', 'message': ""Do not use random to generate token\n\nTo avoid problems with FIPS 140-2 let's generate\nthe token using the secrets module instead of random.\n\nChange-Id: I90c3c94112d093e2309414b9902f58d31d925ad3\nStory: 2007444\nTask: 39104\n""}, {'number': 5, 'created': '2020-03-20 19:30:28.000000000', 'files': ['ironic/tests/unit/conductor/test_utils.py', 'ironic/conductor/utils.py', 'releasenotes/notes/use_secrets_to_generate_token-55af0f43e5a80b9e.yaml'], 'web_link': 'https://opendev.org/openstack/ironic/commit/1425adbb0572d6280ae70635baa53b0d9291ac07', 'message': ""Do not use random to generate token\n\nTo avoid problems with FIPS 140-2 let's generate\nthe token using the secrets module instead of random.\n\nChange-Id: I90c3c94112d093e2309414b9902f58d31d925ad3\nStory: 2007444\nTask: 39104\n""}]",16,713687,1425adbb0572d6280ae70635baa53b0d9291ac07,41,8,5,15519,,,0,"Do not use random to generate token

To avoid problems with FIPS 140-2 let's generate
the token using the secrets module instead of random.

Change-Id: I90c3c94112d093e2309414b9902f58d31d925ad3
Story: 2007444
Task: 39104
",git fetch https://review.opendev.org/openstack/ironic refs/changes/87/713687/5 && git format-patch -1 --stdout FETCH_HEAD,['ironic/conductor/utils.py'],1,dea5cd324b1339d0e5fec073266dd767df40d981,use_secrets,import secrets token = ''.join(secrets.choice(characters) for i in range(128)),import random token = ''.join( random.SystemRandom().choice(characters) for i in range(128)),2,3
openstack%2Frally~master~If3dc215972a28ade37dacbaf8482aa32c848c19d,openstack/rally,master,If3dc215972a28ade37dacbaf8482aa32c848c19d,Remove rally/ui/templates/base.mako,MERGED,2020-03-22 21:33:28.000000000,2020-03-22 22:24:55.000000000,2020-03-22 22:24:55.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 21:33:28.000000000', 'files': ['rally/ui/templates/base.mako'], 'web_link': 'https://opendev.org/openstack/rally/commit/2826e909df880eed7e0d22bc6626857a1bffd2b9', 'message': 'Remove rally/ui/templates/base.mako\n\nIt left since those times when we used mako\n\nChange-Id: If3dc215972a28ade37dacbaf8482aa32c848c19d\n'}]",0,714334,2826e909df880eed7e0d22bc6626857a1bffd2b9,6,2,1,9545,,,0,"Remove rally/ui/templates/base.mako

It left since those times when we used mako

Change-Id: If3dc215972a28ade37dacbaf8482aa32c848c19d
",git fetch https://review.opendev.org/openstack/rally refs/changes/34/714334/1 && git format-patch -1 --stdout FETCH_HEAD,['rally/ui/templates/base.mako'],1,2826e909df880eed7e0d22bc6626857a1bffd2b9,remove_futuru_import,,"<!doctype html> <html<%block name=""html_attr""/>> <head> <meta charset=""utf-8""> <meta name=""viewport"" content=""width=device-width, initial-scale=1.0""> <title>Rally | <%block name=""title_text""/></title> <%block name=""libs""/> <script type=""text/javascript""><%block name=""js_before""/></script> <style> body { margin:0; padding:0 0 50px; font-size:14px; font-family:Helvetica,Arial,sans-serif } a, a:active, a:focus, a:visited { text-decoration:none; outline:none } p { margin:0; padding:5px 0 } p.thesis { padding:10px 0 } h1 { color:#666; margin:0 0 20px; font-size:30px; font-weight:normal } h2 { color:#777; margin:20px 0 10px; font-size:25px; font-weight:normal } h3 { color:#666; margin:13px 0 4px; font-size:18px; font-weight:normal } table { border-collapse:collapse; border-spacing:0; width:100%; font-size:12px; margin:0 0 10px } table th { text-align:left; padding:8px; color:#000; border:2px solid #ddd; border-width:0 0 2px 0 } table th.sortable { cursor:pointer } table td { text-align:left; border-top:1px solid #ddd; padding:8px; color:#333 } table.compact td { padding:4px 8px } table.striped tr:nth-child(odd) td { background:#f9f9f9 } table.linked tbody tr:hover { background:#f9f9f9; cursor:pointer } .richcolor td { color:#036; font-weight:bold } .rich, .rich td { font-weight:bold } .code { padding:10px; font-size:13px; color:#333; background:#f6f6f6; border:1px solid #e5e5e5; border-radius:4px } .header { text-align:left; background:#333; font-size:18px; padding:13px 0; margin-bottom:20px; color:#fff; background-image:linear-gradient(to bottom, #444 0px, #222 100%) } .header a, .header a:visited, .header a:focus { color:#999 } .notify-error { padding:5px 10px; background:#fee; color:red } .status-skip, .status-skip td { color:grey } .status-pass, .status-pass td { color:green } .status-fail, .status-fail td { color:red } .capitalize { text-transform:capitalize } <%block name=""css""/> .content-wrap {<%block name=""css_content_wrap""> margin:0 auto; padding:0 5px </%block>} <%block name=""media_queries""/> </style> </head> <body<%block name=""body_attr""/>> <div class=""header""> <div class=""content-wrap""> <a href=""https://github.com/openstack/rally"">Rally</a>&nbsp; <span><%block name=""header_text""/></span> </div> </div> <div class=""content-wrap""> <%block name=""content""/> </div> <script type=""text/javascript""><%block name=""js_after""/></script> </body> </html> ",0,56
openstack%2Fzun~master~If83de304d8fa609c618030df92c2997fc0731956,openstack/zun,master,If83de304d8fa609c618030df92c2997fc0731956,Support 'host' on creating container,MERGED,2020-03-09 00:07:41.000000000,2020-03-22 22:12:59.000000000,2020-03-22 22:10:55.000000000,"[{'_account_id': 11536}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-09 00:07:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/360873d038fbaaa9d789b03c3619ca43fab59dd9', 'message': ""[WIP] Support 'host' on creating container\n\nChange-Id: If83de304d8fa609c618030df92c2997fc0731956\n""}, {'number': 2, 'created': '2020-03-14 18:23:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/1b30084a34c0fd481d7fa76374ba970322991b24', 'message': ""[WIP] Support 'host' on creating container\n\nChange-Id: If83de304d8fa609c618030df92c2997fc0731956\n""}, {'number': 3, 'created': '2020-03-14 22:22:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/7b365d3dac142bcfab642d22a013c707e8d92a78', 'message': ""[WIP] Support 'host' on creating container\n\nChange-Id: If83de304d8fa609c618030df92c2997fc0731956\n""}, {'number': 4, 'created': '2020-03-14 22:56:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zun/commit/6662dd80b2aa159fd1cf20386a572fbe1f113842', 'message': ""Support 'host' on creating container\n\nImplements: blueprint specify-host-in-creation\nChange-Id: If83de304d8fa609c618030df92c2997fc0731956\n""}, {'number': 5, 'created': '2020-03-14 23:02:00.000000000', 'files': ['api-ref/source/parameters.yaml', 'zun/compute/api.py', 'zun/scheduler/filter_scheduler.py', '.zuul.yaml', 'zun/api/rest_api_version_history.rst', 'zun/tests/unit/compute/test_compute_api.py', 'zun/tests/unit/api/base.py', 'zun/scheduler/utils.py', 'api-ref/source/containers.inc', 'zun/common/exception.py', 'zun/common/policies/container.py', 'releasenotes/notes/add-support-for-requested_host-0ea7e317234c3d0c.yaml', 'zun/api/controllers/v1/schemas/containers.py', 'zun/tests/unit/api/controllers/test_root.py', 'zun/api/controllers/v1/containers.py', 'zun/api/controllers/versions.py'], 'web_link': 'https://opendev.org/openstack/zun/commit/dc49d557f0630d023e547f83504d7e7c996d7444', 'message': ""Support 'host' on creating container\n\nImplements: blueprint specify-host-in-creation\nChange-Id: If83de304d8fa609c618030df92c2997fc0731956\n""}]",0,711864,dc49d557f0630d023e547f83504d7e7c996d7444,16,2,5,11536,,,0,"Support 'host' on creating container

Implements: blueprint specify-host-in-creation
Change-Id: If83de304d8fa609c618030df92c2997fc0731956
",git fetch https://review.opendev.org/openstack/zun refs/changes/64/711864/5 && git format-patch -1 --stdout FETCH_HEAD,"['zun/compute/api.py', 'zun/common/policies/container.py', 'zun/api/controllers/v1/schemas/containers.py', 'zun/scheduler/filter_scheduler.py', 'zun/api/controllers/v1/containers.py']",5,360873d038fbaaa9d789b03c3619ca43fab59dd9,bp/specify-host-in-creation," @base.Controller.api_version(""1.36"", ""1.38"") # noqa @base.Controller.api_version(""1.39"") # noqa @pecan.expose('json') @api_utils.enforce_content_types(['application/json']) @exception.wrap_pecan_controller_exception @validation.validate_query_param(pecan.request, schema.query_param_create) @validation.validated(schema.container_create_v139) def post(self, run=False, **container_dict): return self._do_post(run, **container_dict) requested_host = container_dict.pop('host', None) if requested_host: policy.enforce(context, ""container:create:requested_destination"", action=""container:create:requested_destination"") extra_spec['requested_host'] = requested_host"," @base.Controller.api_version(""1.36"") # noqa",84,5
openstack%2Fzun~master~I5f4b2fe2306d9e667ea61b78fab32389d7c97104,openstack/zun,master,I5f4b2fe2306d9e667ea61b78fab32389d7c97104,Pass runtime handler to CRI,MERGED,2020-03-16 03:07:46.000000000,2020-03-22 21:58:00.000000000,2020-03-22 21:56:40.000000000,"[{'_account_id': 11536}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-16 03:07:46.000000000', 'files': ['zun/container/cri/driver.py'], 'web_link': 'https://opendev.org/openstack/zun/commit/8369d8347fa85c564fae660524d8484d4eb5f05a', 'message': 'Pass runtime handler to CRI\n\nThis basically allows cloud deployers to use kata container as\nthe default runtime.\n\nChange-Id: I5f4b2fe2306d9e667ea61b78fab32389d7c97104\nImplements: blueprint capsule-kata-support\n'}]",0,713159,8369d8347fa85c564fae660524d8484d4eb5f05a,7,2,1,11536,,,0,"Pass runtime handler to CRI

This basically allows cloud deployers to use kata container as
the default runtime.

Change-Id: I5f4b2fe2306d9e667ea61b78fab32389d7c97104
Implements: blueprint capsule-kata-support
",git fetch https://review.opendev.org/openstack/zun refs/changes/59/713159/1 && git format-patch -1 --stdout FETCH_HEAD,['zun/container/cri/driver.py'],1,8369d8347fa85c564fae660524d8484d4eb5f05a,bp/capsule-kata-support," runtime = capsule.runtime or CONF.container_runtime if runtime == ""runc"": # pass """" to specify the default runtime which is runc runtime = """" api_pb2.RunPodSandboxRequest( config=sandbox_config, runtime_handler=runtime, ) )", api_pb2.RunPodSandboxRequest(config=sandbox_config)),9,1
openstack%2Frally~master~I7ab4e5ddaa7a3906495da3f8edb449cbd9c8a437,openstack/rally,master,I7ab4e5ddaa7a3906495da3f8edb449cbd9c8a437,[py3] Remove future imports,MERGED,2020-03-22 20:55:43.000000000,2020-03-22 21:50:42.000000000,2020-03-22 21:50:42.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 20:55:43.000000000', 'files': ['tests/ci/render.py', 'rally/cli/cliutils.py', 'rally/cli/commands/task.py', 'tests/unit/common/test_utils.py', 'rally/cli/commands/deployment.py', 'rally/cli/commands/db.py', 'rally/cli/commands/verify.py', 'rally/cli/commands/plugin.py', 'rally/common/streaming_algorithms.py', 'rally/cli/commands/env.py', 'rally/cli/main.py', 'rally/plugins/task/sla/performance_degradation.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/2255b18be8ee0125a504e625073a218b53541db0', 'message': '[py3] Remove future imports\n\nChange-Id: I7ab4e5ddaa7a3906495da3f8edb449cbd9c8a437\n'}]",0,714332,2255b18be8ee0125a504e625073a218b53541db0,10,2,1,9545,,,0,"[py3] Remove future imports

Change-Id: I7ab4e5ddaa7a3906495da3f8edb449cbd9c8a437
",git fetch https://review.opendev.org/openstack/rally refs/changes/32/714332/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/ci/render.py', 'rally/cli/cliutils.py', 'rally/cli/commands/task.py', 'tests/unit/common/test_utils.py', 'rally/cli/commands/deployment.py', 'rally/cli/commands/db.py', 'rally/cli/commands/verify.py', 'rally/cli/commands/plugin.py', 'rally/common/streaming_algorithms.py', 'rally/cli/commands/env.py', 'rally/cli/main.py', 'rally/plugins/task/sla/performance_degradation.py']",12,2255b18be8ee0125a504e625073a218b53541db0,remove_futuru_import,,from __future__ import division ,0,21
openstack%2Fansible-collections-openstack~master~Ib231a50fa5e8427ef39077bc49ac806aa0a5732e,openstack/ansible-collections-openstack,master,Ib231a50fa5e8427ef39077bc49ac806aa0a5732e,"Add rocky, stein jobs",MERGED,2020-03-20 10:48:47.000000000,2020-03-22 21:40:53.000000000,2020-03-22 21:40:53.000000000,"[{'_account_id': 2}, {'_account_id': 6816}, {'_account_id': 10239}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 10:48:47.000000000', 'files': ['.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/ansible-collections-openstack/commit/42fbc640ee57f388a8ae7d51d7e4012018c7e59c', 'message': 'Add rocky, stein jobs\n\nAdd rocky and stein jobs to check and gate\nand remove non voting jobs of ansible devel from gates.\nChange-Id: Ib231a50fa5e8427ef39077bc49ac806aa0a5732e\n'}]",2,714073,42fbc640ee57f388a8ae7d51d7e4012018c7e59c,7,4,1,10969,,,0,"Add rocky, stein jobs

Add rocky and stein jobs to check and gate
and remove non voting jobs of ansible devel from gates.
Change-Id: Ib231a50fa5e8427ef39077bc49ac806aa0a5732e
",git fetch https://review.opendev.org/openstack/ansible-collections-openstack refs/changes/73/714073/1 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.yaml'],1,42fbc640ee57f388a8ae7d51d7e4012018c7e59c,newbranches,- job: name: ansible-collections-openstack-functional-devstack-stein-ansible-devel parent: ansible-collections-openstack-functional-devstack description: | Run openstack collections functional tests against a stein devstack using stein brach of openstacksdk and devel branch of ansible # non-voting because we can't prevent ansible devel from breaking us voting: false required-projects: - name: github.com/ansible/ansible override-checkout: devel - name: openstack/openstacksdk override-branch: stein - name: openstack/devstack override-checkout: stein - job: name: ansible-collections-openstack-functional-devstack-stein-ansible-2.9 parent: ansible-collections-openstack-functional-devstack-ansible-devel description: | Run openstack collections functional tests against a stein devstack using stein brach of openstacksdk and stable 2.9 branch of ansible # non-voting because we can't prevent ansible devel from breaking us voting: true required-projects: - name: github.com/ansible/ansible override-checkout: stable-2.9 - name: openstack/openstacksdk override-branch: stein - name: openstack/devstack override-checkout: stein - job: name: ansible-collections-openstack-functional-devstack-rocky-ansible-devel parent: ansible-collections-openstack-functional-devstack description: | Run openstack collections functional tests against a rocky devstack using rocky brach of openstacksdk and devel branch of ansible # non-voting because we can't prevent ansible devel from breaking us voting: false required-projects: - name: github.com/ansible/ansible override-checkout: devel - name: openstack/openstacksdk override-branch: rocky - name: openstack/devstack override-checkout: rocky - job: name: ansible-collections-openstack-functional-devstack-rocky-ansible-2.9 parent: ansible-collections-openstack-functional-devstack-ansible-devel description: | Run openstack collections functional tests against a rocky devstack using rocky brach of openstacksdk and stable 2.9 branch of ansible # non-voting because we can't prevent ansible devel from breaking us voting: true required-projects: - name: github.com/ansible/ansible override-checkout: stable-2.9 - name: openstack/openstacksdk override-branch: rocky - name: openstack/devstack override-checkout: rocky - ansible-collections-openstack-functional-devstack-stein-ansible-devel: dependencies: *deps_unit_lint - ansible-collections-openstack-functional-devstack-stein-ansible-2.9: dependencies: *deps_unit_lint - ansible-collections-openstack-functional-devstack-rocky-ansible-devel: dependencies: *deps_unit_lint - ansible-collections-openstack-functional-devstack-rocky-ansible-2.9: dependencies: *deps_unit_lint - ansible-collections-openstack-functional-devstack-ansible-2.9 - ansible-collections-openstack-functional-devstack-train-ansible-2.9 - ansible-collections-openstack-functional-devstack-stein-ansible-2.9 - ansible-collections-openstack-functional-devstack-rocky-ansible-2.9 , - ansible-collections-openstack-functional-devstack-ansible-devel,78,1
openstack%2Fsushy~master~Ief77d3ccce7fda7e6656c494060e47fe27b20f2e,openstack/sushy,master,Ief77d3ccce7fda7e6656c494060e47fe27b20f2e,Lazily load message registries,MERGED,2020-03-17 18:47:59.000000000,2020-03-22 21:39:52.000000000,2020-03-22 21:38:36.000000000,"[{'_account_id': 10239}, {'_account_id': 11655}, {'_account_id': 19339}, {'_account_id': 22348}, {'_account_id': 26340}]","[{'number': 1, 'created': '2020-03-17 18:47:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy/commit/94e9bd922c40d2cfac665c4fac34baca50972951', 'message': 'Lazily load message registries\n\nPostpone (potentially very large) Redfish message registries\ndownload and processing up to the first access by the client.\n\nThe goal is to reduce the amount of unnecessary traffic and\nCPU cycles.\n\nChange-Id: Ief77d3ccce7fda7e6656c494060e47fe27b20f2e\n'}, {'number': 2, 'created': '2020-03-18 16:25:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy/commit/4b38f79c0fa5525320b7dadd13ad9ebe7a77c108', 'message': 'Lazily load message registries\n\nPostpone (potentially very large) Redfish message registries\ndownload and processing up to the first access by the client.\n\nThe goal is to reduce the amount of unnecessary traffic and\nCPU cycles.\n\nChange-Id: Ief77d3ccce7fda7e6656c494060e47fe27b20f2e\n'}, {'number': 3, 'created': '2020-03-18 16:33:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sushy/commit/607865f410ede0030195a734c900e79e9703e860', 'message': 'Lazily load message registries\n\nPostpone (potentially very large) Redfish message registries\ndownload and processing up to the first access by the client.\n\nThe goal is to reduce the amount of unnecessary traffic and\nCPU cycles.\n\nChange-Id: Ief77d3ccce7fda7e6656c494060e47fe27b20f2e\n'}, {'number': 4, 'created': '2020-03-19 11:34:15.000000000', 'files': ['sushy/tests/unit/test_main.py', 'sushy/main.py', 'releasenotes/notes/lazily-load-registries-0e9441e435c2471d.yaml'], 'web_link': 'https://opendev.org/openstack/sushy/commit/c737bf831cde4599796f84daf770159155ea2c1a', 'message': 'Lazily load message registries\n\nPostpone (potentially very large) Redfish message registries\ndownload and processing up to the first access by the client.\n\nThe goal is to reduce the amount of unnecessary traffic and\nCPU cycles.\n\nChange-Id: Ief77d3ccce7fda7e6656c494060e47fe27b20f2e\nStory: 2007442\nTask: 39102\n'}]",1,713512,c737bf831cde4599796f84daf770159155ea2c1a,19,5,4,26340,,,0,"Lazily load message registries

Postpone (potentially very large) Redfish message registries
download and processing up to the first access by the client.

The goal is to reduce the amount of unnecessary traffic and
CPU cycles.

Change-Id: Ief77d3ccce7fda7e6656c494060e47fe27b20f2e
Story: 2007442
Task: 39102
",git fetch https://review.opendev.org/openstack/sushy refs/changes/12/713512/1 && git format-patch -1 --stdout FETCH_HEAD,"['sushy/tests/unit/test_main.py', 'sushy/main.py', 'releasenotes/notes/lazily-load-registries-0e9441e435c2471d.yaml']",3,94e9bd922c40d2cfac665c4fac34baca50972951,02-lazy-registries,--- features: - | Postpones (potentially very large) Redfish message registries download and processing up to the first access by the client. The goal is to reduce the amount of unnecessary traffic and CPU cycles. ,,118,47
openstack%2Fansible-collections-openstack~master~I374bbdd2111073d4f6a9ace22bf11f58afe1c21b,openstack/ansible-collections-openstack,master,I374bbdd2111073d4f6a9ace22bf11f58afe1c21b,Add dependencies for long jobs on linters,MERGED,2020-03-20 10:42:50.000000000,2020-03-22 21:36:46.000000000,2020-03-22 21:36:46.000000000,"[{'_account_id': 2}, {'_account_id': 6816}, {'_account_id': 10239}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 10:42:50.000000000', 'files': ['.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/ansible-collections-openstack/commit/f982797b9463213c7942aa475937783395fd8328', 'message': 'Add dependencies for long jobs on linters\n\nRun long jobs only if linters passed\nChange-Id: I374bbdd2111073d4f6a9ace22bf11f58afe1c21b\n'}]",0,714070,f982797b9463213c7942aa475937783395fd8328,7,4,1,10969,,,0,"Add dependencies for long jobs on linters

Run long jobs only if linters passed
Change-Id: I374bbdd2111073d4f6a9ace22bf11f58afe1c21b
",git fetch https://review.opendev.org/openstack/ansible-collections-openstack refs/changes/70/714070/1 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.yaml'],1,f982797b9463213c7942aa475937783395fd8328,deps, - ansible-collections-openstack-functional-devstack: dependencies: &deps_unit_lint - tox-pep8 - openstack-tox-linters - ansible-collections-openstack-functional-devstack-releases: dependencies: *deps_unit_lint - ansible-collections-openstack-functional-devstack-ansible-devel: dependencies: *deps_unit_lint - ansible-collections-openstack-functional-devstack-ansible-2.9: dependencies: *deps_unit_lint - ansible-collections-openstack-functional-devstack-train-ansible-devel: dependencies: *deps_unit_lint - ansible-collections-openstack-functional-devstack-train-ansible-2.9: dependencies: *deps_unit_lint, - ansible-collections-openstack-functional-devstack - ansible-collections-openstack-functional-devstack-releases - ansible-collections-openstack-functional-devstack-ansible-devel - ansible-collections-openstack-functional-devstack-ansible-2.9 - ansible-collections-openstack-functional-devstack-train-ansible-devel - ansible-collections-openstack-functional-devstack-train-ansible-2.9,14,6
openstack%2Frally~master~Ib53e7413071c46ad1a82008658740f974201443a,openstack/rally,master,Ib53e7413071c46ad1a82008658740f974201443a,Remove install_rally script,MERGED,2020-03-22 20:51:25.000000000,2020-03-22 21:25:01.000000000,2020-03-22 21:25:01.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 20:51:25.000000000', 'files': ['install_rally.sh', 'CHANGELOG.rst'], 'web_link': 'https://opendev.org/openstack/rally/commit/9811aa9726c9a9befbe3acb6610c1c93c9924948', 'message': 'Remove install_rally script\n\nChange-Id: Ib53e7413071c46ad1a82008658740f974201443a\n'}]",0,714331,9811aa9726c9a9befbe3acb6610c1c93c9924948,6,2,1,9545,,,0,"Remove install_rally script

Change-Id: Ib53e7413071c46ad1a82008658740f974201443a
",git fetch https://review.opendev.org/openstack/rally refs/changes/31/714331/1 && git format-patch -1 --stdout FETCH_HEAD,"['install_rally.sh', 'CHANGELOG.rst']",2,9811aa9726c9a9befbe3acb6610c1c93c9924948,install,* *install_rally.sh* script is too complicated and installs only rally framework without plugins. ,,3,826
openstack%2Fwhitebox-tempest-plugin~master~I02744e366cfbc536c94f00ceb1ed19a45f9bc33e,openstack/whitebox-tempest-plugin,master,I02744e366cfbc536c94f00ceb1ed19a45f9bc33e,compute: Add negative volume detach test,MERGED,2019-11-21 20:07:25.000000000,2020-03-22 20:40:02.000000000,2020-03-22 20:40:02.000000000,"[{'_account_id': 8864}, {'_account_id': 10135}, {'_account_id': 11604}, {'_account_id': 22348}, {'_account_id': 27478}, {'_account_id': 31033}, {'_account_id': 31239}]","[{'number': 1, 'created': '2019-11-21 20:07:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/b0f5f2d43f110694f17003ca2a2f4c1e45cb250a', 'message': 'Add a test case that will Detach a volume after failed detach\n\nThis test will include the following methods like\nCreate a volume that can be attached to an instance\nBoot an instance specifying the volume to be attached\nStop libvirt on the compute node hosting the instance\nAttempt to detach the volume from the instance\nThe volume detach fails, the volume still shows up as in-use\nStart libvirt on the compute node hosting the instance\nAttempt to detach the volume from the instance and this time\nit needs to be success to validate the testcase\n\nNote: There still some work to be done to clean the code, just\nwant to have a review before and then I can modify accordingly.\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 2, 'created': '2019-12-12 19:10:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/2885066ba95eeac856e153a8352362558bf41d8c', 'message': '[WIP]Add a test case that will Detach a volume after failed detach\n\nThis test will include the following methods like\nCreate a volume that can be attached to an instance\nBoot an instance specifying the volume to be attached\nStop libvirt on the compute node hosting the instance\nAttempt to detach the volume from the instance\nThe volume detach fails, the volume still shows up as in-use\nStart libvirt on the compute node hosting the instance\nAttempt to detach the volume from the instance and this time\nit needs to be success to validate the testcase\n\nNote: There still some work to be done to clean the code, just\nwant to have a review before and then I can modify accordingly.\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 3, 'created': '2019-12-12 19:20:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/2b9211c1274287d4cc3f0d0c5c077f84b8c91687', 'message': '[WIP]Add a test case that will Detach a volume after failed detach\n\nThis test will include the following methods like\nCreate a volume that can be attached to an instance\nBoot an instance specifying the volume to be attached\nStop libvirt on the compute node hosting the instance\nAttempt to detach the volume from the instance\nThe volume detach fails, the volume still shows up as in-use\nStart libvirt on the compute node hosting the instance\nAttempt to detach the volume from the instance and this time\nit needs to be success to validate the testcase\n\nNote: There still some work to be done to clean the code, just\nwant to have a review before and then I can modify accordingly.\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 4, 'created': '2019-12-13 15:33:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/ef7e80ee29f0ff39903de948a1233fa246deb506', 'message': 'Add a test case that will attach/detach a volume after failed detach\n\nThis test will include the methods like create a volume\nthat can be attached to an instance, boot an instance\nspecifying the volume to be attached, stop libvirt on the\ncompute node hosting the instance, attempt to detach the\nvolume from the instance whcih will fails, the volume still\nshows up as in-use, start libvirt on the compute node hosting\nthe instance and now attempt to detach the volume from the instance\nand this time it needs to be success to validate the testcase\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 5, 'created': '2019-12-13 15:52:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/d0c0c130a2b5c159094a1684c29339624378419f', 'message': 'Add a test case that will attach/detach a volume after failed detach\n\nThis test will include the methods like create a volume\nthat can be attached to an instance, boot an instance\nspecifying the volume to be attached, stop libvirt on the\ncompute node hosting the instance, attempt to detach the\nvolume from the instance whcih will fails, the volume still\nshows up as in-use, start libvirt on the compute node hosting\nthe instance and now attempt to detach the volume from the instance\nand this time it needs to be success to validate the testcase\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 6, 'created': '2020-03-18 03:02:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/f1213553d2a9ba5ae87109c6149bb2475d79d177', 'message': 'Add a test case that will attach/detach a volume after failed detach\n\nThis test will include the methods like create a volume\nthat can be attached to an instance, boot an instance\nspecifying the volume to be attached, stop libvirt on the\ncompute node hosting the instance, attempt to detach the\nvolume from the instance whcih will fails, the volume still\nshows up as in-use, start libvirt on the compute node hosting\nthe instance and now attempt to detach the volume from the instance\nand this time it needs to be success to validate the testcase\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 7, 'created': '2020-03-18 13:38:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/008defd148b64a1d3688579a9be77a44392a837a', 'message': 'Add a test case that will attach/detach a volume after failed detach\n\nThis test will include the methods like create a volume\nthat can be attached to an instance, boot an instance\nspecifying the volume to be attached, stop libvirt on the\ncompute node hosting the instance, attempt to detach the\nvolume from the instance whcih will fails, the volume still\nshows up as in-use, start libvirt on the compute node hosting\nthe instance and now attempt to detach the volume from the instance\nand this time it needs to be success to validate the testcase\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 8, 'created': '2020-03-18 22:08:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/4f7abe9e9a29d5814ef26b5e53611131a22eb33f', 'message': 'Add a test case that will attach/detach a volume after failed detach\n\nThis test will include the methods like create a volume\nthat can be attached to an instance, boot an instance\nspecifying the volume to be attached, stop libvirt on the\ncompute node hosting the instance, attempt to detach the\nvolume from the instance whcih will fails, the volume still\nshows up as in-use, start libvirt on the compute node hosting\nthe instance and now attempt to detach the volume from the instance\nand this time it needs to be success to validate the testcase\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 9, 'created': '2020-03-20 05:21:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/1744d9b224bb5a095ff3b50527b924e3bdb00880', 'message': 'Add a test case that will attach/detach a volume after failed detach\n\nThis test will include the methods like create a volume\nthat can be attached to an instance, boot an instance\nspecifying the volume to be attached, stop libvirt on the\ncompute node hosting the instance, attempt to detach the\nvolume from the instance whcih will fails, the volume still\nshows up as in-use, start libvirt on the compute node hosting\nthe instance and now attempt to detach the volume from the instance\nand this time it needs to be success to validate the testcase\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 10, 'created': '2020-03-20 05:24:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/31cbf68ae958f91025895e27594704a91ab1d68f', 'message': 'Add a test case that will attach/detach a volume after failed detach\n\nThis test will include the methods like create a volume\nthat can be attached to an instance, boot an instance\nspecifying the volume to be attached, stop libvirt on the\ncompute node hosting the instance, attempt to detach the\nvolume from the instance whcih will fails, the volume still\nshows up as in-use, start libvirt on the compute node hosting\nthe instance and now attempt to detach the volume from the instance\nand this time it needs to be success to validate the testcase\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 11, 'created': '2020-03-20 09:56:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/fee20085f502a17c2d2c37b6cadfd139786730b2', 'message': 'compute: Add negative volume detach test\n\nThis negative volume detach test stops libvirtd on the compute to cause\na failure and asserts that the volume state remains in-use and that\nnumber of attached devices to the domain remains the same.\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 12, 'created': '2020-03-20 17:33:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/07d945b95be8744553714e025e848764f3ae729d', 'message': 'compute: Add negative volume detach test\n\nThis negative volume detach test stops libvirtd on the compute to cause\na failure and asserts that the volume state remains in-use and that\nnumber of attached devices to the domain remains the same.\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 13, 'created': '2020-03-20 17:35:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/9df6f3620b6472cdd40613a90cfa07748093b9e8', 'message': 'compute: Add negative volume detach test\n\nThis negative volume detach test stops libvirtd on the compute to cause\na failure and asserts that the volume state remains in-use and that\nnumber of attached devices to the domain remains the same.\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 14, 'created': '2020-03-20 17:39:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/49cb6793bb53deb154c071b86c5019a731cffec3', 'message': 'compute: Add negative volume detach test\n\nThis negative volume detach test stops libvirtd on the compute to cause\na failure and asserts that the volume state remains in-use and that\nnumber of attached devices to the domain remains the same.\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 15, 'created': '2020-03-21 03:40:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/49ae88f39439a8c0849114e9e5cd6fa51bb00dd5', 'message': 'compute: Add negative volume detach test\n\nThis negative volume detach test stops libvirtd on the compute to cause\na failure and asserts that the volume state remains in-use and that\nnumber of attached devices to the domain remains the same.\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}, {'number': 16, 'created': '2020-03-22 18:57:29.000000000', 'files': ['whitebox_tempest_plugin/services/clients.py', 'devstack/plugin.sh', 'whitebox_tempest_plugin/plugin.py', 'whitebox_tempest_plugin/config.py', 'whitebox_tempest_plugin/tests/test_clients.py', 'devstack/settings', 'whitebox_tempest_plugin/api/compute/test_volume_negative.py'], 'web_link': 'https://opendev.org/openstack/whitebox-tempest-plugin/commit/287cdd2d0e5d9b1b0c5b9c2694fa53a9f8708618', 'message': 'compute: Add negative volume detach test\n\nThis negative volume detach test stops libvirtd on the compute to cause\na failure and asserts that the volume state remains in-use and that\nnumber of attached devices to the domain remains the same.\n\nChange-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e\n'}]",41,695566,287cdd2d0e5d9b1b0c5b9c2694fa53a9f8708618,54,7,16,31239,,,0,"compute: Add negative volume detach test

This negative volume detach test stops libvirtd on the compute to cause
a failure and asserts that the volume state remains in-use and that
number of attached devices to the domain remains the same.

Change-Id: I02744e366cfbc536c94f00ceb1ed19a45f9bc33e
",git fetch https://review.opendev.org/openstack/whitebox-tempest-plugin refs/changes/66/695566/16 && git format-patch -1 --stdout FETCH_HEAD,"['whitebox_tempest_plugin/services/clients.py', 'whitebox_tempest_plugin/plugin.py', 'whitebox_tempest_plugin/config.py', 'whitebox_tempest_plugin/api/compute/test_volume_negative.py']",4,b0f5f2d43f110694f17003ca2a2f4c1e45cb250a,test_attach_detach_after_failed_Detach,"# Copyright 2016 NEC Corporation. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from tempest.lib import decorators from tempest.lib import exceptions as lib_exc from tempest.common.utils.linux import remote_client from tempest.common import waiters from tempest import config from whitebox_tempest_plugin.api.compute import base from whitebox_tempest_plugin.services import clients CONF = config.CONF class VolumesAdminNegativeTest(base.BaseWhiteboxComputeTest): @classmethod def skip_checks(cls): super(VolumesAdminNegativeTest, cls).skip_checks() if not CONF.service_available.cinder: skip_msg = (""%s skipped as Cinder is not available"" % cls.__name__) raise cls.skipException(skip_msg) @classmethod def setup_credentials(cls): cls.prepare_instance_network() super(VolumesAdminNegativeTest, cls).setup_credentials() def _create_server(self): # Start a server and wait for it to become ready validation_resources = self.get_test_validation_resources( self.os_primary) server = self.create_test_server( validatable=True, validation_resources=validation_resources, wait_until='ACTIVE', adminPass=self.image_ssh_password) self.addCleanup(self.delete_server, server['id']) #Record addresses so that we can ssh later server['addresses'] = self.servers_client.list_addresses( server['id'])['addresses'] return server, validation_resources def test_detach_volume_after_fail_detach(self): server, validation_resources = self._create_server() # NOTE: Create one remote client used throughout the test. if CONF.validation.run_validation: linux_client = remote_client.RemoteClient( self.get_server_ip(server, validation_resources), self.image_ssh_user, self.image_ssh_password, validation_resources['keypair']['private_key'], server=server, servers_client=self.servers_client) # NOTE: We need to ensure the ssh key has been # injected in the guest before we power cycle linux_client.validate_authentication() disks_list_before_attach = linux_client.list_disks() volume = self.create_volume() # NOTE: As of the 12.0.0 Liberty release, the Nova libvirt driver # no longer honors a user-supplied device name, and there can be # a mismatch between libvirt provide disk name and actual disk name # on instance, hence we no longer validate this test with the supplied # device name rather we count number of disk before attach # detach to validate the testcase. attachment = self.attach_volume(server, volume) if CONF.validation.run_validation: disks_list_after_attach = linux_client.list_disks() self.assertGreater( len(disks_list_after_attach), len(disks_list_before_attach)) hypervisor = self.get_hypervisor_ip(server['id']) print(hypervisor) clients.ServiceManager(hypervisor, 'nova-libvirt').stop() self.servers_client.detach_volume(server['id'], attachment['volumeId']) if CONF.validation.run_validation: disks_list_after_attach = linux_client.list_disks() self.assertGreater( len(disks_list_after_attach), len(disks_list_before_attach)) clients.ServiceManager(hypervisor, 'nova-libvirt').restart() self.servers_client.detach_volume(server['id'], attachment['volumeId']) waiters.wait_for_volume_resource_status( self.volumes_client, attachment['volumeId'], 'available') if CONF.validation.run_validation: disks_list_after_detach = linux_client.list_disks() self.assertEqual(disks_list_before_attach, disks_list_after_detach) ",,139,0
openstack%2Frally~master~I5ce42221610c8929d8ada5a6f6265997b3be2a99,openstack/rally,master,I5ce42221610c8929d8ada5a6f6265997b3be2a99,Add docker image push job to release pipeline,MERGED,2020-03-22 19:35:35.000000000,2020-03-22 20:33:09.000000000,2020-03-22 20:33:09.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 19:35:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a34e2b4320156cafd1ab44fdf73cb7327ce5d17e', 'message': 'WIP: Check docker build job parameters\n\nChange-Id: I5ce42221610c8929d8ada5a6f6265997b3be2a99\n'}, {'number': 2, 'created': '2020-03-22 19:49:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/442eca6a8bdbac56d40b2c2131477b8a9eecc6e9', 'message': 'Add docker image push job to release pipeline\n\nChange-Id: I5ce42221610c8929d8ada5a6f6265997b3be2a99\n'}, {'number': 3, 'created': '2020-03-22 19:57:20.000000000', 'files': ['.zuul.d/zuul.yaml', 'tests/ci/playbooks/roles/build_docker_image/defaults/main.yaml'], 'web_link': 'https://opendev.org/openstack/rally/commit/57a7b45a06e61e247a0179350f14e2647d0c29e2', 'message': 'Add docker image push job to release pipeline\n\nChange-Id: I5ce42221610c8929d8ada5a6f6265997b3be2a99\n'}]",0,714329,57a7b45a06e61e247a0179350f14e2647d0c29e2,9,2,3,9545,,,0,"Add docker image push job to release pipeline

Change-Id: I5ce42221610c8929d8ada5a6f6265997b3be2a99
",git fetch https://review.opendev.org/openstack/rally refs/changes/29/714329/3 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.d/zuul.yaml'],1,a34e2b4320156cafd1ab44fdf73cb7327ce5d17e,docker_push, - rally-docker-build: vars: docker_image_version: self, - rally-tox-docs - rally-tox-pep8 - rally-tox-py36 - rally-tox-py37 - rally-tox-py38 - rally-tox-cover - rally-tox-functional - rally-tox-functional-py38 - rally-tox-self - rally-database-migration - rally-install-ubuntu-bionic - rally-install-centos-7 - rally-install-centos-8 - rally-docker-build,3,14
openstack%2Frally~master~I9a4b35643907ce0f101607314aa70222a883368c,openstack/rally,master,I9a4b35643907ce0f101607314aa70222a883368c,[ci] Remove redundant rally task job files,MERGED,2020-03-22 19:17:55.000000000,2020-03-22 19:56:59.000000000,2020-03-22 19:56:59.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 19:17:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/efd2655f60efdddf4b3cdba9175d0e1ca65e61a7', 'message': '[ci] Remove redundant rally task job files\n\nThese files left after migration of openstack plugins to the separate\nrepo\n\nChange-Id: I9a4b35643907ce0f101607314aa70222a883368c\n'}, {'number': 2, 'created': '2020-03-22 19:24:39.000000000', 'files': ['rally-jobs/extra/instance_test.sh', 'rally-jobs/rally-watcher.yaml', 'rally-jobs/extra/README.rst', 'rally-jobs/rally_args.yaml', 'tests/unit/plugins/common/test_validators.py', 'rally-jobs/extra/install_benchmark.sh', 'rally-jobs/rally-neutron-existing-users.yaml', 'rally-jobs/nova.yaml', 'rally-jobs/README.rst', 'rally-jobs/rally-neutron.yaml', 'rally-jobs/extra/hook_example_script.sh', 'rally-jobs/rally.yaml'], 'web_link': 'https://opendev.org/openstack/rally/commit/8dbfa86d02cb35e14d597236995926286190fd76', 'message': '[ci] Remove redundant rally task job files\n\nThese files left after migration of openstack plugins to the separate\nrepo\n\nAlso, this patch fixes unit test for FileExistsValidator validator which\nuses rally-jobs dir for no reasons.\n\nChange-Id: I9a4b35643907ce0f101607314aa70222a883368c\n'}]",0,714328,8dbfa86d02cb35e14d597236995926286190fd76,7,2,2,9545,,,0,"[ci] Remove redundant rally task job files

These files left after migration of openstack plugins to the separate
repo

Also, this patch fixes unit test for FileExistsValidator validator which
uses rally-jobs dir for no reasons.

Change-Id: I9a4b35643907ce0f101607314aa70222a883368c
",git fetch https://review.opendev.org/openstack/rally refs/changes/28/714328/1 && git format-patch -1 --stdout FETCH_HEAD,"['rally-jobs/extra/instance_test.sh', 'rally-jobs/rally-watcher.yaml', 'rally-jobs/rally_args.yaml', 'rally-jobs/extra/README.rst', 'rally-jobs/extra/install_benchmark.sh', 'rally-jobs/rally-neutron-existing-users.yaml', 'rally-jobs/nova.yaml', 'rally-jobs/README.rst', 'rally-jobs/rally-neutron.yaml', 'rally-jobs/extra/hook_example_script.sh', 'rally-jobs/rally.yaml']",11,efd2655f60efdddf4b3cdba9175d0e1ca65e61a7,co_migration,,"{%- set cirros_image_url = ""http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img"" %} --- KeystoneBasic.create_user: - args: {} runner: type: ""constant"" times: 10 concurrency: 10 sla: failure_rate: max: 0 KeystoneBasic.create_delete_user: - args: {} runner: type: ""constant"" times: 10 concurrency: 10 sla: failure_rate: max: 0 KeystoneBasic.create_user_set_enabled_and_delete: - args: enabled: true runner: type: ""constant"" times: 10 concurrency: 10 sla: failure_rate: max: 0 - args: enabled: false runner: type: ""constant"" times: 10 concurrency: 10 sla: failure_rate: max: 0 KeystoneBasic.create_and_list_tenants: - args: {} runner: type: ""constant"" times: 10 concurrency: 10 sla: failure_rate: max: 0 KeystoneBasic.get_entities: - runner: type: ""constant"" times: 20 concurrency: 10 sla: failure_rate: max: 0 - args: service_name: null runner: type: ""constant"" times: 20 concurrency: 10 sla: failure_rate: max: 0 - args: service_name: ""nova"" runner: type: ""constant"" times: 20 concurrency: 10 sla: failure_rate: max: 0 KeystoneBasic.add_and_remove_user_role: - runner: type: ""constant"" times: 10 concurrency: 5 context: users: tenants: 1 users_per_tenant: 1 sla: failure_rate: max: 0 KeystoneBasic.create_and_delete_role: - runner: type: ""constant"" times: 10 concurrency: 5 sla: failure_rate: max: 0 KeystoneBasic.create_and_get_role: - args: {} runner: type: ""constant"" times: 10 concurrency: 5 context: users: tenants: 2 users_per_tenant: 2 sla: failure_rate: max: 0 KeystoneBasic.create_add_and_list_user_roles: - runner: type: ""constant"" times: 10 concurrency: 5 context: users: tenants: 1 users_per_tenant: 1 sla: failure_rate: max: 0 KeystoneBasic.create_and_list_roles: - runner: type: ""constant"" times: 10 concurrency: 2 context: users: tenants: 3 users_per_tenant: 2 sla: failure_rate: max: 0 KeystoneBasic.create_and_list_users: - args: {} runner: type: ""constant"" times: 10 concurrency: 10 sla: failure_rate: max: 0 KeystoneBasic.create_tenant: - args: {} runner: type: ""constant"" times: 10 concurrency: 10 sla: failure_rate: max: 0 KeystoneBasic.create_tenant_with_users: - args: users_per_tenant: 10 runner: type: ""constant"" times: 10 concurrency: 10 context: users: tenants: 3 sla: failure_rate: max: 0 KeystoneBasic.create_user_update_password: - args: {} runner: type: ""constant"" times: 10 concurrency: 5 sla: failure_rate: max: 0 KeystoneBasic.create_and_update_user: - args: create_user_kwargs: {} update_user_kwargs: enabled: False runner: type: ""constant"" times: 10 concurrency: 2 context: users: tenants: 2 users_per_tenant: 2 sla: failure_rate: max: 0 KeystoneBasic.create_update_and_delete_tenant: - args: {} runner: type: ""constant"" times: 10 concurrency: 5 sla: failure_rate: max: 0 KeystoneBasic.create_and_delete_service: - runner: type: ""constant"" times: 10 concurrency: 5 sla: failure_rate: max: 0 KeystoneBasic.create_and_list_services: - runner: type: ""constant"" times: 10 concurrency: 5 sla: failure_rate: max: 0 KeystoneBasic.create_and_list_ec2credentials: - runner: type: ""constant"" times: 10 concurrency: 5 context: users: tenants: 2 users_per_tenant: 2 sla: failure_rate: max: 0 KeystoneBasic.create_and_delete_ec2credential: - runner: type: ""constant"" times: 10 concurrency: 5 context: users: tenants: 2 users_per_tenant: 2 sla: failure_rate: max: 0 Dummy.openstack: - args: sleep: 0.01 runner: type: ""constant"" times: 1 concurrency: 1 context: users: tenants: 8 users_per_tenant: 4 sla: failure_rate: max: 0 - args: sleep: 0.6 runner: type: ""constant"" concurrency: 2 times: 4 timeout: 1 context: users: tenants: 1 users_per_tenant: 1 sla: failure_rate: max: 0 - args: sleep: 0.6 runner: type: ""rps"" rps: 2 times: 5 timeout: 1 context: users: tenants: 1 users_per_tenant: 1 sla: failure_rate: max: 0 - description: ""Check 'quotas' context."" args: sleep: 0.01 runner: type: ""constant"" times: 1 concurrency: 1 context: quotas: nova: instances: 200 cores: 200 ram: -1 floating_ips: 200 fixed_ips: 200 metadata_items: -1 injected_files: -1 injected_file_content_bytes: -1 injected_file_path_bytes: -1 key_pairs: 500 security_groups: 400 security_group_rules: 600 cinder: gigabytes: -1 snapshots: -1 volumes: -1 sla: failure_rate: max: 0 Authenticate.keystone: - runner: type: ""constant"" times: 40 concurrency: 20 context: users: tenants: 2 users_per_tenant: 10 sla: failure_rate: max: 0 Authenticate.validate_glance: - args: repetitions: 2 runner: type: ""constant"" times: 10 concurrency: 5 context: users: tenants: 3 users_per_tenant: 5 sla: failure_rate: max: 0 HttpRequests.check_request: - args: url: ""http://www.example.com"" method: ""GET"" status_code: 200 runner: type: ""constant"" times: 2 concurrency: 2 sla: failure_rate: max: 0 HttpRequests.check_random_request: - args: requests: - url: ""http://www.example.com"" method: ""GET"" - url: ""http://localhost"" method: ""GET"" status_code: 200 runner: type: ""constant"" times: 2 concurrency: 2 sla: failure_rate: max: 0 GlanceImages.list_images: - runner: type: ""constant"" times: 5 concurrency: 5 context: users: tenants: 1 users_per_tenant: 2 images: image_url: ""{{ cirros_image_url }}"" disk_format: ""qcow2"" container_format: ""bare"" images_per_tenant: 1 sla: failure_rate: max: 100 - runner: type: ""constant"" times: 5 concurrency: 5 context: users: tenants: 2 users_per_tenant: 1 images: image_url: ""~/.rally/extra/fake-image.img"" disk_format: ""qcow2"" container_format: ""bare"" images_per_tenant: 2 sla: failure_rate: max: 100 GlanceImages.create_and_get_image: - args: image_location: ""{{ cirros_image_url }}"" container_format: ""bare"" disk_format: ""qcow2"" runner: type: ""constant"" times: 4 concurrency: 2 context: users: tenants: 2 users_per_tenant: 2 api_versions: glance: version: 2 sla: failure_rate: max: 100 GlanceImages.create_and_download_image: - args: image_location: ""{{ cirros_image_url }}"" container_format: ""bare"" disk_format: ""qcow2"" runner: type: ""constant"" times: 4 concurrency: 2 context: users: tenants: 2 users_per_tenant: 2 sla: failure_rate: max: 0 GlanceImages.create_and_delete_image: # - # args: # image_location: ""{{ cirros_image_url }}"" # container_format: ""bare"" # disk_format: ""qcow2"" # runner: # type: ""constant"" # times: 1 # concurrency: 1 # context: # users: # tenants: 2 # users_per_tenant: 3 # api_versions: # glance: # version: 1 # sla: # failure_rate: # max: 0 # - args: image_location: ""{{ cirros_image_url }}"" container_format: ""bare"" disk_format: ""qcow2"" runner: type: ""constant"" times: 1 concurrency: 1 context: users: tenants: 2 users_per_tenant: 3 api_versions: glance: version: 2 sla: failure_rate: max: 100 # # - # args: # image_location: ""{{ cirros_image_url }}"" # container_format: ""bare"" # disk_format: ""qcow2"" # runner: # type: ""constant"" # times: 1 # concurrency: 1 # context: # users: # tenants: 1 # users_per_tenant: 1 # api_versions: # glance: # version: 1 # roles: # - admin # sla: # failure_rate: # max: 0 GlanceImages.create_and_list_image: # - # args: # image_location: ""~/.rally/extra/fake-image.img"" # container_format: ""bare"" # disk_format: ""qcow2"" # runner: # type: ""constant"" # times: 1 # concurrency: 1 # context: # users: # tenants: 1 # users_per_tenant: 1 # api_versions: # glance: # version: 1 # sla: # failure_rate: # max: 0 # - args: image_location: ""~/.rally/extra/fake-image.img"" container_format: ""bare"" disk_format: ""qcow2"" runner: type: ""constant"" times: 1 concurrency: 1 context: users: tenants: 1 users_per_tenant: 1 api_versions: glance: version: 2 sla: failure_rate: max: 100 GlanceImages.create_image_and_boot_instances: - args: image_location: ""{{ cirros_image_url }}"" container_format: ""bare"" disk_format: ""qcow2"" flavor: name: ""m1.tiny"" number_instances: 2 create_image_kwargs: properties: hw_video_model: vga runner: type: ""constant"" times: 1 concurrency: 1 context: users: tenants: 3 users_per_tenant: 1 sla: failure_rate: max: 100 GlanceImages.create_and_update_image: - args: image_location: ""{{ cirros_image_url }}"" container_format: ""bare"" disk_format: ""qcow2"" runner: type: ""constant"" times: 4 concurrency: 2 context: users: tenants: 2 users_per_tenant: 2 sla: failure_rate: max: 100 GlanceImages.create_and_deactivate_image: - args: image_location: ""{{ cirros_image_url }}"" container_format: ""bare"" disk_format: ""qcow2"" runner: type: ""constant"" times: 4 concurrency: 2 context: users: tenants: 2 users_per_tenant: 2 sla: failure_rate: max: 100 SwiftObjects.create_container_and_object_then_list_objects: - args: objects_per_container: 2 object_size: 5120 runner: type: ""constant"" times: 2 concurrency: 2 context: users: tenants: 1 users_per_tenant: 1 roles: - ""admin"" sla: failure_rate: max: 0 SwiftObjects.create_container_and_object_then_delete_all: - args: objects_per_container: 5 object_size: 102400 runner: type: ""constant"" times: 4 concurrency: 2 context: users: tenants: 1 users_per_tenant: 1 roles: - ""admin"" sla: failure_rate: max: 0 SwiftObjects.create_container_and_object_then_download_object: - args: objects_per_container: 5 object_size: 1024 runner: type: ""constant"" times: 6 concurrency: 3 context: users: tenants: 1 users_per_tenant: 1 roles: - ""admin"" sla: failure_rate: max: 0 SwiftObjects.list_and_download_objects_in_containers: - runner: type: ""constant"" times: 2 concurrency: 2 context: users: tenants: 1 users_per_tenant: 1 roles: - ""admin"" swift_objects: containers_per_tenant: 1 objects_per_container: 5 object_size: 10240 sla: failure_rate: max: 0 SwiftObjects.list_objects_in_containers: - runner: type: ""constant"" times: 6 concurrency: 3 context: users: tenants: 1 users_per_tenant: 1 roles: - ""admin"" swift_objects: containers_per_tenant: 1 objects_per_container: 10 object_size: 1024 sla: failure_rate: max: 0 ",0,3181
openstack%2Frally~master~I97e7e2bd6fd79f1209ca027349d0489729550500,openstack/rally,master,I97e7e2bd6fd79f1209ca027349d0489729550500,Move task results loader helper to a separate module,MERGED,2020-03-22 18:19:17.000000000,2020-03-22 18:57:35.000000000,2020-03-22 18:57:35.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 18:19:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d7d6510bb5d58a509996365d3d4de63842039a46', 'message': 'Move task results loader helper to a separate module\n\nChange-Id: I97e7e2bd6fd79f1209ca027349d0489729550500\n'}, {'number': 2, 'created': '2020-03-22 18:27:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/210d4cdfc84d14e04a427bdb49617fb1c2726f8f', 'message': 'Move task results loader helper to a separate module\n\nChange-Id: I97e7e2bd6fd79f1209ca027349d0489729550500\n'}, {'number': 3, 'created': '2020-03-22 18:28:08.000000000', 'files': ['tests/ci/cover.sh', 'rally/cli/commands/task.py', 'tests/unit/cli/commands/test_task.py', 'tests/unit/cli/test_task_results_loader.py', 'tests/unit/task/test_utils.py', 'rally/cli/task_results_loader.py', 'rally/task/utils.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/6611305108390f6ca0de2dc634e8b572cb2d0583', 'message': 'Move task results loader helper to a separate module\n\nChange-Id: I97e7e2bd6fd79f1209ca027349d0489729550500\n'}]",0,714324,6611305108390f6ca0de2dc634e8b572cb2d0583,8,2,3,9545,,,0,"Move task results loader helper to a separate module

Change-Id: I97e7e2bd6fd79f1209ca027349d0489729550500
",git fetch https://review.opendev.org/openstack/rally refs/changes/24/714324/3 && git format-patch -1 --stdout FETCH_HEAD,"['rally/cli/commands/task.py', 'tests/unit/cli/commands/test_task.py', 'tests/unit/cli/test_task_results_loader.py', 'tests/unit/task/test_utils.py', 'rally/cli/task_results_loader.py', 'rally/task/utils.py']",6,d7d6510bb5d58a509996365d3d4de63842039a46,deprecated,,"import collections # TODO(andreykurilin): We need to implement some wrapper for atomic actions, # we can use these wrapper to simulate new and old format. class WrapperForAtomicActions(list): def __init__(self, atomic_actions, timestamp=0): self.timestamp = timestamp if isinstance(atomic_actions, list): self.__atomic_actions = atomic_actions self.__old_atomic_actions = self._convert_new_atomic_actions( self.__atomic_actions) else: self.__atomic_actions = self._convert_old_atomic_actions( atomic_actions) self.__old_atomic_actions = atomic_actions super(WrapperForAtomicActions, self).__init__(self.__atomic_actions) def _convert_old_atomic_actions(self, old_atomic_actions): """"""Convert atomic actions to new format. """""" atomic_actions = [] started_at = self.timestamp for name, duration in old_atomic_actions.items(): finished_at = started_at + duration atomic_actions.append({""name"": name, ""started_at"": started_at, ""finished_at"": finished_at, ""children"": []}) started_at = finished_at return atomic_actions def _convert_new_atomic_actions(self, atomic_actions): """"""Convert atomic actions to old format. """""" old_style = collections.OrderedDict() for action in atomic_actions: duration = action[""finished_at""] - action[""started_at""] if action[""name""] in old_style: name_template = action[""name""] + "" (%i)"" i = 2 while name_template % i in old_style: i += 1 old_style[name_template % i] = duration else: old_style[action[""name""]] = duration return old_style def items(self): return self.__old_atomic_actions.items() def get(self, name, default=None): return self.__old_atomic_actions.get(name, default) def __iter__(self): return iter(self.__atomic_actions) def __len__(self): return len(self.__atomic_actions) def __getitem__(self, item): if isinstance(item, int): # it is a call to list: return self.__atomic_actions[item] else: return self.__old_atomic_actions[item]",509,514
openstack%2Frally~master~I4c4fe1d4dcf96435729ab13f545c65d72231637e,openstack/rally,master,I4c4fe1d4dcf96435729ab13f545c65d72231637e,Get back yamlutils and dummy scenarios,MERGED,2020-03-22 18:26:16.000000000,2020-03-22 18:55:00.000000000,2020-03-22 18:55:00.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 18:26:16.000000000', 'files': ['tests/ci/cover.sh', 'rally/plugins/common/scenarios/dummy/dummy.py', 'rally/common/yamlutils.py', 'rally/plugins/common/scenarios/dummy/__init__.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/64c133a5b6cc85fdfe48deacf6ea95391e6ae48c', 'message': 'Get back yamlutils and dummy scenarios\n\nrally-openstack uses these modules, so we need to move them in a backward\ncompatible way.\n\nChange-Id: I4c4fe1d4dcf96435729ab13f545c65d72231637e\n'}]",0,714325,64c133a5b6cc85fdfe48deacf6ea95391e6ae48c,6,2,1,9545,,,0,"Get back yamlutils and dummy scenarios

rally-openstack uses these modules, so we need to move them in a backward
compatible way.

Change-Id: I4c4fe1d4dcf96435729ab13f545c65d72231637e
",git fetch https://review.opendev.org/openstack/rally refs/changes/25/714325/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/ci/cover.sh', 'rally/plugins/common/scenarios/dummy/dummy.py', 'rally/common/yamlutils.py', 'rally/plugins/common/scenarios/dummy/__init__.py']",4,64c133a5b6cc85fdfe48deacf6ea95391e6ae48c,fix_it,,,49,1
openstack%2Frally-openstack~master~I76c2d089b5a9d947a910a0410c4784c5a0cdddb3,openstack/rally-openstack,master,I76c2d089b5a9d947a910a0410c4784c5a0cdddb3,Update magnum/utils.py to match Kubernetes changes,MERGED,2020-03-18 19:06:35.000000000,2020-03-22 18:20:51.000000000,2020-03-22 18:20:50.000000000,"[{'_account_id': 9545}, {'_account_id': 11692}, {'_account_id': 14749}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-18 19:06:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally-openstack/commit/6bb0cbc42b0c7decb54f20d73f0bb9a9fd6a5716', 'message': 'Update magnum/utils.py to match Kubernetes API changes\n\nTempest verification as proposed by Rally fails vs Kubernetes 11.0.0 [1].\nIt should be noted that Kubernetes was updated to 11.0.0 in Openstack\nmaster upper-constraints [2].\n\n[1] https://build.opnfv.org/ci/job/functest-opnfv-functest-healthcheck-latest-tempest_smoke-run/749/console\n[2] https://github.com/openstack/requirements/commit/76a83f23a6ff7e7f50be791815966d4512623c7b\n\nChange-Id: I76c2d089b5a9d947a910a0410c4784c5a0cdddb3\nSigned-off-by: Cédric Ollivier <ollivier.cedric@gmail.com>\n'}, {'number': 2, 'created': '2020-03-18 19:09:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally-openstack/commit/909c21f334d8e90248a1f5ddcd3938a6de6a27d8', 'message': 'Update magnum/utils.py to match Kubernetes changes\n\nTempest verification as proposed by Rally fails vs Kubernetes 11.0.0 [1].\nIt should be noted that Kubernetes was updated to 11.0.0 in Openstack\nmaster upper-constraints [2].\n\n[1] https://build.opnfv.org/ci/job/functest-opnfv-functest-healthcheck-latest-tempest_smoke-run/749/console\n[2] https://github.com/openstack/requirements/commit/76a83f23a6ff7e7f50be791815966d4512623c7b\n\nChange-Id: I76c2d089b5a9d947a910a0410c4784c5a0cdddb3\nSigned-off-by: Cédric Ollivier <ollivier.cedric@gmail.com>\n'}, {'number': 3, 'created': '2020-03-19 12:31:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally-openstack/commit/a5520caee1882217ad4dfb131115315b547fc961', 'message': 'Update magnum/utils.py to match Kubernetes changes\n\nTempest verification as proposed by Rally fails vs Kubernetes 11.0.0 [1].\nIt should be noted that Kubernetes was updated to 11.0.0 in Openstack\nmaster upper-constraints [2].\n\nIt sets kubernetes>=11.0.0 as abstract dependency [3][4].\n\n[1] https://build.opnfv.org/ci/job/functest-opnfv-functest-healthcheck-latest-tempest_smoke-run/749/console\n[2] https://github.com/openstack/requirements/commit/76a83f23a6ff7e7f50be791815966d4512623c7b\n[3] https://github.com/openstack/requirements/blob/master/global-requirements.txt#L115\n[4] https://github.com/kubernetes-client/python/commit/5cddbde0bfa2c8e10488978b0294f69c75fe0889\n\nChange-Id: I76c2d089b5a9d947a910a0410c4784c5a0cdddb3\nSigned-off-by: Cédric Ollivier <ollivier.cedric@gmail.com>\n'}, {'number': 4, 'created': '2020-03-22 11:16:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally-openstack/commit/a7ed03a0fbeea951a36e26a4426a6bb706c451a4', 'message': 'Update magnum/utils.py to match Kubernetes changes\n\nTempest verification as proposed by Rally fails vs Kubernetes 11.0.0 [1].\nIt should be noted that Kubernetes was updated to 11.0.0 in Openstack\nmaster upper-constraints [2].\n\nIt sets kubernetes>=11.0.0 as abstract dependency [3][4].\n\nIt replace api_client.models by kubernetes_client.models in magnum\ntest_utils.py (kubernetes_client could rather have been selected).\n\n[1] https://build.opnfv.org/ci/job/functest-opnfv-functest-healthcheck-latest-tempest_smoke-run/749/console\n[2] https://github.com/openstack/requirements/commit/76a83f23a6ff7e7f50be791815966d4512623c7b\n[3] https://github.com/openstack/requirements/blob/master/global-requirements.txt#L115\n[4] https://github.com/kubernetes-client/python/commit/5cddbde0bfa2c8e10488978b0294f69c75fe0889\n\nChange-Id: I76c2d089b5a9d947a910a0410c4784c5a0cdddb3\nSigned-off-by: Cédric Ollivier <ollivier.cedric@gmail.com>\n'}, {'number': 5, 'created': '2020-03-22 11:36:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally-openstack/commit/3a15b37d678c7e5ee0c80cde050a541f084094d8', 'message': 'Update magnum/utils.py to match Kubernetes changes\n\nTempest verification as proposed by Rally fails vs Kubernetes 11.0.0 [1].\nIt should be noted that Kubernetes was updated to 11.0.0 in Openstack\nmaster upper-constraints [2].\n\nIt sets kubernetes>=11.0.0 as abstract dependency [3][4].\n\nIt replace api_client.models by kubernetes_client.models in magnum\ntest_utils.py (kubernetes_client could rather have been selected).\n\n[1] https://build.opnfv.org/ci/job/functest-opnfv-functest-healthcheck-latest-tempest_smoke-run/749/console\n[2] https://github.com/openstack/requirements/commit/76a83f23a6ff7e7f50be791815966d4512623c7b\n[3] https://github.com/openstack/requirements/blob/master/global-requirements.txt#L115\n[4] https://github.com/kubernetes-client/python/commit/5cddbde0bfa2c8e10488978b0294f69c75fe0889\n\nChange-Id: I76c2d089b5a9d947a910a0410c4784c5a0cdddb3\nSigned-off-by: Cédric Ollivier <ollivier.cedric@gmail.com>\n'}, {'number': 6, 'created': '2020-03-22 11:46:54.000000000', 'files': ['requirements.txt', 'rally_openstack/scenarios/magnum/utils.py', 'tests/unit/scenarios/magnum/test_utils.py', 'upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/rally-openstack/commit/5776e015f16321fdb489ebb482d984ccd3cd2903', 'message': 'Update magnum/utils.py to match Kubernetes changes\n\nTempest verification as proposed by Rally fails vs Kubernetes 11.0.0 [1].\nIt should be noted that Kubernetes was updated to 11.0.0 in Openstack\nmaster upper-constraints [2].\n\nIt sets kubernetes>=11.0.0 as abstract dependency [3][4].\n\nIt replace api_client.models by kubernetes_client.models in magnum\ntest_utils.py (kubernetes_client could rather have been selected).\n\n[1] https://build.opnfv.org/ci/job/functest-opnfv-functest-healthcheck-latest-tempest_smoke-run/749/console\n[2] https://github.com/openstack/requirements/commit/76a83f23a6ff7e7f50be791815966d4512623c7b\n[3] https://github.com/openstack/requirements/blob/master/global-requirements.txt#L115\n[4] https://github.com/kubernetes-client/python/commit/5cddbde0bfa2c8e10488978b0294f69c75fe0889\n\nChange-Id: I76c2d089b5a9d947a910a0410c4784c5a0cdddb3\nSigned-off-by: Cédric Ollivier <ollivier.cedric@gmail.com>\n'}]",2,713718,5776e015f16321fdb489ebb482d984ccd3cd2903,17,4,6,11692,,,0,"Update magnum/utils.py to match Kubernetes changes

Tempest verification as proposed by Rally fails vs Kubernetes 11.0.0 [1].
It should be noted that Kubernetes was updated to 11.0.0 in Openstack
master upper-constraints [2].

It sets kubernetes>=11.0.0 as abstract dependency [3][4].

It replace api_client.models by kubernetes_client.models in magnum
test_utils.py (kubernetes_client could rather have been selected).

[1] https://build.opnfv.org/ci/job/functest-opnfv-functest-healthcheck-latest-tempest_smoke-run/749/console
[2] https://github.com/openstack/requirements/commit/76a83f23a6ff7e7f50be791815966d4512623c7b
[3] https://github.com/openstack/requirements/blob/master/global-requirements.txt#L115
[4] https://github.com/kubernetes-client/python/commit/5cddbde0bfa2c8e10488978b0294f69c75fe0889

Change-Id: I76c2d089b5a9d947a910a0410c4784c5a0cdddb3
Signed-off-by: Cédric Ollivier <ollivier.cedric@gmail.com>
",git fetch https://review.opendev.org/openstack/rally-openstack refs/changes/18/713718/3 && git format-patch -1 --stdout FETCH_HEAD,['rally_openstack/scenarios/magnum/utils.py'],1,6bb0cbc42b0c7decb54f20d73f0bb9a9fd6a5716,bug/1843282,from kubernetes.client.api import core_v1_api,from kubernetes.client.apis import core_v1_api,1,1
openstack%2Fnetworking-ovn~stable%2Ftrain~Ibbe7371b10efc87a42177d2a1a10fa576899b613,openstack/networking-ovn,stable/train,Ibbe7371b10efc87a42177d2a1a10fa576899b613,Switch release jobs to OVN 20.03 (v20.03.0 tag),MERGED,2020-03-02 11:11:47.000000000,2020-03-22 17:20:08.000000000,2020-03-22 17:17:48.000000000,"[{'_account_id': 6773}, {'_account_id': 8655}, {'_account_id': 11952}, {'_account_id': 11975}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 23804}]","[{'number': 1, 'created': '2020-03-02 11:11:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/56c33c070b6e1f473760f01d60e853d91a8a7d5f', 'message': ""Switch release jobs to OVN 20.03\n\nOVN 20.03 is the new stable released version of OVN.\n\nThe function use_new_ovn_repository needed to be updated because it\nwasn't account for a major release version bump (from 2 to 20).\n\nChange-Id: Ibbe7371b10efc87a42177d2a1a10fa576899b613\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n""}, {'number': 2, 'created': '2020-03-02 15:48:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/8ab6969e7869d6285be3d7b21a9a01e2c8f320c7', 'message': ""Switch release jobs to OVN 20.03\n\nOVN 20.03 is the new stable released version of OVN.\n\nThe function use_new_ovn_repository needed to be updated because it\nwasn't account for a major release version bump (from 2 to 20).\n\nChange-Id: Ibbe7371b10efc87a42177d2a1a10fa576899b613\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n""}, {'number': 3, 'created': '2020-03-02 15:50:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/7bbce37ca9694ba19d4e6bace9733419fce3ed32', 'message': ""Switch release jobs to OVN 20.03\n\nOVN 20.03 is the new stable released version of OVN.\n\nThe function use_new_ovn_repository needed to be updated because it\nwasn't account for a major release version bump (from 2 to 20).\n\nChange-Id: Ibbe7371b10efc87a42177d2a1a10fa576899b613\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n""}, {'number': 4, 'created': '2020-03-04 13:27:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/f71c83ab15e77be5eb73c5f36488dd75296d5695', 'message': '[OVN] Switch release jobs to OVN 20.03 (v20.03 tag)\n\nOVN 20.03 (v20.03 tag) is the new stable released version of OVN which\nis compatible with the OVS 2.13 version, also bumped in this review.\n\nThe patch also updates the use_new_ovn_repository to strip all\nnon-numeric characters of the branch name when comparing the versions,\nthat way we can compare branch-<version> against v<version> version\nformats.\n\nChange-Id: Ibbe7371b10efc87a42177d2a1a10fa576899b613\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n'}, {'number': 5, 'created': '2020-03-05 09:21:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/d0734e7e792eb497709d498fdf8b2e82d54c0aea', 'message': 'Switch release jobs to OVN 20.03 (v20.03 tag)\n\nOVN 20.03 (v20.03 tag) is the new stable released version of OVN which\nis compatible with the OVS 2.13 version, also bumped in this review.\n\nThe patch also updates the use_new_ovn_repository to strip all\nnon-numeric characters of the branch name when comparing the versions,\nthat way we can compare branch-<version> against v<version> version\nformats.\n\nChange-Id: Ibbe7371b10efc87a42177d2a1a10fa576899b613\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n'}, {'number': 6, 'created': '2020-03-05 14:58:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/58572d16502503f66c605100bb92e678d67c2912', 'message': 'Switch release jobs to OVN 20.03 (v20.03.0 tag)\n\nOVN 20.03 (v20.03.0 tag) is the new stable released version of OVN which\nis compatible with the OVS 2.13 version, also bumped in this review.\n\nThe patch also updates the use_new_ovn_repository to strip all\nnon-numeric characters of the branch name when comparing the versions,\nthat way we can compare branch-<version> against v<version> version\nformats.\n\nChange-Id: Ibbe7371b10efc87a42177d2a1a10fa576899b613\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n'}, {'number': 7, 'created': '2020-03-11 14:40:06.000000000', 'files': ['devstack/devstackgaterc', 'devstack/lib/ovn', 'zuul.d/networking-ovn-jobs.yaml'], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/2c81baa78212faa920dc4e19564a0aa306d48365', 'message': 'Switch release jobs to OVN 20.03 (v20.03.0 tag)\n\nOVN 20.03 (v20.03.0 tag) is the new stable released version of OVN which\nis compatible with the OVS 2.13 version, also bumped in this review.\n\nThe patch also updates the use_new_ovn_repository to strip all\nnon-numeric characters of the branch name when comparing the versions,\nthat way we can compare branch-<version> against v<version> version\nformats.\n\nChange-Id: Ibbe7371b10efc87a42177d2a1a10fa576899b613\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n'}]",8,710740,2c81baa78212faa920dc4e19564a0aa306d48365,42,7,7,6773,,,0,"Switch release jobs to OVN 20.03 (v20.03.0 tag)

OVN 20.03 (v20.03.0 tag) is the new stable released version of OVN which
is compatible with the OVS 2.13 version, also bumped in this review.

The patch also updates the use_new_ovn_repository to strip all
non-numeric characters of the branch name when comparing the versions,
that way we can compare branch-<version> against v<version> version
formats.

Change-Id: Ibbe7371b10efc87a42177d2a1a10fa576899b613
Signed-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>
",git fetch https://review.opendev.org/openstack/networking-ovn refs/changes/40/710740/4 && git format-patch -1 --stdout FETCH_HEAD,"['devstack/devstackgaterc', 'devstack/lib/ovn', 'zuul.d/networking-ovn-jobs.yaml']",3,56c33c070b6e1f473760f01d60e853d91a8a7d5f,, OVN_BRANCH: branch-20.03 OVN_BRANCH: branch-20.03, OVN_BRANCH: branch-2.12 OVN_BRANCH: branch-2.12,6,9
openstack%2Fproject-config~master~I2938d645b53ee4f1c9bec9b2740fff0ed1927571,openstack/project-config,master,I2938d645b53ee4f1c9bec9b2740fff0ed1927571,vexxhost: do not test opendev/base-jobs,MERGED,2020-03-22 16:44:52.000000000,2020-03-22 17:08:01.000000000,2020-03-22 17:08:01.000000000,"[{'_account_id': 1}, {'_account_id': 2}, {'_account_id': 1004}, {'_account_id': 5263}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 16:44:52.000000000', 'files': ['zuul/main.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/9d4008235ea00d19f559a2001fced7308594ae48', 'message': 'vexxhost: do not test opendev/base-jobs\n\nLike Zuul tenant, the Vexxhost tenant should not run tests on\nopendev/base-jobs. Configure Vexxhost tenant to not include\nproject definition.\n\nExample where a test was run:\nhttps://review.opendev.org/714306\n\nChange-Id: I2938d645b53ee4f1c9bec9b2740fff0ed1927571\n'}]",0,714320,9d4008235ea00d19f559a2001fced7308594ae48,9,5,1,6547,,,0,"vexxhost: do not test opendev/base-jobs

Like Zuul tenant, the Vexxhost tenant should not run tests on
opendev/base-jobs. Configure Vexxhost tenant to not include
project definition.

Example where a test was run:
https://review.opendev.org/714306

Change-Id: I2938d645b53ee4f1c9bec9b2740fff0ed1927571
",git fetch https://review.opendev.org/openstack/project-config refs/changes/20/714320/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul/main.yaml'],1,9d4008235ea00d19f559a2001fced7308594ae48,vexxhost," # Only use jobs and secrets from this repo, we do not want # the project definition. - opendev/base-jobs: include: - job - secret - nodeset", - opendev/base-jobs,7,1
openstack%2Frally~master~I28880642e370513c2430e8e0f67dd7a622023a92,openstack/rally,master,I28880642e370513c2430e8e0f67dd7a622023a92,Deprecate rally task results command,MERGED,2020-03-22 13:17:51.000000000,2020-03-22 17:07:13.000000000,2020-03-22 17:07:13.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 13:17:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0cb9b180c2f2b84d44f384d0d358dbe56115f512', 'message': 'Deprecate rally task results command\n\nThis command produces task results in old format that missis a lot of\ninformation. For backward compatibility, the new task results exporter\nis introduced.\n\nChange-Id: I28880642e370513c2430e8e0f67dd7a622023a92\n'}, {'number': 2, 'created': '2020-03-22 16:38:02.000000000', 'files': ['tests/ci/cover.sh', 'rally/cli/commands/task.py', 'tests/functional/test_cli_task.py', 'tests/unit/cli/commands/test_task.py', 'tests/unit/plugins/task/exporters/dummy_data.py', 'tests/unit/plugins/task/exporters/test_html.py', 'rally/plugins/task/exporters/old_json_results.py', 'tests/unit/plugins/task/exporters/test_old_json_results.py', 'tests/unit/plugins/task/exporters/test_json_exporter.py', 'CHANGELOG.rst'], 'web_link': 'https://opendev.org/openstack/rally/commit/8dbad440dbd7868361113d11aa09feea8ab205fb', 'message': 'Deprecate rally task results command\n\nThis command produces task results in old format that missis a lot of\ninformation. For backward compatibility, the new task results exporter\nis introduced.\n\nChange-Id: I28880642e370513c2430e8e0f67dd7a622023a92\n'}]",0,714298,8dbad440dbd7868361113d11aa09feea8ab205fb,8,2,2,9545,,,0,"Deprecate rally task results command

This command produces task results in old format that missis a lot of
information. For backward compatibility, the new task results exporter
is introduced.

Change-Id: I28880642e370513c2430e8e0f67dd7a622023a92
",git fetch https://review.opendev.org/openstack/rally refs/changes/98/714298/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/ci/cover.sh', 'rally/cli/commands/task.py', 'tests/unit/cli/commands/test_task.py', 'tests/unit/plugins/task/exporters/dummy_data.py', 'tests/unit/plugins/task/exporters/test_html.py', 'rally/plugins/task/exporters/old_json_results.py', 'tests/unit/plugins/task/exporters/test_old_json_results.py', 'tests/unit/plugins/task/exporters/test_json_exporter.py', 'CHANGELOG.rst']",9,0cb9b180c2f2b84d44f384d0d358dbe56115f512,deprecate,* Command *rally task results* is deprecated. Use *rally task report --json* instead. ,,382,130
openstack%2Fcinder~master~If443455a513f4ddd035d365922e992620aa3627b,openstack/cinder,master,If443455a513f4ddd035d365922e992620aa3627b,Re locate cli opt register,ABANDONED,2020-03-22 11:28:59.000000000,2020-03-22 15:48:55.000000000,,"[{'_account_id': 9008}, {'_account_id': 9732}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 15386}, {'_account_id': 18120}, {'_account_id': 21884}, {'_account_id': 22348}, {'_account_id': 24921}, {'_account_id': 26537}, {'_account_id': 29705}]","[{'number': 1, 'created': '2020-03-22 11:28:59.000000000', 'files': ['cinder/cmd/volume_usage_audit.py', 'cinder/cmd/volume.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/673157034e57bbb660b21692d5faaa8076c64dd6', 'message': 'Re locate cli opt register\n\nPut register_cli_opt inside main function\n\nChange-Id: If443455a513f4ddd035d365922e992620aa3627b\n'}]",0,714291,673157034e57bbb660b21692d5faaa8076c64dd6,15,11,1,29071,,,0,"Re locate cli opt register

Put register_cli_opt inside main function

Change-Id: If443455a513f4ddd035d365922e992620aa3627b
",git fetch https://review.opendev.org/openstack/cinder refs/changes/91/714291/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/cmd/volume_usage_audit.py', 'cinder/cmd/volume.py']",2,673157034e57bbb660b21692d5faaa8076c64dd6,relocate-cli-register, CONF.register_cli_opt(host_opt) CONF.register_cli_opt(backend_name_opt),CONF.register_cli_opt(host_opt)CONF.register_cli_opt(backend_name_opt),3,3
openstack%2Fopenstack-ansible~master~I168f1b90ea311560a1164f93038a8a83df8ff68d,openstack/openstack-ansible,master,I168f1b90ea311560a1164f93038a8a83df8ff68d,Imported Translations from Zanata,MERGED,2020-03-21 07:51:37.000000000,2020-03-22 15:47:09.000000000,2020-03-22 15:44:58.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}]","[{'number': 1, 'created': '2020-03-21 07:51:37.000000000', 'files': ['releasenotes/source/locale/id/LC_MESSAGES/releasenotes.po', 'doc/source/locale/id/LC_MESSAGES/doc-user.po', 'doc/source/locale/id/LC_MESSAGES/doc-admin.po'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/934ea56e2f0199b0d904bc31999ef593ebe7449b', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I168f1b90ea311560a1164f93038a8a83df8ff68d\n'}]",0,714247,934ea56e2f0199b0d904bc31999ef593ebe7449b,7,2,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: I168f1b90ea311560a1164f93038a8a83df8ff68d
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/47/714247/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/source/locale/id/LC_MESSAGES/releasenotes.po', 'doc/source/locale/id/LC_MESSAGES/doc-user.po', 'doc/source/locale/id/LC_MESSAGES/doc-admin.po']",3,934ea56e2f0199b0d904bc31999ef593ebe7449b,zanata/translations,"# suhartono <cloudsuhartono@gmail.com>, 2020. #zanata""POT-Creation-Date: 2020-03-18 17:43+0000\n""""PO-Revision-Date: 2020-03-20 08:20+0000\n""""Alternatively you can try using new compute nodes deployment script ``/opt/"" ""openstack-ansible/scripts/add-compute.sh``."" msgstr """" ""Atau Anda dapat mencoba menggunakan skrip penyebaran node komputasi baru ``/"" ""opt/openstack-ansible/scripts/add-compute.sh``."" msgid """"msgid """" ""You can provide this script with extra tasks that will be executed before or "" ""right after OSA roles. To do so you should set environment variables "" ""``PRE_OSA_TASKS`` or ``POST_OSA_TASKS`` with plays to run devided with "" ""semicolon:"" msgstr """" ""Anda dapat memberikan skrip ini tugas tambahan yang akan dieksekusi sebelum "" ""atau tepat setelah peran OSA. Untuk melakukannya, Anda harus mengatur "" ""variabel lingkungan ``PRE_OSA_TASKS`` atau ``POST_OSA_TASKS`` dengan "" ""memainkan untuk menjalankan dibagi dengan titik koma (semicolon):"" ","""POT-Creation-Date: 2019-12-18 08:46+0000\n""""PO-Revision-Date: 2019-10-12 06:12+0000\n""",79,6
openstack%2Fpuppet-openstacklib~stable%2Fqueens~Icac198e59c369c6acb63dc4692b3408995a5481e,openstack/puppet-openstacklib,stable/queens,Icac198e59c369c6acb63dc4692b3408995a5481e,DNM: check gate status,ABANDONED,2020-03-22 13:36:14.000000000,2020-03-22 13:37:18.000000000,,[],"[{'number': 1, 'created': '2020-03-22 13:36:14.000000000', 'files': ['dnm'], 'web_link': 'https://opendev.org/openstack/puppet-openstacklib/commit/fb6b79c975ce56828b06016bd7b7b4cb88822571', 'message': 'DNM: check gate status\n\nChange-Id: Icac198e59c369c6acb63dc4692b3408995a5481e\n'}]",0,714300,fb6b79c975ce56828b06016bd7b7b4cb88822571,2,0,1,9816,,,0,"DNM: check gate status

Change-Id: Icac198e59c369c6acb63dc4692b3408995a5481e
",git fetch https://review.opendev.org/openstack/puppet-openstacklib refs/changes/00/714300/1 && git format-patch -1 --stdout FETCH_HEAD,['dnm'],1,fb6b79c975ce56828b06016bd7b7b4cb88822571,dnm,,,0,0
openstack%2Fopenstack-ansible~stable%2Ftrain~Icc044a74dc4c58717a62e22ec0b47b8fadd765c3,openstack/openstack-ansible,stable/train,Icc044a74dc4c58717a62e22ec0b47b8fadd765c3,nova: include hidden instances upgrade fix,ABANDONED,2020-02-09 09:12:02.000000000,2020-03-22 13:06:19.000000000,,"[{'_account_id': 22348}, {'_account_id': 25023}]","[{'number': 1, 'created': '2020-02-09 09:12:02.000000000', 'files': ['playbooks/defaults/repo_packages/openstack_services.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/c9f8c4fd6264454ef13d30cc4bf2cba802841a2a', 'message': 'nova: include hidden instances upgrade fix\n\nThere is a serious upgrade bug in Nova that affects large deployments\nwithout purged database records that can result in all instances\nappearing hidden.\n\nThis patch bumps Nova to the commit that includes the fix in order\nto make sure that our upgrades are reliable for large deployments.\n\nChange-Id: Icc044a74dc4c58717a62e22ec0b47b8fadd765c3\nRelated-Bug: #1862205\n'}]",0,706697,c9f8c4fd6264454ef13d30cc4bf2cba802841a2a,4,2,1,1004,,,0,"nova: include hidden instances upgrade fix

There is a serious upgrade bug in Nova that affects large deployments
without purged database records that can result in all instances
appearing hidden.

This patch bumps Nova to the commit that includes the fix in order
to make sure that our upgrades are reliable for large deployments.

Change-Id: Icc044a74dc4c58717a62e22ec0b47b8fadd765c3
Related-Bug: #1862205
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/97/706697/1 && git format-patch -1 --stdout FETCH_HEAD,['playbooks/defaults/repo_packages/openstack_services.yml'],1,c9f8c4fd6264454ef13d30cc4bf2cba802841a2a,bug/1862205,nova_git_install_branch: 8363905a6a6d5c8b2488619bdf807c5dc17b2842 # HEAD as of 09.02.2020,nova_git_install_branch: 27bfd0bc6233c25114504bb363402807752a7ece # HEAD as of 12.01.2020,1,1
openstack%2Ftrove~stable%2Ftrain~I3dfc894cc001529d60048d53e206251a41d546c0,openstack/trove,stable/train,I3dfc894cc001529d60048d53e206251a41d546c0,"Fixes ""a2ensite"" command arg and adds mod_wsgi package installation",MERGED,2020-03-22 11:16:56.000000000,2020-03-22 11:53:05.000000000,2020-03-22 11:51:36.000000000,"[{'_account_id': 6732}, {'_account_id': 22348}, {'_account_id': 31737}]","[{'number': 1, 'created': '2020-03-22 11:16:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/159d7c9e4f0a36a34257bd13cb64dc478b632053', 'message': 'Fixes ""a2ensite"" command arg and adds mod_wsgi package installation\n\nFixes ""a2ensite"" command arg\n-----------------------------\n""sudo a2ensite trove"" in this file should be ""sudo a2ensite trove-api""\nbecause the following error will occur while following this manual\'s\ninstructions::\n\n  $ sudo a2ensite trove\n  ERROR: Site trove does not exist!\n  $ sudo a2ensite trove-api\n  Enabling site trove-api.\n  To activate the new configuration, you need to run:\n    systemctl reload apache2\n\nHere are steps to reproduce the error::\n\n  $ sudo apt-get install apache2 libapache2-mod-wsgi-py3\n  $ git clone https://opendev.org/openstack/trove.git; cd trove\n  $ sudo cp etc/apache2/trove /etc/apache2/sites-available/trove-api.conf\n  $ sudo a2ensite trove\n  ERROR: Site trove does not exist!\n\nI have tested on Ubuntu 18.04.4 LTS::\n\n  $ uname -a\n  Linux bionic 4.15.0-91-generic #92-Ubuntu SMP Fri Feb 28 11:09:48 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\n  $ cat /etc/os-release\n  NAME=""Ubuntu""\n  VERSION=""18.04.4 LTS (Bionic Beaver)""\n  ID=ubuntu\n  ID_LIKE=debian\n  PRETTY_NAME=""Ubuntu 18.04.4 LTS""\n  VERSION_ID=""18.04""\n  HOME_URL=""https://www.ubuntu.com/""\n  SUPPORT_URL=""https://help.ubuntu.com/""\n  BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""\n  PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""\n  VERSION_CODENAME=bionic\n  UBUNTU_CODENAME=bionic\n\nAdds mod_wsgi package installation\n-----------------------------------\nI adds mod_wsgi package installation next to apache package installation\nbecause I don\'t find its installation through the docs in spite of\nmod_wsgi\'s importance for trove-api.\n\nI also updates description about following parts:\n  * RHEL8 and CentOS8 lines are added.\n  * Fedora21 and Fedora22, which are not supported by Fedora Project,\n    lines are merged to ""Fedora"".\n\nChange-Id: I3dfc894cc001529d60048d53e206251a41d546c0\nTask:  39116\nStory: 2007446\nSigned-off-by: Hirotaka Wakabayashi <hiwkby@yahoo.com>\n'}, {'number': 2, 'created': '2020-03-22 11:18:03.000000000', 'files': ['doc/source/install/apache-mod-wsgi.rst'], 'web_link': 'https://opendev.org/openstack/trove/commit/51ca93dabc37d2d28d9030776caf2cc04605a8c9', 'message': 'Fixes ""a2ensite"" command arg and adds mod_wsgi package installation\n\nFixes ""a2ensite"" command arg\n-----------------------------\n""sudo a2ensite trove"" in this file should be ""sudo a2ensite trove-api""\nbecause the following error will occur while following this manual\'s\ninstructions::\n\n  $ sudo a2ensite trove\n  ERROR: Site trove does not exist!\n  $ sudo a2ensite trove-api\n  Enabling site trove-api.\n  To activate the new configuration, you need to run:\n    systemctl reload apache2\n\nHere are steps to reproduce the error::\n\n  $ sudo apt-get install apache2 libapache2-mod-wsgi-py3\n  $ git clone https://opendev.org/openstack/trove.git; cd trove\n  $ sudo cp etc/apache2/trove /etc/apache2/sites-available/trove-api.conf\n  $ sudo a2ensite trove\n  ERROR: Site trove does not exist!\n\nI have tested on Ubuntu 18.04.4 LTS::\n\n  $ uname -a\n  Linux bionic 4.15.0-91-generic #92-Ubuntu SMP Fri Feb 28 11:09:48 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\n  $ cat /etc/os-release\n  NAME=""Ubuntu""\n  VERSION=""18.04.4 LTS (Bionic Beaver)""\n  ID=ubuntu\n  ID_LIKE=debian\n  PRETTY_NAME=""Ubuntu 18.04.4 LTS""\n  VERSION_ID=""18.04""\n  HOME_URL=""https://www.ubuntu.com/""\n  SUPPORT_URL=""https://help.ubuntu.com/""\n  BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""\n  PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""\n  VERSION_CODENAME=bionic\n  UBUNTU_CODENAME=bionic\n\nAdds mod_wsgi package installation\n-----------------------------------\nI adds mod_wsgi package installation next to apache package installation\nbecause I don\'t find its installation through the docs in spite of\nmod_wsgi\'s importance for trove-api.\n\nI also updates description about following parts:\n  * RHEL8 and CentOS8 lines are added.\n  * Fedora21 and Fedora22, which are not supported by Fedora Project,\n    lines are merged to ""Fedora"".\n\nChange-Id: I3dfc894cc001529d60048d53e206251a41d546c0\nTask: 39116\nStory: 2007446\nSigned-off-by: Hirotaka Wakabayashi <hiwkby@yahoo.com>\n'}]",0,714290,51ca93dabc37d2d28d9030776caf2cc04605a8c9,8,3,2,6732,,,0,"Fixes ""a2ensite"" command arg and adds mod_wsgi package installation

Fixes ""a2ensite"" command arg
-----------------------------
""sudo a2ensite trove"" in this file should be ""sudo a2ensite trove-api""
because the following error will occur while following this manual's
instructions::

  $ sudo a2ensite trove
  ERROR: Site trove does not exist!
  $ sudo a2ensite trove-api
  Enabling site trove-api.
  To activate the new configuration, you need to run:
    systemctl reload apache2

Here are steps to reproduce the error::

  $ sudo apt-get install apache2 libapache2-mod-wsgi-py3
  $ git clone https://opendev.org/openstack/trove.git; cd trove
  $ sudo cp etc/apache2/trove /etc/apache2/sites-available/trove-api.conf
  $ sudo a2ensite trove
  ERROR: Site trove does not exist!

I have tested on Ubuntu 18.04.4 LTS::

  $ uname -a
  Linux bionic 4.15.0-91-generic #92-Ubuntu SMP Fri Feb 28 11:09:48 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
  $ cat /etc/os-release
  NAME=""Ubuntu""
  VERSION=""18.04.4 LTS (Bionic Beaver)""
  ID=ubuntu
  ID_LIKE=debian
  PRETTY_NAME=""Ubuntu 18.04.4 LTS""
  VERSION_ID=""18.04""
  HOME_URL=""https://www.ubuntu.com/""
  SUPPORT_URL=""https://help.ubuntu.com/""
  BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
  PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
  VERSION_CODENAME=bionic
  UBUNTU_CODENAME=bionic

Adds mod_wsgi package installation
-----------------------------------
I adds mod_wsgi package installation next to apache package installation
because I don't find its installation through the docs in spite of
mod_wsgi's importance for trove-api.

I also updates description about following parts:
  * RHEL8 and CentOS8 lines are added.
  * Fedora21 and Fedora22, which are not supported by Fedora Project,
    lines are merged to ""Fedora"".

Change-Id: I3dfc894cc001529d60048d53e206251a41d546c0
Task: 39116
Story: 2007446
Signed-off-by: Hirotaka Wakabayashi <hiwkby@yahoo.com>
",git fetch https://review.opendev.org/openstack/trove refs/changes/90/714290/2 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/install/apache-mod-wsgi.rst'],1,159d7c9e4f0a36a34257bd13cb64dc478b632053,hiwkby-fixes-apache-mod-wsgi-rst-stable/train, RHEL7/CentOS7: sudo yum install httpd mod_wsgi RHEL8/CentOS8: sudo dnf install httpd python3-mod_wsgi Fedora: sudo dnf install httpd mod_wsgi sudo apt-get install apache2 libapache2-mod-wsgi-py3 Fedora/RHEL/CentOS: sudo a2ensite trove-api, Fedora 21/RHEL7/CentOS7: sudo yum install httpd Fedora 22 (or higher): sudo dnf install httpd apt-get install apache2 Fedora/RHEL7/CentOS7: sudo a2ensite trove,10,7
openstack%2Ftrove~master~I3dfc894cc001529d60048d53e206251a41d546c0,openstack/trove,master,I3dfc894cc001529d60048d53e206251a41d546c0,"Fixes ""a2ensite"" command arg and adds mod_wsgi package installation",MERGED,2020-03-22 11:05:14.000000000,2020-03-22 11:28:02.000000000,2020-03-22 11:26:43.000000000,"[{'_account_id': 6732}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 11:05:14.000000000', 'files': ['doc/source/install/apache-mod-wsgi.rst'], 'web_link': 'https://opendev.org/openstack/trove/commit/9e588c04aa44321b704c22aec27ac964e44235d7', 'message': 'Fixes ""a2ensite"" command arg and adds mod_wsgi package installation\n\nFixes ""a2ensite"" command arg\n-----------------------------\n""sudo a2ensite trove"" in this file should be ""sudo a2ensite trove-api""\nbecause the following error will occur while following this manual\'s\ninstructions::\n\n  $ sudo a2ensite trove\n  ERROR: Site trove does not exist!\n  $ sudo a2ensite trove-api\n  Enabling site trove-api.\n  To activate the new configuration, you need to run:\n    systemctl reload apache2\n\nHere are steps to reproduce the error::\n\n  $ sudo apt-get install apache2 libapache2-mod-wsgi-py3\n  $ git clone https://opendev.org/openstack/trove.git; cd trove\n  $ sudo cp etc/apache2/trove /etc/apache2/sites-available/trove-api.conf\n  $ sudo a2ensite trove\n  ERROR: Site trove does not exist!\n\nI have tested on Ubuntu 18.04.4 LTS::\n\n  $ uname -a\n  Linux bionic 4.15.0-91-generic #92-Ubuntu SMP Fri Feb 28 11:09:48 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\n  $ cat /etc/os-release\n  NAME=""Ubuntu""\n  VERSION=""18.04.4 LTS (Bionic Beaver)""\n  ID=ubuntu\n  ID_LIKE=debian\n  PRETTY_NAME=""Ubuntu 18.04.4 LTS""\n  VERSION_ID=""18.04""\n  HOME_URL=""https://www.ubuntu.com/""\n  SUPPORT_URL=""https://help.ubuntu.com/""\n  BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""\n  PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""\n  VERSION_CODENAME=bionic\n  UBUNTU_CODENAME=bionic\n\nAdds mod_wsgi package installation\n-----------------------------------\nI adds mod_wsgi package installation next to apache package installation\nbecause I don\'t find its installation through the docs in spite of\nmod_wsgi\'s importance for trove-api.\n\nI also updates description about following parts:\n  * RHEL8 and CentOS8 lines are added.\n  * Fedora21 and Fedora22, which are not supported by Fedora Project,\n    lines are merged to ""Fedora"".\n\nChange-Id: I3dfc894cc001529d60048d53e206251a41d546c0\nTask:  39116\nStory: 2007446\nSigned-off-by: Hirotaka Wakabayashi <hiwkby@yahoo.com>\n'}]",0,714289,9e588c04aa44321b704c22aec27ac964e44235d7,8,2,1,31737,,,0,"Fixes ""a2ensite"" command arg and adds mod_wsgi package installation

Fixes ""a2ensite"" command arg
-----------------------------
""sudo a2ensite trove"" in this file should be ""sudo a2ensite trove-api""
because the following error will occur while following this manual's
instructions::

  $ sudo a2ensite trove
  ERROR: Site trove does not exist!
  $ sudo a2ensite trove-api
  Enabling site trove-api.
  To activate the new configuration, you need to run:
    systemctl reload apache2

Here are steps to reproduce the error::

  $ sudo apt-get install apache2 libapache2-mod-wsgi-py3
  $ git clone https://opendev.org/openstack/trove.git; cd trove
  $ sudo cp etc/apache2/trove /etc/apache2/sites-available/trove-api.conf
  $ sudo a2ensite trove
  ERROR: Site trove does not exist!

I have tested on Ubuntu 18.04.4 LTS::

  $ uname -a
  Linux bionic 4.15.0-91-generic #92-Ubuntu SMP Fri Feb 28 11:09:48 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
  $ cat /etc/os-release
  NAME=""Ubuntu""
  VERSION=""18.04.4 LTS (Bionic Beaver)""
  ID=ubuntu
  ID_LIKE=debian
  PRETTY_NAME=""Ubuntu 18.04.4 LTS""
  VERSION_ID=""18.04""
  HOME_URL=""https://www.ubuntu.com/""
  SUPPORT_URL=""https://help.ubuntu.com/""
  BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
  PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
  VERSION_CODENAME=bionic
  UBUNTU_CODENAME=bionic

Adds mod_wsgi package installation
-----------------------------------
I adds mod_wsgi package installation next to apache package installation
because I don't find its installation through the docs in spite of
mod_wsgi's importance for trove-api.

I also updates description about following parts:
  * RHEL8 and CentOS8 lines are added.
  * Fedora21 and Fedora22, which are not supported by Fedora Project,
    lines are merged to ""Fedora"".

Change-Id: I3dfc894cc001529d60048d53e206251a41d546c0
Task:  39116
Story: 2007446
Signed-off-by: Hirotaka Wakabayashi <hiwkby@yahoo.com>
",git fetch https://review.opendev.org/openstack/trove refs/changes/89/714289/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/install/apache-mod-wsgi.rst'],1,9e588c04aa44321b704c22aec27ac964e44235d7,hiwkby-fixes-apache-mod-wsgi-rst, RHEL7/CentOS7: sudo yum install httpd mod_wsgi RHEL8/CentOS8: sudo dnf install httpd python3-mod_wsgi Fedora: sudo dnf install httpd mod_wsgi sudo apt-get install apache2 libapache2-mod-wsgi-py3 Fedora/RHEL/CentOS: sudo a2ensite trove-api, Fedora 21/RHEL7/CentOS7: sudo yum install httpd Fedora 22 (or higher): sudo dnf install httpd apt-get install apache2 Fedora/RHEL7/CentOS7: sudo a2ensite trove,10,7
openstack%2Ftrove~stable%2Ftrain~I4856fc493f46e04b6c74e042f80c896e0cac997b,openstack/trove,stable/train,I4856fc493f46e04b6c74e042f80c896e0cac997b,Fix devstack installation guide,MERGED,2020-03-22 10:45:09.000000000,2020-03-22 11:20:15.000000000,2020-03-22 11:18:49.000000000,"[{'_account_id': 6732}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 10:45:09.000000000', 'files': ['doc/source/install/install-devstack.rst'], 'web_link': 'https://opendev.org/openstack/trove/commit/8e6ce1962538e4252b193cd810e857372aded9d1', 'message': 'Fix devstack installation guide\n\nStory: 2007447\nTask: 39117\nChange-Id: I4856fc493f46e04b6c74e042f80c896e0cac997b\n(cherry picked from commit 94af8337393de2f2a47c9c6f828ed2f21120f997)\n'}]",0,714287,8e6ce1962538e4252b193cd810e857372aded9d1,7,2,1,6732,,,0,"Fix devstack installation guide

Story: 2007447
Task: 39117
Change-Id: I4856fc493f46e04b6c74e042f80c896e0cac997b
(cherry picked from commit 94af8337393de2f2a47c9c6f828ed2f21120f997)
",git fetch https://review.opendev.org/openstack/trove refs/changes/87/714287/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/install/install-devstack.rst'],1,8e6ce1962538e4252b193cd810e857372aded9d1,fix-doc-stable/train, enable_plugin trove https://opendev.org/openstack/trove, enable_plugin trove https:/opendev.org/openstack/trove,1,1
openstack%2Ftrove~master~I4856fc493f46e04b6c74e042f80c896e0cac997b,openstack/trove,master,I4856fc493f46e04b6c74e042f80c896e0cac997b,Fix devstack installation guide,MERGED,2020-03-22 04:42:03.000000000,2020-03-22 10:45:09.000000000,2020-03-22 10:35:14.000000000,"[{'_account_id': 6732}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-22 04:42:03.000000000', 'files': ['doc/source/install/install-devstack.rst'], 'web_link': 'https://opendev.org/openstack/trove/commit/94af8337393de2f2a47c9c6f828ed2f21120f997', 'message': 'Fix devstack installation guide\n\nStory: 2007447\nTask: 39117\nChange-Id: I4856fc493f46e04b6c74e042f80c896e0cac997b\n'}]",0,714282,94af8337393de2f2a47c9c6f828ed2f21120f997,8,2,1,6732,,,0,"Fix devstack installation guide

Story: 2007447
Task: 39117
Change-Id: I4856fc493f46e04b6c74e042f80c896e0cac997b
",git fetch https://review.opendev.org/openstack/trove refs/changes/82/714282/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/install/install-devstack.rst'],1,94af8337393de2f2a47c9c6f828ed2f21120f997,fix-doc, enable_plugin trove https://opendev.org/openstack/trove, enable_plugin trove https:/opendev.org/openstack/trove,1,1
openstack%2Frally~master~I9d639d2baa9298a26e7670c5b4b356a8b56583a2,openstack/rally,master,I9d639d2baa9298a26e7670c5b4b356a8b56583a2,Deprecate rally.common.yamlutils and rally.common.fileutils,MERGED,2020-03-21 22:52:39.000000000,2020-03-22 10:07:56.000000000,2020-03-22 10:07:56.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-21 22:52:39.000000000', 'files': ['tests/unit/cli/commands/test_deployment.py', 'rally/common/fileutils.py', 'tests/unit/cli/commands/test_task.py', 'rally/cli/envutils.py', 'tests/unit/test_mock.py', 'tests/unit/common/test_fileutils.py', 'tests/unit/cli/commands/test_verify.py', 'tests/unit/cli/test_yamlutils.py', 'rally/cli/commands/task.py', 'tests/unit/cli/test_envutils.py', 'rally/cli/commands/deployment.py', 'tests/unit/cli/commands/test_env.py', 'rally/cli/commands/verify.py', 'rally/common/utils.py', 'tests/unit/test_test_mock.py', 'rally/cli/commands/env.py', 'rally/cli/yamlutils.py', 'CHANGELOG.rst'], 'web_link': 'https://opendev.org/openstack/rally/commit/30d5a8ed044af7e140d814110d1d834d42ea9e73', 'message': 'Deprecate rally.common.yamlutils and rally.common.fileutils\n\n* rally.common.fileutils contains stuff that related only to\n  rally.cli.envutils + openstack related helper\n\n* rally.common.yamlutils was designed for CLI purpose and should not be\n  places under rally.common.\n\nChange-Id: I9d639d2baa9298a26e7670c5b4b356a8b56583a2\n'}]",0,714278,30d5a8ed044af7e140d814110d1d834d42ea9e73,6,2,1,9545,,,0,"Deprecate rally.common.yamlutils and rally.common.fileutils

* rally.common.fileutils contains stuff that related only to
  rally.cli.envutils + openstack related helper

* rally.common.yamlutils was designed for CLI purpose and should not be
  places under rally.common.

Change-Id: I9d639d2baa9298a26e7670c5b4b356a8b56583a2
",git fetch https://review.opendev.org/openstack/rally refs/changes/78/714278/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/cli/commands/test_deployment.py', 'rally/common/fileutils.py', 'tests/unit/cli/commands/test_task.py', 'rally/cli/envutils.py', 'tests/unit/test_mock.py', 'tests/unit/common/test_fileutils.py', 'tests/unit/cli/commands/test_verify.py', 'tests/unit/cli/test_yamlutils.py', 'rally/cli/commands/task.py', 'tests/unit/cli/test_envutils.py', 'rally/cli/commands/deployment.py', 'tests/unit/cli/commands/test_env.py', 'rally/cli/commands/verify.py', 'rally/common/utils.py', 'tests/unit/test_test_mock.py', 'rally/cli/commands/env.py', 'rally/cli/yamlutils.py', 'CHANGELOG.rst']",18,30d5a8ed044af7e140d814110d1d834d42ea9e73,deprecate,* Module *rally.common.yamlutils* is deprecated. It was designed for CLI usage and moves to right place. * Module *rally.common.fileutils* is deprecated. ,,163,195
openstack%2Ftacker~master~Ie2901d81e3b2a9a6d6054bfc3227a0afcf1eb3b9,openstack/tacker,master,Ie2901d81e3b2a9a6d6054bfc3227a0afcf1eb3b9,Ensure to use python3 to install horizon,MERGED,2020-02-27 11:51:02.000000000,2020-03-22 07:58:51.000000000,2020-03-12 01:14:26.000000000,"[{'_account_id': 18955}, {'_account_id': 22348}, {'_account_id': 26588}]","[{'number': 1, 'created': '2020-02-27 11:51:02.000000000', 'files': ['devstack/lib/tacker'], 'web_link': 'https://opendev.org/openstack/tacker/commit/c4cd7c807fcd710b6373de5a2745463b1e588d78', 'message': 'Ensure to use python3 to install horizon\n\nIn function tacker_horizon_install, it is failed if your runtime\nreferred as `python` is python2. It is because the function runs\ninstaller script without using $PYTHON which is defined in devstack\nfor referring appropriate runtime.\n\nChange-Id: Ie2901d81e3b2a9a6d6054bfc3227a0afcf1eb3b9\n'}]",0,710236,c4cd7c807fcd710b6373de5a2745463b1e588d78,9,3,1,25701,,,0,"Ensure to use python3 to install horizon

In function tacker_horizon_install, it is failed if your runtime
referred as `python` is python2. It is because the function runs
installer script without using $PYTHON which is defined in devstack
for referring appropriate runtime.

Change-Id: Ie2901d81e3b2a9a6d6054bfc3227a0afcf1eb3b9
",git fetch https://review.opendev.org/openstack/tacker refs/changes/36/710236/1 && git format-patch -1 --stdout FETCH_HEAD,['devstack/lib/tacker'],1,c4cd7c807fcd710b6373de5a2745463b1e588d78,horizon-ins-py3, $PYTHON $DEST/tacker-horizon/manage.py collectstatic --noinput echo yes | $PYTHON $DEST/tacker-horizon/manage.py compress, $DEST/tacker-horizon/manage.py collectstatic --noinput echo yes | $DEST/tacker-horizon/manage.py compress,2,2
openstack%2Fswift~master~I6bc912ce5eee95b443e1e260bc445c58efb2fbf8,openstack/swift,master,I6bc912ce5eee95b443e1e260bc445c58efb2fbf8,Delay ring lookup until we know we need it,MERGED,2020-03-07 00:17:05.000000000,2020-03-22 05:49:47.000000000,2020-03-22 05:48:25.000000000,"[{'_account_id': 597}, {'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-07 00:17:05.000000000', 'files': ['swift/proxy/controllers/base.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/601ced55757ee8081710ccb0553d8447263e7c7d', 'message': 'Delay ring lookup until we know we need it\n\nChange-Id: I6bc912ce5eee95b443e1e260bc445c58efb2fbf8\nCloses-Bug: #1699726\n'}]",0,711789,601ced55757ee8081710ccb0553d8447263e7c7d,14,3,1,15343,,,0,"Delay ring lookup until we know we need it

Change-Id: I6bc912ce5eee95b443e1e260bc445c58efb2fbf8
Closes-Bug: #1699726
",git fetch https://review.opendev.org/openstack/swift refs/changes/89/711789/1 && git format-patch -1 --stdout FETCH_HEAD,['swift/proxy/controllers/base.py'],1,601ced55757ee8081710ccb0553d8447263e7c7d,bug/1699726," partition, nodes = self.app.account_ring.get_nodes(account) part, nodes = self.app.container_ring.get_nodes(account, container)"," partition, nodes = self.app.account_ring.get_nodes(account) part, nodes = self.app.container_ring.get_nodes(account, container)",2,2
openstack%2Ftempest~master~I22e62f33b67002b8a0e1bd24f311df367500c32b,openstack/tempest,master,I22e62f33b67002b8a0e1bd24f311df367500c32b,add unittest for delete resource types association,MERGED,2020-03-07 03:11:32.000000000,2020-03-22 01:53:22.000000000,2020-03-22 01:50:47.000000000,"[{'_account_id': 8556}, {'_account_id': 22348}, {'_account_id': 23186}]","[{'number': 1, 'created': '2020-03-07 03:11:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/610ca63b6cd268de2d45a027579897b9ddd6c568', 'message': 'add unittest for delete resource types association\n\nadd unittest for delete resource types association interface\nof metadata in glance module\n\nChange-Id: I22e62f33b67002b8a0e1bd24f311df367500c32b\n'}, {'number': 2, 'created': '2020-03-07 04:24:39.000000000', 'files': ['tempest/tests/lib/services/image/v2/test_resource_types_client.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/65e37a2bf50b3533f76da46a253a819cf08991dc', 'message': 'add unittest for delete resource types association\n\nadd unittest for delete resource types association interface\nof metadata in glance module\n\nChange-Id: I22e62f33b67002b8a0e1bd24f311df367500c32b\n'}]",0,711797,65e37a2bf50b3533f76da46a253a819cf08991dc,10,3,2,30409,,,0,"add unittest for delete resource types association

add unittest for delete resource types association interface
of metadata in glance module

Change-Id: I22e62f33b67002b8a0e1bd24f311df367500c32b
",git fetch https://review.opendev.org/openstack/tempest refs/changes/97/711797/2 && git format-patch -1 --stdout FETCH_HEAD,['tempest/tests/lib/services/image/v2/test_resource_types_client.py'],1,610ca63b6cd268de2d45a027579897b9ddd6c568,master14," def test_delete_resource_type_association(self): self.check_service_client_function( self.client.delete_resource_type_association, 'tempest.lib.common.rest_client.RestClient.delete', {}, namespace_id=""OS::Compute::Hypervisor"", resource_name=""OS::Glance::Image"", status=204)",,8,0
openstack%2Ftempest~master~I0171dc0a87800ad33c176c6b6540fbc3db025709,openstack/tempest,master,I0171dc0a87800ad33c176c6b6540fbc3db025709,services: Introduce a cinder v3 attachments client,MERGED,2020-01-16 13:32:47.000000000,2020-03-22 01:52:10.000000000,2020-03-22 01:50:44.000000000,"[{'_account_id': 8367}, {'_account_id': 8556}, {'_account_id': 9732}, {'_account_id': 22348}, {'_account_id': 23186}]","[{'number': 1, 'created': '2020-01-16 13:32:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8ff2b53eb94f7b35545dfd04faaa387917c06aef', 'message': 'services: Introduce a cinder v3 attachments client\n\nThis change adds a new attachment client to allow for CRUD operations\nagainst volume attachments as now exposed by the Cinder v3 API.\n\nChange-Id: I0171dc0a87800ad33c176c6b6540fbc3db025709\n'}, {'number': 2, 'created': '2020-01-28 20:37:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0a98a43d2e3c0c6274190b733bd36b89144d4538', 'message': 'services: Introduce a cinder v3 attachments client\n\nThis change adds a new attachment client to allow for CRUD operations\nagainst volume attachments as now exposed by the Cinder v3 API.\n\nChange-Id: I0171dc0a87800ad33c176c6b6540fbc3db025709\n'}, {'number': 3, 'created': '2020-02-13 15:33:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/72e6e8207098894d978fc21e7c9a6faafd3c9025', 'message': 'services: Introduce a cinder v3 attachments client\n\nThis change adds a new attachment client to allow for CRUD operations\nagainst volume attachments as now exposed by the Cinder v3 API.\n\nChange-Id: I0171dc0a87800ad33c176c6b6540fbc3db025709\n'}, {'number': 4, 'created': '2020-03-02 09:03:13.000000000', 'files': ['tempest/lib/services/volume/v3/attachments_client.py', 'releasenotes/notes/introduce-attachments-client-add-show-attachment-api-c3111f7e560a87b3.yaml', 'tempest/clients.py', 'tempest/lib/services/volume/v3/__init__.py', 'tempest/tests/lib/services/volume/v3/test_attachments_client.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/ec3ae5ea0e19ca45f4c81336e6e9499e884c6ba5', 'message': 'services: Introduce a cinder v3 attachments client\n\nThis change adds a new attachment client to allow for CRUD operations\nagainst volume attachments as now exposed by the Cinder v3 API.\n\nChange-Id: I0171dc0a87800ad33c176c6b6540fbc3db025709\n'}]",3,702864,ec3ae5ea0e19ca45f4c81336e6e9499e884c6ba5,22,5,4,10135,,,0,"services: Introduce a cinder v3 attachments client

This change adds a new attachment client to allow for CRUD operations
against volume attachments as now exposed by the Cinder v3 API.

Change-Id: I0171dc0a87800ad33c176c6b6540fbc3db025709
",git fetch https://review.opendev.org/openstack/tempest refs/changes/64/702864/4 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/lib/services/volume/v3/attachments_client.py', 'releasenotes/notes/introduce-attachments-client-add-show-attachment-api-c3111f7e560a87b3.yaml', 'tempest/clients.py', 'tempest/lib/services/volume/v3/__init__.py']",4,8ff2b53eb94f7b35545dfd04faaa387917c06aef,negative_multiattach_swap_volume_tests,"from tempest.lib.services.volume.v3.attachments_client import AttachmentsClient__all__ = ['AttachmentsClient', 'AvailabilityZoneClient', 'BackupsClient', 'BaseClient', 'CapabilitiesClient', 'EncryptionTypesClient', 'ExtensionsClient', 'GroupSnapshotsClient', 'GroupTypesClient', 'GroupsClient', 'HostsClient', 'LimitsClient', 'MessagesClient', 'QosSpecsClient', 'QuotaClassesClient', 'QuotasClient', 'SchedulerStatsClient', 'ServicesClient', 'SnapshotManageClient', 'SnapshotsClient', 'TransfersClient', 'TypesClient', 'VersionsClient', 'VolumeManageClient', 'VolumesClient']"," __all__ = ['AvailabilityZoneClient', 'BackupsClient', 'BaseClient', 'CapabilitiesClient', 'EncryptionTypesClient', 'ExtensionsClient', 'GroupSnapshotsClient', 'GroupTypesClient', 'GroupsClient', 'HostsClient', 'LimitsClient', 'MessagesClient', 'QosSpecsClient', 'QuotaClassesClient', 'QuotasClient', 'SchedulerStatsClient', 'ServicesClient', 'SnapshotManageClient', 'SnapshotsClient', 'TransfersClient', 'TypesClient', 'VersionsClient', 'VolumeManageClient', 'VolumesClient']",47,9
openstack%2Frequirements~stable%2Fstein~I8a855d3c6a5fd3c12fb647c72cee4db4c80d37e6,openstack/requirements,stable/stein,I8a855d3c6a5fd3c12fb647c72cee4db4c80d37e6,update constraint for keystonemiddleware to new release 6.0.1,MERGED,2020-03-20 11:01:54.000000000,2020-03-21 23:44:09.000000000,2020-03-21 23:44:09.000000000,"[{'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 11:01:54.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/fe57b5beea4539b4e2270a30f28405d3cd6e57cf', 'message': 'update constraint for keystonemiddleware to new release 6.0.1\n\nChange-Id: I8a855d3c6a5fd3c12fb647c72cee4db4c80d37e6\nmeta:version: 6.0.1\nmeta:diff-start: -\nmeta:series: stein\nmeta:release-type: release\nmeta:pypi: yes\nmeta:first: no\nmeta:release:Author: Colleen Murphy <colleen.murphy@suse.com>\nmeta:release:Commit: Colleen Murphy <colleen.murphy@suse.com>\nmeta:release:Change-Id: Ic49c04559ff4e5664e8ed50c8ebced3c7f879403\nmeta:release:Code-Review+2: Sean McGinnis <sean.mcginnis@gmail.com>\nmeta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nmeta:release:Workflow+1: Thierry Carrez <thierry@openstack.org>\n'}]",0,714081,fe57b5beea4539b4e2270a30f28405d3cd6e57cf,12,3,1,11131,,,0,"update constraint for keystonemiddleware to new release 6.0.1

Change-Id: I8a855d3c6a5fd3c12fb647c72cee4db4c80d37e6
meta:version: 6.0.1
meta:diff-start: -
meta:series: stein
meta:release-type: release
meta:pypi: yes
meta:first: no
meta:release:Author: Colleen Murphy <colleen.murphy@suse.com>
meta:release:Commit: Colleen Murphy <colleen.murphy@suse.com>
meta:release:Change-Id: Ic49c04559ff4e5664e8ed50c8ebced3c7f879403
meta:release:Code-Review+2: Sean McGinnis <sean.mcginnis@gmail.com>
meta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
meta:release:Workflow+1: Thierry Carrez <thierry@openstack.org>
",git fetch https://review.opendev.org/openstack/requirements refs/changes/81/714081/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,fe57b5beea4539b4e2270a30f28405d3cd6e57cf,new-release,keystonemiddleware===6.0.1,keystonemiddleware===6.0.0,1,1
openstack%2Ftripleo-heat-templates~stable%2Ftrain~I263133c046c8c48f838da6d506b5dd046fbf7048,openstack/tripleo-heat-templates,stable/train,I263133c046c8c48f838da6d506b5dd046fbf7048,Fix dashboard_frontend_vip parameter,MERGED,2020-03-20 15:18:46.000000000,2020-03-21 23:16:53.000000000,2020-03-21 23:16:53.000000000,"[{'_account_id': 3153}, {'_account_id': 6796}, {'_account_id': 14985}, {'_account_id': 18002}, {'_account_id': 20182}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25402}, {'_account_id': 25877}]","[{'number': 1, 'created': '2020-03-20 15:18:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/9055718070f792625bb8e6d0f6a647293e972e13', 'message': 'Fix dashboard_frontend_vip parameter\n\nThis patch fixes the dashboard_frontend_vip network\nused to properly configure grafana in HA, reflecting\nthe change we have in haproxy.pp\nThis change also exposes a new variable to give the\noperators the chance to configure a read-only admin\nuser (e.g., no pools can be created through the\ndashboard).\n\nDepends-On: https://review.opendev.org/#/c/713892\nChange-Id: I263133c046c8c48f838da6d506b5dd046fbf7048\nCloses-Bug: #1868118\n(cherry picked from commit a6ff7e2eacf27536b98ee10d511281b3d19a2b0a)\n'}, {'number': 2, 'created': '2020-03-20 15:20:56.000000000', 'files': ['deployment/ceph-ansible/ceph-mgr.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/bf9c153d08842719e10730c8ef612e37984f5b26', 'message': 'Fix dashboard_frontend_vip parameter\n\nThis patch fixes the dashboard_frontend_vip network\nused to properly configure grafana in HA, reflecting\nthe change we have in haproxy.pp\nThis change also exposes a new variable to give the\noperators the chance to configure a read-only admin\nuser (e.g., no pools can be created through the\ndashboard).\n\nDepends-On: https://review.opendev.org/#/c/714141\nChange-Id: I263133c046c8c48f838da6d506b5dd046fbf7048\nCloses-Bug: #1868118\n(cherry picked from commit a6ff7e2eacf27536b98ee10d511281b3d19a2b0a)\n'}]",0,714145,bf9c153d08842719e10730c8ef612e37984f5b26,15,9,2,25402,,,0,"Fix dashboard_frontend_vip parameter

This patch fixes the dashboard_frontend_vip network
used to properly configure grafana in HA, reflecting
the change we have in haproxy.pp
This change also exposes a new variable to give the
operators the chance to configure a read-only admin
user (e.g., no pools can be created through the
dashboard).

Depends-On: https://review.opendev.org/#/c/714141
Change-Id: I263133c046c8c48f838da6d506b5dd046fbf7048
Closes-Bug: #1868118
(cherry picked from commit a6ff7e2eacf27536b98ee10d511281b3d19a2b0a)
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/45/714145/2 && git format-patch -1 --stdout FETCH_HEAD,['deployment/ceph-ansible/ceph-mgr.yaml'],1,9055718070f792625bb8e6d0f6a647293e972e13,lp1868118," CephDashboardAdminRO: type: boolean default: true description: Parameter used to set a read-only admin user. dashboard_frontend_vip: {get_param: [EndpointMap, CephDashboardInternal, host]} dashboard_admin_user_ro: {get_param: CephDashboardAdminRO}"," dashboard_frontend_vip: {get_param: [EndpointMap, CephGrafanaInternal, host]}",6,1
openstack%2Frally~master~I9df675afdf0a6b76916fba7ed9c3712136ad4780,openstack/rally,master,I9df675afdf0a6b76916fba7ed9c3712136ad4780,Restruct rally.plugins.common module,MERGED,2020-03-21 21:31:29.000000000,2020-03-21 22:52:57.000000000,2020-03-21 22:52:57.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-21 21:31:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/28ddad95c90693deef8973b731d199da4ec6e79b', 'message': 'Restruct rally.plugins.common module\n\nChange-Id: I9df675afdf0a6b76916fba7ed9c3712136ad4780\n'}, {'number': 2, 'created': '2020-03-21 21:41:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ec3684e9e2f2020c870cc52c2f4e69187117b23e', 'message': 'Restruct rally.plugins.common module\n\nChange-Id: I9df675afdf0a6b76916fba7ed9c3712136ad4780\n'}, {'number': 3, 'created': '2020-03-21 22:05:24.000000000', 'files': ['tests/unit/plugins/task/runners/__init__.py', 'rally/plugins/verification/reporters.py', 'tests/unit/plugins/task/hooks/test_sys_call.py', 'tests/unit/plugins/task/exporters/elastic/test_exporter.py', 'tests/unit/plugins/task/runners/test_constant.py', 'tests/unit/plugins/verification/junit_report.xml', 'rally/plugins/task/exporters/junit.py', 'rally/plugins/task/runners/rps.py', 'tests/unit/plugins/task/scenarios/dummy/__init__.py', 'rally/plugins/common/verification/testr.py', 'tests/unit/plugins/task/sla/test_max_average_duration.py', 'rally/plugins/task/sla/max_average_duration.py', 'rally/plugins/common/sla/performance_degradation.py', 'rally/plugins/task/scenarios/dummy/__init__.py', 'rally/plugins/task/sla/__init__.py', 'tests/unit/plugins/task/exporters/elastic/test_client.py', 'tests/unit/plugins/task/hook_triggers/test_event.py', 'tests/unit/plugins/task/exporters/test_json_exporter.py', 'tests/unit/plugins/task/exporters/test_trends.py', 'rally/plugins/common/sla/failure_rate.py', 'rally/plugins/task/exporters/elastic/flatten.py', 'tests/unit/plugins/task/hook_triggers/test_periodic.py', 'rally/plugins/common/types.py', 'rally/plugins/common/sla/iteration_time.py', 'rally/plugins/task/scenarios/dummy/dummy.py', 'rally/plugins/common/hook/triggers/periodic.py', 'rally/common/logging.py', 'tests/unit/plugins/task/exporters/elastic/__init__.py', 'tests/unit/plugins/task/runners/test_rps.py', 'tests/unit/plugins/task/sla/test_performance_degradation.py', 'rally/plugins/common/exporters/html.py', 'rally/plugins/common/runners/rps.py', 'tests/unit/plugins/task/hook_triggers/__init__.py', 'rally/plugins/task/hooks/sys_call.py', 'rally/plugins/task/hooks/__init__.py', 'rally/plugins/task/runners/serial.py', 'rally/plugins/task/exporters/elastic/client.py', 'tests/unit/plugins/task/exporters/__init__.py', 'rally/plugins/common/hook/sys_call.py', 'CHANGELOG.rst', 'rally/plugins/task/exporters/trends.py', 'tests/unit/plugins/verification/test_testr.py', 'rally/plugins/task/scenarios/requests/utils.py', 'rally/plugins/task/runners/constant.py', 'rally/plugins/task/exporters/html.py', 'rally/plugins/common/runners/serial.py', 'tests/unit/task/test_runner.py', 'rally/plugins/task/contexts/dummy.py', 'rally/plugins/task/scenarios/__init__.py', 'rally/plugins/task/scenarios/requests/__init__.py', 'tests/unit/plugins/task/scenarios/requests/test_utils.py', 'rally/plugins/common/runners/constant.py', 'rally/plugins/common/scenarios/requests/utils.py', 'rally/plugins/task/exporters/__init__.py', 'rally/plugins/__init__.py', 'rally/plugins/common/sla/max_average_duration_per_atomic.py', 'tests/unit/plugins/task/sla/__init__.py', 'rally/plugins/common/exporters/elastic/flatten.py', 'tests/unit/plugins/task/exporters/test_junit.py', 'tests/unit/plugins/task/exporters/test_html.py', 'rally/plugins/task/exporters/elastic/exporter.py', 'rally/plugins/common/hook/triggers/event.py', 'rally/plugins/common/sla/max_average_duration.py', 'tests/unit/plugins/task/contexts/__init__.py', 'rally/plugins/task/types.py', 'tests/unit/plugins/task/exporters/elastic/test_flatten.py', 'rally/plugins/common/sla/outliers.py', 'rally/plugins/task/hook_triggers/__init__.py', 'rally/plugins/task/hook_triggers/periodic.py', 'rally/plugins/task/runners/__init__.py', 'tests/unit/plugins/task/sla/test_outliers.py', 'rally/plugins/common/exporters/junit.py', 'tests/unit/plugins/task/scenarios/requests/__init__.py', 'tests/unit/plugins/task/contexts/test_dummy.py', 'rally/plugins/task/exporters/elastic/__init__.py', 'tests/ci/cover.sh', 'tests/unit/plugins/task/runners/test_serial.py', 'rally/plugins/verification/testr.py', 'tests/unit/plugins/task/sla/test_failure_rate.py', 'rally/plugins/common/verification/reporters.py', 'rally/plugins/task/scenarios/requests/http_requests.py', 'tests/unit/plugins/task/scenarios/requests/test_http_requests.py', 'tests/unit/plugins/verification/__init__.py', 'tests/unit/plugins/task/sla/test_iteration_time.py', 'tests/unit/plugins/task/exporters/junit_report.xml', 'tests/unit/plugins/task/test_types.py', 'tests/unit/plugins/task/scenarios/__init__.py', 'rally/plugins/common/exporters/elastic/exporter.py', 'rally/plugins/task/sla/max_average_duration_per_atomic.py', 'rally/plugins/task/sla/failure_rate.py', 'tests/unit/plugins/task/sla/test_max_average_duration_per_atomic.py', 'tests/unit/plugins/verification/test_reporters.py', 'tests/unit/plugins/task/scenarios/dummy/test_dummy.py', 'rally/plugins/task/contexts/__init__.py', 'rally/plugins/common/exporters/elastic/client.py', 'tests/unit/plugins/task/hooks/__init__.py', 'rally/plugins/common/exporters/trends.py', 'rally/plugins/task/hook_triggers/event.py', 'rally/plugins/common/scenarios/requests/http_requests.py', 'rally/plugins/task/sla/outliers.py', 'tests/unit/plugins/task/__init__.py', 'rally/plugins/task/exporters/json_exporter.py', 'rally/common/sshutils.py', 'rally/plugins/common/exporters/json_exporter.py', 'rally/plugins/task/sla/iteration_time.py', 'rally/plugins/task/sla/performance_degradation.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/4a466f2bc5d57bdb68ee1055f874ad4cc798b990', 'message': 'Restruct rally.plugins.common module\n\nChange-Id: I9df675afdf0a6b76916fba7ed9c3712136ad4780\n'}]",0,714277,4a466f2bc5d57bdb68ee1055f874ad4cc798b990,9,2,3,9545,,,0,"Restruct rally.plugins.common module

Change-Id: I9df675afdf0a6b76916fba7ed9c3712136ad4780
",git fetch https://review.opendev.org/openstack/rally refs/changes/77/714277/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/plugins/task/runners/__init__.py', 'rally/plugins/verification/reporters.py', 'tests/unit/plugins/task/hooks/test_sys_call.py', 'tests/unit/plugins/task/exporters/elastic/test_exporter.py', 'tests/unit/plugins/task/runners/test_constant.py', 'tests/unit/plugins/verification/junit_report.xml', 'rally/plugins/task/exporters/junit.py', 'rally/plugins/task/runners/rps.py', 'tests/unit/plugins/task/scenarios/dummy/__init__.py', 'rally/plugins/common/verification/testr.py', 'tests/unit/plugins/task/sla/test_max_average_duration.py', 'rally/plugins/task/sla/max_average_duration.py', 'rally/plugins/common/sla/performance_degradation.py', 'rally/plugins/task/scenarios/dummy/__init__.py', 'rally/plugins/task/sla/__init__.py', 'tests/unit/plugins/task/exporters/elastic/test_client.py', 'tests/unit/plugins/task/hook_triggers/test_event.py', 'tests/unit/plugins/task/exporters/test_json_exporter.py', 'tests/unit/plugins/task/exporters/test_trends.py', 'rally/plugins/common/sla/failure_rate.py', 'rally/plugins/task/exporters/elastic/flatten.py', 'tests/unit/plugins/task/hook_triggers/test_periodic.py', 'rally/plugins/common/types.py', 'rally/plugins/common/sla/iteration_time.py', 'rally/plugins/task/scenarios/dummy/dummy.py', 'rally/plugins/common/hook/triggers/periodic.py', 'rally/common/logging.py', 'tests/unit/plugins/task/exporters/elastic/__init__.py', 'tests/unit/plugins/task/runners/test_rps.py', 'tests/unit/plugins/task/sla/test_performance_degradation.py', 'rally/plugins/common/exporters/html.py', 'rally/plugins/common/runners/rps.py', 'tests/unit/plugins/task/hook_triggers/__init__.py', 'rally/plugins/task/hooks/sys_call.py', 'rally/plugins/task/hooks/__init__.py', 'rally/plugins/task/runners/serial.py', 'rally/plugins/task/exporters/elastic/client.py', 'tests/unit/plugins/task/exporters/__init__.py', 'rally/plugins/common/hook/sys_call.py', 'CHANGELOG.rst', 'rally/plugins/task/exporters/trends.py', 'tests/unit/plugins/verification/test_testr.py', 'rally/plugins/task/scenarios/requests/utils.py', 'rally/plugins/task/runners/constant.py', 'rally/plugins/task/exporters/html.py', 'rally/plugins/common/runners/serial.py', 'tests/unit/task/test_runner.py', 'rally/plugins/task/contexts/dummy.py', 'rally/plugins/task/scenarios/__init__.py', 'rally/plugins/task/scenarios/requests/__init__.py', 'tests/unit/plugins/task/scenarios/requests/test_utils.py', 'rally/plugins/common/runners/constant.py', 'rally/plugins/common/scenarios/requests/utils.py', 'rally/plugins/task/exporters/__init__.py', 'rally/plugins/__init__.py', 'rally/plugins/common/sla/max_average_duration_per_atomic.py', 'tests/unit/plugins/task/sla/__init__.py', 'rally/plugins/common/exporters/elastic/flatten.py', 'tests/unit/plugins/task/exporters/test_junit.py', 'tests/unit/plugins/task/exporters/test_html.py', 'rally/plugins/task/exporters/elastic/exporter.py', 'rally/plugins/common/hook/triggers/event.py', 'rally/plugins/common/sla/max_average_duration.py', 'tests/unit/plugins/task/contexts/__init__.py', 'rally/plugins/task/types.py', 'tests/unit/plugins/task/exporters/elastic/test_flatten.py', 'rally/plugins/common/sla/outliers.py', 'rally/plugins/task/hook_triggers/__init__.py', 'rally/plugins/task/hook_triggers/periodic.py', 'rally/plugins/task/runners/__init__.py', 'tests/unit/plugins/task/sla/test_outliers.py', 'rally/plugins/common/exporters/junit.py', 'tests/unit/plugins/task/scenarios/requests/__init__.py', 'tests/unit/plugins/task/contexts/test_dummy.py', 'rally/plugins/task/exporters/elastic/__init__.py', 'tests/unit/plugins/task/runners/test_serial.py', 'rally/plugins/verification/testr.py', 'tests/unit/plugins/task/sla/test_failure_rate.py', 'rally/plugins/common/verification/reporters.py', 'rally/plugins/task/scenarios/requests/http_requests.py', 'tests/unit/plugins/task/scenarios/requests/test_http_requests.py', 'tests/unit/plugins/verification/__init__.py', 'tests/unit/plugins/task/sla/test_iteration_time.py', 'tests/unit/plugins/task/exporters/junit_report.xml', 'tests/unit/plugins/task/test_types.py', 'tests/unit/plugins/task/scenarios/__init__.py', 'rally/plugins/common/exporters/elastic/exporter.py', 'rally/plugins/task/sla/max_average_duration_per_atomic.py', 'rally/plugins/task/sla/failure_rate.py', 'tests/unit/plugins/task/sla/test_max_average_duration_per_atomic.py', 'tests/unit/plugins/verification/test_reporters.py', 'tests/unit/plugins/task/scenarios/dummy/test_dummy.py', 'rally/plugins/task/contexts/__init__.py', 'rally/plugins/common/exporters/elastic/client.py', 'tests/unit/plugins/task/hooks/__init__.py', 'rally/plugins/common/exporters/trends.py', 'rally/plugins/task/hook_triggers/event.py', 'rally/plugins/common/scenarios/requests/http_requests.py', 'rally/plugins/task/sla/outliers.py', 'tests/unit/plugins/task/__init__.py', 'rally/plugins/task/exporters/json_exporter.py', 'rally/common/sshutils.py', 'rally/plugins/common/exporters/json_exporter.py', 'rally/plugins/task/sla/iteration_time.py', 'rally/plugins/task/sla/performance_degradation.py']",105,28ddad95c90693deef8973b731d199da4ec6e79b,deprecate,"# Copyright 2016: Mirantis Inc. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """""" SLA (Service-level agreement) is set of details for determining compliance with contracted values such as maximum error rate or minimum response time. """""" from __future__ import division from rally.common import streaming_algorithms from rally import consts from rally.task import sla from rally.utils import strutils @sla.configure(name=""performance_degradation"") class PerformanceDegradation(sla.SLA): """"""Calculates performance degradation based on iteration time This SLA plugin finds minimum and maximum duration of iterations completed without errors during Rally task execution. Assuming that minimum duration is 100%, it calculates performance degradation against maximum duration. """""" CONFIG_SCHEMA = { ""type"": ""object"", ""$schema"": consts.JSON_SCHEMA7, ""properties"": { ""max_degradation"": { ""type"": ""number"", ""minimum"": 0.0, }, }, ""required"": [ ""max_degradation"", ], ""additionalProperties"": False, } def __init__(self, criterion_value): super(PerformanceDegradation, self).__init__(criterion_value) self.max_degradation = self.criterion_value[""max_degradation""] self.degradation = streaming_algorithms.DegradationComputation() def add_iteration(self, iteration): if not iteration.get(""error""): self.degradation.add(iteration[""duration""]) self.success = self.degradation.result() <= self.max_degradation return self.success def merge(self, other): self.degradation.merge(other.degradation) self.success = self.degradation.result() <= self.max_degradation return self.success def details(self): res = strutils.format_float_to_str(self.degradation.result() or 0.0) return ""Current degradation: %s%% - %s"" % (res, self.status()) ",,3331,2713
openstack%2Foslo.privsep~master~Ia97ecf965d807f12524d5b6602446934b5813ce6,openstack/oslo.privsep,master,Ia97ecf965d807f12524d5b6602446934b5813ce6,Make compatible with msgpack 1.0.0,MERGED,2020-03-17 17:36:48.000000000,2020-03-21 22:25:50.000000000,2020-03-21 22:24:33.000000000,"[{'_account_id': 6928}, {'_account_id': 9531}, {'_account_id': 11904}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 17:36:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.privsep/commit/f47d64ce7d8b593b4cbe1b72bd25cd6c006c72c7', 'message': ""Make compatible with msgpack 1.0.0\n\nThere are a couple of things that changed in msgpack 1.0.0 that were\nbreaking oslo.privsep:\n\n1) The encoding parameter to Unpacker was removed. This has been\n   deprecated for a while in favor of the `raw` parameter[0], so this\n   change switches to using raw.\n\n2) The strict_map_key parameter default was changed from False to\n   True.[1] I haven't found an explanation of why this was done, but\n   we can explicitly set it False to maintain the previous behavior.\n\nChange-Id: Ia97ecf965d807f12524d5b6602446934b5813ce6\nCloses-Bug: 1855914\nCloses-Bug: 1864811\n0: https://msgpack-python.readthedocs.io/en/latest/api.html#msgpack.Unpacker\n1: https://github.com/msgpack/msgpack-python/pull/392/commits/6e1d12c0a2072572890cbb124b94cb08b582dd2d\n""}, {'number': 2, 'created': '2020-03-17 21:04:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.privsep/commit/542d79ad6638739094368c53f2c9235b6b477c8b', 'message': ""Make compatible with msgpack 1.0.0\n\nThere are a couple of things that changed in msgpack 1.0.0 that were\nbreaking oslo.privsep:\n\n1) The encoding parameter to Unpacker was removed. This has been\n   deprecated for a while in favor of the `raw` parameter[0], so this\n   change switches to using raw.\n\n2) The strict_map_key parameter default was changed from False to\n   True.[1] I haven't found an explanation of why this was done, but\n   we can explicitly set it False to maintain the previous behavior.\n\nChange-Id: Ia97ecf965d807f12524d5b6602446934b5813ce6\nCloses-Bug: 1855914\nCloses-Bug: 1864811\n0: https://msgpack-python.readthedocs.io/en/latest/api.html#msgpack.Unpacker\n1: https://github.com/msgpack/msgpack-python/pull/392/commits/6e1d12c0a2072572890cbb124b94cb08b582dd2d\n""}, {'number': 3, 'created': '2020-03-18 21:02:52.000000000', 'files': ['oslo_privsep/comm.py'], 'web_link': 'https://opendev.org/openstack/oslo.privsep/commit/f19765c683ce847f8d0ae0499d9e67c41114c0b7', 'message': ""Make compatible with msgpack 1.0.0\n\nThere are a couple of things that changed in msgpack 1.0.0 that were\nbreaking oslo.privsep:\n\n1) The encoding parameter to Unpacker was removed. This has been\n   deprecated for a while in favor of the `raw` parameter[0], so this\n   change switches to using raw.\n\n2) The strict_map_key parameter default was changed from False to\n   True.[1] I haven't found an explanation of why this was done, but\n   we can explicitly set it False to maintain the previous behavior.\n\nChange-Id: Ia97ecf965d807f12524d5b6602446934b5813ce6\nCloses-Bug: 1855914\nCloses-Bug: 1864811\n0: https://msgpack-python.readthedocs.io/en/latest/api.html#msgpack.Unpacker\n1: https://github.com/msgpack/msgpack-python/pull/392/commits/6e1d12c0a2072572890cbb124b94cb08b582dd2d\n""}]",0,713496,f19765c683ce847f8d0ae0499d9e67c41114c0b7,14,5,3,6928,,,0,"Make compatible with msgpack 1.0.0

There are a couple of things that changed in msgpack 1.0.0 that were
breaking oslo.privsep:

1) The encoding parameter to Unpacker was removed. This has been
   deprecated for a while in favor of the `raw` parameter[0], so this
   change switches to using raw.

2) The strict_map_key parameter default was changed from False to
   True.[1] I haven't found an explanation of why this was done, but
   we can explicitly set it False to maintain the previous behavior.

Change-Id: Ia97ecf965d807f12524d5b6602446934b5813ce6
Closes-Bug: 1855914
Closes-Bug: 1864811
0: https://msgpack-python.readthedocs.io/en/latest/api.html#msgpack.Unpacker
1: https://github.com/msgpack/msgpack-python/pull/392/commits/6e1d12c0a2072572890cbb124b94cb08b582dd2d
",git fetch https://review.opendev.org/openstack/oslo.privsep refs/changes/96/713496/1 && git format-patch -1 --stdout FETCH_HEAD,['oslo_privsep/comm.py'],1,f47d64ce7d8b593b4cbe1b72bd25cd6c006c72c7,bug/1855914," self.unpacker = msgpack.Unpacker(use_list=False, raw=False, strict_map_key=False,"," self.unpacker = msgpack.Unpacker(use_list=False, encoding='utf-8',",2,1
openstack%2Foslo.privsep~master~I69dae65d3e0a40bb2304d74de078ab84fc778d58,openstack/oslo.privsep,master,I69dae65d3e0a40bb2304d74de078ab84fc778d58,Bring sanity to lower-constraints,MERGED,2020-03-17 21:04:45.000000000,2020-03-21 22:19:59.000000000,2020-03-21 22:18:55.000000000,"[{'_account_id': 11904}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 21:04:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.privsep/commit/0cc6db66e64b502c8fb9f4302d26b3a44732bcf6', 'message': ""Bring sanity to lower-constraints\n\nThis is all necessary to bump the lower-constraint on msgpack so we\ncan support msgpack 1.0 without version-specific logic in our code.\n\nThe reasons for the changes are as follows:\n\n* greenlet and PyYAML bumped because the previous lower-constraints\n  for those didn't install on python 3.7, which is a supported Python\n  version.\n* msgpack-python is removed. It was replaced by msgpack and there is\n  no new enough release of it to satisfy the needed minimum version bump.\n* oslo.serialization is bumped because the old version was pulling in\n  msgpack-python, which overwrote our new needed version of msgpack.\n* I went ahead and included the msgpack bump to 0.6.0 so we can move\n  forward in the subsequent patch with supporting msgpack 1.0.0.\n\nIt could be argued that this should be included in the msgpack 1.0.0\nchange, but it ended up being complex enough that I thought it was\nworth it to split it out.\n\nChange-Id: I69dae65d3e0a40bb2304d74de078ab84fc778d58\n""}, {'number': 2, 'created': '2020-03-18 21:02:52.000000000', 'files': ['requirements.txt', 'lower-constraints.txt'], 'web_link': 'https://opendev.org/openstack/oslo.privsep/commit/d18643ada7a9337a60fae11baa0359a3714dcc3a', 'message': ""Bring sanity to lower-constraints\n\nThis is all necessary to bump the lower-constraint on msgpack so we\ncan support msgpack 1.0 without version-specific logic in our code.\n\nThe reasons for the changes are as follows:\n\n* greenlet and PyYAML bumped because the previous lower-constraints\n  for those didn't install on python 3.7, which is a supported Python\n  version.\n* msgpack-python is removed. It was replaced by msgpack and there is\n  no new enough release of it to satisfy the needed minimum version bump.\n* oslo.serialization is bumped because the old version was pulling in\n  msgpack-python, which overwrote our new needed version of msgpack.\n* I went ahead and included the msgpack bump to 0.6.0 so we can move\n  forward in the subsequent patch with supporting msgpack 1.0.0.\n\nIt could be argued that this should be included in the msgpack 1.0.0\nchange, but it ended up being complex enough that I thought it was\nworth it to split it out.\n\nChange-Id: I69dae65d3e0a40bb2304d74de078ab84fc778d58\n""}]",0,713538,d18643ada7a9337a60fae11baa0359a3714dcc3a,10,3,2,6928,,,0,"Bring sanity to lower-constraints

This is all necessary to bump the lower-constraint on msgpack so we
can support msgpack 1.0 without version-specific logic in our code.

The reasons for the changes are as follows:

* greenlet and PyYAML bumped because the previous lower-constraints
  for those didn't install on python 3.7, which is a supported Python
  version.
* msgpack-python is removed. It was replaced by msgpack and there is
  no new enough release of it to satisfy the needed minimum version bump.
* oslo.serialization is bumped because the old version was pulling in
  msgpack-python, which overwrote our new needed version of msgpack.
* I went ahead and included the msgpack bump to 0.6.0 so we can move
  forward in the subsequent patch with supporting msgpack 1.0.0.

It could be argued that this should be included in the msgpack 1.0.0
change, but it ended up being complex enough that I thought it was
worth it to split it out.

Change-Id: I69dae65d3e0a40bb2304d74de078ab84fc778d58
",git fetch https://review.opendev.org/openstack/oslo.privsep refs/changes/38/713538/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'lower-constraints.txt']",2,0cc6db66e64b502c8fb9f4302d26b3a44732bcf6,bug/1855914,greenlet==0.4.14msgpack==0.6.0oslo.serialization==2.24.0PyYAML==3.13,greenlet==0.4.10msgpack-python==0.4.0 msgpack==0.5.0oslo.serialization==2.18.0PyYAML==3.12,5,6
openstack%2Fcinder~stable%2Ftrain~I1198c0d07a925d64a4e77b80f4bc22764bfa2f11,openstack/cinder,stable/train,I1198c0d07a925d64a4e77b80f4bc22764bfa2f11,Add new license scheme for Flashsystem9000 series,MERGED,2020-03-12 05:34:25.000000000,2020-03-21 22:18:24.000000000,2020-03-21 22:17:00.000000000,"[{'_account_id': 5997}, {'_account_id': 7198}, {'_account_id': 10118}, {'_account_id': 11611}, {'_account_id': 11904}, {'_account_id': 12369}, {'_account_id': 18883}, {'_account_id': 19933}, {'_account_id': 21863}, {'_account_id': 21976}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23613}, {'_account_id': 24921}, {'_account_id': 26537}, {'_account_id': 29716}]","[{'number': 1, 'created': '2020-03-12 05:34:25.000000000', 'files': ['cinder/tests/unit/volume/drivers/ibm/test_storwize_svc.py', 'cinder/volume/drivers/ibm/storwize_svc/storwize_svc_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/905bca2b7df95789c3e5cd6591bdaa8188f0a424', 'message': 'Add new license scheme for Flashsystem9000 series\n\nWith this change, cinder driver can create compressed volume on DRP.\n\nCloses-bug: #1863578\n\nChange-Id: I1198c0d07a925d64a4e77b80f4bc22764bfa2f11\n(cherry picked from commit 5c2f839a02f94df858903db7bf166d8cdad07e94)\n'}]",0,712613,905bca2b7df95789c3e5cd6591bdaa8188f0a424,24,17,1,30428,,,0,"Add new license scheme for Flashsystem9000 series

With this change, cinder driver can create compressed volume on DRP.

Closes-bug: #1863578

Change-Id: I1198c0d07a925d64a4e77b80f4bc22764bfa2f11
(cherry picked from commit 5c2f839a02f94df858903db7bf166d8cdad07e94)
",git fetch https://review.opendev.org/openstack/cinder refs/changes/13/712613/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/unit/volume/drivers/ibm/test_storwize_svc.py', 'cinder/volume/drivers/ibm/storwize_svc/storwize_svc_common.py']",2,905bca2b7df95789c3e5cd6591bdaa8188f0a424,bug/1863578-stable/train," if resp.get('license_scheme', '0') == 'flex': return True",,7,2
openstack%2Ftripleo-heat-templates~master~I561c52ce09c66a7f79763c59cd25f15949c054af,openstack/tripleo-heat-templates,master,I561c52ce09c66a7f79763c59cd25f15949c054af,Switch to Podman by default,MERGED,2019-02-12 21:37:12.000000000,2020-03-21 22:14:50.000000000,2020-03-21 22:14:50.000000000,"[{'_account_id': 3153}, {'_account_id': 4571}, {'_account_id': 6926}, {'_account_id': 7144}, {'_account_id': 8449}, {'_account_id': 9592}, {'_account_id': 11082}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2019-02-12 21:37:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/45d0eecc66284d84a0183c165430844b9f8492c9', 'message': 'Drop Docker services from Overcloud roles\n\nChange-Id: I561c52ce09c66a7f79763c59cd25f15949c054af\n'}, {'number': 2, 'created': '2019-05-13 14:24:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/62146f2d88ffc3299364daa0c86eb50ccc59b987', 'message': 'Drop Docker services from Overcloud roles\n\nChange-Id: I561c52ce09c66a7f79763c59cd25f15949c054af\n'}, {'number': 3, 'created': '2020-03-12 20:38:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/69c865ee9b2e7abeffbe9704f1500848c571a8d7', 'message': 'Switch to Podman by default\n\n- Remove Docker service from all the roles; not needed anymore\n- Switch ContainerCli to podman for docker-ha environment. Note; this\n  environment might be renamed at some point to, container-ha.yaml. But\n  for backward compatibility we still use it now.\n  Also switch EnablePaunch to false since we were waiting for the podman\n  switch to do it.\n- In the overcloud registry, disable Docker by default and enable Podman\n  by default.\n\nThis patch will only work for centos8/rhel8 based deployments.\n\nChange-Id: I561c52ce09c66a7f79763c59cd25f15949c054af\n'}, {'number': 4, 'created': '2020-03-12 20:39:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/844d63f36c6f5969dfa00389ec237e71eb494aab', 'message': 'Switch to Podman by default\n\n- Remove Docker service from all the roles; not needed anymore\n- Switch ContainerCli to podman for docker-ha environment. Note; this\n  environment might be renamed at some point to, container-ha.yaml. But\n  for backward compatibility we still use it now.\n  Also switch EnablePaunch to false since we were waiting for the podman\n  switch to do it.\n- In the overcloud registry, disable Docker by default and enable Podman\n  by default.\n\nThis patch will only work for centos8/rhel8 based deployments.\n\nChange-Id: I561c52ce09c66a7f79763c59cd25f15949c054af\n'}, {'number': 5, 'created': '2020-03-16 19:34:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/b46e561520d1aab713461c3ad72b974326b950a0', 'message': 'Switch to Podman by default\n\n- Remove Docker service from all the roles; not needed anymore\n- Switch ContainerCli to podman for docker-ha environment. Note; this\n  environment might be renamed at some point to, container-ha.yaml. But\n  for backward compatibility we still use it now.\n  Also switch EnablePaunch to false since we were waiting for the podman\n  switch to do it.\n- In the overcloud registry, disable Docker by default and enable Podman\n  by default.\n\nThis patch will only work for centos8/rhel8 based deployments.\n\nChange-Id: I561c52ce09c66a7f79763c59cd25f15949c054af\n'}, {'number': 6, 'created': '2020-03-18 13:27:41.000000000', 'files': ['roles/DistributedComputeScaleOut.yaml', 'roles/ComputeOvsDpdk.yaml', 'roles_data.yaml', 'roles/ControllerStorageDashboard.yaml', 'roles/HciCephMon.yaml', 'roles/ComputeDVR.yaml', 'roles/ControllerAllNovaStandalone.yaml', 'roles/CephObject.yaml', 'roles/Controller.yaml', 'roles/ComputePPC64LE.yaml', 'roles/ComputeRealTime.yaml', 'roles/CephFile.yaml', 'roles/README.rst', 'roles/ComputeSriovIB.yaml', 'roles/ControllerOpenstack.yaml', 'roles/Standalone.yaml', 'roles/HciCephFile.yaml', 'ci/environments/scenario001-multinode-containers.yaml', 'roles/ComputeInstanceHA.yaml', 'roles/DistributedCompute.yaml', 'roles/ComputeLocalEphemeral.yaml', 'roles/BlockStorage.yaml', 'roles/DistributedComputeHCIScaleOut.yaml', 'roles/Compute.yaml', 'roles/ComputeLiquidio.yaml', 'roles/ComputeHCI.yaml', 'roles/ComputeHCISriov.yaml', 'roles/ComputeOvsDpdkSriovRT.yaml', 'roles/CellController.yaml', 'ci/environments/multinode-containers.yaml', 'roles/ComputeRBDEphemeral.yaml', 'ci/environments/scenario010-multinode-containers.yaml', 'roles/ControllerNovaStandalone.yaml', 'roles/HciCephAll.yaml', 'roles/HciCephObject.yaml', 'roles/ComputeOvsDpdkRT.yaml', 'environments/docker-ha.yaml', 'ci/environments/scenario007-multinode-containers.yaml', 'roles/CephStorage.yaml', 'roles/ComputeSriov.yaml', 'roles/ComputeSriovRT.yaml', 'environments/hyperconverged-ceph.yaml', 'roles/DistributedComputeHCI.yaml', 'roles/ControllerStorageNfs.yaml', 'roles/Messaging.yaml', 'roles/CephAll.yaml', 'roles/Novacontrol.yaml', 'roles/ComputeOvsDpdkSriov.yaml', 'roles/Telemetry.yaml', 'ci/environments/scenario004-standalone.yaml', 'roles/ComputeHCIOvsDpdk.yaml', 'roles/ControllerNoCeph.yaml', 'roles/Database.yaml', 'overcloud-resource-registry-puppet.j2.yaml', 'roles/ObjectStorage.yaml', 'roles/IronicConductor.yaml', 'roles/Networker.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/ac2711c72bcfc87635bd7bdfe44d67b96296e957', 'message': 'Switch to Podman by default\n\n- Remove Docker service from all the roles; not needed anymore\n- Switch ContainerCli to podman for docker-ha environment. Note; this\n  environment might be renamed at some point to, container-ha.yaml. But\n  for backward compatibility we still use it now.\n  Also switch EnablePaunch to false since we were waiting for the podman\n  switch to do it.\n- In the overcloud registry, disable Docker by default and enable Podman\n  by default.\n\nThis patch will only work for centos8/rhel8 based deployments.\n\nChange-Id: I561c52ce09c66a7f79763c59cd25f15949c054af\n'}]",0,636441,ac2711c72bcfc87635bd7bdfe44d67b96296e957,52,10,6,360,,,0,"Switch to Podman by default

- Remove Docker service from all the roles; not needed anymore
- Switch ContainerCli to podman for docker-ha environment. Note; this
  environment might be renamed at some point to, container-ha.yaml. But
  for backward compatibility we still use it now.
  Also switch EnablePaunch to false since we were waiting for the podman
  switch to do it.
- In the overcloud registry, disable Docker by default and enable Podman
  by default.

This patch will only work for centos8/rhel8 based deployments.

Change-Id: I561c52ce09c66a7f79763c59cd25f15949c054af
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/41/636441/6 && git format-patch -1 --stdout FETCH_HEAD,"['roles/ControllerNovaStandalone.yaml', 'roles/HciCephAll.yaml', 'roles/HciCephObject.yaml', 'roles/ComputeOvsDpdkRT.yaml', 'roles/ComputeOvsDpdk.yaml', 'roles_data.yaml', 'roles/OpenShiftAllInOne.yaml', 'roles/HciCephMon.yaml', 'roles/OpenShiftMaster.yaml', 'roles/OpenShiftWorker.yaml', 'roles/ComputeDVR.yaml', 'roles/ControllerAllNovaStandalone.yaml', 'roles/CephStorage.yaml', 'roles/ComputeSriov.yaml', 'roles/CephObject.yaml', 'roles/ComputeSriovRT.yaml', 'roles/Controller.yaml', 'roles/ComputePPC64LE.yaml', 'roles/ComputeRealTime.yaml', 'roles/CephFile.yaml', 'roles/README.rst', 'roles/ControllerOpenstack.yaml', 'roles/DistributedComputeHCI.yaml', 'roles/ControllerStorageNfs.yaml', 'roles/HciCephFile.yaml', 'roles/ComputeInstanceHA.yaml', 'roles/DistributedCompute.yaml', 'roles/Messaging.yaml', 'roles/CephAll.yaml', 'roles/Novacontrol.yaml', 'roles/BlockStorage.yaml', 'roles/ComputeOvsDpdkSriov.yaml', 'roles/Compute.yaml', 'roles/OpenShiftInfra.yaml', 'roles/Telemetry.yaml', 'roles/ComputeLiquidio.yaml', 'roles/ComputeHCI.yaml', 'roles/ControllerNoCeph.yaml', 'roles/ComputeOvsDpdkSriovRT.yaml', 'roles/Database.yaml', 'roles/ObjectStorage.yaml', 'roles/IronicConductor.yaml', 'roles/Networker.yaml']",43,45d0eecc66284d84a0183c165430844b9f8492c9,tripleo/drop_docker,, - OS::TripleO::Services::Docker,0,47
openstack%2Foslo.privsep~master~I5de22b72059133b05d64be47f4c1d3f566b46a6e,openstack/oslo.privsep,master,I5de22b72059133b05d64be47f4c1d3f566b46a6e,Add lock around channel creation,MERGED,2020-02-25 16:02:01.000000000,2020-03-21 22:13:49.000000000,2020-03-21 22:12:37.000000000,"[{'_account_id': 2733}, {'_account_id': 6928}, {'_account_id': 11904}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-02-25 16:02:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.privsep/commit/c53bf93ebbd2aa6b7fd1b1580791f5e5f8a5cf5f', 'message': 'Add lock around channel creation\n\nWhen neutron agents start they process resources in concurrent\nthreads. These can race creating the channel which results in\nredundant privsep-helper processes. This patch fixes this by\nadding a lock to channel creation.\n\nChange-Id: I5de22b72059133b05d64be47f4c1d3f566b46a6e\nCloses-Bug: #1864664\n'}, {'number': 2, 'created': '2020-03-13 17:25:54.000000000', 'files': ['oslo_privsep/tests/test_priv_context.py', 'oslo_privsep/priv_context.py'], 'web_link': 'https://opendev.org/openstack/oslo.privsep/commit/bbe24aa6e08b253303a0f2332e92a7ce209c1afa', 'message': 'Add lock around channel creation\n\nWhen a neutron agent starts up, it processes different resources\nin separate eventlet threads. These can race creating the channel\nwhich results in redundant privsep-helper processes. This patch\nfixes that by adding a lock around channel creation.\n\nChange-Id: I5de22b72059133b05d64be47f4c1d3f566b46a6e\nCloses-Bug: #1864664\n'}]",1,709764,bbe24aa6e08b253303a0f2332e92a7ce209c1afa,13,5,2,2733,,,0,"Add lock around channel creation

When a neutron agent starts up, it processes different resources
in separate eventlet threads. These can race creating the channel
which results in redundant privsep-helper processes. This patch
fixes that by adding a lock around channel creation.

Change-Id: I5de22b72059133b05d64be47f4c1d3f566b46a6e
Closes-Bug: #1864664
",git fetch https://review.opendev.org/openstack/oslo.privsep refs/changes/64/709764/2 && git format-patch -1 --stdout FETCH_HEAD,['oslo_privsep/priv_context.py'],1,c53bf93ebbd2aa6b7fd1b1580791f5e5f8a5cf5f,bug/1864664,import threading self.start_lock = threading.Lock() with self.start_lock: self.start(), self.start(),4,1
openstack%2Foslo.vmware~master~I7b1fd3412b815d88b6d54178566f4089433f98ff,openstack/oslo.vmware,master,I7b1fd3412b815d88b6d54178566f4089433f98ff,Drop use of six,MERGED,2020-03-02 11:13:05.000000000,2020-03-21 22:07:16.000000000,2020-03-21 22:05:59.000000000,"[{'_account_id': 11904}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-02 11:13:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/72cb6393632deac6ff6d0384cf09736437a1388b', 'message': 'Drop use of six\n\nChange-Id: I7b1fd3412b815d88b6d54178566f4089433f98ff\n'}, {'number': 2, 'created': '2020-03-02 13:50:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/d0eca399f470018d9f09caacaec069403ef85af4', 'message': 'Drop use of six\n\nChange-Id: I7b1fd3412b815d88b6d54178566f4089433f98ff\n'}, {'number': 3, 'created': '2020-03-02 13:52:31.000000000', 'files': ['oslo_vmware/exceptions.py', 'oslo_vmware/objects/datastore.py', 'oslo_vmware/tests/objects/test_datastore.py', 'lower-constraints.txt', 'oslo_vmware/api.py', 'oslo_vmware/rw_handles.py', 'oslo_vmware/service.py', 'oslo_vmware/image_transfer.py', 'requirements.txt', 'oslo_vmware/tests/test_api.py', 'oslo_vmware/tests/test_service.py', 'oslo_vmware/tests/test_image_transfer.py', 'oslo_vmware/tests/test_pbm.py', 'oslo_vmware/tests/test_rw_handles.py', 'oslo_vmware/pbm.py'], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/88798e9ad8d251dfd76ff34be622c467b1b2d94a', 'message': 'Drop use of six\n\nChange-Id: I7b1fd3412b815d88b6d54178566f4089433f98ff\n'}]",2,710741,88798e9ad8d251dfd76ff34be622c467b1b2d94a,11,3,3,28522,,,0,"Drop use of six

Change-Id: I7b1fd3412b815d88b6d54178566f4089433f98ff
",git fetch https://review.opendev.org/openstack/oslo.vmware refs/changes/41/710741/1 && git format-patch -1 --stdout FETCH_HEAD,"['oslo_vmware/exceptions.py', 'oslo_vmware/objects/datastore.py', 'oslo_vmware/tests/objects/test_datastore.py', 'lower-constraints.txt', 'oslo_vmware/api.py', 'oslo_vmware/rw_handles.py', 'oslo_vmware/service.py', 'oslo_vmware/image_transfer.py', 'requirements.txt', 'oslo_vmware/tests/test_api.py', 'oslo_vmware/tests/test_service.py', 'oslo_vmware/tests/test_image_transfer.py', 'oslo_vmware/tests/test_pbm.py', 'oslo_vmware/tests/test_rw_handles.py', 'oslo_vmware/pbm.py']",15,72cb6393632deac6ff6d0384cf09736437a1388b,drop-six,import urllib.parse as urlparse import urllib.request as urllib,import six.moves.urllib.parse as urlparse import six.moves.urllib.request as urllib,38,54
openstack%2Foslo.log~master~I0ad972f48ee6bc331b96aa67bd7e69c97fa8e2c3,openstack/oslo.log,master,I0ad972f48ee6bc331b96aa67bd7e69c97fa8e2c3,drop use of six,MERGED,2020-03-02 14:54:24.000000000,2020-03-21 21:57:16.000000000,2020-03-21 21:56:08.000000000,"[{'_account_id': 11904}, {'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-03-02 14:54:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/c0a07b5d870d3b26acab351e4b03359d072aa56e', 'message': 'drop use of six\n\nChange-Id: I0ad972f48ee6bc331b96aa67bd7e69c97fa8e2c3\n'}, {'number': 2, 'created': '2020-03-12 08:28:49.000000000', 'files': ['oslo_log/tests/unit/test_log.py', 'oslo_log/formatters.py', 'oslo_log/handlers.py', 'oslo_log/tests/unit/test_rate_limit.py', 'oslo_log/versionutils.py', 'requirements.txt', 'oslo_log/tests/unit/test_versionutils.py', 'lower-constraints.txt', 'oslo_log/cmds/convert_json.py', 'oslo_log/log.py', 'oslo_log/tests/unit/test_convert_json.py'], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/55ef517065b63e20534bfccd22585a52d1f0f9f1', 'message': 'drop use of six\n\nChange-Id: I0ad972f48ee6bc331b96aa67bd7e69c97fa8e2c3\n'}]",2,710791,55ef517065b63e20534bfccd22585a52d1f0f9f1,13,4,2,28522,,,0,"drop use of six

Change-Id: I0ad972f48ee6bc331b96aa67bd7e69c97fa8e2c3
",git fetch https://review.opendev.org/openstack/oslo.log refs/changes/91/710791/2 && git format-patch -1 --stdout FETCH_HEAD,"['oslo_log/formatters.py', 'oslo_log/tests/unit/test_log.py', 'oslo_log/handlers.py', 'oslo_log/tests/unit/test_rate_limit.py', 'oslo_log/versionutils.py', 'requirements.txt', 'oslo_log/tests/unit/test_versionutils.py', 'lower-constraints.txt', 'oslo_log/cmds/convert_json.py', 'oslo_log/log.py', 'oslo_log/tests/unit/test_convert_json.py']",11,c0a07b5d870d3b26acab351e4b03359d072aa56e,drop-six,import io fh = io.StringIO(text),import six fh = six.StringIO(text),73,152
openstack%2Fnova~stable%2Frocky~Icd7ab2ca4ddbed92c7e883a63a23245920d961e7,openstack/nova,stable/rocky,Icd7ab2ca4ddbed92c7e883a63a23245920d961e7,nova-live-migration: Wait for n-cpu services to come up after configuring Ceph,MERGED,2020-03-19 11:05:42.000000000,2020-03-21 21:21:18.000000000,2020-03-21 21:21:17.000000000,"[{'_account_id': 4690}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-03-19 11:05:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8b89fdab746228766385b5b2c2b3d918e84862d8', 'message': 'nova-live-migration: Wait for n-cpu services to come up after configuring Ceph\n\nPreviously the ceph.sh script used during the nova-live-migration job\nwould only grep for a `compute` process when checking if the services\nhad been restarted. This check was bogus and would always return 0 as it\nwould always match itself. For example:\n\n2020-03-13 21:06:47.682073 | primary | 2020-03-13 21:06:47.681 | root\n29529  0.0  0.0   4500   736 pts/0    S+   21:06   0:00 /bin/sh -c ps\n       aux | grep compute\n2020-03-13 21:06:47.683964 | primary | 2020-03-13 21:06:47.683 | root\n29531  0.0  0.0  14616   944 pts/0    S+   21:06   0:00 grep compute\n\nFailures of this job were seen on the stable/pike branch where slower CI\nnodes appeared to struggle to allow Libvirt to report to n-cpu in time\nbefore Tempest was started. This in-turn caused instance build failures\nand the overall failure of the job.\n\nThis change resolves this issue by switching to pgrep and ensuring\nn-cpu services are reported as fully up after a cold restart before\nstarting the Tempest test run.\n\nCloses-Bug: 1867380\nChange-Id: Icd7ab2ca4ddbed92c7e883a63a23245920d961e7\n(cherry picked from commit e23c3c2c8df3843c5853c87ef684bd21c4af95d8)\n(cherry picked from commit 70447bca2f4f33c6872eaf94a2e4351bb257c22a)\n(cherry picked from commit 373c4ffde2053c7ff11bd38339b88d144cd442f2)\n'}, {'number': 2, 'created': '2020-03-20 12:30:41.000000000', 'files': ['nova/tests/live_migration/hooks/ceph.sh'], 'web_link': 'https://opendev.org/openstack/nova/commit/63ed32ef49adcb6830ef3b5329a561542bddf656', 'message': 'nova-live-migration: Wait for n-cpu services to come up after configuring Ceph\n\nPreviously the ceph.sh script used during the nova-live-migration job\nwould only grep for a `compute` process when checking if the services\nhad been restarted. This check was bogus and would always return 0 as it\nwould always match itself. For example:\n\n2020-03-13 21:06:47.682073 | primary | 2020-03-13 21:06:47.681 | root\n29529  0.0  0.0   4500   736 pts/0    S+   21:06   0:00 /bin/sh -c ps\n       aux | grep compute\n2020-03-13 21:06:47.683964 | primary | 2020-03-13 21:06:47.683 | root\n29531  0.0  0.0  14616   944 pts/0    S+   21:06   0:00 grep compute\n\nFailures of this job were seen on the stable/pike branch where slower CI\nnodes appeared to struggle to allow Libvirt to report to n-cpu in time\nbefore Tempest was started. This in-turn caused instance build failures\nand the overall failure of the job.\n\nThis change resolves this issue by switching to pgrep and ensuring\nn-cpu services are reported as fully up after a cold restart before\nstarting the Tempest test run.\n\nCloses-Bug: 1867380\nChange-Id: Icd7ab2ca4ddbed92c7e883a63a23245920d961e7\n(cherry picked from commit e23c3c2c8df3843c5853c87ef684bd21c4af95d8)\n(cherry picked from commit 70447bca2f4f33c6872eaf94a2e4351bb257c22a)\n(cherry picked from commit 373c4ffde2053c7ff11bd38339b88d144cd442f2)\n'}]",0,713840,63ed32ef49adcb6830ef3b5329a561542bddf656,15,5,2,10135,,,0,"nova-live-migration: Wait for n-cpu services to come up after configuring Ceph

Previously the ceph.sh script used during the nova-live-migration job
would only grep for a `compute` process when checking if the services
had been restarted. This check was bogus and would always return 0 as it
would always match itself. For example:

2020-03-13 21:06:47.682073 | primary | 2020-03-13 21:06:47.681 | root
29529  0.0  0.0   4500   736 pts/0    S+   21:06   0:00 /bin/sh -c ps
       aux | grep compute
2020-03-13 21:06:47.683964 | primary | 2020-03-13 21:06:47.683 | root
29531  0.0  0.0  14616   944 pts/0    S+   21:06   0:00 grep compute

Failures of this job were seen on the stable/pike branch where slower CI
nodes appeared to struggle to allow Libvirt to report to n-cpu in time
before Tempest was started. This in-turn caused instance build failures
and the overall failure of the job.

This change resolves this issue by switching to pgrep and ensuring
n-cpu services are reported as fully up after a cold restart before
starting the Tempest test run.

Closes-Bug: 1867380
Change-Id: Icd7ab2ca4ddbed92c7e883a63a23245920d961e7
(cherry picked from commit e23c3c2c8df3843c5853c87ef684bd21c4af95d8)
(cherry picked from commit 70447bca2f4f33c6872eaf94a2e4351bb257c22a)
(cherry picked from commit 373c4ffde2053c7ff11bd38339b88d144cd442f2)
",git fetch https://review.opendev.org/openstack/nova refs/changes/40/713840/2 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/live_migration/hooks/ceph.sh'],1,8b89fdab746228766385b5b2c2b3d918e84862d8,bug/1867380,"function _wait_for_nova_compute_service_state { source $BASE/new/devstack/openrc admin admin local status=$1 local attempt=1 local max_attempts=24 local attempt_sleep=5 local computes_count=$(openstack compute service list | grep -c nova-compute) local computes_ready=$(openstack compute service list | grep nova-compute | grep $status | wc -l) echo ""Waiting for $computes_count computes to report as $status"" while [ ""$computes_ready"" -ne ""$computes_count"" ]; do if [ ""$attempt"" -eq ""$max_attempts"" ]; then echo ""Failed waiting for computes to report as ${status}, ${computes_ready}/${computes_count} ${status} after ${max_attempts} attempts"" exit 4 fi echo ""Waiting ${attempt_sleep} seconds for ${computes_count} computes to report as ${status}, ${computes_ready}/${computes_count} ${status} after ${attempt}/${max_attempts} attempts"" sleep $attempt_sleep attempt=$((attempt+1)) computes_ready=$(openstack compute service list | grep nova-compute | grep $status | wc -l) done echo ""All computes are now reporting as ${status} after ${attempt} attempts"" } function configure_and_start_nova { echo ""Checking all n-cpu services"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""pgrep -u stack -a nova-compute"" # stop nova-compute echo ""Stopping all n-cpu services"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""systemctl stop devstack@n-cpu"" # Wait for the service to be marked as down _wait_for_nova_compute_service_state ""down"" # start nova-compute echo ""Starting all n-cpu services"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""systemctl start devstack@n-cpu"" echo ""Checking all n-cpu services"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""pgrep -u stack -a nova-compute"" # Wait for the service to be marked as up _wait_for_nova_compute_service_state ""up""","function configure_and_start_nova { echo 'check compute processes before restart' $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""ps aux | grep compute"" # restart nova-compute $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""systemctl restart devstack@n-cpu"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""ps aux | grep compute"" ",42,5
openstack%2Fnova~stable%2Frocky~I40f40766a7b84423c1dcf9d5ed58476b86d61cc4,openstack/nova,stable/rocky,I40f40766a7b84423c1dcf9d5ed58476b86d61cc4,Replace ansible --sudo with --become in live_migration/hooks scripts,MERGED,2020-03-19 11:05:42.000000000,2020-03-21 21:21:13.000000000,2020-03-21 21:21:13.000000000,"[{'_account_id': 6873}, {'_account_id': 11904}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-03-19 11:05:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f077fbfae21d8ec33c7944362fee33a5b359da71', 'message': 'Replace ansible --sudo with --become in live_migration/hooks scripts\n\nAnsible deprecated --sudo in 1.9 so this change replaces\nit with --become.\n\nChange-Id: I40f40766a7b84423c1dcf9d5ed58476b86d61cc4\n(cherry picked from commit 7f16800f71f6124736382be51d9da234800f7618)\n'}, {'number': 2, 'created': '2020-03-20 12:30:41.000000000', 'files': ['nova/tests/live_migration/hooks/ceph.sh', 'nova/tests/live_migration/hooks/nfs.sh', 'nova/tests/live_migration/hooks/run_tests.sh'], 'web_link': 'https://opendev.org/openstack/nova/commit/18931544d8a57953c6ce9ee4bf4bcc7a4e9e4295', 'message': 'Replace ansible --sudo with --become in live_migration/hooks scripts\n\nAnsible deprecated --sudo in 1.9 so this change replaces\nit with --become.\n\nNOTE(lyarwood): An additional --sudo is replaced in ceph.sh due to\nI8da38aec0fe4808273b8587ace3df9dbbc3ab576 removing this call in\nstable/stein.\n\nChange-Id: I40f40766a7b84423c1dcf9d5ed58476b86d61cc4\n(cherry picked from commit 7f16800f71f6124736382be51d9da234800f7618)\n'}]",1,713839,18931544d8a57953c6ce9ee4bf4bcc7a4e9e4295,15,6,2,10135,,,0,"Replace ansible --sudo with --become in live_migration/hooks scripts

Ansible deprecated --sudo in 1.9 so this change replaces
it with --become.

NOTE(lyarwood): An additional --sudo is replaced in ceph.sh due to
I8da38aec0fe4808273b8587ace3df9dbbc3ab576 removing this call in
stable/stein.

Change-Id: I40f40766a7b84423c1dcf9d5ed58476b86d61cc4
(cherry picked from commit 7f16800f71f6124736382be51d9da234800f7618)
",git fetch https://review.opendev.org/openstack/nova refs/changes/39/713839/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/live_migration/hooks/ceph.sh', 'nova/tests/live_migration/hooks/nfs.sh', 'nova/tests/live_migration/hooks/run_tests.sh']",3,f077fbfae21d8ec33c7944362fee33a5b359da71,bug/1867380,"$ANSIBLE primary --become -f 5 -i ""$WORKSPACE/inventory"" -m ini_file -a ""dest=$BASE/new/tempest/etc/tempest.conf section=compute-feature-enabled option=block_migration_for_live_migration value=False""","$ANSIBLE primary --sudo -f 5 -i ""$WORKSPACE/inventory"" -m ini_file -a ""dest=$BASE/new/tempest/etc/tempest.conf section=compute-feature-enabled option=block_migration_for_live_migration value=False""",58,58
openstack%2Fhorizon~stable%2Fqueens~I519e15afc975e6da2afb9c72a05448541572bd10,openstack/horizon,stable/queens,I519e15afc975e6da2afb9c72a05448541572bd10,Fix typo in publicize_image policy name,MERGED,2020-01-09 16:29:12.000000000,2020-03-21 21:07:10.000000000,2020-03-21 21:05:30.000000000,"[{'_account_id': 841}, {'_account_id': 1736}, {'_account_id': 11904}, {'_account_id': 22348}, {'_account_id': 27822}, {'_account_id': 29313}]","[{'number': 1, 'created': '2020-01-09 16:29:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/ab959f9d1753c309f205dd7a231e3dad6bba95e3', 'message': 'Fix typo in publicize_image policy name\n\nThis patch is not cherry-picked from stable/train because if was fixed\nin a scope of a new feature implementation with\nhttps://review.opendev.org/#/c/602468/ commit.\n\nChange-Id: I519e15afc975e6da2afb9c72a05448541572bd10\nCloses-Bug: 1859041\n(cherry picked from commit 1fdde52f0b46963703ca786b45d1efa66d0e1f6e)\n(cherry picked from commit 72c56ac8e13466062b35472209e8c2814cfd63bd)\n'}, {'number': 2, 'created': '2020-01-17 11:14:15.000000000', 'files': ['openstack_dashboard/static/app/core/images/steps/edit-image/edit-image.controller.js', 'releasenotes/notes/publicize-image-policy-name-5d7fd5ecbdcfa893.yaml'], 'web_link': 'https://opendev.org/openstack/horizon/commit/e3f40725a4f16140f1c423bc9456b670b9bb76b9', 'message': 'Fix typo in publicize_image policy name\n\nThis patch is not cherry-picked from stable/train because if was fixed\nin a scope of a new feature implementation with\nhttps://review.opendev.org/#/c/602468/ commit.\n\nChange-Id: I519e15afc975e6da2afb9c72a05448541572bd10\nCloses-Bug: 1859041\n(cherry picked from commit fe61f2358a6e16ea462630747180b83337eb5b55)\n(cherry picked from commit ed23eb60d4674e7d50f9f13cc926d3e3bb4f1121)\n'}]",0,701768,e3f40725a4f16140f1c423bc9456b670b9bb76b9,13,6,2,1736,,,0,"Fix typo in publicize_image policy name

This patch is not cherry-picked from stable/train because if was fixed
in a scope of a new feature implementation with
https://review.opendev.org/#/c/602468/ commit.

Change-Id: I519e15afc975e6da2afb9c72a05448541572bd10
Closes-Bug: 1859041
(cherry picked from commit fe61f2358a6e16ea462630747180b83337eb5b55)
(cherry picked from commit ed23eb60d4674e7d50f9f13cc926d3e3bb4f1121)
",git fetch https://review.opendev.org/openstack/horizon refs/changes/68/701768/2 && git format-patch -1 --stdout FETCH_HEAD,['openstack_dashboard/static/app/core/images/steps/edit-image/edit-image.controller.js'],1,ab959f9d1753c309f205dd7a231e3dad6bba95e3,bug/1859041," ctrl.allowPublicizeImage = { rules: [['image', 'publicize_image']] };"," ctrl.allowPublicizeImage = { rules: [['image', 'image:publicize_image']] };",1,1
openstack%2Fpython-glanceclient~stable%2Focata~Ie080f9a8fa7f4407b1fcbb7fb7c763152c5ec295,openstack/python-glanceclient,stable/ocata,Ie080f9a8fa7f4407b1fcbb7fb7c763152c5ec295,Remove deprecated ssl options,ABANDONED,2019-03-12 08:45:25.000000000,2020-03-21 20:31:29.000000000,,"[{'_account_id': 11904}, {'_account_id': 22348}, {'_account_id': 25111}]","[{'number': 1, 'created': '2019-03-12 08:45:25.000000000', 'files': ['glanceclient/tests/unit/test_shell.py', 'glanceclient/shell.py', 'releasenotes/notes/rm-deprecate-ssl-opts-c88225a4ba2285ad.yaml'], 'web_link': 'https://opendev.org/openstack/python-glanceclient/commit/30f52d293a7f67307434af34af0aa9a5c7db735d', 'message': 'Remove deprecated ssl options\n\nOld deprecated ssl options block the new keystoneauth parser get the\ncorrect value, should be removed.\n\nChange-Id: Ie080f9a8fa7f4407b1fcbb7fb7c763152c5ec295\nCloses-Bug: 1697163\nCloses: PROD-28185 (PROD:28185)\n(cherry picked from commit 8e862b6018404117263e817a896728e344858d94)\n'}]",0,642692,30f52d293a7f67307434af34af0aa9a5c7db735d,5,3,1,8478,,,0,"Remove deprecated ssl options

Old deprecated ssl options block the new keystoneauth parser get the
correct value, should be removed.

Change-Id: Ie080f9a8fa7f4407b1fcbb7fb7c763152c5ec295
Closes-Bug: 1697163
Closes: PROD-28185 (PROD:28185)
(cherry picked from commit 8e862b6018404117263e817a896728e344858d94)
",git fetch https://review.opendev.org/openstack/python-glanceclient refs/changes/92/642692/1 && git format-patch -1 --stdout FETCH_HEAD,"['glanceclient/tests/unit/test_shell.py', 'glanceclient/shell.py', 'releasenotes/notes/rm-deprecate-ssl-opts-c88225a4ba2285ad.yaml']",3,30f52d293a7f67307434af34af0aa9a5c7db735d,bug/1697163,"--- other: - | The following options to the command line client, which have been deprecated since Icehouse, have been removed: * ``--key-file`` (use ``--os-key`` instead) * ``--ca-file`` (use ``--os-cacert`` instead) * ``--cert-file`` (use ``--os-cert`` instead) ",,9,23
openstack%2Fpython-glanceclient~stable%2Fpike~Ie080f9a8fa7f4407b1fcbb7fb7c763152c5ec295,openstack/python-glanceclient,stable/pike,Ie080f9a8fa7f4407b1fcbb7fb7c763152c5ec295,Remove deprecated ssl options,ABANDONED,2019-03-12 08:36:54.000000000,2020-03-21 20:31:05.000000000,,"[{'_account_id': 8478}, {'_account_id': 11904}, {'_account_id': 22348}, {'_account_id': 25111}]","[{'number': 1, 'created': '2019-03-12 08:36:54.000000000', 'files': ['glanceclient/tests/unit/test_shell.py', 'glanceclient/shell.py', 'releasenotes/notes/rm-deprecate-ssl-opts-c88225a4ba2285ad.yaml', 'doc/source/cli/details.rst'], 'web_link': 'https://opendev.org/openstack/python-glanceclient/commit/e19512404e70d203cdde8e4aeb2427d3b22283da', 'message': 'Remove deprecated ssl options\n\nOld deprecated ssl options block the new keystoneauth parser get the\ncorrect value, should be removed.\n\nChange-Id: Ie080f9a8fa7f4407b1fcbb7fb7c763152c5ec295\nCloses-Bug: 1697163\n(cherry picked from commit 8e862b6018404117263e817a896728e344858d94)\n'}]",0,642690,e19512404e70d203cdde8e4aeb2427d3b22283da,7,4,1,8478,,,0,"Remove deprecated ssl options

Old deprecated ssl options block the new keystoneauth parser get the
correct value, should be removed.

Change-Id: Ie080f9a8fa7f4407b1fcbb7fb7c763152c5ec295
Closes-Bug: 1697163
(cherry picked from commit 8e862b6018404117263e817a896728e344858d94)
",git fetch https://review.opendev.org/openstack/python-glanceclient refs/changes/90/642690/1 && git format-patch -1 --stdout FETCH_HEAD,"['glanceclient/tests/unit/test_shell.py', 'glanceclient/shell.py', 'releasenotes/notes/rm-deprecate-ssl-opts-c88225a4ba2285ad.yaml', 'doc/source/cli/details.rst']",4,e19512404e70d203cdde8e4aeb2427d3b22283da,bug/1697163, [--profile HMAC_KEY] [--os-region-name OS_REGION_NAME], [--profile HMAC_KEY] [--key-file OS_KEY] [--ca-file OS_CACERT] [--cert-file OS_CERT] [--os-region-name OS_REGION_NAME]``--key-file OS_KEY`` **DEPRECATED!** Use --os-key. ``--ca-file OS_CACERT`` **DEPRECATED!** Use --os-cacert. ``--cert-file OS_CERT`` **DEPRECATED!** Use --os-cert. ,10,34
openstack%2Fironic-tempest-plugin~master~I583840f2a50753fd169149d76c7123392eb362b5,openstack/ironic-tempest-plugin,master,I583840f2a50753fd169149d76c7123392eb362b5,Make local boot explicit on software raid tests,MERGED,2019-08-29 13:56:32.000000000,2020-03-21 19:57:10.000000000,2020-03-21 19:55:53.000000000,"[{'_account_id': 6618}, {'_account_id': 10239}, {'_account_id': 11655}, {'_account_id': 22348}, {'_account_id': 23851}, {'_account_id': 24828}]","[{'number': 1, 'created': '2019-08-29 13:56:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-tempest-plugin/commit/bd963eb6449a647a4136a3c5b9802a2b46b7ef29', 'message': 'Make local boot explicit on software raid tests\n\nSoftware RAID requies the installation of a boot loader\nand in this scenario, we need to explicitly state that\nlocal boot is desired since ironic deployments do have\na default_boot_option which can be set.\n\nIn the default case of netboot, these tests will silently\nwork if the content written to disk is also bootable\nbecause the netboot default causes ironic to skip\nbootloader installation.\n\nChange-Id: I583840f2a50753fd169149d76c7123392eb362b5\nTask: 36415\nStory: 2006474\n'}, {'number': 2, 'created': '2019-08-30 00:16:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-tempest-plugin/commit/897d92d044222dc4aa55fec0b5d6e2e0b158e79e', 'message': 'Make local boot explicit on software raid tests\n\nSoftware RAID requies the installation of a boot loader\nand in this scenario, we need to explicitly state that\nlocal boot is desired since ironic deployments do have\na default_boot_option which can be set.\n\nIn the default case of netboot, these tests will silently\nwork if the content written to disk is also bootable\nbecause the netboot default causes ironic to skip\nbootloader installation.\n\nDepends-On: https://review.opendev.org/#/c/679258/\nChange-Id: I583840f2a50753fd169149d76c7123392eb362b5\nTask: 36415\nStory: 2006474\n'}, {'number': 3, 'created': '2019-08-30 00:51:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-tempest-plugin/commit/cb33e4539a62fc747a3275246861b0e9da0e2bc6', 'message': 'Make local boot explicit on software raid tests\n\nSoftware RAID requies the installation of a boot loader\nand in this scenario, we need to explicitly state that\nlocal boot is desired since ironic deployments do have\na default_boot_option which can be set.\n\nIn the default case of netboot, these tests will silently\nwork if the content written to disk is also bootable\nbecause the netboot default causes ironic to skip\nbootloader installation.\n\nDepends-On: https://review.opendev.org/679333\nChange-Id: I583840f2a50753fd169149d76c7123392eb362b5\nTask: 36415\nStory: 2006474\n'}, {'number': 4, 'created': '2019-12-02 20:20:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-tempest-plugin/commit/d58e513fb0111cfcf5e57ccde30c01f572503169', 'message': 'Make local boot explicit on software raid tests\n\nSoftware RAID requies the installation of a boot loader\nand in this scenario, we need to explicitly state that\nlocal boot is desired since ironic deployments do have\na default_boot_option which can be set.\n\nIn the default case of netboot, these tests will silently\nwork if the content written to disk is also bootable\nbecause the netboot default causes ironic to skip\nbootloader installation.\n\nDepends-On: https://review.opendev.org/679333\nChange-Id: I583840f2a50753fd169149d76c7123392eb362b5\nTask: 36415\nStory: 2006474\n'}, {'number': 5, 'created': '2019-12-13 00:49:06.000000000', 'files': ['ironic_tempest_plugin/tests/scenario/ironic_standalone/test_cleaning.py'], 'web_link': 'https://opendev.org/openstack/ironic-tempest-plugin/commit/4f3d5eb6ae1fb749bb1a6280a3223e14cfdce943', 'message': 'Make local boot explicit on software raid tests\n\nSoftware RAID requies the installation of a boot loader\nand in this scenario, we need to explicitly state that\nlocal boot is desired since ironic deployments do have\na default_boot_option which can be set.\n\nIn the default case of netboot, these tests will silently\nwork if the content written to disk is also bootable\nbecause the netboot default causes ironic to skip\nbootloader installation.\n\nChange-Id: I583840f2a50753fd169149d76c7123392eb362b5\nTask: 36415\nStory: 2006474\n'}]",4,679258,4f3d5eb6ae1fb749bb1a6280a3223e14cfdce943,38,6,5,11655,,,0,"Make local boot explicit on software raid tests

Software RAID requies the installation of a boot loader
and in this scenario, we need to explicitly state that
local boot is desired since ironic deployments do have
a default_boot_option which can be set.

In the default case of netboot, these tests will silently
work if the content written to disk is also bootable
because the netboot default causes ironic to skip
bootloader installation.

Change-Id: I583840f2a50753fd169149d76c7123392eb362b5
Task: 36415
Story: 2006474
",git fetch https://review.opendev.org/openstack/ironic-tempest-plugin refs/changes/58/679258/3 && git format-patch -1 --stdout FETCH_HEAD,['ironic_tempest_plugin/tests/scenario/ironic_standalone/test_cleaning.py'],1,bd963eb6449a647a4136a3c5b9802a2b46b7ef29,678633, # Software RAID is always local boot boot_option = 'local' # Software RAID is always local boot boot_option = 'local',,4,0
openstack%2Ftripleo-common~stable%2Fstein~I777cd2c33992254e926ba7e937f73aa5c48b786b,openstack/tripleo-common,stable/stein,I777cd2c33992254e926ba7e937f73aa5c48b786b,Improve authentication retries for slow transfers,MERGED,2020-03-20 01:23:31.000000000,2020-03-21 19:40:26.000000000,2020-03-21 19:39:06.000000000,"[{'_account_id': 13861}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-20 01:23:31.000000000', 'files': ['tripleo_common/image/image_export.py', 'tripleo_common/image/image_uploader.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/0eb7532d392fc2b4c6530157c66d8fff4f5d31be', 'message': 'Improve authentication retries for slow transfers\n\nIf during the image prepare process the layer transfers take longer\nthan 5 minutes (the default TTL for tokens for most authenticated\nregistries), the process may fail to fetch and push the container to the\nundercloud registry.\n\nThis change does two things:\n\n1) Raises the HTTPError exceptions when exporting layers correctly so\n   that it is retried\n2) Improves problemattic re-authentication logic for registries other\n   than docker.io which resulted in the process failing with 401 errors.\n\nChange-Id: I777cd2c33992254e926ba7e937f73aa5c48b786b\nCloses-Bug: #1867981\n(cherry picked from commit e269e34738d43f7f924d08897831095397626848)\n'}]",0,714013,0eb7532d392fc2b4c6530157c66d8fff4f5d31be,10,4,1,3153,,,0,"Improve authentication retries for slow transfers

If during the image prepare process the layer transfers take longer
than 5 minutes (the default TTL for tokens for most authenticated
registries), the process may fail to fetch and push the container to the
undercloud registry.

This change does two things:

1) Raises the HTTPError exceptions when exporting layers correctly so
   that it is retried
2) Improves problemattic re-authentication logic for registries other
   than docker.io which resulted in the process failing with 401 errors.

Change-Id: I777cd2c33992254e926ba7e937f73aa5c48b786b
Closes-Bug: #1867981
(cherry picked from commit e269e34738d43f7f924d08897831095397626848)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/13/714013/1 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_common/image/image_export.py', 'tripleo_common/image/image_uploader.py']",2,0eb7532d392fc2b4c6530157c66d8fff4f5d31be,bug/1867981-stable/stein," # Handle docker.io shenanigans. docker.io will return 401 # for 403 and 404 but provide an error string. Other registries # like registry.redhat.io and quay.io do not do this. So if # we find an error string, check to see if we should reauth. do_reauth = allow_reauth do_reauth = (error == 'invalid_token' and allow_reauth) if do_reauth: # Because the image layer fetching can exceed the auth # token lifetime, we may have a bad token here and don't want # to retry all of the layer fetching to just fetch the config # data. Let's try a single retry here (as check_status with # reauth by default). try: r = source_session.get(source_config_url, timeout=30) cls.check_status( session=source_session, request=r ) except requests.exceptions.HTTPError as e: LOG.debug('[%s] Config fetch failed, retrying: %s' % (image, source_config_url)) if e.response.status_code == 401: # check_status should have reauthed so try on more # time and raise again if we still have problems. r = source_session.get(source_config_url, timeout=30) cls.check_status( session=source_session, request=r ) else: raise "," if error == 'invalid_token' and allow_reauth: r = source_session.get(source_config_url, timeout=30) cls.check_status( session=source_session, request=r )",39,6
openstack%2Fproject-config~master~I46bf9e9527941f2120de59fdbc355337f4c019c1,openstack/project-config,master,I46bf9e9527941f2120de59fdbc355337f4c019c1,Update infra-manual links,MERGED,2020-03-19 16:28:52.000000000,2020-03-21 17:36:50.000000000,2020-03-21 17:33:25.000000000,"[{'_account_id': 2}, {'_account_id': 5263}, {'_account_id': 6547}, {'_account_id': 22348}, {'_account_id': 24162}]","[{'number': 1, 'created': '2020-03-19 16:28:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/a6f9a79aeb2ae40ae47188af1774bec09b60c865', 'message': 'Update infra-manual links\n\nThe infra-manual publishes now to docs.opendev.org, change links to it.\n\nChange-Id: I46bf9e9527941f2120de59fdbc355337f4c019c1\n'}, {'number': 2, 'created': '2020-03-19 16:38:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/9302a645c0c9324c9236429ef2a121acc61520f9', 'message': 'Update infra-manual links\n\nThe infra-manual publishes now to docs.opendev.org, change links to it.\n\nDepends-On: https://review.opendev.org/713929\nChange-Id: I46bf9e9527941f2120de59fdbc355337f4c019c1\n'}, {'number': 3, 'created': '2020-03-19 16:39:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/24f7b9a41c7eda8e72cc44fa71758b7b97b8e79d', 'message': 'Update infra-manual links\n\nThe infra-manual publishes now to docs.opendev.org, change links to it.\n\nDepends-On: https://review.opendev.org/713929\nNeeded-By: https://review.opendev.org/713930\nChange-Id: I46bf9e9527941f2120de59fdbc355337f4c019c1\n'}, {'number': 4, 'created': '2020-03-20 16:03:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/113c924ec24e5342084c1e68213f974631e77272', 'message': 'Update infra-manual links\n\nThe infra-manual publishes now to docs.opendev.org, change links to it.\n\nDepends-On: https://review.opendev.org/713929\nNeeded-By: https://review.opendev.org/713930\nChange-Id: I46bf9e9527941f2120de59fdbc355337f4c019c1\n'}, {'number': 5, 'created': '2020-03-20 20:42:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/4c1898cab648dae28e55bb0693e38e2700c43063', 'message': 'Update infra-manual links\n\nThe infra-manual publishes now to docs.opendev.org, change links to it.\n\nNew location is: https://docs.opendev.org/opendev/infra-manual/latest/\n\nDepends-On: https://review.opendev.org/713929\nNeeded-By: https://review.opendev.org/713930\nChange-Id: I46bf9e9527941f2120de59fdbc355337f4c019c1\n'}, {'number': 6, 'created': '2020-03-20 21:08:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/07f8c328f211a89dda1507e4be28712c5327756f', 'message': 'Update infra-manual links\n\nThe infra-manual publishes now to docs.opendev.org, change links to it.\n\nNew location is: https://docs.opendev.org/opendev/infra-manual/latest/\n\nDepends-On: https://review.opendev.org/713929\nNeeded-By: https://review.opendev.org/713930\nChange-Id: I46bf9e9527941f2120de59fdbc355337f4c019c1\n'}, {'number': 7, 'created': '2020-03-21 08:29:50.000000000', 'files': ['grafana/neutron-stable-minustwo.yaml', 'grafana/tap-as-a-service.yaml', 'grafana/nodepool-linaro.yaml', 'README.rst', 'grafana/openstack-ansible.yaml', 'grafana/horizon.yaml', 'docs-site/infra-index.html.tmpl', 'CONTRIBUTING.rst', 'grafana/neutron-stable-minusone.yaml', 'grafana/kolla.yaml', 'grafana/bridge.yaml', 'grafana/networking-midonet.yaml', 'grafana/nodepool-vexxhost.yaml', 'grafana/neutron-dynamic-routing.yaml', 'grafana/neutron-vpnaas.yaml', 'grafana/ceph.yaml', 'grafana/mosquitto.yaml', 'grafana/networking-ovn.yaml', 'grafana/nodepool-rax.yaml', 'grafana/nodepool-inap.yaml', 'grafana/ovsdbapp.yaml', 'grafana/nodepool-dib.yaml', 'specs/index.html.tmpl', 'grafana/neutron-fwaas.yaml', 'zuul.d/pipelines.yaml', 'grafana/nodepool-ovh.yaml', 'grafana/networking-sfc.yaml', 'grafana/networking-odl.yaml', 'grafana/nodepool.yaml', 'grafana/neutron.yaml', 'grafana/nodepool-citycloud.yaml', 'grafana/octavia.yaml', 'grafana/networking-bgpvpn.yaml', 'grafana/nodepool-openedge.yaml', 'grafana/nodepool-airship-citycloud.yaml', 'grafana/afs.yaml', 'grafana/tempest.yaml', 'grafana/git.yaml', 'grafana/nodepool-dib.base.template', 'grafana/vmware-nsx.yaml', 'grafana/neutron-lib.yaml', 'grafana/nodepool-limestone.yaml', 'specs/specs.opml.tmpl', 'grafana/neutron-tempest-plugin.yaml', 'grafana/nodepool.template', 'grafana/networking-bagpipe.yaml', 'grafana/zuul-status.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/a494d238b0d9e72193cb8dab92796a11e559aa51', 'message': 'Update infra-manual links\n\nThe infra-manual publishes now to docs.opendev.org, change links to it.\n\nNew location is: https://docs.opendev.org/opendev/infra-manual/latest/\n\nLink specs.o.o contributors link to OpenStack Contributor Guide.\n\nChange http to https for docs.o.o and specs.o.o everywhere.\n\nDepends-On: https://review.opendev.org/713929\nNeeded-By: https://review.opendev.org/713930\nChange-Id: I46bf9e9527941f2120de59fdbc355337f4c019c1\n'}]",3,713925,a494d238b0d9e72193cb8dab92796a11e559aa51,24,5,7,6547,,,0,"Update infra-manual links

The infra-manual publishes now to docs.opendev.org, change links to it.

New location is: https://docs.opendev.org/opendev/infra-manual/latest/

Link specs.o.o contributors link to OpenStack Contributor Guide.

Change http to https for docs.o.o and specs.o.o everywhere.

Depends-On: https://review.opendev.org/713929
Needed-By: https://review.opendev.org/713930
Change-Id: I46bf9e9527941f2120de59fdbc355337f4c019c1
",git fetch https://review.opendev.org/openstack/project-config refs/changes/25/713925/1 && git format-patch -1 --stdout FETCH_HEAD,"['docs-site/infra-index.html.tmpl', 'CONTRIBUTING.rst', 'zuul.d/pipelines.yaml', 'README.rst', 'specs/index.html.tmpl']",5,a6f9a79aeb2ae40ae47188af1774bec09b60c865,rename-infra-manual," <li><a href=""https://docs.opendev.org/infra-manual/developers.html"">Contribute</a></li>"," <li><a href=""https://docs.openstack.org/infra/manual/developers.html"">Contribute</a></li>",20,19
openstack%2Fproject-config~master~I11844b2a926f38f8daec83b1bf6bd01485c0b2df,openstack/project-config,master,I11844b2a926f38f8daec83b1bf6bd01485c0b2df,Update third-party-check pipeline description,MERGED,2020-03-21 15:49:34.000000000,2020-03-21 17:33:27.000000000,2020-03-21 17:33:27.000000000,"[{'_account_id': 2}, {'_account_id': 5263}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-21 15:49:34.000000000', 'files': ['zuul.d/pipelines.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/af3bcbebf829faf10ab80f6e087766c68d9fe8f9', 'message': 'Update third-party-check pipeline description\n\nThe pipeline is not OpenStack specific, update description.\n\nChange-Id: I11844b2a926f38f8daec83b1bf6bd01485c0b2df\n'}]",0,714265,af3bcbebf829faf10ab80f6e087766c68d9fe8f9,7,3,1,6547,,,0,"Update third-party-check pipeline description

The pipeline is not OpenStack specific, update description.

Change-Id: I11844b2a926f38f8daec83b1bf6bd01485c0b2df
",git fetch https://review.opendev.org/openstack/project-config refs/changes/65/714265/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/pipelines.yaml'],1,af3bcbebf829faf10ab80f6e087766c68d9fe8f9,rename-infra-manual," Newly uploaded patchsets to projects that are not hosted on OpenDev # TODO(mordred) We should write a document for non-OpenDev developers Build failed (third-party-check pipeline) integration testing on OpenDev. For information on how to proceed, see"," Newly uploaded patchsets to projects that are external to OpenStack # TODO(mordred) We should write a document for non-OpenStack developers Build failed (third-party-check pipeline) integration testing with OpenStack. For information on how to proceed, see",4,4
openstack%2Fcinder~master~I3048efa77a6935f097ba8f96e887ccc2a1db0018,openstack/cinder,master,I3048efa77a6935f097ba8f96e887ccc2a1db0018,Enable mutate for cinder scheduler,MERGED,2019-08-17 06:47:13.000000000,2020-03-21 17:21:30.000000000,2020-03-19 14:03:18.000000000,"[{'_account_id': 5997}, {'_account_id': 7198}, {'_account_id': 9008}, {'_account_id': 9732}, {'_account_id': 10118}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 14384}, {'_account_id': 15386}, {'_account_id': 15670}, {'_account_id': 15831}, {'_account_id': 15941}, {'_account_id': 16897}, {'_account_id': 18120}, {'_account_id': 18883}, {'_account_id': 19933}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 21976}, {'_account_id': 22126}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23613}, {'_account_id': 24815}, {'_account_id': 24921}, {'_account_id': 25243}, {'_account_id': 25678}, {'_account_id': 26077}, {'_account_id': 26537}, {'_account_id': 28801}, {'_account_id': 29705}, {'_account_id': 29716}, {'_account_id': 30688}]","[{'number': 1, 'created': '2019-08-17 06:47:13.000000000', 'files': ['cinder/service.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/8ba809240697b439312e98d69ee04654b2e3283c', 'message': 'Enable mutate for cinder scheduler\n\nEnable cinder service to launch with restart_method as mutate\nso that cinder scheduler can enable debug dynamically\n\nChange-Id: I3048efa77a6935f097ba8f96e887ccc2a1db0018\nCloses-Bug: 1840487\n'}]",0,677053,8ba809240697b439312e98d69ee04654b2e3283c,76,36,1,13637,,,0,"Enable mutate for cinder scheduler

Enable cinder service to launch with restart_method as mutate
so that cinder scheduler can enable debug dynamically

Change-Id: I3048efa77a6935f097ba8f96e887ccc2a1db0018
Closes-Bug: 1840487
",git fetch https://review.opendev.org/openstack/cinder refs/changes/53/677053/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/service.py'],1,8ba809240697b439312e98d69ee04654b2e3283c,bug/1840487," _launcher = service.launch(CONF, server, workers=workers, restart_method='mutate')"," _launcher = service.launch(CONF, server, workers=workers)",2,1
openstack%2Ftripleo-common~master~I5fb83a13e385ed1a35a6cb735e50df2b5b368164,openstack/tripleo-common,master,I5fb83a13e385ed1a35a6cb735e50df2b5b368164,Handle race for the already existing layer,MERGED,2020-03-02 17:13:16.000000000,2020-03-21 17:16:36.000000000,2020-03-20 01:21:16.000000000,"[{'_account_id': 3153}, {'_account_id': 4571}, {'_account_id': 6926}, {'_account_id': 8175}, {'_account_id': 8449}, {'_account_id': 9592}, {'_account_id': 9976}, {'_account_id': 10022}, {'_account_id': 10969}, {'_account_id': 12393}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 31245}]","[{'number': 1, 'created': '2020-03-02 17:13:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/5c5b6492e72d3e1b7c1a0dc8338a091d45562127', 'message': 'Handle race for the already existing layer\n\nHard copying a large layer takes time.\nHandle race for the already existing layers by skipping competing\nsymlinking made by concurrent workers\n\nChange-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164\nCloses-bug: #1864953\nSigned-off-by: Bogdan Dobrelya <bdobreli@redhat.com>\n'}, {'number': 2, 'created': '2020-03-02 17:26:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/7e866c4fe6b18020718a6d094f73e05e2da05c2d', 'message': 'Handle race for the already existing layer\n\nHard copying a large layer takes time.\nRenaming of a blob may also be not instant for some cases.\nHandle race for the already existing layers by skipping competing\nlayers symlinking or renaming blobs made by concurrent workers\n\nChange-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164\nCloses-bug: #1864953\nSigned-off-by: Bogdan Dobrelya <bdobreli@redhat.com>\n'}, {'number': 3, 'created': '2020-03-16 15:48:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/fcc349ab5e0fccdeb2625042c29ddcf142fe05f8', 'message': 'Handle race for the already existing layer\n\nHard copying a large layer takes time.\nRenaming of a blob may also be not instant for some cases.\nHandle race for the already existing layers by skipping competing\nlayers symlinking or renaming blobs made by concurrent workers\n\nChange-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164\nCloses-bug: #1864953\nSigned-off-by: Bogdan Dobrelya <bdobreli@redhat.com>\n'}, {'number': 4, 'created': '2020-03-16 15:55:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/90f75e48bd6affe5101bcdce3d25f52d87880d9a', 'message': 'Handle race for the already existing layer\n\nHard copying a large layer takes time.\nRenaming of a blob may also be not instant for some cases.\nHandle race for the already existing layers by skipping competing\nlayers symlinking or renaming blobs made by concurrent workers\n\nChange-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164\nCloses-bug: #1864953\nSigned-off-by: Bogdan Dobrelya <bdobreli@redhat.com>\n'}, {'number': 5, 'created': '2020-03-17 09:39:46.000000000', 'files': ['tripleo_common/image/image_export.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/28b8bf0fe637683123500d15ca20fdd427c2fd7d', 'message': 'Handle race for the already existing layer\n\nHard copying a large layer takes time.\nRenaming of a blob may also be not instant for some cases.\nHandle race for the already existing layers by skipping competing\nlayers symlinking or renaming blobs made by concurrent workers\n\nChange-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164\nCloses-bug: #1864953\nSigned-off-by: Bogdan Dobrelya <bdobreli@redhat.com>\n'}]",3,710836,28b8bf0fe637683123500d15ca20fdd427c2fd7d,36,14,5,6926,,,0,"Handle race for the already existing layer

Hard copying a large layer takes time.
Renaming of a blob may also be not instant for some cases.
Handle race for the already existing layers by skipping competing
layers symlinking or renaming blobs made by concurrent workers

Change-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164
Closes-bug: #1864953
Signed-off-by: Bogdan Dobrelya <bdobreli@redhat.com>
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/36/710836/5 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_common/image/image_export.py'],1,5c5b6492e72d3e1b7c1a0dc8338a091d45562127,," try: os.link(blob_path, target_blob_path) except os.error: # Handle race for the already existing layer LOG.debug('[%s] Skipped linking %s -> %s: existing layer' % (image, blob_path, target_blob_path)) pass"," os.link(blob_path, target_blob_path)",7,1
openstack%2Fpuppet-tripleo~stable%2Ftrain~I7894c51d18961c5cab7ac62e5eec5d515e2667c8,openstack/puppet-tripleo,stable/train,I7894c51d18961c5cab7ac62e5eec5d515e2667c8,Fix grafana haproxy frontend ip variable,MERGED,2020-03-20 15:13:20.000000000,2020-03-21 17:00:19.000000000,2020-03-21 17:00:19.000000000,"[{'_account_id': 3153}, {'_account_id': 6796}, {'_account_id': 14985}, {'_account_id': 18002}, {'_account_id': 20182}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25402}, {'_account_id': 25877}]","[{'number': 1, 'created': '2020-03-20 15:13:20.000000000', 'files': ['manifests/haproxy.pp'], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/02b308343773684cc0757e51e4398f4c0a143583', 'message': ""Fix grafana haproxy frontend ip variable\n\nGrafana could be exposed along with ceph dashboard\nbut it's actually embedded by in a view created for\nthis purpose.\nFor this reason the ceph-dashboard component should\nbe able to reach grafana or the requests will fail.\n\nCloses-Bug: #1868118\nChange-Id: I7894c51d18961c5cab7ac62e5eec5d515e2667c8\n(cherry picked from commit b6175ece1ac77647e78e38f69648a8ac6c8795ee)\n""}]",0,714141,02b308343773684cc0757e51e4398f4c0a143583,15,9,1,25402,,,0,"Fix grafana haproxy frontend ip variable

Grafana could be exposed along with ceph dashboard
but it's actually embedded by in a view created for
this purpose.
For this reason the ceph-dashboard component should
be able to reach grafana or the requests will fail.

Closes-Bug: #1868118
Change-Id: I7894c51d18961c5cab7ac62e5eec5d515e2667c8
(cherry picked from commit b6175ece1ac77647e78e38f69648a8ac6c8795ee)
",git fetch https://review.opendev.org/openstack/puppet-tripleo refs/changes/41/714141/1 && git format-patch -1 --stdout FETCH_HEAD,['manifests/haproxy.pp'],1,02b308343773684cc0757e51e4398f4c0a143583,lp1868118," internal_ip => hiera('ceph_dashboard_vip', $controller_virtual_ip),"," internal_ip => hiera('ceph_grafana_vip', $controller_virtual_ip),",1,1
openstack%2Frally~master~I07400c7fd829dea86774f3f695c8a976550c6baf,openstack/rally,master,I07400c7fd829dea86774f3f695c8a976550c6baf,Use unittests.mock instead of mock lib,MERGED,2020-03-21 13:24:31.000000000,2020-03-21 16:52:02.000000000,2020-03-21 16:52:02.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-21 13:24:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/973ce5fbd4f64dfee2d2a7a0a8d040900a544c49', 'message': 'Use unittests.mock instead of mock lib\n\nChange-Id: I07400c7fd829dea86774f3f695c8a976550c6baf\n'}, {'number': 2, 'created': '2020-03-21 13:24:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/14100a8f497e0ee1eaaf8c150bf28aebada18202', 'message': 'Use unittests.mock instead of mock lib\n\nChange-Id: I07400c7fd829dea86774f3f695c8a976550c6baf\n'}, {'number': 3, 'created': '2020-03-21 13:45:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/594cd26ae9da0c641ff9654ccfee980cd068c992', 'message': 'Use unittests.mock instead of mock lib\n\nChange-Id: I07400c7fd829dea86774f3f695c8a976550c6baf\n'}, {'number': 4, 'created': '2020-03-21 13:46:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ec3c7e7dd902f05f8f16bce6befbe94eca70ddc9', 'message': 'Use unittests.mock instead of mock lib\n\nChange-Id: I07400c7fd829dea86774f3f695c8a976550c6baf\n'}, {'number': 5, 'created': '2020-03-21 16:13:47.000000000', 'files': ['tests/unit/test_pytest_launcher.py', 'tests/unit/verification/test_context.py', 'tests/unit/cli/commands/test_db.py', 'test-requirements.txt', 'tests/unit/plugins/common/exporters/test_trends.py', 'tests/unit/plugins/common/scenarios/dummy/test_dummy.py', 'tests/unit/task/test_utils.py', 'tests/unit/verification/test_reporter.py', 'tests/unit/common/io/test_junit.py', 'tests/unit/plugins/common/hook/test_sys_call.py', 'tests/functional/test_cli_verify.py', 'tests/unit/doc/utils.py', 'tests/unit/common/db/test_api.py', 'tests/unit/plugins/common/test_types.py', 'tests/unit/task/test_types.py', 'tests/unit/cli/commands/test_env.py', 'tests/unit/common/test_version.py', 'tests/unit/env/test_env_mgr.py', 'tests/unit/plugins/common/runners/test_constant.py', 'tests/unit/common/test_opts.py', 'tests/unit/task/processing/test_charts.py', 'tests/unit/task/test_services.py', 'tests/unit/test_exceptions.py', 'tests/unit/verification/test_manager.py', 'tests/unit/verification/test_utils.py', 'tests/unit/plugins/common/runners/test_serial.py', 'tests/unit/common/io/test_subunit_v2.py', 'tests/unit/task/test_scenario.py', 'tests/unit/task/test_task_cfg.py', 'tests/unit/plugins/common/scenarios/requests/test_http_requests.py', 'tests/unit/plugins/common/exporters/elastic/test_exporter.py', 'tests/unit/task/test_sla.py', 'tests/unit/common/objects/test_task.py', 'tests/functional/test_cli_task.py', 'tests/unit/cli/test_envutils.py', 'tests/unit/common/plugin/test_discover.py', 'tests/unit/cli/commands/test_deployment.py', 'tests/unit/cli/commands/test_task.py', 'tests/unit/common/test_broker.py', 'tests/unit/plugins/common/hook/triggers/test_periodic.py', 'tests/unit/plugins/common/runners/test_rps.py', 'tests/unit/task/test_engine.py', 'tests/unit/cli/commands/test_plugin.py', 'tests/unit/cli/test_cliutils.py', 'tests/unit/common/test_utils.py', 'tests/unit/plugins/common/exporters/elastic/test_client.py', 'tests/unit/plugins/common/scenarios/requests/test_utils.py', 'tests/unit/plugins/common/test_validators.py', 'tests/unit/task/test_exporter.py', 'tests/unit/plugins/common/contexts/test_dummy.py', 'optional-requirements.txt', 'tests/unit/task/test_context.py', 'tests/unit/utils/test_sshutils.py', 'tests/unit/task/test_atomic.py', 'tests/unit/common/objects/test_verifier.py', 'tests/unit/utils/test_encodeutils.py', 'tests/unit/test_api.py', 'tests/unit/plugins/common/exporters/test_junit.py', 'tests/unit/task/test_hook.py', 'tests/unit/common/db/test_migrations.py', 'tests/unit/plugins/common/verification/test_testr.py', 'tests/unit/fakes.py', 'tests/unit/common/test_fileutils.py', 'tests/unit/common/objects/test_deploy.py', 'tests/unit/test.py', 'tests/unit/cli/commands/test_verify.py', 'tests/unit/plugins/common/hook/triggers/test_event.py', 'tests/unit/common/db/test_types.py', 'tests/unit/utils/test_strutils.py', 'tests/unit/plugins/common/verification/test_reporters.py', 'tests/unit/task/processing/test_plot.py', 'tests/unit/plugins/common/exporters/test_html.py', 'tests/unit/task/test_runner.py', 'tests/unit/common/test_logging.py', 'tests/unit/test_test_mock.py', 'tests/unit/common/objects/test_verification.py', 'tests/unit/plugins/common/exporters/test_json_exporter.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/b086b2cd502cd2c255d1b8ea3eb5592e88c4c895', 'message': 'Use unittests.mock instead of mock lib\n\nChange-Id: I07400c7fd829dea86774f3f695c8a976550c6baf\n'}]",0,714256,b086b2cd502cd2c255d1b8ea3eb5592e88c4c895,14,2,5,9545,,,0,"Use unittests.mock instead of mock lib

Change-Id: I07400c7fd829dea86774f3f695c8a976550c6baf
",git fetch https://review.opendev.org/openstack/rally refs/changes/56/714256/2 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/test_pytest_launcher.py', 'tests/unit/verification/test_context.py', 'tests/unit/cli/commands/test_db.py', 'tests/unit/plugins/common/exporters/test_trends.py', 'tests/unit/plugins/common/scenarios/dummy/test_dummy.py', 'tests/unit/task/test_utils.py', 'tests/unit/verification/test_reporter.py', 'tests/unit/common/io/test_junit.py', 'tests/unit/plugins/common/hook/test_sys_call.py', 'tests/unit/doc/utils.py', 'tests/unit/common/db/test_api.py', 'tests/unit/plugins/common/test_types.py', 'tests/unit/task/test_types.py', 'tests/unit/cli/commands/test_env.py', 'tests/unit/common/test_version.py', 'tests/unit/env/test_env_mgr.py', 'tests/unit/plugins/common/runners/test_constant.py', 'tests/unit/common/test_opts.py', 'tests/unit/task/processing/test_charts.py', 'tests/unit/task/test_services.py', 'tests/unit/test_exceptions.py', 'tests/unit/verification/test_manager.py', 'tests/unit/verification/test_utils.py', 'tests/unit/plugins/common/runners/test_serial.py', 'tests/unit/common/io/test_subunit_v2.py', 'tests/unit/task/test_scenario.py', 'tests/unit/task/test_task_cfg.py', 'tests/unit/plugins/common/scenarios/requests/test_http_requests.py', 'tests/unit/plugins/common/exporters/elastic/test_exporter.py', 'tests/unit/task/test_sla.py', 'tests/unit/common/objects/test_task.py', 'tests/unit/cli/test_envutils.py', 'tests/unit/common/plugin/test_discover.py', 'tests/unit/cli/commands/test_deployment.py', 'tests/unit/cli/commands/test_task.py', 'tests/unit/common/test_broker.py', 'tests/unit/plugins/common/hook/triggers/test_periodic.py', 'tests/unit/plugins/common/runners/test_rps.py', 'tests/unit/task/test_engine.py', 'tests/unit/cli/commands/test_plugin.py', 'tests/unit/cli/test_cliutils.py', 'tests/unit/common/test_utils.py', 'tests/unit/plugins/common/exporters/elastic/test_client.py', 'tests/unit/plugins/common/scenarios/requests/test_utils.py', 'tests/unit/plugins/common/test_validators.py', 'tests/unit/task/test_exporter.py', 'tests/unit/plugins/common/contexts/test_dummy.py', 'tests/unit/task/test_context.py', 'tests/unit/utils/test_sshutils.py', 'tests/unit/task/test_atomic.py', 'tests/unit/common/objects/test_verifier.py', 'tests/unit/utils/test_encodeutils.py', 'tests/unit/test_api.py', 'tests/unit/plugins/common/exporters/test_junit.py', 'tests/unit/task/test_hook.py', 'tests/unit/common/db/test_migrations.py', 'tests/unit/plugins/common/verification/test_testr.py', 'tests/unit/fakes.py', 'tests/unit/common/test_fileutils.py', 'tests/unit/common/objects/test_deploy.py', 'tests/unit/test.py', 'tests/unit/cli/commands/test_verify.py', 'tests/unit/plugins/common/hook/triggers/test_event.py', 'tests/unit/common/db/test_types.py', 'tests/unit/utils/test_strutils.py', 'tests/unit/plugins/common/verification/test_reporters.py', 'tests/unit/task/processing/test_plot.py', 'tests/unit/plugins/common/exporters/test_html.py', 'tests/unit/task/test_runner.py', 'tests/unit/common/test_logging.py', 'tests/unit/test_test_mock.py', 'tests/unit/common/objects/test_verification.py', 'tests/unit/plugins/common/exporters/test_json_exporter.py']",73,973ce5fbd4f64dfee2d2a7a0a8d040900a544c49,mock,from unittest import mock, import mock,93,113
openstack%2Fironic~master~I6f1704212cd3a770b0e8d46656016a27e5fc8de6,openstack/ironic,master,I6f1704212cd3a770b0e8d46656016a27e5fc8de6,WIP: Make pxe dual IP stack configurable,ABANDONED,2020-03-20 20:18:55.000000000,2020-03-21 15:45:22.000000000,,"[{'_account_id': 14629}, {'_account_id': 19339}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 20:18:55.000000000', 'files': ['ironic/common/pxe_utils.py', 'ironic/conf/default.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/598d363fd546249037bb36dfa0d0a6acd7273e42', 'message': 'WIP: Make pxe dual IP stack configurable\n\nChange-Id: I6f1704212cd3a770b0e8d46656016a27e5fc8de6\n'}]",0,714213,598d363fd546249037bb36dfa0d0a6acd7273e42,5,3,1,11655,,,0,"WIP: Make pxe dual IP stack configurable

Change-Id: I6f1704212cd3a770b0e8d46656016a27e5fc8de6
",git fetch https://review.opendev.org/openstack/ironic refs/changes/13/714213/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/common/pxe_utils.py', 'ironic/conf/default.py']",2,598d363fd546249037bb36dfa0d0a6acd7273e42,," help=_('IPv4 address of this host. If unset, will determine the ' cfg.StrOpt('my_ipv6', default=None, sample_default='2001:db8::1', help=_('IP address of this host using IPv6')),"," help=_('IP address of this host. If unset, will determine the '",11,4
openstack%2Fcinder~master~Idc0e51015e80cdce8412d664351033725af13134,openstack/cinder,master,Idc0e51015e80cdce8412d664351033725af13134,Extend remove_version_from_href support,MERGED,2019-02-09 04:03:55.000000000,2020-03-21 14:05:21.000000000,2020-03-19 14:03:15.000000000,"[{'_account_id': 5997}, {'_account_id': 6413}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 13144}, {'_account_id': 15296}, {'_account_id': 15386}, {'_account_id': 15670}, {'_account_id': 15831}, {'_account_id': 15941}, {'_account_id': 16897}, {'_account_id': 18120}, {'_account_id': 18883}, {'_account_id': 19933}, {'_account_id': 20284}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 21976}, {'_account_id': 22126}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 23601}, {'_account_id': 23613}, {'_account_id': 24230}, {'_account_id': 24236}, {'_account_id': 24496}, {'_account_id': 24814}, {'_account_id': 24815}, {'_account_id': 24863}, {'_account_id': 24921}, {'_account_id': 25243}, {'_account_id': 25678}, {'_account_id': 26077}, {'_account_id': 26537}, {'_account_id': 27615}, {'_account_id': 28801}, {'_account_id': 29637}, {'_account_id': 29705}, {'_account_id': 29716}, {'_account_id': 30688}]","[{'number': 1, 'created': '2019-02-09 04:03:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a5ea8fc6eeb6e47f8fd787d68ed35e5919f758bb', 'message': ""Extend remove_version_from_href support\n\nCurrent method cannot process adequately when\na URL has the format HOST/volume/version/xxx.\n\nThis is the default way we deploy cinder in devstack\n\nChange-Id: Idc0e51015e80cdce8412d664351033725af13134\nGiven: 'http://cinder.example.com/volume/v1.1/123'\nReturns: 'http://cinder.example.com/volume/123'\n""}, {'number': 2, 'created': '2019-02-09 04:05:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/ffc2d3263007eb747494201c4fdb9af6d3c3435e', 'message': ""Extend remove_version_from_href support\n\nCurrent method cannot process adequately when\na URL has the format HOST/volume/version/xxx.\n\nThis is the default way we deploy cinder in devstack\n\nGiven: 'http://cinder.example.com/volume/v1.1/123'\nReturns: 'http://cinder.example.com/volume/123'\n\nCloses-Bug: #1815260\n\nChange-Id: Idc0e51015e80cdce8412d664351033725af13134\n""}, {'number': 3, 'created': '2019-02-15 18:44:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9fcb2504c7314cca796f1e4713400b4c9af2bfc9', 'message': ""Extend remove_version_from_href support\n\nCurrent method cannot process adequately when\na URL has the format HOST/volume/version/xxx.\n\nThis is the default way we deploy cinder in devstack\n\nGiven: 'http://cinder.example.com/volume/v1.1/123'\nReturns: 'http://cinder.example.com/volume/123'\n\nCloses-Bug: #1815260\n\nChange-Id: Idc0e51015e80cdce8412d664351033725af13134\n""}, {'number': 4, 'created': '2019-02-22 15:08:47.000000000', 'files': ['cinder/api/common.py', 'cinder/tests/unit/api/test_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/7c8b567ba70cbc6e54d60bc4d94adcda25c36dfa', 'message': ""Extend remove_version_from_href support\n\nCurrent method cannot process adequately when\na URL has the format HOST/volume/version/xxx.\n\nThis is the default way we deploy cinder in devstack\n\nGiven: 'http://cinder.example.com/volume/v1.1/123'\nReturns: 'http://cinder.example.com/volume/123'\n\nCloses-Bug: #1815260\n\nChange-Id: Idc0e51015e80cdce8412d664351033725af13134\n""}]",4,635995,7c8b567ba70cbc6e54d60bc4d94adcda25c36dfa,142,45,4,6413,,,0,"Extend remove_version_from_href support

Current method cannot process adequately when
a URL has the format HOST/volume/version/xxx.

This is the default way we deploy cinder in devstack

Given: 'http://cinder.example.com/volume/v1.1/123'
Returns: 'http://cinder.example.com/volume/123'

Closes-Bug: #1815260

Change-Id: Idc0e51015e80cdce8412d664351033725af13134
",git fetch https://review.opendev.org/openstack/cinder refs/changes/95/635995/4 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/api/common.py', 'cinder/tests/unit/api/test_common.py']",2,a5ea8fc6eeb6e47f8fd787d68ed35e5919f758bb,bug/1815260," 'http://cinder.example.com/cinder'), ('http://cinder.example.com/volume/v2/123', 'http://cinder.example.com/volume/123'))", 'http://cinder.example.com/cinder')),4,2
openstack%2Frally~master~Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166,openstack/rally,master,Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166,Rework docker build,MERGED,2020-03-17 14:38:00.000000000,2020-03-21 14:03:47.000000000,2020-03-21 14:03:47.000000000,"[{'_account_id': 9545}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 14:38:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/02424a4dad80a7acba3f48895c96a4889cae8231', 'message': 'WIP: Improve docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 2, 'created': '2020-03-17 16:23:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d8531371f0a35f014fc29d41db85d85ebf626c72', 'message': 'WIP: Improve docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 3, 'created': '2020-03-17 16:55:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8d058751cba87e080ee9947b5c37cd4b8b28dac5', 'message': 'WIP: Improve docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 4, 'created': '2020-03-19 12:13:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/95a8bfa591dea20ab69e7bc26ba9af8e3464b81f', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 5, 'created': '2020-03-19 12:35:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a89c7c8155b892aee4427915153f6ee6c369e4ba', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 6, 'created': '2020-03-19 12:49:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d76ae011bd5aa25e56adbf97411e4f83bf3211cd', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 7, 'created': '2020-03-19 13:40:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ae6e11b898157c5d244df2e47aee3a86fc331290', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 8, 'created': '2020-03-19 15:04:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e831a0163269e5a78a9423528ad8bf6d03a114df', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 9, 'created': '2020-03-19 15:17:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/52bb1d1df1c8f0ed3e4e487cefc1acead87892b1', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 10, 'created': '2020-03-19 15:29:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d539bcfb2124fbb918d5e97c7d99c4f15ae994e7', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 11, 'created': '2020-03-19 15:46:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/309787f4a604c3479f41056c41cb05403a07fbb2', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 12, 'created': '2020-03-19 16:07:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1ae54b320cc6cedb171214bb9ef73540f159a58d', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 13, 'created': '2020-03-19 16:08:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/fd3c00c5758964b424fca54ab6f9f9c7ea248481', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 14, 'created': '2020-03-19 16:26:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1b0b4c7147f9a7d64d84c3878b362af2caa56729', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 15, 'created': '2020-03-19 16:51:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/f08346041b218ceb599525dc2a8b6e86dc093780', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 16, 'created': '2020-03-19 17:41:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c4bb536a73e6654ba75de8c6b8b36f8d313da6b1', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 17, 'created': '2020-03-19 18:43:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/f9ffbe88fcd480c8f127f8309adf4aba76e774a3', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 18, 'created': '2020-03-19 19:36:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/40f9d09f7293320d6efe7981011b5f1cf1b0365d', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 19, 'created': '2020-03-19 19:43:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/69ba7cbedf55eee928d269e9724c82f16fe45b06', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 20, 'created': '2020-03-19 19:55:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/6417965488e6b7aad6c8574b215233259052d81c', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 21, 'created': '2020-03-19 20:11:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/95eb7cb1262aa7eb668c885091379ffa5bf57130', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 22, 'created': '2020-03-19 20:25:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0b356222463a2415ba9c97cc85d0dda66331ffef', 'message': 'WIP: rework docker\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 23, 'created': '2020-03-20 15:40:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/abc270c6aba47ab283e9610f79708eeb27c0f5d9', 'message': 'Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 24, 'created': '2020-03-20 15:40:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/2937cde827cf03b5a7990413945a259db276317e', 'message': 'Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 25, 'created': '2020-03-20 15:43:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/f4bbfd27464fb1fdee015ce902e6b4647e1cb889', 'message': 'WIP: Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 26, 'created': '2020-03-20 15:44:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/206b8c93714308ca83d936f3e85c95c2574b05d5', 'message': 'WIP: Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 27, 'created': '2020-03-20 16:26:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/cd325333b1467671be7176de9ab6666c824fa9fe', 'message': 'WIP: Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 28, 'created': '2020-03-20 16:32:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/719772f4f5dbec33ac6a13d73b31f4cfc3f0c549', 'message': 'WIP: Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 29, 'created': '2020-03-20 16:58:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/8c79a8b6c8bf743afcc9214fe71917f2f491d8ae', 'message': 'Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n* push docker image at gate pipeline\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 30, 'created': '2020-03-20 17:04:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/31807af4c6631a50f9078d746ddbdc6cfa4d4d29', 'message': 'Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n* push docker image at gate pipeline\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 31, 'created': '2020-03-20 17:32:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/19d680a0784c4973aca0391c8659d9b20a72adbd', 'message': 'Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n* push docker image at gate pipeline\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 32, 'created': '2020-03-20 17:56:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4165d759ee78b813e3ad9e1cee3ac0679cb5d908', 'message': 'Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n* push docker image at gate pipeline\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 33, 'created': '2020-03-20 19:06:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a58419b883cad929b41036383fa0be3df28d4fad', 'message': 'Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n* push docker image at gate pipeline\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 34, 'created': '2020-03-21 11:00:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/9bd6af39fd203de8297f05a28fcde84c75b5f07b', 'message': 'Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n* push docker image at gate pipeline\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}, {'number': 35, 'created': '2020-03-21 13:37:47.000000000', 'files': ['tests/ci/rally_self_job.sh', '.dockerignore', 'tests/unit/doc/test_docker_readme.py', 'tests/ci/playbooks/roles/push_docker_image/tasks/main.yaml', 'DOCKER_README.md', 'tests/ci/playbooks/rally-install-run.yaml', 'tests/ci/playbooks/roles/build_docker_image/tasks/main.yaml', 'tests/ci/playbooks/roles/build_docker_image/defaults/main.yaml', 'etc/motd_for_docker', 'tests/ci/playbooks/build-and-check-docker-image.yaml', 'tests/ci/playbooks/roles/push_docker_image/defaults/main.yaml', 'Dockerfile', 'tests/ci/playbooks/build-check-and-push-docker-image.yaml', 'tests/ci/playbooks/rally-docker-build-pre.yaml', 'tests/ci/rally_self_job.py', '.zuul.d/zuul.yaml', 'etc/docker/hooks/post_checkout', 'etc/docker/README_FIRST.rst', 'tests/ci/playbooks/rally-docker-build-run.yaml', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/rally/commit/c239b13b27615a115a847a3d4af88a2808e212d9', 'message': 'Rework docker build\n\n* put Dockerfile at root level and remove all hooks for autobuilds at\n  hub.docker.org\n* run the similar wokrload as `tox -e self` using rally docker image\n* rewrite workload that is used by `tox -e self` at python\n* push docker image at gate pipeline\n\nChange-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166\n'}]",1,713459,c239b13b27615a115a847a3d4af88a2808e212d9,72,2,35,9545,,,0,"Rework docker build

* put Dockerfile at root level and remove all hooks for autobuilds at
  hub.docker.org
* run the similar wokrload as `tox -e self` using rally docker image
* rewrite workload that is used by `tox -e self` at python
* push docker image at gate pipeline

Change-Id: Iddd4b3ad518dcf46bf9cb1d67e0b161ad1728166
",git fetch https://review.opendev.org/openstack/rally refs/changes/59/713459/1 && git format-patch -1 --stdout FETCH_HEAD,"['Dockerfile', 'tests/ci/rally_self_job.sh', 'tests/ci/playbooks/rally-docker-build/pre.yaml', 'etc/docker/motd', 'DOCKER_README.md', 'etc/docker/Dockerfile', 'etc/docker/hooks/post_checkout', 'tests/ci/playbooks/rally-docker-build/run.yaml', 'etc/docker/README_FIRST.rst']",9,02424a4dad80a7acba3f48895c96a4889cae8231,docker,,"ReadMe of <rally-repo>/etc/docker dir ===================================== We are using automated docker image builds on `Docker Hub <https://hub.docker.com/>`_ which allows to reduce time of making new releases. Docker Hub has one specific feature - each time it builds new image, it updates the description of the image. The description it takes from README file of the same directory as Dockerfile is located. That is why Dockerfile is placed to the separate directory with 2 README files: one for Docker Hub, another one (this one) for explanation of situation. ",82,97
openstack%2Ftrove~master~Ia010941fd9fde2036672f6e6ae8ac488d63c1d4a,openstack/trove,master,Ia010941fd9fde2036672f6e6ae8ac488d63c1d4a,Fixes the following syntax error of etc/apache2/trove apache conf,MERGED,2020-03-19 00:54:48.000000000,2020-03-21 14:01:34.000000000,2020-03-21 14:00:04.000000000,"[{'_account_id': 6732}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 00:54:48.000000000', 'files': ['etc/apache2/trove'], 'web_link': 'https://opendev.org/openstack/trove/commit/2f7f2c743bd470f26d551dce187c8bf0682e0f48', 'message': 'Fixes the following syntax error of etc/apache2/trove apache conf\n\n$ apache2ctl configtest\nAH00526: Syntax error on line 16 of /etc/apache2/sites-enabled/trove-api.conf:\nInvalid command \'through\', perhaps misspelled or defined by a module not included in the server configuration\nAction \'configtest\' failed.\nThe Apache error log may have more information.\n\nI refere the manual to configure my trove env.\nhttps://docs.openstack.org/trove/latest/install/apache-mod-wsgi.html\n\nHere are three steps to reproduce the error.\n\nStep1. Install apache2 and mod_wsgi\n-----------------------------------\n$ sudo apt-get install apache2 libapache2-mod-wsgi-py3\n\nStep2. Git clone and Configure apache\n-------------------------------------\n$ git clone https://opendev.org/openstack/trove.git; cd trove\n$ sudo cp etc/apache2/trove /etc/apache2/sites-available/trove-api.conf\n$ sudo a2ensite trove\nERROR: Site trove does not exist!\n\nI files this in https://bugs.launchpad.net/trove/+bug/1867811\n\n$ sudo a2ensite trove-api\nEnabling site trove-api.\nTo activate the new configuration, you need to run:\n  systemctl reload apache2\n\nStep3. Syntax error happens\n----------------------------\nBefore starting apache2, Add ""stack"" user and mkdir ""/var/log/httpd"" directory.\n$ sudo useradd -s /bin/bash -d /opt/stack -m stack\n$ sudo mkdir -p /var/log/httpd\n\n$ apache2ctl configtest\nAH00526: Syntax error on line 16 of /etc/apache2/sites-enabled/trove-api.conf:\nInvalid command \'through\', perhaps misspelled or defined by a module not included in the server configuration\npAction \'configtest\' failed.\nThe Apache error log may have more information.\n\n$ apache2ctl configtest\nAH00526: Syntax error on line 16 of /etc/apache2/sites-enabled/trove-api.conf:\nInvalid command \'through\', perhaps misspelled or defined by a module not included in the server configuration\nAction \'configtest\' failed.\nThe Apache error log may have more information.\n\n$ sudo vi /etc/apache2/sites-enabled/trove-api.conf\n$ diff  /etc/apache2/sites-enabled/trove-api.conf /etc/apache2/sites-enabled/trove-api.conf.orig\n16c16\n< # through mod_wsgi\n---\n> through mod_wsgi\n\n$ apache2ctl configtest\nAH00558: apache2: Could not reliably determine the server\'s fully qualified domain name, using 127.0.1.1. Set the \'ServerName\' directive globally to suppress this message\nSyntax OK\n\nI have tested on Ubuntu 18.04.4 LTS.\n\n$ uname -a\nLinux ubuntu-bionic 4.15.0-76-generic #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\n$ cat /etc/os-release\nNAME=""Ubuntu""\nVERSION=""18.04.4 LTS (Bionic Beaver)""\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=""Ubuntu 18.04.4 LTS""\nVERSION_ID=""18.04""\nHOME_URL=""https://www.ubuntu.com/""\nSUPPORT_URL=""https://help.ubuntu.com/""\nBUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""\nPRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""\nVERSION_CODENAME=bionic\nUBUNTU_CODENAME=bionic\n\nSigned-off-by: Hirotaka Wakabayashi <hiwkby@yahoo.com>\nChange-Id: Ia010941fd9fde2036672f6e6ae8ac488d63c1d4a\n'}]",0,713767,2f7f2c743bd470f26d551dce187c8bf0682e0f48,17,2,1,31737,,,0,"Fixes the following syntax error of etc/apache2/trove apache conf

$ apache2ctl configtest
AH00526: Syntax error on line 16 of /etc/apache2/sites-enabled/trove-api.conf:
Invalid command 'through', perhaps misspelled or defined by a module not included in the server configuration
Action 'configtest' failed.
The Apache error log may have more information.

I refere the manual to configure my trove env.
https://docs.openstack.org/trove/latest/install/apache-mod-wsgi.html

Here are three steps to reproduce the error.

Step1. Install apache2 and mod_wsgi
-----------------------------------
$ sudo apt-get install apache2 libapache2-mod-wsgi-py3

Step2. Git clone and Configure apache
-------------------------------------
$ git clone https://opendev.org/openstack/trove.git; cd trove
$ sudo cp etc/apache2/trove /etc/apache2/sites-available/trove-api.conf
$ sudo a2ensite trove
ERROR: Site trove does not exist!

I files this in https://bugs.launchpad.net/trove/+bug/1867811

$ sudo a2ensite trove-api
Enabling site trove-api.
To activate the new configuration, you need to run:
  systemctl reload apache2

Step3. Syntax error happens
----------------------------
Before starting apache2, Add ""stack"" user and mkdir ""/var/log/httpd"" directory.
$ sudo useradd -s /bin/bash -d /opt/stack -m stack
$ sudo mkdir -p /var/log/httpd

$ apache2ctl configtest
AH00526: Syntax error on line 16 of /etc/apache2/sites-enabled/trove-api.conf:
Invalid command 'through', perhaps misspelled or defined by a module not included in the server configuration
pAction 'configtest' failed.
The Apache error log may have more information.

$ apache2ctl configtest
AH00526: Syntax error on line 16 of /etc/apache2/sites-enabled/trove-api.conf:
Invalid command 'through', perhaps misspelled or defined by a module not included in the server configuration
Action 'configtest' failed.
The Apache error log may have more information.

$ sudo vi /etc/apache2/sites-enabled/trove-api.conf
$ diff  /etc/apache2/sites-enabled/trove-api.conf /etc/apache2/sites-enabled/trove-api.conf.orig
16c16
< # through mod_wsgi
---
> through mod_wsgi

$ apache2ctl configtest
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message
Syntax OK

I have tested on Ubuntu 18.04.4 LTS.

$ uname -a
Linux ubuntu-bionic 4.15.0-76-generic #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
$ cat /etc/os-release
NAME=""Ubuntu""
VERSION=""18.04.4 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.4 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic

Signed-off-by: Hirotaka Wakabayashi <hiwkby@yahoo.com>
Change-Id: Ia010941fd9fde2036672f6e6ae8ac488d63c1d4a
",git fetch https://review.opendev.org/openstack/trove refs/changes/67/713767/1 && git format-patch -1 --stdout FETCH_HEAD,['etc/apache2/trove'],1,2f7f2c743bd470f26d551dce187c8bf0682e0f48,hiwkby-apache-syntax-error,# through mod_wsgi,through mod_wsgi,1,1
openstack%2Fcinderlib~master~I0993e2aab8511400247042c128dc3aa1a06486ac,openstack/cinderlib,master,I0993e2aab8511400247042c128dc3aa1a06486ac,Fix CI jobs,MERGED,2020-03-19 16:43:10.000000000,2020-03-21 13:34:15.000000000,2020-03-21 13:32:25.000000000,"[{'_account_id': 12033}, {'_account_id': 22348}, {'_account_id': 27615}]","[{'number': 1, 'created': '2020-03-19 16:43:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinderlib/commit/2b335ddc565446526bad7a5ab8be8b4ae9d0a7fa', 'message': ""Fix test_set_snapshot\n\nCinder added a new restriction to the snapshots in the DB in change\nI47ff8115ae74e1a7ad41869159871ba614c388ac and now snapshot's\nvolume_type_id cannot be null.\n\nThis broke the cinderlib CI because one of our tests (test_set_snapshot\nof the DBMS persistence plugin) didn't create the volume in the DB\n(which can never happen in real use because it is saved on creation)\nwhich is when the volume type gets assigned.\n\nThis patch saves the volume so that it gets a volume type and its ID can\nbe used by the Snapshot.\n\nChange-Id: I0993e2aab8511400247042c128dc3aa1a06486ac\n""}, {'number': 2, 'created': '2020-03-20 17:17:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinderlib/commit/4a4fca120a255461d9d3237d1e51515a8029d0ba', 'message': ""Fix CI jobs\n\nCinder added a new restriction to the snapshots in the DB in change\nI47ff8115ae74e1a7ad41869159871ba614c388ac and now snapshot's\nvolume_type_id cannot be null.\n\nThis broke the cinderlib CI because one of our tests (test_set_snapshot\nof the DBMS persistence plugin) didn't create the volume in the DB\n(which can never happen in real use because it is saved on creation)\nwhich is when the volume type gets assigned.\n\nThis patch saves the volume so that it gets a volume type and its ID can\nbe used by the Snapshot.\n\nIt also updates the setuptools to a newer version to ensure SQLAlchemy\ninstalls properly.\n\nChange-Id: I0993e2aab8511400247042c128dc3aa1a06486ac\n""}, {'number': 3, 'created': '2020-03-20 19:08:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinderlib/commit/830d2a60f4e28bafd84ae2ca6838abae719606b8', 'message': ""Fix CI jobs\n\nCinder added a new restriction to the snapshots in the DB in change\nI47ff8115ae74e1a7ad41869159871ba614c388ac and now snapshot's\nvolume_type_id cannot be null.\n\nThis broke the cinderlib CI because one of our tests (test_set_snapshot\nof the DBMS persistence plugin) didn't create the volume in the DB\n(which can never happen in real use because it is saved on creation)\nwhich is when the volume type gets assigned.\n\nThis patch saves the volume so that it gets a volume type and its ID can\nbe used by the Snapshot.\n\nIt also includes a workaround for the LVM job for\nhttps://github.com/pypa/pip/issues/6264\n\nChange-Id: I0993e2aab8511400247042c128dc3aa1a06486ac\n""}, {'number': 4, 'created': '2020-03-20 19:27:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinderlib/commit/79ec9dbc48a83bbe7ac8ad98e41c4b0118867ae2', 'message': ""Fix CI jobs\n\nCinder added a new restriction to the snapshots in the DB in change\nI47ff8115ae74e1a7ad41869159871ba614c388ac and now snapshot's\nvolume_type_id cannot be null.\n\nThis broke the cinderlib CI because one of our tests (test_set_snapshot\nof the DBMS persistence plugin) didn't create the volume in the DB\n(which can never happen in real use because it is saved on creation)\nwhich is when the volume type gets assigned.\n\nThis patch saves the volume so that it gets a volume type and its ID can\nbe used by the Snapshot.\n\nIt also includes a workaround for the LVM job for\nhttps://github.com/pypa/pip/issues/6264\n\nChange-Id: I0993e2aab8511400247042c128dc3aa1a06486ac\n""}, {'number': 5, 'created': '2020-03-21 11:33:34.000000000', 'files': ['cinderlib/tests/unit/persistence/test_dbms.py', '.zuul.yaml', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/cinderlib/commit/3db2b2a85455f958da12e9f484467ba11afc2853', 'message': ""Fix CI jobs\n\nCinder added a new restriction to the snapshots in the DB in change\nI47ff8115ae74e1a7ad41869159871ba614c388ac and now snapshot's\nvolume_type_id cannot be null.\n\nThis broke the cinderlib CI because one of our tests (test_set_snapshot\nof the DBMS persistence plugin) didn't create the volume in the DB\n(which can never happen in real use because it is saved on creation)\nwhich is when the volume type gets assigned.\n\nThis patch saves the volume so that it gets a volume type and its ID can\nbe used by the Snapshot.\n\nIt also includes a workaround for the LVM job for\nhttps://github.com/pypa/pip/issues/6264\n\nChange-Id: I0993e2aab8511400247042c128dc3aa1a06486ac\n""}]",0,713931,3db2b2a85455f958da12e9f484467ba11afc2853,20,3,5,9535,,,0,"Fix CI jobs

Cinder added a new restriction to the snapshots in the DB in change
I47ff8115ae74e1a7ad41869159871ba614c388ac and now snapshot's
volume_type_id cannot be null.

This broke the cinderlib CI because one of our tests (test_set_snapshot
of the DBMS persistence plugin) didn't create the volume in the DB
(which can never happen in real use because it is saved on creation)
which is when the volume type gets assigned.

This patch saves the volume so that it gets a volume type and its ID can
be used by the Snapshot.

It also includes a workaround for the LVM job for
https://github.com/pypa/pip/issues/6264

Change-Id: I0993e2aab8511400247042c128dc3aa1a06486ac
",git fetch https://review.opendev.org/openstack/cinderlib refs/changes/31/713931/2 && git format-patch -1 --stdout FETCH_HEAD,['cinderlib/tests/unit/persistence/test_dbms.py'],1,2b335ddc565446526bad7a5ab8be8b4ae9d0a7fa,fix-gate-snapshot-vol-type-id," # This will assign a volume type, which is necessary for the snapshot vol.save()",,2,0
openstack%2Fkolla-ansible~stable%2Ftrain~I37409ca8d273b0426df0a648db222dc5432e738a,openstack/kolla-ansible,stable/train,I37409ca8d273b0426df0a648db222dc5432e738a,Fix service_mapped_to_host filter,MERGED,2020-03-21 08:38:55.000000000,2020-03-21 12:13:04.000000000,2020-03-21 12:11:18.000000000,"[{'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-21 08:38:55.000000000', 'files': ['kolla_ansible/tests/unit/test_filters.py', 'kolla_ansible/filters.py'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/8f566781051ee7b227e018d51296f24589384786', 'message': ""Fix service_mapped_to_host filter\n\nThe service_mapped_to_host filter is used to check if a service is\nmapped to a host, based on the group for the service or its\nhost_in_groups attribute if one exists. We check if the service's group\nis in the 'groups' list. However, to get the list of groups to which a\nhost belongs, we should use the 'group_names' list.\n\nThis filter is currently only used in neutron IPv6 module loading, so\nthe effects are minimal.\n\nChange-Id: I37409ca8d273b0426df0a648db222dc5432e738a\nCloses-Bug: #1868285\n(cherry picked from commit 35966c9186d868948a037bae7c58ac6e6279a4f0)\n""}]",0,714250,8f566781051ee7b227e018d51296f24589384786,8,3,1,30491,,,0,"Fix service_mapped_to_host filter

The service_mapped_to_host filter is used to check if a service is
mapped to a host, based on the group for the service or its
host_in_groups attribute if one exists. We check if the service's group
is in the 'groups' list. However, to get the list of groups to which a
host belongs, we should use the 'group_names' list.

This filter is currently only used in neutron IPv6 module loading, so
the effects are minimal.

Change-Id: I37409ca8d273b0426df0a648db222dc5432e738a
Closes-Bug: #1868285
(cherry picked from commit 35966c9186d868948a037bae7c58ac6e6279a4f0)
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/50/714250/1 && git format-patch -1 --stdout FETCH_HEAD,"['kolla_ansible/tests/unit/test_filters.py', 'kolla_ansible/filters.py']",2,8f566781051ee7b227e018d51296f24589384786,bug/1868285-stable/train," return group in context.get(""group_names"")"," return group in context.get(""groups"")",3,3
openstack%2Fdevstack~master~Ide7724ed3b4b5bb709b7dd79a367a5d2420bcda2,openstack/devstack,master,Ide7724ed3b4b5bb709b7dd79a367a5d2420bcda2,Updated from generate-devstack-plugins-list,MERGED,2020-03-21 06:15:51.000000000,2020-03-21 10:22:25.000000000,2020-03-21 10:21:02.000000000,"[{'_account_id': 22348}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-21 06:15:51.000000000', 'files': ['doc/source/plugin-registry.rst'], 'web_link': 'https://opendev.org/openstack/devstack/commit/37659927923473c13f4bec88855205d0ee28bcfb', 'message': 'Updated from generate-devstack-plugins-list\n\nChange-Id: Ide7724ed3b4b5bb709b7dd79a367a5d2420bcda2\n'}]",0,714244,37659927923473c13f4bec88855205d0ee28bcfb,7,2,1,11131,,,0,"Updated from generate-devstack-plugins-list

Change-Id: Ide7724ed3b4b5bb709b7dd79a367a5d2420bcda2
",git fetch https://review.opendev.org/openstack/devstack refs/changes/44/714244/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/plugin-registry.rst'],1,37659927923473c13f4bec88855205d0ee28bcfb,openstack/devstack/plugins,openstack/devstack-plugin-nfs `https://opendev.org/openstack/devstack-plugin-nfs <https://opendev.org/openstack/devstack-plugin-nfs>`__,x/devstack-plugin-nfs `https://opendev.org/x/devstack-plugin-nfs <https://opendev.org/x/devstack-plugin-nfs>`__,1,1
openstack%2Fneutron~master~Icf346e58904412a97e5e22155166821faed19fc2,openstack/neutron,master,Icf346e58904412a97e5e22155166821faed19fc2,Remove native openflow driver 'deferred' code,MERGED,2020-03-04 04:11:21.000000000,2020-03-21 09:33:25.000000000,2020-03-20 00:08:12.000000000,"[{'_account_id': 6854}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 13995}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 24791}, {'_account_id': 26622}]","[{'number': 1, 'created': '2020-03-04 04:11:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9209c41b69fabbe7a71c123b5dec9e86bd2111ac', 'message': ""Remove native openflow driver 'deffered' code\n\nFrom the comments, this code existed to have API compatibility\nbetween the native openflow and ovs-ofctl of_interface drivers,\nbut since the latter was removed, this code is no longer\nnecessary.\n\nChange-Id: Icf346e58904412a97e5e22155166821faed19fc2\n""}, {'number': 2, 'created': '2020-03-04 04:12:05.000000000', 'files': ['neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py', 'neutron/plugins/ml2/drivers/openvswitch/agent/openflow/native/br_tun.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/test_ovs_tunnel.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/test_ovs_neutron_agent.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/d954cd4fea64fff3714d7bd1a609dea850c7f056', 'message': ""Remove native openflow driver 'deferred' code\n\nFrom the comments, this code existed to have API compatibility\nbetween the native openflow and ovs-ofctl of_interface drivers,\nbut since the latter was removed, this code is no longer\nnecessary.\n\nChange-Id: Icf346e58904412a97e5e22155166821faed19fc2\n""}]",8,711158,d954cd4fea64fff3714d7bd1a609dea850c7f056,20,8,2,1131,,,0,"Remove native openflow driver 'deferred' code

From the comments, this code existed to have API compatibility
between the native openflow and ovs-ofctl of_interface drivers,
but since the latter was removed, this code is no longer
necessary.

Change-Id: Icf346e58904412a97e5e22155166821faed19fc2
",git fetch https://review.opendev.org/openstack/neutron refs/changes/58/711158/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py', 'neutron/plugins/ml2/drivers/openvswitch/agent/openflow/native/br_tun.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/test_ovs_tunnel.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/test_ovs_neutron_agent.py']",4,9209c41b69fabbe7a71c123b5dec9e86bd2111ac,remove-openflow-deferred," with mock.patch.object(self.agent, 'tun_br', autospec=True) as tun_br: tun_br.add_port.assert_not_called() tun_br.delete_port.assert_not_called() mock.call.install_arp_responder('vlan1', FAKE_IP1, FAKE_MAC), mock.call.install_unicast_to_tun('vlan1', 'seg1', '2', FAKE_MAC), mock.call.install_flood_to_tun('vlan1', 'seg1', set(['1', '2'])), mock.call.delete_arp_responder('vlan2', FAKE_IP1), mock.call.delete_unicast_to_tun('vlan2', FAKE_MAC), mock.call.install_flood_to_tun('vlan2', 'seg2', set(['1'])), mock.call.delete_port('gre-02020202'), mock.call.cleanup_tunnel_port('2'), tun_br, 'gre-0a0a0a0a', '10.10.10.10', 'gre') with mock.patch.object(self.agent, 'tun_br', autospec=True) as tun_br: tun_br.delete_port.assert_called_once_with('gre-02020202') with mock.patch.object(self.agent, 'tun_br', autospec=True) as tun_br: tun_br.assert_has_calls([","import contextlib @contextlib.contextmanager def bridge_deferred(*args, **kwargs): yield self.agent.int_br.deferred = mock.Mock(side_effect=bridge_deferred) with mock.patch.object(self.agent.tun_br, ""deferred"") as defer_fn: defer_fn.assert_not_called() defer_fn.assert_not_called() deferred_br_call = mock.call.deferred().__enter__() deferred_br_call.install_arp_responder('vlan1', FAKE_IP1, FAKE_MAC), deferred_br_call.install_unicast_to_tun('vlan1', 'seg1', '2', FAKE_MAC), deferred_br_call.install_flood_to_tun('vlan1', 'seg1', set(['1', '2'])), deferred_br_call = mock.call.deferred().__enter__() mock.call.deferred(), mock.call.deferred().__enter__(), deferred_br_call.delete_arp_responder('vlan2', FAKE_IP1), deferred_br_call.delete_unicast_to_tun('vlan2', FAKE_MAC), deferred_br_call.install_flood_to_tun('vlan2', 'seg2', set(['1'])), deferred_br_call.delete_port('gre-02020202'), deferred_br_call.cleanup_tunnel_port('2'), mock.call.deferred().__exit__(None, None, None), deferred_br = tun_br.deferred().__enter__() deferred_br, 'gre-0a0a0a0a', '10.10.10.10', 'gre') with mock.patch.object(self.agent.tun_br, 'deferred') as defer_fn,\ mock.patch.object(self.agent.tun_br, 'delete_port') as delete_port_fn: deferred_br = defer_fn().__enter__() deferred_br.delete_port.assert_called_once_with('gre-02020202') delete_port_fn.assert_not_called() with mock.patch.object(self.agent.tun_br, 'deferred') as deferred_fn: deferred_br = deferred_fn().__enter__() deferred_br.assert_has_calls([",59,123
openstack%2Fsecurity-doc~master~I52c340824332ef698d07dbff6e3262e0e9d225b9,openstack/security-doc,master,I52c340824332ef698d07dbff6e3262e0e9d225b9,Updated from openstack-manuals,MERGED,2020-03-21 08:38:56.000000000,2020-03-21 09:11:29.000000000,2020-03-21 09:10:06.000000000,"[{'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-21 08:38:56.000000000', 'files': ['common/source/locale/id/LC_MESSAGES/common.po'], 'web_link': 'https://opendev.org/openstack/security-doc/commit/95483ac0c3dc55eedd353a133eed893b3ceb7434', 'message': 'Updated from openstack-manuals\n\nChange-Id: I52c340824332ef698d07dbff6e3262e0e9d225b9\n'}]",0,714251,95483ac0c3dc55eedd353a133eed893b3ceb7434,7,2,1,11131,,,0,"Updated from openstack-manuals

Change-Id: I52c340824332ef698d07dbff6e3262e0e9d225b9
",git fetch https://review.opendev.org/openstack/security-doc refs/changes/51/714251/1 && git format-patch -1 --stdout FETCH_HEAD,['common/source/locale/id/LC_MESSAGES/common.po'],1,95483ac0c3dc55eedd353a133eed893b3ceb7434,openstack/openstack-manuals,"""POT-Creation-Date: 2020-03-15 09:19+0000\n""""PO-Revision-Date: 2020-03-20 08:32+0000\n""""Each OpenStack release has a code name. Code names ascend in alphabetical "" ""order: Austin, Bexar, Cactus, Diablo, Essex, Folsom, Grizzly, Havana, "" ""Icehouse, Juno, Kilo, Liberty, Mitaka, Newton, Ocata, Pike, Queens, Rocky, "" ""Stein, Train, Ussuri, Victoria, and Wallaby."" msgstr """" ""Setiap rilis OpenStack memiliki nama kode. Nama kode naik dalam urutan "" ""abjad: Austin, Bexar, Cactus, Diablo, Essex, Folsom, Grizzly, Havana, "" ""Icehouse, Juno, Kilo, Liberty, Mitaka, Newton, Ocata, Pike, Queens, Rocky, "" ""Stein, Train, Ussuri, Victoria , dan Wallaby."" msgid """"msgid """" ""The Victoria name was the last name where code names are cities or counties "" ""near where the corresponding OpenStack design summit took place. An "" ""exception, called the Waldon exception, was granted to elements of the state "" ""flag that sound especially cool. Code names are chosen by popular vote."" msgstr """" ""Nama Victoria adalah nama terakhir di mana nama kode adalah kota atau "" ""kabupaten di dekat tempat pertemuan puncak desain OpenStack. Pengecualian, "" ""yang disebut pengecualian Waldon, diberikan kepada elemen-elemen bendera "" ""negara yang terdengar sangat keren. Nama kode dipilih dengan suara populer."" msgid """" ""The code name for the twenty third release of OpenStack. Wallabies are "" ""native to Australia, which at the start of this naming period was "" ""experiencing unprecedented wild fires."" msgstr """" ""Nama kode untuk rilis OpenStack kedua puluh tiga. Walabi adalah penduduk "" ""asli Australia, yang pada awal periode penamaan ini mengalami kebakaran "" ""hutan yang belum pernah terjadi sebelumnya."" msgid ""Wallaby"" msgstr ""Wallaby"" msgid """" ""Wallaby was the first code name choosen by a new policy: Code names are "" ""choosen by the community following the alphabet, for details see `release "" ""name criteria <https://governance.openstack.org/tc/reference/release-naming."" ""html#release-name-criteria>`__."" msgstr """" ""Wallaby adalah nama kode pertama yang dipilih oleh kebijakan baru: Nama kode "" ""dipilih oleh komunitas mengikuti alfabet, untuk detail lihat `release name "" ""criteria <https://governance.openstack.org/tc/reference/release-naming."" ""html#release-name-criteria>`__."" ","""POT-Creation-Date: 2020-03-05 11:23+0000\n""""PO-Revision-Date: 2020-01-22 04:06+0000\n""",47,2
openstack%2Fopenstack-manuals~master~I3daa33d83dd87dcce29f67a7005b11f0dd850004,openstack/openstack-manuals,master,I3daa33d83dd87dcce29f67a7005b11f0dd850004,Imported Translations from Zanata,MERGED,2020-03-21 07:03:22.000000000,2020-03-21 09:09:22.000000000,2020-03-21 08:36:01.000000000,"[{'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-21 07:03:22.000000000', 'files': ['doc/common/source/locale/id/LC_MESSAGES/common.po'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/2b3beb127795d29b247264fa0ba9e742c183e077', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I3daa33d83dd87dcce29f67a7005b11f0dd850004\n'}]",0,714245,2b3beb127795d29b247264fa0ba9e742c183e077,7,2,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: I3daa33d83dd87dcce29f67a7005b11f0dd850004
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/45/714245/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/common/source/locale/id/LC_MESSAGES/common.po'],1,2b3beb127795d29b247264fa0ba9e742c183e077,zanata/translations,"""POT-Creation-Date: 2020-03-15 09:19+0000\n""""PO-Revision-Date: 2020-03-20 08:32+0000\n""""Each OpenStack release has a code name. Code names ascend in alphabetical "" ""order: Austin, Bexar, Cactus, Diablo, Essex, Folsom, Grizzly, Havana, "" ""Icehouse, Juno, Kilo, Liberty, Mitaka, Newton, Ocata, Pike, Queens, Rocky, "" ""Stein, Train, Ussuri, Victoria, and Wallaby."" msgstr """" ""Setiap rilis OpenStack memiliki nama kode. Nama kode naik dalam urutan "" ""abjad: Austin, Bexar, Cactus, Diablo, Essex, Folsom, Grizzly, Havana, "" ""Icehouse, Juno, Kilo, Liberty, Mitaka, Newton, Ocata, Pike, Queens, Rocky, "" ""Stein, Train, Ussuri, Victoria , dan Wallaby."" msgid """"msgid """" ""The Victoria name was the last name where code names are cities or counties "" ""near where the corresponding OpenStack design summit took place. An "" ""exception, called the Waldon exception, was granted to elements of the state "" ""flag that sound especially cool. Code names are chosen by popular vote."" msgstr """" ""Nama Victoria adalah nama terakhir di mana nama kode adalah kota atau "" ""kabupaten di dekat tempat pertemuan puncak desain OpenStack. Pengecualian, "" ""yang disebut pengecualian Waldon, diberikan kepada elemen-elemen bendera "" ""negara yang terdengar sangat keren. Nama kode dipilih dengan suara populer."" msgid """" ""The code name for the twenty third release of OpenStack. Wallabies are "" ""native to Australia, which at the start of this naming period was "" ""experiencing unprecedented wild fires."" msgstr """" ""Nama kode untuk rilis OpenStack kedua puluh tiga. Walabi adalah penduduk "" ""asli Australia, yang pada awal periode penamaan ini mengalami kebakaran "" ""hutan yang belum pernah terjadi sebelumnya."" msgid ""Wallaby"" msgstr ""Wallaby"" msgid """" ""Wallaby was the first code name choosen by a new policy: Code names are "" ""choosen by the community following the alphabet, for details see `release "" ""name criteria <https://governance.openstack.org/tc/reference/release-naming."" ""html#release-name-criteria>`__."" msgstr """" ""Wallaby adalah nama kode pertama yang dipilih oleh kebijakan baru: Nama kode "" ""dipilih oleh komunitas mengikuti alfabet, untuk detail lihat `release name "" ""criteria <https://governance.openstack.org/tc/reference/release-naming."" ""html#release-name-criteria>`__."" ","""POT-Creation-Date: 2020-03-05 11:23+0000\n""""PO-Revision-Date: 2020-01-22 04:06+0000\n""",47,2
openstack%2Fneutron~master~I4d19f2a457e004306fdf40980a943073f1b8704a,openstack/neutron,master,I4d19f2a457e004306fdf40980a943073f1b8704a,Run fullstack security group test always serially,MERGED,2020-03-03 10:30:11.000000000,2020-03-21 08:52:29.000000000,2020-03-21 08:49:14.000000000,"[{'_account_id': 1131}, {'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 13995}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 24791}, {'_account_id': 26622}, {'_account_id': 27654}]","[{'number': 1, 'created': '2020-03-03 10:30:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/59cb9bbd6ff35ccb7aacf69aaa7320688791bc4e', 'message': 'Run fullstack security group test always serially\n\nSecurity groups are based on iptables in many cases and can interfere\nbetween other tests. So it means that if some other test will manipulate\nwith iptables on host during SG related test is run, this SG related\ntest may fail without any obvious reason.\nSo lets try to run those test serially to be sure that no other agents\nwill manipulate iptables/openflow rules in same time.\n\nChange-Id: I4d19f2a457e004306fdf40980a943073f1b8704a\nCloses-Bug: #1779328\n'}, {'number': 2, 'created': '2020-03-03 14:18:33.000000000', 'files': ['zuul.d/base.yaml', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/neutron/commit/8456240e7da79f0409259597d796b7d6cab3298d', 'message': 'Run fullstack security group test always serially\n\nSecurity groups are based on iptables in many cases and can interfere\nbetween other tests. So it means that if some other test will manipulate\nwith iptables on host during SG related test is run, this SG related\ntest may fail without any obvious reason.\nSo lets try to run those test serially to be sure that no other agents\nwill manipulate iptables/openflow rules in same time.\n\nChange-Id: I4d19f2a457e004306fdf40980a943073f1b8704a\nCloses-Bug: #1779328\n'}]",0,710964,8456240e7da79f0409259597d796b7d6cab3298d,20,9,2,11975,,,0,"Run fullstack security group test always serially

Security groups are based on iptables in many cases and can interfere
between other tests. So it means that if some other test will manipulate
with iptables on host during SG related test is run, this SG related
test may fail without any obvious reason.
So lets try to run those test serially to be sure that no other agents
will manipulate iptables/openflow rules in same time.

Change-Id: I4d19f2a457e004306fdf40980a943073f1b8704a
Closes-Bug: #1779328
",git fetch https://review.opendev.org/openstack/neutron refs/changes/64/710964/2 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/base.yaml', 'tox.ini']",2,59cb9bbd6ff35ccb7aacf69aaa7320688791bc4e,bug/1779328,[testenv:dsvm-fullstack-gate] setenv = {[testenv:dsvm-fullstack]setenv} deps = {[testenv:dsvm-fullstack]deps} commands = {toxinidir}/tools/generate_dhclient_script_for_fullstack.sh {envdir} {toxinidir}/tools/deploy_rootwrap.sh {toxinidir} {envdir}/etc {envdir}/bin stestr run --concurrency 4 --black-regex neutron.tests.fullstack.test_securitygroup.TestSecurityGroupsSameNetwork.test_securitygroup {posargs} stestr run --concurrency 1 neutron.tests.fullstack.test_securitygroup.TestSecurityGroupsSameNetwork.test_securitygroup {posargs} ,,10,1
openstack%2Fnova~master~Ifb3afb13aff7e103c2e80ade817d0e63b624604a,openstack/nova,master,Ifb3afb13aff7e103c2e80ade817d0e63b624604a,Add config option for neutron client retries,MERGED,2020-03-11 02:43:24.000000000,2020-03-21 08:52:25.000000000,2020-03-21 08:49:20.000000000,"[{'_account_id': 4690}, {'_account_id': 5754}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10135}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-03-11 02:43:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/873896922ba33b299876fe9348ee2674825d9a5a', 'message': ""Add config option for neutron client retries\n\nNova can occasionally fail to carry out server actions which require\ncalls to neutron API if haproxy happens to close a connection after\nidle time if an incoming request attempts to re-use the connection\nwhile it is being torn down.\n\nIn order to be more resilient to this scenario, we can add a config\noption for neutron client to retry requests, similar to our existing\nCONF.cinder.http_retries and CONF.glance.num_retries options.\n\nBecause we create our neutron client [1] using a keystoneauth1 session\n[2], we can set the 'connect_retries' keyword argument to let\nkeystoneauth1 handle connection retries.\n\nCloses-Bug: #1866937\n\n[1] https://github.com/openstack/nova/blob/57459c3429ce62975cebd0cd70936785bdf2f3a4/nova/network/neutron.py#L226-L237\n[2] https://docs.openstack.org/keystoneauth/latest/api/keystoneauth1.session.html#keystoneauth1.session.Session\n\nChange-Id: Ifb3afb13aff7e103c2e80ade817d0e63b624604a\n""}, {'number': 2, 'created': '2020-03-11 18:55:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/cadfc8933e73e97524f8889147ccb2e539fc930b', 'message': ""Add config option for neutron client retries\n\nNova can occasionally fail to carry out server actions which require\ncalls to neutron API if haproxy happens to close a connection after\nidle time if an incoming request attempts to re-use the connection\nwhile it is being torn down.\n\nIn order to be more resilient to this scenario, we can add a config\noption for neutron client to retry requests, similar to our existing\nCONF.cinder.http_retries and CONF.glance.num_retries options.\n\nBecause we create our neutron client [1] using a keystoneauth1 session\n[2], we can set the 'connect_retries' keyword argument to let\nkeystoneauth1 handle connection retries.\n\nCloses-Bug: #1866937\n\n[1] https://github.com/openstack/nova/blob/57459c3429ce62975cebd0cd70936785bdf2f3a4/nova/network/neutron.py#L226-L237\n[2] https://docs.openstack.org/keystoneauth/latest/api/keystoneauth1.session.html#keystoneauth1.session.Session\n\nChange-Id: Ifb3afb13aff7e103c2e80ade817d0e63b624604a\n""}, {'number': 3, 'created': '2020-03-11 18:56:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/86596c5d1e32012e4ea489e8a9770b7c92474d6b', 'message': ""Add config option for neutron client retries\n\nNova can occasionally fail to carry out server actions which require\ncalls to neutron API if haproxy happens to close a connection after\nidle time if an incoming request attempts to re-use the connection\nwhile it is being torn down.\n\nIn order to be more resilient to this scenario, we can add a config\noption for neutron client to retry requests, similar to our existing\nCONF.cinder.http_retries and CONF.glance.num_retries options.\n\nBecause we create our neutron client [1] using a keystoneauth1 session\n[2], we can set the 'connect_retries' keyword argument to let\nkeystoneauth1 handle connection retries.\n\nCloses-Bug: #1866937\n\n[1] https://github.com/openstack/nova/blob/57459c3429ce62975cebd0cd70936785bdf2f3a4/nova/network/neutron.py#L226-L237\n[2] https://docs.openstack.org/keystoneauth/latest/api/keystoneauth1.session.html#keystoneauth1.session.Session\n\nChange-Id: Ifb3afb13aff7e103c2e80ade817d0e63b624604a\n""}, {'number': 4, 'created': '2020-03-12 18:06:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/dbb17cc70956f597a8d46bf573d7bacefe6bb72b', 'message': ""Add config option for neutron client retries\n\nNova can occasionally fail to carry out server actions which require\ncalls to neutron API if haproxy happens to close a connection after\nidle time if an incoming request attempts to re-use the connection\nwhile it is being torn down.\n\nIn order to be more resilient to this scenario, we can add a config\noption for neutron client to retry requests, similar to our existing\nCONF.cinder.http_retries and CONF.glance.num_retries options.\n\nBecause we create our neutron client [1] using a keystoneauth1 session\n[2], we can set the 'connect_retries' keyword argument to let\nkeystoneauth1 handle connection retries.\n\nCloses-Bug: #1866937\n\n[1] https://github.com/openstack/nova/blob/57459c3429ce62975cebd0cd70936785bdf2f3a4/nova/network/neutron.py#L226-L237\n[2] https://docs.openstack.org/keystoneauth/latest/api/keystoneauth1.session.html#keystoneauth1.session.Session\n\nChange-Id: Ifb3afb13aff7e103c2e80ade817d0e63b624604a\n""}, {'number': 5, 'created': '2020-03-19 15:25:58.000000000', 'files': ['nova/conf/neutron.py', 'nova/tests/unit/network/test_neutron.py', 'nova/network/neutron.py', 'releasenotes/notes/neutron-connection-retries-c276010afe238abc.yaml'], 'web_link': 'https://opendev.org/openstack/nova/commit/0e34ed9733e3f23d162e3e428795807386abf1cb', 'message': ""Add config option for neutron client retries\n\nNova can occasionally fail to carry out server actions which require\ncalls to neutron API if haproxy happens to close a connection after\nidle time if an incoming request attempts to re-use the connection\nwhile it is being torn down.\n\nIn order to be more resilient to this scenario, we can add a config\noption for neutron client to retry requests, similar to our existing\nCONF.cinder.http_retries and CONF.glance.num_retries options.\n\nBecause we create our neutron client [1] using a keystoneauth1 session\n[2], we can set the 'connect_retries' keyword argument to let\nkeystoneauth1 handle connection retries.\n\nCloses-Bug: #1866937\n\n[1] https://github.com/openstack/nova/blob/57459c3429ce62975cebd0cd70936785bdf2f3a4/nova/network/neutron.py#L226-L237\n[2] https://docs.openstack.org/keystoneauth/latest/api/keystoneauth1.session.html#keystoneauth1.session.Session\n\nChange-Id: Ifb3afb13aff7e103c2e80ade817d0e63b624604a\n""}]",9,712226,0e34ed9733e3f23d162e3e428795807386abf1cb,48,13,5,4690,,,0,"Add config option for neutron client retries

Nova can occasionally fail to carry out server actions which require
calls to neutron API if haproxy happens to close a connection after
idle time if an incoming request attempts to re-use the connection
while it is being torn down.

In order to be more resilient to this scenario, we can add a config
option for neutron client to retry requests, similar to our existing
CONF.cinder.http_retries and CONF.glance.num_retries options.

Because we create our neutron client [1] using a keystoneauth1 session
[2], we can set the 'connect_retries' keyword argument to let
keystoneauth1 handle connection retries.

Closes-Bug: #1866937

[1] https://github.com/openstack/nova/blob/57459c3429ce62975cebd0cd70936785bdf2f3a4/nova/network/neutron.py#L226-L237
[2] https://docs.openstack.org/keystoneauth/latest/api/keystoneauth1.session.html#keystoneauth1.session.Session

Change-Id: Ifb3afb13aff7e103c2e80ade817d0e63b624604a
",git fetch https://review.opendev.org/openstack/nova refs/changes/26/712226/4 && git format-patch -1 --stdout FETCH_HEAD,"['nova/conf/neutron.py', 'nova/tests/unit/network/test_neutron.py', 'nova/network/neutron.py']",3,873896922ba33b299876fe9348ee2674825d9a5a,bug/1866937," global_request_id=context.global_id, connect_retries=CONF.neutron.http_retries)", global_request_id=context.global_id),24,1
openstack%2Fcontributor-guide~master~Id69bd73ae0cb9588f74e01b94d4b1ba5d0ea6179,openstack/contributor-guide,master,Id69bd73ae0cb9588f74e01b94d4b1ba5d0ea6179,Imported Translations from Zanata,MERGED,2020-03-21 07:04:30.000000000,2020-03-21 08:39:51.000000000,2020-03-21 08:37:53.000000000,"[{'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-21 07:04:30.000000000', 'files': ['doc/source/locale/id/LC_MESSAGES/doc-common.po', 'doc/source/locale/id/LC_MESSAGES/doc-code-and-documentation.po'], 'web_link': 'https://opendev.org/openstack/contributor-guide/commit/7723e0798721e6450ca1ee089eca9bcd0a48aea9', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: Id69bd73ae0cb9588f74e01b94d4b1ba5d0ea6179\n'}]",0,714246,7723e0798721e6450ca1ee089eca9bcd0a48aea9,7,2,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: Id69bd73ae0cb9588f74e01b94d4b1ba5d0ea6179
",git fetch https://review.opendev.org/openstack/contributor-guide refs/changes/46/714246/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/locale/id/LC_MESSAGES/doc-common.po', 'doc/source/locale/id/LC_MESSAGES/doc-code-and-documentation.po']",2,7723e0798721e6450ca1ee089eca9bcd0a48aea9,zanata/translations,"""POT-Creation-Date: 2020-03-11 07:39+0000\n""""PO-Revision-Date: 2020-03-20 07:48+0000\n""""There is a `Sandbox project <https://launchpad.net/openstack-dev-sandbox>`_ "" ""available there which is created to track bugs and issues related to the "" ""opendev/sandbox repository."" msgstr """" ""Ada `Sandbox project <https://launchpad.net/openstack-dev-sandbox>` _ "" ""tersedia di sana yang dibuat untuk melacak bug dan masalah yang berkaitan "" ""dengan repositori opendev/sandbox."" msgid """"""This tutorial walks through a simple scenario of developing multiple change "" ""sets in a series on the same branch. If you wish, you can follow along, "" ""using the `sandbox repository <https://opendev.org/opendev/sandbox/>`_, "" ""executing the commands exactly as they're laid out."" msgstr """" ""Tutorial ini membahas skenario sederhana untuk mengembangkan beberapa set "" ""perubahan dalam seri di branch yang sama. Jika Anda mau, Anda bisa "" ""mengikuti, menggunakan `sandbox repository <https://opendev.org/opendev/"" ""sandbox/>` _, menjalankan perintah persis seperti yang mereka buat."" msgid """"msgid """" ""To get DevStack you will need to clone it from the ``devstack`` repository "" ""under ``openstack``."" msgstr """" ""Untuk mendapatkan DevStack Anda harus mengkloningnya dari repositori "" ""``devstack`` di bawah ``openstack``."" msgid """" ""You can find a `sandbox Git repository <https://opendev.org/opendev/"" ""sandbox>`_ where you can practice the Git commands before you put together a "" ""patch to resolve a bug or implement a new functionality."" msgstr """" ""Anda dapat menemukan repositori `sandbox Git <https://opendev.org/opendev/"" ""sandbox>` _ di mana Anda dapat mempraktikkan perintah Git sebelum Anda "" ""membuat patch untuk menyelesaikan bug atau mengimplementasikan "" ""fungsionalitas baru."" ","""POT-Creation-Date: 2020-03-09 17:00+0000\n""""PO-Revision-Date: 2020-02-27 06:43+0000\n""",108,4
openstack%2Fkolla-ansible~master~I37409ca8d273b0426df0a648db222dc5432e738a,openstack/kolla-ansible,master,I37409ca8d273b0426df0a648db222dc5432e738a,Fix service_mapped_to_host filter,MERGED,2020-03-20 17:04:00.000000000,2020-03-21 08:38:55.000000000,2020-03-20 20:22:17.000000000,"[{'_account_id': 22348}, {'_account_id': 24072}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-20 17:04:00.000000000', 'files': ['kolla_ansible/tests/unit/test_filters.py', 'kolla_ansible/filters.py'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/35966c9186d868948a037bae7c58ac6e6279a4f0', 'message': ""Fix service_mapped_to_host filter\n\nThe service_mapped_to_host filter is used to check if a service is\nmapped to a host, based on the group for the service or its\nhost_in_groups attribute if one exists. We check if the service's group\nis in the 'groups' list. However, to get the list of groups to which a\nhost belongs, we should use the 'group_names' list.\n\nThis filter is currently only used in neutron IPv6 module loading, so\nthe effects are minimal.\n\nChange-Id: I37409ca8d273b0426df0a648db222dc5432e738a\nCloses-Bug: #1868285\n""}]",0,714177,35966c9186d868948a037bae7c58ac6e6279a4f0,10,3,1,14826,,,0,"Fix service_mapped_to_host filter

The service_mapped_to_host filter is used to check if a service is
mapped to a host, based on the group for the service or its
host_in_groups attribute if one exists. We check if the service's group
is in the 'groups' list. However, to get the list of groups to which a
host belongs, we should use the 'group_names' list.

This filter is currently only used in neutron IPv6 module loading, so
the effects are minimal.

Change-Id: I37409ca8d273b0426df0a648db222dc5432e738a
Closes-Bug: #1868285
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/77/714177/1 && git format-patch -1 --stdout FETCH_HEAD,"['kolla_ansible/tests/unit/test_filters.py', 'kolla_ansible/filters.py']",2,35966c9186d868948a037bae7c58ac6e6279a4f0,bug/1868285," return group in context.get(""group_names"")"," return group in context.get(""groups"")",3,3
openstack%2Fcyborg~master~I7fa9e31d80f38ad3b5a2a56b899edfcbadcaf961,openstack/cyborg,master,I7fa9e31d80f38ad3b5a2a56b899edfcbadcaf961,PDF documentation build depends packages,ABANDONED,2020-03-21 07:53:44.000000000,2020-03-21 08:13:23.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2020-03-21 07:53:44.000000000', 'files': ['doc/requirements.txt'], 'web_link': 'https://opendev.org/openstack/cyborg/commit/877a86c0e48ef61424cca8b873de7609ff619ac5', 'message': ""PDF documentation build depends packages\n\n- sphinxcontrib-svg2pdfconverter is used to handle SVG properly.\n- Remove the py27's sphinx\n\nChange-Id: I7fa9e31d80f38ad3b5a2a56b899edfcbadcaf961\n""}]",0,714248,877a86c0e48ef61424cca8b873de7609ff619ac5,3,1,1,26458,,,0,"PDF documentation build depends packages

- sphinxcontrib-svg2pdfconverter is used to handle SVG properly.
- Remove the py27's sphinx

Change-Id: I7fa9e31d80f38ad3b5a2a56b899edfcbadcaf961
",git fetch https://review.opendev.org/openstack/cyborg refs/changes/48/714248/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/requirements.txt'],1,877a86c0e48ef61424cca8b873de7609ff619ac5,build-pdf-docs,"sphinx>=1.8.0,!=2.1.0 # BSDsphinxcontrib-svg2pdfconverter>=0.1.0 # BSD","sphinx!=1.6.6,!=1.6.7,>=1.6.2,<2.0.0;python_version=='2.7' # BSD sphinx!=1.6.6,!=1.6.7,>=1.6.2;python_version>='3.4' # BSD",2,2
openstack%2Fcinder~master~I181fefbc8cc3d3fdfc996f4367658ea59a51a9fc,openstack/cinder,master,I181fefbc8cc3d3fdfc996f4367658ea59a51a9fc,QNAP: Fix login on Python3,MERGED,2019-07-23 11:25:27.000000000,2020-03-21 07:32:57.000000000,2020-03-19 14:03:11.000000000,"[{'_account_id': 5997}, {'_account_id': 9008}, {'_account_id': 9732}, {'_account_id': 10118}, {'_account_id': 11611}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 13144}, {'_account_id': 14384}, {'_account_id': 15386}, {'_account_id': 15670}, {'_account_id': 15941}, {'_account_id': 16897}, {'_account_id': 18120}, {'_account_id': 18883}, {'_account_id': 20284}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 21976}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23613}, {'_account_id': 24814}, {'_account_id': 24815}, {'_account_id': 24921}, {'_account_id': 25243}, {'_account_id': 25678}, {'_account_id': 26077}, {'_account_id': 26537}, {'_account_id': 28801}, {'_account_id': 29705}, {'_account_id': 29716}, {'_account_id': 30688}]","[{'number': 1, 'created': '2019-07-23 11:25:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6eb882f9e330076bc7c01c73d1069bbb17ee7f41', 'message': 'QNAP: Fix login on Python3\n\nMethod b64encode from base64 library returns different values for Python\n2 and 3.\n\nFor Python 2 you get a string, but on Python 3 you get a bytes object,\nwhich when converted to a string using the six.text_type method will\nresult in a string of the form ""b\'password\'"" instead of getting\n""password"".\n\nThis will result in posting a request with the wrong password and\nfailing to authenticate.\n\nTo fix this we only need to call the decode method on the result of\nb64encode, as it works for both string and bytes objects and will always\nreturn the string we want.\n\nCloses-Bug: #1837538\nChange-Id: I181fefbc8cc3d3fdfc996f4367658ea59a51a9fc\n'}, {'number': 2, 'created': '2020-03-16 19:11:28.000000000', 'files': ['cinder/volume/drivers/qnap.py', 'cinder/tests/unit/volume/drivers/test_qnap.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/d5950b36d136a49a3e34cf6654a732a577b8a576', 'message': 'QNAP: Fix login on Python3\n\nMethod b64encode from base64 library returns different values for Python\n2 and 3.\n\nFor Python 2 you get a string, but on Python 3 you get a bytes object,\nwhich when converted to a string using the six.text_type method will\nresult in a string of the form ""b\'password\'"" instead of getting\n""password"".\n\nThis will result in posting a request with the wrong password and\nfailing to authenticate.\n\nTo fix this we only need to call the decode method on the result of\nb64encode, as it works for both string and bytes objects and will always\nreturn the string we want.\n\nCloses-Bug: #1837538\nChange-Id: I181fefbc8cc3d3fdfc996f4367658ea59a51a9fc\n'}]",0,672265,d5950b36d136a49a3e34cf6654a732a577b8a576,64,37,2,9535,,,0,"QNAP: Fix login on Python3

Method b64encode from base64 library returns different values for Python
2 and 3.

For Python 2 you get a string, but on Python 3 you get a bytes object,
which when converted to a string using the six.text_type method will
result in a string of the form ""b'password'"" instead of getting
""password"".

This will result in posting a request with the wrong password and
failing to authenticate.

To fix this we only need to call the decode method on the result of
b64encode, as it works for both string and bytes objects and will always
return the string we want.

Closes-Bug: #1837538
Change-Id: I181fefbc8cc3d3fdfc996f4367658ea59a51a9fc
",git fetch https://review.opendev.org/openstack/cinder refs/changes/65/672265/2 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/qnap.py', 'cinder/tests/unit/volume/drivers/test_qnap.py']",2,6eb882f9e330076bc7c01c73d1069bbb17ee7f41,bug/1837538,"FAKE_USER = 'admin'FAKE_PASSWORD_ENCODED = 'cW5hcGFkbWlu' # Base64 encoding of FAKE_PASSWORD FAKE_PARMS = OrderedDict(pwd=FAKE_PASSWORD_ENCODED, serviceKey='1', user=FAKE_USER) global_sanitized_params = urllib.parse.urlencode(FAKE_PARMS) FAKE_USER, FAKE_PASSWORD, mng_url, self.assertEqual(FAKE_USER, self.driver.api_executor.username) self.assertEqual(FAKE_PASSWORD, self.driver.api_executor.password) FAKE_USER, FAKE_PASSWORD, mng_url, FAKE_USER, FAKE_PASSWORD, mng_url, FAKE_USER, FAKE_PASSWORD, mng_url, self.assertEqual(FAKE_USER, self.driver.api_executor.username) self.assertEqual(FAKE_PASSWORD, self.driver.api_executor.password) FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD, FAKE_USER, FAKE_PASSWORD,","import base64FAKE_PARMS = OrderedDict() FAKE_PARMS['pwd'] = base64.b64encode(FAKE_PASSWORD.encode(""utf-8"")) FAKE_PARMS['serviceKey'] = 1 FAKE_PARMS['user'] = 'admin' sanitized_params = OrderedDict() for key in FAKE_PARMS: value = FAKE_PARMS[key] if value is not None: sanitized_params[key] = six.text_type(value) sanitized_params = utils.create_ordereddict(sanitized_params) global_sanitized_params = urllib.parse.urlencode(sanitized_params) 'admin', 'qnapadmin', mng_url, self.assertEqual('admin', self.driver.api_executor.username) self.assertEqual('qnapadmin', self.driver.api_executor.password) 'admin', 'qnapadmin', mng_url, 'admin', 'qnapadmin', mng_url, 'admin', 'qnapadmin', mng_url, self.assertEqual('admin', self.driver.api_executor.username) self.assertEqual('qnapadmin', self.driver.api_executor.password) 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin', 'admin', 'qnapadmin',",279,293
openstack%2Ftripleo-heat-templates~stable%2Frocky~Id369154d147cd5cf0a6f997bf806084fc7580e01,openstack/tripleo-heat-templates,stable/rocky,Id369154d147cd5cf0a6f997bf806084fc7580e01,HA: minor update of arbitrary container image name,MERGED,2020-03-17 11:36:47.000000000,2020-03-21 06:12:28.000000000,2020-03-21 06:08:41.000000000,"[{'_account_id': 3153}, {'_account_id': 8297}, {'_account_id': 14985}, {'_account_id': 20778}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-17 11:36:47.000000000', 'files': ['docker/services/pacemaker/rabbitmq.yaml', 'docker/services/pacemaker/cinder-volume.yaml', 'docker/services/pacemaker/haproxy.yaml', 'docker/services/pacemaker/database/mysql.yaml', 'docker/services/pacemaker/rpc-rabbitmq.yaml', 'docker/services/pacemaker/database/redis.yaml', 'docker/services/pacemaker/notify-rabbitmq.yaml', 'docker/services/pacemaker/cinder-backup.yaml', 'docker/services/pacemaker/manila-share.yaml', 'releasenotes/notes/pacemaker-cluster-common-tag-45c4e8a6e7b08735.yaml', 'docker/services/pacemaker/ovn-dbs.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/2487becc6a6d263e55c9a642d51994a72c1cb9aa', 'message': 'HA: minor update of arbitrary container image name\n\nHA services get their container image name from a pacemaker\nresource configuration. This image name is shared between\nall cluster nodes.\n\nTo achieve image update without service disruption, a pacemaker\nresource is configured to use an intermediate image name\n""<registry>/<namespace>/<servicename>:pcmklatest"" pointing to\nthe real image name configured in Heat. This tag can then be\nupdated independently on every node during the minor update.\n\nIn order to support the same rolling update when the <namespace>\nchanges in the container image, we need a similar floating\napproach for the prefix part of the container image.\n\nIntroduce a new Heat parameter ClusterCommonTag that, when enabled,\nsets the intermediate image name to\n""cluster-common-tag/<servicename>:pcmklatest"". By default, this\nparameter is disabled and the original naming scheme is conserved.\n\nNote: by introducing this new naming scheme, we stop seeing a\nmeaningful image name prefix when doing a ""pcs status"", but since\nwe already can\'t tell what image ID the :pcmklatest tag points to,\nwe don\'t lose much information really.\n\nRelated-Bug: #1854730\n\nChange-Id: Id369154d147cd5cf0a6f997bf806084fc7580e01\n(cherry picked from commit a166ec6bcaae078a4f7ed91feb8e431fe031e0cb)\n(cherry picked from commit 44b6e6b8520bc9660dd1c6ac44d91523c7c7c84a)\n(cherry picked from commit f366adbd151193ba524d70c4f904be35f1d048a0)\n'}]",0,713411,2487becc6a6d263e55c9a642d51994a72c1cb9aa,11,6,1,8297,,,0,"HA: minor update of arbitrary container image name

HA services get their container image name from a pacemaker
resource configuration. This image name is shared between
all cluster nodes.

To achieve image update without service disruption, a pacemaker
resource is configured to use an intermediate image name
""<registry>/<namespace>/<servicename>:pcmklatest"" pointing to
the real image name configured in Heat. This tag can then be
updated independently on every node during the minor update.

In order to support the same rolling update when the <namespace>
changes in the container image, we need a similar floating
approach for the prefix part of the container image.

Introduce a new Heat parameter ClusterCommonTag that, when enabled,
sets the intermediate image name to
""cluster-common-tag/<servicename>:pcmklatest"". By default, this
parameter is disabled and the original naming scheme is conserved.

Note: by introducing this new naming scheme, we stop seeing a
meaningful image name prefix when doing a ""pcs status"", but since
we already can't tell what image ID the :pcmklatest tag points to,
we don't lose much information really.

Related-Bug: #1854730

Change-Id: Id369154d147cd5cf0a6f997bf806084fc7580e01
(cherry picked from commit a166ec6bcaae078a4f7ed91feb8e431fe031e0cb)
(cherry picked from commit 44b6e6b8520bc9660dd1c6ac44d91523c7c7c84a)
(cherry picked from commit f366adbd151193ba524d70c4f904be35f1d048a0)
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/11/713411/1 && git format-patch -1 --stdout FETCH_HEAD,"['docker/services/pacemaker/rabbitmq.yaml', 'docker/services/pacemaker/cinder-volume.yaml', 'docker/services/pacemaker/database/mysql.yaml', 'docker/services/pacemaker/haproxy.yaml', 'docker/services/pacemaker/rpc-rabbitmq.yaml', 'docker/services/pacemaker/database/redis.yaml', 'docker/services/pacemaker/notify-rabbitmq.yaml', 'docker/services/pacemaker/cinder-backup.yaml', 'docker/services/pacemaker/manila-share.yaml', 'releasenotes/notes/pacemaker-cluster-common-tag-45c4e8a6e7b08735.yaml', 'docker/services/pacemaker/ovn-dbs.yaml']",11,2487becc6a6d263e55c9a642d51994a72c1cb9aa,," ClusterCommonTag: default: false description: When set to false, a pacemaker service is configured to use a floating tag for its container image name, e.g. 'REGISTRY/NAMESPACE/IMAGENAME:pcmklatest'. When set to true, the service uses a floating prefix as well, e.g. 'cluster-common-tag/IMAGENAME:pcmklatest'. type: boolean common_tag_enabled: {equals: [{get_param: ClusterCommonTag}, true]} yaql: data: if: - common_tag_enabled - yaql: data: {get_param: DockerOvnDbsImage} expression: concat(""cluster-common-tag/"", $.data.rightSplit(separator => ""/"", maxSplits => 1)[1]) - {get_param: DockerOvnDbsImage} expression: concat($.data.rightSplit(separator => "":"", maxSplits => 1)[0], "":pcmklatest"")"," list_join: - ':' - - yaql: data: {get_param: DockerOvnDbsImage} expression: $.data.rightSplit(separator => "":"", maxSplits => 1)[0] - 'pcmklatest'",192,60
openstack%2Ftripleo-common~stable%2Ftrain~I2fc2072f0ab8b761a2b8079f746c3b91cacc8733,openstack/tripleo-common,stable/train,I2fc2072f0ab8b761a2b8079f746c3b91cacc8733,Refactor registry request actions,MERGED,2020-03-20 01:23:10.000000000,2020-03-21 06:12:02.000000000,2020-03-21 06:08:46.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-20 01:23:10.000000000', 'files': ['tripleo_common/tests/image/test_image_uploader.py', 'tripleo_common/image/image_uploader.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/4ee02060ee9e742abc912c6508d0d9714aa13a72', 'message': 'Refactor registry request actions\n\nIn order to handle re-authentication at request time rather than\nletting tenancity trigger it, this change creates a session helper class\nthat wraps the various get/patch/push/put actions that we use in the\nimage uploader to perform a single re-autentication action when we get a\n401 from a registry.  Previously we would let the reauthentication occur\nwhen tenancity would retry various functions.  There are several\nfunctions which perform muiltple requests that could exceed the TTL for\nauthentication tokens. This leads to failures that could have been\nprevented if we re-auth at the time of the request rather than retrying\nthe entire action.\n\nChange-Id: I2fc2072f0ab8b761a2b8079f746c3b91cacc8733\nRelated-Bug: #1867981\n(cherry picked from commit 19dba3f385139ff1e37a9561d62b5f4d9eccedac)\n'}]",0,714012,4ee02060ee9e742abc912c6508d0d9714aa13a72,16,4,1,3153,,,0,"Refactor registry request actions

In order to handle re-authentication at request time rather than
letting tenancity trigger it, this change creates a session helper class
that wraps the various get/patch/push/put actions that we use in the
image uploader to perform a single re-autentication action when we get a
401 from a registry.  Previously we would let the reauthentication occur
when tenancity would retry various functions.  There are several
functions which perform muiltple requests that could exceed the TTL for
authentication tokens. This leads to failures that could have been
prevented if we re-auth at the time of the request rather than retrying
the entire action.

Change-Id: I2fc2072f0ab8b761a2b8079f746c3b91cacc8733
Related-Bug: #1867981
(cherry picked from commit 19dba3f385139ff1e37a9561d62b5f4d9eccedac)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/12/714012/1 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_common/tests/image/test_image_uploader.py', 'tripleo_common/image/image_uploader.py']",2,4ee02060ee9e742abc912c6508d0d9714aa13a72,bug/1867981,"class RegistrySessionHelper(object): """""" Class with various registry session helpers This class contains a bunch of static methods to be used when making session requests against a container registry. The methods are primarily used to handle authentication/reauthentication for the requests against registries that require auth. """""" @staticmethod def check_status(session, request, allow_reauth=True): """""" Check request status and trigger reauth This function can be used to check if we need to perform authentication for a container registry request because we've gotten a 401. """""" hash_request_id = hashlib.sha1(str(request.url).encode()) request_id = hash_request_id.hexdigest() text = getattr(request, 'text', 'unknown') reason = getattr(request, 'reason', 'unknown') status_code = getattr(request, 'status_code', None) headers = getattr(request, 'headers', {}) session_headers = getattr(session, 'headers', {}) if status_code >= 300: LOG.info( 'Non-2xx: id {}, status {}, reason {}, text {}'.format( request_id, status_code, reason, text ) ) if status_code == 401: LOG.warning( 'Failure: id {}, status {}, reason {} text {}'.format( request_id, status_code, reason, text ) ) LOG.debug( 'Request headers after 401: id {}, headers {}'.format( request_id, headers ) ) LOG.debug( 'Session headers after 401: id {}, headers {}'.format( request_id, session_headers ) ) www_auth = headers.get( 'www-authenticate', headers.get( 'Www-Authenticate' ) ) if www_auth: error = None # Handle docker.io shenanigans. docker.io will return 401 # for 403 and 404 but provide an error string. Other registries # like registry.redhat.io and quay.io do not do this. So if # we find an error string, check to see if we should reauth. do_reauth = allow_reauth if 'error=' in www_auth: error = re.search('error=""(.*?)""', www_auth).group(1) LOG.warning( 'Error detected in auth headers: error {}'.format( error ) ) do_reauth = (error == 'invalid_token' and allow_reauth) if do_reauth: if hasattr(session, 'reauthenticate'): reauth = int(session.headers.get('_TripleOReAuth', 0)) reauth += 1 session.headers['_TripleOReAuth'] = str(reauth) LOG.warning( 'Re-authenticating: id {}, count {}'.format( request_id, reauth ) ) session.reauthenticate(**session.auth_args) request.raise_for_status() @staticmethod def _action(action, request_session, *args, **kwargs): """""" Perform a session action and retry if auth fails This function dynamically performs a specific type of call using the provided session (get, patch, post, etc). It will attempt a single re-authentication if the initial request fails with a 401. """""" _action = getattr(request_session, action) try: req = _action(*args, **kwargs) RegistrySessionHelper.check_status(session=request_session, request=req) except requests.exceptions.HTTPError as e: if e.response.status_code == 401: req = _action(*args, **kwargs) RegistrySessionHelper.check_status(session=request_session, request=req) else: raise return req @staticmethod def get(request_session, *args, **kwargs): """""" Perform a get and retry if auth fails This function is designed to be used when we perform a get to an authenticated source. This function will attempt a single re-authentication request if the first one fails. """""" return RegistrySessionHelper._action('get', request_session, *args, **kwargs) @staticmethod def patch(request_session, *args, **kwargs): """""" Perform a patch and retry if auth fails This function is designed to be used when we perform a path to an authenticated source. This function will attempt a single re-authentication request if the first one fails. """""" return RegistrySessionHelper._action('patch', request_session, *args, **kwargs) @staticmethod def post(request_session, *args, **kwargs): """""" Perform a post and retry if auth fails This function is designed to be used when we perform a post to an authenticated source. This function will attempt a single re-authentication request if the first one fails. """""" return RegistrySessionHelper._action('post', request_session, *args, **kwargs) @staticmethod def put(request_session, *args, **kwargs): """""" Perform a put and retry if auth fails This function is designed to be used when we perform a put to an authenticated source. This function will attempt a single re-authentication request if the first one fails. """""" return RegistrySessionHelper._action('put', request_session, *args, **kwargs) try: manifest_r = RegistrySessionHelper.get( session, manifest_url, headers=manifest_headers, timeout=30 ) except requests.exceptions.HTTPError as e: if e.response.status_code in (403, 404): raise ImageNotFoundException('Not found image: %s' % image_url.geturl()) else: raise tags_r = RegistrySessionHelper.get(session, tags_url, timeout=30) config_r = RegistrySessionHelper.get( session, config_url, headers=config_headers, timeout=30 ) r = RegistrySessionHelper.post(session, url, data=data, timeout=30) try: RegistrySessionHelper.post( session, upload_req_url, timeout=30 ) except requests.exceptions.HTTPError as e: if e.response.status_code in (501, 403, 404, 405): cls.export_registries.add(image_url.netloc) return True else: raise try: r = RegistrySessionHelper.get( session, url, headers=manifest_headers, timeout=30 ) except requests.exceptions.HTTPError as e: if e.response.status_code in (403, 404): raise ImageNotFoundException('Not found image: %s' % url) else: raise r = RegistrySessionHelper.post( session, upload_req_url, timeout=30 ) RegistrySessionHelper.check_status(session=session, request=blob_req) r = RegistrySessionHelper.get( source_session, source_config_url, timeout=30 ) r = RegistrySessionHelper.put( target_session, try: r = RegistrySessionHelper.put( target_session, manifest_url, timeout=30, data=manifest_str.encode('utf-8'), headers={ 'Content-Type': manifest_type } ) except requests.exceptions.HTTPError as e: if e.response.status_code == 400: LOG.error(cls._get_response_text(r)) raise ImageUploaderException('Pushing manifest failed') else: raise upload_resp = RegistrySessionHelper.patch( session, upload_resp = RegistrySessionHelper.put( session,"," @staticmethod def check_status(session, request, allow_reauth=True): hash_request_id = hashlib.sha1(str(request.url).encode()) request_id = hash_request_id.hexdigest() text = getattr(request, 'text', 'unknown') reason = getattr(request, 'reason', 'unknown') status_code = getattr(request, 'status_code', None) headers = getattr(request, 'headers', {}) session_headers = getattr(session, 'headers', {}) if status_code >= 300: LOG.info( 'Non-2xx: id {}, status {}, reason {}, text {}'.format( request_id, status_code, reason, text ) ) if status_code == 401: LOG.warning( 'Failure: id {}, status {}, reason {} text {}'.format( request_id, status_code, reason, text ) ) LOG.debug( 'Request headers after 401: id {}, headers {}'.format( request_id, headers ) ) LOG.debug( 'Session headers after 401: id {}, headers {}'.format( request_id, session_headers ) ) www_auth = headers.get( 'www-authenticate', headers.get( 'Www-Authenticate' ) ) if www_auth: error = None # Handle docker.io shenanigans. docker.io will return 401 # for 403 and 404 but provide an error string. Other registries # like registry.redhat.io and quay.io do not do this. So if # we find an error string, check to see if we should reauth. do_reauth = allow_reauth if 'error=' in www_auth: error = re.search('error=""(.*?)""', www_auth).group(1) LOG.warning( 'Error detected in auth headers: error {}'.format( error ) ) do_reauth = (error == 'invalid_token' and allow_reauth) if do_reauth: if hasattr(session, 'reauthenticate'): reauth = int(session.headers.get('_TripleOReAuth', 0)) reauth += 1 session.headers['_TripleOReAuth'] = str(reauth) LOG.warning( 'Re-authenticating: id {}, count {}'.format( request_id, reauth ) ) session.reauthenticate(**session.auth_args) request.raise_for_status() manifest_r = session.get(manifest_url, headers=manifest_headers, timeout=30) if manifest_r.status_code in (403, 404): raise ImageNotFoundException('Not found image: %s' % image_url.geturl()) cls.check_status(session=session, request=manifest_r) tags_r = session.get(tags_url, timeout=30) cls.check_status(session=session, request=tags_r) config_r = session.get(config_url, headers=config_headers, timeout=30) cls.check_status(session=session, request=config_r) r = session.post(url, data=data, timeout=30) cls.check_status(session=session, request=r) r = session.post(upload_req_url, timeout=30) if r.status_code in (501, 403, 404, 405): cls.export_registries.add(image_url.netloc) return True cls.check_status(session=session, request=r) r = session.get(url, headers=manifest_headers, timeout=30) if r.status_code in (403, 404): raise ImageNotFoundException('Not found image: %s' % url) cls.check_status(session=session, request=r) r = session.post(upload_req_url, timeout=30) cls.check_status(session=session, request=r) cls.check_status(session=session, request=blob_req) # Because the image layer fetching can exceed the auth # token lifetime, we may have a bad token here and don't want # to retry all of the layer fetching to just fetch the config # data. Let's try a single retry here (as check_status with # reauth by default). try: r = source_session.get(source_config_url, timeout=30) cls.check_status( session=source_session, request=r ) except requests.exceptions.HTTPError as e: LOG.debug('[%s] Config fetch failed, retrying: %s' % (image, source_config_url)) if e.response.status_code == 401: # check_status should have reauthed so try on more # time and raise again if we still have problems. r = source_session.get(source_config_url, timeout=30) cls.check_status( session=source_session, request=r ) else: raise r = target_session.put( cls.check_status(session=target_session, request=r) r = target_session.put( manifest_url, timeout=30, data=manifest_str.encode('utf-8'), headers={ 'Content-Type': manifest_type } ) if r.status_code == 400: LOG.error(cls._get_response_text(r)) raise ImageUploaderException('Pushing manifest failed') cls.check_status(session=target_session, request=r) upload_resp = session.patch( cls.check_status(session=session, request=upload_resp) upload_resp = session.put( cls.check_status(session=session, request=upload_resp)",400,189
openstack%2Frequirements~stable%2Fstein~Iab8237390093cc51714c8b99d573719acf4a2905,openstack/requirements,stable/stein,Iab8237390093cc51714c8b99d573719acf4a2905,update constraint for python-keystoneclient to new release 3.19.1,MERGED,2020-03-20 11:00:56.000000000,2020-03-21 06:11:15.000000000,2020-03-21 06:11:14.000000000,"[{'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 11:00:56.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/cebe7cb0f5c37bb8fb36600f098c5859670e931a', 'message': 'update constraint for python-keystoneclient to new release 3.19.1\n\nChange-Id: Iab8237390093cc51714c8b99d573719acf4a2905\nmeta:version: 3.19.1\nmeta:diff-start: -\nmeta:series: stein\nmeta:release-type: release\nmeta:pypi: yes\nmeta:first: no\nmeta:release:Author: Javier Pena <jpena@redhat.com>\nmeta:release:Commit: Javier Pena <jpena@redhat.com>\nmeta:release:Change-Id: I30eff07200e674f664b2139e0065c3aba494e527\nmeta:release:Code-Review+1: Colleen Murphy <colleen@gazlene.net>\nmeta:release:Code-Review+2: Sean McGinnis <sean.mcginnis@gmail.com>\nmeta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nmeta:release:Workflow+1: Thierry Carrez <thierry@openstack.org>\n'}]",0,714080,cebe7cb0f5c37bb8fb36600f098c5859670e931a,7,3,1,11131,,,0,"update constraint for python-keystoneclient to new release 3.19.1

Change-Id: Iab8237390093cc51714c8b99d573719acf4a2905
meta:version: 3.19.1
meta:diff-start: -
meta:series: stein
meta:release-type: release
meta:pypi: yes
meta:first: no
meta:release:Author: Javier Pena <jpena@redhat.com>
meta:release:Commit: Javier Pena <jpena@redhat.com>
meta:release:Change-Id: I30eff07200e674f664b2139e0065c3aba494e527
meta:release:Code-Review+1: Colleen Murphy <colleen@gazlene.net>
meta:release:Code-Review+2: Sean McGinnis <sean.mcginnis@gmail.com>
meta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
meta:release:Workflow+1: Thierry Carrez <thierry@openstack.org>
",git fetch https://review.opendev.org/openstack/requirements refs/changes/80/714080/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,cebe7cb0f5c37bb8fb36600f098c5859670e931a,new-release,python-keystoneclient===3.19.1,python-keystoneclient===3.19.0,1,1
openstack%2Ftripleo-heat-templates~master~If3702efc940cb2082fd998f770a66ae6d6151d88,openstack/tripleo-heat-templates,master,If3702efc940cb2082fd998f770a66ae6d6151d88,Enable port_forwarding by default in ML2/OVS+DVR environment,MERGED,2020-03-16 08:20:37.000000000,2020-03-21 06:08:43.000000000,2020-03-21 06:08:43.000000000,"[{'_account_id': 3153}, {'_account_id': 11090}, {'_account_id': 11975}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-16 08:20:37.000000000', 'files': ['environments/services/neutron-ovs-dvr.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/d3b7a3f0d31f3a0f3a6a74b8ebc6b8fda89dea37', 'message': 'Enable port_forwarding by default in ML2/OVS+DVR environment\n\nPort forwarding is already enabled by default in the neutron-ovs.yaml,\nso lets do the same in neutron-ovs-dvr.yaml too.\n\nChange-Id: If3702efc940cb2082fd998f770a66ae6d6151d88\n'}]",0,713170,d3b7a3f0d31f3a0f3a6a74b8ebc6b8fda89dea37,12,6,1,11975,,,0,"Enable port_forwarding by default in ML2/OVS+DVR environment

Port forwarding is already enabled by default in the neutron-ovs.yaml,
so lets do the same in neutron-ovs-dvr.yaml too.

Change-Id: If3702efc940cb2082fd998f770a66ae6d6151d88
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/70/713170/1 && git format-patch -1 --stdout FETCH_HEAD,['environments/services/neutron-ovs-dvr.yaml'],1,d3b7a3f0d31f3a0f3a6a74b8ebc6b8fda89dea37,enable-port-forwarding," NeutronServicePlugins: 'router,qos,segments,trunk,port_forwarding' NeutronL3AgentExtensions: ""port_forwarding"" "," NeutronServicePlugins: 'router,qos,segments,trunk'",3,1
openstack%2Ftripleo-heat-templates~master~Ie03450aa0796614a686f6c390c9b0088fcf591f0,openstack/tripleo-heat-templates,master,Ie03450aa0796614a686f6c390c9b0088fcf591f0,Remove neutron wrappers usage,MERGED,2020-03-20 06:17:24.000000000,2020-03-21 06:08:32.000000000,2020-03-21 06:08:31.000000000,"[{'_account_id': 3153}, {'_account_id': 6926}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25877}]","[{'number': 1, 'created': '2020-03-20 06:17:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/ab45d4b9c43f5ecaf530fa4cfc0143d7cc82e99a', 'message': ""Remove neutron wrappers usage\n\nWith I2feb9e81bc40e44cb2c7a2972366fa4b16590227, we don't need to\nset wrapper parameters as everything is deployed by Ansible.\n\nChange-Id: Ie03450aa0796614a686f6c390c9b0088fcf591f0\nBlueprint: safe-side-containers\n""}, {'number': 2, 'created': '2020-03-20 09:16:04.000000000', 'files': ['deployment/neutron/neutron-dhcp-container-puppet.yaml', 'deployment/neutron/neutron-l3-container-puppet.yaml', 'deployment/ovn/ovn-metadata-container-puppet.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/f4f3045c413e7da083dbd8495ef758c2ac86870d', 'message': ""Remove neutron wrappers usage\n\nWith I2feb9e81bc40e44cb2c7a2972366fa4b16590227, we don't need to\nset wrapper parameters as everything is deployed by Ansible.\n\nChange-Id: Ie03450aa0796614a686f6c390c9b0088fcf591f0\nBlueprint: safe-side-containers\n""}]",2,714033,f4f3045c413e7da083dbd8495ef758c2ac86870d,16,6,2,20733,,,0,"Remove neutron wrappers usage

With I2feb9e81bc40e44cb2c7a2972366fa4b16590227, we don't need to
set wrapper parameters as everything is deployed by Ansible.

Change-Id: Ie03450aa0796614a686f6c390c9b0088fcf591f0
Blueprint: safe-side-containers
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/33/714033/1 && git format-patch -1 --stdout FETCH_HEAD,"['deployment/neutron/neutron-dhcp-container-puppet.yaml', 'deployment/neutron/neutron-l3-container-puppet.yaml', 'deployment/ovn/ovn-metadata-container-puppet.yaml']",3,ab45d4b9c43f5ecaf530fa4cfc0143d7cc82e99a,bp/safe-side-containers,, - tripleo::profile::base::neutron::ovn_metadata_agent_wrappers::enable_haproxy_wrapper: {get_param: OVNEnableHaproxyDockerWrapper} tripleo::profile::base::neutron::ovn_metadata_agent_wrappers::haproxy_process_wrapper: '/var/lib/neutron/ovn_metadata_haproxy_wrapper' tripleo::profile::base::neutron::ovn_metadata_agent_wrappers::haproxy_image: {get_param: ContainerOvnMetadataImage} tripleo::profile::base::neutron::ovn_metadata_agent_wrappers::debug: if: - service_debug_unset - {get_param: Debug } - {get_param: OVNWrapperDebug} tripleo::profile::base::neutron::container_cli: {get_param: ContainerCli},0,40
openstack%2Fneutron~master~I6c8ff0056f7f0792e41fba0b93d6af3defb7d453,openstack/neutron,master,I6c8ff0056f7f0792e41fba0b93d6af3defb7d453,[ovn] Documentation: Fix broken links in the OVN Doc,MERGED,2020-03-20 14:54:50.000000000,2020-03-21 05:02:12.000000000,2020-03-21 04:58:33.000000000,"[{'_account_id': 5756}, {'_account_id': 6773}, {'_account_id': 8655}, {'_account_id': 22348}, {'_account_id': 26622}]","[{'number': 1, 'created': '2020-03-20 14:54:50.000000000', 'files': ['doc/source/admin/ovn/ovn.rst'], 'web_link': 'https://opendev.org/openstack/neutron/commit/e92e31123297a9cbdbbc45ed71e430f3df9cc3ae', 'message': '[ovn] Documentation: Fix broken links in the OVN Doc\n\nThis patch is fixing the links that got broken after the split\nfrom OVS.\n\nAlso adding: http://dani.foroselectronica.es/category/openstack/ovn/\nto the blog references.\n\nChange-Id: I6c8ff0056f7f0792e41fba0b93d6af3defb7d453\nSigned-off-by: Daniel Alvarez <dalvarez@redhat.com>\n'}]",2,714133,e92e31123297a9cbdbbc45ed71e430f3df9cc3ae,10,5,1,23804,,,0,"[ovn] Documentation: Fix broken links in the OVN Doc

This patch is fixing the links that got broken after the split
from OVS.

Also adding: http://dani.foroselectronica.es/category/openstack/ovn/
to the blog references.

Change-Id: I6c8ff0056f7f0792e41fba0b93d6af3defb7d453
Signed-off-by: Daniel Alvarez <dalvarez@redhat.com>
",git fetch https://review.opendev.org/openstack/neutron refs/changes/33/714133/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/admin/ovn/ovn.rst'],1,e92e31123297a9cbdbbc45ed71e430f3df9cc3ae,,* http://www.ovn.org/support/dist-docs/ovn-architecture.7.html* https://blog.spinhirne.com/posts/an-introduction-to-ovn/a-primer-on-ovn/ * https://docs.ovn.org/en/stable/tutorials/ovn-sandbox.html* https://docs.ovn.org/en/stable/tutorials/ovn-openstack.html* http://www.ovn.org/support/dist-docs/ovn-nb.5.html * http://www.ovn.org/support/dist-docs/ovn-sb.5.html * http://www.ovn.org/support/dist-docs/ovn-nbctl.8.html * http://www.ovn.org/support/dist-docs/ovn-sbctl.8.html * http://www.ovn.org/support/dist-docs/ovn-northd.8.html * http://www.ovn.org/support/dist-docs/ovn-controller.8.html * http://www.ovn.org/support/dist-docs/ovn-controller-vtep.8.html* http://docs.ovn.org/en/latest/ref/* http://dani.foroselectronica.es/category/openstack/ovn/,* http://www.openvswitch.org/support/dist-docs/ovn-architecture.7.html* http://blog.spinhirne.com/p/blog-series.html#introToOVN * http://docs.openvswitch.org/en/stable/tutorials/ovn-sandbox/* http://docs.openvswitch.org/en/stable/tutorials/ovn-openstack/* http://www.openvswitch.org/support/dist-docs/ovn-nb.5.html * http://www.openvswitch.org/support/dist-docs/ovn-sb.5.html * http://www.openvswitch.org/support/dist-docs/ovn-nbctl.8.html * http://www.openvswitch.org/support/dist-docs/ovn-sbctl.8.html * http://www.openvswitch.org/support/dist-docs/ovn-northd.8.html * http://www.openvswitch.org/support/dist-docs/ovn-controller.8.html * http://www.openvswitch.org/support/dist-docs/ovn-controller-vtep.8.html* http://docs.openvswitch.org/en/latest/ref/,13,12
openstack%2Fneutron~master~I5b87726eafa71d717ae22f48d1c9c6343b680c7f,openstack/neutron,master,I5b87726eafa71d717ae22f48d1c9c6343b680c7f,[ovn] Stricter matching on metadata port binding event,MERGED,2020-03-19 18:09:20.000000000,2020-03-21 05:00:40.000000000,2020-03-21 04:58:15.000000000,"[{'_account_id': 5756}, {'_account_id': 6773}, {'_account_id': 8655}, {'_account_id': 11952}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 23804}, {'_account_id': 24791}, {'_account_id': 26622}]","[{'number': 1, 'created': '2020-03-19 18:09:20.000000000', 'files': ['neutron/tests/functional/agent/ovn/metadata/test_metadata_agent.py', 'neutron/agent/ovn/metadata/agent.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/06fde66b0b05dc987a4280275813f0cf4bf54c88', 'message': ""[ovn] Stricter matching on metadata port binding event\n\nPreviously the Port Binding event spawned haproxy process even if the\nchange wasn't done in chassis column, it attempted to spawn haproxy on\nall port bindings changes.\n\nThis patch spawns haproxy only if new chassis is the one managed by the\nagent and old chassis was empty. Similarly it destroys haproxy if new\nchassis is empty and old chassis was the one managed by the agent.\n\nCloses-bug: #1868125\n\nCo-Authored-By: Daniel Alvarez <dalvarez@redhat.com>\nSigned-off-by: Jakub Libosvar <libosvar@redhat.com>\nChange-Id: I5b87726eafa71d717ae22f48d1c9c6343b680c7f\n""}]",10,713956,06fde66b0b05dc987a4280275813f0cf4bf54c88,15,9,1,8655,,,0,"[ovn] Stricter matching on metadata port binding event

Previously the Port Binding event spawned haproxy process even if the
change wasn't done in chassis column, it attempted to spawn haproxy on
all port bindings changes.

This patch spawns haproxy only if new chassis is the one managed by the
agent and old chassis was empty. Similarly it destroys haproxy if new
chassis is empty and old chassis was the one managed by the agent.

Closes-bug: #1868125

Co-Authored-By: Daniel Alvarez <dalvarez@redhat.com>
Signed-off-by: Jakub Libosvar <libosvar@redhat.com>
Change-Id: I5b87726eafa71d717ae22f48d1c9c6343b680c7f
",git fetch https://review.opendev.org/openstack/neutron refs/changes/56/713956/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/functional/agent/ovn/metadata/test_metadata_agent.py', 'neutron/agent/ovn/metadata/agent.py']",2,06fde66b0b05dc987a4280275813f0cf4bf54c88,bug/1868125," LOG.info(self.LOG_MSG, row.logical_port, str(row.datapath.uuid)) self.agent.update_datapath(str(row.datapath.uuid))class PortBindingChassisCreatedEvent(PortBindingChassisEvent): LOG_MSG = ""Port %s in datapath %s bound to our chassis"" def match_fn(self, event, row, old): try: return (row.chassis[0].name == self.agent.chassis and not old.chassis) except (IndexError, AttributeError): return False class PortBindingChassisDeletedEvent(PortBindingChassisEvent): LOG_MSG = ""Port %s in datapath %s unbound from our chassis"" def match_fn(self, event, row, old): try: return (old.chassis[0].name == self.agent.chassis and not row.chassis) except (IndexError, AttributeError): return False events=[PortBindingChassisCreatedEvent(self), PortBindingChassisDeletedEvent(self), ChassisCreateEvent(self),"," new_chassis = getattr(row, 'chassis', []) old_chassis = getattr(old, 'chassis', []) if new_chassis and new_chassis[0].name == self.agent.chassis: LOG.info(""Port %s in datapath %s bound to our chassis"", row.logical_port, str(row.datapath.uuid)) self.agent.update_datapath(str(row.datapath.uuid)) elif old_chassis and old_chassis[0].name == self.agent.chassis: LOG.info(""Port %s in datapath %s unbound from our chassis"", row.logical_port, str(row.datapath.uuid)) self.agent.update_datapath(str(row.datapath.uuid)) events=[PortBindingChassisEvent(self), ChassisCreateEvent(self),",84,20
openstack%2Fneutron~master~I9ffdde96873fa3a557e4a4ddeeb9abdf89d3045a,openstack/neutron,master,I9ffdde96873fa3a557e4a4ddeeb9abdf89d3045a,Add usage note in utils.Timer class,MERGED,2020-03-20 12:22:07.000000000,2020-03-21 04:58:21.000000000,2020-03-21 04:58:21.000000000,"[{'_account_id': 9732}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 13995}, {'_account_id': 22348}, {'_account_id': 26622}]","[{'number': 1, 'created': '2020-03-20 12:22:07.000000000', 'files': ['neutron/common/utils.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/79af9e786d6feb19b8f282f0c69afa1516c4b929', 'message': 'Add usage note in utils.Timer class\n\nThis class, when a timeout is defined, cannot be used in other\nthan the main thread. When a timeout is defined, an alarm signal is\nset. Only the main thread is allowed to set a signal handler and the\nsignal handlers are always executed in this main thread [1].\n\n[1] https://docs.python.org/3/library/signal.html#signals-and-threads\n\nChange-Id: I9ffdde96873fa3a557e4a4ddeeb9abdf89d3045a\nRelated-Bug: #1832925\n'}]",0,714100,79af9e786d6feb19b8f282f0c69afa1516c4b929,10,6,1,16688,,,0,"Add usage note in utils.Timer class

This class, when a timeout is defined, cannot be used in other
than the main thread. When a timeout is defined, an alarm signal is
set. Only the main thread is allowed to set a signal handler and the
signal handlers are always executed in this main thread [1].

[1] https://docs.python.org/3/library/signal.html#signals-and-threads

Change-Id: I9ffdde96873fa3a557e4a4ddeeb9abdf89d3045a
Related-Bug: #1832925
",git fetch https://review.opendev.org/openstack/neutron refs/changes/00/714100/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/common/utils.py'],1,79af9e786d6feb19b8f282f0c69afa1516c4b929,bug/1832925," NOTE(ralonsoh): this class, when a timeout is defined, cannot be used in other than the main thread. When a timeout is defined, an alarm signal is set. Only the main thread is allowed to set a signal handler and the signal handlers are always executed in this main thread [1]. [1] https://docs.python.org/3/library/signal.html#signals-and-threads",,6,0
openstack%2Fcinder~master~I44650912e88808ce56b347633093e05d5741252c,openstack/cinder,master,I44650912e88808ce56b347633093e05d5741252c,[Unity] Fix TypeError for test case test_delete_host_wo_lock,MERGED,2020-03-16 12:40:45.000000000,2020-03-21 04:17:57.000000000,2020-03-19 12:53:49.000000000,"[{'_account_id': 5997}, {'_account_id': 10379}, {'_account_id': 11611}, {'_account_id': 11904}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 14384}, {'_account_id': 15386}, {'_account_id': 15670}, {'_account_id': 15941}, {'_account_id': 18883}, {'_account_id': 20284}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 21976}, {'_account_id': 22126}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23613}, {'_account_id': 24815}, {'_account_id': 24921}, {'_account_id': 25678}, {'_account_id': 26114}, {'_account_id': 26537}, {'_account_id': 29705}, {'_account_id': 29716}, {'_account_id': 30688}]","[{'number': 1, 'created': '2020-03-16 12:40:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/8a132403eca3520b598cc9b25dd706ad8a84e49a', 'message': '[Unity] Fix TypeError for test case test_delete_host_wo_lock\n\nFix TypeError ""\'NoneType\' object is not callable"" in test case\ntest_delete_host_wo_lock\n\nChange-Id: I44650912e88808ce56b347633093e05d5741252c\nCloses-bug: #1867619\n'}, {'number': 2, 'created': '2020-03-16 15:46:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/61aba28c44580cc1fc4698e1958211fba2f1cf97', 'message': '[Unity] Fix TypeError for test case test_delete_host_wo_lock\n\nFix TypeError ""\'NoneType\' object is not callable"" in test case\ntest_delete_host_wo_lock\n\nChange-Id: I44650912e88808ce56b347633093e05d5741252c\nCloses-bug: #1867619\n'}, {'number': 3, 'created': '2020-03-17 03:14:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/f90e2180e9c60fc9a66b528e8c19a6cf6037458b', 'message': '[Unity] Fix TypeError for test case test_delete_host_wo_lock\n\nFix TypeError ""\'NoneType\' object is not callable"" in test case\ntest_delete_host_wo_lock\n\nChange-Id: I44650912e88808ce56b347633093e05d5741252c\nCloses-bug: #1867619\n'}, {'number': 4, 'created': '2020-03-17 06:48:37.000000000', 'files': ['cinder/tests/unit/volume/drivers/dell_emc/unity/test_client.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/f88845d756ebd50713a6533d0e3108c4ff9ce00d', 'message': '[Unity] Fix TypeError for test case test_delete_host_wo_lock\n\nFix TypeError ""\'NoneType\' object is not callable"" in test case\ntest_delete_host_wo_lock\n\nChange-Id: I44650912e88808ce56b347633093e05d5741252c\nCloses-bug: #1867619\n'}]",0,713217,f88845d756ebd50713a6533d0e3108c4ff9ce00d,77,28,4,26114,,,0,"[Unity] Fix TypeError for test case test_delete_host_wo_lock

Fix TypeError ""'NoneType' object is not callable"" in test case
test_delete_host_wo_lock

Change-Id: I44650912e88808ce56b347633093e05d5741252c
Closes-bug: #1867619
",git fetch https://review.opendev.org/openstack/cinder refs/changes/17/713217/4 && git format-patch -1 --stdout FETCH_HEAD,['cinder/tests/unit/volume/drivers/dell_emc/unity/test_client.py'],1,8a132403eca3520b598cc9b25dd706ad8a84e49a,unity_fix_ut_issue," self.client.delete_host_wo_lock, host=host)", self.client.delete_host_wo_lock(host)),2,1
openstack%2Fswift~master~I190ee3ebd42ab63a1a0caef6ccaa6a9a747fc4cb,openstack/swift,master,I190ee3ebd42ab63a1a0caef6ccaa6a9a747fc4cb,cors: Add Content-Length to default-exposed headers,NEW,2020-03-11 06:08:22.000000000,2020-03-21 04:10:20.000000000,,"[{'_account_id': 597}, {'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-11 06:08:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/94f73d32d0c3071b7402af7aa30733acde751ed9', 'message': 'cors: Add Content-Length to default-exposed headers\n\nBrowser implementations are a little spotty; Chrome will expose it\neven without this change, while Firefox does not. Exposing it should\nbe fairly safe, though; the client could always just check the\nreceived message length if it wanted.\n\nChange-Id: I190ee3ebd42ab63a1a0caef6ccaa6a9a747fc4cb\n'}, {'number': 2, 'created': '2020-03-11 17:01:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/41203aaa394abe4a5393f9fc3267c5eade3a77bc', 'message': 'cors: Add Content-Length to default-exposed headers\n\nBrowser implementations are a little spotty; Chrome will expose it\neven without this change, while Firefox does not. Exposing it should\nbe fairly safe, though; the client could always just check the\nreceived message length if it wanted.\n\nChange-Id: I190ee3ebd42ab63a1a0caef6ccaa6a9a747fc4cb\n'}, {'number': 3, 'created': '2020-03-11 22:09:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/fef2b2bda5de9257ed2513423274e667f751d338', 'message': 'cors: Add Content-Length to default-exposed headers\n\nBrowser implementations are a little spotty; Chrome will expose it\neven without this change, while Firefox does not. Exposing it should\nbe fairly safe, though; the client could always just check the\nreceived message length if it wanted.\n\nChange-Id: I190ee3ebd42ab63a1a0caef6ccaa6a9a747fc4cb\n'}, {'number': 4, 'created': '2020-03-19 20:56:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/332ef3884d4403a1f69e36689e7e7f5ab9c2ec1b', 'message': 'cors: Add Content-Length to default-exposed headers\n\nBrowser implementations are a little spotty; Chrome will expose it\neven without this change, while Firefox does not. Exposing it should\nbe fairly safe, though; the client could always just check the\nreceived message length if it wanted.\n\nChange-Id: I190ee3ebd42ab63a1a0caef6ccaa6a9a747fc4cb\n'}, {'number': 5, 'created': '2020-03-19 21:38:40.000000000', 'files': ['swift/proxy/controllers/info.py', 'test/unit/proxy/test_server.py', 'test/cors/harness.js', 'doc/source/cors.rst', 'swift/proxy/controllers/base.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/525d78500810574d263448b410295dd89ca2c685', 'message': 'cors: Add Content-Length to default-exposed headers\n\nBrowser implementations are a little spotty; Chrome will expose it\neven without this change, while Firefox does not. Exposing it should\nbe fairly safe, though; the client could always just check the\nreceived message length if it wanted.\n\nChange-Id: I190ee3ebd42ab63a1a0caef6ccaa6a9a747fc4cb\n'}]",0,712238,525d78500810574d263448b410295dd89ca2c685,12,3,5,15343,,,0,"cors: Add Content-Length to default-exposed headers

Browser implementations are a little spotty; Chrome will expose it
even without this change, while Firefox does not. Exposing it should
be fairly safe, though; the client could always just check the
received message length if it wanted.

Change-Id: I190ee3ebd42ab63a1a0caef6ccaa6a9a747fc4cb
",git fetch https://review.opendev.org/openstack/swift refs/changes/38/712238/1 && git format-patch -1 --stdout FETCH_HEAD,"['swift/proxy/controllers/info.py', 'swift/proxy/controllers/base.py']",2,94f73d32d0c3071b7402af7aa30733acde751ed9,713754," 'x-timestamp', 'x-trans-id', 'x-openstack-request-id', 'content-length'])"," 'x-timestamp', 'x-trans-id', 'x-openstack-request-id'])",3,2
openstack%2Ftripleo-common~stable%2Fqueens~Ifd28fd60e4bc1218acc24381d13534c227f2877e,openstack/tripleo-common,stable/queens,Ifd28fd60e4bc1218acc24381d13534c227f2877e,container_update: Retry removing containers.,MERGED,2020-01-28 18:31:50.000000000,2020-03-21 04:05:51.000000000,2020-03-21 04:05:51.000000000,"[{'_account_id': 3153}, {'_account_id': 7353}, {'_account_id': 11090}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25877}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-01-28 18:31:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/50c6aa7ba7285b37ca8d15ba3fb66799b4911174', 'message': 'container_update: Retry removing containers.\n\nChange-Id: Ifd28fd60e4bc1218acc24381d13534c227f2877e\nRelated-Bug: #1856086\nSigned-off-by: Luke Short <ekultails@gmail.com>\n(cherry picked from commit ef77f081a21e8483b143558dbf5ace5d909a368f)\n(cherry picked from commit 64718a796125dd1197fb4c64ca5cd8db44b750be)\n(cherry picked from commit 179f6953cf4b9623fd7a5f8ada37888b3aad34ca)\n(cherry picked from commit b91249bf7e60b2436f32496dd3df0f1e90c355fb)\n'}, {'number': 2, 'created': '2020-02-14 19:37:03.000000000', 'files': ['scripts/container-update.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/9ab61955a2155b3ad62fa657a31a02f5123dfe5c', 'message': 'container_update: Retry removing containers.\n\nChange-Id: Ifd28fd60e4bc1218acc24381d13534c227f2877e\nRelated-Bug: #1856086\nSigned-off-by: Luke Short <ekultails@gmail.com>\n(cherry picked from commit ef77f081a21e8483b143558dbf5ace5d909a368f)\n(cherry picked from commit 64718a796125dd1197fb4c64ca5cd8db44b750be)\n(cherry picked from commit 179f6953cf4b9623fd7a5f8ada37888b3aad34ca)\n(cherry picked from commit b91249bf7e60b2436f32496dd3df0f1e90c355fb)\n'}]",0,704656,9ab61955a2155b3ad62fa657a31a02f5123dfe5c,17,8,2,25877,,,0,"container_update: Retry removing containers.

Change-Id: Ifd28fd60e4bc1218acc24381d13534c227f2877e
Related-Bug: #1856086
Signed-off-by: Luke Short <ekultails@gmail.com>
(cherry picked from commit ef77f081a21e8483b143558dbf5ace5d909a368f)
(cherry picked from commit 64718a796125dd1197fb4c64ca5cd8db44b750be)
(cherry picked from commit 179f6953cf4b9623fd7a5f8ada37888b3aad34ca)
(cherry picked from commit b91249bf7e60b2436f32496dd3df0f1e90c355fb)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/56/704656/1 && git format-patch -1 --stdout FETCH_HEAD,['scripts/container-update.py'],1,50c6aa7ba7285b37ca8d15ba3fb66799b4911174,,"import tenacity@tenacity.retry( reraise=True, retry=tenacity.retry_if_exception_type(RuntimeError), stop=tenacity.stop_after_attempt(3), wait=tenacity.wait_fixed(1) ) rc = subproc.returncode if rc != 0: if 'No such container' in cmd_stderr: log.warn('Container that does not exist cannot be deleted: ' '%s' % name) else: log.error('Error removing container: %s' % name) log.error(cmd_stderr) raise RuntimeError(cmd_stdout, cmd_stderr, rc)", if cmd_stderr and \ cmd_stderr != 'Error response from daemon: ' \ 'No such container: {}\n'.format(name): log.debug(cmd_stderr),19,4
openstack%2Ftripleo-ci~master~I0a68b7d6181654f108e6095ff84ea83bbbb05e3c,openstack/tripleo-ci,master,I0a68b7d6181654f108e6095ff84ea83bbbb05e3c,Allow standalone_upgrade to be overridable,MERGED,2020-03-19 08:54:20.000000000,2020-03-21 04:01:50.000000000,2020-03-21 04:01:50.000000000,"[{'_account_id': 6469}, {'_account_id': 8449}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 08:54:20.000000000', 'files': ['roles/run-test/tasks/main.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/db1f60f18adccc11260b5c1909d6ac0ce477d95e', 'message': 'Allow standalone_upgrade to be overridable\n\nAllowing standalone_upgrade to be overridable enables jobs not to have\nto create new featuresets where the only delta is the standalone_upgrade\nsetting.\n\nThis was the case for the TripleO Octavia upgrades job and discussed in\nhttps://review.opendev.org/#/c/710556/\n\nChange-Id: I0a68b7d6181654f108e6095ff84ea83bbbb05e3c\n'}]",0,713801,db1f60f18adccc11260b5c1909d6ac0ce477d95e,11,5,1,6469,,,0,"Allow standalone_upgrade to be overridable

Allowing standalone_upgrade to be overridable enables jobs not to have
to create new featuresets where the only delta is the standalone_upgrade
setting.

This was the case for the TripleO Octavia upgrades job and discussed in
https://review.opendev.org/#/c/710556/

Change-Id: I0a68b7d6181654f108e6095ff84ea83bbbb05e3c
",git fetch https://review.opendev.org/openstack/tripleo-ci refs/changes/01/713801/1 && git format-patch -1 --stdout FETCH_HEAD,['roles/run-test/tasks/main.yaml'],1,db1f60f18adccc11260b5c1909d6ac0ce477d95e,, - 'standalone_upgrade',,1,0
openstack%2Ftripleo-common~stable%2Ftrain~I777cd2c33992254e926ba7e937f73aa5c48b786b,openstack/tripleo-common,stable/train,I777cd2c33992254e926ba7e937f73aa5c48b786b,Improve authentication retries for slow transfers,MERGED,2020-03-19 16:23:00.000000000,2020-03-21 03:47:15.000000000,2020-03-21 03:45:29.000000000,"[{'_account_id': 3153}, {'_account_id': 6816}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 16:23:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/21d1f2d83942b1d88531d190c046bc7f270999d0', 'message': 'Improve authentication retries for slow transfers\n\nIf during the image prepare process the layer transfers take longer\nthan 5 minutes (the default TTL for tokens for most authenticated\nregistries), the process may fail to fetch and push the container to the\nundercloud registry.\n\nThis change does two things:\n\n1) Raises the HTTPError exceptions when exporting layers correctly so\n   that it is retried\n2) Improves problemattic re-authentication logic for registries other\n   than docker.io which resulted in the process failing with 401 errors.\n\nChange-Id: I777cd2c33992254e926ba7e937f73aa5c48b786b\nCloses-Bug: #1867981\n(cherry picked from commit e269e34738d43f7f924d08897831095397626848)\n'}, {'number': 2, 'created': '2020-03-20 01:22:20.000000000', 'files': ['tripleo_common/image/image_export.py', 'tripleo_common/image/image_uploader.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/7b51726748c370a27e93a2be2cdadfdd1387fdd8', 'message': 'Improve authentication retries for slow transfers\n\nIf during the image prepare process the layer transfers take longer\nthan 5 minutes (the default TTL for tokens for most authenticated\nregistries), the process may fail to fetch and push the container to the\nundercloud registry.\n\nThis change does two things:\n\n1) Raises the HTTPError exceptions when exporting layers correctly so\n   that it is retried\n2) Improves problemattic re-authentication logic for registries other\n   than docker.io which resulted in the process failing with 401 errors.\n\nChange-Id: I777cd2c33992254e926ba7e937f73aa5c48b786b\nCloses-Bug: #1867981\n(cherry picked from commit e269e34738d43f7f924d08897831095397626848)\n'}]",0,713923,7b51726748c370a27e93a2be2cdadfdd1387fdd8,29,5,2,14985,,,0,"Improve authentication retries for slow transfers

If during the image prepare process the layer transfers take longer
than 5 minutes (the default TTL for tokens for most authenticated
registries), the process may fail to fetch and push the container to the
undercloud registry.

This change does two things:

1) Raises the HTTPError exceptions when exporting layers correctly so
   that it is retried
2) Improves problemattic re-authentication logic for registries other
   than docker.io which resulted in the process failing with 401 errors.

Change-Id: I777cd2c33992254e926ba7e937f73aa5c48b786b
Closes-Bug: #1867981
(cherry picked from commit e269e34738d43f7f924d08897831095397626848)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/23/713923/1 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_common/image/image_export.py', 'tripleo_common/image/image_uploader.py']",2,21d1f2d83942b1d88531d190c046bc7f270999d0,bug/1867981," # Handle docker.io shenanigans. docker.io will return 401 # for 403 and 404 but provide an error string. Other registries # like registry.redhat.io and quay.io do not do this. So if # we find an error string, check to see if we should reauth. do_reauth = allow_reauth do_reauth = (error == 'invalid_token' and allow_reauth) if do_reauth: # Because the image layer fetching can exceed the auth # token lifetime, we may have a bad token here and don't want # to retry all of the layer fetching to just fetch the config # data. Let's try a single retry here (as check_status with # reauth by default). try: r = source_session.get(source_config_url, timeout=30) cls.check_status( session=source_session, request=r ) except requests.exceptions.HTTPError as e: LOG.debug('[%s] Config fetch failed, retrying: %s' % (image, source_config_url)) if e.response.status_code == 401: # check_status should have reauthed so try on more # time and raise again if we still have problems. r = source_session.get(source_config_url, timeout=30) cls.check_status( session=source_session, request=r ) else: raise "," if error == 'invalid_token' and allow_reauth: r = source_session.get(source_config_url, timeout=30) cls.check_status( session=source_session, request=r )",39,6
openstack%2Fos-refresh-config~master~I7a6bea2e6e6fffcc262dd4241a7d6b75913d87aa,openstack/os-refresh-config,master,I7a6bea2e6e6fffcc262dd4241a7d6b75913d87aa,Use unittest.mock instead of third party mock,MERGED,2020-03-13 16:50:19.000000000,2020-03-21 03:45:36.000000000,2020-03-21 03:45:36.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-13 16:50:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-refresh-config/commit/8986e04325e8f8274cb5166c141f54a4ff01e078', 'message': 'Use unittest.mock instead of third party mock\n\nNow that we no longer support py27, we can use the standard library\nunittest.mock module instead of the third party mock lib.\n\nChange-Id: I7a6bea2e6e6fffcc262dd4241a7d6b75913d87aa\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}, {'number': 2, 'created': '2020-03-13 19:48:55.000000000', 'files': ['test-requirements.txt', 'os_refresh_config/tests/test_os_refresh_config.py'], 'web_link': 'https://opendev.org/openstack/os-refresh-config/commit/b324bcece152f590ee1b605b37d169b9964939ca', 'message': 'Use unittest.mock instead of third party mock\n\nNow that we no longer support py27, we can use the standard library\nunittest.mock module instead of the third party mock lib.\n\nChange-Id: I7a6bea2e6e6fffcc262dd4241a7d6b75913d87aa\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}]",0,713027,b324bcece152f590ee1b605b37d169b9964939ca,11,4,2,11904,,,0,"Use unittest.mock instead of third party mock

Now that we no longer support py27, we can use the standard library
unittest.mock module instead of the third party mock lib.

Change-Id: I7a6bea2e6e6fffcc262dd4241a7d6b75913d87aa
Signed-off-by: Sean McGinnis <sean.mcginnis@gmail.com>
",git fetch https://review.opendev.org/openstack/os-refresh-config refs/changes/27/713027/1 && git format-patch -1 --stdout FETCH_HEAD,"['test-requirements.txt', 'os_refresh_config/tests/test_os_refresh_config.py']",2,8986e04325e8f8274cb5166c141f54a4ff01e078,unittest.mock,from unittest import mock ,import mock,2,2
openstack%2Fos-refresh-config~master~I842cf731fbad0dae72cf902366655eb7b4f8ca1b,openstack/os-refresh-config,master,I842cf731fbad0dae72cf902366655eb7b4f8ca1b,Stop testing python2.7,MERGED,2019-10-28 09:10:27.000000000,2020-03-21 03:45:35.000000000,2020-03-21 03:45:35.000000000,"[{'_account_id': 3153}, {'_account_id': 8556}, {'_account_id': 11904}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2019-10-28 09:10:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-refresh-config/commit/bdb6e3d22d96e7376104bf0f26560a259623dc95', 'message': ""Stop testing python2.7\n\nTrain was the last release supporting python2. Let's rip the bandaid\noff.\n\nChange-Id: I842cf731fbad0dae72cf902366655eb7b4f8ca1b\n""}, {'number': 2, 'created': '2020-03-13 19:48:04.000000000', 'files': ['releasenotes/notes/drop-py2-76af844b0b952a91.yaml', 'zuul.d/layout.yaml', 'setup.cfg', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/os-refresh-config/commit/cc543ed5afe0db8a8ce1d8b80d0297a32533f169', 'message': ""Stop testing python2.7\n\nTrain was the last release supporting python2. Let's rip the bandaid\noff.\n\nChange-Id: I842cf731fbad0dae72cf902366655eb7b4f8ca1b\n""}]",0,691636,cc543ed5afe0db8a8ce1d8b80d0297a32533f169,15,6,2,30092,,,0,"Stop testing python2.7

Train was the last release supporting python2. Let's rip the bandaid
off.

Change-Id: I842cf731fbad0dae72cf902366655eb7b4f8ca1b
",git fetch https://review.opendev.org/openstack/os-refresh-config refs/changes/36/691636/2 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/layout.yaml', 'setup.cfg', 'tox.ini']",3,bdb6e3d22d96e7376104bf0f26560a259623dc95,,"envlist = py37,pep8basepython = python3","envlist = py27,py37,pep8basepython = python3basepython = python3basepython = python3",6,11
openstack%2Ftripleo-heat-templates~master~I3b25bfdef91b0bfc8d624d71a884d57508eaf004,openstack/tripleo-heat-templates,master,I3b25bfdef91b0bfc8d624d71a884d57508eaf004,Rename roles that we have missed,MERGED,2020-03-19 07:21:26.000000000,2020-03-21 03:45:34.000000000,2020-03-21 03:45:34.000000000,"[{'_account_id': 3153}, {'_account_id': 6926}, {'_account_id': 7353}, {'_account_id': 14985}, {'_account_id': 20733}, {'_account_id': 21129}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 07:21:26.000000000', 'files': ['deployment/cinder/cinder-backup-pacemaker-puppet.yaml', 'deployment/manila/manila-share-pacemaker-puppet.yaml', 'deployment/cinder/cinder-volume-pacemaker-puppet.yaml', 'deployment/clients/openstack-clients-baremetal-ansible.yaml', 'deployment/barbican/barbican-api-container-puppet.yaml', 'deployment/rabbitmq/rabbitmq-messaging-notify-pacemaker-puppet.yaml', 'deployment/rabbitmq/rabbitmq-messaging-pacemaker-puppet.yaml', 'deployment/podman/podman-baremetal-ansible.yaml', 'deployment/haproxy/haproxy-pacemaker-puppet.yaml', 'deployment/database/redis-pacemaker-puppet.yaml', 'deployment/ovn/ovn-dbs-pacemaker-puppet.yaml', 'deployment/database/mysql-pacemaker-puppet.yaml', 'deployment/ipsec/ipsec-baremetal-ansible.yaml', 'deployment/rabbitmq/rabbitmq-messaging-rpc-pacemaker-puppet.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/62fbe15d0b05f14bdb6e76e3b405573efa7fe62d', 'message': 'Rename roles that we have missed\n\nThese roles were not renamed when we removed all of the hyphens.\nThis change removes the remaining hyphenated roles.\n\nChange-Id: I3b25bfdef91b0bfc8d624d71a884d57508eaf004\n'}]",0,713792,62fbe15d0b05f14bdb6e76e3b405573efa7fe62d,13,8,1,20733,,,0,"Rename roles that we have missed

These roles were not renamed when we removed all of the hyphens.
This change removes the remaining hyphenated roles.

Change-Id: I3b25bfdef91b0bfc8d624d71a884d57508eaf004
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/92/713792/1 && git format-patch -1 --stdout FETCH_HEAD,"['deployment/cinder/cinder-backup-pacemaker-puppet.yaml', 'deployment/manila/manila-share-pacemaker-puppet.yaml', 'deployment/cinder/cinder-volume-pacemaker-puppet.yaml', 'deployment/clients/openstack-clients-baremetal-ansible.yaml', 'deployment/barbican/barbican-api-container-puppet.yaml', 'deployment/rabbitmq/rabbitmq-messaging-notify-pacemaker-puppet.yaml', 'deployment/rabbitmq/rabbitmq-messaging-pacemaker-puppet.yaml', 'deployment/podman/podman-baremetal-ansible.yaml', 'deployment/haproxy/haproxy-pacemaker-puppet.yaml', 'deployment/database/redis-pacemaker-puppet.yaml', 'deployment/ovn/ovn-dbs-pacemaker-puppet.yaml', 'deployment/database/mysql-pacemaker-puppet.yaml', 'deployment/ipsec/ipsec-baremetal-ansible.yaml', 'deployment/rabbitmq/rabbitmq-messaging-rpc-pacemaker-puppet.yaml']",14,62fbe15d0b05f14bdb6e76e3b405573efa7fe62d,renaming-all-the-things, name: tripleo_container_rm, name: tripleo-container-rm,25,25
openstack%2Ftripleo-heat-templates~stable%2Fqueens~Ic12c3c287a921d696de1395bc887691c48146359,openstack/tripleo-heat-templates,stable/queens,Ic12c3c287a921d696de1395bc887691c48146359,Check Ceph*Key value format and halt on error,MERGED,2020-03-12 15:56:39.000000000,2020-03-21 03:45:32.000000000,2020-03-21 03:45:32.000000000,"[{'_account_id': 3153}, {'_account_id': 6796}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-12 15:56:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/1c847df9065d632cd95c84d6ff238e2bc7234f2d', 'message': 'Check Ceph*Key value format and halt on error\n\nThe CephX keys secret is expected to have a specific format, this\nadds a constraint in the templates to ensure it has the correct\nformat.\n\nChange-Id: Ic12c3c287a921d696de1395bc887691c48146359\nCloses-Bug: 1864185\n(cherry picked from commit 0940dfd95ef64cb4d1d87edfe74374b08db9c9f2)\n(cherry picked from commit 33ce60d6acafc8e06b42366a7ef873871597a736)\n(cherry picked from commit 98652da2ab272c8919a31348a4e7f6f33b8321c9)\n(cherry picked from commit 909d57ccc140964ce3347d9353e2efc8c785a882)\n'}, {'number': 2, 'created': '2020-03-16 15:53:29.000000000', 'files': ['puppet/services/nova-libvirt.yaml', 'puppet/services/ceph-base.yaml', 'puppet/services/ceph-mon.yaml', 'puppet/services/nova-compute.yaml', 'puppet/services/ceph-external.yaml', 'docker/services/ceph-ansible/ceph-base.yaml', 'docker/services/ceph-ansible/ceph-mon.yaml', 'puppet/services/manila-backend-cephfs.yaml', 'docker/services/nova-libvirt.yaml', 'puppet/services/ceph-rgw.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/3d8fa5a841ccfeffb7746ef405e7dc64b041ce3f', 'message': 'Check Ceph*Key value format and halt on error\n\nThe CephX keys secret is expected to have a specific format, this\nadds a constraint in the templates to ensure it has the correct\nformat.\n\nChange-Id: Ic12c3c287a921d696de1395bc887691c48146359\nCloses-Bug: 1864185\n(cherry picked from commit 0940dfd95ef64cb4d1d87edfe74374b08db9c9f2)\n(cherry picked from commit 33ce60d6acafc8e06b42366a7ef873871597a736)\n(cherry picked from commit 98652da2ab272c8919a31348a4e7f6f33b8321c9)\n(cherry picked from commit 909d57ccc140964ce3347d9353e2efc8c785a882)\n'}]",0,712721,3d8fa5a841ccfeffb7746ef405e7dc64b041ce3f,18,4,2,6796,,,0,"Check Ceph*Key value format and halt on error

The CephX keys secret is expected to have a specific format, this
adds a constraint in the templates to ensure it has the correct
format.

Change-Id: Ic12c3c287a921d696de1395bc887691c48146359
Closes-Bug: 1864185
(cherry picked from commit 0940dfd95ef64cb4d1d87edfe74374b08db9c9f2)
(cherry picked from commit 33ce60d6acafc8e06b42366a7ef873871597a736)
(cherry picked from commit 98652da2ab272c8919a31348a4e7f6f33b8321c9)
(cherry picked from commit 909d57ccc140964ce3347d9353e2efc8c785a882)
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/21/712721/2 && git format-patch -1 --stdout FETCH_HEAD,"['puppet/services/nova-libvirt.yaml', 'puppet/services/nova-compute.yaml', 'docker/services/ceph-ansible/ceph-base.yaml', 'docker/services/ceph-ansible/ceph-mon.yaml', 'puppet/services/manila-backend-cephfs.yaml', 'docker/services/nova-libvirt.yaml']",6,1c847df9065d632cd95c84d6ff238e2bc7234f2d,," constraints: - allowed_pattern: ""^[a-zA-Z0-9+/]{38}==$""",,18,0
openstack%2Fnova~master~Ie9685e6a4798357d4979aa6b4ff8a03663a9c71c,openstack/nova,master,Ie9685e6a4798357d4979aa6b4ff8a03663a9c71c,doc: Improve PDF document structure,MERGED,2019-09-17 21:02:11.000000000,2020-03-21 03:13:14.000000000,2019-10-08 16:06:38.000000000,"[{'_account_id': 841}, {'_account_id': 8556}, {'_account_id': 14070}, {'_account_id': 14384}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 16128}, {'_account_id': 16376}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26458}, {'_account_id': 26515}]","[{'number': 1, 'created': '2019-09-17 21:02:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/389fb105b09f7eb265ff86f36d4911ed307aaa71', 'message': 'doc: Improve PDF document structure\n\nThis is a follow-up patch for https://review.opendev.org/676730.\n\nIn the TOC of the current PDF file [1], most contents related to\nuser and admin guides are located under ""For Contributors"" section.\nThis is weird. It happens because the latex builder constructs\nthe document tree based on ""toctree"" directives even though they\nare marked as ""hidden"".\n\nThis commit reorganizes ""toctree"" per section.\nThe ""toctree"" directives must be placed at the end of\nindividual sections. Otherwise, content of a last section and\ncontent just after ""toctree"" directive are concatenated\ninto a same section in the rendered LaTeX document.\n\nThis commit also improves the following as well:\n\n* Specify ""openany"" as ""extraclassoptions"" to skip blank pages\n  along with ""oneside"" to use same page style for odd and even pages.\n* Set ""tocdepth"" and ""secnumdepth"" to 3 respectively.\n  ""tocdepth"" controls the depth of TOC and ""secnumdepth"" controls\n  the level of numbered sections in TOC.\n\nNote that this commit does not reorganize file structure under doc/source.\nI believe this should be done separately.\n\n[1] https://docs.openstack.org/nova/latest/doc-nova.pdf\n\nChange-Id: Ie9685e6a4798357d4979aa6b4ff8a03663a9c71c\nStory: 2006100\nTask: 35140\n'}, {'number': 2, 'created': '2019-09-24 19:41:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/47c1be5ec088e6f757553dc892aaebe932c9f4ce', 'message': 'doc: Improve PDF document structure\n\nThis is a follow-up patch for https://review.opendev.org/676730.\n\nIn the TOC of the current PDF file [1], most contents related to\nuser and admin guides are located under ""For Contributors"" section.\nThis is weird. It happens because the latex builder constructs\nthe document tree based on ""toctree"" directives even though they\nare marked as ""hidden"".\n\nThis commit reorganizes ""toctree"" per section.\nThe ""toctree"" directives must be placed at the end of\nindividual sections. Otherwise, content of a last section and\ncontent just after ""toctree"" directive are concatenated\ninto a same section in the rendered LaTeX document.\n\nThis commit also improves the following as well:\n\n* Specify ""openany"" as ""extraclassoptions"" to skip blank pages\n  along with ""oneside"" to use same page style for odd and even pages.\n* Set ""tocdepth"" and ""secnumdepth"" to 3 respectively.\n  ""tocdepth"" controls the depth of TOC and ""secnumdepth"" controls\n  the level of numbered sections in TOC.\n\nNote that this commit does not reorganize file structure under doc/source.\nI believe this should be done separately.\n\n[1] https://docs.openstack.org/nova/latest/doc-nova.pdf\n\nChange-Id: Ie9685e6a4798357d4979aa6b4ff8a03663a9c71c\nStory: 2006100\nTask: 35140\n'}, {'number': 3, 'created': '2019-10-08 07:39:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/039775d17e6b395f0047c012d53afc592fe260a8', 'message': 'doc: Improve PDF document structure\n\nThis is a follow-up patch for https://review.opendev.org/676730.\n\nIn the TOC of the current PDF file [1], most contents related to\nuser and admin guides are located under ""For Contributors"" section.\nThis is weird. It happens because the latex builder constructs\nthe document tree based on ""toctree"" directives even though they\nare marked as ""hidden"".\n\nThis commit reorganizes ""toctree"" per section.\nThe ""toctree"" directives must be placed at the end of\nindividual sections. Otherwise, content of a last section and\ncontent just after ""toctree"" directive are concatenated\ninto a same section in the rendered LaTeX document.\n\nThis commit also improves the following as well:\n\n* Specify ""openany"" as ""extraclassoptions"" to skip blank pages\n  along with ""oneside"" to use same page style for odd and even pages.\n* Set ""tocdepth"" and ""secnumdepth"" to 3 respectively.\n  ""tocdepth"" controls the depth of TOC and ""secnumdepth"" controls\n  the level of numbered sections in TOC.\n\nNote that this commit does not reorganize file structure under doc/source.\nI believe this should be done separately.\n\n[1] https://docs.openstack.org/nova/latest/doc-nova.pdf\n\nChange-Id: Ie9685e6a4798357d4979aa6b4ff8a03663a9c71c\nStory: 2006100\nTask: 35140\n'}, {'number': 4, 'created': '2019-10-08 10:08:22.000000000', 'files': ['doc/source/configuration/index.rst', 'doc/source/index.rst', 'doc/source/reference/index.rst', 'doc/source/user/index.rst', 'doc/source/contributor/index.rst', 'doc/source/conf.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/152d5c359c4615efed046a9f310019c18880455e', 'message': 'doc: Improve PDF document structure\n\nThis is a follow-up patch for https://review.opendev.org/676730.\n\nIn the TOC of the current PDF file [1], most contents related to\nuser and admin guides are located under ""For Contributors"" section.\nThis is weird. It happens because the latex builder constructs\nthe document tree based on ""toctree"" directives even though they\nare marked as ""hidden"".\n\nThis commit reorganizes ""toctree"" per section.\nThe ""toctree"" directives must be placed at the end of\nindividual sections. Otherwise, content of a last section and\ncontent just after ""toctree"" directive are concatenated\ninto a same section in the rendered LaTeX document.\n\nThis commit also improves the following as well:\n\n* Specify ""openany"" as ""extraclassoptions"" to skip blank pages\n  along with ""oneside"" to use same page style for odd and even pages.\n* Set ""tocdepth"" and ""secnumdepth"" to 3 respectively.\n  ""tocdepth"" controls the depth of TOC and ""secnumdepth"" controls\n  the level of numbered sections in TOC.\n\nNote that this commit does not reorganize file structure under doc/source.\nI believe this should be done separately.\n\n[1] https://docs.openstack.org/nova/latest/doc-nova.pdf\n\nChange-Id: Ie9685e6a4798357d4979aa6b4ff8a03663a9c71c\nStory: 2006100\nTask: 35140\n'}]",1,682746,152d5c359c4615efed046a9f310019c18880455e,34,12,4,841,,,0,"doc: Improve PDF document structure

This is a follow-up patch for https://review.opendev.org/676730.

In the TOC of the current PDF file [1], most contents related to
user and admin guides are located under ""For Contributors"" section.
This is weird. It happens because the latex builder constructs
the document tree based on ""toctree"" directives even though they
are marked as ""hidden"".

This commit reorganizes ""toctree"" per section.
The ""toctree"" directives must be placed at the end of
individual sections. Otherwise, content of a last section and
content just after ""toctree"" directive are concatenated
into a same section in the rendered LaTeX document.

This commit also improves the following as well:

* Specify ""openany"" as ""extraclassoptions"" to skip blank pages
  along with ""oneside"" to use same page style for odd and even pages.
* Set ""tocdepth"" and ""secnumdepth"" to 3 respectively.
  ""tocdepth"" controls the depth of TOC and ""secnumdepth"" controls
  the level of numbered sections in TOC.

Note that this commit does not reorganize file structure under doc/source.
I believe this should be done separately.

[1] https://docs.openstack.org/nova/latest/doc-nova.pdf

Change-Id: Ie9685e6a4798357d4979aa6b4ff8a03663a9c71c
Story: 2006100
Task: 35140
",git fetch https://review.opendev.org/openstack/nova refs/changes/46/682746/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/configuration/index.rst', 'doc/source/index.rst', 'doc/source/reference/index.rst', 'doc/source/user/index.rst', 'doc/source/contributor/index.rst', 'doc/source/conf.py']",6,389fb105b09f7eb265ff86f36d4911ed307aaa71,build-pdf-docs,"latex_elements = { 'maxlistdepth': 10, 'extraclassoptions': 'openany,oneside', 'preamble': r''' \setcounter{tocdepth}{3} \setcounter{secnumdepth}{3} ''', }",latex_elements = {'maxlistdepth': 10},182,87
openstack%2Fcyborg~master~I393ccbf020cc4f80ce78b1ba4564a833abec02bf,openstack/cyborg,master,I393ccbf020cc4f80ce78b1ba4564a833abec02bf,Fix setting cyborg agent hostname,MERGED,2020-02-25 14:59:47.000000000,2020-03-21 01:27:24.000000000,2020-03-21 01:24:45.000000000,"[{'_account_id': 11604}, {'_account_id': 22348}, {'_account_id': 25738}, {'_account_id': 28748}]","[{'number': 1, 'created': '2020-02-25 14:59:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/eea3a8401bfa72feefc0fd61b35eedec7ae97b1c', 'message': 'Fix setting cyborg agent hostname\n\nThis removes the devstack plugin line which overrides the CONF.host in\nthe cyborg agent with the $LOCAL_HOSTNAME. The latter may be different\nthan what the nova-compute service generates (as the nova devstack\nmodule does not override the host). Since these need to match, just\nremove the override here so that both services use the hostname from\nthe service framework.\n\nCloses Task: #38811\n\nChange-Id: I393ccbf020cc4f80ce78b1ba4564a833abec02bf\n'}, {'number': 2, 'created': '2020-03-16 13:50:23.000000000', 'files': ['devstack/lib/cyborg'], 'web_link': 'https://opendev.org/openstack/cyborg/commit/f5a47abe5e5ab371be368ab72a34a637884c4bd5', 'message': 'Fix setting cyborg agent hostname\n\nThis removes the devstack plugin line which overrides the CONF.host in\nthe cyborg agent with the $LOCAL_HOSTNAME. The latter may be different\nthan what the nova-compute service generates (as the nova devstack\nmodule does not override the host). Since these need to match, just\nremove the override here so that both services use the hostname from\nthe service framework.\n\nCloses Task: #38811\n\nChange-Id: I393ccbf020cc4f80ce78b1ba4564a833abec02bf\n'}]",0,709749,f5a47abe5e5ab371be368ab72a34a637884c4bd5,11,4,2,4393,,,0,"Fix setting cyborg agent hostname

This removes the devstack plugin line which overrides the CONF.host in
the cyborg agent with the $LOCAL_HOSTNAME. The latter may be different
than what the nova-compute service generates (as the nova devstack
module does not override the host). Since these need to match, just
remove the override here so that both services use the hostname from
the service framework.

Closes Task: #38811

Change-Id: I393ccbf020cc4f80ce78b1ba4564a833abec02bf
",git fetch https://review.opendev.org/openstack/cyborg refs/changes/49/709749/1 && git format-patch -1 --stdout FETCH_HEAD,['devstack/lib/cyborg'],1,eea3a8401bfa72feefc0fd61b35eedec7ae97b1c,multinode,, iniset $CYBORG_CONF_FILE DEFAULT host $LOCAL_HOSTNAME,0,1
openstack%2Fcyborg~master~I1dd616fe1e0c5b03118a23fc5afe2a4d3bb3686d,openstack/cyborg,master,I1dd616fe1e0c5b03118a23fc5afe2a4d3bb3686d,add support for installing cyborg client,MERGED,2020-02-24 22:59:46.000000000,2020-03-21 01:26:21.000000000,2020-03-21 01:24:44.000000000,"[{'_account_id': 22348}, {'_account_id': 26458}, {'_account_id': 28748}]","[{'number': 1, 'created': '2020-02-24 22:59:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/8734c28fef33b41ca1fe6f3d2c3e45ce3b16c790', 'message': 'add support for installing cyborg client\n\nThis change adds support for installing the\npython-cyborgclient from git. By default it is installed\nbut it can be disabled by setting CYBORG_CLIENT_INSTALL=False.\nThe version and repo can be set using CYBORG_CLIENT_REPO and\nCYBORG_CLIENT_BRANCH.\n\nChange-Id: I1dd616fe1e0c5b03118a23fc5afe2a4d3bb3686d\n'}, {'number': 2, 'created': '2020-02-24 23:19:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/e30868dab75ed32605032b464b0da7efff454ca6', 'message': 'add support for installing cyborg client\n\nThis change adds support for installing the\npython-cyborgclient from git. By default it is installed\nbut it can be disabled by setting CYBORG_CLIENT_INSTALL=False.\nThe version and repo can be set using CYBORG_CLIENT_REPO and\nCYBORG_CLIENT_BRANCH.\n\nChange-Id: I1dd616fe1e0c5b03118a23fc5afe2a4d3bb3686d\n'}, {'number': 3, 'created': '2020-02-25 00:45:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/20ccc6d7e50b985e5eba096b8e13799e50c1cc2f', 'message': 'add support for installing cyborg client\n\nThis change adds support for installing the\npython-cyborgclient from git. By default it is installed\nbut it can be disabled by setting CYBORG_CLIENT_INSTALL=False.\nThe version and repo can be set using CYBORG_CLIENT_REPO and\nCYBORG_CLIENT_BRANCH.\n\nChange-Id: I1dd616fe1e0c5b03118a23fc5afe2a4d3bb3686d\n'}, {'number': 4, 'created': '2020-03-16 13:50:23.000000000', 'files': ['devstack/lib/cyborg', 'devstack/plugin.sh', 'devstack/settings'], 'web_link': 'https://opendev.org/openstack/cyborg/commit/76bdd3f8a3dbe30b0173a7b8128a63d2561cb7df', 'message': 'add support for installing cyborg client\n\nThis change adds support for installing the\npython-cyborgclient from git. By default it is installed\nbut it can be disabled by setting CYBORG_CLIENT_INSTALL=False.\nThe version and repo can be set using CYBORG_CLIENT_REPO and\nCYBORG_CLIENT_BRANCH.\n\nChange-Id: I1dd616fe1e0c5b03118a23fc5afe2a4d3bb3686d\n'}]",1,709629,76bdd3f8a3dbe30b0173a7b8128a63d2561cb7df,14,3,4,11604,,,0,"add support for installing cyborg client

This change adds support for installing the
python-cyborgclient from git. By default it is installed
but it can be disabled by setting CYBORG_CLIENT_INSTALL=False.
The version and repo can be set using CYBORG_CLIENT_REPO and
CYBORG_CLIENT_BRANCH.

Change-Id: I1dd616fe1e0c5b03118a23fc5afe2a4d3bb3686d
",git fetch https://review.opendev.org/openstack/cyborg refs/changes/29/709629/1 && git format-patch -1 --stdout FETCH_HEAD,"['devstack/lib/cyborg', 'devstack/plugin.sh', 'devstack/settings']",3,8734c28fef33b41ca1fe6f3d2c3e45ce3b16c790,multinode," # client settings CYBORG_CLIENT_INSTALL=$(trueorfalse True CYBORG_CLIENT_INSTALL) CYBORG_CLIENT_REPO=${CYBORG_CLIENT_REPO:-""$GIT_BASE/openstack/python-cyborgclient""} CYBORG_CLIENT_BRANCH=${CYBORG_CLIENT_BRANCH:-master} CYBORG_CLIENT_DIR=""${DEST}/python-cyborgclient""",,21,0
openstack%2Fcyborg~master~Ia5ad1235ae4a5a76bcd498a743ca6be6b3634c5c,openstack/cyborg,master,Ia5ad1235ae4a5a76bcd498a743ca6be6b3634c5c,simplify devstack plugin,MERGED,2020-02-20 00:31:19.000000000,2020-03-21 01:13:42.000000000,2020-03-21 01:10:21.000000000,"[{'_account_id': 14131}, {'_account_id': 21672}, {'_account_id': 22348}, {'_account_id': 26458}, {'_account_id': 28748}]","[{'number': 1, 'created': '2020-02-20 00:31:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/6904019724fb9ac0f59de8c155a9969c4e78791d', 'message': 'simplify devstack plugin\n\nThis change replaces the nested if used\nin plugin.sh with nested case statements for\ngreater readablity.\n\nThis change removes an unnessasary check to determin\nif services are enabled. by default all cyborg\nservice are enabled by the plugin. Since the plugin is\nnot loaded unless the user specifies the enable_plugin\nline in there local.conf we can assume they want teh plugin\nto run and there for can remove the service check.\n\nThis change removes the install_agent function and renames\ninstall_cyborg_in_controller function to\ncheck_cyborg_service_deps to better reflect what the function\ndoes.\n\nChange-Id: Ia5ad1235ae4a5a76bcd498a743ca6be6b3634c5c\n'}, {'number': 2, 'created': '2020-02-24 22:13:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/5d439bf86d02868be5a2a1aef11216c1928de862', 'message': 'simplify devstack plugin\n\nThis change replaces the nested if used\nin plugin.sh with nested case statements for\ngreater readablity.\n\nThis change removes an unnecessary check to determine\nif services are enabled. By default all cyborg\nservice are enabled by the plugin. Since the plugin is\nnot loaded unless the user specifies the enable_plugin\nline in there local.conf we can assume they want the plugin\nto run and there for can remove the service check.\n\nThis change removes the install_agent function and renames\ninstall_cyborg_in_controller function to\ncheck_cyborg_service_deps to better reflect what the function\ndoes.\n\nChange-Id: Ia5ad1235ae4a5a76bcd498a743ca6be6b3634c5c\n'}, {'number': 3, 'created': '2020-02-24 23:19:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/d7a96775f6b8dec2f477c14db2418cf9bb94ee88', 'message': 'simplify devstack plugin\n\nThis change replaces the nested if used\nin plugin.sh with nested case statements for\ngreater readablity.\n\nThis change removes an unnecessary check to determine\nif services are enabled. By default all cyborg\nservice are enabled by the plugin. Since the plugin is\nnot loaded unless the user specifies the enable_plugin\nline in there local.conf we can assume they want the plugin\nto run and there for can remove the service check.\n\nThis change removes the install_agent function and renames\ninstall_cyborg_in_controller function to\ncheck_cyborg_service_deps to better reflect what the function\ndoes.\n\nChange-Id: Ia5ad1235ae4a5a76bcd498a743ca6be6b3634c5c\n'}, {'number': 4, 'created': '2020-02-25 00:45:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/c9fc2e1b3a8253e8a90b84a6c86aff4f649be67c', 'message': 'simplify devstack plugin\n\nThis change replaces the nested if used\nin plugin.sh with nested case statements for\ngreater readablity.\n\nThis change removes an unnecessary check to determine\nif services are enabled. By default all cyborg\nservice are enabled by the plugin. Since the plugin is\nnot loaded unless the user specifies the enable_plugin\nline in there local.conf we can assume they want the plugin\nto run and there for can remove the service check.\n\nThis change removes the install_agent function and renames\ninstall_cyborg_in_controller function to\ncheck_cyborg_service_deps to better reflect what the function\ndoes.\n\nChange-Id: Ia5ad1235ae4a5a76bcd498a743ca6be6b3634c5c\n'}, {'number': 5, 'created': '2020-03-16 13:50:23.000000000', 'files': ['devstack/lib/cyborg', 'devstack/plugin.sh'], 'web_link': 'https://opendev.org/openstack/cyborg/commit/dd2024ee95f4e6b81b48466ab10d5b69ff14e719', 'message': 'simplify devstack plugin\n\nThis change replaces the nested if used\nin plugin.sh with nested case statements for\ngreater readablity.\n\nThis change removes an unnecessary check to determine\nif services are enabled. By default all cyborg\nservice are enabled by the plugin. Since the plugin is\nnot loaded unless the user specifies the enable_plugin\nline in there local.conf we can assume they want the plugin\nto run and there for can remove the service check.\n\nThis change removes the install_agent function and renames\ninstall_cyborg_in_controller function to\ncheck_cyborg_service_deps to better reflect what the function\ndoes.\n\nChange-Id: Ia5ad1235ae4a5a76bcd498a743ca6be6b3634c5c\n'}]",4,708773,dd2024ee95f4e6b81b48466ab10d5b69ff14e719,23,5,5,11604,,,0,"simplify devstack plugin

This change replaces the nested if used
in plugin.sh with nested case statements for
greater readablity.

This change removes an unnecessary check to determine
if services are enabled. By default all cyborg
service are enabled by the plugin. Since the plugin is
not loaded unless the user specifies the enable_plugin
line in there local.conf we can assume they want the plugin
to run and there for can remove the service check.

This change removes the install_agent function and renames
install_cyborg_in_controller function to
check_cyborg_service_deps to better reflect what the function
does.

Change-Id: Ia5ad1235ae4a5a76bcd498a743ca6be6b3634c5c
",git fetch https://review.opendev.org/openstack/cyborg refs/changes/73/708773/5 && git format-patch -1 --stdout FETCH_HEAD,"['devstack/lib/cyborg', 'devstack/plugin.sh']",2,6904019724fb9ac0f59de8c155a9969c4e78791d,multinode,"case $1 in ""stack"") case $2 in ""pre-install"") pre_install_agent ;; ""install"") echo_summary ""Installing Cyborg"" install_cyborg ;; ""post-config"") # stack/post-config - Called after the layer 0 and 2 services # have been configured. All configuration files for enabled # services should exist at this point. echo_summary ""Configuring Cyborg"" configure_cyborg create_cyborg_accounts ;; ""extra"") # stack/extra - Called near the end after layer 1 and 2 # services have been started. # Initialize cyborg init_cyborg # Start the cyborg API and cyborg taskmgr components echo_summary ""Starting Cyborg"" start_cyborg ;; esac ;; ""unstack"") # unstack - Called by unstack.sh before other services are shut down. ;; ""clean"") # clean - Called by clean.sh before other services are cleaned, but after # unstack.sh has been called. ;; esac"," if is_service_enabled cyborg-api cyborg-cond || is_service_enabled cyborg-agent; then if [[ ""$1"" == ""stack"" ]]; then if [[ ""$2"" == ""pre-install"" ]]; then pre_install_agent elif [[ ""$2"" == ""install"" ]]; then echo_summary ""Installing Cyborg"" install_agent install_cyborg elif [[ ""$2"" == ""post-config"" ]]; then # stack/post-config - Called after the layer 0 and 2 services have been # configured. All configuration files for enabled services should exist # at this point. echo_summary ""Configuring Cyborg"" configure_cyborg create_cyborg_accounts elif [[ ""$2"" == ""extra"" ]]; then # stack/extra - Called near the end after layer 1 and 2 services have # been started. # Initialize cyborg init_cyborg # Start the cyborg API and cyborg taskmgr components echo_summary ""Starting Cyborg"" start_cyborg fi fi if [[ ""$1"" == ""unstack"" ]]; then # unstack - Called by unstack.sh before other services are shut down. fi if [[ ""$1"" == ""clean"" ]]; then # clean - Called by clean.sh before other services are cleaned, but after # unstack.sh has been called. fi fi",39,55
openstack%2Fcyborg~master~I61e169d5723720236bcf5ce3883fae6164ef6352,openstack/cyborg,master,I61e169d5723720236bcf5ce3883fae6164ef6352,move functions from devstack/plugin.sh to devstack/lib/cyborg,MERGED,2020-02-24 22:13:54.000000000,2020-03-21 01:12:10.000000000,2020-03-21 01:10:20.000000000,"[{'_account_id': 4393}, {'_account_id': 11604}, {'_account_id': 22348}, {'_account_id': 24872}, {'_account_id': 26458}, {'_account_id': 28748}]","[{'number': 1, 'created': '2020-02-24 22:13:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/3283f7f7d532eda44f9c79c1e3bcf78d1374a011', 'message': 'move functions from devstack/plugin.sh to devstack/lib/cyborg\n\nThis change refactors the devstack plugin to call functions\nprovided by the devstack/lib/* files\n\nChange-Id: I61e169d5723720236bcf5ce3883fae6164ef6352\n'}, {'number': 2, 'created': '2020-02-24 23:19:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/faf085e5d5fc4c6153bb5a4263e4feb2cfac26f7', 'message': 'move functions from devstack/plugin.sh to devstack/lib/cyborg\n\nThis change refactors the devstack plugin to call functions\nprovided by the devstack/lib/* files\n\nChange-Id: I61e169d5723720236bcf5ce3883fae6164ef6352\n'}, {'number': 3, 'created': '2020-02-25 00:45:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/58c1decf1ba501a435e9614b2352adacd0fec44f', 'message': 'move functions from devstack/plugin.sh to devstack/lib/cyborg\n\nThis change refactors the devstack plugin to call functions\nprovided by the devstack/lib/* files\n\nChange-Id: I61e169d5723720236bcf5ce3883fae6164ef6352\n'}, {'number': 4, 'created': '2020-03-16 13:50:23.000000000', 'files': ['devstack/lib/cyborg', 'devstack/plugin.sh'], 'web_link': 'https://opendev.org/openstack/cyborg/commit/b3aa875c3cf356cd60fcf7d549a436b5abe11ca8', 'message': 'move functions from devstack/plugin.sh to devstack/lib/cyborg\n\nThis change refactors the devstack plugin to call functions\nprovided by the devstack/lib/* files\n\nChange-Id: I61e169d5723720236bcf5ce3883fae6164ef6352\n'}]",7,709620,b3aa875c3cf356cd60fcf7d549a436b5abe11ca8,24,6,4,11604,,,0,"move functions from devstack/plugin.sh to devstack/lib/cyborg

This change refactors the devstack plugin to call functions
provided by the devstack/lib/* files

Change-Id: I61e169d5723720236bcf5ce3883fae6164ef6352
",git fetch https://review.opendev.org/openstack/cyborg refs/changes/20/709620/2 && git format-patch -1 --stdout FETCH_HEAD,"['devstack/lib/cyborg', 'devstack/plugin.sh']",2,3283f7f7d532eda44f9c79c1e3bcf78d1374a011,multinode," echo_summary ""Installing Cyborg""","function pre_install_agent { if is_service_enabled cyborg-agent; then # stack/pre-install - Called after (OS) setup is complete and before # project source is installed echo_summary ""Installing additional Cyborg packages"" if [[ ""$OPAE_INSTALL_ENABLE"" == ""True"" ]] && install_opae_packages; then echo_summary ""INFO: Additional Cyborg packages installed"" elif [[ ""$OPAE_INSTALL_ENABLE"" == ""True"" ]]; then echo ""WARNING: Failed to install additional Cyborg packages"" fi fi } function install_agent { if is_service_enabled cyborg-agent; then # stack/install - Called after the layer 1 and 2 projects source and # their dependencies have been installed echo_summary ""Installing Cyborg"" if ! is_service_enabled n-cpu; then source $RC_DIR/lib/nova_plugins/functions-libvirt install_libvirt fi fi } function cleanup_agent { if is_service_enabled cyborg-agent; then if [[ ""$OPAE_INSTALL_ENABLE"" == ""True"" ]]; then uninstall_opae_packages fi fi } ",67,57
openstack%2Fpuppet-nova~master~I1789bd6afbdfb279f674b82cfb946698fa07248f,openstack/puppet-nova,master,I1789bd6afbdfb279f674b82cfb946698fa07248f,Add `pmem_namespaces` parameter,MERGED,2020-03-17 07:22:43.000000000,2020-03-21 01:10:05.000000000,2020-03-21 01:08:32.000000000,"[{'_account_id': 3153}, {'_account_id': 9414}, {'_account_id': 9816}, {'_account_id': 20733}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 07:22:43.000000000', 'files': ['spec/classes/nova_compute_libvirt_spec.rb', 'manifests/compute/libvirt.pp', 'releasenotes/notes/add_pmem_namespaces-7c425a8c65ecd119.yaml'], 'web_link': 'https://opendev.org/openstack/puppet-nova/commit/4139563f1c06a8a55da415978dcf60c1684a999d', 'message': 'Add `pmem_namespaces` parameter\n\nThis change adds parameter `libvirt/pmem_namespaces` that\nconfigure persistent memory(pmem) namespaces for Nova.\n\nChange-Id: I1789bd6afbdfb279f674b82cfb946698fa07248f\n'}]",1,713377,4139563f1c06a8a55da415978dcf60c1684a999d,10,5,1,30133,,,0,"Add `pmem_namespaces` parameter

This change adds parameter `libvirt/pmem_namespaces` that
configure persistent memory(pmem) namespaces for Nova.

Change-Id: I1789bd6afbdfb279f674b82cfb946698fa07248f
",git fetch https://review.opendev.org/openstack/puppet-nova refs/changes/77/713377/1 && git format-patch -1 --stdout FETCH_HEAD,"['spec/classes/nova_compute_libvirt_spec.rb', 'manifests/compute/libvirt.pp', 'releasenotes/notes/add_pmem_namespaces-7c425a8c65ecd119.yaml']",3,4139563f1c06a8a55da415978dcf60c1684a999d,pmem_namespaces,--- features: - | Add support for managing configuration for persistent memory(pmem) namespaces.,,19,0
openstack%2Fcyborg~master~I473ef276197ad6578f64a708f4b66562a2ff1d49,openstack/cyborg,master,I473ef276197ad6578f64a708f4b66562a2ff1d49,support standard devstack plugin cloning,MERGED,2020-02-20 00:31:19.000000000,2020-03-21 01:05:57.000000000,2020-03-21 01:02:50.000000000,"[{'_account_id': 4393}, {'_account_id': 11604}, {'_account_id': 21672}, {'_account_id': 22348}, {'_account_id': 26458}, {'_account_id': 28748}]","[{'number': 1, 'created': '2020-02-20 00:31:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/89ed92a5f2316602bc2b953216e0e25f0e5bb140', 'message': 'support standard devstack plugin cloning\n\nThis change removes the non standard mechcanium\nfor specifying the cyborg repo and branch\nvia CYBORG_REPO and CYBORG_BRANCH.\n\nin-tree devstack modules use the\n<PROJECT NAME>_REPO and <PROJECT NAME>_BARNCH\nconvention for specifying revisios of proejct to clone\nhowever devstack plugins should only use that convention\nfor cloning other git repos.\n\nA devstack plugin should never clone the repo that contains\nthe plugin itself as doing so will replace the code while it\nis executing. This can lead to the code that is running and the\ncode on disk being different if the plugin line and CYBORG_*\noptions do not match.\n\nThis change updates the devstack_setup docs with an example of\ntesting an unmerged change.\n\nThis change removes some commented out code and uneeded checks in\ndevstack/lib/*\n\nChange-Id: I473ef276197ad6578f64a708f4b66562a2ff1d49\n'}, {'number': 2, 'created': '2020-02-24 22:13:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/3b1b765d8e65524144614acb30656ff13157c73a', 'message': 'support standard devstack plugin cloning\n\nThis change removes the non standard mechanism\nfor specifying the cyborg repo and branch\nvia CYBORG_REPO and CYBORG_BRANCH.\n\nin-tree devstack modules use the\n<PROJECT NAME>_REPO and <PROJECT NAME>_BRANCH\nconvention for specifying revisions of project to clone\nhowever devstack plugins should only use that convention\nfor cloning other git repos.\n\nA devstack plugin should never clone the repo that contains\nthe plugin itself as doing so will replace the code while it\nis executing. This can lead to the code that is running and the\ncode on disk being different if the plugin line and CYBORG_*\noptions do not match.\n\nThis change updates the devstack_setup docs with an example of\ntesting an unmerged change.\n\nThis change removes some commented out code and unneeded checks in\ndevstack/lib/*\n\nChange-Id: I473ef276197ad6578f64a708f4b66562a2ff1d49\n'}, {'number': 3, 'created': '2020-02-24 23:19:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/5a7164601b3a3070d008caaa4375635b62b4580f', 'message': 'support standard devstack plugin cloning\n\nThis change removes the non standard mechanism\nfor specifying the cyborg repo and branch\nvia CYBORG_REPO and CYBORG_BRANCH.\n\nin-tree devstack modules use the\n<PROJECT NAME>_REPO and <PROJECT NAME>_BRANCH\nconvention for specifying revisions of project to clone\nhowever devstack plugins should only use that convention\nfor cloning other git repos.\n\nA devstack plugin should never clone the repo that contains\nthe plugin itself as doing so will replace the code while it\nis executing. This can lead to the code that is running and the\ncode on disk being different if the plugin line and CYBORG_*\noptions do not match.\n\nThis change updates the devstack_setup docs with an example of\ntesting an unmerged change.\n\nThis change removes some commented out code and unneeded checks in\ndevstack/lib/*\n\nChange-Id: I473ef276197ad6578f64a708f4b66562a2ff1d49\n'}, {'number': 4, 'created': '2020-03-16 13:50:23.000000000', 'files': ['devstack/lib/cyborg', 'doc/source/contributor/devstack_setup.rst', 'devstack/lib/opae', 'devstack/settings'], 'web_link': 'https://opendev.org/openstack/cyborg/commit/67099829fa2c8fe62c6d75a85641cedf3cdffebd', 'message': 'support standard devstack plugin cloning\n\nThis change removes the non standard mechanism\nfor specifying the cyborg repo and branch\nvia CYBORG_REPO and CYBORG_BRANCH.\n\nin-tree devstack modules use the\n<PROJECT NAME>_REPO and <PROJECT NAME>_BRANCH\nconvention for specifying revisions of project to clone\nhowever devstack plugins should only use that convention\nfor cloning other git repos.\n\nA devstack plugin should never clone the repo that contains\nthe plugin itself as doing so will replace the code while it\nis executing. This can lead to the code that is running and the\ncode on disk being different if the plugin line and CYBORG_*\noptions do not match.\n\nThis change updates the devstack_setup docs with an example of\ntesting an unmerged change.\n\nThis change removes some commented out code and unneeded checks in\ndevstack/lib/*\n\nChange-Id: I473ef276197ad6578f64a708f4b66562a2ff1d49\n'}]",16,708771,67099829fa2c8fe62c6d75a85641cedf3cdffebd,25,6,4,11604,,,0,"support standard devstack plugin cloning

This change removes the non standard mechanism
for specifying the cyborg repo and branch
via CYBORG_REPO and CYBORG_BRANCH.

in-tree devstack modules use the
<PROJECT NAME>_REPO and <PROJECT NAME>_BRANCH
convention for specifying revisions of project to clone
however devstack plugins should only use that convention
for cloning other git repos.

A devstack plugin should never clone the repo that contains
the plugin itself as doing so will replace the code while it
is executing. This can lead to the code that is running and the
code on disk being different if the plugin line and CYBORG_*
options do not match.

This change updates the devstack_setup docs with an example of
testing an unmerged change.

This change removes some commented out code and unneeded checks in
devstack/lib/*

Change-Id: I473ef276197ad6578f64a708f4b66562a2ff1d49
",git fetch https://review.opendev.org/openstack/cyborg refs/changes/71/708771/2 && git format-patch -1 --stdout FETCH_HEAD,"['devstack/lib/cyborg', 'doc/source/contributor/devstack_setup.rst', 'devstack/lib/opae']",3,89ed92a5f2316602bc2b953216e0e25f0e5bb140,multinode,,"# TODO(sean-k-mooney): this should not be required. # ensure we don't re-source this in the same environment [[ -z ""$_OPAE_PKG_FNS"" ]] || return 0 declare -r -g _OPAE_PKG_FNS=1 ",17,27
openstack%2Fcyborg~master~I1a8f56e1b891d85bd35a416bbd0a5be6b30d9f33,openstack/cyborg,master,I1a8f56e1b891d85bd35a416bbd0a5be6b30d9f33,Remove useless interfaces in agent,MERGED,2020-03-03 06:57:31.000000000,2020-03-21 00:56:16.000000000,2020-03-21 00:53:55.000000000,"[{'_account_id': 14107}, {'_account_id': 14131}, {'_account_id': 21672}, {'_account_id': 22348}, {'_account_id': 24872}, {'_account_id': 25738}, {'_account_id': 26458}, {'_account_id': 27458}, {'_account_id': 28748}, {'_account_id': 30759}]","[{'number': 1, 'created': '2020-03-03 06:57:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/30baf0f8c9241cf0563ae3853ace97fcb5843b81', 'message': 'Remove useless interfaces in agent\n\nThese APIs are introduced in V1 and will not\nbe used in V2. At the same time, it will cause\nus some misunderstandings.\n\nChange-Id: I1a8f56e1b891d85bd35a416bbd0a5be6b30d9f33\nStory: 38918\n'}, {'number': 2, 'created': '2020-03-03 07:00:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/ae1ccd10ac8983d2024b76941bd50da24ccf675e', 'message': 'Remove useless interfaces in agent\n\nThese APIs are introduced in V1 and will not\nbe used in V2. At the same time, it will cause\nus some misunderstandings.\n\nChange-Id: I1a8f56e1b891d85bd35a416bbd0a5be6b30d9f33\nStory: 38918\n'}, {'number': 3, 'created': '2020-03-03 07:12:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/5d8e336489f4556c2413067fb54d33e79b3e7c56', 'message': 'Remove useless interfaces in agent\n\nThese APIs are introduced in V1 and will not\nbe used in V2. At the same time, it will cause\nus some misunderstandings.\n\nChange-Id: I1a8f56e1b891d85bd35a416bbd0a5be6b30d9f33\nStory: 2007363\n'}, {'number': 4, 'created': '2020-03-03 07:18:00.000000000', 'files': ['cyborg/agent/rpcapi.py', 'cyborg/agent/manager.py'], 'web_link': 'https://opendev.org/openstack/cyborg/commit/c616da80487b83e8506e3fa18cc707fd50f5eb5c', 'message': 'Remove useless interfaces in agent\n\nThese APIs are introduced in V1 and will not\nbe used in V2. At the same time, it will cause\nus some misunderstandings.\n\nChange-Id: I1a8f56e1b891d85bd35a416bbd0a5be6b30d9f33\nStory: 2007363\nTask: 38918\n'}]",13,710911,c616da80487b83e8506e3fa18cc707fd50f5eb5c,21,10,4,28748,,,0,"Remove useless interfaces in agent

These APIs are introduced in V1 and will not
be used in V2. At the same time, it will cause
us some misunderstandings.

Change-Id: I1a8f56e1b891d85bd35a416bbd0a5be6b30d9f33
Story: 2007363
Task: 38918
",git fetch https://review.opendev.org/openstack/cyborg refs/changes/11/710911/4 && git format-patch -1 --stdout FETCH_HEAD,"['cyborg/agent/rpcapi.py', 'cyborg/agent/manager.py']",2,30baf0f8c9241cf0563ae3853ace97fcb5843b81,,," def hardware_list(self, context, values): """"""List installed hardware."""""" pass def fpga_program(self, context, deployable_uuid, image_uuid): """"""Program a FPGA region, image can be a url or local file"""""" # TODO(Shaohe Feng) Get image from glance. # And add claim and rollback logical. path = self._download_bitstream(context, image_uuid) dep = self.cond_api.deployable_get(context, deployable_uuid) driver = self.fpga_driver.create(dep.vendor) driver.program(dep.address, path) def _download_bitstream(self, context, bitstream_uuid): """"""Download the bistream :param context: the context :param bistream_uuid: v4 uuid of the bitstream to reprogram :returns: the path to bitstream downloaded, None if fail to download """""" download_path = ""/tmp/"" + bitstream_uuid + "".bin"" self.image_api.download(context, bitstream_uuid, dest_path=download_path) return download_path ",0,47
openstack%2Ftripleo-common~stable%2Ftrain~I5fb83a13e385ed1a35a6cb735e50df2b5b368164,openstack/tripleo-common,stable/train,I5fb83a13e385ed1a35a6cb735e50df2b5b368164,Handle race for the already existing layer,MERGED,2020-03-20 13:34:39.000000000,2020-03-21 00:42:52.000000000,2020-03-21 00:35:13.000000000,"[{'_account_id': 3153}, {'_account_id': 6926}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-20 13:34:39.000000000', 'files': ['tripleo_common/image/image_export.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/419598117d1690e3228f34b4bdbd49333b535ef8', 'message': 'Handle race for the already existing layer\n\nHard copying a large layer takes time.\nRenaming of a blob may also be not instant for some cases.\nHandle race for the already existing layers by skipping competing\nlayers symlinking or renaming blobs made by concurrent workers\n\nChange-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164\nCloses-bug: #1864953\nSigned-off-by: Bogdan Dobrelya <bdobreli@redhat.com>\n(cherry picked from commit 28b8bf0fe637683123500d15ca20fdd427c2fd7d)\n'}]",0,714112,419598117d1690e3228f34b4bdbd49333b535ef8,11,5,1,14985,,,0,"Handle race for the already existing layer

Hard copying a large layer takes time.
Renaming of a blob may also be not instant for some cases.
Handle race for the already existing layers by skipping competing
layers symlinking or renaming blobs made by concurrent workers

Change-Id: I5fb83a13e385ed1a35a6cb735e50df2b5b368164
Closes-bug: #1864953
Signed-off-by: Bogdan Dobrelya <bdobreli@redhat.com>
(cherry picked from commit 28b8bf0fe637683123500d15ca20fdd427c2fd7d)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/12/714112/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_common/image/image_export.py'],1,419598117d1690e3228f34b4bdbd49333b535ef8,,"import errnoimport sixdef skip_if_exists(f): @six.wraps(f) def wrapper(*args, **kwargs): try: return f(*args, **kwargs) except OSError as e: # Handle race for the already existing entity if e.errno == errno.EEXIST: pass else: raise e return wrapper @skip_if_exists os.makedirs(path, 0o775)@skip_if_exists@skip_if_exists"," try: os.makedirs(path, 0o775) except os.error: # Handle race for directory already existing pass",20,5
openstack%2Fdevstack~master~I31bcc285ff8e373abbacb303c1269857c9cfa9ed,openstack/devstack,master,I31bcc285ff8e373abbacb303c1269857c9cfa9ed,Make database code work with mysql8,MERGED,2020-02-13 09:41:36.000000000,2020-03-21 00:36:32.000000000,2020-03-20 14:26:50.000000000,"[{'_account_id': 7118}, {'_account_id': 13252}, {'_account_id': 22348}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-02-13 09:41:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/4fb7b4e94ef225c55f91eb7014654686fed08487', 'message': 'Make database code work with mysql8\n\nThe GRANT command in mysql8 can no longer create a user implicitly.\nSplit that part into a dedicated CREATE USER command.\n\nAlso drop disabling the query_cache, it is off by default for some time\nand the option got removed in mysql8.\n\nChange-Id: I31bcc285ff8e373abbacb303c1269857c9cfa9ed\n'}, {'number': 2, 'created': '2020-02-17 11:33:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/85d509b4a86faa339c5f6e18c0a91cde6019dced', 'message': 'Make database code work with mysql8\n\nThe GRANT command in mysql8 can no longer create a user implicitly.\nSplit that part into a dedicated CREATE USER command.\n\nAlso drop disabling the query_cache, it is off by default for some time\nand the option got removed in mysql8.\n\nChange-Id: I31bcc285ff8e373abbacb303c1269857c9cfa9ed\n'}, {'number': 3, 'created': '2020-02-17 13:35:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/a1c8d67054dae150417329a307be75387c8153c4', 'message': 'Make database code work with mysql8\n\nThe GRANT command in mysql8 can no longer create a user implicitly.\nSplit that part into a dedicated CREATE USER command.\n\nAlso drop disabling the query_cache, it is off by default for some time\nand the option got removed in mysql8.\n\nChange-Id: I31bcc285ff8e373abbacb303c1269857c9cfa9ed\n'}, {'number': 4, 'created': '2020-02-17 17:15:13.000000000', 'files': ['lib/databases/mysql'], 'web_link': 'https://opendev.org/openstack/devstack/commit/08d84bc47f7341e64d6dd33f6f6a515f92840ac4', 'message': 'Make database code work with mysql8\n\nThe GRANT command in mysql8 can no longer create a user implicitly.\nSplit that part into a dedicated CREATE USER command.\n\nAlso drop disabling the query_cache, it is off by default for some time\nand the option got removed in mysql8.\n\nChange-Id: I31bcc285ff8e373abbacb303c1269857c9cfa9ed\n'}]",2,707595,08d84bc47f7341e64d6dd33f6f6a515f92840ac4,19,4,4,13252,,,0,"Make database code work with mysql8

The GRANT command in mysql8 can no longer create a user implicitly.
Split that part into a dedicated CREATE USER command.

Also drop disabling the query_cache, it is off by default for some time
and the option got removed in mysql8.

Change-Id: I31bcc285ff8e373abbacb303c1269857c9cfa9ed
",git fetch https://review.opendev.org/openstack/devstack refs/changes/95/707595/2 && git format-patch -1 --stdout FETCH_HEAD,['lib/databases/mysql'],1,4fb7b4e94ef225c55f91eb7014654686fed08487,run-focal," # Create DB user, ignore error if it already exists sudo mysql $cmd_args -e ""CREATE USER '$DATABASE_USER'@'%' identified by '$DATABASE_PASSWORD';"" || true # Update the DB to give user '$DATABASE_USER'@'%' full control of the all databases: sudo mysql $cmd_args -e ""GRANT ALL PRIVILEGES ON *.* TO '$DATABASE_USER'@'%';"""," # Update the DB to give user '$DATABASE_USER'@'%' full control of the all databases: sudo mysql $cmd_args -e ""GRANT ALL PRIVILEGES ON *.* TO '$DATABASE_USER'@'%' identified by '$DATABASE_PASSWORD';"" iniset -sudo $my_conf mysqld query_cache_type OFF iniset -sudo $my_conf mysqld query_cache_size 0",3,3
openstack%2Fcinder~master~I7d0d3174d091c1f1da384cd796e5887f89a71209,openstack/cinder,master,I7d0d3174d091c1f1da384cd796e5887f89a71209,Imported Translations from Zanata,MERGED,2020-03-19 07:17:13.000000000,2020-03-21 00:30:22.000000000,2020-03-19 12:53:47.000000000,"[{'_account_id': 9008}, {'_account_id': 11611}, {'_account_id': 11904}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 14384}, {'_account_id': 15386}, {'_account_id': 15941}, {'_account_id': 18883}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 21976}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 24921}, {'_account_id': 25678}, {'_account_id': 26537}, {'_account_id': 29705}, {'_account_id': 29716}]","[{'number': 1, 'created': '2020-03-19 07:17:13.000000000', 'files': ['cinder/locale/ja/LC_MESSAGES/cinder.po', 'cinder/locale/it/LC_MESSAGES/cinder.po', 'releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po', 'cinder/locale/zh_CN/LC_MESSAGES/cinder.po', 'cinder/locale/zh_TW/LC_MESSAGES/cinder.po', 'cinder/locale/ko_KR/LC_MESSAGES/cinder.po', 'cinder/locale/de/LC_MESSAGES/cinder.po', 'cinder/locale/fr/LC_MESSAGES/cinder.po', 'cinder/locale/pt_BR/LC_MESSAGES/cinder.po', 'cinder/locale/ru/LC_MESSAGES/cinder.po', 'cinder/locale/es/LC_MESSAGES/cinder.po'], 'web_link': 'https://opendev.org/openstack/cinder/commit/c8692366d5a61e5e0dd64479dd7d97712de37c84', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I7d0d3174d091c1f1da384cd796e5887f89a71209\n'}]",0,713791,c8692366d5a61e5e0dd64479dd7d97712de37c84,27,19,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: I7d0d3174d091c1f1da384cd796e5887f89a71209
",git fetch https://review.opendev.org/openstack/cinder refs/changes/91/713791/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/locale/ja/LC_MESSAGES/cinder.po', 'cinder/locale/it/LC_MESSAGES/cinder.po', 'cinder/locale/zh_CN/LC_MESSAGES/cinder.po', 'cinder/locale/zh_TW/LC_MESSAGES/cinder.po', 'releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po', 'cinder/locale/ko_KR/LC_MESSAGES/cinder.po', 'cinder/locale/de/LC_MESSAGES/cinder.po', 'cinder/locale/fr/LC_MESSAGES/cinder.po', 'cinder/locale/pt_BR/LC_MESSAGES/cinder.po', 'cinder/locale/ru/LC_MESSAGES/cinder.po', 'cinder/locale/es/LC_MESSAGES/cinder.po']",11,c8692366d5a61e5e0dd64479dd7d97712de37c84,zanata/translations,"""POT-Creation-Date: 2020-03-18 14:40+0000\n""msgid ""Converted to %(vol_format)s, but format is now %(file_format)s"" msgstr """" ""Se ha convertido a %(vol_format)s, pero ahora el formato es %(file_format)s"" #, python-format","""POT-Creation-Date: 2020-03-16 13:44+0000\n""#, python-format msgid """" ""Failed to make a request to Datera cluster endpoint due to the following "" ""reason: %s"" msgstr """" ""No se ha podido hacer una solicitud al punto final del clúster de Datera "" ""debido al siguiente motivo: %s"" msgid ""Request to Datera cluster returned bad status: %(status)s | %(reason)s"" msgstr """" ""La solicitud al clúster de Datera ha devuelto un estado incorrecto: "" ""%(status)s | %(reason)s"" #, python-format",56,182
openstack%2Fpuppet-keystone~master~I507d1b736dbbb147c67b9d399c033703b432b16d,openstack/puppet-keystone,master,I507d1b736dbbb147c67b9d399c033703b432b16d,Update ldap-backend options,MERGED,2020-03-18 18:07:22.000000000,2020-03-21 00:05:50.000000000,2020-03-21 00:05:50.000000000,"[{'_account_id': 3153}, {'_account_id': 5046}, {'_account_id': 9816}, {'_account_id': 9954}, {'_account_id': 14985}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-18 18:07:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-keystone/commit/cd2e0cd248211908250067272bdba29b0336ca40', 'message': ""Update ldap-backend options\n\nSeveral of the options defined in ldap_backend.pp were using duplicate\nvalues already specified in keystone/conf/ldap.py. Instead of\nduplicating the same value, we can set them to undefined and just let\nthe default values from keystone come through.\n\nThis commit also updates the values of use_pool and use_auth_pool to\nTrue so they're consistent with the default values in keystone.\n\nChange-Id: I507d1b736dbbb147c67b9d399c033703b432b16d\n""}, {'number': 2, 'created': '2020-03-18 18:19:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-keystone/commit/b874865ab2983a8badeec3b3943df4f514ee5496', 'message': ""Update ldap-backend options\n\nSeveral of the options defined in ldap_backend.pp were using duplicate\nvalues already specified in keystone/conf/ldap.py. Instead of\nduplicating the same value, we can set them to undefined and just let\nthe default values from keystone come through.\n\nThis commit also updates the values of use_pool and use_auth_pool to\nTrue so they're consistent with the default values in keystone.\n\nCo-Authored-By: Dave Wilde <dwilde@redhat.com>\n\nChange-Id: I507d1b736dbbb147c67b9d399c033703b432b16d\n""}, {'number': 3, 'created': '2020-03-18 20:56:46.000000000', 'files': ['manifests/ldap_backend.pp', 'spec/acceptance/keystone_wsgi_apache_spec.rb'], 'web_link': 'https://opendev.org/openstack/puppet-keystone/commit/1081ac51db7f560e5856809480fb98c215747476', 'message': ""Update ldap-backend options\n\nSeveral of the options defined in ldap_backend.pp were using duplicate\nvalues already specified in keystone/conf/ldap.py. Instead of\nduplicating the same value, we can set them to undefined and just let\nthe default values from keystone come through.\n\nThis commit also updates the values of use_pool and use_auth_pool to\nTrue so they're consistent with the default values in keystone.\n\nCo-Authored-By: Dave Wilde <dwilde@redhat.com>\n\nChange-Id: I507d1b736dbbb147c67b9d399c033703b432b16d\n""}]",0,713708,1081ac51db7f560e5856809480fb98c215747476,15,6,3,5046,,,0,"Update ldap-backend options

Several of the options defined in ldap_backend.pp were using duplicate
values already specified in keystone/conf/ldap.py. Instead of
duplicating the same value, we can set them to undefined and just let
the default values from keystone come through.

This commit also updates the values of use_pool and use_auth_pool to
True so they're consistent with the default values in keystone.

Co-Authored-By: Dave Wilde <dwilde@redhat.com>

Change-Id: I507d1b736dbbb147c67b9d399c033703b432b16d
",git fetch https://review.opendev.org/openstack/puppet-keystone refs/changes/08/713708/1 && git format-patch -1 --stdout FETCH_HEAD,['manifests/ldap_backend.pp'],1,cd2e0cd248211908250067272bdba29b0336ca40,remove-duplicate-ldap-defaults,"# Defaults to 'undef'# Defaults to 'undef'# Defaults to 'undef'# Defaults to 'undef'# Defaults to 'undef'# Defaults to 'undef' $use_pool = undef, $pool_size = undef, $pool_retry_max = undef, $pool_retry_delay = undef, $pool_connection_timeout = undef, $pool_connection_lifetime = undef, $use_auth_pool = undef, $auth_pool_size = undef, $auth_pool_connection_lifetime = undef,","# Defaults to false# Defaults to '10'# Defaults to '600'# Defaults to false# Defaults to '100'# Defaults to '60' $use_pool = false, $pool_size = 10, $pool_retry_max = 3, $pool_retry_delay = 0.1, $pool_connection_timeout = -1, $pool_connection_lifetime = 600, $use_auth_pool = false, $auth_pool_size = 100, $auth_pool_connection_lifetime = 60,",15,15
openstack%2Fpuppet-swift~master~I3b507d14202e92095d7e9a4e8a6b1abf4a52a2f5,openstack/puppet-swift,master,I3b507d14202e92095d7e9a4e8a6b1abf4a52a2f5,Add missing unit tests for s3api middleware,MERGED,2020-03-16 12:35:31.000000000,2020-03-20 23:56:14.000000000,2020-03-20 23:56:14.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-16 12:35:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-swift/commit/14a47197a007450abb9588ef382454e7c5b63e43', 'message': 'Add missing unit tests for s3api middleware\n\nChange-Id: I3b507d14202e92095d7e9a4e8a6b1abf4a52a2f5\n'}, {'number': 2, 'created': '2020-03-19 05:57:04.000000000', 'files': ['spec/classes/swift_proxy_s3api_spec.rb'], 'web_link': 'https://opendev.org/openstack/puppet-swift/commit/153fe8570bdde73ebead70ba4d45c05cdb9d0ab7', 'message': 'Add missing unit tests for s3api middleware\n\nAdd missing unit tests for s3api middleware, including tests to\noverride parameters.\n\nChange-Id: I3b507d14202e92095d7e9a4e8a6b1abf4a52a2f5\n'}]",0,713215,153fe8570bdde73ebead70ba4d45c05cdb9d0ab7,9,3,2,9816,,,0,"Add missing unit tests for s3api middleware

Add missing unit tests for s3api middleware, including tests to
override parameters.

Change-Id: I3b507d14202e92095d7e9a4e8a6b1abf4a52a2f5
",git fetch https://review.opendev.org/openstack/puppet-swift refs/changes/15/713215/1 && git format-patch -1 --stdout FETCH_HEAD,['spec/classes/swift_proxy_s3api_spec.rb'],1,14a47197a007450abb9588ef382454e7c5b63e43,ut-s3api," let :params do {} end context 'with default parameters' do it 'configures with default' do is_expected.to contain_swift_proxy_config('filter:s3api/use').with_value('egg:swift#s3api') is_expected.to contain_swift_proxy_config('filter:s3api/auth_pipeline_check').with_value('false') is_expected.to contain_swift_proxy_config('filter:s3api/max_upload_part_num').with_value('1000') end end context 'with overriding parameters' do before do params.merge!({ :auth_pipeline_check => true, :max_upload_part_num => '2000' }) end it 'configures with overridden parameters' do is_expected.to contain_swift_proxy_config('filter:s3api/use').with_value('egg:swift#s3api') is_expected.to contain_swift_proxy_config('filter:s3api/auth_pipeline_check').with_value('true') is_expected.to contain_swift_proxy_config('filter:s3api/max_upload_part_num').with_value('2000') end end", it { is_expected.to contain_swift_proxy_config('filter:s3api/use').with_value('egg:swift#s3api') } it { is_expected.to contain_swift_proxy_config('filter:s3api/auth_pipeline_check').with_value('false') },26,2
openstack%2Fpuppet-openstack-integration~master~Ic6abf9f09bb687e830a06e7129f77e2504ea8e2a,openstack/puppet-openstack-integration,master,Ic6abf9f09bb687e830a06e7129f77e2504ea8e2a,Use CentOS8 to run unit tests and lint tests,MERGED,2020-03-16 13:23:58.000000000,2020-03-20 23:37:32.000000000,2020-03-20 23:37:32.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-16 13:23:58.000000000', 'files': ['zuul.d/linters.yaml', 'zuul.d/unit.yaml'], 'web_link': 'https://opendev.org/openstack/puppet-openstack-integration/commit/fc189a89739c71f8cb4b8b3ba7c60333ff0735c6', 'message': 'Use CentOS8 to run unit tests and lint tests\n\nThis patch makes CentOS8 jobs voting and CentOS7 jobs disabled for\nunit tests and lint tests, so that we use CentOS8 for default CentOS\nto run jobs for current master.\n\nChange-Id: Ic6abf9f09bb687e830a06e7129f77e2504ea8e2a\n'}]",0,713226,fc189a89739c71f8cb4b8b3ba7c60333ff0735c6,7,3,1,9816,,,0,"Use CentOS8 to run unit tests and lint tests

This patch makes CentOS8 jobs voting and CentOS7 jobs disabled for
unit tests and lint tests, so that we use CentOS8 for default CentOS
to run jobs for current master.

Change-Id: Ic6abf9f09bb687e830a06e7129f77e2504ea8e2a
",git fetch https://review.opendev.org/openstack/puppet-openstack-integration refs/changes/26/713226/1 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/linters.yaml', 'zuul.d/unit.yaml']",2,fc189a89739c71f8cb4b8b3ba7c60333ff0735c6,ut-centos8, branches: ^stable/(ocata|pike|queens|rocky|stein|train)$, voting: false voting: false,2,4
openstack%2Fpuppet-gnocchi~master~I88dff282df4ce477e543dd2fcf5052a9ac472b84,openstack/puppet-gnocchi,master,I88dff282df4ce477e543dd2fcf5052a9ac472b84,Deprecate parameters under gnocchi::storage,MERGED,2020-03-17 13:55:18.000000000,2020-03-20 23:28:05.000000000,2020-03-20 23:26:42.000000000,"[{'_account_id': 3153}, {'_account_id': 9816}, {'_account_id': 14985}, {'_account_id': 16137}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 13:55:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-gnocchi/commit/c58f417dc41939bdc1f4452106ab78de6a86a372', 'message': 'Set coordination_url under DEFAULT section\n\n... because the one in storage section was deprecated in gnocchi[1] .\n\n[1] https://github.com/gnocchixyz/gnocchi/commit/70b9ca427ba7710f5b45dda0b2e1490c08219f37\n\nChange-Id: I88dff282df4ce477e543dd2fcf5052a9ac472b84\n'}, {'number': 2, 'created': '2020-03-18 13:42:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-gnocchi/commit/ce7652607522de3425dc16ad05273477e1be0d15', 'message': 'Deprecate parameters under gnocchi::storage\n\ncoordination_url[1] and metric_processing_delay[2] under storage\nsection are both deprecated, so deprecate parameters under\ngnocchi::storage and migrate these parameters to appropriate\nclasses, gnocchi and gnocchi::metricd .\n\n[1] https://github.com/gnocchixyz/gnocchi/commit/70b9ca427ba7710f5b45dda0b2e1490c08219f37\n[2] https://github.com/gnocchixyz/gnocchi/commit/72fdba704d8b3862b2b0adfb0e38c8429149c1b0\nChange-Id: I88dff282df4ce477e543dd2fcf5052a9ac472b84\n'}, {'number': 3, 'created': '2020-03-18 23:00:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-gnocchi/commit/12c889f1c0124d8cc9beb827bc09da84e4a82673', 'message': 'Deprecate parameters under gnocchi::storage\n\ncoordination_url[1] and metric_processing_delay[2] under storage\nsection are both deprecated, so deprecate parameters under\ngnocchi::storage and migrate these parameters to appropriate\nclasses, gnocchi and gnocchi::metricd .\n\n[1] https://github.com/gnocchixyz/gnocchi/commit/70b9ca427ba7710f5b45dda0b2e1490c08219f37\n[2] https://github.com/gnocchixyz/gnocchi/commit/72fdba704d8b3862b2b0adfb0e38c8429149c1b0\nChange-Id: I88dff282df4ce477e543dd2fcf5052a9ac472b84\n'}, {'number': 4, 'created': '2020-03-19 12:57:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-gnocchi/commit/451e0e2da6a3f053255e9bcd963b96818bdaaf44', 'message': 'Deprecate parameters under gnocchi::storage\n\ncoordination_url[1] and metric_processing_delay[2] under storage\nsection are both deprecated, so deprecate parameters under\ngnocchi::storage and migrate these parameters to appropriate\nclasses, gnocchi and gnocchi::metricd .\n\n[1] https://github.com/gnocchixyz/gnocchi/commit/70b9ca427ba7710f5b45dda0b2e1490c08219f37\n[2] https://github.com/gnocchixyz/gnocchi/commit/72fdba704d8b3862b2b0adfb0e38c8429149c1b0\nChange-Id: I88dff282df4ce477e543dd2fcf5052a9ac472b84\n'}, {'number': 5, 'created': '2020-03-19 13:25:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-gnocchi/commit/5d8de592c728d846a7e9c23fba20bf19e5bf14db', 'message': 'Deprecate parameters under gnocchi::storage\n\ncoordination_url[1] and metric_processing_delay[2] under storage\nsection are both deprecated, so deprecate parameters under\ngnocchi::storage and migrate these parameters to appropriate\nclasses, gnocchi and gnocchi::metricd .\n\n[1] https://github.com/gnocchixyz/gnocchi/commit/70b9ca427ba7710f5b45dda0b2e1490c08219f37\n[2] https://github.com/gnocchixyz/gnocchi/commit/72fdba704d8b3862b2b0adfb0e38c8429149c1b0\nChange-Id: I88dff282df4ce477e543dd2fcf5052a9ac472b84\n'}, {'number': 6, 'created': '2020-03-19 14:30:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-gnocchi/commit/f38f7182dec091cc6c3c3055d4d2f0b7ca6a20ba', 'message': 'Deprecate parameters under gnocchi::storage\n\ncoordination_url[1] and metric_processing_delay[2] under storage\nsection are both deprecated, so deprecate parameters under\ngnocchi::storage and migrate these parameters to appropriate\nclasses, gnocchi and gnocchi::metricd .\n\n[1] https://github.com/gnocchixyz/gnocchi/commit/70b9ca427ba7710f5b45dda0b2e1490c08219f37\n[2] https://github.com/gnocchixyz/gnocchi/commit/72fdba704d8b3862b2b0adfb0e38c8429149c1b0\nChange-Id: I88dff282df4ce477e543dd2fcf5052a9ac472b84\n'}, {'number': 7, 'created': '2020-03-20 13:07:01.000000000', 'files': ['releasenotes/notes/deprecate-storage-options-bff9a8cde960a8bc.yaml', 'spec/classes/gnocchi_storage_spec.rb', 'spec/classes/gnocchi_init_spec.rb', 'manifests/init.pp', 'manifests/metricd.pp', 'manifests/storage.pp'], 'web_link': 'https://opendev.org/openstack/puppet-gnocchi/commit/271ccb6103bab6db6a08d9f8cace803b969d699a', 'message': 'Deprecate parameters under gnocchi::storage\n\ncoordination_url[1] and metric_processing_delay[2] under storage\nsection are both deprecated, so deprecate parameters under\ngnocchi::storage and migrate these parameters to appropriate\nclasses, gnocchi and gnocchi::metricd .\n\n[1] https://github.com/gnocchixyz/gnocchi/commit/70b9ca427ba7710f5b45dda0b2e1490c08219f37\n[2] https://github.com/gnocchixyz/gnocchi/commit/72fdba704d8b3862b2b0adfb0e38c8429149c1b0\nChange-Id: I88dff282df4ce477e543dd2fcf5052a9ac472b84\n'}]",1,713448,271ccb6103bab6db6a08d9f8cace803b969d699a,24,5,7,9816,,,0,"Deprecate parameters under gnocchi::storage

coordination_url[1] and metric_processing_delay[2] under storage
section are both deprecated, so deprecate parameters under
gnocchi::storage and migrate these parameters to appropriate
classes, gnocchi and gnocchi::metricd .

[1] https://github.com/gnocchixyz/gnocchi/commit/70b9ca427ba7710f5b45dda0b2e1490c08219f37
[2] https://github.com/gnocchixyz/gnocchi/commit/72fdba704d8b3862b2b0adfb0e38c8429149c1b0
Change-Id: I88dff282df4ce477e543dd2fcf5052a9ac472b84
",git fetch https://review.opendev.org/openstack/puppet-gnocchi refs/changes/48/713448/1 && git format-patch -1 --stdout FETCH_HEAD,"['spec/classes/gnocchi_storage_spec.rb', 'manifests/storage.pp']",2,c58f417dc41939bdc1f4452106ab78de6a86a372,storage-deprecated, 'DEFAULT/coordination_url' : value => $coordination_url;, 'storage/coordination_url' : value => $coordination_url;,2,2
openstack%2Fpuppet-openstack-integration~master~I7d54591d6732797ceaf1d859343e58ece85c23a4,openstack/puppet-openstack-integration,master,I7d54591d6732797ceaf1d859343e58ece85c23a4,Updated from Puppet OpenStack modules constraints,MERGED,2020-03-18 06:14:39.000000000,2020-03-20 23:26:43.000000000,2020-03-20 23:26:43.000000000,"[{'_account_id': 3153}, {'_account_id': 16137}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-18 06:14:39.000000000', 'files': ['Puppetfile'], 'web_link': 'https://opendev.org/openstack/puppet-openstack-integration/commit/dc32b627820cf9f199b1455270c1740e4def7c11', 'message': 'Updated from Puppet OpenStack modules constraints\n\nChange-Id: I7d54591d6732797ceaf1d859343e58ece85c23a4\n'}]",0,713572,dc32b627820cf9f199b1455270c1740e4def7c11,7,3,1,11131,,,0,"Updated from Puppet OpenStack modules constraints

Change-Id: I7d54591d6732797ceaf1d859343e58ece85c23a4
",git fetch https://review.opendev.org/openstack/puppet-openstack-integration refs/changes/72/713572/1 && git format-patch -1 --stdout FETCH_HEAD,['Puppetfile'],1,dc32b627820cf9f199b1455270c1740e4def7c11,openstack/puppet/constraints, :ref => 'v6.4.0', :ref => 'v6.3.0',1,1
openstack%2Ftripleo-common~stable%2Fstein~I59f89730e6ed52e4ca54dbd19fcbf6aea0506450,openstack/tripleo-common,stable/stein,I59f89730e6ed52e4ca54dbd19fcbf6aea0506450,Fix path of log file.,MERGED,2020-03-20 08:39:40.000000000,2020-03-20 23:16:42.000000000,2020-03-20 23:14:54.000000000,"[{'_account_id': 8297}, {'_account_id': 14985}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 28522}, {'_account_id': 31245}]","[{'number': 1, 'created': '2020-03-20 08:39:40.000000000', 'files': ['workbooks/package_update.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/b30b33e93529656aa865693919d5c69347a82f62', 'message': ""Fix path of log file.\n\nIncorrect log file location afer node upgrade failed. The ansible.log\nfile doesn't exists.\n\nChange-Id: I59f89730e6ed52e4ca54dbd19fcbf6aea0506450\n(cherry picked from commit 92879675cd83d73b42f273b040722367f9bad580)\n""}]",0,714048,b30b33e93529656aa865693919d5c69347a82f62,11,7,1,31245,,,0,"Fix path of log file.

Incorrect log file location afer node upgrade failed. The ansible.log
file doesn't exists.

Change-Id: I59f89730e6ed52e4ca54dbd19fcbf6aea0506450
(cherry picked from commit 92879675cd83d73b42f273b040722367f9bad580)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/48/714048/1 && git format-patch -1 --stdout FETCH_HEAD,['workbooks/package_update.yaml'],1,b30b33e93529656aa865693919d5c69347a82f62,stein," message: Ansible failed, check log at <% $.work_dir %>/<% execution().id %>/package_update.log. message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/package_update.log."," message: Ansible failed, check log at <% $.work_dir %>/<% execution().id %>/ansible.log. message: Ansible failed, check log at <% $.get('work_dir') %>/<% execution().id %>/ansible.log.",2,2
openstack%2Ftripleo-upgrade~stable%2Ftrain~I71fd88ca085fc614bee6cba6136eb4dd6f997984,openstack/tripleo-upgrade,stable/train,I71fd88ca085fc614bee6cba6136eb4dd6f997984,Fix bindeps,MERGED,2020-03-19 09:28:21.000000000,2020-03-20 23:14:52.000000000,2020-03-20 23:14:52.000000000,"[{'_account_id': 6816}, {'_account_id': 11090}, {'_account_id': 11166}, {'_account_id': 16515}, {'_account_id': 20775}, {'_account_id': 22348}, {'_account_id': 26343}]","[{'number': 1, 'created': '2020-03-19 09:28:21.000000000', 'files': ['molecule/bindep.txt'], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/9da69c75457768a61d9bd2389188dd127744ab4d', 'message': 'Fix bindeps\n\nPackages for python3 support need to be\nadjusted to run the tests locally.\n\nChange-Id: I71fd88ca085fc614bee6cba6136eb4dd6f997984\n(cherry picked from commit 99bbdd3c7da587ab885f78eba3ad4ddd26bef879)\n'}]",0,713804,9da69c75457768a61d9bd2389188dd127744ab4d,11,7,1,6816,,,0,"Fix bindeps

Packages for python3 support need to be
adjusted to run the tests locally.

Change-Id: I71fd88ca085fc614bee6cba6136eb4dd6f997984
(cherry picked from commit 99bbdd3c7da587ab885f78eba3ad4ddd26bef879)
",git fetch https://review.opendev.org/openstack/tripleo-upgrade refs/changes/04/713804/1 && git format-patch -1 --stdout FETCH_HEAD,['molecule/bindep.txt'],1,9da69c75457768a61d9bd2389188dd127744ab4d,ffwd-upgrade2-stable/train,python-devel [platform:rpm !platform:rhel-8 !platform:centos-8] python3-devel [platform:rpm !platform:rhel-7 !platform:centos-7] PyYAML [platform:rpm !platform:rhel-8 !platform:centos-8] python3-pyyaml [platform:rpm !platform:rhel-7 !platform:centos-7] python3-dnf [platform:rpm !platform:rhel-7 !platform:centos-7]libselinux-python [platform:rpm !platform:rhel-8 !platform:centos-8] libsemanage-python [platform:redhat !platform:rhel-8 !platform:centos-8] libselinux-python3 [platform:rpm !platform:rhel-7 !platform:centos-7] libsemanage-python3 [platform:redhat !platform:rhel-7 !platform:centos-7],python-devel [platform:rpm] python2-dnf [platform:fedora]libselinux-python [platform:rpm] libsemanage-python [platform:redhat],9,4
openstack%2Fnova~master~I0b6d1a98726ffa1ebc78fb3c4563a2e4b40ddeff,openstack/nova,master,I0b6d1a98726ffa1ebc78fb3c4563a2e4b40ddeff,images: Allow the output format of qemu-img info to be controlled,MERGED,2020-02-10 16:29:30.000000000,2020-03-20 22:46:14.000000000,2020-03-19 12:53:43.000000000,"[{'_account_id': 9008}, {'_account_id': 9708}, {'_account_id': 9732}, {'_account_id': 10118}, {'_account_id': 10135}, {'_account_id': 11604}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26515}, {'_account_id': 28522}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-02-10 16:29:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/49ae7ef02f2b6732c578bcf28eafbbc1766e4d70', 'message': 'images: Use JSON as the output format of qemu-img\n\nThe JSON format is easier to parse within QemuImgInfo and should allow\nadditional information to be extracted from qemu-img calls in the future\nsuch as ``format specific details`` etc.\n\nChange-Id: I0b6d1a98726ffa1ebc78fb3c4563a2e4b40ddeff\n'}, {'number': 2, 'created': '2020-02-11 18:11:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/4aaca71b5444d6fb5fd80efddb333aa11326a2c4', 'message': 'images: Allow the output format of qemu-img info to be controlled\n\nThis will allow for the use of the JSON output format that is easier to\nparse within QemuImgInfo and should allow additional information to be\nextracted from qemu-img calls in the future.\n\nChange-Id: I0b6d1a98726ffa1ebc78fb3c4563a2e4b40ddeff\n'}, {'number': 3, 'created': '2020-02-19 15:53:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/152278f7e1c96237acc80d197453de3d651d5b14', 'message': 'images: Allow the output format of qemu-img info to be controlled\n\nThis will allow for the use of the JSON output format that is easier to\nparse within QemuImgInfo and should allow additional information to be\nextracted from qemu-img calls in the future.\n\nChange-Id: I0b6d1a98726ffa1ebc78fb3c4563a2e4b40ddeff\n'}, {'number': 4, 'created': '2020-03-02 10:05:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ad42a0bc9e412283b4830b545f7d71448c1cf73a', 'message': 'images: Allow the output format of qemu-img info to be controlled\n\nThis will allow for the use of the JSON output format that is easier to\nparse within QemuImgInfo and should allow additional information to be\nextracted from qemu-img calls in the future.\n\nChange-Id: I0b6d1a98726ffa1ebc78fb3c4563a2e4b40ddeff\n'}, {'number': 5, 'created': '2020-03-02 14:46:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a12924871e916b36e18dbd00d79bad31b7572bd6', 'message': 'images: Allow the output format of qemu-img info to be controlled\n\nThis will allow for the use of the JSON output format that is easier to\nparse within QemuImgInfo and should allow additional information to be\nextracted from qemu-img calls in the future.\n\nChange-Id: I0b6d1a98726ffa1ebc78fb3c4563a2e4b40ddeff\n'}, {'number': 6, 'created': '2020-03-03 09:09:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b2416d2a940ffdd25224dcbd560fe6f2b24fbf61', 'message': 'images: Allow the output format of qemu-img info to be controlled\n\nThis will allow for the use of the JSON output format that is easier to\nparse within QemuImgInfo and should allow additional information to be\nextracted from qemu-img calls in the future.\n\nChange-Id: I0b6d1a98726ffa1ebc78fb3c4563a2e4b40ddeff\n'}, {'number': 7, 'created': '2020-03-09 19:19:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8c74b127c175a387d1407fc5d2091b43ee1f30f1', 'message': 'images: Allow the output format of qemu-img info to be controlled\n\nThis will allow for the use of the JSON output format that is easier to\nparse within QemuImgInfo and should allow additional information to be\nextracted from qemu-img calls in the future.\n\nChange-Id: I0b6d1a98726ffa1ebc78fb3c4563a2e4b40ddeff\n'}, {'number': 8, 'created': '2020-03-11 10:52:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/5fc7bca27ed754ccab9006c9a9602eb9bc14664d', 'message': 'images: Allow the output format of qemu-img info to be controlled\n\nThis will allow for the use of the JSON output format that is easier to\nparse within QemuImgInfo and should allow additional information to be\nextracted from qemu-img calls in the future.\n\nChange-Id: I0b6d1a98726ffa1ebc78fb3c4563a2e4b40ddeff\n'}, {'number': 9, 'created': '2020-03-16 09:45:47.000000000', 'files': ['nova/privsep/qemu.py', 'nova/tests/unit/virt/libvirt/test_utils.py', 'nova/virt/images.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/28f3e0070a214f750ecad8bd8001ccb5c5c27fd4', 'message': 'images: Allow the output format of qemu-img info to be controlled\n\nThis will allow for the use of the JSON output format that is easier to\nparse within QemuImgInfo and should allow additional information to be\nextracted from qemu-img calls in the future.\n\nChange-Id: I0b6d1a98726ffa1ebc78fb3c4563a2e4b40ddeff\n'}]",8,706898,28f3e0070a214f750ecad8bd8001ccb5c5c27fd4,123,15,9,10135,,,0,"images: Allow the output format of qemu-img info to be controlled

This will allow for the use of the JSON output format that is easier to
parse within QemuImgInfo and should allow additional information to be
extracted from qemu-img calls in the future.

Change-Id: I0b6d1a98726ffa1ebc78fb3c4563a2e4b40ddeff
",git fetch https://review.opendev.org/openstack/nova refs/changes/98/706898/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/privsep/qemu.py', 'nova/virt/images.py']",2,49ae7ef02f2b6732c578bcf28eafbbc1766e4d70,bug/1861071," return imageutils.QemuImgInfo(info, format='json')", return imageutils.QemuImgInfo(info),3,2
openstack%2Ftripleo-docs~master~I95c414be4881a230466527e399f2490659a6b61e,openstack/tripleo-docs,master,I95c414be4881a230466527e399f2490659a6b61e,Update the ldap configuration guide to use correct defaults,MERGED,2020-03-19 01:07:02.000000000,2020-03-20 22:38:30.000000000,2020-03-20 22:36:52.000000000,"[{'_account_id': 3153}, {'_account_id': 9954}, {'_account_id': 14985}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 01:07:02.000000000', 'files': ['deploy-guide/source/features/domain_specific_ldap_backends.rst'], 'web_link': 'https://opendev.org/openstack/tripleo-docs/commit/d2a0f148c405ba47be760af490d261a0b495a8db', 'message': ""Update the ldap configuration guide to use correct defaults\n\nThis change goes hand-in-hand with another patch to puppet-keystone [0]\nthat removes default values from puppet-keystone. If configuration\nvalues aren't defined in keystone's configuration file, it will use it's\ndefaults. Having defaults in two places just makes it easier for them to\nget out-of-sync, which is the case in 713708.\n\nThis updates the docs to reflect the default values in keystone.\n\n[0]  https://review.opendev.org/#/c/713708/\n\nChange-Id: I95c414be4881a230466527e399f2490659a6b61e\n""}]",0,713768,d2a0f148c405ba47be760af490d261a0b495a8db,9,4,1,5046,,,0,"Update the ldap configuration guide to use correct defaults

This change goes hand-in-hand with another patch to puppet-keystone [0]
that removes default values from puppet-keystone. If configuration
values aren't defined in keystone's configuration file, it will use it's
defaults. Having defaults in two places just makes it easier for them to
get out-of-sync, which is the case in 713708.

This updates the docs to reflect the default values in keystone.

[0]  https://review.opendev.org/#/c/713708/

Change-Id: I95c414be4881a230466527e399f2490659a6b61e
",git fetch https://review.opendev.org/openstack/tripleo-docs refs/changes/68/713768/1 && git format-patch -1 --stdout FETCH_HEAD,['deploy-guide/source/features/domain_specific_ldap_backends.rst'],1,d2a0f148c405ba47be760af490d261a0b495a8db,remove-duplicate-ldap-defaults, true) all. (boolean value and defaults to true), false) all. (boolean value and defaults to false),2,2
openstack%2Ftripleo-ansible~master~I03cd204f80c5cdc0be939b5f70762067f64c9ba7,openstack/tripleo-ansible,master,I03cd204f80c5cdc0be939b5f70762067f64c9ba7,Add Centos 8 to galaxy_info versions,MERGED,2020-03-18 11:53:30.000000000,2020-03-20 22:38:29.000000000,2020-03-20 22:38:29.000000000,"[{'_account_id': 3153}, {'_account_id': 6926}, {'_account_id': 7353}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25877}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-03-18 11:53:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/110222c4a3a872544da46c01bf9013dfc5f0888f', 'message': 'Add Centos 8 to galaxy_info versions\n\nChange-Id: I03cd204f80c5cdc0be939b5f70762067f64c9ba7\nSigned-off-by: Bogdan Dobrelya <bdobreli@redhat.com>\n'}, {'number': 2, 'created': '2020-03-19 13:22:29.000000000', 'files': ['tripleo_ansible/roles/tripleo_ovs_dpdk/meta/main.yml', 'tripleo_ansible/roles/tuned/meta/main.yml', 'tripleo_ansible/roles/tripleo_image_serve/meta/main.yml', 'tripleo_ansible/roles/backup_and_restore/meta/main.yml', '_skeleton_role_/meta/main.yml.j2', 'tripleo_ansible/roles/tripleo_create_admin/meta/main.yml', 'tripleo_ansible/roles/tripleo_sshd/meta/main.yml', 'tripleo_ansible/roles/tripleo_nova_image_cache/meta/main.yml', 'tripleo_ansible/roles/tripleo_ssh_known_hosts/meta/main.yml', 'tripleo_ansible/roles/tripleo_systemd_wrapper/meta/main.yml', 'tripleo_ansible/roles/tripleo_validations_package/meta/main.yml', 'tripleo_ansible/roles/tripleo_container_stop/meta/main.yml', 'tripleo_ansible/roles/octavia_common/meta/main.yml', 'tripleo_ansible/roles/tripleo_clients_install/meta/main.yml', 'tripleo_ansible/roles/tripleo_puppet_cache/meta/main.yml', 'tripleo_ansible/roles/octavia_overcloud_config/meta/main.yml', 'tripleo_ansible/roles/tripleo_ptp/meta/main.yml', 'tripleo_ansible/roles/login_defs/meta/main.yml', 'tripleo_ansible/roles/tripleo_securetty/meta/main.yml', 'tripleo_ansible/roles/test_package_action/meta/main.yml', 'tripleo_ansible/roles/octavia_controller_post_config/meta/main.yml', 'tripleo_ansible/roles/tripleo_hosts_entries/meta/main.yml', 'tripleo_ansible/roles/tripleo_container_manage/meta/main.yml', 'tripleo_ansible/roles/tripleo_keystone_resources/meta/main.yml', 'tripleo_ansible/roles/octavia_controller_config/meta/main.yml', 'tripleo_ansible/roles/tripleo_config/meta/main.yml', 'tripleo_ansible/roles/tripleo_ceph_work_dir/meta/main.yml', 'tripleo_ansible/roles/tripleo_kernel/meta/main.yml', 'tripleo_ansible/roles/tripleo_ceph_run_ansible/meta/main.yml', 'tripleo_ansible/roles/tripleo_ceph_uuid/meta/main.yml', 'tripleo_ansible/roles/tripleo_hieradata/meta/main.yml', 'tripleo_ansible/roles/tripleo_packages/meta/main.yml', 'tripleo_ansible/roles/tripleo_bootstrap/meta/main.yml', 'tripleo_ansible/roles/tripleo_container_image_prepare/meta/main.yml', 'tripleo_ansible/roles/tripleo_podman/meta/main.yml', 'tripleo_ansible/roles/octavia_undercloud/meta/main.yml', 'tripleo_ansible/roles/tripleo_firewall/meta/main.yml', 'tripleo_ansible/roles/tripleo_upgrade_hiera/meta/main.yml', 'tripleo_ansible/roles/tripleo_cellv2/meta/main.yml', 'tripleo_ansible/roles/tripleo_container_rm/meta/main.yml', 'tripleo_ansible/roles/tripleo_container_tag/meta/main.yml', 'tripleo_ansible/roles/tripleo_transfer/meta/main.yml', 'tripleo_ansible/roles/tripleo_module_load/meta/main.yml', 'tripleo_ansible/roles/aide/meta/main.yml', 'tripleo_ansible/roles/tripleo_timezone/meta/main.yml', 'tripleo_ansible/roles/test_deps/meta/main.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/b338e1669218e933aba67febc05531dae258042e', 'message': 'Add Centos 8 to galaxy_info versions\n\nAlso drop Fedora 28 support\n\nChange-Id: I03cd204f80c5cdc0be939b5f70762067f64c9ba7\nSigned-off-by: Bogdan Dobrelya <bdobreli@redhat.com>\n'}]",1,713631,b338e1669218e933aba67febc05531dae258042e,18,8,2,6926,,,0,"Add Centos 8 to galaxy_info versions

Also drop Fedora 28 support

Change-Id: I03cd204f80c5cdc0be939b5f70762067f64c9ba7
Signed-off-by: Bogdan Dobrelya <bdobreli@redhat.com>
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/31/713631/1 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_ansible/roles/tripleo_ovs_dpdk/meta/main.yml', 'tripleo_ansible/roles/tuned/meta/main.yml', 'tripleo_ansible/roles/tripleo_image_serve/meta/main.yml', 'tripleo_ansible/roles/backup_and_restore/meta/main.yml', '_skeleton_role_/meta/main.yml.j2', 'tripleo_ansible/roles/tripleo_create_admin/meta/main.yml', 'tripleo_ansible/roles/tripleo_sshd/meta/main.yml', 'tripleo_ansible/roles/tripleo_nova_image_cache/meta/main.yml', 'tripleo_ansible/roles/tripleo_ssh_known_hosts/meta/main.yml', 'tripleo_ansible/roles/tripleo_systemd_wrapper/meta/main.yml', 'tripleo_ansible/roles/tripleo_validations_package/meta/main.yml', 'tripleo_ansible/roles/tripleo_container_stop/meta/main.yml', 'tripleo_ansible/roles/octavia_common/meta/main.yml', 'tripleo_ansible/roles/tripleo_clients_install/meta/main.yml', 'tripleo_ansible/roles/tripleo_puppet_cache/meta/main.yml', 'tripleo_ansible/roles/octavia_overcloud_config/meta/main.yml', 'tripleo_ansible/roles/tripleo_ptp/meta/main.yml', 'tripleo_ansible/roles/login_defs/meta/main.yml', 'tripleo_ansible/roles/tripleo_securetty/meta/main.yml', 'tripleo_ansible/roles/test_package_action/meta/main.yml', 'tripleo_ansible/roles/octavia_controller_post_config/meta/main.yml', 'tripleo_ansible/roles/tripleo_hosts_entries/meta/main.yml', 'tripleo_ansible/roles/tripleo_container_manage/meta/main.yml', 'tripleo_ansible/roles/tripleo_keystone_resources/meta/main.yml', 'tripleo_ansible/roles/octavia_controller_config/meta/main.yml', 'tripleo_ansible/roles/tripleo_config/meta/main.yml', 'tripleo_ansible/roles/tripleo_ceph_work_dir/meta/main.yml', 'tripleo_ansible/roles/tripleo_kernel/meta/main.yml', 'tripleo_ansible/roles/tripleo_ceph_run_ansible/meta/main.yml', 'tripleo_ansible/roles/tripleo_ceph_uuid/meta/main.yml', 'tripleo_ansible/roles/tripleo_hieradata/meta/main.yml', 'tripleo_ansible/roles/tripleo_packages/meta/main.yml', 'tripleo_ansible/roles/tripleo_bootstrap/meta/main.yml', 'tripleo_ansible/roles/tripleo_container_image_prepare/meta/main.yml', 'tripleo_ansible/roles/tripleo_podman/meta/main.yml', 'tripleo_ansible/roles/octavia_undercloud/meta/main.yml', 'tripleo_ansible/roles/tripleo_firewall/meta/main.yml', 'tripleo_ansible/roles/tripleo_upgrade_hiera/meta/main.yml', 'tripleo_ansible/roles/tripleo_cellv2/meta/main.yml', 'tripleo_ansible/roles/tripleo_container_rm/meta/main.yml', 'tripleo_ansible/roles/tripleo_container_tag/meta/main.yml', 'tripleo_ansible/roles/tripleo_transfer/meta/main.yml', 'tripleo_ansible/roles/tripleo_module_load/meta/main.yml', 'tripleo_ansible/roles/aide/meta/main.yml', 'tripleo_ansible/roles/tripleo_timezone/meta/main.yml', 'tripleo_ansible/roles/test_deps/meta/main.yml']",46,110222c4a3a872544da46c01bf9013dfc5f0888f,, - 8,,46,0
openstack%2Ftripleo-ipsec~stable%2Frocky~I3b29c7671a3349ca8f7fba0918804888e4bf4c80,openstack/tripleo-ipsec,stable/rocky,I3b29c7671a3349ca8f7fba0918804888e4bf4c80,Update TOX/UPPER_CONSTRAINTS_FILE for stable/rocky,MERGED,2020-03-19 21:48:49.000000000,2020-03-20 22:35:11.000000000,2020-03-20 22:35:10.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 21:48:49.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/tripleo-ipsec/commit/ec8aec1d42b9085ed9152ff767eb6744095d9e16', 'message': 'Update TOX/UPPER_CONSTRAINTS_FILE for stable/rocky\n\nUpdate the URL to the upper-constraints file to point to the redirect\nrule on releases.openstack.org so that anyone working on this branch\nwill switch to the correct upper-constraints list automatically when\nthe requirements repository branches.\n\nUntil the requirements repository has as stable/rocky branch, tests will\ncontinue to use the upper-constraints list on master.\n\nChange-Id: I3b29c7671a3349ca8f7fba0918804888e4bf4c80\n'}]",0,713994,ec8aec1d42b9085ed9152ff767eb6744095d9e16,7,3,1,22816,,,0,"Update TOX/UPPER_CONSTRAINTS_FILE for stable/rocky

Update the URL to the upper-constraints file to point to the redirect
rule on releases.openstack.org so that anyone working on this branch
will switch to the correct upper-constraints list automatically when
the requirements repository branches.

Until the requirements repository has as stable/rocky branch, tests will
continue to use the upper-constraints list on master.

Change-Id: I3b29c7671a3349ca8f7fba0918804888e4bf4c80
",git fetch https://review.opendev.org/openstack/tripleo-ipsec refs/changes/94/713994/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,ec8aec1d42b9085ed9152ff767eb6744095d9e16,create-rocky, pip install -c{env:UPPER_CONSTRAINTS_FILE:https://releases.openstack.org/constraints/upper/rocky} {opts} {packages}, pip install -c{env:UPPER_CONSTRAINTS_FILE:https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt} {opts} {packages},1,1
openstack%2Ftripleo-operator-ansible~master~Ibca6de9c050276621542a7941d4d1c72f7a9121f,openstack/tripleo-operator-ansible,master,Ibca6de9c050276621542a7941d4d1c72f7a9121f,add missing tripleo_overcloud_image_upload_output fact,MERGED,2020-03-20 19:30:51.000000000,2020-03-20 22:35:10.000000000,2020-03-20 22:35:10.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 19:30:51.000000000', 'files': ['roles/tripleo_overcloud_image_upload/tasks/main.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/18823a7585a7538e41ee9fdbb248b1ba23fceac6', 'message': 'add missing tripleo_overcloud_image_upload_output fact\n\nChange-Id: Ibca6de9c050276621542a7941d4d1c72f7a9121f\n'}]",0,714200,18823a7585a7538e41ee9fdbb248b1ba23fceac6,7,3,1,25001,,,0,"add missing tripleo_overcloud_image_upload_output fact

Change-Id: Ibca6de9c050276621542a7941d4d1c72f7a9121f
",git fetch https://review.opendev.org/openstack/tripleo-operator-ansible refs/changes/00/714200/1 && git format-patch -1 --stdout FETCH_HEAD,['roles/tripleo_overcloud_image_upload/tasks/main.yml'],1,18823a7585a7538e41ee9fdbb248b1ba23fceac6,," - name: Set output fact set_fact: tripleo_overcloud_image_upload_output: ""{{ tripleo_overcloud_image_upload_result.stdout }}""",,4,0
openstack%2Ftripleo-ipsec~stable%2Frocky~If5f14c2ea4b59f2147afcebf2d15bd06cb219f8b,openstack/tripleo-ipsec,stable/rocky,If5f14c2ea4b59f2147afcebf2d15bd06cb219f8b,Update .gitreview for stable/rocky,MERGED,2020-03-19 21:48:48.000000000,2020-03-20 22:35:01.000000000,2020-03-20 22:35:01.000000000,"[{'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 21:48:48.000000000', 'files': ['.gitreview'], 'web_link': 'https://opendev.org/openstack/tripleo-ipsec/commit/984dd52643ed85c8ebfbf7d3208cd34819524b99', 'message': 'Update .gitreview for stable/rocky\n\nChange-Id: If5f14c2ea4b59f2147afcebf2d15bd06cb219f8b\n'}]",0,713993,984dd52643ed85c8ebfbf7d3208cd34819524b99,7,3,1,22816,,,0,"Update .gitreview for stable/rocky

Change-Id: If5f14c2ea4b59f2147afcebf2d15bd06cb219f8b
",git fetch https://review.opendev.org/openstack/tripleo-ipsec refs/changes/93/713993/1 && git format-patch -1 --stdout FETCH_HEAD,['.gitreview'],1,984dd52643ed85c8ebfbf7d3208cd34819524b99,create-rocky,defaultbranch=stable/rocky,,1,0
openstack%2Fopenstack-virtual-baremetal~master~I098bffd81d387cefaa8d07a9db1ec28c1c5d961c,openstack/openstack-virtual-baremetal,master,I098bffd81d387cefaa8d07a9db1ec28c1c5d961c,Update default power managemnt driver to 'ipmi',MERGED,2020-03-16 13:30:40.000000000,2020-03-20 22:32:05.000000000,2020-03-20 22:32:05.000000000,"[{'_account_id': 6928}, {'_account_id': 9592}, {'_account_id': 10239}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 24245}]","[{'number': 1, 'created': '2020-03-16 13:30:40.000000000', 'files': ['openstack_virtual_baremetal/build_nodes_json.py'], 'web_link': 'https://opendev.org/openstack/openstack-virtual-baremetal/commit/92df14be1bb865ee2dd9b352c2d0821832f5497a', 'message': ""Update default power managemnt driver to 'ipmi'\n\nThe 'pxe_ipmitool' driver was replaced by the 'ipmi'\ndriver. Let's update the default.\n\nChange-Id: I098bffd81d387cefaa8d07a9db1ec28c1c5d961c\n""}]",0,713229,92df14be1bb865ee2dd9b352c2d0821832f5497a,13,7,1,24245,,,0,"Update default power managemnt driver to 'ipmi'

The 'pxe_ipmitool' driver was replaced by the 'ipmi'
driver. Let's update the default.

Change-Id: I098bffd81d387cefaa8d07a9db1ec28c1c5d961c
",git fetch https://review.opendev.org/openstack/openstack-virtual-baremetal refs/changes/29/713229/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack_virtual_baremetal/build_nodes_json.py'],1,92df14be1bb865ee2dd9b352c2d0821832f5497a,update-default-pm-driver," parser.add_argument('--driver', default='ipmi',"," # TODO(dtantsur): change the default to ipmi when Ocata is not supported parser.add_argument('--driver', default='pxe_ipmitool',",1,2
openstack%2Ftripleo-operator-ansible~master~Ia905d8ad948a5b034f7e28e66b1ce9ca3b5b8892,openstack/tripleo-operator-ansible,master,Ia905d8ad948a5b034f7e28e66b1ce9ca3b5b8892,Add overcloud external upgrade run,MERGED,2020-03-17 21:49:33.000000000,2020-03-20 22:23:48.000000000,2020-03-20 22:23:48.000000000,"[{'_account_id': 3153}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 21:49:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/640b051d8ec48092b74440444b840589e22bf28a', 'message': 'Add overcloud external upgrade run\n\nAdds tripleo_overcloud_external_upgrade_run to run the external upgrade\nprocess.\n\nChange-Id: Ia905d8ad948a5b034f7e28e66b1ce9ca3b5b8892\n'}, {'number': 2, 'created': '2020-03-17 21:57:25.000000000', 'files': ['roles/tripleo_overcloud_external_upgrade_run/meta/main.yml', 'zuul.d/molecule.yaml', 'roles/tripleo_overcloud_external_upgrade_run/tasks/main.yml', 'roles/tripleo_overcloud_external_upgrade_run/defaults/main.yml', 'roles/tripleo_overcloud_external_upgrade_run/tests/test.yml', 'roles/tripleo_overcloud_external_upgrade_run/molecule/default/converge.yml', 'roles/tripleo_overcloud_external_upgrade_run/molecule/default/prepare.yml', 'roles/tripleo_overcloud_external_upgrade_run/tests/inventory', 'roles/tripleo_overcloud_external_upgrade_run/molecule/default/molecule.yml', 'roles/tripleo_overcloud_external_upgrade_run/README.md'], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/70d42736498a27311b29f6b05e66112ba0c70b8a', 'message': 'Add overcloud external upgrade run\n\nAdds tripleo_overcloud_external_upgrade_run to run the external upgrade\nprocess.\n\nChange-Id: Ia905d8ad948a5b034f7e28e66b1ce9ca3b5b8892\n'}]",0,713544,70d42736498a27311b29f6b05e66112ba0c70b8a,8,2,2,14985,,,0,"Add overcloud external upgrade run

Adds tripleo_overcloud_external_upgrade_run to run the external upgrade
process.

Change-Id: Ia905d8ad948a5b034f7e28e66b1ce9ca3b5b8892
",git fetch https://review.opendev.org/openstack/tripleo-operator-ansible refs/changes/44/713544/2 && git format-patch -1 --stdout FETCH_HEAD,"['roles/tripleo_overcloud_external_upgrade_run/meta/main.yml', 'zuul.d/molecule.yaml', 'roles/tripleo_overcloud_external_upgrade_run/defaults/main.yml', 'roles/tripleo_overcloud_external_upgrade_run/tasks/main.yml', 'roles/tripleo_overcloud_external_upgrade_run/tests/test.yml', 'roles/tripleo_overcloud_external_upgrade_run/molecule/default/converge.yml', 'roles/tripleo_overcloud_external_upgrade_run/molecule/default/prepare.yml', 'roles/tripleo_overcloud_external_upgrade_run/tests/inventory', 'roles/tripleo_overcloud_external_upgrade_run/README.md', 'roles/tripleo_overcloud_external_upgrade_run/molecule/default/molecule.yml']",10,640b051d8ec48092b74440444b840589e22bf28a,overcloud-external,--- driver: name: delegated options: managed: false ansible_connection_options: ansible_connection: local log: true platforms: - name: instance provisioner: name: ansible scenario: name: default test_sequence: - prepare - syntax - converge - verify ,,316,0
openstack%2Ftripleo-operator-ansible~master~Ibe1da4d580061e15bd4ccf8cb162f76573cfb0a8,openstack/tripleo-operator-ansible,master,Ibe1da4d580061e15bd4ccf8cb162f76573cfb0a8,Add overcloud external update run,MERGED,2020-03-17 21:46:18.000000000,2020-03-20 22:23:48.000000000,2020-03-20 22:23:48.000000000,"[{'_account_id': 3153}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 21:46:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/4f12c264343f75e8e9e2a2287f7272de7ff52cb6', 'message': 'Add overcloud external update run\n\nAdds tripleo_overcloud_external_update_run to execute an external update\nprocess.\n\nChange-Id: Ibe1da4d580061e15bd4ccf8cb162f76573cfb0a8\n'}, {'number': 2, 'created': '2020-03-17 21:57:25.000000000', 'files': ['roles/tripleo_overcloud_external_update_run/tasks/main.yml', 'zuul.d/molecule.yaml', 'roles/tripleo_overcloud_external_update_run/README.md', 'roles/tripleo_overcloud_external_update_run/molecule/default/prepare.yml', 'roles/tripleo_overcloud_external_update_run/molecule/default/converge.yml', 'roles/tripleo_overcloud_external_update_run/tests/test.yml', 'roles/tripleo_overcloud_external_update_run/tests/inventory', 'roles/tripleo_overcloud_external_update_run/molecule/default/molecule.yml', 'roles/tripleo_overcloud_external_update_run/meta/main.yml', 'roles/tripleo_overcloud_external_update_run/defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/3ed70ebce16169bbbe469e6024fddec759943fa5', 'message': 'Add overcloud external update run\n\nAdds tripleo_overcloud_external_update_run to execute an external update\nprocess.\n\nChange-Id: Ibe1da4d580061e15bd4ccf8cb162f76573cfb0a8\n'}]",0,713543,3ed70ebce16169bbbe469e6024fddec759943fa5,9,2,2,14985,,,0,"Add overcloud external update run

Adds tripleo_overcloud_external_update_run to execute an external update
process.

Change-Id: Ibe1da4d580061e15bd4ccf8cb162f76573cfb0a8
",git fetch https://review.opendev.org/openstack/tripleo-operator-ansible refs/changes/43/713543/2 && git format-patch -1 --stdout FETCH_HEAD,"['roles/tripleo_overcloud_external_update_run/tasks/main.yml', 'zuul.d/molecule.yaml', 'roles/tripleo_overcloud_external_update_run/README.md', 'roles/tripleo_overcloud_external_update_run/molecule/default/prepare.yml', 'roles/tripleo_overcloud_external_update_run/molecule/default/converge.yml', 'roles/tripleo_overcloud_external_update_run/tests/test.yml', 'roles/tripleo_overcloud_external_update_run/tests/inventory', 'roles/tripleo_overcloud_external_update_run/molecule/default/molecule.yml', 'roles/tripleo_overcloud_external_update_run/meta/main.yml', 'roles/tripleo_overcloud_external_update_run/defaults/main.yml']",10,4f12c264343f75e8e9e2a2287f7272de7ff52cb6,overcloud-external,"--- # defaults file for tripleo_overcloud_external_update_run openstack_bin: openstack tripleo_overcloud_external_update_run_debug: false tripleo_overcloud_external_update_run_extra_vars: [] tripleo_overcloud_external_update_run_home_dir: ""{{ ansible_env.HOME }}"" tripleo_overcloud_external_update_run_log: ""{{ tripleo_overcloud_external_update_run_home_dir }}/overcloud_external_update_run.log"" tripleo_overcloud_external_update_run_log_combine: true tripleo_overcloud_external_update_run_log_output: true tripleo_overcloud_external_update_run_os_cloud: ""{{ tripleo_os_cloud | default('') }}"" tripleo_overcloud_external_update_run_playbook: [] tripleo_overcloud_external_update_run_poll: 10 tripleo_overcloud_external_update_run_rc_file: ""{{ tripleo_rc_file | default(ansible_env.HOME ~ '/stackrc') }}"" tripleo_overcloud_external_update_run_skip_tags: tripleo_overcloud_external_update_run_ssh_user: tripleo_overcloud_external_update_run_stack: tripleo_overcloud_external_update_run_static_inventory: tripleo_overcloud_external_update_run_tags: tripleo_overcloud_external_update_run_timeout: 5700 ",,316,0
openstack%2Ftripleo-operator-ansible~master~I67acaeb91dd476c195ee96ca44267722c45932ae,openstack/tripleo-operator-ansible,master,I67acaeb91dd476c195ee96ca44267722c45932ae,Add overcloud generate fencing,MERGED,2020-03-17 21:08:24.000000000,2020-03-20 22:23:47.000000000,2020-03-20 22:23:47.000000000,"[{'_account_id': 3153}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 21:08:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/72ba60e9e9a64a78f5713d23c1b3d3728bfcdb1e', 'message': 'Add overcloud generate fencing\n\nAdds tripleo_overcloud_generate_fencing to generate fencing parameters\nfor HA.\n\nChange-Id: I67acaeb91dd476c195ee96ca44267722c45932ae\n'}, {'number': 2, 'created': '2020-03-17 21:58:03.000000000', 'files': ['roles/tripleo_overcloud_generate_fencing/tasks/main.yml', 'zuul.d/molecule.yaml', 'roles/tripleo_overcloud_generate_fencing/README.md', 'roles/tripleo_overcloud_generate_fencing/defaults/main.yml', 'roles/tripleo_overcloud_generate_fencing/tests/test.yml', 'roles/tripleo_overcloud_generate_fencing/molecule/default/molecule.yml', 'roles/tripleo_overcloud_generate_fencing/molecule/default/prepare.yml', 'roles/tripleo_overcloud_generate_fencing/tests/inventory', 'roles/tripleo_overcloud_generate_fencing/molecule/default/converge.yml', 'roles/tripleo_overcloud_generate_fencing/meta/main.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/7efc2d5c393ac8350b2dc2c4bb8be707348ba67d', 'message': 'Add overcloud generate fencing\n\nAdds tripleo_overcloud_generate_fencing to generate fencing parameters\nfor HA.\n\nChange-Id: I67acaeb91dd476c195ee96ca44267722c45932ae\n'}]",0,713539,7efc2d5c393ac8350b2dc2c4bb8be707348ba67d,9,2,2,14985,,,0,"Add overcloud generate fencing

Adds tripleo_overcloud_generate_fencing to generate fencing parameters
for HA.

Change-Id: I67acaeb91dd476c195ee96ca44267722c45932ae
",git fetch https://review.opendev.org/openstack/tripleo-operator-ansible refs/changes/39/713539/1 && git format-patch -1 --stdout FETCH_HEAD,"['roles/tripleo_overcloud_generate_fencing/tasks/main.yml', 'zuul.d/molecule.yaml', 'roles/tripleo_overcloud_generate_fencing/README.md', 'roles/tripleo_overcloud_generate_fencing/defaults/main.yml', 'roles/tripleo_overcloud_generate_fencing/tests/test.yml', 'roles/tripleo_overcloud_generate_fencing/molecule/default/molecule.yml', 'roles/tripleo_overcloud_generate_fencing/molecule/default/prepare.yml', 'roles/tripleo_overcloud_generate_fencing/tests/inventory', 'roles/tripleo_overcloud_generate_fencing/meta/main.yml', 'roles/tripleo_overcloud_generate_fencing/molecule/default/converge.yml']",10,72ba60e9e9a64a78f5713d23c1b3d3728bfcdb1e,overcloud-generate-fencing,"--- - name: Converge hosts: all collections: - tripleo.operator vars: openstack_bin: echo tripleo_os_cloud: undercloud tripleo_overcloud_generate_fencing_log_output: false tripleo_overcloud_generate_fencing_poll: 1 tripleo_overcloud_generate_fencing_debug: true tasks: - name: ""Include tripleo_overcloud_generate_fencing"" include_role: name: ""tripleo_overcloud_generate_fencing"" - name: Check role assert: that: - tripleo_overcloud_generate_fencing_output == ""overcloud generate fencing"" - name: Assert ""tripleo_overcloud_generate_fencing_action"" include_role: name: ""tripleo_overcloud_generate_fencing"" vars: tripleo_overcloud_generate_fencing_action: foo - name: Validate tripleo_overcloud_generate_fencing_action assert: that: - tripleo_overcloud_generate_fencing_output == ""overcloud generate fencing --action foo"" - name: Assert ""tripleo_overcloud_generate_fencing_delay"" include_role: name: ""tripleo_overcloud_generate_fencing"" vars: tripleo_overcloud_generate_fencing_delay: 10 - name: Validate tripleo_overcloud_generate_fencing_delay assert: that: - tripleo_overcloud_generate_fencing_output == ""overcloud generate fencing --delay 10"" - name: Assert ""tripleo_overcloud_generate_fencing_ipmi_lanplus"" include_role: name: ""tripleo_overcloud_generate_fencing"" vars: tripleo_overcloud_generate_fencing_ipmi_lanplus: true - name: Validate tripleo_overcloud_generate_fencing_ipmi_lanplus assert: that: - tripleo_overcloud_generate_fencing_output == ""overcloud generate fencing --ipmi-lanplus"" - name: Assert ""tripleo_overcloud_generate_fencing_ipmi_no_lanplus"" include_role: name: ""tripleo_overcloud_generate_fencing"" vars: tripleo_overcloud_generate_fencing_ipmi_no_lanplus: true - name: Validate tripleo_overcloud_generate_fencing_ipmi_no_lanplus assert: that: - tripleo_overcloud_generate_fencing_output == ""overcloud generate fencing --ipmi-no-lanplus"" - name: Assert ""tripleo_overcloud_generate_fencing_ipmi_cipher"" include_role: name: ""tripleo_overcloud_generate_fencing"" vars: tripleo_overcloud_generate_fencing_ipmi_cipher: foo - name: Validate tripleo_overcloud_generate_fencing_ipmi_cipher assert: that: - tripleo_overcloud_generate_fencing_output == ""overcloud generate fencing --ipmi-cipher foo"" - name: Assert ""tripleo_overcloud_generate_fencing_ipmi_level"" include_role: name: ""tripleo_overcloud_generate_fencing"" vars: tripleo_overcloud_generate_fencing_ipmi_level: foo - name: Validate tripleo_overcloud_generate_fencing_ipmi_level assert: that: - tripleo_overcloud_generate_fencing_output == ""overcloud generate fencing --ipmi-level foo"" - name: Assert ""tripleo_overcloud_generate_fencing_output_file"" include_role: name: ""tripleo_overcloud_generate_fencing"" vars: tripleo_overcloud_generate_fencing_output_file: foo.yaml - name: Validate tripleo_overcloud_generate_fencing_output_file assert: that: - tripleo_overcloud_generate_fencing_output == ""overcloud generate fencing --output foo.yaml"" - name: Assert ""tripleo_overcloud_generate_fencing_environment_file"" include_role: name: ""tripleo_overcloud_generate_fencing"" vars: tripleo_overcloud_generate_fencing_environment_file: foo.yaml - name: Validate tripleo_overcloud_generate_fencing_environment_file assert: that: - tripleo_overcloud_generate_fencing_output == ""overcloud generate fencing foo.yaml"" ",,348,0
openstack%2Ftripleo-operator-ansible~master~Iff5ce1730e0d1133573a9892076a49c8d3ef0f00,openstack/tripleo-operator-ansible,master,Iff5ce1730e0d1133573a9892076a49c8d3ef0f00,Quote values for shell_arg_list,MERGED,2020-03-17 23:21:14.000000000,2020-03-20 22:19:46.000000000,2020-03-20 22:19:45.000000000,"[{'_account_id': 3153}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 23:21:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/5a2d9ace08a9ea80191d09ad0518be7b04a07121', 'message': 'Quote values for shell_arg_list\n\nWe should properly shell quote the values provided to shell_arg_list to\nensure they are handled as expected.\n\nChange-Id: Iff5ce1730e0d1133573a9892076a49c8d3ef0f00\n'}, {'number': 2, 'created': '2020-03-17 23:23:21.000000000', 'files': ['plugins/filter/shell_args.py', 'tests/plugins/filter/test_shell_args.py'], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/6f3e6e9fbe51ba1a4a5f1d8f0c7713814c93ab83', 'message': 'Quote values for shell_arg_list\n\nWe should properly shell quote the values provided to shell_arg_list to\nensure they are handled as expected.\n\nChange-Id: Iff5ce1730e0d1133573a9892076a49c8d3ef0f00\n'}]",0,713553,6f3e6e9fbe51ba1a4a5f1d8f0c7713814c93ab83,8,2,2,14985,,,0,"Quote values for shell_arg_list

We should properly shell quote the values provided to shell_arg_list to
ensure they are handled as expected.

Change-Id: Iff5ce1730e0d1133573a9892076a49c8d3ef0f00
",git fetch https://review.opendev.org/openstack/tripleo-operator-ansible refs/changes/53/713553/1 && git format-patch -1 --stdout FETCH_HEAD,"['plugins/filter/shell_args.py', 'tests/plugins/filter/test_shell_args.py']",2,5a2d9ace08a9ea80191d09ad0518be7b04a07121,quote-shell-list," def test_shell_arg_quote(self): arg = [""a b""] expected = ""--p 'a b'"" self.assertEqual(expected, self.filter.shell_arg_list(arg, parameter='--p')) ",,13,1
openstack%2Ftripleo-operator-ansible~master~I08f26759a25fe0adfc1a1a0ec5f44daaf139e24a,openstack/tripleo-operator-ansible,master,I08f26759a25fe0adfc1a1a0ec5f44daaf139e24a,Add overcloud upgrade run,MERGED,2020-03-17 20:08:04.000000000,2020-03-20 22:19:46.000000000,2020-03-20 22:19:46.000000000,"[{'_account_id': 3153}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 20:08:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/dbff78f65b9e4633d81c5afaf03cfa17ac4f8bac', 'message': 'Add overcloud upgrade run\n\nAdds tripleo_overcloud_upgrade_run to handle running the upgrade run\nprocess.\n\nChange-Id: I08f26759a25fe0adfc1a1a0ec5f44daaf139e24a\n'}, {'number': 2, 'created': '2020-03-17 20:21:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/50e4e9f4c6828209c5dea5eb0217c6d5e8aa3e9a', 'message': 'Add overcloud upgrade run\n\nAdds tripleo_overcloud_upgrade_run to handle running the upgrade run\nprocess.\n\nChange-Id: I08f26759a25fe0adfc1a1a0ec5f44daaf139e24a\n'}, {'number': 3, 'created': '2020-03-17 21:58:49.000000000', 'files': ['zuul.d/molecule.yaml', 'roles/tripleo_overcloud_upgrade_run/molecule/default/converge.yml', 'roles/tripleo_overcloud_upgrade_run/molecule/default/prepare.yml', 'roles/tripleo_overcloud_upgrade_run/README.md', 'roles/tripleo_overcloud_upgrade_run/defaults/main.yml', 'roles/tripleo_overcloud_upgrade_run/tests/inventory', 'roles/tripleo_overcloud_upgrade_run/molecule/default/molecule.yml', 'roles/tripleo_overcloud_upgrade_run/meta/main.yml', 'roles/tripleo_overcloud_upgrade_run/tests/test.yml', 'roles/tripleo_overcloud_upgrade_run/tasks/main.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/ae37b35252a5de3489e52d7e1c53ae9676b0f3d6', 'message': 'Add overcloud upgrade run\n\nAdds tripleo_overcloud_upgrade_run to handle running the upgrade run\nprocess.\n\nChange-Id: I08f26759a25fe0adfc1a1a0ec5f44daaf139e24a\n'}]",0,713525,ae37b35252a5de3489e52d7e1c53ae9676b0f3d6,11,2,3,14985,,,0,"Add overcloud upgrade run

Adds tripleo_overcloud_upgrade_run to handle running the upgrade run
process.

Change-Id: I08f26759a25fe0adfc1a1a0ec5f44daaf139e24a
",git fetch https://review.opendev.org/openstack/tripleo-operator-ansible refs/changes/25/713525/3 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/molecule.yaml', 'roles/tripleo_overcloud_upgrade_run/molecule/default/converge.yml', 'roles/tripleo_overcloud_upgrade_run/molecule/default/prepare.yml', 'roles/tripleo_overcloud_upgrade_run/README.md', 'roles/tripleo_overcloud_upgrade_run/defaults/main.yml', 'roles/tripleo_overcloud_upgrade_run/tests/inventory', 'roles/tripleo_overcloud_upgrade_run/molecule/default/molecule.yml', 'roles/tripleo_overcloud_upgrade_run/meta/main.yml', 'roles/tripleo_overcloud_upgrade_run/tests/test.yml', 'roles/tripleo_overcloud_upgrade_run/tasks/main.yml']",10,dbff78f65b9e4633d81c5afaf03cfa17ac4f8bac,overcloud-upgrade-run,"--- # tasks file for tripleo_overcloud_upgrade_run - name: Setup overcloud upgrade run facts set_fact: _run_cmd: >- {{ tripleo_overcloud_upgrade_run_os_cloud | ternary('', ""source "" ~ tripleo_overcloud_upgrade_run_rc_file ~ ""; "") }} {{ openstack_bin }} overcloud upgrade run {{ tripleo_overcloud_upgrade_run_limit | ternary('--limit $UPGRADE_LIMIT', '') }} {{ tripleo_overcloud_upgrade_run_playbook | tripleo.operator.shell_arg_list('--playbook') }} {{ tripleo_overcloud_upgrade_run_ssh_user | ternary('--ssh-user $UPGRADE_SSH_USER', '') }} {{ tripleo_overcloud_upgrade_run_static_inventory | ternary('--static-inventory $UPGRADE_STATIC_INVENTORY', '') }} {{ tripleo_overcloud_upgrade_run_tags | ternary('--tags $UPGRADE_TAGS', '') }} {{ tripleo_overcloud_upgrade_run_skip_tags | ternary('--skip-tags $UPGRADE_SKIP_TAGS', '') }} {{ tripleo_overcloud_upgrade_run_stack | ternary('--stack $UPGRADE_STACK', '') }} {{ tripleo_overcloud_upgrade_run_log_output | ternary(("">"" ~ tripleo_overcloud_upgrade_run_log), '') }} {{ tripleo_overcloud_upgrade_run_log_combine | ternary(""2>&1"", '') }} _run_env: UPGRADE_LIMIT: ""{{ tripleo_overcloud_upgrade_run_limit | quote }}"" UPGRADE_PLAYBOOK: ""{{ tripleo_overcloud_upgrade_run_playbook| list | join(' ') }}"" UPGRADE_SSH_USER: ""{{ tripleo_overcloud_upgrade_run_ssh_user }}"" UPGRADE_STATIC_INVENTORY: ""{{ tripleo_overcloud_upgrade_run_static_inventory }}"" UPGRADE_TAGS: ""{{ tripleo_overcloud_upgrade_run_tags }}"" UPGRADE_SKIP_TAGS: ""{{ tripleo_overcloud_upgrade_run_skip_tags}}"" UPGRADE_STACK: ""{{ tripleo_overcloud_upgrade_run_stack }}"" OS_CLOUD: ""{{ tripleo_overcloud_upgrade_run_os_cloud }}"" - name: Preserve existing log file if exists timestamp_file: path: ""{{ tripleo_overcloud_upgrade_run_log }}"" when: tripleo_overcloud_upgrade_run_log_output|bool - name: Show debug information when: tripleo_overcloud_upgrade_run_debug|bool block: - name: Show the overcloud upgrade run command debug: var: _run_cmd - name: Show the overcloud upgrade run environment debug: var: _run_env - name: Overcloud upgrade run shell: ""{{ _run_cmd }}"" # noqa 305 environment: ""{{ _run_env }}"" args: chdir: ""{{ tripleo_overcloud_upgrade_run_home_dir }}"" warn: false register: tripleo_overcloud_upgrade_run_result async: ""{{ tripleo_overcloud_upgrade_run_timeout }}"" poll: ""{{ tripleo_overcloud_upgrade_run_poll }}"" changed_when: false when: not ansible_check_mode|bool - name: Set output fact set_fact: tripleo_overcloud_upgrade_run_output: ""{{ tripleo_overcloud_upgrade_run_result.stdout }}"" ",,319,0
openstack%2Ftripleo-operator-ansible~master~Ibd71e23e9ee1f7bfe0589a21979de63d94ce056f,openstack/tripleo-operator-ansible,master,Ibd71e23e9ee1f7bfe0589a21979de63d94ce056f,Add overcloud raid create,MERGED,2020-03-17 23:04:26.000000000,2020-03-20 22:18:07.000000000,2020-03-20 22:18:07.000000000,"[{'_account_id': 3153}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 23:04:26.000000000', 'files': ['roles/tripleo_overcloud_raid_create/tests/test.yml', 'zuul.d/molecule.yaml', 'roles/tripleo_overcloud_raid_create/defaults/main.yml', 'roles/tripleo_overcloud_raid_create/molecule/default/prepare.yml', 'roles/tripleo_overcloud_raid_create/README.md', 'roles/tripleo_overcloud_raid_create/tests/inventory', 'roles/tripleo_overcloud_raid_create/tasks/main.yml', 'roles/tripleo_overcloud_raid_create/molecule/default/converge.yml', 'roles/tripleo_overcloud_raid_create/meta/main.yml', 'roles/tripleo_overcloud_raid_create/molecule/default/molecule.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/7df7c520d8e22410cf9eb0ff5fed576b2bba2e85', 'message': 'Add overcloud raid create\n\nAdds tripleo_overcloud_raid_create to handle raid creation on nodes.\n\nChange-Id: Ibd71e23e9ee1f7bfe0589a21979de63d94ce056f\n'}]",0,713552,7df7c520d8e22410cf9eb0ff5fed576b2bba2e85,7,2,1,14985,,,0,"Add overcloud raid create

Adds tripleo_overcloud_raid_create to handle raid creation on nodes.

Change-Id: Ibd71e23e9ee1f7bfe0589a21979de63d94ce056f
",git fetch https://review.opendev.org/openstack/tripleo-operator-ansible refs/changes/52/713552/1 && git format-patch -1 --stdout FETCH_HEAD,"['roles/tripleo_overcloud_raid_create/tests/test.yml', 'zuul.d/molecule.yaml', 'roles/tripleo_overcloud_raid_create/defaults/main.yml', 'roles/tripleo_overcloud_raid_create/molecule/default/prepare.yml', 'roles/tripleo_overcloud_raid_create/README.md', 'roles/tripleo_overcloud_raid_create/tasks/main.yml', 'roles/tripleo_overcloud_raid_create/tests/inventory', 'roles/tripleo_overcloud_raid_create/meta/main.yml', 'roles/tripleo_overcloud_raid_create/molecule/default/converge.yml', 'roles/tripleo_overcloud_raid_create/molecule/default/molecule.yml']",10,7df7c520d8e22410cf9eb0ff5fed576b2bba2e85,overcloud-raid,--- driver: name: delegated options: managed: false ansible_connection_options: ansible_connection: local log: true platforms: - name: instance provisioner: name: ansible scenario: name: default test_sequence: - prepare - syntax - converge - verify ,,258,0
openstack%2Fironic~master~I47f301cb4cdc8de158fa52aad35b7136d1356b93,openstack/ironic,master,I47f301cb4cdc8de158fa52aad35b7136d1356b93,Clean up nits from adding additional node update policies,MERGED,2020-03-17 14:45:53.000000000,2020-03-20 22:13:12.000000000,2020-03-20 22:09:12.000000000,"[{'_account_id': 7386}, {'_account_id': 10239}, {'_account_id': 11655}, {'_account_id': 14629}, {'_account_id': 19339}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-03-17 14:45:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/421b46585ff4cf9cbb4f0bf85312ae47f36ceaaf', 'message': 'Clean up nits from adding additional node update policies\n\nThis patch resolves some minor nits.\n\nChange-Id: I47f301cb4cdc8de158fa52aad35b7136d1356b93\n'}, {'number': 2, 'created': '2020-03-17 15:12:10.000000000', 'files': ['releasenotes/notes/node-update-instance-info-extra-policies-862b2a70b941cf39.yaml', 'ironic/tests/unit/api/controllers/v1/test_node.py', 'ironic/api/controllers/v1/node.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/6e9901583697e4684478e1dba680738a58cc30c1', 'message': 'Clean up nits from adding additional node update policies\n\nThis patch resolves some minor nits.\n\nChange-Id: I47f301cb4cdc8de158fa52aad35b7136d1356b93\n'}]",4,713462,6e9901583697e4684478e1dba680738a58cc30c1,27,7,2,7386,,,0,"Clean up nits from adding additional node update policies

This patch resolves some minor nits.

Change-Id: I47f301cb4cdc8de158fa52aad35b7136d1356b93
",git fetch https://review.opendev.org/openstack/ironic refs/changes/62/713462/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/node-update-instance-info-extra-policies-862b2a70b941cf39.yaml', 'ironic/tests/unit/api/controllers/v1/test_node.py', 'ironic/api/controllers/v1/node.py']",3,421b46585ff4cf9cbb4f0bf85312ae47f36ceaaf,node-policy-update, if generic_update or not policy_checks:, if generic_update or policy_checks == []:,6,5
openstack%2Ftripleo-ansible~master~Id7bd3e751f9530f0c7e5df8152eea9a6bcf49b8d,openstack/tripleo-ansible,master,Id7bd3e751f9530f0c7e5df8152eea9a6bcf49b8d,DNM - CI test,ABANDONED,2020-03-20 19:34:02.000000000,2020-03-20 22:04:38.000000000,,[{'_account_id': 23181}],"[{'number': 1, 'created': '2020-03-20 19:34:02.000000000', 'files': ['tripleo_ansible/roles/tripleo_container_manage/tasks/podman/create.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/de7beea1014d144d493c118d3fbef2f8d7d29af6', 'message': 'DNM - CI test\n\nChange-Id: Id7bd3e751f9530f0c7e5df8152eea9a6bcf49b8d\n'}]",0,714201,de7beea1014d144d493c118d3fbef2f8d7d29af6,3,1,1,3153,,,0,"DNM - CI test

Change-Id: Id7bd3e751f9530f0c7e5df8152eea9a6bcf49b8d
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/01/714201/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_ansible/roles/tripleo_container_manage/tasks/podman/create.yml'],1,de7beea1014d144d493c118d3fbef2f8d7d29af6,ci," async: ""{{ (not ansible_check_mode | bool) | ternary('100', omit) }}"" retries: 1"," no_log: ""{{ not tripleo_container_manage_debug }}"" async: ""{{ (not ansible_check_mode | bool) | ternary('600', omit) }}"" no_log: ""{{ not tripleo_container_manage_debug }}"" retries: 60",2,4
openstack%2Foctavia~master~I611d9cd5a519abc6a330ec2c46a2f5a1fdb89313,openstack/octavia,master,I611d9cd5a519abc6a330ec2c46a2f5a1fdb89313,Add a periodic image build job,MERGED,2020-02-06 21:52:25.000000000,2020-03-20 22:01:21.000000000,2020-03-20 21:58:10.000000000,"[{'_account_id': 1131}, {'_account_id': 6469}, {'_account_id': 10273}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-02-06 21:52:25.000000000', 'files': ['zuul.d/projects.yaml', 'zuul.d/jobs.yaml'], 'web_link': 'https://opendev.org/openstack/octavia/commit/1f9915037ad68356231691607b461c30b28df3b7', 'message': 'Add a periodic image build job\n\nThis patch adds a periodic job to build the amphora image using the\nreleased version of diskimage-builder (our other jobs use Git master).\nThis job will not publish the produced image, it is only to test the\nbuild.\n\nChange-Id: I611d9cd5a519abc6a330ec2c46a2f5a1fdb89313\n'}]",0,706393,1f9915037ad68356231691607b461c30b28df3b7,10,4,1,11628,,,0,"Add a periodic image build job

This patch adds a periodic job to build the amphora image using the
released version of diskimage-builder (our other jobs use Git master).
This job will not publish the produced image, it is only to test the
build.

Change-Id: I611d9cd5a519abc6a330ec2c46a2f5a1fdb89313
",git fetch https://review.opendev.org/openstack/octavia refs/changes/93/706393/1 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/projects.yaml', 'zuul.d/jobs.yaml']",2,1f9915037ad68356231691607b461c30b28df3b7,," - job: name: octavia-amphora-image-build parent: base description: | Builds the amphora image using the released diskimage-builder version, not Git master. This job does not publish the image. run: playbooks/image-build/run.yaml required-projects: - openstack/octavia - openstack/octavia-lib vars: amphora_os: ubuntu amphora_os_release: bionic",,15,0
openstack%2Foctavia~stable%2Frocky~I3c6459becc415d6dc0792c44ca75e717b239cd92,openstack/octavia,stable/rocky,I3c6459becc415d6dc0792c44ca75e717b239cd92,"Remove the barbican ""Grant access"" from cookbook",MERGED,2020-03-17 20:56:36.000000000,2020-03-20 21:44:47.000000000,2020-03-20 21:43:04.000000000,"[{'_account_id': 6469}, {'_account_id': 10273}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 20:56:36.000000000', 'files': ['doc/source/user/guides/basic-cookbook.rst'], 'web_link': 'https://opendev.org/openstack/octavia/commit/3b213f38a0704b327841d42bd480bb99300736f3', 'message': 'Remove the barbican ""Grant access"" from cookbook\n\nWe missed a line when removing the requirement to grant Octavia\naccess to the secret in barbican.\nThis patch corrects that oversight.\n\nChange-Id: I3c6459becc415d6dc0792c44ca75e717b239cd92\n'}]",0,713537,3b213f38a0704b327841d42bd480bb99300736f3,8,3,1,11628,,,0,"Remove the barbican ""Grant access"" from cookbook

We missed a line when removing the requirement to grant Octavia
access to the secret in barbican.
This patch corrects that oversight.

Change-Id: I3c6459becc415d6dc0792c44ca75e717b239cd92
",git fetch https://review.opendev.org/openstack/octavia refs/changes/37/713537/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/user/guides/basic-cookbook.rst'],1,3b213f38a0704b327841d42bd480bb99300736f3,,3. Create load balancer *lb1* on subnet *public-subnet*. 4. Create listener *listener1* as a TERMINATED_HTTPS listener referencing5. Create pool *pool1* as *listener1*'s default pool. 6. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*.3. Create load balancer *lb1* on subnet *public-subnet*. 4. Create listener *listener1* as a TERMINATED_HTTPS listener referencing5. Create pool *pool1* as *listener1*'s default pool. 6. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*.3. Create load balancer *lb1* on subnet *public-subnet*. 4. Create listener *listener1* as a TERMINATED_HTTPS listener referencing5. Create pool *pool1* as *listener1*'s default pool. 6. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*. 7. Create listener *listener2* as an HTTP listener with *pool1* as its,3. Grant the *admin* user access to the *tls_secret1* barbican resource. 4. Create load balancer *lb1* on subnet *public-subnet*. 5. Create listener *listener1* as a TERMINATED_HTTPS listener referencing6. Create pool *pool1* as *listener1*'s default pool. 7. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*.3. Grant the *admin* user access to both *tls_secret* barbican resources. 4. Create load balancer *lb1* on subnet *public-subnet*. 5. Create listener *listener1* as a TERMINATED_HTTPS listener referencing6. Create pool *pool1* as *listener1*'s default pool. 7. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*.3. Grant the *admin* user access to the *tls_secret1* barbican resource. 4. Create load balancer *lb1* on subnet *public-subnet*. 5. Create listener *listener1* as a TERMINATED_HTTPS listener referencing6. Create pool *pool1* as *listener1*'s default pool. 7. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*. 8. Create listener *listener2* as an HTTP listener with *pool1* as its,13,16
openstack%2Foctavia~stable%2Ftrain~I3c6459becc415d6dc0792c44ca75e717b239cd92,openstack/octavia,stable/train,I3c6459becc415d6dc0792c44ca75e717b239cd92,"Remove the barbican ""Grant access"" from cookbook",MERGED,2020-03-17 20:53:56.000000000,2020-03-20 21:44:45.000000000,2020-03-20 21:43:02.000000000,"[{'_account_id': 6469}, {'_account_id': 10273}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 20:53:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/770e7d1e198cfb69efe35d69ad96e18830e445ee', 'message': 'Remove the barbican ""Grant access"" from cookbook\n\nWe missed a line when removing the requirement to grant Octavia\naccess to the secret in barbican.\nThis patch corrects that oversight.\n\nChange-Id: I3c6459becc415d6dc0792c44ca75e717b239cd92\n'}, {'number': 2, 'created': '2020-03-17 20:55:40.000000000', 'files': ['doc/source/user/guides/basic-cookbook.rst'], 'web_link': 'https://opendev.org/openstack/octavia/commit/cb55a8cf78605c3d0b9c483b922f0156a6b3286f', 'message': 'Remove the barbican ""Grant access"" from cookbook\n\nWe missed a line when removing the requirement to grant Octavia\naccess to the secret in barbican.\nThis patch corrects that oversight.\n\nChange-Id: I3c6459becc415d6dc0792c44ca75e717b239cd92\n'}]",0,713535,cb55a8cf78605c3d0b9c483b922f0156a6b3286f,10,3,2,11628,,,0,"Remove the barbican ""Grant access"" from cookbook

We missed a line when removing the requirement to grant Octavia
access to the secret in barbican.
This patch corrects that oversight.

Change-Id: I3c6459becc415d6dc0792c44ca75e717b239cd92
",git fetch https://review.opendev.org/openstack/octavia refs/changes/35/713535/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/user/guides/basic-cookbook.rst'],1,770e7d1e198cfb69efe35d69ad96e18830e445ee,,3. Create load balancer *lb1* on subnet *public-subnet*. 4. Create listener *listener1* as a TERMINATED_HTTPS listener referencing5. Create pool *pool1* as *listener1*'s default pool. 6. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*. 7. Create listener *listener2* as an HTTP listener with *pool1* as its,3. Grant the *admin* user access to the *tls_secret1* barbican resource. 4. Create load balancer *lb1* on subnet *public-subnet*. 5. Create listener *listener1* as a TERMINATED_HTTPS listener referencing6. Create pool *pool1* as *listener1*'s default pool. 7. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*. 8. Create listener *listener2* as an HTTP listener with *pool1* as its,5,6
openstack%2Foctavia~master~I10db6226750f7b7c703067d2ab82eea3a9875112,openstack/octavia,master,I10db6226750f7b7c703067d2ab82eea3a9875112,Add oslo middleware healthcheck to Octavia API,MERGED,2020-03-03 23:39:34.000000000,2020-03-20 21:44:45.000000000,2020-03-20 21:43:00.000000000,"[{'_account_id': 3031}, {'_account_id': 6469}, {'_account_id': 10273}, {'_account_id': 11628}, {'_account_id': 22348}, {'_account_id': 28691}]","[{'number': 1, 'created': '2020-03-03 23:39:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/1333f550a2662a8706542d5f5d5914e8eb7f5fcf', 'message': 'Add oslomiddleware healthcheck middleware to wsgi pipeline\n\nhealthcheck middleware adds a /healthcheck url that allows\nunauthenticated access to provide a simple check when running\noctavia-api behind a load balancer\n\nhttps://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html\n\nChange-Id: I10db6226750f7b7c703067d2ab82eea3a9875112\n'}, {'number': 2, 'created': '2020-03-14 02:09:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/9a1220e09a4c4d82c79f41db2cdc17b913671fbe', 'message': 'Add oslo middleware healthcheck to Octavia API\n\nhealthcheck middleware adds a /healthcheck url that allows\nunauthenticated access to provide a simple check when running\noctavia-api behind a load balancer\n\nhttps://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html\n\nChange-Id: I10db6226750f7b7c703067d2ab82eea3a9875112\n'}, {'number': 3, 'created': '2020-03-14 02:25:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b578686abe00113ba0286ef40e658dffc139e006', 'message': 'Add oslo middleware healthcheck to Octavia API\n\nhealthcheck middleware adds a /healthcheck url that allows\nunauthenticated access to provide a simple check when running\noctavia-api behind a load balancer\n\nhttps://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html\n\nCo-authored-by: Michael Johnson <johnsomor@gmail.com>\nChange-Id: I10db6226750f7b7c703067d2ab82eea3a9875112\n'}, {'number': 4, 'created': '2020-03-14 06:30:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/05d3af8669391b907129bb1ca11861ff67279b22', 'message': 'Add oslo middleware healthcheck to Octavia API\n\nhealthcheck middleware adds a /healthcheck url that allows\nunauthenticated access to provide a simple check when running\noctavia-api behind a load balancer\n\nhttps://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html\n\nCo-authored-by: Michael Johnson <johnsomor@gmail.com>\nChange-Id: I10db6226750f7b7c703067d2ab82eea3a9875112\n'}, {'number': 5, 'created': '2020-03-16 06:48:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cfd99680f40b463391f3d693fbef8140433bfc4c', 'message': 'Add oslo middleware healthcheck to Octavia API\n\nhealthcheck middleware adds a /healthcheck url that allows\nunauthenticated access to provide a simple check when running\noctavia-api behind a load balancer\n\nhttps://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html\n\nCo-authored-by: Michael Johnson <johnsomor@gmail.com>\nChange-Id: I10db6226750f7b7c703067d2ab82eea3a9875112\n'}, {'number': 6, 'created': '2020-03-17 01:12:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/ee4d6918a6e093704f0b705d107fea2d9867eb51', 'message': 'Add oslo middleware healthcheck to Octavia API\n\nhealthcheck middleware adds a /healthcheck url that allows\nunauthenticated access to provide a simple check when running\noctavia-api behind a load balancer\n\nhttps://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html\n\nCo-authored-by: Michael Johnson <johnsomor@gmail.com>\nChange-Id: I10db6226750f7b7c703067d2ab82eea3a9875112\n'}, {'number': 7, 'created': '2020-03-17 17:23:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d3666ff33cc24c49b3d392509a88d6f896149e8f', 'message': 'Add oslo middleware healthcheck to Octavia API\n\nhealthcheck middleware adds a /healthcheck url that allows\nunauthenticated access to provide a simple check when running\noctavia-api behind a load balancer\n\nhttps://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html\n\nCo-authored-by: Michael Johnson <johnsomor@gmail.com>\nChange-Id: I10db6226750f7b7c703067d2ab82eea3a9875112\n'}, {'number': 8, 'created': '2020-03-17 17:54:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/e3a04efcc7d28152fe831aeff138d70d7bae21c7', 'message': 'Add oslo middleware healthcheck to Octavia API\n\nhealthcheck middleware adds a /healthcheck url that allows\nunauthenticated access to provide a simple check when running\noctavia-api behind a load balancer\n\nhttps://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html\n\nCo-authored-by: Michael Johnson <johnsomor@gmail.com>\nChange-Id: I10db6226750f7b7c703067d2ab82eea3a9875112\n'}, {'number': 9, 'created': '2020-03-17 18:07:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/4442eb371d35825f961d71220bcc9ed2c1985423', 'message': 'Add oslo middleware healthcheck to Octavia API\n\nhealthcheck middleware adds a /healthcheck url that allows\nunauthenticated access to provide a simple check when running\noctavia-api behind a load balancer\n\nhttps://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html\n\nCo-authored-by: Michael Johnson <johnsomor@gmail.com>\nChange-Id: I10db6226750f7b7c703067d2ab82eea3a9875112\n'}, {'number': 10, 'created': '2020-03-17 18:08:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/8bc60f20acd485fb289bc0eb30da3a5656058bc3', 'message': 'Add oslo middleware healthcheck to Octavia API\n\nhealthcheck middleware adds a /healthcheck url that allows\nunauthenticated access to provide a simple check when running\noctavia-api behind a load balancer\n\nhttps://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html\n\nCo-authored-by: Michael Johnson <johnsomor@gmail.com>\nChange-Id: I10db6226750f7b7c703067d2ab82eea3a9875112\n'}, {'number': 11, 'created': '2020-03-19 18:24:17.000000000', 'files': ['octavia/common/config.py', 'doc/source/admin/index.rst', 'octavia/api/v2/controllers/availability_zones.py', 'octavia/api/v2/controllers/load_balancer.py', 'octavia/tests/functional/api/test_healthcheck.py', 'etc/octavia.conf', 'requirements.txt', 'octavia/common/keystone.py', 'octavia/api/app.py', 'octavia/api/v2/controllers/flavors.py', 'doc/source/admin/healthcheck.rst', 'octavia/api/v2/controllers/base.py', 'octavia/api/v2/controllers/provider.py', 'octavia/api/v2/controllers/quotas.py', 'lower-constraints.txt', 'octavia/api/v2/controllers/availability_zone_profiles.py', 'octavia/api/v2/controllers/health_monitor.py', 'octavia/db/healthcheck.py', 'octavia/api/v2/controllers/pool.py', 'releasenotes/notes/add-healthcheck-middleware-6c09150bddd3113f.yaml', 'octavia/api/healthcheck/healthcheck_plugins.py', 'octavia/api/root_controller.py', 'octavia/api/v2/controllers/l7policy.py', 'octavia/api/v2/controllers/l7rule.py', 'octavia/api/v2/controllers/member.py', 'octavia/api/v2/controllers/amphora.py', 'setup.cfg', 'octavia/api/v2/controllers/listener.py', 'octavia/api/v2/controllers/flavor_profiles.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/18020e6c88bc1be11c7f19ec2a74a79a09bc2b88', 'message': 'Add oslo middleware healthcheck to Octavia API\n\nhealthcheck middleware adds a /healthcheck url that allows\nunauthenticated access to provide a simple check when running\noctavia-api behind a load balancer\n\nhttps://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html\n\nCo-authored-by: Michael Johnson <johnsomor@gmail.com>\nChange-Id: I10db6226750f7b7c703067d2ab82eea3a9875112\n'}]",3,711127,18020e6c88bc1be11c7f19ec2a74a79a09bc2b88,48,6,11,3031,,,0,"Add oslo middleware healthcheck to Octavia API

healthcheck middleware adds a /healthcheck url that allows
unauthenticated access to provide a simple check when running
octavia-api behind a load balancer

https://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html

Co-authored-by: Michael Johnson <johnsomor@gmail.com>
Change-Id: I10db6226750f7b7c703067d2ab82eea3a9875112
",git fetch https://review.opendev.org/openstack/octavia refs/changes/27/711127/8 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/common/keystone.py', 'releasenotes/notes/add-healthcheck-middleware-6c09150bddd3113f.yaml', 'octavia/api/app.py']",3,1333f550a2662a8706542d5f5d5914e8eb7f5fcf,healthcheck,from oslo_middleware import healthcheck app = healthcheck.Healthcheck(app),,9,1
openstack%2Foctavia~stable%2Fstein~I3c6459becc415d6dc0792c44ca75e717b239cd92,openstack/octavia,stable/stein,I3c6459becc415d6dc0792c44ca75e717b239cd92,"Remove the barbican ""Grant access"" from cookbook",MERGED,2020-03-17 20:56:14.000000000,2020-03-20 21:44:38.000000000,2020-03-20 21:43:03.000000000,"[{'_account_id': 6469}, {'_account_id': 10273}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 20:56:14.000000000', 'files': ['doc/source/user/guides/basic-cookbook.rst'], 'web_link': 'https://opendev.org/openstack/octavia/commit/e3a95c9b0f1f59cea52097f03bbd8ab1a20e89c6', 'message': 'Remove the barbican ""Grant access"" from cookbook\n\nWe missed a line when removing the requirement to grant Octavia\naccess to the secret in barbican.\nThis patch corrects that oversight.\n\nChange-Id: I3c6459becc415d6dc0792c44ca75e717b239cd92\n'}]",0,713536,e3a95c9b0f1f59cea52097f03bbd8ab1a20e89c6,9,3,1,11628,,,0,"Remove the barbican ""Grant access"" from cookbook

We missed a line when removing the requirement to grant Octavia
access to the secret in barbican.
This patch corrects that oversight.

Change-Id: I3c6459becc415d6dc0792c44ca75e717b239cd92
",git fetch https://review.opendev.org/openstack/octavia refs/changes/36/713536/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/user/guides/basic-cookbook.rst'],1,e3a95c9b0f1f59cea52097f03bbd8ab1a20e89c6,,3. Create load balancer *lb1* on subnet *public-subnet*. 4. Create listener *listener1* as a TERMINATED_HTTPS listener referencing5. Create pool *pool1* as *listener1*'s default pool. 6. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*.3. Create load balancer *lb1* on subnet *public-subnet*. 4. Create listener *listener1* as a TERMINATED_HTTPS listener referencing5. Create pool *pool1* as *listener1*'s default pool. 6. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*.3. Create load balancer *lb1* on subnet *public-subnet*. 4. Create listener *listener1* as a TERMINATED_HTTPS listener referencing5. Create pool *pool1* as *listener1*'s default pool. 6. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*. 7. Create listener *listener2* as an HTTP listener with *pool1* as its,3. Grant the *admin* user access to the *tls_secret1* barbican resource. 4. Create load balancer *lb1* on subnet *public-subnet*. 5. Create listener *listener1* as a TERMINATED_HTTPS listener referencing6. Create pool *pool1* as *listener1*'s default pool. 7. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*.3. Grant the *admin* user access to both *tls_secret* barbican resources. 4. Create load balancer *lb1* on subnet *public-subnet*. 5. Create listener *listener1* as a TERMINATED_HTTPS listener referencing6. Create pool *pool1* as *listener1*'s default pool. 7. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*.3. Grant the *admin* user access to the *tls_secret1* barbican resource. 4. Create load balancer *lb1* on subnet *public-subnet*. 5. Create listener *listener1* as a TERMINATED_HTTPS listener referencing6. Create pool *pool1* as *listener1*'s default pool. 7. Add members 192.0.2.10 and 192.0.2.11 on *private-subnet* to *pool1*. 8. Create listener *listener2* as an HTTP listener with *pool1* as its,13,16
openstack%2Fnova~master~I0ef84dd0a6c3f1788caf7d3a8e3837203f6d5429,openstack/nova,master,I0ef84dd0a6c3f1788caf7d3a8e3837203f6d5429,virt: Pass request context to extend_volume,MERGED,2020-02-10 16:29:30.000000000,2020-03-20 20:43:46.000000000,2020-03-19 16:48:38.000000000,"[{'_account_id': 9008}, {'_account_id': 9708}, {'_account_id': 10118}, {'_account_id': 10135}, {'_account_id': 11604}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-02-10 16:29:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/194feb533ba12dc9d2bab7c7443c76c6111fe2f7', 'message': 'virt: Pass request context to extend_volume\n\nChange-Id: I0ef84dd0a6c3f1788caf7d3a8e3837203f6d5429\n'}, {'number': 2, 'created': '2020-02-11 18:11:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/73b6daa92206f5797e1973ba37bf6202b3122331', 'message': ""virt: Pass request context to extend_volume\n\nRequired by an upcoming bugfix to the Libvirt driver's implementation of\nextend_volume that will require external authenticated calls to Cinder\nand Barbican.\n\nChange-Id: I0ef84dd0a6c3f1788caf7d3a8e3837203f6d5429\n""}, {'number': 3, 'created': '2020-02-19 15:53:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/58d9917c151be86ba169f039588c9f8e603c4736', 'message': ""virt: Pass request context to extend_volume\n\nRequired by an upcoming bugfix to the Libvirt driver's implementation of\nextend_volume that will require external authenticated calls to Cinder\nand Barbican.\n\nChange-Id: I0ef84dd0a6c3f1788caf7d3a8e3837203f6d5429\n""}, {'number': 4, 'created': '2020-03-02 10:05:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a65df10d9c308f368d1df96d011b3563d43a4187', 'message': ""virt: Pass request context to extend_volume\n\nRequired by an upcoming bugfix to the Libvirt driver's implementation of\nextend_volume that will require external authenticated calls to Cinder\nand Barbican.\n\nChange-Id: I0ef84dd0a6c3f1788caf7d3a8e3837203f6d5429\n""}, {'number': 5, 'created': '2020-03-02 14:46:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/def99116c028fc0ff499e18e56e7d0909f0ad498', 'message': ""virt: Pass request context to extend_volume\n\nRequired by an upcoming bugfix to the Libvirt driver's implementation of\nextend_volume that will require external authenticated calls to Cinder\nand Barbican.\n\nChange-Id: I0ef84dd0a6c3f1788caf7d3a8e3837203f6d5429\n""}, {'number': 6, 'created': '2020-03-03 09:09:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/83b71449a0038f2bb8ec5c3a9f5722bebd931124', 'message': ""virt: Pass request context to extend_volume\n\nRequired by an upcoming bugfix to the Libvirt driver's implementation of\nextend_volume that will require external authenticated calls to Cinder\nand Barbican.\n\nChange-Id: I0ef84dd0a6c3f1788caf7d3a8e3837203f6d5429\n""}, {'number': 7, 'created': '2020-03-09 19:19:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/59d32360db66c68ace3b40519805ec840f41c7dc', 'message': ""virt: Pass request context to extend_volume\n\nRequired by an upcoming bugfix to the Libvirt driver's implementation of\nextend_volume that will require external authenticated calls to Cinder\nand Barbican.\n\nChange-Id: I0ef84dd0a6c3f1788caf7d3a8e3837203f6d5429\n""}, {'number': 8, 'created': '2020-03-11 10:52:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a7f413f798d6d63084620fad197084e79767e0c0', 'message': ""virt: Pass request context to extend_volume\n\nRequired by an upcoming bugfix to the Libvirt driver's implementation of\nextend_volume that will require external authenticated calls to Cinder\nand Barbican.\n\nChange-Id: I0ef84dd0a6c3f1788caf7d3a8e3837203f6d5429\n""}, {'number': 9, 'created': '2020-03-16 09:45:47.000000000', 'files': ['nova/tests/unit/compute/test_compute_mgr.py', 'nova/virt/libvirt/driver.py', 'nova/virt/fake.py', 'nova/tests/unit/virt/powervm/test_driver.py', 'nova/virt/driver.py', 'nova/compute/manager.py', 'nova/tests/unit/virt/libvirt/test_driver.py', 'nova/virt/powervm/driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/7f12c785bc284f4436ee8c32c4c0987e836ae9ed', 'message': ""virt: Pass request context to extend_volume\n\nRequired by an upcoming bugfix to the Libvirt driver's implementation of\nextend_volume that will require external authenticated calls to Cinder\nand Barbican.\n\nChange-Id: I0ef84dd0a6c3f1788caf7d3a8e3837203f6d5429\n""}]",2,706899,7f12c785bc284f4436ee8c32c4c0987e836ae9ed,121,13,9,10135,,,0,"virt: Pass request context to extend_volume

Required by an upcoming bugfix to the Libvirt driver's implementation of
extend_volume that will require external authenticated calls to Cinder
and Barbican.

Change-Id: I0ef84dd0a6c3f1788caf7d3a8e3837203f6d5429
",git fetch https://review.opendev.org/openstack/nova refs/changes/99/706899/8 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/libvirt/driver.py', 'nova/tests/unit/virt/powervm/test_driver.py', 'nova/virt/driver.py', 'nova/compute/manager.py', 'nova/tests/unit/virt/libvirt/test_driver.py', 'nova/virt/powervm/driver.py']",6,194feb533ba12dc9d2bab7c7443c76c6111fe2f7,bug/1861071," def extend_volume(self, context, connection_info, instance, requested_size): :param context: security context"," def extend_volume(self, connection_info, instance, requested_size):",20,12
openstack%2Fopenstack-helm-images~master~I7bb994800b3409ecdce5a964dae88658813a5a80,openstack/openstack-helm-images,master,I7bb994800b3409ecdce5a964dae88658813a5a80,Change kubectl binary to kubernetes library,ABANDONED,2020-03-20 20:36:22.000000000,2020-03-20 20:37:12.000000000,,[],"[{'number': 1, 'created': '2020-03-20 20:36:22.000000000', 'files': ['nagios/plugins/pvc_pv_rbd_monitoring.py'], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/7b5418f9a7d2815f4d0769578fabc12c0ed51621', 'message': 'Change kubectl binary to kubernetes library\n\nChange-Id: I9f6ed6f286a50a70380625411b8447709bc64d42\n\nchange python2.7 to python3\n\nChange-Id: I7bb994800b3409ecdce5a964dae88658813a5a80\n'}]",0,714220,7b5418f9a7d2815f4d0769578fabc12c0ed51621,2,0,1,10108,,,0,"Change kubectl binary to kubernetes library

Change-Id: I9f6ed6f286a50a70380625411b8447709bc64d42

change python2.7 to python3

Change-Id: I7bb994800b3409ecdce5a964dae88658813a5a80
",git fetch https://review.opendev.org/openstack/openstack-helm-images refs/changes/20/714220/1 && git format-patch -1 --stdout FETCH_HEAD,['nagios/plugins/pvc_pv_rbd_monitoring.py'],1,7b5418f9a7d2815f4d0769578fabc12c0ed51621,,"import astfrom kubernetes import client from kubernetes import config from kubernetes.client.rest import ApiException from kubernetes.stream import stream def get_pod_list(api_instance): """""" get list of all pods in all namespaces """""" # Configs can be set in Configuration class directly or using helper utility logger.debug(""Listing pods:"") try: pod_list = api_instance.list_pod_for_all_namespaces(pretty='false', timeout_seconds=60) except ApiException as e: logger.error(""Exception when calling CoreV1Api->list_pods: %s\n"" % e) return pod_list.items def get_name_and_namespace(resource_list): logger.debug(""Getting resource names in all namespaces"") for item in resource_list: podnames.append({""name"":item.metadata.name, ""namespace"":item.metadata.namespace}) except ApiException as e: print(""Exception when calling CoreV1Api->list_node: %s\n"" % e)def exec_command(api_instance, name, namespace, exec_command): """""" execute command inside pod """""" resp = None try: resp = api_instance.read_namespaced_pod(name=name, namespace=namespace) except ApiException as e: if e.status != 404: logger.error(""Unknown error: %s"" % e) exit(1) # Calling exec and waiting for response resp = stream(api_instance.connect_get_namespaced_pod_exec, name, namespace, command=exec_command, stderr=True, stdin=False, stdout=True, tty=False) return resp def get_pvc_list(api_instance): logger.debug(""Listing pvc:"") ret = api_instance.list_persistent_volume_claim_for_all_namespaces() return ret.items def get_pv_list(api_instance): logger.debug(""Listing pv:"") ret = api_instance.list_persistent_volume() return ret.items def get_list_namespaces(api_instance): # Configs can be set in Configuration class directly or using helper utility ret = api_instance.list_namespace() return ret.items def get_rbd_list(api_instance): """""" return list of rbd in json format """""" logger.debug(""Getting list of rbd"") ceph_mon = api_instance.list_pod_for_all_namespaces(label_selector=""application=ceph,component=mon,release_group=clcp-ucp-ceph-mon"", limit=1) ceph_pod = get_name_and_namespace(ceph_mon.items) command = [ ""rbd"", ""ls"", ""--format"", ""json""] rbd_list_str = exec_command(api_instance, ceph_pod[0]['name'], ceph_pod[0]['namespace'], command) rbd_list = ast.literal_eval(rbd_list_str) # rbd = rbd_list_str[1:-1].split(',') # rbd_list = [] # for i in rbd: # rbd_list.append(i[2:-1].replace(""'"", '')) return rbd_list def check_rbd_status(api_instance, rbd): ceph_mon = api_instance.list_pod_for_all_namespaces(label_selector=""application=ceph,component=mon,release_group=clcp-ucp-ceph-mon"", limit=1) ceph_pod = get_name_and_namespace(ceph_mon.items) command = [""rbd"", ""status"", ""--format"", ""json"", ""{rbd}"".format(rbd=rbd)] rbd_status_str = exec_command(api_instance, ceph_pod[0]['name'], ceph_pod[0]['namespace'], command) status = ast.literal_eval(rbd_status_str) rbd_status = json.dumps(status) return rbd_status def monitoring_pvc(api_instance): logger.debug(""PVCs aren't associated with RBD"") rbds = get_rbd_list(api_instance) pvs = get_pv_list(api_instance) return_code = 0 for pv in pvs: rbd = pv.spec.rbd.image if rbd not in rbds: print (""pvc_doesnot_have_rbd:{{namespace={},name={}}} 0"".format(ns, pv['metadata']['name'])) return_code = 1def monitoring_rbd(api_instance): logger.debug(""RBD volumes aren't associated with PVC"") r = get_rbd_list(api_instance) pvs = get_pv_list(api_instance) return_code = 0 for pv in pvs: rbd = pv.spec.rbd.image rbds.append(rbd) logger.debug(rbd) logger.debug(""rbd {i} not in pv list"".format(i=i)) print (""rbd_doesnot_have_pvc:{{name={}}} {}"".format(i, len(json.loads(check_rbd_status(api_instance,i))['watchers']))) return_code = 1def monitoring_pv(api_instance): logger.debug(""PVs aren't associated with PVC "") pvs = get_pv_list(api_instance) return_code = 0 for pv in pvs: if pv.status.phase == ""Released"": print (""pv_released:{{name={},status={}}} 0"".format(pv.metadatai.name, pv.status.phase)) return_code = 1 def monitoring(api_instance): return_code = monitoring_rbd(api_instance) return_code = return_code | monitoring_pvc(api_instance) return_code = return_code | monitoring_pv(api_instance) return return_code config.load_kube_config() kube_api = client.CoreV1Api() logger.debug(""all"") sys.exit(monitoring(kube_api)) if options.rbd: logger.debug(""rbd"") sys.exit(monitoring_rbd(kube_api)) if options.pvc: logger.debug(""pvc"") sys.exit(monitoring_pvc(kube_api)) if options.pv: logger.debug(""pv"") sys.exit(monitoring_pv(kube_api))"," # need to export path to configuration file. should work faster if os.environ[""KUBECONFIG""] !="""": CONFIG = ""KUBECONFIG={}"".format(os.environ[""KUBECONFIG""]) else: CONFIG = ""KUBECONFIG=/etc/kubernetes/admin/kubeconfig.yaml"" if os.environ[""KUBECTL""] != """": KUBECTL = os.environ[""KUBECTL""] else: KUBECTL = ""/usr/local/bin/kubectl"" OPTIONS = { 'read': ""-o json"", 'ns': ""-n"", 'exec': """" }kubectl = ""{config} {kubectl} {options} {namespace}"".format(config=CONFIG, kubectl=KUBECTL, namespace=OPTIONS[""ns""], options=OPTIONS[""read""]) kubectl_exec = ""{config} {kubectl} {options} {namespace}"".format(config=CONFIG, kubectl=KUBECTL, namespace=OPTIONS[""ns""], options=OPTIONS[""exec""]) OK = 0 # 0 - Service is OK WARNING = 1 # 1 - Service has a WARNING. CRITICAL = 2 # 2 - Service is in a CRITICAL status. UNKNOWN = 3 # 3 - Service status is UNKNOWN # configuration options g.add_argument(""--bin"", ""-e"", action=""store_const"", const=KUBECTL, help=""path to kubecl binary"") g.add_argument(""--config"", ""-c"", action=""store_const"", const=CONFIG, help=""path to kubecl config"") def get_podname(label, namespace): logger.debug(""Getting pod names with labels {label} in namespcae {namespace}"".format(label=label, namespace=namespace)) command = ""{command} {namespace} get pods -l {label}"".format(command=kubectl, namespace=namespace, label=label) logger.debug(command) try: output = run_command(command) except Exception as e: logger.error(""Getting pod names for labels {label} in namespace {namespace}"".format(label=label, namespace=namespace)) sys.exit(1) out = json.loads(output) for item in out[""items""]: if item['kind'] == ""Pod"": podnames.append(item['metadata']['name']) except Exception as e: sys.exit(1) def run_command(command): """""" run command in linux shell and return a result """""" try: result = subprocess.check_output(command, shell=True).strip().decode('utf-8') except Exception as e: logger.error(e) sys.exit(1) return result def check_rbd_status(namespace, rbd): ceph_mon = get_podname(""application=ceph,component=mon"", namespace)[0] command = ""{command} {namespace} exec -it {pod} -- rbd status --format json {rbd}"".format(command=kubectl_exec, namespace=namespace, rbd=rbd, pod=ceph_mon) out = run_command(command) try: output = json.loads(out) except Exception as e: output = ""{{'rbd': {}, 'status': {} }}"".format(rbd, e) return output def get_pvc(namespace): """""" return list of permanent volume claim in specific namespace """""" command = ""{command} {namespace} get pvc"".format(command=kubectl, namespace=namespace) out = run_command(command) try: output = json.loads(out) except Exception as e: # output = {'namespace': namespace, 'status': e} output = [] return output def get_pv(persistentvolume, namespace): """""" return list of persistent volumes in specific namespace """""" command = ""{command} {namespace} get pv {pv}"".format(command=kubectl, namespace=namespace, pv=persistentvolume) out = run_command(command) try: output = json.loads(out) except Exception as e: output = {'persistentvolume': namespace, 'status': e} return output def get_namespaces(): """""" return list of namespaces """""" command = ""{command} {namespace} get ns -o json"".format(command=kubectl_exec, namespace=""ceph"") out = run_command(command) try: output = json.loads(out) except Exception as e: output = {""Error"": e} return output def get_rbd_list(namespace): """""" return list of rbd in json format """""" ceph_mon = get_podname(""application=ceph,component=mon"", namespace)[0] command = ""{command} {namespace} exec -it {pod} -- rbd ls --format json"".format(command=kubectl_exec, namespace=namespace, pod=ceph_mon) out = run_command(command) logger.debug(out) try: output = json.loads(out) except Exception as e: output = ""{{'namaspace': {}, 'status': {} }}"".format(namespace, e) logger.error(""parse json rbd list output: {}"".format(e)) return output def monitoring_pvc(): logger.info(""PVCs aren't associated with RBD"") rbds = get_rbd_list(""ceph"") logger.info(""Gettign list of namespaces"") namespaces = get_namespaces() return_code = OK for namespace in namespaces['items']: ns = namespace['metadata']['name'] logger.info(""Checking namespace: {}"".format(ns)) pvc = get_pvc(ns) for pv in pvc['items']: rbd = get_pv(pv['spec']['volumeName'], ns) rbd = rbd['spec']['rbd']['image'] if rbd not in rbds: return_code = WARNING print (""pvc_doesnot_have_rbd:{{namespace={},name={}}} 0"".format(ns, pv['metadata']['name']))def monitoring_rbd(): logger.info(""RBD volumes aren't associated with PVC"") r = get_rbd_list(""ceph"") namespaces = get_namespaces() return_code = OK for namespace in namespaces['items']: ns = namespace['metadata']['name'] pvc = get_pvc(ns) for pv in pvc['items']: rbd = get_pv(pv['spec']['volumeName'], ns) rbd = rbd['spec']['rbd']['image'] rbds.append(rbd) logger.info(rbd) return_code = WARNING logger.debug(i) print (""rbd_doesnot_have_pvc:{{name={}}} {}"".format(i, len(check_rbd_status(""ceph"", i)['watchers'])))def monitoring_pv(): logger.info(""PVs aren't associated with PVC "") command = ""{config} {kubectl} get pv -o json"".format(config=CONFIG, kubectl=KUBECTL) logger.debug(command) out = run_command(command) return_code = OK logger.debug(out) try: output = json.loads(out) if len(output['items']) == 0: return_code = OK else: return_code = WARNING for pv in output['items']: if pv['status']['phase'] == ""Released"": print (""pv_released:{{name={},status={}}} 0"".format(pv['metadata']['name'], pv['status']['phase'])) except Exception as e: logger.error(""gettign rbd list: {}"".format(e)) return_code = UNKNOWNdef monitoring(): return_code = [] return_code.append(monitoring_rbd()) return_code.append(monitoring_pvc()) return_code.append(monitoring_pv()) return max(return_code) return_code = OK logger.info(""all"") return_code = monitoring() if options.rbd: logger.info(""rbd"") return_code = monitoring_rbd() if options.pvc: logger.info(""pvc"") return_code = monitoring_pvc() if options.pv: logger.info(""pv"") return_code = monitoring_pv() sys.exit(return_code) ",125,206
openstack%2Fkolla-ansible~master~Ie3f61e2e186c8e77e21a7b53d2bd7d2a27eee18e,openstack/kolla-ansible,master,Ie3f61e2e186c8e77e21a7b53d2bd7d2a27eee18e,Support disabling Prometheus server,MERGED,2020-03-20 17:18:51.000000000,2020-03-20 20:30:46.000000000,2020-03-20 20:28:08.000000000,"[{'_account_id': 22348}, {'_account_id': 24072}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-20 17:18:51.000000000', 'files': ['releasenotes/notes/support-disabling-prometheus-server-907be6846bc364df.yaml', 'ansible/group_vars/all.yml', 'ansible/roles/prometheus/defaults/main.yml', 'ansible/roles/prometheus/tasks/precheck.yml', 'etc/kolla/globals.yml'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/505cded29b4f399a2dd9494409548a6fb7e4ccb8', 'message': 'Support disabling Prometheus server\n\nThis is useful to people who manage their Prometheus Server\nexternally to Kolla Ansible, or want to use the exporters with\nanother framework such as Monasca.\n\nChange-Id: Ie3f61e2e186c8e77e21a7b53d2bd7d2a27eee18e\n'}]",0,714180,505cded29b4f399a2dd9494409548a6fb7e4ccb8,9,3,1,17669,,,0,"Support disabling Prometheus server

This is useful to people who manage their Prometheus Server
externally to Kolla Ansible, or want to use the exporters with
another framework such as Monasca.

Change-Id: Ie3f61e2e186c8e77e21a7b53d2bd7d2a27eee18e
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/80/714180/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/support-disabling-prometheus-server-907be6846bc364df.yaml', 'ansible/group_vars/all.yml', 'ansible/roles/prometheus/defaults/main.yml', 'ansible/roles/prometheus/tasks/precheck.yml', 'etc/kolla/globals.yml']",5,505cded29b4f399a2dd9494409548a6fb7e4ccb8,,"#enable_prometheus_server: ""{{ enable_prometheus | bool }}""",,11,2
openstack%2Fkolla-ansible~stable%2Fstein~Id65ada7cd6766d3a907a5a1da54978b56319979c,openstack/kolla-ansible,stable/stein,Id65ada7cd6766d3a907a5a1da54978b56319979c,Ironic: fix documentation,MERGED,2020-03-19 08:39:24.000000000,2020-03-20 20:24:50.000000000,2020-03-20 20:22:19.000000000,"[{'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-19 08:39:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/b7333390b27d6c9663bc1625c1c765976855972b', 'message': 'Ironic: fix documentation\n\nBackport: train\n\n* Ironic dropped CoreOS IPA images in Train - use CentOS DIB images\n* Nova flavor requires dropping standard resources\n* Link to sections in ironic docs\n\nPartial-Bug: #1862628\nChange-Id: Id65ada7cd6766d3a907a5a1da54978b56319979c\n(cherry picked from commit a8529db5fc131ffdb961a2bf113f6c7f1c3be978)\n(cherry picked from commit e00053fee54251c6c74fce55583d59314e92db2e)\n'}, {'number': 2, 'created': '2020-03-19 09:31:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/ed4e512155382cf40a87948fbf22bfe4f5945bb5', 'message': 'Ironic: fix documentation\n\nBackport: train\n\n* Ironic dropped CoreOS IPA images in Train - use CentOS DIB images\n* Nova flavor requires dropping standard resources\n* Link to sections in ironic docs\n\nPartial-Bug: #1862628\nChange-Id: Id65ada7cd6766d3a907a5a1da54978b56319979c\n(cherry picked from commit a8529db5fc131ffdb961a2bf113f6c7f1c3be978)\n(cherry picked from commit e00053fee54251c6c74fce55583d59314e92db2e)\n'}, {'number': 3, 'created': '2020-03-19 09:37:08.000000000', 'files': ['doc/source/reference/bare-metal/ironic-guide.rst'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/89e875fb6764f068766f0c5600765e220dc812bf', 'message': 'Ironic: fix documentation\n\nBackport: train\n\n* Nova flavor requires dropping standard resources\n* Link to sections in ironic docs\n\nAdapted for Stein to ignore CoreOS->CentOS change.\n\nPartial-Bug: #1862628\nChange-Id: Id65ada7cd6766d3a907a5a1da54978b56319979c\n(cherry picked from commit a8529db5fc131ffdb961a2bf113f6c7f1c3be978)\n(cherry picked from commit e00053fee54251c6c74fce55583d59314e92db2e)\n'}]",0,713794,89e875fb6764f068766f0c5600765e220dc812bf,12,3,3,30491,,,0,"Ironic: fix documentation

Backport: train

* Nova flavor requires dropping standard resources
* Link to sections in ironic docs

Adapted for Stein to ignore CoreOS->CentOS change.

Partial-Bug: #1862628
Change-Id: Id65ada7cd6766d3a907a5a1da54978b56319979c
(cherry picked from commit a8529db5fc131ffdb961a2bf113f6c7f1c3be978)
(cherry picked from commit e00053fee54251c6c74fce55583d59314e92db2e)
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/94/713794/2 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/reference/bare-metal/ironic-guide.rst'],1,b7333390b27d6c9663bc1625c1c765976855972b,bug/1862628,"Ironic works well in Kolla, though it is not thoroughly tested as part of Kolla $ curl https://tarballs.openstack.org/ironic-python-agent/dib/files/ipa-centos7-master.kernel \ $ curl https://tarballs.openstack.org/ironic-python-agent/dib/files/ipa-centos7-master.initramfs \The :ironic-doc:`Ironic documentation <install/configure-glance-images>` describes how to create the deploy kernel and ramdisk and register them with Glance. In this example we're reusing the same images that were fetched for the Inspector:The :ironic-doc:`Ironic documentation <install/configure-nova-flavors>` describes how to create Nova flavors for bare metal. For example: resources:CUSTOM_BAREMETAL_RESOURCE_CLASS=1 \ resources:resources:VCPU=0 \ resources:resources:MEMORY_MB=0 \ resources:resources:DISK_GB=0 The :ironic-doc:`Ironic documentation <install/enrollment>` describes how to enroll baremetal nodes and ports. In the following example ensure to substitute correct values for the kernel, ramdisk, and MAC address for your baremetal node.","Ironic works well in Kolla, though it is not currently tested as part of Kolla $ curl https://tarballs.openstack.org/ironic-python-agent/coreos/files/coreos_production_pxe.vmlinuz \ $ curl https://tarballs.openstack.org/ironic-python-agent/coreos/files/coreos_production_pxe_image-oem.cpio.gz \Add the deploy kernel and ramdisk to Glance. Here we're reusing the same images that were fetched for the Inspector:Create a baremetal flavor: resources:CUSTOM_BAREMETAL_RESOURCE_CLASS=1 Create the baremetal node and associate a port. (Ensure to substitute correct values for the kernel, ramdisk, and MAC address for your baremetal node)",17,9
openstack%2Frpm-packaging~stable%2Fstein~I6dee3791b4e697eb0a576025b13e39795284f77d,openstack/rpm-packaging,stable/stein,I6dee3791b4e697eb0a576025b13e39795284f77d,python-keystoneclient: update to version 3.19.1,MERGED,2020-03-20 10:48:00.000000000,2020-03-20 20:22:16.000000000,2020-03-20 20:22:16.000000000,"[{'_account_id': 7102}, {'_account_id': 8482}, {'_account_id': 13294}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-20 10:48:00.000000000', 'files': ['openstack/python-keystoneclient/python-keystoneclient.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/0b263375e3021b3c1f9851a2171ae19af67f3b0e', 'message': 'python-keystoneclient: update to version 3.19.1\n\nDepends-On: https://review.opendev.org/713938\nChange-Id: I6dee3791b4e697eb0a576025b13e39795284f77d\n'}]",0,714071,0b263375e3021b3c1f9851a2171ae19af67f3b0e,13,6,1,13294,,,0,"python-keystoneclient: update to version 3.19.1

Depends-On: https://review.opendev.org/713938
Change-Id: I6dee3791b4e697eb0a576025b13e39795284f77d
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/71/714071/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/python-keystoneclient/python-keystoneclient.spec.j2'],1,0b263375e3021b3c1f9851a2171ae19af67f3b0e,,{% set upstream_version = upstream_version('3.19.1') %},{% set upstream_version = upstream_version('3.19.0') %},1,1
openstack%2Foctavia-tempest-plugin~master~I0fc3f5e3a48dc9dc0286cf9b11847a77573ac411,openstack/octavia-tempest-plugin,master,I0fc3f5e3a48dc9dc0286cf9b11847a77573ac411,Use same flake8 extensions as Octavia,MERGED,2020-03-20 15:16:54.000000000,2020-03-20 20:07:44.000000000,2020-03-20 20:06:12.000000000,"[{'_account_id': 6469}, {'_account_id': 11628}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 15:16:54.000000000', 'files': ['octavia_tempest_plugin/services/load_balancer/v2/availability_zone_profile_client.py', 'octavia_tempest_plugin/tests/scenario/v2/test_pool.py', 'octavia_tempest_plugin/tests/act_stdby_scenario/v2/test_active_standby_iptables.py', 'octavia_tempest_plugin/tests/scenario/v2/test_ipv6_traffic_ops.py', 'octavia_tempest_plugin/services/load_balancer/v2/flavor_profile_client.py', 'tox.ini', 'octavia_tempest_plugin/tests/barbican_scenario/v2/test_tls_barbican.py'], 'web_link': 'https://opendev.org/openstack/octavia-tempest-plugin/commit/0ae7f108b91eea0a363236cb12bcb03c098d321d', 'message': 'Use same flake8 extensions as Octavia\n\nEnabled the same flake8 extensions as the Octavia tree,\nfixing the bugs that they now caught.\n\nTrivialfix\n\nChange-Id: I0fc3f5e3a48dc9dc0286cf9b11847a77573ac411\n'}]",0,714142,0ae7f108b91eea0a363236cb12bcb03c098d321d,8,3,1,1131,,,0,"Use same flake8 extensions as Octavia

Enabled the same flake8 extensions as the Octavia tree,
fixing the bugs that they now caught.

Trivialfix

Change-Id: I0fc3f5e3a48dc9dc0286cf9b11847a77573ac411
",git fetch https://review.opendev.org/openstack/octavia-tempest-plugin refs/changes/42/714142/1 && git format-patch -1 --stdout FETCH_HEAD,"['octavia_tempest_plugin/services/load_balancer/v2/availability_zone_profile_client.py', 'octavia_tempest_plugin/tests/act_stdby_scenario/v2/test_active_standby_iptables.py', 'octavia_tempest_plugin/tests/scenario/v2/test_ipv6_traffic_ops.py', 'octavia_tempest_plugin/tests/scenario/v2/test_pool.py', 'octavia_tempest_plugin/services/load_balancer/v2/flavor_profile_client.py', 'tox.ini', 'octavia_tempest_plugin/tests/barbican_scenario/v2/test_tls_barbican.py']",7,0ae7f108b91eea0a363236cb12bcb03c098d321d,flake8," LOG.debug('CA Cert: %s', cls.ca_cert.public_bytes( LOG.debug('CA private Key: %s', ca_key.private_bytes( LOG.debug('CA public Key: %s', ca_key.public_key().public_bytes( LOG.debug('Server (default) UUID: %s', cls.server_uuid) LOG.debug('SNI1 UUID: %s', cls.SNI1_uuid) LOG.debug('SNI2 UUID: %s', cls.SNI2_uuid)", LOG.debug('CA Cert: %s' % cls.ca_cert.public_bytes( LOG.debug('CA private Key: %s' % ca_key.private_bytes( LOG.debug('CA public Key: %s' % ca_key.public_key().public_bytes( LOG.debug('Server (default) UUID: %s' % cls.server_uuid) LOG.debug('SNI1 UUID: %s' % cls.SNI1_uuid) LOG.debug('SNI2 UUID: %s' % cls.SNI2_uuid),22,17
openstack%2Fopenstack-zuul-jobs~master~I1a6e183d1bc9349cb400a087d3640a185b76b950,openstack/openstack-zuul-jobs,master,I1a6e183d1bc9349cb400a087d3640a185b76b950,Don't run infra-puppet-apply on docker and ansible,MERGED,2020-03-20 19:35:59.000000000,2020-03-20 19:57:37.000000000,2020-03-20 19:54:24.000000000,"[{'_account_id': 1}, {'_account_id': 5263}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 19:35:59.000000000', 'files': ['zuul.d/jobs.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-zuul-jobs/commit/b66c46434f4190ccc483b8f4d1357239386d9eb2', 'message': ""Don't run infra-puppet-apply on docker and ansible\n\nThe apply test does not test anything related to docker or ansible.\nStop running it on docker and ansible related changes.\n\nChange-Id: I1a6e183d1bc9349cb400a087d3640a185b76b950\n""}]",0,714202,b66c46434f4190ccc483b8f4d1357239386d9eb2,8,3,1,2,,,0,"Don't run infra-puppet-apply on docker and ansible

The apply test does not test anything related to docker or ansible.
Stop running it on docker and ansible related changes.

Change-Id: I1a6e183d1bc9349cb400a087d3640a185b76b950
",git fetch https://review.opendev.org/openstack/openstack-zuul-jobs refs/changes/02/714202/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/jobs.yaml'],1,b66c46434f4190ccc483b8f4d1357239386d9eb2,, irrelevant-files: - docker/.* - playbooks/.*,,3,0
openstack%2Frpm-packaging~stable%2Frocky~I052b3ce7d87901591b7ea8979e556e630a2eb52c,openstack/rpm-packaging,stable/rocky,I052b3ce7d87901591b7ea8979e556e630a2eb52c,Update to Rocky EM releases (part III),MERGED,2020-03-19 10:57:51.000000000,2020-03-20 19:56:11.000000000,2020-03-20 19:56:11.000000000,"[{'_account_id': 7102}, {'_account_id': 8482}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 10:57:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/33b98358f3dddd1a1284d96b8ae86a185845ae28', 'message': 'Update to Rocky EM releases (part III)\n\nChange-Id: I052b3ce7d87901591b7ea8979e556e630a2eb52c\n'}, {'number': 2, 'created': '2020-03-20 17:09:33.000000000', 'files': ['openstack/python-cinderclient/python-cinderclient.spec.j2', 'openstack/python-novaclient/python-novaclient.spec.j2', 'openstack/python-watcherclient/python-watcherclient.spec.j2', 'openstack/python-octaviaclient/python-octaviaclient.spec.j2', 'openstack/python-swiftclient/python-swiftclient.spec.j2', 'openstack/python-openstackclient/python-openstackclient.spec.j2', 'openstack/python-ironicclient/python-ironicclient.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/854f7bc247e6a9fcd47e3297a36c961730ff9049', 'message': 'Update to Rocky EM releases (part III)\n\nChange-Id: I052b3ce7d87901591b7ea8979e556e630a2eb52c\n'}]",0,713835,854f7bc247e6a9fcd47e3297a36c961730ff9049,13,5,2,6593,,,0,"Update to Rocky EM releases (part III)

Change-Id: I052b3ce7d87901591b7ea8979e556e630a2eb52c
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/35/713835/2 && git format-patch -1 --stdout FETCH_HEAD,"['openstack/python-cinderclient/python-cinderclient.spec.j2', 'openstack/python-novaclient/python-novaclient.spec.j2', 'openstack/python-watcherclient/python-watcherclient.spec.j2', 'openstack/python-octaviaclient/python-octaviaclient.spec.j2', 'openstack/python-swiftclient/python-swiftclient.spec.j2', 'openstack/python-openstackclient/python-openstackclient.spec.j2', 'openstack/python-ironicclient/python-ironicclient.spec.j2']",7,33b98358f3dddd1a1284d96b8ae86a185845ae28,,{% set upstream_version = upstream_version('2.5.4') %},{% set upstream_version = upstream_version('2.5.3') %},7,7
openstack%2Freleases~master~I6c1d8412aee4a695187892f8568291c1203c9116,openstack/releases,master,I6c1d8412aee4a695187892f8568291c1203c9116,oslo.policy 3.0.2,MERGED,2020-03-20 19:15:51.000000000,2020-03-20 19:50:48.000000000,2020-03-20 19:50:48.000000000,"[{'_account_id': 5314}, {'_account_id': 11904}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 19:15:51.000000000', 'files': ['deliverables/ussuri/oslo.policy.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/cd5f2011a7646d64e2c03fafce56a0fef934314a', 'message': 'oslo.policy 3.0.2\n\nFixes a bug that is breaking the cinder docs build.\n\nChange-Id: I6c1d8412aee4a695187892f8568291c1203c9116\n'}]",0,714195,cd5f2011a7646d64e2c03fafce56a0fef934314a,8,3,1,6928,,,0,"oslo.policy 3.0.2

Fixes a bug that is breaking the cinder docs build.

Change-Id: I6c1d8412aee4a695187892f8568291c1203c9116
",git fetch https://review.opendev.org/openstack/releases refs/changes/95/714195/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/ussuri/oslo.policy.yaml'],1,cd5f2011a7646d64e2c03fafce56a0fef934314a,oslo-master, - version: 3.0.2 projects: - repo: openstack/oslo.policy hash: a0d99e1046f99607609f5b0e75942f90e85946ad,,4,0
openstack%2Foctavia~master~If460cc248059deae639ac632faa6a410fe9b205d,openstack/octavia,master,If460cc248059deae639ac632faa6a410fe9b205d,"[UDP] Bring back new session_persistence type ""OPS""",NEW,2018-08-08 08:01:10.000000000,2020-03-20 19:49:16.000000000,,"[{'_account_id': 10273}, {'_account_id': 10850}, {'_account_id': 22348}]","[{'number': 1, 'created': '2018-08-08 08:01:10.000000000', 'files': ['api-ref/source/parameters.yaml', 'octavia/tests/functional/api/v2/test_pool.py', 'octavia/tests/unit/common/sample_configs/sample_configs.py', 'octavia/common/constants.py', 'octavia/tests/unit/common/jinja/lvs/test_lvs_jinja_cfg.py', 'octavia/common/jinja/lvs/templates/macros.j2', 'api-ref/source/v2/pool.inc', 'octavia/api/v2/controllers/pool.py', 'releasenotes/notes/Add-UDP-protocol-support-9c011a23525092a1.yaml', 'octavia/db/migration/alembic_migrations/versions/76aacf2e176c_extend_support_udp_protocol.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/8a195503a8b9d492d559d55e721ce275eb1cbaba', 'message': '[UDP] Bring back new session_persistence type ""OPS""\n\nThis patch bring the OPS back for new pool\'s session_persistence type.\nAlso update the api-ref and releasenotes.\n\nChange-Id: If460cc248059deae639ac632faa6a410fe9b205d\n'}]",0,589748,8a195503a8b9d492d559d55e721ce275eb1cbaba,4,3,1,15309,,,0,"[UDP] Bring back new session_persistence type ""OPS""

This patch bring the OPS back for new pool's session_persistence type.
Also update the api-ref and releasenotes.

Change-Id: If460cc248059deae639ac632faa6a410fe9b205d
",git fetch https://review.opendev.org/openstack/octavia refs/changes/48/589748/1 && git format-patch -1 --stdout FETCH_HEAD,"['api-ref/source/parameters.yaml', 'octavia/tests/functional/api/v2/test_pool.py', 'octavia/tests/unit/common/sample_configs/sample_configs.py', 'octavia/common/constants.py', 'octavia/tests/unit/common/jinja/lvs/test_lvs_jinja_cfg.py', 'octavia/common/jinja/lvs/templates/macros.j2', 'api-ref/source/v2/pool.inc', 'octavia/api/v2/controllers/pool.py', 'releasenotes/notes/Add-UDP-protocol-support-9c011a23525092a1.yaml', 'octavia/db/migration/alembic_migrations/versions/76aacf2e176c_extend_support_udp_protocol.py']",10,8a195503a8b9d492d559d55e721ce275eb1cbaba,udp_support,"tables = [u'protocol', u'session_persistence_type', u'health_monitor_type'] new_fields = ['UDP', 'OPS', 'UDP-CONNECT'] # New ONE_PACKET_SCHEDULING session persistence type addition.","tables = [u'protocol', u'health_monitor_type'] new_fields = ['UDP', 'UDP-CONNECT']",186,39
openstack%2Fopenstack-zuul-jobs~master~I460ba0a7e06dd462319792296eb952e31211fd0a,openstack/openstack-zuul-jobs,master,I460ba0a7e06dd462319792296eb952e31211fd0a,Stop running logstash legacy job on ansible and docker,ABANDONED,2020-03-20 19:40:10.000000000,2020-03-20 19:48:20.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2020-03-20 19:40:10.000000000', 'files': ['zuul.d/zuul-legacy-jobs.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-zuul-jobs/commit/13057febbeb3d141c7e64b9992923e656bf8121a', 'message': ""Stop running logstash legacy job on ansible and docker\n\nLike the previous two, we don't need to run logstash filter\ntests when we make changes to docker or ansible files.\n\nChange-Id: I460ba0a7e06dd462319792296eb952e31211fd0a\n""}]",0,714204,13057febbeb3d141c7e64b9992923e656bf8121a,3,1,1,2,,,0,"Stop running logstash legacy job on ansible and docker

Like the previous two, we don't need to run logstash filter
tests when we make changes to docker or ansible files.

Change-Id: I460ba0a7e06dd462319792296eb952e31211fd0a
",git fetch https://review.opendev.org/openstack/openstack-zuul-jobs refs/changes/04/714204/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/zuul-legacy-jobs.yaml'],1,13057febbeb3d141c7e64b9992923e656bf8121a,, irrelevant-files: - ^docker/.*$ - ^playbooks/.*$,,3,0
openstack%2Fopenstack-zuul-jobs~master~I8f3eb52e343dd1e485b46a91d2a785ed17c288cb,openstack/openstack-zuul-jobs,master,I8f3eb52e343dd1e485b46a91d2a785ed17c288cb,Don't run beaker tests on ansible or docker changes,ABANDONED,2020-03-20 19:35:59.000000000,2020-03-20 19:48:10.000000000,,"[{'_account_id': 1}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 19:35:59.000000000', 'files': ['zuul.d/jobs.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-zuul-jobs/commit/5809a0d69e71e71060839cf05e3715e0bd411c6c', 'message': 'Don\'t run beaker tests on ansible or docker changes\n\nThe beaker tests run basic ansible - but just the ""run ansible\nto run puppet"" codepath. If we\'re just running ansible or docker\nthings, these don\'t test anything.\n\nChange-Id: I8f3eb52e343dd1e485b46a91d2a785ed17c288cb\n'}]",1,714203,5809a0d69e71e71060839cf05e3715e0bd411c6c,4,2,1,2,,,0,"Don't run beaker tests on ansible or docker changes

The beaker tests run basic ansible - but just the ""run ansible
to run puppet"" codepath. If we're just running ansible or docker
things, these don't test anything.

Change-Id: I8f3eb52e343dd1e485b46a91d2a785ed17c288cb
",git fetch https://review.opendev.org/openstack/openstack-zuul-jobs refs/changes/03/714203/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/jobs.yaml'],1,5809a0d69e71e71060839cf05e3715e0bd411c6c,, - ^docker/.*$ - ^playbooks/.*$,,2,0
openstack%2Fdevstack-plugin-nfs~master~I510655dabea6886a27bec52e83a3cc86bd7ebd15,openstack/devstack-plugin-nfs,master,I510655dabea6886a27bec52e83a3cc86bd7ebd15,Update .gitreview and .zuul.yaml after rename,MERGED,2020-03-20 15:07:14.000000000,2020-03-20 19:28:26.000000000,2020-03-20 19:28:26.000000000,"[{'_account_id': 6547}, {'_account_id': 8556}, {'_account_id': 9535}, {'_account_id': 10459}, {'_account_id': 11904}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 15:07:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-plugin-nfs/commit/8351afad2788716fc61b73e20fa36d337526b5ab', 'message': 'Update .gitreview after rename\n\nThe repo got moved from x/devstack-plugin-nfs to\nopenstack/devstack-plugin-nfs, update .gitreview for this.\n\nChange-Id: I510655dabea6886a27bec52e83a3cc86bd7ebd15\n'}, {'number': 2, 'created': '2020-03-20 15:40:28.000000000', 'files': ['.gitreview', '.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/devstack-plugin-nfs/commit/78bfd45d63e0a4b4db06c908e0084e583a8f5eb2', 'message': 'Update .gitreview and .zuul.yaml after rename\n\nThe repo got moved from x/devstack-plugin-nfs to\nopenstack/devstack-plugin-nfs, update .gitreview and jobs\nfor this.\n\nChange-Id: I510655dabea6886a27bec52e83a3cc86bd7ebd15\n'}]",0,714139,78bfd45d63e0a4b4db06c908e0084e583a8f5eb2,10,6,2,6547,,,0,"Update .gitreview and .zuul.yaml after rename

The repo got moved from x/devstack-plugin-nfs to
openstack/devstack-plugin-nfs, update .gitreview and jobs
for this.

Change-Id: I510655dabea6886a27bec52e83a3cc86bd7ebd15
",git fetch https://review.opendev.org/openstack/devstack-plugin-nfs refs/changes/39/714139/1 && git format-patch -1 --stdout FETCH_HEAD,['.gitreview'],1,8351afad2788716fc61b73e20fa36d337526b5ab,,project=openstack/devstack-plugin-nfs.git,project=x/devstack-plugin-nfs.git,1,1
openstack%2Fironic~stable%2Fstein~I4f2cecdc0af366364b18232dbd8ea4ffdd3165d2,openstack/ironic,stable/stein,I4f2cecdc0af366364b18232dbd8ea4ffdd3165d2,Don't require root partition when installing a whole disk image,MERGED,2020-02-13 15:59:39.000000000,2020-03-20 19:26:59.000000000,2020-03-20 19:23:10.000000000,"[{'_account_id': 11076}, {'_account_id': 11655}, {'_account_id': 14826}, {'_account_id': 15519}, {'_account_id': 19339}, {'_account_id': 21909}, {'_account_id': 22348}, {'_account_id': 23851}]","[{'number': 1, 'created': '2020-02-13 15:59:39.000000000', 'files': ['ironic/tests/unit/drivers/modules/test_agent_base_vendor.py', 'releasenotes/notes/whole-disk-scsi-install-bootloader-f7e791d82da476ca.yaml', 'ironic/drivers/modules/agent_base_vendor.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/d68ece1f1a450505b3120f9f27e8a207f7f4fc33', 'message': ""Don't require root partition when installing a whole disk image\n\nWith the change to find the bootloader on disk\n(https://review.opendev.org/#/c/696914/)\ninstall_bootloader should now be invoked when doing an iscsi_deploy\neven if the root partition cannot be found.\n\nFixed Conflicts:\n- ironic/drivers/modules/agent_base_vendor.py\n- ironic/tests/unit/drivers/modules/test_agent_base_vendor.py\n\nDepends-On: I7167e71e5d2352a045565289b200e5530d0ba11d\nChange-Id: I4f2cecdc0af366364b18232dbd8ea4ffdd3165d2\n(cherry picked from commit d614d86eef0db7fd9784f4d81a3369841c5a9eb3)\n""}]",0,707676,d68ece1f1a450505b3120f9f27e8a207f7f4fc33,52,8,1,15519,,,0,"Don't require root partition when installing a whole disk image

With the change to find the bootloader on disk
(https://review.opendev.org/#/c/696914/)
install_bootloader should now be invoked when doing an iscsi_deploy
even if the root partition cannot be found.

Fixed Conflicts:
- ironic/drivers/modules/agent_base_vendor.py
- ironic/tests/unit/drivers/modules/test_agent_base_vendor.py

Depends-On: I7167e71e5d2352a045565289b200e5530d0ba11d
Change-Id: I4f2cecdc0af366364b18232dbd8ea4ffdd3165d2
(cherry picked from commit d614d86eef0db7fd9784f4d81a3369841c5a9eb3)
",git fetch https://review.opendev.org/openstack/ironic refs/changes/76/707676/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/tests/unit/drivers/modules/test_agent_base_vendor.py', 'releasenotes/notes/whole-disk-scsi-install-bootloader-f7e791d82da476ca.yaml', 'ironic/drivers/modules/agent_base_vendor.py']",3,d68ece1f1a450505b3120f9f27e8a207f7f4fc33,whole-disk-install-bootloader,"from ironic.drivers.modules import boot_mode_utils # For whole disk images it is not necessary that the root_uuid # be provided since the bootloaders on the disk will be used whole_disk_image = node.driver_internal_info.get('is_whole_disk_image') if ((root_uuid and not whole_disk_image) or (whole_disk_image and boot_mode_utils.get_boot_mode(node) == 'uefi')): if not whole_disk_image: msg = (_(""Failed to install a bootloader when "" ""deploying node %(node)s. Error: %(error)s"") % {'node': node.uuid, 'error': result['command_error']}) log_and_raise_deployment_error(task, msg) else: # Its possible the install will fail if the IPA image # has not been updated, log this and continue LOG.info('Could not install bootloader for whole disk ' 'image for node %(node)s, Error: %(error)s""', {'node': node.uuid, 'error': result['command_error']}) return"," if not node.driver_internal_info.get( 'is_whole_disk_image') and root_uuid: msg = (_(""Failed to install a bootloader when "" ""deploying node %(node)s. Error: %(error)s"") % {'node': node.uuid, 'error': result['command_error']}) log_and_raise_deployment_error(task, msg)",53,7
openstack%2Fopenstack-helm-infra~master~Ic39413f39c86a9c637f707540a1c9410ca1c6efb,openstack/openstack-helm-infra,master,Ic39413f39c86a9c637f707540a1c9410ca1c6efb,Enable MariaDB Ingress image version overwrite,ABANDONED,2019-08-07 18:59:36.000000000,2020-03-20 19:25:45.000000000,,"[{'_account_id': 18236}, {'_account_id': 20466}, {'_account_id': 22348}, {'_account_id': 22636}, {'_account_id': 23928}, {'_account_id': 24780}, {'_account_id': 28372}, {'_account_id': 28618}]","[{'number': 1, 'created': '2019-08-07 18:59:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/c67d4c41ed4bdb8779e86e93bbe5da37db94dd74', 'message': 'Add ClusterRole for mariadb ingress controller\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 2, 'created': '2019-08-11 21:56:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/ae8b53965d31949cd834b63d36b1f7b803884a06', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 3, 'created': '2019-08-12 15:01:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/6c7e89ddc01ae9aa5e96d738c9877b23427199d8', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 4, 'created': '2019-08-12 20:02:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/bd3f61e0314f74b2ab4d413ac7da2e98c4daf436', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 5, 'created': '2019-08-13 14:52:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/7df99f9b94b8ebdff0e8dc57202c3019dfd5d520', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 6, 'created': '2019-08-14 15:38:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/67b65869d190e7401b8d1eb009448590cd61c7d0', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 7, 'created': '2019-08-14 15:53:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/431abc61ee6a3f3a71e43104d952cc709d798f76', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 8, 'created': '2019-08-14 21:23:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/3399b8662e2fd06b227b1bd8a9a8014dbe7c62d5', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 9, 'created': '2019-08-15 16:46:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/9c1ea439d69f03848814cd615336523f5bfb6b70', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 10, 'created': '2019-08-15 20:57:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/6a47a3a1d968dc59e3e7cd845fd842fce64a0340', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 11, 'created': '2019-08-15 21:06:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/59f7908e29e844428ec68e16f932216ec73ada30', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 12, 'created': '2019-08-15 22:15:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/193cc6bf73742bcb3eb08fcc956af41e79185254', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 13, 'created': '2019-08-15 22:27:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/76a3a5ae2da476d88e8a2931e8e14d3b30217785', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 14, 'created': '2019-08-15 22:30:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/cae83a7adfa66e3fd41a022e1b4919feebabc8f4', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 15, 'created': '2019-08-16 04:33:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/192e5e4b9476572b93dd9f7eea91fdbdfe1007e8', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 16, 'created': '2019-08-16 20:26:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/74a7e8b543cb9e5b54bb9b682c1755aa87afae79', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 17, 'created': '2019-08-16 20:36:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/06a88fb6c17319a01371d0324c7f67b21296279f', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 18, 'created': '2019-08-22 20:22:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/f0723b8311fb544ec48c40ba633d646adba2e916', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 19, 'created': '2019-08-22 22:18:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/f70d41b792e502464d6d3a193e5bbc434c4818ac', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 20, 'created': '2019-08-22 23:34:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/9425ad63b974e857defddb558fb9ab79dc565ab8', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 21, 'created': '2019-08-24 14:47:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/379ef2a486da9fe7b8679c1ebdce099cdb2bb733', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 22, 'created': '2019-08-29 05:23:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/a584aaf9ed7f5efd4a716101fca1b088b93c22c5', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 23, 'created': '2019-09-03 15:07:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/fd9b8f5ad5381f96d50edd61896e41356d264425', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}, {'number': 24, 'created': '2019-09-04 03:24:37.000000000', 'files': ['mariadb/templates/deployment-ingress.yaml', 'mariadb/files/nginx.tmpl', 'mariadb/values.yaml', 'mariadb/templates/configmap-etc.yaml', 'mariadb/templates/bin/_mariadb-ingress-controller.sh.tpl', 'mariadb/templates/configmap-ingress-conf.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/6e2f72c2c96b0b4ce2111269a3ebae53bcd114dc', 'message': 'Enable MariaDB Ingress image version overwrite\n\nClusterRole and ClusterRoleBinding is added for mariadb ingress\ncontroller to enable its access to the node.\n\nCurrent mariadb ingress version related values are made overwritable\nsuch that image with a different version can be used in downstream.\n\nChange-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb\nSigned-off-by: Huang, Sophie (sh879n) <sh879n@att.com>\n'}]",27,675181,6e2f72c2c96b0b4ce2111269a3ebae53bcd114dc,84,8,24,18236,,,0,"Enable MariaDB Ingress image version overwrite

ClusterRole and ClusterRoleBinding is added for mariadb ingress
controller to enable its access to the node.

Current mariadb ingress version related values are made overwritable
such that image with a different version can be used in downstream.

Change-Id: Ic39413f39c86a9c637f707540a1c9410ca1c6efb
Signed-off-by: Huang, Sophie (sh879n) <sh879n@att.com>
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/81/675181/12 && git format-patch -1 --stdout FETCH_HEAD,['mariadb/templates/deployment-ingress.yaml'],1,c67d4c41ed4bdb8779e86e93bbe5da37db94dd74,mariadb-ingress,"apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: {{ $serviceAccountName }} rules: - apiGroups: - """" resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - """" resources: - nodes verbs: - get - apiGroups: - """" resources: - services verbs: - get - list - watch - apiGroups: - ""extensions"" resources: - ingresses verbs: - get - list - watch - apiGroups: - """" resources: - events verbs: - create - patch - apiGroups: - ""extensions"" resources: - ingresses/status verbs: - update --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: {{ $serviceAccountName }} roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: {{ $serviceAccountName }} subjects: - kind: ServiceAccount name: {{ $serviceAccountName }} namespace: {{ $envAll.Release.Namespace }} ---",,65,0
openstack%2Fnetworking-ovn~master~Ifb189579894d0b5dba16d5386cc15016520e7ccf,openstack/networking-ovn,master,Ifb189579894d0b5dba16d5386cc15016520e7ccf,DevStack: do not overshadow the default of geneve max_header_size,ABANDONED,2020-03-20 09:01:22.000000000,2020-03-20 19:14:51.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2020-03-20 09:01:22.000000000', 'files': ['devstack/lib/networking-ovn'], 'web_link': 'https://opendev.org/openstack/networking-ovn/commit/496b99de002ac62a5b501188bd36c47e1a7c7a47', 'message': 'DevStack: do not overshadow the default of geneve max_header_size\n\nThis is to test the real default as set by\nGENEVE_ENCAP_MIN_OVERHEAD from neutron-lib.\n\nChange-Id: Ifb189579894d0b5dba16d5386cc15016520e7ccf\nRelated-bug: #1868137\n'}]",0,714053,496b99de002ac62a5b501188bd36c47e1a7c7a47,3,1,1,30491,,,0,"DevStack: do not overshadow the default of geneve max_header_size

This is to test the real default as set by
GENEVE_ENCAP_MIN_OVERHEAD from neutron-lib.

Change-Id: Ifb189579894d0b5dba16d5386cc15016520e7ccf
Related-bug: #1868137
",git fetch https://review.opendev.org/openstack/networking-ovn refs/changes/53/714053/1 && git format-patch -1 --stdout FETCH_HEAD,['devstack/lib/networking-ovn'],1,496b99de002ac62a5b501188bd36c47e1a7c7a47,bug/1868137-related,"OVN_GENEVE_OVERHEAD=${OVN_GENEVE_OVERHEAD:-} if [ ! -z ""$OVN_GENEVE_OVERHEAD"" ] ; then populate_ml2_config /$Q_PLUGIN_CONF_FILE ml2_type_geneve max_header_size=$OVN_GENEVE_OVERHEAD fi ",OVN_GENEVE_OVERHEAD=${OVN_GENEVE_OVERHEAD:-38} populate_ml2_config /$Q_PLUGIN_CONF_FILE ml2_type_geneve max_header_size=$OVN_GENEVE_OVERHEAD,6,2
openstack%2Fswift~master~I4e6ffa8eb8ba9e5588aba09c95b059f3ed3bb1f1,openstack/swift,master,I4e6ffa8eb8ba9e5588aba09c95b059f3ed3bb1f1,Add CORS func tests for s3api,ABANDONED,2020-02-28 00:22:25.000000000,2020-03-20 19:14:48.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2020-02-28 00:22:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4220df47489fa4408bc9b292183960c459298e6f', 'message': 'WIP: Add CORS func tests for s3api\n\nChange-Id: I4e6ffa8eb8ba9e5588aba09c95b059f3ed3bb1f1\n'}, {'number': 2, 'created': '2020-02-28 15:58:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/065ec74c64bf2233dc7a2ce83194286f55063896', 'message': 'WIP: Add CORS func tests for s3api\n\nChange-Id: I4e6ffa8eb8ba9e5588aba09c95b059f3ed3bb1f1\n'}, {'number': 3, 'created': '2020-03-11 20:49:41.000000000', 'files': ['test/functional/cors/main.py', 'test/functional/cors/index.html', 'test/functional/cors/vendor/aws-sdk-2.552.0-S3.js', 'test/functional/cors/test-s3-obj.js'], 'web_link': 'https://opendev.org/openstack/swift/commit/257d1580771244958395bdf61e9b5f55ca6a8e7b', 'message': 'Add CORS func tests for s3api\n\nChange-Id: I4e6ffa8eb8ba9e5588aba09c95b059f3ed3bb1f1\n'}]",0,710354,257d1580771244958395bdf61e9b5f55ca6a8e7b,7,1,3,15343,,,0,"Add CORS func tests for s3api

Change-Id: I4e6ffa8eb8ba9e5588aba09c95b059f3ed3bb1f1
",git fetch https://review.opendev.org/openstack/swift refs/changes/54/710354/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/functional/cors/index.html', 'test/functional/cors/test-s3-obj.js', 'test/functional/cors/vendor/aws-sdk-2.552.0-S3.js']",3,4220df47489fa4408bc9b292183960c459298e6f,cors-functests,"var _xamzrequire = function() { function r(e, n, t) { function o(i, f) { if (!n[i]) { if (!e[i]) { var c = ""function"" == typeof require && require; if (!f && c) return c(i, !0); if (u) return u(i, !0); var a = new Error(""Cannot find module '"" + i + ""'""); throw a.code = ""MODULE_NOT_FOUND"", a; } var p = n[i] = { exports: {} }; e[i][0].call(p.exports, function(r) { var n = e[i][1][r]; return o(n || r); }, p, p.exports, r, e, n, t); } return n[i].exports; } for (var u = ""function"" == typeof require && require, i = 0; i < t.length; i++) o(t[i]); return o; } return r; }()({ 38: [ function(require, module, exports) { var AWS = { util: require(""./util"") }; var _hidden = {}; _hidden.toString(); module.exports = AWS; AWS.util.update(AWS, { VERSION: ""2.552.0"", Signers: {}, Protocol: { Json: require(""./protocol/json""), Query: require(""./protocol/query""), Rest: require(""./protocol/rest""), RestJson: require(""./protocol/rest_json""), RestXml: require(""./protocol/rest_xml"") }, XML: { Builder: require(""./xml/builder""), Parser: null }, JSON: { Builder: require(""./json/builder""), Parser: require(""./json/parser"") }, Model: { Api: require(""./model/api""), Operation: require(""./model/operation""), Shape: require(""./model/shape""), Paginator: require(""./model/paginator""), ResourceWaiter: require(""./model/resource_waiter"") }, apiLoader: require(""./api_loader""), EndpointCache: require(""../vendor/endpoint-cache"").EndpointCache }); require(""./sequential_executor""); require(""./service""); require(""./config""); require(""./http""); require(""./event_listeners""); require(""./request""); require(""./response""); require(""./resource_waiter""); require(""./signers/request_signer""); require(""./param_validator""); AWS.events = new AWS.SequentialExecutor(); AWS.util.memoizedProperty(AWS, ""endpointCache"", function() { return new AWS.EndpointCache(AWS.config.endpointCacheSize); }, true); }, { ""../vendor/endpoint-cache"": 123, ""./api_loader"": 27, ""./config"": 37, ""./event_listeners"": 59, ""./http"": 60, ""./json/builder"": 62, ""./json/parser"": 63, ""./model/api"": 64, ""./model/operation"": 66, ""./model/paginator"": 67, ""./model/resource_waiter"": 68, ""./model/shape"": 69, ""./param_validator"": 70, ""./protocol/json"": 73, ""./protocol/query"": 74, ""./protocol/rest"": 75, ""./protocol/rest_json"": 76, ""./protocol/rest_xml"": 77, ""./request"": 83, ""./resource_waiter"": 84, ""./response"": 85, ""./sequential_executor"": 87, ""./service"": 88, ""./signers/request_signer"": 108, ""./util"": 116, ""./xml/builder"": 118 } ], 123: [ function(require, module, exports) { ""use strict""; Object.defineProperty(exports, ""__esModule"", { value: true }); var LRU_1 = require(""./utils/LRU""); var CACHE_SIZE = 1e3; var EndpointCache = function() { function EndpointCache(maxSize) { if (maxSize === void 0) { maxSize = CACHE_SIZE; } this.maxSize = maxSize; this.cache = new LRU_1.LRUCache(maxSize); } Object.defineProperty(EndpointCache.prototype, ""size"", { get: function() { return this.cache.length; }, enumerable: true, configurable: true }); EndpointCache.prototype.put = function(key, value) { var keyString = typeof key !== ""string"" ? EndpointCache.getKeyString(key) : key; var endpointRecord = this.populateValue(value); this.cache.put(keyString, endpointRecord); }; EndpointCache.prototype.get = function(key) { var keyString = typeof key !== ""string"" ? EndpointCache.getKeyString(key) : key; var now = Date.now(); var records = this.cache.get(keyString); if (records) { for (var i = 0; i < records.length; i++) { var record = records[i]; if (record.Expire < now) { this.cache.remove(keyString); return undefined; } } } return records; }; EndpointCache.getKeyString = function(key) { var identifiers = []; var identifierNames = Object.keys(key).sort(); for (var i = 0; i < identifierNames.length; i++) { var identifierName = identifierNames[i]; if (key[identifierName] === undefined) continue; identifiers.push(key[identifierName]); } return identifiers.join("" ""); }; EndpointCache.prototype.populateValue = function(endpoints) { var now = Date.now(); return endpoints.map(function(endpoint) { return { Address: endpoint.Address || """", Expire: now + (endpoint.CachePeriodInMinutes || 1) * 60 * 1e3 }; }); }; EndpointCache.prototype.empty = function() { this.cache.empty(); }; EndpointCache.prototype.remove = function(key) { var keyString = typeof key !== ""string"" ? EndpointCache.getKeyString(key) : key; this.cache.remove(keyString); }; return EndpointCache; }(); exports.EndpointCache = EndpointCache; }, { ""./utils/LRU"": 124 } ], 124: [ function(require, module, exports) { ""use strict""; Object.defineProperty(exports, ""__esModule"", { value: true }); var LinkedListNode = function() { function LinkedListNode(key, value) { this.key = key; this.value = value; } return LinkedListNode; }(); var LRUCache = function() { function LRUCache(size) { this.nodeMap = {}; this.size = 0; if (typeof size !== ""number"" || size < 1) { throw new Error(""Cache size can only be positive number""); } this.sizeLimit = size; } Object.defineProperty(LRUCache.prototype, ""length"", { get: function() { return this.size; }, enumerable: true, configurable: true }); LRUCache.prototype.prependToList = function(node) { if (!this.headerNode) { this.tailNode = node; } else { this.headerNode.prev = node; node.next = this.headerNode; } this.headerNode = node; this.size++; }; LRUCache.prototype.removeFromTail = function() { if (!this.tailNode) { return undefined; } var node = this.tailNode; var prevNode = node.prev; if (prevNode) { prevNode.next = undefined; } node.prev = undefined; this.tailNode = prevNode; this.size--; return node; }; LRUCache.prototype.detachFromList = function(node) { if (this.headerNode === node) { this.headerNode = node.next; } if (this.tailNode === node) { this.tailNode = node.prev; } if (node.prev) { node.prev.next = node.next; } if (node.next) { node.next.prev = node.prev; } node.next = undefined; node.prev = undefined; this.size--; }; LRUCache.prototype.get = function(key) { if (this.nodeMap[key]) { var node = this.nodeMap[key]; this.detachFromList(node); this.prependToList(node); return node.value; } }; LRUCache.prototype.remove = function(key) { if (this.nodeMap[key]) { var node = this.nodeMap[key]; this.detachFromList(node); delete this.nodeMap[key]; } }; LRUCache.prototype.put = function(key, value) { if (this.nodeMap[key]) { this.remove(key); } else if (this.size === this.sizeLimit) { var tailNode = this.removeFromTail(); var key_1 = tailNode.key; delete this.nodeMap[key_1]; } var newNode = new LinkedListNode(key, value); this.nodeMap[key] = newNode; this.prependToList(newNode); }; LRUCache.prototype.empty = function() { var keys = Object.keys(this.nodeMap); for (var i = 0; i < keys.length; i++) { var key = keys[i]; var node = this.nodeMap[key]; this.detachFromList(node); delete this.nodeMap[key]; } }; return LRUCache; }(); exports.LRUCache = LRUCache; }, {} ], 118: [ function(require, module, exports) { var util = require(""../util""); var XmlNode = require(""./xml-node"").XmlNode; var XmlText = require(""./xml-text"").XmlText; function XmlBuilder() {} XmlBuilder.prototype.toXML = function(params, shape, rootElement, noEmpty) { var xml = new XmlNode(rootElement); applyNamespaces(xml, shape, true); serialize(xml, params, shape); return xml.children.length > 0 || noEmpty ? xml.toString() : """"; }; function serialize(xml, value, shape) { switch (shape.type) { case ""structure"": return serializeStructure(xml, value, shape); case ""map"": return serializeMap(xml, value, shape); case ""list"": return serializeList(xml, value, shape); default: return serializeScalar(xml, value, shape); } } function serializeStructure(xml, params, shape) { util.arrayEach(shape.memberNames, function(memberName) { var memberShape = shape.members[memberName]; if (memberShape.location !== ""body"") return; var value = params[memberName]; var name = memberShape.name; if (value !== undefined && value !== null) { if (memberShape.isXmlAttribute) { xml.addAttribute(name, value); } else if (memberShape.flattened) { serialize(xml, value, memberShape); } else { var element = new XmlNode(name); xml.addChildNode(element); applyNamespaces(element, memberShape); serialize(element, value, memberShape); } } }); } function serializeMap(xml, map, shape) { var xmlKey = shape.key.name || ""key""; var xmlValue = shape.value.name || ""value""; util.each(map, function(key, value) { var entry = new XmlNode(shape.flattened ? shape.name : ""entry""); xml.addChildNode(entry); var entryKey = new XmlNode(xmlKey); var entryValue = new XmlNode(xmlValue); entry.addChildNode(entryKey); entry.addChildNode(entryValue); serialize(entryKey, key, shape.key); serialize(entryValue, value, shape.value); }); } function serializeList(xml, list, shape) { if (shape.flattened) { util.arrayEach(list, function(value) { var name = shape.member.name || shape.name; var element = new XmlNode(name); xml.addChildNode(element); serialize(element, value, shape.member); }); } else { util.arrayEach(list, function(value) { var name = shape.member.name || ""member""; var element = new XmlNode(name); xml.addChildNode(element); serialize(element, value, shape.member); }); } } function serializeScalar(xml, value, shape) { xml.addChildNode(new XmlText(shape.toWireFormat(value))); } function applyNamespaces(xml, shape, isRoot) { var uri, prefix = ""xmlns""; if (shape.xmlNamespaceUri) { uri = shape.xmlNamespaceUri; if (shape.xmlNamespacePrefix) prefix += "":"" + shape.xmlNamespacePrefix; } else if (isRoot && shape.api.xmlNamespaceUri) { uri = shape.api.xmlNamespaceUri; } if (uri) xml.addAttribute(prefix, uri); } module.exports = XmlBuilder; }, { ""../util"": 116, ""./xml-node"": 121, ""./xml-text"": 122 } ], 122: [ function(require, module, exports) { var escapeElement = require(""./escape-element"").escapeElement; function XmlText(value) { this.value = value; } XmlText.prototype.toString = function() { return escapeElement("""" + this.value); }; module.exports = { XmlText: XmlText }; }, { ""./escape-element"": 120 } ], 120: [ function(require, module, exports) { function escapeElement(value) { return value.replace(/&/g, ""&amp;"").replace(/</g, ""&lt;"").replace(/>/g, ""&gt;""); } module.exports = { escapeElement: escapeElement }; }, {} ], 121: [ function(require, module, exports) { var escapeAttribute = require(""./escape-attribute"").escapeAttribute; function XmlNode(name, children) { if (children === void 0) { children = []; } this.name = name; this.children = children; this.attributes = {}; } XmlNode.prototype.addAttribute = function(name, value) { this.attributes[name] = value; return this; }; XmlNode.prototype.addChildNode = function(child) { this.children.push(child); return this; }; XmlNode.prototype.removeAttribute = function(name) { delete this.attributes[name]; return this; }; XmlNode.prototype.toString = function() { var hasChildren = Boolean(this.children.length); var xmlText = ""<"" + this.name; var attributes = this.attributes; for (var i = 0, attributeNames = Object.keys(attributes); i < attributeNames.length; i++) { var attributeName = attributeNames[i]; var attribute = attributes[attributeName]; if (typeof attribute !== ""undefined"" && attribute !== null) { xmlText += "" "" + attributeName + '=""' + escapeAttribute("""" + attribute) + '""'; } } return xmlText += !hasChildren ? ""/>"" : "">"" + this.children.map(function(c) { return c.toString(); }).join("""") + ""</"" + this.name + "">""; }; module.exports = { XmlNode: XmlNode }; }, { ""./escape-attribute"": 119 } ], 119: [ function(require, module, exports) { function escapeAttribute(value) { return value.replace(/&/g, ""&amp;"").replace(/'/g, ""&apos;"").replace(/</g, ""&lt;"").replace(/>/g, ""&gt;"").replace(/""/g, ""&quot;""); } module.exports = { escapeAttribute: escapeAttribute }; }, {} ], 108: [ function(require, module, exports) { var AWS = require(""../core""); var inherit = AWS.util.inherit; AWS.Signers.RequestSigner = inherit({ constructor: function RequestSigner(request) { this.request = request; }, setServiceClientId: function setServiceClientId(id) { this.serviceClientId = id; }, getServiceClientId: function getServiceClientId() { return this.serviceClientId; } }); AWS.Signers.RequestSigner.getVersion = function getVersion(version) { switch (version) { case ""v2"": return AWS.Signers.V2; case ""v3"": return AWS.Signers.V3; case ""s3v4"": return AWS.Signers.V4; case ""v4"": return AWS.Signers.V4; case ""s3"": return AWS.Signers.S3; case ""v3https"": return AWS.Signers.V3Https; } throw new Error(""Unknown signing version "" + version); }; require(""./v2""); require(""./v3""); require(""./v3https""); require(""./v4""); require(""./s3""); require(""./presign""); }, { ""../core"": 38, ""./presign"": 107, ""./s3"": 109, ""./v2"": 110, ""./v3"": 111, ""./v3https"": 112, ""./v4"": 113 } ], 113: [ function(require, module, exports) { var AWS = require(""../core""); var v4Credentials = require(""./v4_credentials""); var inherit = AWS.util.inherit; var expiresHeader = ""presigned-expires""; AWS.Signers.V4 = inherit(AWS.Signers.RequestSigner, { constructor: function V4(request, serviceName, options) { AWS.Signers.RequestSigner.call(this, request); this.serviceName = serviceName; options = options || {}; this.signatureCache = typeof options.signatureCache === ""boolean"" ? options.signatureCache : true; this.operation = options.operation; this.signatureVersion = options.signatureVersion; }, algorithm: ""AWS4-HMAC-SHA256"", addAuthorization: function addAuthorization(credentials, date) { var datetime = AWS.util.date.iso8601(date).replace(/[:\-]|\.\d{3}/g, """"); if (this.isPresigned()) { this.updateForPresigned(credentials, datetime); } else { this.addHeaders(credentials, datetime); } this.request.headers[""Authorization""] = this.authorization(credentials, datetime); }, addHeaders: function addHeaders(credentials, datetime) { this.request.headers[""X-Amz-Date""] = datetime; if (credentials.sessionToken) { this.request.headers[""x-amz-security-token""] = credentials.sessionToken; } }, updateForPresigned: function updateForPresigned(credentials, datetime) { var credString = this.credentialString(datetime); var qs = { ""X-Amz-Date"": datetime, ""X-Amz-Algorithm"": this.algorithm, ""X-Amz-Credential"": credentials.accessKeyId + ""/"" + credString, ""X-Amz-Expires"": this.request.headers[expiresHeader], ""X-Amz-SignedHeaders"": this.signedHeaders() }; if (credentials.sessionToken) { qs[""X-Amz-Security-Token""] = credentials.sessionToken; } if (this.request.headers[""Content-Type""]) { qs[""Content-Type""] = this.request.headers[""Content-Type""]; } if (this.request.headers[""Content-MD5""]) { qs[""Content-MD5""] = this.request.headers[""Content-MD5""]; } if (this.request.headers[""Cache-Control""]) { qs[""Cache-Control""] = this.request.headers[""Cache-Control""]; } AWS.util.each.call(this, this.request.headers, function(key, value) { if (key === expiresHeader) return; if (this.isSignableHeader(key)) { var lowerKey = key.toLowerCase(); if (lowerKey.indexOf(""x-amz-meta-"") === 0) { qs[lowerKey] = value; } else if (lowerKey.indexOf(""x-amz-"") === 0) { qs[key] = value; } } }); var sep = this.request.path.indexOf(""?"") >= 0 ? ""&"" : ""?""; this.request.path += sep + AWS.util.queryParamsToString(qs); }, authorization: function authorization(credentials, datetime) { var parts = []; var credString = this.credentialString(datetime); parts.push(this.algorithm + "" Credential="" + credentials.accessKeyId + ""/"" + credString); parts.push(""SignedHeaders="" + this.signedHeaders()); parts.push(""Signature="" + this.signature(credentials, datetime)); return parts.join("", ""); }, signature: function signature(credentials, datetime) { var signingKey = v4Credentials.getSigningKey(credentials, datetime.substr(0, 8), this.request.region, this.serviceName, this.signatureCache); return AWS.util.crypto.hmac(signingKey, this.stringToSign(datetime), ""hex""); }, stringToSign: function stringToSign(datetime) { var parts = []; parts.push(""AWS4-HMAC-SHA256""); parts.push(datetime); parts.push(this.credentialString(datetime)); parts.push(this.hexEncodedHash(this.canonicalString())); return parts.join(""\n""); }, canonicalString: function canonicalString() { var parts = [], pathname = this.request.pathname(); if (this.serviceName !== ""s3"" && this.signatureVersion !== ""s3v4"") pathname = AWS.util.uriEscapePath(pathname); parts.push(this.request.method); parts.push(pathname); parts.push(this.request.search()); parts.push(this.canonicalHeaders() + ""\n""); parts.push(this.signedHeaders()); parts.push(this.hexEncodedBodyHash()); return parts.join(""\n""); }, canonicalHeaders: function canonicalHeaders() { var headers = []; AWS.util.each.call(this, this.request.headers, function(key, item) { headers.push([ key, item ]); }); headers.sort(function(a, b) { return a[0].toLowerCase() < b[0].toLowerCase() ? -1 : 1; }); var parts = []; AWS.util.arrayEach.call(this, headers, function(item) { var key = item[0].toLowerCase(); if (this.isSignableHeader(key)) { var value = item[1]; if (typeof value === ""undefined"" || value === null || typeof value.toString !== ""function"") { throw AWS.util.error(new Error(""Header "" + key + "" contains invalid value""), { code: ""InvalidHeader"" }); } parts.push(key + "":"" + this.canonicalHeaderValues(value.toString())); } }); return parts.join(""\n""); }, canonicalHeaderValues: function canonicalHeaderValues(values) { return values.replace(/\s+/g, "" "").replace(/^\s+|\s+$/g, """"); }, signedHeaders: function signedHeaders() { var keys = []; AWS.util.each.call(this, this.request.headers, function(key) { key = key.toLowerCase(); if (this.isSignableHeader(key)) keys.push(key); }); return keys.sort().join("";""); }, credentialString: function credentialString(datetime) { return v4Credentials.createScope(datetime.substr(0, 8), this.request.region, this.serviceName); }, hexEncodedHash: function hash(string) { return AWS.util.crypto.sha256(string, ""hex""); }, hexEncodedBodyHash: function hexEncodedBodyHash() { var request = this.request; if (this.isPresigned() && this.serviceName === ""s3"" && !request.body) { return ""UNSIGNED-PAYLOAD""; } else if (request.headers[""X-Amz-Content-Sha256""]) { return request.headers[""X-Amz-Content-Sha256""]; } else { return this.hexEncodedHash(this.request.body || """"); } }, unsignableHeaders: [ ""authorization"", ""content-type"", ""content-length"", ""user-agent"", expiresHeader, ""expect"", ""x-amzn-trace-id"" ], isSignableHeader: function isSignableHeader(key) { if (key.toLowerCase().indexOf(""x-amz-"") === 0) return true; return this.unsignableHeaders.indexOf(key) < 0; }, isPresigned: function isPresigned() { return this.request.headers[expiresHeader] ? true : false; } }); module.exports = AWS.Signers.V4; }, { ""../core"": 38, ""./v4_credentials"": 114 } ], 114: [ function(require, module, exports) { var AWS = require(""../core""); var cachedSecret = {}; var cacheQueue = []; var maxCacheEntries = 50; var v4Identifier = ""aws4_request""; module.exports = { createScope: function createScope(date, region, serviceName) { return [ date.substr(0, 8), region, serviceName, v4Identifier ].join(""/""); }, getSigningKey: function getSigningKey(credentials, date, region, service, shouldCache) { var credsIdentifier = AWS.util.crypto.hmac(credentials.secretAccessKey, credentials.accessKeyId, ""base64""); var cacheKey = [ credsIdentifier, date, region, service ].join(""_""); shouldCache = shouldCache !== false; if (shouldCache && cacheKey in cachedSecret) { return cachedSecret[cacheKey]; } var kDate = AWS.util.crypto.hmac(""AWS4"" + credentials.secretAccessKey, date, ""buffer""); var kRegion = AWS.util.crypto.hmac(kDate, region, ""buffer""); var kService = AWS.util.crypto.hmac(kRegion, service, ""buffer""); var signingKey = AWS.util.crypto.hmac(kService, v4Identifier, ""buffer""); if (shouldCache) { cachedSecret[cacheKey] = signingKey; cacheQueue.push(cacheKey); if (cacheQueue.length > maxCacheEntries) { delete cachedSecret[cacheQueue.shift()]; } } return signingKey; }, emptyCache: function emptyCache() { cachedSecret = {}; cacheQueue = []; } }; }, { ""../core"": 38 } ], 112: [ function(require, module, exports) { var AWS = require(""../core""); var inherit = AWS.util.inherit; require(""./v3""); AWS.Signers.V3Https = inherit(AWS.Signers.V3, { authorization: function authorization(credentials) { return ""AWS3-HTTPS "" + ""AWSAccessKeyId="" + credentials.accessKeyId + "","" + ""Algorithm=HmacSHA256,"" + ""Signature="" + this.signature(credentials); }, stringToSign: function stringToSign() { return this.request.headers[""X-Amz-Date""]; } }); module.exports = AWS.Signers.V3Https; }, { ""../core"": 38, ""./v3"": 111 } ], 111: [ function(require, module, exports) { var AWS = require(""../core""); var inherit = AWS.util.inherit; AWS.Signers.V3 = inherit(AWS.Signers.RequestSigner, { addAuthorization: function addAuthorization(credentials, date) { var datetime = AWS.util.date.rfc822(date); this.request.headers[""X-Amz-Date""] = datetime; if (credentials.sessionToken) { this.request.headers[""x-amz-security-token""] = credentials.sessionToken; } this.request.headers[""X-Amzn-Authorization""] = this.authorization(credentials, datetime); }, authorization: function authorization(credentials) { return ""AWS3 "" + ""AWSAccessKeyId="" + credentials.accessKeyId + "","" + ""Algorithm=HmacSHA256,"" + ""SignedHeaders="" + this.signedHeaders() + "","" + ""Signature="" + this.signature(credentials); }, signedHeaders: function signedHeaders() { var headers = []; AWS.util.arrayEach(this.headersToSign(), function iterator(h) { headers.push(h.toLowerCase()); }); return headers.sort().join("";""); }, canonicalHeaders: function canonicalHeaders() { var headers = this.request.headers; var parts = []; AWS.util.arrayEach(this.headersToSign(), function iterator(h) { parts.push(h.toLowerCase().trim() + "":"" + String(headers[h]).trim()); }); return parts.sort().join(""\n"") + ""\n""; }, headersToSign: function headersToSign() { var headers = []; AWS.util.each(this.request.headers, function iterator(k) { if (k === ""Host"" || k === ""Content-Encoding"" || k.match(/^X-Amz/i)) { headers.push(k); } }); return headers; }, signature: function signature(credentials) { return AWS.util.crypto.hmac(credentials.secretAccessKey, this.stringToSign(), ""base64""); }, stringToSign: function stringToSign() { var parts = []; parts.push(this.request.method); parts.push(""/""); parts.push(""""); parts.push(this.canonicalHeaders()); parts.push(this.request.body); return AWS.util.crypto.sha256(parts.join(""\n"")); } }); module.exports = AWS.Signers.V3; }, { ""../core"": 38 } ], 110: [ function(require, module, exports) { var AWS = require(""../core""); var inherit = AWS.util.inherit; AWS.Signers.V2 = inherit(AWS.Signers.RequestSigner, { addAuthorization: function addAuthorization(credentials, date) { if (!date) date = AWS.util.date.getDate(); var r = this.request; r.params.Timestamp = AWS.util.date.iso8601(date); r.params.SignatureVersion = ""2""; r.params.SignatureMethod = ""HmacSHA256""; r.params.AWSAccessKeyId = credentials.accessKeyId; if (credentials.sessionToken) { r.params.SecurityToken = credentials.sessionToken; } delete r.params.Signature; r.params.Signature = this.signature(credentials); r.body = AWS.util.queryParamsToString(r.params); r.headers[""Content-Length""] = r.body.length; }, signature: function signature(credentials) { return AWS.util.crypto.hmac(credentials.secretAccessKey, this.stringToSign(), ""base64""); }, stringToSign: function stringToSign() { var parts = []; parts.push(this.request.method); parts.push(this.request.endpoint.host.toLowerCase()); parts.push(this.request.pathname()); parts.push(AWS.util.queryParamsToString(this.request.params)); return parts.join(""\n""); } }); module.exports = AWS.Signers.V2; }, { ""../core"": 38 } ], 109: [ function(require, module, exports) { var AWS = require(""../core""); var inherit = AWS.util.inherit; AWS.Signers.S3 = inherit(AWS.Signers.RequestSigner, { subResources: { acl: 1, accelerate: 1, analytics: 1, cors: 1, lifecycle: 1, delete: 1, inventory: 1, location: 1, logging: 1, metrics: 1, notification: 1, partNumber: 1, policy: 1, requestPayment: 1, replication: 1, restore: 1, tagging: 1, torrent: 1, uploadId: 1, uploads: 1, versionId: 1, versioning: 1, versions: 1, website: 1 }, responseHeaders: { ""response-content-type"": 1, ""response-content-language"": 1, ""response-expires"": 1, ""response-cache-control"": 1, ""response-content-disposition"": 1, ""response-content-encoding"": 1 }, addAuthorization: function addAuthorization(credentials, date) { if (!this.request.headers[""presigned-expires""]) { this.request.headers[""X-Amz-Date""] = AWS.util.date.rfc822(date); } if (credentials.sessionToken) { this.request.headers[""x-amz-security-token""] = credentials.sessionToken; } var signature = this.sign(credentials.secretAccessKey, this.stringToSign()); var auth = ""AWS "" + credentials.accessKeyId + "":"" + signature; this.request.headers[""Authorization""] = auth; }, stringToSign: function stringToSign() { var r = this.request; var parts = []; parts.push(r.method); parts.push(r.headers[""Content-MD5""] || """"); parts.push(r.headers[""Content-Type""] || """"); parts.push(r.headers[""presigned-expires""] || """"); var headers = this.canonicalizedAmzHeaders(); if (headers) parts.push(headers); parts.push(this.canonicalizedResource()); return parts.join(""\n""); }, canonicalizedAmzHeaders: function canonicalizedAmzHeaders() { var amzHeaders = []; AWS.util.each(this.request.headers, function(name) { if (name.match(/^x-amz-/i)) amzHeaders.push(name); }); amzHeaders.sort(function(a, b) { return a.toLowerCase() < b.toLowerCase() ? -1 : 1; }); var parts = []; AWS.util.arrayEach.call(this, amzHeaders, function(name) { parts.push(name.toLowerCase() + "":"" + String(this.request.headers[name])); }); return parts.join(""\n""); }, canonicalizedResource: function canonicalizedResource() { var r = this.request; var parts = r.path.split(""?""); var path = parts[0]; var querystring = parts[1]; var resource = """"; if (r.virtualHostedBucket) resource += ""/"" + r.virtualHostedBucket; resource += path; if (querystring) { var resources = []; AWS.util.arrayEach.call(this, querystring.split(""&""), function(param) { var name = param.split(""="")[0]; var value = param.split(""="")[1]; if (this.subResources[name] || this.responseHeaders[name]) { var subresource = { name: name }; if (value !== undefined) { if (this.subResources[name]) { subresource.value = value; } else { subresource.value = decodeURIComponent(value); } } resources.push(subresource); } }); resources.sort(function(a, b) { return a.name < b.name ? -1 : 1; }); if (resources.length) { querystring = []; AWS.util.arrayEach(resources, function(res) { if (res.value === undefined) { querystring.push(res.name); } else { querystring.push(res.name + ""="" + res.value); } }); resource += ""?"" + querystring.join(""&""); } } return resource; }, sign: function sign(secret, string) { return AWS.util.crypto.hmac(secret, string, ""base64"", ""sha1""); } }); module.exports = AWS.Signers.S3; }, { ""../core"": 38 } ], 107: [ function(require, module, exports) { var AWS = require(""../core""); var inherit = AWS.util.inherit; var expiresHeader = ""presigned-expires""; function signedUrlBuilder(request) { var expires = request.httpRequest.headers[expiresHeader]; var signerClass = request.service.getSignerClass(request); delete request.httpRequest.headers[""User-Agent""]; delete request.httpRequest.headers[""X-Amz-User-Agent""]; if (signerClass === AWS.Signers.V4) { if (expires > 604800) { var message = ""Presigning does not support expiry time greater "" + ""than a week with SigV4 signing.""; throw AWS.util.error(new Error(), { code: ""InvalidExpiryTime"", message: message, retryable: false }); } request.httpRequest.headers[expiresHeader] = expires; } else if (signerClass === AWS.Signers.S3) { var now = request.service ? request.service.getSkewCorrectedDate() : AWS.util.date.getDate(); request.httpRequest.headers[expiresHeader] = parseInt(AWS.util.date.unixTimestamp(now) + expires, 10).toString(); } else { throw AWS.util.error(new Error(), { message: ""Presigning only supports S3 or SigV4 signing."", code: ""UnsupportedSigner"", retryable: false }); } } function signedUrlSigner(request) { var endpoint = request.httpRequest.endpoint; var parsedUrl = AWS.util.urlParse(request.httpRequest.path); var queryParams = {}; if (parsedUrl.search) { queryParams = AWS.util.queryStringParse(parsedUrl.search.substr(1)); } var auth = request.httpRequest.headers[""Authorization""].split("" ""); if (auth[0] === ""AWS"") { auth = auth[1].split("":""); queryParams[""Signature""] = auth.pop(); queryParams[""AWSAccessKeyId""] = auth.join("":""); AWS.util.each(request.httpRequest.headers, function(key, value) { if (key === expiresHeader) key = ""Expires""; if (key.indexOf(""x-amz-meta-"") === 0) { delete queryParams[key]; key = key.toLowerCase(); } queryParams[key] = value; }); delete request.httpRequest.headers[expiresHeader]; delete queryParams[""Authorization""]; delete queryParams[""Host""]; } else if (auth[0] === ""AWS4-HMAC-SHA256"") { auth.shift(); var rest = auth.join("" ""); var signature = rest.match(/Signature=(.*?)(?:,|\s|\r?\n|$)/)[1]; queryParams[""X-Amz-Signature""] = signature; delete queryParams[""Expires""]; } endpoint.pathname = parsedUrl.pathname; endpoint.search = AWS.util.queryParamsToString(queryParams); } AWS.Signers.Presign = inherit({ sign: function sign(request, expireTime, callback) { request.httpRequest.headers[expiresHeader] = expireTime || 3600; request.on(""build"", signedUrlBuilder); request.on(""sign"", signedUrlSigner); request.removeListener(""afterBuild"", AWS.EventListeners.Core.SET_CONTENT_LENGTH); request.removeListener(""afterBuild"", AWS.EventListeners.Core.COMPUTE_SHA256); request.emit(""beforePresign"", [ request ]); if (callback) { request.build(function() { if (this.response.error) callback(this.response.error); else { callback(null, AWS.util.urlFormat(request.httpRequest.endpoint)); } }); } else { request.build(); if (request.response.error) throw request.response.error; return AWS.util.urlFormat(request.httpRequest.endpoint); } } }); module.exports = AWS.Signers.Presign; }, { ""../core"": 38 } ], 88: [ function(require, module, exports) { (function(process) { var AWS = require(""./core""); var Api = require(""./model/api""); var regionConfig = require(""./region_config""); var inherit = AWS.util.inherit; var clientCount = 0; AWS.Service = inherit({ constructor: function Service(config) { if (!this.loadServiceClass) { throw AWS.util.error(new Error(), ""Service must be constructed with `new' operator""); } var ServiceClass = this.loadServiceClass(config || {}); if (ServiceClass) { var originalConfig = AWS.util.copy(config); var svc = new ServiceClass(config); Object.defineProperty(svc, ""_originalConfig"", { get: function() { return originalConfig; }, enumerable: false, configurable: true }); svc._clientId = ++clientCount; return svc; } this.initialize(config); }, initialize: function initialize(config) { var svcConfig = AWS.config[this.serviceIdentifier]; this.config = new AWS.Config(AWS.config); if (svcConfig) this.config.update(svcConfig, true); if (config) this.config.update(config, true); this.validateService(); if (!this.config.endpoint) regionConfig(this); this.config.endpoint = this.endpointFromTemplate(this.config.endpoint); this.setEndpoint(this.config.endpoint); AWS.SequentialExecutor.call(this); AWS.Service.addDefaultMonitoringListeners(this); if ((this.config.clientSideMonitoring || AWS.Service._clientSideMonitoring) && this.publisher) { var publisher = this.publisher; this.addNamedListener(""PUBLISH_API_CALL"", ""apiCall"", function PUBLISH_API_CALL(event) { process.nextTick(function() { publisher.eventHandler(event); }); }); this.addNamedListener(""PUBLISH_API_ATTEMPT"", ""apiCallAttempt"", function PUBLISH_API_ATTEMPT(event) { process.nextTick(function() { publisher.eventHandler(event); }); }); } }, validateService: function validateService() {}, loadServiceClass: function loadServiceClass(serviceConfig) { var config = serviceConfig; if (!AWS.util.isEmpty(this.api)) { return null; } else if (config.apiConfig) { return AWS.Service.defineServiceApi(this.constructor, config.apiConfig); } else if (!this.constructor.services) { return null; } else { config = new AWS.Config(AWS.config); config.update(serviceConfig, true); var version = config.apiVersions[this.constructor.serviceIdentifier]; version = version || config.apiVersion; return this.getLatestServiceClass(version); } }, getLatestServiceClass: function getLatestServiceClass(version) { version = this.getLatestServiceVersion(version); if (this.constructor.services[version] === null) { AWS.Service.defineServiceApi(this.constructor, version); } return this.constructor.services[version]; }, getLatestServiceVersion: function getLatestServiceVersion(version) { if (!this.constructor.services || this.constructor.services.length === 0) { throw new Error(""No services defined on "" + this.constructor.serviceIdentifier); } if (!version) { version = ""latest""; } else if (AWS.util.isType(version, Date)) { version = AWS.util.date.iso8601(version).split(""T"")[0]; } if (Object.hasOwnProperty(this.constructor.services, version)) { return version; } var keys = Object.keys(this.constructor.services).sort(); var selectedVersion = null; for (var i = keys.length - 1; i >= 0; i--) { if (keys[i][keys[i].length - 1] !== ""*"") { selectedVersion = keys[i]; } if (keys[i].substr(0, 10) <= version) { return selectedVersion; } } throw new Error(""Could not find "" + this.constructor.serviceIdentifier + "" API to satisfy version constraint `"" + version + ""'""); }, api: {}, defaultRetryCount: 3, customizeRequests: function customizeRequests(callback) { if (!callback) { this.customRequestHandler = null; } else if (typeof callback === ""function"") { this.customRequestHandler = callback; } else { throw new Error(""Invalid callback type '"" + typeof callback + ""' provided in customizeRequests""); } }, makeRequest: function makeRequest(operation, params, callback) { if (typeof params === ""function"") { callback = params; params = null; } params = params || {}; if (this.config.params) { var rules = this.api.operations[operation]; if (rules) { params = AWS.util.copy(params); AWS.util.each(this.config.params, function(key, value) { if (rules.input.members[key]) { if (params[key] === undefined || params[key] === null) { params[key] = value; } } }); } } var request = new AWS.Request(this, operation, params); this.addAllRequestListeners(request); this.attachMonitoringEmitter(request); if (callback) request.send(callback); return request; }, makeUnauthenticatedRequest: function makeUnauthenticatedRequest(operation, params, callback) { if (typeof params === ""function"") { callback = params; params = {}; } var request = this.makeRequest(operation, params).toUnauthenticated(); return callback ? request.send(callback) : request; }, waitFor: function waitFor(state, params, callback) { var waiter = new AWS.ResourceWaiter(this, state); return waiter.wait(params, callback); }, addAllRequestListeners: function addAllRequestListeners(request) { var list = [ AWS.events, AWS.EventListeners.Core, this.serviceInterface(), AWS.EventListeners.CorePost ]; for (var i = 0; i < list.length; i++) { if (list[i]) request.addListeners(list[i]); } if (!this.config.paramValidation) { request.removeListener(""validate"", AWS.EventListeners.Core.VALIDATE_PARAMETERS); } if (this.config.logger) { request.addListeners(AWS.EventListeners.Logger); } this.setupRequestListeners(request); if (typeof this.constructor.prototype.customRequestHandler === ""function"") { this.constructor.prototype.customRequestHandler(request); } if (Object.prototype.hasOwnProperty.call(this, ""customRequestHandler"") && typeof this.customRequestHandler === ""function"") { this.customRequestHandler(request); } }, apiCallEvent: function apiCallEvent(request) { var api = request.service.api.operations[request.operation]; var monitoringEvent = { Type: ""ApiCall"", Api: api ? api.name : request.operation, Version: 1, Service: request.service.api.serviceId || request.service.api.endpointPrefix, Region: request.httpRequest.region, MaxRetriesExceeded: 0, UserAgent: request.httpRequest.getUserAgent() }; var response = request.response; if (response.httpResponse.statusCode) { monitoringEvent.FinalHttpStatusCode = response.httpResponse.statusCode; } if (response.error) { var error = response.error; var statusCode = response.httpResponse.statusCode; if (statusCode > 299) { if (error.code) monitoringEvent.FinalAwsException = error.code; if (error.message) monitoringEvent.FinalAwsExceptionMessage = error.message; } else { if (error.code || error.name) monitoringEvent.FinalSdkException = error.code || error.name; if (error.message) monitoringEvent.FinalSdkExceptionMessage = error.message; } } return monitoringEvent; }, apiAttemptEvent: function apiAttemptEvent(request) { var api = request.service.api.operations[request.operation]; var monitoringEvent = { Type: ""ApiCallAttempt"", Api: api ? api.name : request.operation, Version: 1, Service: request.service.api.serviceId || request.service.api.endpointPrefix, Fqdn: request.httpRequest.endpoint.hostname, UserAgent: request.httpRequest.getUserAgent() }; var response = request.response; if (response.httpResponse.statusCode) { monitoringEvent.HttpStatusCode = response.httpResponse.statusCode; } if (!request._unAuthenticated && request.service.config.credentials && request.service.config.credentials.accessKeyId) { monitoringEvent.AccessKey = request.service.config.credentials.accessKeyId; } if (!response.httpResponse.headers) return monitoringEvent; if (request.httpRequest.headers[""x-amz-security-token""]) { monitoringEvent.SessionToken = request.httpRequest.headers[""x-amz-security-token""]; } if (response.httpResponse.headers[""x-amzn-requestid""]) { monitoringEvent.XAmznRequestId = response.httpResponse.headers[""x-amzn-requestid""]; } if (response.httpResponse.headers[""x-amz-request-id""]) { monitoringEvent.XAmzRequestId = response.httpResponse.headers[""x-amz-request-id""]; } if (response.httpResponse.headers[""x-amz-id-2""]) { monitoringEvent.XAmzId2 = response.httpResponse.headers[""x-amz-id-2""]; } return monitoringEvent; }, attemptFailEvent: function attemptFailEvent(request) { var monitoringEvent = this.apiAttemptEvent(request); var response = request.response; var error = response.error; if (response.httpResponse.statusCode > 299) { if (error.code) monitoringEvent.AwsException = error.code; if (error.message) monitoringEvent.AwsExceptionMessage = error.message; } else { if (error.code || error.name) monitoringEvent.SdkException = error.code || error.name; if (error.message) monitoringEvent.SdkExceptionMessage = error.message; } return monitoringEvent; }, attachMonitoringEmitter: function attachMonitoringEmitter(request) { var attemptTimestamp; var attemptStartRealTime; var attemptLatency; var callStartRealTime; var attemptCount = 0; var region; var callTimestamp; var self = this; var addToHead = true; request.on(""validate"", function() { callStartRealTime = AWS.util.realClock.now(); callTimestamp = Date.now(); }, addToHead); request.on(""sign"", function() { attemptStartRealTime = AWS.util.realClock.now(); attemptTimestamp = Date.now(); region = request.httpRequest.region; attemptCount++; }, addToHead); request.on(""validateResponse"", function() { attemptLatency = Math.round(AWS.util.realClock.now() - attemptStartRealTime); }); request.addNamedListener(""API_CALL_ATTEMPT"", ""success"", function API_CALL_ATTEMPT() { var apiAttemptEvent = self.apiAttemptEvent(request); apiAttemptEvent.Timestamp = attemptTimestamp; apiAttemptEvent.AttemptLatency = attemptLatency >= 0 ? attemptLatency : 0; apiAttemptEvent.Region = region; self.emit(""apiCallAttempt"", [ apiAttemptEvent ]); }); request.addNamedListener(""API_CALL_ATTEMPT_RETRY"", ""retry"", function API_CALL_ATTEMPT_RETRY() { var apiAttemptEvent = self.attemptFailEvent(request); apiAttemptEvent.Timestamp = attemptTimestamp; attemptLatency = attemptLatency || Math.round(AWS.util.realClock.now() - attemptStartRealTime); apiAttemptEvent.AttemptLatency = attemptLatency >= 0 ? attemptLatency : 0; apiAttemptEvent.Region = region; self.emit(""apiCallAttempt"", [ apiAttemptEvent ]); }); request.addNamedListener(""API_CALL"", ""complete"", function API_CALL() { var apiCallEvent = self.apiCallEvent(request); apiCallEvent.AttemptCount = attemptCount; if (apiCallEvent.AttemptCount <= 0) return; apiCallEvent.Timestamp = callTimestamp; var latency = Math.round(AWS.util.realClock.now() - callStartRealTime); apiCallEvent.Latency = latency >= 0 ? latency : 0; var response = request.response; if (typeof response.retryCount === ""number"" && typeof response.maxRetries === ""number"" && response.retryCount >= response.maxRetries) { apiCallEvent.MaxRetriesExceeded = 1; } self.emit(""apiCall"", [ apiCallEvent ]); }); }, setupRequestListeners: function setupRequestListeners(request) {}, getSignerClass: function getSignerClass(request) { var version; var operation = null; var authtype = """"; if (request) { var operations = request.service.api.operations || {}; operation = operations[request.operation] || null; authtype = operation ? operation.authtype : """"; } if (this.config.signatureVersion) { version = this.config.signatureVersion; } else if (authtype === ""v4"" || authtype === ""v4-unsigned-body"") { version = ""v4""; } else { version = this.api.signatureVersion; } return AWS.Signers.RequestSigner.getVersion(version); }, serviceInterface: function serviceInterface() { switch (this.api.protocol) { case ""ec2"": return AWS.EventListeners.Query; case ""query"": return AWS.EventListeners.Query; case ""json"": return AWS.EventListeners.Json; case ""rest-json"": return AWS.EventListeners.RestJson; case ""rest-xml"": return AWS.EventListeners.RestXml; } if (this.api.protocol) { throw new Error(""Invalid service `protocol' "" + this.api.protocol + "" in API config""); } }, successfulResponse: function successfulResponse(resp) { return resp.httpResponse.statusCode < 300; }, numRetries: function numRetries() { if (this.config.maxRetries !== undefined) { return this.config.maxRetries; } else { return this.defaultRetryCount; } }, retryDelays: function retryDelays(retryCount) { return AWS.util.calculateRetryDelay(retryCount, this.config.retryDelayOptions); }, retryableError: function retryableError(error) { if (this.timeoutError(error)) return true; if (this.networkingError(error)) return true; if (this.expiredCredentialsError(error)) return true; if (this.throttledError(error)) return true; if (error.statusCode >= 500) return true; return false; }, networkingError: function networkingError(error) { return error.code === ""NetworkingError""; }, timeoutError: function timeoutError(error) { return error.code === ""TimeoutError""; }, expiredCredentialsError: function expiredCredentialsError(error) { return error.code === ""ExpiredTokenException""; }, clockSkewError: function clockSkewError(error) { switch (error.code) { case ""RequestTimeTooSkewed"": case ""RequestExpired"": case ""InvalidSignatureException"": case ""SignatureDoesNotMatch"": case ""AuthFailure"": case ""RequestInTheFuture"": return true; default: return false; } }, getSkewCorrectedDate: function getSkewCorrectedDate() { return new Date(Date.now() + this.config.systemClockOffset); }, applyClockOffset: function applyClockOffset(newServerTime) { if (newServerTime) { this.config.systemClockOffset = newServerTime - Date.now(); } }, isClockSkewed: function isClockSkewed(newServerTime) { if (newServerTime) { return Math.abs(this.getSkewCorrectedDate().getTime() - newServerTime) >= 3e4; } }, throttledError: function throttledError(error) { if (error.statusCode === 429) return true; switch (error.code) { case ""ProvisionedThroughputExceededException"": case ""Throttling"": case ""ThrottlingException"": case ""RequestLimitExceeded"": case ""RequestThrottled"": case ""RequestThrottledException"": case ""TooManyRequestsException"": case ""TransactionInProgressException"": return true; default: return false; } }, endpointFromTemplate: function endpointFromTemplate(endpoint) { if (typeof endpoint !== ""string"") return endpoint; var e = endpoint; e = e.replace(/\{service\}/g, this.api.endpointPrefix); e = e.replace(/\{region\}/g, this.config.region); e = e.replace(/\{scheme\}/g, this.config.sslEnabled ? ""https"" : ""http""); return e; }, setEndpoint: function setEndpoint(endpoint) { this.endpoint = new AWS.Endpoint(endpoint, this.config); }, paginationConfig: function paginationConfig(operation, throwException) { var paginator = this.api.operations[operation].paginator; if (!paginator) { if (throwException) { var e = new Error(); throw AWS.util.error(e, ""No pagination configuration for "" + operation); } return null; } return paginator; } }); AWS.util.update(AWS.Service, { defineMethods: function defineMethods(svc) { AWS.util.each(svc.prototype.api.operations, function iterator(method) { if (svc.prototype[method]) return; var operation = svc.prototype.api.operations[method]; if (operation.authtype === ""none"") { svc.prototype[method] = function(params, callback) { return this.makeUnauthenticatedRequest(method, params, callback); }; } else { svc.prototype[method] = function(params, callback) { return this.makeRequest(method, params, callback); }; } }); }, defineService: function defineService(serviceIdentifier, versions, features) { AWS.Service._serviceMap[serviceIdentifier] = true; if (!Array.isArray(versions)) { features = versions; versions = []; } var svc = inherit(AWS.Service, features || {}); if (typeof serviceIdentifier === ""string"") { AWS.Service.addVersions(svc, versions); var identifier = svc.serviceIdentifier || serviceIdentifier; svc.serviceIdentifier = identifier; } else { svc.prototype.api = serviceIdentifier; AWS.Service.defineMethods(svc); } AWS.SequentialExecutor.call(this.prototype); if (!this.prototype.publisher && AWS.util.clientSideMonitoring) { var Publisher = AWS.util.clientSideMonitoring.Publisher; var configProvider = AWS.util.clientSideMonitoring.configProvider; var publisherConfig = configProvider(); this.prototype.publisher = new Publisher(publisherConfig); if (publisherConfig.enabled) { AWS.Service._clientSideMonitoring = true; } } AWS.SequentialExecutor.call(svc.prototype); AWS.Service.addDefaultMonitoringListeners(svc.prototype); return svc; }, addVersions: function addVersions(svc, versions) { if (!Array.isArray(versions)) versions = [ versions ]; svc.services = svc.services || {}; for (var i = 0; i < versions.length; i++) { if (svc.services[versions[i]] === undefined) { svc.services[versions[i]] = null; } } svc.apiVersions = Object.keys(svc.services).sort(); }, defineServiceApi: function defineServiceApi(superclass, version, apiConfig) { var svc = inherit(superclass, { serviceIdentifier: superclass.serviceIdentifier }); function setApi(api) { if (api.isApi) { svc.prototype.api = api; } else { svc.prototype.api = new Api(api); } } if (typeof version === ""string"") { if (apiConfig) { setApi(apiConfig); } else { try { setApi(AWS.apiLoader(superclass.serviceIdentifier, version)); } catch (err) { throw AWS.util.error(err, { message: ""Could not find API configuration "" + superclass.serviceIdentifier + ""-"" + version }); } } if (!Object.prototype.hasOwnProperty.call(superclass.services, version)) { superclass.apiVersions = superclass.apiVersions.concat(version).sort(); } superclass.services[version] = svc; } else { setApi(version); } AWS.Service.defineMethods(svc); return svc; }, hasService: function(identifier) { return Object.prototype.hasOwnProperty.call(AWS.Service._serviceMap, identifier); }, addDefaultMonitoringListeners: function addDefaultMonitoringListeners(attachOn) { attachOn.addNamedListener(""MONITOR_EVENTS_BUBBLE"", ""apiCallAttempt"", function EVENTS_BUBBLE(event) { var baseClass = Object.getPrototypeOf(attachOn); if (baseClass._events) baseClass.emit(""apiCallAttempt"", [ event ]); }); attachOn.addNamedListener(""CALL_EVENTS_BUBBLE"", ""apiCall"", function CALL_EVENTS_BUBBLE(event) { var baseClass = Object.getPrototypeOf(attachOn); if (baseClass._events) baseClass.emit(""apiCall"", [ event ]); }); }, _serviceMap: {} }); AWS.util.mixin(AWS.Service, AWS.SequentialExecutor); module.exports = AWS.Service; }).call(this, require(""_process"")); }, { ""./core"": 38, ""./model/api"": 64, ""./region_config"": 81, _process: 8 } ], 81: [ function(require, module, exports) { var util = require(""./util""); var regionConfig = require(""./region_config_data.json""); function generateRegionPrefix(region) { if (!region) return null; var parts = region.split(""-""); if (parts.length < 3) return null; return parts.slice(0, parts.length - 2).join(""-"") + ""-*""; } function derivedKeys(service) { var region = service.config.region; var regionPrefix = generateRegionPrefix(region); var endpointPrefix = service.api.endpointPrefix; return [ [ region, endpointPrefix ], [ regionPrefix, endpointPrefix ], [ region, ""*"" ], [ regionPrefix, ""*"" ], [ ""*"", endpointPrefix ], [ ""*"", ""*"" ] ].map(function(item) { return item[0] && item[1] ? item.join(""/"") : null; }); } function applyConfig(service, config) { util.each(config, function(key, value) { if (key === ""globalEndpoint"") return; if (service.config[key] === undefined || service.config[key] === null) { service.config[key] = value; } }); } function configureEndpoint(service) { var keys = derivedKeys(service); for (var i = 0; i < keys.length; i++) { var key = keys[i]; if (!key) continue; if (Object.prototype.hasOwnProperty.call(regionConfig.rules, key)) { var config = regionConfig.rules[key]; if (typeof config === ""string"") { config = regionConfig.patterns[config]; } if (service.config.useDualstack && util.isDualstackAvailable(service)) { config = util.copy(config); config.endpoint = ""{service}.dualstack.{region}.amazonaws.com""; } service.isGlobalEndpoint = !!config.globalEndpoint; if (!config.signatureVersion) config.signatureVersion = ""v4""; applyConfig(service, config); return; } } } module.exports = configureEndpoint; }, { ""./region_config_data.json"": 82, ""./util"": 116 } ], 82: [ function(require, module, exports) { module.exports = { rules: { ""*/*"": { endpoint: ""{service}.{region}.amazonaws.com"" }, ""cn-*/*"": { endpoint: ""{service}.{region}.amazonaws.com.cn"" }, ""*/budgets"": ""globalSSL"", ""*/cloudfront"": ""globalSSL"", ""*/iam"": ""globalSSL"", ""*/sts"": ""globalSSL"", ""*/importexport"": { endpoint: ""{service}.amazonaws.com"", signatureVersion: ""v2"", globalEndpoint: true }, ""*/route53"": { endpoint: ""https://{service}.amazonaws.com"", signatureVersion: ""v3https"", globalEndpoint: true }, ""*/waf"": ""globalSSL"", ""us-gov-*/iam"": ""globalGovCloud"", ""us-gov-*/sts"": { endpoint: ""{service}.{region}.amazonaws.com"" }, ""us-gov-west-1/s3"": ""s3signature"", ""us-west-1/s3"": ""s3signature"", ""us-west-2/s3"": ""s3signature"", ""eu-west-1/s3"": ""s3signature"", ""ap-southeast-1/s3"": ""s3signature"", ""ap-southeast-2/s3"": ""s3signature"", ""ap-northeast-1/s3"": ""s3signature"", ""sa-east-1/s3"": ""s3signature"", ""us-east-1/s3"": { endpoint: ""{service}.amazonaws.com"", signatureVersion: ""s3"" }, ""us-east-1/sdb"": { endpoint: ""{service}.amazonaws.com"", signatureVersion: ""v2"" }, ""*/sdb"": { endpoint: ""{service}.{region}.amazonaws.com"", signatureVersion: ""v2"" } }, patterns: { globalSSL: { endpoint: ""https://{service}.amazonaws.com"", globalEndpoint: true }, globalGovCloud: { endpoint: ""{service}.us-gov.amazonaws.com"" }, s3signature: { endpoint: ""{service}.{region}.amazonaws.com"", signatureVersion: ""s3"" } } }; }, {} ], 85: [ function(require, module, exports) { var AWS = require(""./core""); var inherit = AWS.util.inherit; var jmespath = require(""jmespath""); AWS.Response = inherit({ constructor: function Response(request) { this.request = request; this.data = null; this.error = null; this.retryCount = 0; this.redirectCount = 0; this.httpResponse = new AWS.HttpResponse(); if (request) { this.maxRetries = request.service.numRetries(); this.maxRedirects = request.service.config.maxRedirects; } }, nextPage: function nextPage(callback) { var config; var service = this.request.service; var operation = this.request.operation; try { config = service.paginationConfig(operation, true); } catch (e) { this.error = e; } if (!this.hasNextPage()) { if (callback) callback(this.error, null); else if (this.error) throw this.error; return null; } var params = AWS.util.copy(this.request.params); if (!this.nextPageTokens) { return callback ? callback(null, null) : null; } else { var inputTokens = config.inputToken; if (typeof inputTokens === ""string"") inputTokens = [ inputTokens ]; for (var i = 0; i < inputTokens.length; i++) { params[inputTokens[i]] = this.nextPageTokens[i]; } return service.makeRequest(this.request.operation, params, callback); } }, hasNextPage: function hasNextPage() { this.cacheNextPageTokens(); if (this.nextPageTokens) return true; if (this.nextPageTokens === undefined) return undefined; else return false; }, cacheNextPageTokens: function cacheNextPageTokens() { if (Object.prototype.hasOwnProperty.call(this, ""nextPageTokens"")) return this.nextPageTokens; this.nextPageTokens = undefined; var config = this.request.service.paginationConfig(this.request.operation); if (!config) return this.nextPageTokens; this.nextPageTokens = null; if (config.moreResults) { if (!jmespath.search(this.data, config.moreResults)) { return this.nextPageTokens; } } var exprs = config.outputToken; if (typeof exprs === ""string"") exprs = [ exprs ]; AWS.util.arrayEach.call(this, exprs, function(expr) { var output = jmespath.search(this.data, expr); if (output) { this.nextPageTokens = this.nextPageTokens || []; this.nextPageTokens.push(output); } }); return this.nextPageTokens; } }); }, { ""./core"": 38, jmespath: 7 } ], 84: [ function(require, module, exports) { var AWS = require(""./core""); var inherit = AWS.util.inherit; var jmespath = require(""jmespath""); function CHECK_ACCEPTORS(resp) { var waiter = resp.request._waiter; var acceptors = waiter.config.acceptors; var acceptorMatched = false; var state = ""retry""; acceptors.forEach(function(acceptor) { if (!acceptorMatched) { var matcher = waiter.matchers[acceptor.matcher]; if (matcher && matcher(resp, acceptor.expected, acceptor.argument)) { acceptorMatched = true; state = acceptor.state; } } }); if (!acceptorMatched && resp.error) state = ""failure""; if (state === ""success"") { waiter.setSuccess(resp); } else { waiter.setError(resp, state === ""retry""); } } AWS.ResourceWaiter = inherit({ constructor: function constructor(service, state) { this.service = service; this.state = state; this.loadWaiterConfig(this.state); }, service: null, state: null, config: null, matchers: { path: function(resp, expected, argument) { try { var result = jmespath.search(resp.data, argument); } catch (err) { return false; } return jmespath.strictDeepEqual(result, expected); }, pathAll: function(resp, expected, argument) { try { var results = jmespath.search(resp.data, argument); } catch (err) { return false; } if (!Array.isArray(results)) results = [ results ]; var numResults = results.length; if (!numResults) return false; for (var ind = 0; ind < numResults; ind++) { if (!jmespath.strictDeepEqual(results[ind], expected)) { return false; } } return true; }, pathAny: function(resp, expected, argument) { try { var results = jmespath.search(resp.data, argument); } catch (err) { return false; } if (!Array.isArray(results)) results = [ results ]; var numResults = results.length; for (var ind = 0; ind < numResults; ind++) { if (jmespath.strictDeepEqual(results[ind], expected)) { return true; } } return false; }, status: function(resp, expected) { var statusCode = resp.httpResponse.statusCode; return typeof statusCode === ""number"" && statusCode === expected; }, error: function(resp, expected) { if (typeof expected === ""string"" && resp.error) { return expected === resp.error.code; } return expected === !!resp.error; } }, listeners: new AWS.SequentialExecutor().addNamedListeners(function(add) { add(""RETRY_CHECK"", ""retry"", function(resp) { var waiter = resp.request._waiter; if (resp.error && resp.error.code === ""ResourceNotReady"") { resp.error.retryDelay = (waiter.config.delay || 0) * 1e3; } }); add(""CHECK_OUTPUT"", ""extractData"", CHECK_ACCEPTORS); add(""CHECK_ERROR"", ""extractError"", CHECK_ACCEPTORS); }), wait: function wait(params, callback) { if (typeof params === ""function"") { callback = params; params = undefined; } if (params && params.$waiter) { params = AWS.util.copy(params); if (typeof params.$waiter.delay === ""number"") { this.config.delay = params.$waiter.delay; } if (typeof params.$waiter.maxAttempts === ""number"") { this.config.maxAttempts = params.$waiter.maxAttempts; } delete params.$waiter; } var request = this.service.makeRequest(this.config.operation, params); request._waiter = this; request.response.maxRetries = this.config.maxAttempts; request.addListeners(this.listeners); if (callback) request.send(callback); return request; }, setSuccess: function setSuccess(resp) { resp.error = null; resp.data = resp.data || {}; resp.request.removeAllListeners(""extractData""); }, setError: function setError(resp, retryable) { resp.data = null; resp.error = AWS.util.error(resp.error || new Error(), { code: ""ResourceNotReady"", message: ""Resource is not in the state "" + this.state, retryable: retryable }); }, loadWaiterConfig: function loadWaiterConfig(state) { if (!this.service.api.waiters[state]) { throw new AWS.util.error(new Error(), { code: ""StateNotFoundError"", message: ""State "" + state + "" not found."" }); } this.config = AWS.util.copy(this.service.api.waiters[state]); } }); }, { ""./core"": 38, jmespath: 7 } ], 83: [ function(require, module, exports) { (function(process) { var AWS = require(""./core""); var AcceptorStateMachine = require(""./state_machine""); var inherit = AWS.util.inherit; var domain = AWS.util.domain; var jmespath = require(""jmespath""); var hardErrorStates = { success: 1, error: 1, complete: 1 }; function isTerminalState(machine) { return Object.prototype.hasOwnProperty.call(hardErrorStates, machine._asm.currentState); } var fsm = new AcceptorStateMachine(); fsm.setupStates = function() { var transition = function(_, done) { var self = this; self._haltHandlersOnError = false; self.emit(self._asm.currentState, function(err) { if (err) { if (isTerminalState(self)) { if (domain && self.domain instanceof domain.Domain) { err.domainEmitter = self; err.domain = self.domain; err.domainThrown = false; self.domain.emit(""error"", err); } else { throw err; } } else { self.response.error = err; done(err); } } else { done(self.response.error); } }); }; this.addState(""validate"", ""build"", ""error"", transition); this.addState(""build"", ""afterBuild"", ""restart"", transition); this.addState(""afterBuild"", ""sign"", ""restart"", transition); this.addState(""sign"", ""send"", ""retry"", transition); this.addState(""retry"", ""afterRetry"", ""afterRetry"", transition); this.addState(""afterRetry"", ""sign"", ""error"", transition); this.addState(""send"", ""validateResponse"", ""retry"", transition); this.addState(""validateResponse"", ""extractData"", ""extractError"", transition); this.addState(""extractError"", ""extractData"", ""retry"", transition); this.addState(""extractData"", ""success"", ""retry"", transition); this.addState(""restart"", ""build"", ""error"", transition); this.addState(""success"", ""complete"", ""complete"", transition); this.addState(""error"", ""complete"", ""complete"", transition); this.addState(""complete"", null, null, transition); }; fsm.setupStates(); AWS.Request = inherit({ constructor: function Request(service, operation, params) { var endpoint = service.endpoint; var region = service.config.region; var customUserAgent = service.config.customUserAgent; if (service.isGlobalEndpoint) region = ""us-east-1""; this.domain = domain && domain.active; this.service = service; this.operation = operation; this.params = params || {}; this.httpRequest = new AWS.HttpRequest(endpoint, region); this.httpRequest.appendToUserAgent(customUserAgent); this.startTime = service.getSkewCorrectedDate(); this.response = new AWS.Response(this); this._asm = new AcceptorStateMachine(fsm.states, ""validate""); this._haltHandlersOnError = false; AWS.SequentialExecutor.call(this); this.emit = this.emitEvent; }, send: function send(callback) { if (callback) { this.httpRequest.appendToUserAgent(""callback""); this.on(""complete"", function(resp) { callback.call(resp, resp.error, resp.data); }); } this.runTo(); return this.response; }, build: function build(callback) { return this.runTo(""send"", callback); }, runTo: function runTo(state, done) { this._asm.runTo(state, done, this); return this; }, abort: function abort() { this.removeAllListeners(""validateResponse""); this.removeAllListeners(""extractError""); this.on(""validateResponse"", function addAbortedError(resp) { resp.error = AWS.util.error(new Error(""Request aborted by user""), { code: ""RequestAbortedError"", retryable: false }); }); if (this.httpRequest.stream && !this.httpRequest.stream.didCallback) { this.httpRequest.stream.abort(); if (this.httpRequest._abortCallback) { this.httpRequest._abortCallback(); } else { this.removeAllListeners(""send""); } } return this; }, eachPage: function eachPage(callback) { callback = AWS.util.fn.makeAsync(callback, 3); function wrappedCallback(response) { callback.call(response, response.error, response.data, function(result) { if (result === false) return; if (response.hasNextPage()) { response.nextPage().on(""complete"", wrappedCallback).send(); } else { callback.call(response, null, null, AWS.util.fn.noop); } }); } this.on(""complete"", wrappedCallback).send(); }, eachItem: function eachItem(callback) { var self = this; function wrappedCallback(err, data) { if (err) return callback(err, null); if (data === null) return callback(null, null); var config = self.service.paginationConfig(self.operation); var resultKey = config.resultKey; if (Array.isArray(resultKey)) resultKey = resultKey[0]; var items = jmespath.search(data, resultKey); var continueIteration = true; AWS.util.arrayEach(items, function(item) { continueIteration = callback(null, item); if (continueIteration === false) { return AWS.util.abort; } }); return continueIteration; } this.eachPage(wrappedCallback); }, isPageable: function isPageable() { return this.service.paginationConfig(this.operation) ? true : false; }, createReadStream: function createReadStream() { var streams = AWS.util.stream; var req = this; var stream = null; if (AWS.HttpClient.streamsApiVersion === 2) { stream = new streams.PassThrough(); process.nextTick(function() { req.send(); }); } else { stream = new streams.Stream(); stream.readable = true; stream.sent = false; stream.on(""newListener"", function(event) { if (!stream.sent && event === ""data"") { stream.sent = true; process.nextTick(function() { req.send(); }); } }); } this.on(""error"", function(err) { stream.emit(""error"", err); }); this.on(""httpHeaders"", function streamHeaders(statusCode, headers, resp) { if (statusCode < 300) { req.removeListener(""httpData"", AWS.EventListeners.Core.HTTP_DATA); req.removeListener(""httpError"", AWS.EventListeners.Core.HTTP_ERROR); req.on(""httpError"", function streamHttpError(error) { resp.error = error; resp.error.retryable = false; }); var shouldCheckContentLength = false; var expectedLen; if (req.httpRequest.method !== ""HEAD"") { expectedLen = parseInt(headers[""content-length""], 10); } if (expectedLen !== undefined && !isNaN(expectedLen) && expectedLen >= 0) { shouldCheckContentLength = true; var receivedLen = 0; } var checkContentLengthAndEmit = function checkContentLengthAndEmit() { if (shouldCheckContentLength && receivedLen !== expectedLen) { stream.emit(""error"", AWS.util.error(new Error(""Stream content length mismatch. Received "" + receivedLen + "" of "" + expectedLen + "" bytes.""), { code: ""StreamContentLengthMismatch"" })); } else if (AWS.HttpClient.streamsApiVersion === 2) { stream.end(); } else { stream.emit(""end""); } }; var httpStream = resp.httpResponse.createUnbufferedStream(); if (AWS.HttpClient.streamsApiVersion === 2) { if (shouldCheckContentLength) { var lengthAccumulator = new streams.PassThrough(); lengthAccumulator._write = function(chunk) { if (chunk && chunk.length) { receivedLen += chunk.length; } return streams.PassThrough.prototype._write.apply(this, arguments); }; lengthAccumulator.on(""end"", checkContentLengthAndEmit); stream.on(""error"", function(err) { shouldCheckContentLength = false; httpStream.unpipe(lengthAccumulator); lengthAccumulator.emit(""end""); lengthAccumulator.end(); }); httpStream.pipe(lengthAccumulator).pipe(stream, { end: false }); } else { httpStream.pipe(stream); } } else { if (shouldCheckContentLength) { httpStream.on(""data"", function(arg) { if (arg && arg.length) { receivedLen += arg.length; } }); } httpStream.on(""data"", function(arg) { stream.emit(""data"", arg); }); httpStream.on(""end"", checkContentLengthAndEmit); } httpStream.on(""error"", function(err) { shouldCheckContentLength = false; stream.emit(""error"", err); }); } }); return stream; }, emitEvent: function emit(eventName, args, done) { if (typeof args === ""function"") { done = args; args = null; } if (!done) done = function() {}; if (!args) args = this.eventParameters(eventName, this.response); var origEmit = AWS.SequentialExecutor.prototype.emit; origEmit.call(this, eventName, args, function(err) { if (err) this.response.error = err; done.call(this, err); }); }, eventParameters: function eventParameters(eventName) { switch (eventName) { case ""restart"": case ""validate"": case ""sign"": case ""build"": case ""afterValidate"": case ""afterBuild"": return [ this ]; case ""error"": return [ this.response.error, this.response ]; default: return [ this.response ]; } }, presign: function presign(expires, callback) { if (!callback && typeof expires === ""function"") { callback = expires; expires = null; } return new AWS.Signers.Presign().sign(this.toGet(), expires, callback); }, isPresigned: function isPresigned() { return Object.prototype.hasOwnProperty.call(this.httpRequest.headers, ""presigned-expires""); }, toUnauthenticated: function toUnauthenticated() { this._unAuthenticated = true; this.removeListener(""validate"", AWS.EventListeners.Core.VALIDATE_CREDENTIALS); this.removeListener(""sign"", AWS.EventListeners.Core.SIGN); return this; }, toGet: function toGet() { if (this.service.api.protocol === ""query"" || this.service.api.protocol === ""ec2"") { this.removeListener(""build"", this.buildAsGet); this.addListener(""build"", this.buildAsGet); } return this; }, buildAsGet: function buildAsGet(request) { request.httpRequest.method = ""GET""; request.httpRequest.path = request.service.endpoint.path + ""?"" + request.httpRequest.body; request.httpRequest.body = """"; delete request.httpRequest.headers[""Content-Length""]; delete request.httpRequest.headers[""Content-Type""]; }, haltHandlersOnError: function haltHandlersOnError() { this._haltHandlersOnError = true; } }); AWS.Request.addPromisesToClass = function addPromisesToClass(PromiseDependency) { this.prototype.promise = function promise() { var self = this; this.httpRequest.appendToUserAgent(""promise""); return new PromiseDependency(function(resolve, reject) { self.on(""complete"", function(resp) { if (resp.error) { reject(resp.error); } else { resolve(Object.defineProperty(resp.data || {}, ""$response"", { value: resp })); } }); self.runTo(); }); }; }; AWS.Request.deletePromisesFromClass = function deletePromisesFromClass() { delete this.prototype.promise; }; AWS.util.addPromises(AWS.Request); AWS.util.mixin(AWS.Request, AWS.SequentialExecutor); }).call(this, require(""_process"")); }, { ""./core"": 38, ""./state_machine"": 115, _process: 8, jmespath: 7 } ], 115: [ function(require, module, exports) { function AcceptorStateMachine(states, state) { this.currentState = state || null; this.states = states || {}; } AcceptorStateMachine.prototype.runTo = function runTo(finalState, done, bindObject, inputError) { if (typeof finalState === ""function"") { inputError = bindObject; bindObject = done; done = finalState; finalState = null; } var self = this; var state = self.states[self.currentState]; state.fn.call(bindObject || self, inputError, function(err) { if (err) { if (state.fail) self.currentState = state.fail; else return done ? done.call(bindObject, err) : null; } else { if (state.accept) self.currentState = state.accept; else return done ? done.call(bindObject) : null; } if (self.currentState === finalState) { return done ? done.call(bindObject, err) : null; } self.runTo(finalState, done, bindObject, err); }); }; AcceptorStateMachine.prototype.addState = function addState(name, acceptState, failState, fn) { if (typeof acceptState === ""function"") { fn = acceptState; acceptState = null; failState = null; } else if (typeof failState === ""function"") { fn = failState; failState = null; } if (!this.currentState) this.currentState = name; this.states[name] = { accept: acceptState, fail: failState, fn: fn }; return this; }; module.exports = AcceptorStateMachine; }, {} ], 70: [ function(require, module, exports) { var AWS = require(""./core""); AWS.ParamValidator = AWS.util.inherit({ constructor: function ParamValidator(validation) { if (validation === true || validation === undefined) { validation = { min: true }; } this.validation = validation; }, validate: function validate(shape, params, context) { this.errors = []; this.validateMember(shape, params || {}, context || ""params""); if (this.errors.length > 1) { var msg = this.errors.join(""\n* ""); msg = ""There were "" + this.errors.length + "" validation errors:\n* "" + msg; throw AWS.util.error(new Error(msg), { code: ""MultipleValidationErrors"", errors: this.errors }); } else if (this.errors.length === 1) { throw this.errors[0]; } else { return true; } }, fail: function fail(code, message) { this.errors.push(AWS.util.error(new Error(message), { code: code })); }, validateStructure: function validateStructure(shape, params, context) { this.validateType(params, context, [ ""object"" ], ""structure""); var paramName; for (var i = 0; shape.required && i < shape.required.length; i++) { paramName = shape.required[i]; var value = params[paramName]; if (value === undefined || value === null) { this.fail(""MissingRequiredParameter"", ""Missing required key '"" + paramName + ""' in "" + context); } } for (paramName in params) { if (!Object.prototype.hasOwnProperty.call(params, paramName)) continue; var paramValue = params[paramName], memberShape = shape.members[paramName]; if (memberShape !== undefined) { var memberContext = [ context, paramName ].join("".""); this.validateMember(memberShape, paramValue, memberContext); } else { this.fail(""UnexpectedParameter"", ""Unexpected key '"" + paramName + ""' found in "" + context); } } return true; }, validateMember: function validateMember(shape, param, context) { switch (shape.type) { case ""structure"": return this.validateStructure(shape, param, context); case ""list"": return this.validateList(shape, param, context); case ""map"": return this.validateMap(shape, param, context); default: return this.validateScalar(shape, param, context); } }, validateList: function validateList(shape, params, context) { if (this.validateType(params, context, [ Array ])) { this.validateRange(shape, params.length, context, ""list member count""); for (var i = 0; i < params.length; i++) { this.validateMember(shape.member, params[i], context + ""["" + i + ""]""); } } }, validateMap: function validateMap(shape, params, context) { if (this.validateType(params, context, [ ""object"" ], ""map"")) { var mapCount = 0; for (var param in params) { if (!Object.prototype.hasOwnProperty.call(params, param)) continue; this.validateMember(shape.key, param, context + ""[key='"" + param + ""']""); this.validateMember(shape.value, params[param], context + ""['"" + param + ""']""); mapCount++; } this.validateRange(shape, mapCount, context, ""map member count""); } }, validateScalar: function validateScalar(shape, value, context) { switch (shape.type) { case null: case undefined: case ""string"": return this.validateString(shape, value, context); case ""base64"": case ""binary"": return this.validatePayload(value, context); case ""integer"": case ""float"": return this.validateNumber(shape, value, context); case ""boolean"": return this.validateType(value, context, [ ""boolean"" ]); case ""timestamp"": return this.validateType(value, context, [ Date, /^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(\.\d+)?Z$/, ""number"" ], ""Date object, ISO-8601 string, or a UNIX timestamp""); default: return this.fail(""UnkownType"", ""Unhandled type "" + shape.type + "" for "" + context); } }, validateString: function validateString(shape, value, context) { var validTypes = [ ""string"" ]; if (shape.isJsonValue) { validTypes = validTypes.concat([ ""number"", ""object"", ""boolean"" ]); } if (value !== null && this.validateType(value, context, validTypes)) { this.validateEnum(shape, value, context); this.validateRange(shape, value.length, context, ""string length""); this.validatePattern(shape, value, context); this.validateUri(shape, value, context); } }, validateUri: function validateUri(shape, value, context) { if (shape[""location""] === ""uri"") { if (value.length === 0) { this.fail(""UriParameterError"", ""Expected uri parameter to have length >= 1,"" + ' but found ""' + value + '"" for ' + context); } } }, validatePattern: function validatePattern(shape, value, context) { if (this.validation[""pattern""] && shape[""pattern""] !== undefined) { if (!new RegExp(shape[""pattern""]).test(value)) { this.fail(""PatternMatchError"", 'Provided value ""' + value + '"" ' + ""does not match regex pattern /"" + shape[""pattern""] + ""/ for "" + context); } } }, validateRange: function validateRange(shape, value, context, descriptor) { if (this.validation[""min""]) { if (shape[""min""] !== undefined && value < shape[""min""]) { this.fail(""MinRangeError"", ""Expected "" + descriptor + "" >= "" + shape[""min""] + "", but found "" + value + "" for "" + context); } } if (this.validation[""max""]) { if (shape[""max""] !== undefined && value > shape[""max""]) { this.fail(""MaxRangeError"", ""Expected "" + descriptor + "" <= "" + shape[""max""] + "", but found "" + value + "" for "" + context); } } }, validateEnum: function validateRange(shape, value, context) { if (this.validation[""enum""] && shape[""enum""] !== undefined) { if (shape[""enum""].indexOf(value) === -1) { this.fail(""EnumError"", ""Found string value of "" + value + "", but "" + ""expected "" + shape[""enum""].join(""|"") + "" for "" + context); } } }, validateType: function validateType(value, context, acceptedTypes, type) { if (value === null || value === undefined) return false; var foundInvalidType = false; for (var i = 0; i < acceptedTypes.length; i++) { if (typeof acceptedTypes[i] === ""string"") { if (typeof value === acceptedTypes[i]) return true; } else if (acceptedTypes[i] instanceof RegExp) { if ((value || """").toString().match(acceptedTypes[i])) return true; } else { if (value instanceof acceptedTypes[i]) return true; if (AWS.util.isType(value, acceptedTypes[i])) return true; if (!type && !foundInvalidType) acceptedTypes = acceptedTypes.slice(); acceptedTypes[i] = AWS.util.typeName(acceptedTypes[i]); } foundInvalidType = true; } var acceptedType = type; if (!acceptedType) { acceptedType = acceptedTypes.join("", "").replace(/,([^,]+)$/, "", or$1""); } var vowel = acceptedType.match(/^[aeiou]/i) ? ""n"" : """"; this.fail(""InvalidParameterType"", ""Expected "" + context + "" to be a"" + vowel + "" "" + acceptedType); return false; }, validateNumber: function validateNumber(shape, value, context) { if (value === null || value === undefined) return; if (typeof value === ""string"") { var castedValue = parseFloat(value); if (castedValue.toString() === value) value = castedValue; } if (this.validateType(value, context, [ ""number"" ])) { this.validateRange(shape, value, context, ""numeric value""); } }, validatePayload: function validatePayload(value, context) { if (value === null || value === undefined) return; if (typeof value === ""string"") return; if (value && typeof value.byteLength === ""number"") return; if (AWS.util.isNode()) { var Stream = AWS.util.stream.Stream; if (AWS.util.Buffer.isBuffer(value) || value instanceof Stream) return; } else { if (typeof Blob !== void 0 && value instanceof Blob) return; } var types = [ ""Buffer"", ""Stream"", ""File"", ""Blob"", ""ArrayBuffer"", ""DataView"" ]; if (value) { for (var i = 0; i < types.length; i++) { if (AWS.util.isType(value, types[i])) return; if (AWS.util.typeName(value.constructor) === types[i]) return; } } this.fail(""InvalidParameterType"", ""Expected "" + context + "" to be a "" + ""string, Buffer, Stream, Blob, or typed array object""); } }); }, { ""./core"": 38 } ], 64: [ function(require, module, exports) { var Collection = require(""./collection""); var Operation = require(""./operation""); var Shape = require(""./shape""); var Paginator = require(""./paginator""); var ResourceWaiter = require(""./resource_waiter""); var util = require(""../util""); var property = util.property; var memoizedProperty = util.memoizedProperty; function Api(api, options) { var self = this; api = api || {}; options = options || {}; options.api = this; api.metadata = api.metadata || {}; property(this, ""isApi"", true, false); property(this, ""apiVersion"", api.metadata.apiVersion); property(this, ""endpointPrefix"", api.metadata.endpointPrefix); property(this, ""signingName"", api.metadata.signingName); property(this, ""globalEndpoint"", api.metadata.globalEndpoint); property(this, ""signatureVersion"", api.metadata.signatureVersion); property(this, ""jsonVersion"", api.metadata.jsonVersion); property(this, ""targetPrefix"", api.metadata.targetPrefix); property(this, ""protocol"", api.metadata.protocol); property(this, ""timestampFormat"", api.metadata.timestampFormat); property(this, ""xmlNamespaceUri"", api.metadata.xmlNamespace); property(this, ""abbreviation"", api.metadata.serviceAbbreviation); property(this, ""fullName"", api.metadata.serviceFullName); property(this, ""serviceId"", api.metadata.serviceId); memoizedProperty(this, ""className"", function() { var name = api.metadata.serviceAbbreviation || api.metadata.serviceFullName; if (!name) return null; name = name.replace(/^Amazon|AWS\s*|\(.*|\s+|\W+/g, """"); if (name === ""ElasticLoadBalancing"") name = ""ELB""; return name; }); function addEndpointOperation(name, operation) { if (operation.endpointoperation === true) { property(self, ""endpointOperation"", util.string.lowerFirst(name)); } } property(this, ""operations"", new Collection(api.operations, options, function(name, operation) { return new Operation(name, operation, options); }, util.string.lowerFirst, addEndpointOperation)); property(this, ""shapes"", new Collection(api.shapes, options, function(name, shape) { return Shape.create(shape, options); })); property(this, ""paginators"", new Collection(api.paginators, options, function(name, paginator) { return new Paginator(name, paginator, options); })); property(this, ""waiters"", new Collection(api.waiters, options, function(name, waiter) { return new ResourceWaiter(name, waiter, options); }, util.string.lowerFirst)); if (options.documentation) { property(this, ""documentation"", api.documentation); property(this, ""documentationUrl"", api.documentationUrl); } } module.exports = Api; }, { ""../util"": 116, ""./collection"": 65, ""./operation"": 66, ""./paginator"": 67, ""./resource_waiter"": 68, ""./shape"": 69 } ], 68: [ function(require, module, exports) { var util = require(""../util""); var property = util.property; function ResourceWaiter(name, waiter, options) { options = options || {}; property(this, ""name"", name); property(this, ""api"", options.api, false); if (waiter.operation) { property(this, ""operation"", util.string.lowerFirst(waiter.operation)); } var self = this; var keys = [ ""type"", ""description"", ""delay"", ""maxAttempts"", ""acceptors"" ]; keys.forEach(function(key) { var value = waiter[key]; if (value) { property(self, key, value); } }); } module.exports = ResourceWaiter; }, { ""../util"": 116 } ], 67: [ function(require, module, exports) { var property = require(""../util"").property; function Paginator(name, paginator) { property(this, ""inputToken"", paginator.input_token); property(this, ""limitKey"", paginator.limit_key); property(this, ""moreResults"", paginator.more_results); property(this, ""outputToken"", paginator.output_token); property(this, ""resultKey"", paginator.result_key); } module.exports = Paginator; }, { ""../util"": 116 } ], 66: [ function(require, module, exports) { var Shape = require(""./shape""); var util = require(""../util""); var property = util.property; var memoizedProperty = util.memoizedProperty; function Operation(name, operation, options) { var self = this; options = options || {}; property(this, ""name"", operation.name || name); property(this, ""api"", options.api, false); operation.http = operation.http || {}; property(this, ""endpoint"", operation.endpoint); property(this, ""httpMethod"", operation.http.method || ""POST""); property(this, ""httpPath"", operation.http.requestUri || ""/""); property(this, ""authtype"", operation.authtype || """"); property(this, ""endpointDiscoveryRequired"", operation.endpointdiscovery ? operation.endpointdiscovery.required ? ""REQUIRED"" : ""OPTIONAL"" : ""NULL""); memoizedProperty(this, ""input"", function() { if (!operation.input) { return new Shape.create({ type: ""structure"" }, options); } return Shape.create(operation.input, options); }); memoizedProperty(this, ""output"", function() { if (!operation.output) { return new Shape.create({ type: ""structure"" }, options); } return Shape.create(operation.output, options); }); memoizedProperty(this, ""errors"", function() { var list = []; if (!operation.errors) return null; for (var i = 0; i < operation.errors.length; i++) { list.push(Shape.create(operation.errors[i], options)); } return list; }); memoizedProperty(this, ""paginator"", function() { return options.api.paginators[name]; }); if (options.documentation) { property(this, ""documentation"", operation.documentation); property(this, ""documentationUrl"", operation.documentationUrl); } memoizedProperty(this, ""idempotentMembers"", function() { var idempotentMembers = []; var input = self.input; var members = input.members; if (!input.members) { return idempotentMembers; } for (var name in members) { if (!members.hasOwnProperty(name)) { continue; } if (members[name].isIdempotent === true) { idempotentMembers.push(name); } } return idempotentMembers; }); memoizedProperty(this, ""hasEventOutput"", function() { var output = self.output; return hasEventStream(output); }); } function hasEventStream(topLevelShape) { var members = topLevelShape.members; var payload = topLevelShape.payload; if (!topLevelShape.members) { return false; } if (payload) { var payloadMember = members[payload]; return payloadMember.isEventStream; } for (var name in members) { if (!members.hasOwnProperty(name)) { if (members[name].isEventStream === true) { return true; } } } return false; } module.exports = Operation; }, { ""../util"": 116, ""./shape"": 69 } ], 60: [ function(require, module, exports) { var AWS = require(""./core""); var inherit = AWS.util.inherit; AWS.Endpoint = inherit({ constructor: function Endpoint(endpoint, config) { AWS.util.hideProperties(this, [ ""slashes"", ""auth"", ""hash"", ""search"", ""query"" ]); if (typeof endpoint === ""undefined"" || endpoint === null) { throw new Error(""Invalid endpoint: "" + endpoint); } else if (typeof endpoint !== ""string"") { return AWS.util.copy(endpoint); } if (!endpoint.match(/^http/)) { var useSSL = config && config.sslEnabled !== undefined ? config.sslEnabled : AWS.config.sslEnabled; endpoint = (useSSL ? ""https"" : ""http"") + ""://"" + endpoint; } AWS.util.update(this, AWS.util.urlParse(endpoint)); if (this.port) { this.port = parseInt(this.port, 10); } else { this.port = this.protocol === ""https:"" ? 443 : 80; } } }); AWS.HttpRequest = inherit({ constructor: function HttpRequest(endpoint, region) { endpoint = new AWS.Endpoint(endpoint); this.method = ""POST""; this.path = endpoint.path || ""/""; this.headers = {}; this.body = """"; this.endpoint = endpoint; this.region = region; this._userAgent = """"; this.setUserAgent(); }, setUserAgent: function setUserAgent() { this._userAgent = this.headers[this.getUserAgentHeaderName()] = AWS.util.userAgent(); }, getUserAgentHeaderName: function getUserAgentHeaderName() { var prefix = AWS.util.isBrowser() ? ""X-Amz-"" : """"; return prefix + ""User-Agent""; }, appendToUserAgent: function appendToUserAgent(agentPartial) { if (typeof agentPartial === ""string"" && agentPartial) { this._userAgent += "" "" + agentPartial; } this.headers[this.getUserAgentHeaderName()] = this._userAgent; }, getUserAgent: function getUserAgent() { return this._userAgent; }, pathname: function pathname() { return this.path.split(""?"", 1)[0]; }, search: function search() { var query = this.path.split(""?"", 2)[1]; if (query) { query = AWS.util.queryStringParse(query); return AWS.util.queryParamsToString(query); } return """"; }, updateEndpoint: function updateEndpoint(endpointStr) { var newEndpoint = new AWS.Endpoint(endpointStr); this.endpoint = newEndpoint; this.path = newEndpoint.path || ""/""; } }); AWS.HttpResponse = inherit({ constructor: function HttpResponse() { this.statusCode = undefined; this.headers = {}; this.body = undefined; this.streaming = false; this.stream = null; }, createUnbufferedStream: function createUnbufferedStream() { this.streaming = true; return this.stream; } }); AWS.HttpClient = inherit({}); AWS.HttpClient.getInstance = function getInstance() { if (this.singleton === undefined) { this.singleton = new this(); } return this.singleton; }; }, { ""./core"": 38 } ], 59: [ function(require, module, exports) { var AWS = require(""./core""); var SequentialExecutor = require(""./sequential_executor""); var DISCOVER_ENDPOINT = require(""./discover_endpoint"").discoverEndpoint; AWS.EventListeners = { Core: {} }; function getOperationAuthtype(req) { if (!req.service.api.operations) { return """"; } var operation = req.service.api.operations[req.operation]; return operation ? operation.authtype : """"; } AWS.EventListeners = { Core: new SequentialExecutor().addNamedListeners(function(add, addAsync) { addAsync(""VALIDATE_CREDENTIALS"", ""validate"", function VALIDATE_CREDENTIALS(req, done) { if (!req.service.api.signatureVersion && !req.service.config.signatureVersion) return done(); req.service.config.getCredentials(function(err) { if (err) { req.response.error = AWS.util.error(err, { code: ""CredentialsError"", message: ""Missing credentials in config"" }); } done(); }); }); add(""VALIDATE_REGION"", ""validate"", function VALIDATE_REGION(req) { if (!req.service.config.region && !req.service.isGlobalEndpoint) { req.response.error = AWS.util.error(new Error(), { code: ""ConfigError"", message: ""Missing region in config"" }); } }); add(""BUILD_IDEMPOTENCY_TOKENS"", ""validate"", function BUILD_IDEMPOTENCY_TOKENS(req) { if (!req.service.api.operations) { return; } var operation = req.service.api.operations[req.operation]; if (!operation) { return; } var idempotentMembers = operation.idempotentMembers; if (!idempotentMembers.length) { return; } var params = AWS.util.copy(req.params); for (var i = 0, iLen = idempotentMembers.length; i < iLen; i++) { if (!params[idempotentMembers[i]]) { params[idempotentMembers[i]] = AWS.util.uuid.v4(); } } req.params = params; }); add(""VALIDATE_PARAMETERS"", ""validate"", function VALIDATE_PARAMETERS(req) { if (!req.service.api.operations) { return; } var rules = req.service.api.operations[req.operation].input; var validation = req.service.config.paramValidation; new AWS.ParamValidator(validation).validate(rules, req.params); }); addAsync(""COMPUTE_SHA256"", ""afterBuild"", function COMPUTE_SHA256(req, done) { req.haltHandlersOnError(); if (!req.service.api.operations) { return; } var operation = req.service.api.operations[req.operation]; var authtype = operation ? operation.authtype : """"; if (!req.service.api.signatureVersion && !authtype && !req.service.config.signatureVersion) return done(); if (req.service.getSignerClass(req) === AWS.Signers.V4) { var body = req.httpRequest.body || """"; if (authtype.indexOf(""unsigned-body"") >= 0) { req.httpRequest.headers[""X-Amz-Content-Sha256""] = ""UNSIGNED-PAYLOAD""; return done(); } AWS.util.computeSha256(body, function(err, sha) { if (err) { done(err); } else { req.httpRequest.headers[""X-Amz-Content-Sha256""] = sha; done(); } }); } else { done(); } }); add(""SET_CONTENT_LENGTH"", ""afterBuild"", function SET_CONTENT_LENGTH(req) { var authtype = getOperationAuthtype(req); var payloadMember = AWS.util.getRequestPayloadShape(req); if (req.httpRequest.headers[""Content-Length""] === undefined) { try { var length = AWS.util.string.byteLength(req.httpRequest.body); req.httpRequest.headers[""Content-Length""] = length; } catch (err) { if (payloadMember && payloadMember.isStreaming) { if (payloadMember.requiresLength) { throw err; } else if (authtype.indexOf(""unsigned-body"") >= 0) { req.httpRequest.headers[""Transfer-Encoding""] = ""chunked""; return; } else { throw err; } } throw err; } } }); add(""SET_HTTP_HOST"", ""afterBuild"", function SET_HTTP_HOST(req) { req.httpRequest.headers[""Host""] = req.httpRequest.endpoint.host; }); add(""RESTART"", ""restart"", function RESTART() { var err = this.response.error; if (!err || !err.retryable) return; this.httpRequest = new AWS.HttpRequest(this.service.endpoint, this.service.region); if (this.response.retryCount < this.service.config.maxRetries) { this.response.retryCount++; } else { this.response.error = null; } }); var addToHead = true; addAsync(""DISCOVER_ENDPOINT"", ""sign"", DISCOVER_ENDPOINT, addToHead); addAsync(""SIGN"", ""sign"", function SIGN(req, done) { var service = req.service; var operations = req.service.api.operations || {}; var operation = operations[req.operation]; var authtype = operation ? operation.authtype : """"; if (!service.api.signatureVersion && !authtype && !service.config.signatureVersion) return done(); service.config.getCredentials(function(err, credentials) { if (err) { req.response.error = err; return done(); } try { var date = service.getSkewCorrectedDate(); var SignerClass = service.getSignerClass(req); var signer = new SignerClass(req.httpRequest, service.api.signingName || service.api.endpointPrefix, { signatureCache: service.config.signatureCache, operation: operation, signatureVersion: service.api.signatureVersion }); signer.setServiceClientId(service._clientId); delete req.httpRequest.headers[""Authorization""]; delete req.httpRequest.headers[""Date""]; delete req.httpRequest.headers[""X-Amz-Date""]; signer.addAuthorization(credentials, date); req.signedAt = date; } catch (e) { req.response.error = e; } done(); }); }); add(""VALIDATE_RESPONSE"", ""validateResponse"", function VALIDATE_RESPONSE(resp) { if (this.service.successfulResponse(resp, this)) { resp.data = {}; resp.error = null; } else { resp.data = null; resp.error = AWS.util.error(new Error(), { code: ""UnknownError"", message: ""An unknown error occurred."" }); } }); addAsync(""SEND"", ""send"", function SEND(resp, done) { resp.httpResponse._abortCallback = done; resp.error = null; resp.data = null; function callback(httpResp) { resp.httpResponse.stream = httpResp; var stream = resp.request.httpRequest.stream; var service = resp.request.service; var api = service.api; var operationName = resp.request.operation; var operation = api.operations[operationName] || {}; httpResp.on(""headers"", function onHeaders(statusCode, headers, statusMessage) { resp.request.emit(""httpHeaders"", [ statusCode, headers, resp, statusMessage ]); if (!resp.httpResponse.streaming) { if (AWS.HttpClient.streamsApiVersion === 2) { if (operation.hasEventOutput && service.successfulResponse(resp)) { resp.request.emit(""httpDone""); done(); return; } httpResp.on(""readable"", function onReadable() { var data = httpResp.read(); if (data !== null) { resp.request.emit(""httpData"", [ data, resp ]); } }); } else { httpResp.on(""data"", function onData(data) { resp.request.emit(""httpData"", [ data, resp ]); }); } } }); httpResp.on(""end"", function onEnd() { if (!stream || !stream.didCallback) { if (AWS.HttpClient.streamsApiVersion === 2 && (operation.hasEventOutput && service.successfulResponse(resp))) { return; } resp.request.emit(""httpDone""); done(); } }); } function progress(httpResp) { httpResp.on(""sendProgress"", function onSendProgress(value) { resp.request.emit(""httpUploadProgress"", [ value, resp ]); }); httpResp.on(""receiveProgress"", function onReceiveProgress(value) { resp.request.emit(""httpDownloadProgress"", [ value, resp ]); }); } function error(err) { if (err.code !== ""RequestAbortedError"") { var errCode = err.code === ""TimeoutError"" ? err.code : ""NetworkingError""; err = AWS.util.error(err, { code: errCode, region: resp.request.httpRequest.region, hostname: resp.request.httpRequest.endpoint.hostname, retryable: true }); } resp.error = err; resp.request.emit(""httpError"", [ resp.error, resp ], function() { done(); }); } function executeSend() { var http = AWS.HttpClient.getInstance(); var httpOptions = resp.request.service.config.httpOptions || {}; try { var stream = http.handleRequest(resp.request.httpRequest, httpOptions, callback, error); progress(stream); } catch (err) { error(err); } } var timeDiff = (resp.request.service.getSkewCorrectedDate() - this.signedAt) / 1e3; if (timeDiff >= 60 * 10) { this.emit(""sign"", [ this ], function(err) { if (err) done(err); else executeSend(); }); } else { executeSend(); } }); add(""HTTP_HEADERS"", ""httpHeaders"", function HTTP_HEADERS(statusCode, headers, resp, statusMessage) { resp.httpResponse.statusCode = statusCode; resp.httpResponse.statusMessage = statusMessage; resp.httpResponse.headers = headers; resp.httpResponse.body = AWS.util.buffer.toBuffer(""""); resp.httpResponse.buffers = []; resp.httpResponse.numBytes = 0; var dateHeader = headers.date || headers.Date; var service = resp.request.service; if (dateHeader) { var serverTime = Date.parse(dateHeader); if (service.config.correctClockSkew && service.isClockSkewed(serverTime)) { service.applyClockOffset(serverTime); } } }); add(""HTTP_DATA"", ""httpData"", function HTTP_DATA(chunk, resp) { if (chunk) { if (AWS.util.isNode()) { resp.httpResponse.numBytes += chunk.length; var total = resp.httpResponse.headers[""content-length""]; var progress = { loaded: resp.httpResponse.numBytes, total: total }; resp.request.emit(""httpDownloadProgress"", [ progress, resp ]); } resp.httpResponse.buffers.push(AWS.util.buffer.toBuffer(chunk)); } }); add(""HTTP_DONE"", ""httpDone"", function HTTP_DONE(resp) { if (resp.httpResponse.buffers && resp.httpResponse.buffers.length > 0) { var body = AWS.util.buffer.concat(resp.httpResponse.buffers); resp.httpResponse.body = body; } delete resp.httpResponse.numBytes; delete resp.httpResponse.buffers; }); add(""FINALIZE_ERROR"", ""retry"", function FINALIZE_ERROR(resp) { if (resp.httpResponse.statusCode) { resp.error.statusCode = resp.httpResponse.statusCode; if (resp.error.retryable === undefined) { resp.error.retryable = this.service.retryableError(resp.error, this); } } }); add(""INVALIDATE_CREDENTIALS"", ""retry"", function INVALIDATE_CREDENTIALS(resp) { if (!resp.error) return; switch (resp.error.code) { case ""RequestExpired"": case ""ExpiredTokenException"": case ""ExpiredToken"": resp.error.retryable = true; resp.request.service.config.credentials.expired = true; } }); add(""EXPIRED_SIGNATURE"", ""retry"", function EXPIRED_SIGNATURE(resp) { var err = resp.error; if (!err) return; if (typeof err.code === ""string"" && typeof err.message === ""string"") { if (err.code.match(/Signature/) && err.message.match(/expired/)) { resp.error.retryable = true; } } }); add(""CLOCK_SKEWED"", ""retry"", function CLOCK_SKEWED(resp) { if (!resp.error) return; if (this.service.clockSkewError(resp.error) && this.service.config.correctClockSkew) { resp.error.retryable = true; } }); add(""REDIRECT"", ""retry"", function REDIRECT(resp) { if (resp.error && resp.error.statusCode >= 300 && resp.error.statusCode < 400 && resp.httpResponse.headers[""location""]) { this.httpRequest.endpoint = new AWS.Endpoint(resp.httpResponse.headers[""location""]); this.httpRequest.headers[""Host""] = this.httpRequest.endpoint.host; resp.error.redirect = true; resp.error.retryable = true; } }); add(""RETRY_CHECK"", ""retry"", function RETRY_CHECK(resp) { if (resp.error) { if (resp.error.redirect && resp.redirectCount < resp.maxRedirects) { resp.error.retryDelay = 0; } else if (resp.retryCount < resp.maxRetries) { resp.error.retryDelay = this.service.retryDelays(resp.retryCount) || 0; } } }); addAsync(""RESET_RETRY_STATE"", ""afterRetry"", function RESET_RETRY_STATE(resp, done) { var delay, willRetry = false; if (resp.error) { delay = resp.error.retryDelay || 0; if (resp.error.retryable && resp.retryCount < resp.maxRetries) { resp.retryCount++; willRetry = true; } else if (resp.error.redirect && resp.redirectCount < resp.maxRedirects) { resp.redirectCount++; willRetry = true; } } if (willRetry) { resp.error = null; setTimeout(done, delay); } else { done(); } }); }), CorePost: new SequentialExecutor().addNamedListeners(function(add) { add(""EXTRACT_REQUEST_ID"", ""extractData"", AWS.util.extractRequestId); add(""EXTRACT_REQUEST_ID"", ""extractError"", AWS.util.extractRequestId); add(""ENOTFOUND_ERROR"", ""httpError"", function ENOTFOUND_ERROR(err) { if (err.code === ""NetworkingError"" && err.errno === ""ENOTFOUND"") { var message = ""Inaccessible host: `"" + err.hostname + ""'. This service may not be available in the `"" + err.region + ""' region.""; this.response.error = AWS.util.error(new Error(message), { code: ""UnknownEndpoint"", region: err.region, hostname: err.hostname, retryable: true, originalError: err }); } }); }), Logger: new SequentialExecutor().addNamedListeners(function(add) { add(""LOG_REQUEST"", ""complete"", function LOG_REQUEST(resp) { var req = resp.request; var logger = req.service.config.logger; if (!logger) return; function filterSensitiveLog(inputShape, shape) { if (!shape) { return shape; } switch (inputShape.type) { case ""structure"": var struct = {}; AWS.util.each(shape, function(subShapeName, subShape) { if (Object.prototype.hasOwnProperty.call(inputShape.members, subShapeName)) { struct[subShapeName] = filterSensitiveLog(inputShape.members[subShapeName], subShape); } else { struct[subShapeName] = subShape; } }); return struct; case ""list"": var list = []; AWS.util.arrayEach(shape, function(subShape, index) { list.push(filterSensitiveLog(inputShape.member, subShape)); }); return list; case ""map"": var map = {}; AWS.util.each(shape, function(key, value) { map[key] = filterSensitiveLog(inputShape.value, value); }); return map; default: if (inputShape.isSensitive) { return ""***SensitiveInformation***""; } else { return shape; } } } function buildMessage() { var time = resp.request.service.getSkewCorrectedDate().getTime(); var delta = (time - req.startTime.getTime()) / 1e3; var ansi = logger.isTTY ? true : false; var status = resp.httpResponse.statusCode; var censoredParams = req.params; if (req.service.api.operations && req.service.api.operations[req.operation] && req.service.api.operations[req.operation].input) { var inputShape = req.service.api.operations[req.operation].input; censoredParams = filterSensitiveLog(inputShape, req.params); } var params = require(""util"").inspect(censoredParams, true, null); var message = """"; if (ansi) message += ""[33m""; message += ""[AWS "" + req.service.serviceIdentifier + "" "" + status; message += "" "" + delta.toString() + ""s "" + resp.retryCount + "" retries]""; if (ansi) message += ""[0;1m""; message += "" "" + AWS.util.string.lowerFirst(req.operation); message += ""("" + params + "")""; if (ansi) message += ""[0m""; return message; } var line = buildMessage(); if (typeof logger.log === ""function"") { logger.log(line); } else if (typeof logger.write === ""function"") { logger.write(line + ""\n""); } }); }), Json: new SequentialExecutor().addNamedListeners(function(add) { var svc = require(""./protocol/json""); add(""BUILD"", ""build"", svc.buildRequest); add(""EXTRACT_DATA"", ""extractData"", svc.extractData); add(""EXTRACT_ERROR"", ""extractError"", svc.extractError); }), Rest: new SequentialExecutor().addNamedListeners(function(add) { var svc = require(""./protocol/rest""); add(""BUILD"", ""build"", svc.buildRequest); add(""EXTRACT_DATA"", ""extractData"", svc.extractData); add(""EXTRACT_ERROR"", ""extractError"", svc.extractError); }), RestJson: new SequentialExecutor().addNamedListeners(function(add) { var svc = require(""./protocol/rest_json""); add(""BUILD"", ""build"", svc.buildRequest); add(""EXTRACT_DATA"", ""extractData"", svc.extractData); add(""EXTRACT_ERROR"", ""extractError"", svc.extractError); }), RestXml: new SequentialExecutor().addNamedListeners(function(add) { var svc = require(""./protocol/rest_xml""); add(""BUILD"", ""build"", svc.buildRequest); add(""EXTRACT_DATA"", ""extractData"", svc.extractData); add(""EXTRACT_ERROR"", ""extractError"", svc.extractError); }), Query: new SequentialExecutor().addNamedListeners(function(add) { var svc = require(""./protocol/query""); add(""BUILD"", ""build"", svc.buildRequest); add(""EXTRACT_DATA"", ""extractData"", svc.extractData); add(""EXTRACT_ERROR"", ""extractError"", svc.extractError); }) }; }, { ""./core"": 38, ""./discover_endpoint"": 46, ""./protocol/json"": 73, ""./protocol/query"": 74, ""./protocol/rest"": 75, ""./protocol/rest_json"": 76, ""./protocol/rest_xml"": 77, ""./sequential_executor"": 87, util: 20 } ], 87: [ function(require, module, exports) { var AWS = require(""./core""); AWS.SequentialExecutor = AWS.util.inherit({ constructor: function SequentialExecutor() { this._events = {}; }, listeners: function listeners(eventName) { return this._events[eventName] ? this._events[eventName].slice(0) : []; }, on: function on(eventName, listener, toHead) { if (this._events[eventName]) { toHead ? this._events[eventName].unshift(listener) : this._events[eventName].push(listener); } else { this._events[eventName] = [ listener ]; } return this; }, onAsync: function onAsync(eventName, listener, toHead) { listener._isAsync = true; return this.on(eventName, listener, toHead); }, removeListener: function removeListener(eventName, listener) { var listeners = this._events[eventName]; if (listeners) { var length = listeners.length; var position = -1; for (var i = 0; i < length; ++i) { if (listeners[i] === listener) { position = i; } } if (position > -1) { listeners.splice(position, 1); } } return this; }, removeAllListeners: function removeAllListeners(eventName) { if (eventName) { delete this._events[eventName]; } else { this._events = {}; } return this; }, emit: function emit(eventName, eventArgs, doneCallback) { if (!doneCallback) doneCallback = function() {}; var listeners = this.listeners(eventName); var count = listeners.length; this.callListeners(listeners, eventArgs, doneCallback); return count > 0; }, callListeners: function callListeners(listeners, args, doneCallback, prevError) { var self = this; var error = prevError || null; function callNextListener(err) { if (err) { error = AWS.util.error(error || new Error(), err); if (self._haltHandlersOnError) { return doneCallback.call(self, error); } } self.callListeners(listeners, args, doneCallback, error); } while (listeners.length > 0) { var listener = listeners.shift(); if (listener._isAsync) { listener.apply(self, args.concat([ callNextListener ])); return; } else { try { listener.apply(self, args); } catch (err) { error = AWS.util.error(error || new Error(), err); } if (error && self._haltHandlersOnError) { doneCallback.call(self, error); return; } } } doneCallback.call(self, error); }, addListeners: function addListeners(listeners) { var self = this; if (listeners._events) listeners = listeners._events; AWS.util.each(listeners, function(event, callbacks) { if (typeof callbacks === ""function"") callbacks = [ callbacks ]; AWS.util.arrayEach(callbacks, function(callback) { self.on(event, callback); }); }); return self; }, addNamedListener: function addNamedListener(name, eventName, callback, toHead) { this[name] = callback; this.addListener(eventName, callback, toHead); return this; }, addNamedAsyncListener: function addNamedAsyncListener(name, eventName, callback, toHead) { callback._isAsync = true; return this.addNamedListener(name, eventName, callback, toHead); }, addNamedListeners: function addNamedListeners(callback) { var self = this; callback(function() { self.addNamedListener.apply(self, arguments); }, function() { self.addNamedAsyncListener.apply(self, arguments); }); return this; } }); AWS.SequentialExecutor.prototype.addListener = AWS.SequentialExecutor.prototype.on; module.exports = AWS.SequentialExecutor; }, { ""./core"": 38 } ], 77: [ function(require, module, exports) { var AWS = require(""../core""); var util = require(""../util""); var Rest = require(""./rest""); function populateBody(req) { var input = req.service.api.operations[req.operation].input; var builder = new AWS.XML.Builder(); var params = req.params; var payload = input.payload; if (payload) { var payloadMember = input.members[payload]; params = params[payload]; if (params === undefined) return; if (payloadMember.type === ""structure"") { var rootElement = payloadMember.name; req.httpRequest.body = builder.toXML(params, payloadMember, rootElement, true); } else { req.httpRequest.body = params; } } else { req.httpRequest.body = builder.toXML(params, input, input.name || input.shape || util.string.upperFirst(req.operation) + ""Request""); } } function buildRequest(req) { Rest.buildRequest(req); if ([ ""GET"", ""HEAD"" ].indexOf(req.httpRequest.method) < 0) { populateBody(req); } } function extractError(resp) { Rest.extractError(resp); var data; try { data = new AWS.XML.Parser().parse(resp.httpResponse.body.toString()); } catch (e) { data = { Code: resp.httpResponse.statusCode, Message: resp.httpResponse.statusMessage }; } if (data.Errors) data = data.Errors; if (data.Error) data = data.Error; if (data.Code) { resp.error = util.error(new Error(), { code: data.Code, message: data.Message }); } else { resp.error = util.error(new Error(), { code: resp.httpResponse.statusCode, message: null }); } } function extractData(resp) { Rest.extractData(resp); var parser; var req = resp.request; var body = resp.httpResponse.body; var operation = req.service.api.operations[req.operation]; var output = operation.output; var hasEventOutput = operation.hasEventOutput; var payload = output.payload; if (payload) { var payloadMember = output.members[payload]; if (payloadMember.isEventStream) { parser = new AWS.XML.Parser(); resp.data[payload] = util.createEventStream(AWS.HttpClient.streamsApiVersion === 2 ? resp.httpResponse.stream : resp.httpResponse.body, parser, payloadMember); } else if (payloadMember.type === ""structure"") { parser = new AWS.XML.Parser(); resp.data[payload] = parser.parse(body.toString(), payloadMember); } else if (payloadMember.type === ""binary"" || payloadMember.isStreaming) { resp.data[payload] = body; } else { resp.data[payload] = payloadMember.toType(body); } } else if (body.length > 0) { parser = new AWS.XML.Parser(); var data = parser.parse(body.toString(), output); util.update(resp.data, data); } } module.exports = { buildRequest: buildRequest, extractError: extractError, extractData: extractData }; }, { ""../core"": 38, ""../util"": 116, ""./rest"": 75 } ], 76: [ function(require, module, exports) { var util = require(""../util""); var Rest = require(""./rest""); var Json = require(""./json""); var JsonBuilder = require(""../json/builder""); var JsonParser = require(""../json/parser""); function populateBody(req) { var builder = new JsonBuilder(); var input = req.service.api.operations[req.operation].input; if (input.payload) { var params = {}; var payloadShape = input.members[input.payload]; params = req.params[input.payload]; if (params === undefined) return; if (payloadShape.type === ""structure"") { req.httpRequest.body = builder.build(params, payloadShape); applyContentTypeHeader(req); } else { req.httpRequest.body = params; if (payloadShape.type === ""binary"" || payloadShape.isStreaming) { applyContentTypeHeader(req, true); } } } else { var body = builder.build(req.params, input); if (body !== ""{}"" || req.httpRequest.method !== ""GET"") { req.httpRequest.body = body; } applyContentTypeHeader(req); } } function applyContentTypeHeader(req, isBinary) { var operation = req.service.api.operations[req.operation]; var input = operation.input; if (!req.httpRequest.headers[""Content-Type""]) { var type = isBinary ? ""binary/octet-stream"" : ""application/json""; req.httpRequest.headers[""Content-Type""] = type; } } function buildRequest(req) { Rest.buildRequest(req); if ([ ""HEAD"", ""DELETE"" ].indexOf(req.httpRequest.method) < 0) { populateBody(req); } } function extractError(resp) { Json.extractError(resp); } function extractData(resp) { Rest.extractData(resp); var req = resp.request; var operation = req.service.api.operations[req.operation]; var rules = req.service.api.operations[req.operation].output || {}; var parser; var hasEventOutput = operation.hasEventOutput; if (rules.payload) { var payloadMember = rules.members[rules.payload]; var body = resp.httpResponse.body; if (payloadMember.isEventStream) { parser = new JsonParser(); resp.data[payload] = util.createEventStream(AWS.HttpClient.streamsApiVersion === 2 ? resp.httpResponse.stream : body, parser, payloadMember); } else if (payloadMember.type === ""structure"" || payloadMember.type === ""list"") { var parser = new JsonParser(); resp.data[rules.payload] = parser.parse(body, payloadMember); } else if (payloadMember.type === ""binary"" || payloadMember.isStreaming) { resp.data[rules.payload] = body; } else { resp.data[rules.payload] = payloadMember.toType(body); } } else { var data = resp.data; Json.extractData(resp); resp.data = util.merge(data, resp.data); } } module.exports = { buildRequest: buildRequest, extractError: extractError, extractData: extractData }; }, { ""../json/builder"": 62, ""../json/parser"": 63, ""../util"": 116, ""./json"": 73, ""./rest"": 75 } ], 75: [ function(require, module, exports) { var util = require(""../util""); var populateHostPrefix = require(""./helpers"").populateHostPrefix; function populateMethod(req) { req.httpRequest.method = req.service.api.operations[req.operation].httpMethod; } function generateURI(endpointPath, operationPath, input, params) { var uri = [ endpointPath, operationPath ].join(""/""); uri = uri.replace(/\/+/g, ""/""); var queryString = {}, queryStringSet = false; util.each(input.members, function(name, member) { var paramValue = params[name]; if (paramValue === null || paramValue === undefined) return; if (member.location === ""uri"") { var regex = new RegExp(""\\{"" + member.name + ""(\\+)?\\}""); uri = uri.replace(regex, function(_, plus) { var fn = plus ? util.uriEscapePath : util.uriEscape; return fn(String(paramValue)); }); } else if (member.location === ""querystring"") { queryStringSet = true; if (member.type === ""list"") { queryString[member.name] = paramValue.map(function(val) { return util.uriEscape(member.member.toWireFormat(val).toString()); }); } else if (member.type === ""map"") { util.each(paramValue, function(key, value) { if (Array.isArray(value)) { queryString[key] = value.map(function(val) { return util.uriEscape(String(val)); }); } else { queryString[key] = util.uriEscape(String(value)); } }); } else { queryString[member.name] = util.uriEscape(member.toWireFormat(paramValue).toString()); } } }); if (queryStringSet) { uri += uri.indexOf(""?"") >= 0 ? ""&"" : ""?""; var parts = []; util.arrayEach(Object.keys(queryString).sort(), function(key) { if (!Array.isArray(queryString[key])) { queryString[key] = [ queryString[key] ]; } for (var i = 0; i < queryString[key].length; i++) { parts.push(util.uriEscape(String(key)) + ""="" + queryString[key][i]); } }); uri += parts.join(""&""); } return uri; } function populateURI(req) { var operation = req.service.api.operations[req.operation]; var input = operation.input; var uri = generateURI(req.httpRequest.endpoint.path, operation.httpPath, input, req.params); req.httpRequest.path = uri; } function populateHeaders(req) { var operation = req.service.api.operations[req.operation]; util.each(operation.input.members, function(name, member) { var value = req.params[name]; if (value === null || value === undefined) return; if (member.location === ""headers"" && member.type === ""map"") { util.each(value, function(key, memberValue) { req.httpRequest.headers[member.name + key] = memberValue; }); } else if (member.location === ""header"") { value = member.toWireFormat(value).toString(); if (member.isJsonValue) { value = util.base64.encode(value); } req.httpRequest.headers[member.name] = value; } }); } function buildRequest(req) { populateMethod(req); populateURI(req); populateHeaders(req); populateHostPrefix(req); } function extractError() {} function extractData(resp) { var req = resp.request; var data = {}; var r = resp.httpResponse; var operation = req.service.api.operations[req.operation]; var output = operation.output; var headers = {}; util.each(r.headers, function(k, v) { headers[k.toLowerCase()] = v; }); util.each(output.members, function(name, member) { var header = (member.name || name).toLowerCase(); if (member.location === ""headers"" && member.type === ""map"") { data[name] = {}; var location = member.isLocationName ? member.name : """"; var pattern = new RegExp(""^"" + location + ""(.+)"", ""i""); util.each(r.headers, function(k, v) { var result = k.match(pattern); if (result !== null) { data[name][result[1]] = v; } }); } else if (member.location === ""header"") { if (headers[header] !== undefined) { var value = member.isJsonValue ? util.base64.decode(headers[header]) : headers[header]; data[name] = member.toType(value); } } else if (member.location === ""statusCode"") { data[name] = parseInt(r.statusCode, 10); } }); resp.data = data; } module.exports = { buildRequest: buildRequest, extractError: extractError, extractData: extractData, generateURI: generateURI }; }, { ""../util"": 116, ""./helpers"": 72 } ], 74: [ function(require, module, exports) { var AWS = require(""../core""); var util = require(""../util""); var QueryParamSerializer = require(""../query/query_param_serializer""); var Shape = require(""../model/shape""); var populateHostPrefix = require(""./helpers"").populateHostPrefix; function buildRequest(req) { var operation = req.service.api.operations[req.operation]; var httpRequest = req.httpRequest; httpRequest.headers[""Content-Type""] = ""application/x-www-form-urlencoded; charset=utf-8""; httpRequest.params = { Version: req.service.api.apiVersion, Action: operation.name }; var builder = new QueryParamSerializer(); builder.serialize(req.params, operation.input, function(name, value) { httpRequest.params[name] = value; }); httpRequest.body = util.queryParamsToString(httpRequest.params); populateHostPrefix(req); } function extractError(resp) { var data, body = resp.httpResponse.body.toString(); if (body.match(""<UnknownOperationException"")) { data = { Code: ""UnknownOperation"", Message: ""Unknown operation "" + resp.request.operation }; } else { try { data = new AWS.XML.Parser().parse(body); } catch (e) { data = { Code: resp.httpResponse.statusCode, Message: resp.httpResponse.statusMessage }; } } if (data.requestId && !resp.requestId) resp.requestId = data.requestId; if (data.Errors) data = data.Errors; if (data.Error) data = data.Error; if (data.Code) { resp.error = util.error(new Error(), { code: data.Code, message: data.Message }); } else { resp.error = util.error(new Error(), { code: resp.httpResponse.statusCode, message: null }); } } function extractData(resp) { var req = resp.request; var operation = req.service.api.operations[req.operation]; var shape = operation.output || {}; var origRules = shape; if (origRules.resultWrapper) { var tmp = Shape.create({ type: ""structure"" }); tmp.members[origRules.resultWrapper] = shape; tmp.memberNames = [ origRules.resultWrapper ]; util.property(shape, ""name"", shape.resultWrapper); shape = tmp; } var parser = new AWS.XML.Parser(); if (shape && shape.members && !shape.members._XAMZRequestId) { var requestIdShape = Shape.create({ type: ""string"" }, { api: { protocol: ""query"" } }, ""requestId""); shape.members._XAMZRequestId = requestIdShape; } var data = parser.parse(resp.httpResponse.body.toString(), shape); resp.requestId = data._XAMZRequestId || data.requestId; if (data._XAMZRequestId) delete data._XAMZRequestId; if (origRules.resultWrapper) { if (data[origRules.resultWrapper]) { util.update(data, data[origRules.resultWrapper]); delete data[origRules.resultWrapper]; } } resp.data = data; } module.exports = { buildRequest: buildRequest, extractError: extractError, extractData: extractData }; }, { ""../core"": 38, ""../model/shape"": 69, ""../query/query_param_serializer"": 78, ""../util"": 116, ""./helpers"": 72 } ], 78: [ function(require, module, exports) { var util = require(""../util""); function QueryParamSerializer() {} QueryParamSerializer.prototype.serialize = function(params, shape, fn) { serializeStructure("""", params, shape, fn); }; function ucfirst(shape) { if (shape.isQueryName || shape.api.protocol !== ""ec2"") { return shape.name; } else { return shape.name[0].toUpperCase() + shape.name.substr(1); } } function serializeStructure(prefix, struct, rules, fn) { util.each(rules.members, function(name, member) { var value = struct[name]; if (value === null || value === undefined) return; var memberName = ucfirst(member); memberName = prefix ? prefix + ""."" + memberName : memberName; serializeMember(memberName, value, member, fn); }); } function serializeMap(name, map, rules, fn) { var i = 1; util.each(map, function(key, value) { var prefix = rules.flattened ? ""."" : "".entry.""; var position = prefix + i++ + "".""; var keyName = position + (rules.key.name || ""key""); var valueName = position + (rules.value.name || ""value""); serializeMember(name + keyName, key, rules.key, fn); serializeMember(name + valueName, value, rules.value, fn); }); } function serializeList(name, list, rules, fn) { var memberRules = rules.member || {}; if (list.length === 0) { fn.call(this, name, null); return; } util.arrayEach(list, function(v, n) { var suffix = ""."" + (n + 1); if (rules.api.protocol === ""ec2"") { suffix = suffix + """"; } else if (rules.flattened) { if (memberRules.name) { var parts = name.split("".""); parts.pop(); parts.push(ucfirst(memberRules)); name = parts.join("".""); } } else { suffix = ""."" + (memberRules.name ? memberRules.name : ""member"") + suffix; } serializeMember(name + suffix, v, memberRules, fn); }); } function serializeMember(name, value, rules, fn) { if (value === null || value === undefined) return; if (rules.type === ""structure"") { serializeStructure(name, value, rules, fn); } else if (rules.type === ""list"") { serializeList(name, value, rules, fn); } else if (rules.type === ""map"") { serializeMap(name, value, rules, fn); } else { fn(name, rules.toWireFormat(value).toString()); } } module.exports = QueryParamSerializer; }, { ""../util"": 116 } ], 69: [ function(require, module, exports) { var Collection = require(""./collection""); var util = require(""../util""); function property(obj, name, value) { if (value !== null && value !== undefined) { util.property.apply(this, arguments); } } function memoizedProperty(obj, name) { if (!obj.constructor.prototype[name]) { util.memoizedProperty.apply(this, arguments); } } function Shape(shape, options, memberName) { options = options || {}; property(this, ""shape"", shape.shape); property(this, ""api"", options.api, false); property(this, ""type"", shape.type); property(this, ""enum"", shape.enum); property(this, ""min"", shape.min); property(this, ""max"", shape.max); property(this, ""pattern"", shape.pattern); property(this, ""location"", shape.location || this.location || ""body""); property(this, ""name"", this.name || shape.xmlName || shape.queryName || shape.locationName || memberName); property(this, ""isStreaming"", shape.streaming || this.isStreaming || false); property(this, ""requiresLength"", shape.requiresLength, false); property(this, ""isComposite"", shape.isComposite || false); property(this, ""isShape"", true, false); property(this, ""isQueryName"", Boolean(shape.queryName), false); property(this, ""isLocationName"", Boolean(shape.locationName), false); property(this, ""isIdempotent"", shape.idempotencyToken === true); property(this, ""isJsonValue"", shape.jsonvalue === true); property(this, ""isSensitive"", shape.sensitive === true || shape.prototype && shape.prototype.sensitive === true); property(this, ""isEventStream"", Boolean(shape.eventstream), false); property(this, ""isEvent"", Boolean(shape.event), false); property(this, ""isEventPayload"", Boolean(shape.eventpayload), false); property(this, ""isEventHeader"", Boolean(shape.eventheader), false); property(this, ""isTimestampFormatSet"", Boolean(shape.timestampFormat) || shape.prototype && shape.prototype.isTimestampFormatSet === true, false); property(this, ""endpointDiscoveryId"", Boolean(shape.endpointdiscoveryid), false); property(this, ""hostLabel"", Boolean(shape.hostLabel), false); if (options.documentation) { property(this, ""documentation"", shape.documentation); property(this, ""documentationUrl"", shape.documentationUrl); } if (shape.xmlAttribute) { property(this, ""isXmlAttribute"", shape.xmlAttribute || false); } property(this, ""defaultValue"", null); this.toWireFormat = function(value) { if (value === null || value === undefined) return """"; return value; }; this.toType = function(value) { return value; }; } Shape.normalizedTypes = { character: ""string"", double: ""float"", long: ""integer"", short: ""integer"", biginteger: ""integer"", bigdecimal: ""float"", blob: ""binary"" }; Shape.types = { structure: StructureShape, list: ListShape, map: MapShape, boolean: BooleanShape, timestamp: TimestampShape, float: FloatShape, integer: IntegerShape, string: StringShape, base64: Base64Shape, binary: BinaryShape }; Shape.resolve = function resolve(shape, options) { if (shape.shape) { var refShape = options.api.shapes[shape.shape]; if (!refShape) { throw new Error(""Cannot find shape reference: "" + shape.shape); } return refShape; } else { return null; } }; Shape.create = function create(shape, options, memberName) { if (shape.isShape) return shape; var refShape = Shape.resolve(shape, options); if (refShape) { var filteredKeys = Object.keys(shape); if (!options.documentation) { filteredKeys = filteredKeys.filter(function(name) { return !name.match(/documentation/); }); } var InlineShape = function() { refShape.constructor.call(this, shape, options, memberName); }; InlineShape.prototype = refShape; return new InlineShape(); } else { if (!shape.type) { if (shape.members) shape.type = ""structure""; else if (shape.member) shape.type = ""list""; else if (shape.key) shape.type = ""map""; else shape.type = ""string""; } var origType = shape.type; if (Shape.normalizedTypes[shape.type]) { shape.type = Shape.normalizedTypes[shape.type]; } if (Shape.types[shape.type]) { return new Shape.types[shape.type](shape, options, memberName); } else { throw new Error(""Unrecognized shape type: "" + origType); } } }; function CompositeShape(shape) { Shape.apply(this, arguments); property(this, ""isComposite"", true); if (shape.flattened) { property(this, ""flattened"", shape.flattened || false); } } function StructureShape(shape, options) { var self = this; var requiredMap = null, firstInit = !this.isShape; CompositeShape.apply(this, arguments); if (firstInit) { property(this, ""defaultValue"", function() { return {}; }); property(this, ""members"", {}); property(this, ""memberNames"", []); property(this, ""required"", []); property(this, ""isRequired"", function() { return false; }); } if (shape.members) { property(this, ""members"", new Collection(shape.members, options, function(name, member) { return Shape.create(member, options, name); })); memoizedProperty(this, ""memberNames"", function() { return shape.xmlOrder || Object.keys(shape.members); }); if (shape.event) { memoizedProperty(this, ""eventPayloadMemberName"", function() { var members = self.members; var memberNames = self.memberNames; for (var i = 0, iLen = memberNames.length; i < iLen; i++) { if (members[memberNames[i]].isEventPayload) { return memberNames[i]; } } }); memoizedProperty(this, ""eventHeaderMemberNames"", function() { var members = self.members; var memberNames = self.memberNames; var eventHeaderMemberNames = []; for (var i = 0, iLen = memberNames.length; i < iLen; i++) { if (members[memberNames[i]].isEventHeader) { eventHeaderMemberNames.push(memberNames[i]); } } return eventHeaderMemberNames; }); } } if (shape.required) { property(this, ""required"", shape.required); property(this, ""isRequired"", function(name) { if (!requiredMap) { requiredMap = {}; for (var i = 0; i < shape.required.length; i++) { requiredMap[shape.required[i]] = true; } } return requiredMap[name]; }, false, true); } property(this, ""resultWrapper"", shape.resultWrapper || null); if (shape.payload) { property(this, ""payload"", shape.payload); } if (typeof shape.xmlNamespace === ""string"") { property(this, ""xmlNamespaceUri"", shape.xmlNamespace); } else if (typeof shape.xmlNamespace === ""object"") { property(this, ""xmlNamespacePrefix"", shape.xmlNamespace.prefix); property(this, ""xmlNamespaceUri"", shape.xmlNamespace.uri); } } function ListShape(shape, options) { var self = this, firstInit = !this.isShape; CompositeShape.apply(this, arguments); if (firstInit) { property(this, ""defaultValue"", function() { return []; }); } if (shape.member) { memoizedProperty(this, ""member"", function() { return Shape.create(shape.member, options); }); } if (this.flattened) { var oldName = this.name; memoizedProperty(this, ""name"", function() { return self.member.name || oldName; }); } } function MapShape(shape, options) { var firstInit = !this.isShape; CompositeShape.apply(this, arguments); if (firstInit) { property(this, ""defaultValue"", function() { return {}; }); property(this, ""key"", Shape.create({ type: ""string"" }, options)); property(this, ""value"", Shape.create({ type: ""string"" }, options)); } if (shape.key) { memoizedProperty(this, ""key"", function() { return Shape.create(shape.key, options); }); } if (shape.value) { memoizedProperty(this, ""value"", function() { return Shape.create(shape.value, options); }); } } function TimestampShape(shape) { var self = this; Shape.apply(this, arguments); if (shape.timestampFormat) { property(this, ""timestampFormat"", shape.timestampFormat); } else if (self.isTimestampFormatSet && this.timestampFormat) { property(this, ""timestampFormat"", this.timestampFormat); } else if (this.location === ""header"") { property(this, ""timestampFormat"", ""rfc822""); } else if (this.location === ""querystring"") { property(this, ""timestampFormat"", ""iso8601""); } else if (this.api) { switch (this.api.protocol) { case ""json"": case ""rest-json"": property(this, ""timestampFormat"", ""unixTimestamp""); break; case ""rest-xml"": case ""query"": case ""ec2"": property(this, ""timestampFormat"", ""iso8601""); break; } } this.toType = function(value) { if (value === null || value === undefined) return null; if (typeof value.toUTCString === ""function"") return value; return typeof value === ""string"" || typeof value === ""number"" ? util.date.parseTimestamp(value) : null; }; this.toWireFormat = function(value) { return util.date.format(value, self.timestampFormat); }; } function StringShape() { Shape.apply(this, arguments); var nullLessProtocols = [ ""rest-xml"", ""query"", ""ec2"" ]; this.toType = function(value) { value = this.api && nullLessProtocols.indexOf(this.api.protocol) > -1 ? value || """" : value; if (this.isJsonValue) { return JSON.parse(value); } return value && typeof value.toString === ""function"" ? value.toString() : value; }; this.toWireFormat = function(value) { return this.isJsonValue ? JSON.stringify(value) : value; }; } function FloatShape() { Shape.apply(this, arguments); this.toType = function(value) { if (value === null || value === undefined) return null; return parseFloat(value); }; this.toWireFormat = this.toType; } function IntegerShape() { Shape.apply(this, arguments); this.toType = function(value) { if (value === null || value === undefined) return null; return parseInt(value, 10); }; this.toWireFormat = this.toType; } function BinaryShape() { Shape.apply(this, arguments); this.toType = function(value) { var buf = util.base64.decode(value); if (this.isSensitive && util.isNode() && typeof util.Buffer.alloc === ""function"") { var secureBuf = util.Buffer.alloc(buf.length, buf); buf.fill(0); buf = secureBuf; } return buf; }; this.toWireFormat = util.base64.encode; } function Base64Shape() { BinaryShape.apply(this, arguments); } function BooleanShape() { Shape.apply(this, arguments); this.toType = function(value) { if (typeof value === ""boolean"") return value; if (value === null || value === undefined) return null; return value === ""true""; }; } Shape.shapes = { StructureShape: StructureShape, ListShape: ListShape, MapShape: MapShape, StringShape: StringShape, BooleanShape: BooleanShape, Base64Shape: Base64Shape }; module.exports = Shape; }, { ""../util"": 116, ""./collection"": 65 } ], 65: [ function(require, module, exports) { var memoizedProperty = require(""../util"").memoizedProperty; function memoize(name, value, factory, nameTr) { memoizedProperty(this, nameTr(name), function() { return factory(name, value); }); } function Collection(iterable, options, factory, nameTr, callback) { nameTr = nameTr || String; var self = this; for (var id in iterable) { if (Object.prototype.hasOwnProperty.call(iterable, id)) { memoize.call(self, id, iterable[id], factory, nameTr); if (callback) callback(id, iterable[id]); } } } module.exports = Collection; }, { ""../util"": 116 } ], 73: [ function(require, module, exports) { var util = require(""../util""); var JsonBuilder = require(""../json/builder""); var JsonParser = require(""../json/parser""); var populateHostPrefix = require(""./helpers"").populateHostPrefix; function buildRequest(req) { var httpRequest = req.httpRequest; var api = req.service.api; var target = api.targetPrefix + ""."" + api.operations[req.operation].name; var version = api.jsonVersion || ""1.0""; var input = api.operations[req.operation].input; var builder = new JsonBuilder(); if (version === 1) version = ""1.0""; httpRequest.body = builder.build(req.params || {}, input); httpRequest.headers[""Content-Type""] = ""application/x-amz-json-"" + version; httpRequest.headers[""X-Amz-Target""] = target; populateHostPrefix(req); } function extractError(resp) { var error = {}; var httpResponse = resp.httpResponse; error.code = httpResponse.headers[""x-amzn-errortype""] || ""UnknownError""; if (typeof error.code === ""string"") { error.code = error.code.split("":"")[0]; } if (httpResponse.body.length > 0) { try { var e = JSON.parse(httpResponse.body.toString()); if (e.__type || e.code) { error.code = (e.__type || e.code).split(""#"").pop(); } if (error.code === ""RequestEntityTooLarge"") { error.message = ""Request body must be less than 1 MB""; } else { error.message = e.message || e.Message || null; } } catch (e) { error.statusCode = httpResponse.statusCode; error.message = httpResponse.statusMessage; } } else { error.statusCode = httpResponse.statusCode; error.message = httpResponse.statusCode.toString(); } resp.error = util.error(new Error(), error); } function extractData(resp) { var body = resp.httpResponse.body.toString() || ""{}""; if (resp.request.service.config.convertResponseTypes === false) { resp.data = JSON.parse(body); } else { var operation = resp.request.service.api.operations[resp.request.operation]; var shape = operation.output || {}; var parser = new JsonParser(); resp.data = parser.parse(body, shape); } } module.exports = { buildRequest: buildRequest, extractError: extractError, extractData: extractData }; }, { ""../json/builder"": 62, ""../json/parser"": 63, ""../util"": 116, ""./helpers"": 72 } ], 72: [ function(require, module, exports) { var util = require(""../util""); var AWS = require(""../core""); function populateHostPrefix(request) { var enabled = request.service.config.hostPrefixEnabled; if (!enabled) return request; var operationModel = request.service.api.operations[request.operation]; if (hasEndpointDiscover(request)) return request; if (operationModel.endpoint && operationModel.endpoint.hostPrefix) { var hostPrefixNotation = operationModel.endpoint.hostPrefix; var hostPrefix = expandHostPrefix(hostPrefixNotation, request.params, operationModel.input); prependEndpointPrefix(request.httpRequest.endpoint, hostPrefix); validateHostname(request.httpRequest.endpoint.hostname); } return request; } function hasEndpointDiscover(request) { var api = request.service.api; var operationModel = api.operations[request.operation]; var isEndpointOperation = api.endpointOperation && api.endpointOperation === util.string.lowerFirst(operationModel.name); return operationModel.endpointDiscoveryRequired !== ""NULL"" || isEndpointOperation === true; } function expandHostPrefix(hostPrefixNotation, params, shape) { util.each(shape.members, function(name, member) { if (member.hostLabel === true) { if (typeof params[name] !== ""string"" || params[name] === """") { throw util.error(new Error(), { message: ""Parameter "" + name + "" should be a non-empty string."", code: ""InvalidParameter"" }); } var regex = new RegExp(""\\{"" + name + ""\\}"", ""g""); hostPrefixNotation = hostPrefixNotation.replace(regex, params[name]); } }); return hostPrefixNotation; } function prependEndpointPrefix(endpoint, prefix) { if (endpoint.host) { endpoint.host = prefix + endpoint.host; } if (endpoint.hostname) { endpoint.hostname = prefix + endpoint.hostname; } } function validateHostname(hostname) { var labels = hostname.split("".""); var hostPattern = /^[a-zA-Z0-9]{1}$|^[a-zA-Z0-9][a-zA-Z0-9\-]*[a-zA-Z0-9]$/; util.arrayEach(labels, function(label) { if (!label.length || label.length < 1 || label.length > 63) { throw util.error(new Error(), { code: ""ValidationError"", message: ""Hostname label length should be between 1 to 63 characters, inclusive."" }); } if (!hostPattern.test(label)) { throw AWS.util.error(new Error(), { code: ""ValidationError"", message: label + "" is not hostname compatible."" }); } }); } module.exports = { populateHostPrefix: populateHostPrefix }; }, { ""../core"": 38, ""../util"": 116 } ], 63: [ function(require, module, exports) { var util = require(""../util""); function JsonParser() {} JsonParser.prototype.parse = function(value, shape) { return translate(JSON.parse(value), shape); }; function translate(value, shape) { if (!shape || value === undefined) return undefined; switch (shape.type) { case ""structure"": return translateStructure(value, shape); case ""map"": return translateMap(value, shape); case ""list"": return translateList(value, shape); default: return translateScalar(value, shape); } } function translateStructure(structure, shape) { if (structure == null) return undefined; var struct = {}; var shapeMembers = shape.members; util.each(shapeMembers, function(name, memberShape) { var locationName = memberShape.isLocationName ? memberShape.name : name; if (Object.prototype.hasOwnProperty.call(structure, locationName)) { var value = structure[locationName]; var result = translate(value, memberShape); if (result !== undefined) struct[name] = result; } }); return struct; } function translateList(list, shape) { if (list == null) return undefined; var out = []; util.arrayEach(list, function(value) { var result = translate(value, shape.member); if (result === undefined) out.push(null); else out.push(result); }); return out; } function translateMap(map, shape) { if (map == null) return undefined; var out = {}; util.each(map, function(key, value) { var result = translate(value, shape.value); if (result === undefined) out[key] = null; else out[key] = result; }); return out; } function translateScalar(value, shape) { return shape.toType(value); } module.exports = JsonParser; }, { ""../util"": 116 } ], 62: [ function(require, module, exports) { var util = require(""../util""); function JsonBuilder() {} JsonBuilder.prototype.build = function(value, shape) { return JSON.stringify(translate(value, shape)); }; function translate(value, shape) { if (!shape || value === undefined || value === null) return undefined; switch (shape.type) { case ""structure"": return translateStructure(value, shape); case ""map"": return translateMap(value, shape); case ""list"": return translateList(value, shape); default: return translateScalar(value, shape); } } function translateStructure(structure, shape) { var struct = {}; util.each(structure, function(name, value) { var memberShape = shape.members[name]; if (memberShape) { if (memberShape.location !== ""body"") return; var locationName = memberShape.isLocationName ? memberShape.name : name; var result = translate(value, memberShape); if (result !== undefined) struct[locationName] = result; } }); return struct; } function translateList(list, shape) { var out = []; util.arrayEach(list, function(value) { var result = translate(value, shape.member); if (result !== undefined) out.push(result); }); return out; } function translateMap(map, shape) { var out = {}; util.each(map, function(key, value) { var result = translate(value, shape.value); if (result !== undefined) out[key] = result; }); return out; } function translateScalar(value, shape) { return shape.toWireFormat(value); } module.exports = JsonBuilder; }, { ""../util"": 116 } ], 46: [ function(require, module, exports) { (function(process) { var AWS = require(""./core""); var util = require(""./util""); var endpointDiscoveryEnabledEnvs = [ ""AWS_ENABLE_ENDPOINT_DISCOVERY"", ""AWS_ENDPOINT_DISCOVERY_ENABLED"" ]; function getCacheKey(request) { var service = request.service; var api = service.api || {}; var operations = api.operations; var identifiers = {}; if (service.config.region) { identifiers.region = service.config.region; } if (api.serviceId) { identifiers.serviceId = api.serviceId; } if (service.config.credentials.accessKeyId) { identifiers.accessKeyId = service.config.credentials.accessKeyId; } return identifiers; } function marshallCustomIdentifiersHelper(result, params, shape) { if (!shape || params === undefined || params === null) return; if (shape.type === ""structure"" && shape.required && shape.required.length > 0) { util.arrayEach(shape.required, function(name) { var memberShape = shape.members[name]; if (memberShape.endpointDiscoveryId === true) { var locationName = memberShape.isLocationName ? memberShape.name : name; result[locationName] = String(params[name]); } else { marshallCustomIdentifiersHelper(result, params[name], memberShape); } }); } } function marshallCustomIdentifiers(request, shape) { var identifiers = {}; marshallCustomIdentifiersHelper(identifiers, request.params, shape); return identifiers; } function optionalDiscoverEndpoint(request) { var service = request.service; var api = service.api; var operationModel = api.operations ? api.operations[request.operation] : undefined; var inputShape = operationModel ? operationModel.input : undefined; var identifiers = marshallCustomIdentifiers(request, inputShape); var cacheKey = getCacheKey(request); if (Object.keys(identifiers).length > 0) { cacheKey = util.update(cacheKey, identifiers); if (operationModel) cacheKey.operation = operationModel.name; } var endpoints = AWS.endpointCache.get(cacheKey); if (endpoints && endpoints.length === 1 && endpoints[0].Address === """") { return; } else if (endpoints && endpoints.length > 0) { request.httpRequest.updateEndpoint(endpoints[0].Address); } else { var endpointRequest = service.makeRequest(api.endpointOperation, { Operation: operationModel.name, Identifiers: identifiers }); addApiVersionHeader(endpointRequest); endpointRequest.removeListener(""validate"", AWS.EventListeners.Core.VALIDATE_PARAMETERS); endpointRequest.removeListener(""retry"", AWS.EventListeners.Core.RETRY_CHECK); AWS.endpointCache.put(cacheKey, [ { Address: """", CachePeriodInMinutes: 1 } ]); endpointRequest.send(function(err, data) { if (data && data.Endpoints) { AWS.endpointCache.put(cacheKey, data.Endpoints); } else if (err) { AWS.endpointCache.put(cacheKey, [ { Address: """", CachePeriodInMinutes: 1 } ]); } }); } } var requestQueue = {}; function requiredDiscoverEndpoint(request, done) { var service = request.service; var api = service.api; var operationModel = api.operations ? api.operations[request.operation] : undefined; var inputShape = operationModel ? operationModel.input : undefined; var identifiers = marshallCustomIdentifiers(request, inputShape); var cacheKey = getCacheKey(request); if (Object.keys(identifiers).length > 0) { cacheKey = util.update(cacheKey, identifiers); if (operationModel) cacheKey.operation = operationModel.name; } var cacheKeyStr = AWS.EndpointCache.getKeyString(cacheKey); var endpoints = AWS.endpointCache.get(cacheKeyStr); if (endpoints && endpoints.length === 1 && endpoints[0].Address === """") { if (!requestQueue[cacheKeyStr]) requestQueue[cacheKeyStr] = []; requestQueue[cacheKeyStr].push({ request: request, callback: done }); return; } else if (endpoints && endpoints.length > 0) { request.httpRequest.updateEndpoint(endpoints[0].Address); done(); } else { var endpointRequest = service.makeRequest(api.endpointOperation, { Operation: operationModel.name, Identifiers: identifiers }); endpointRequest.removeListener(""validate"", AWS.EventListeners.Core.VALIDATE_PARAMETERS); addApiVersionHeader(endpointRequest); AWS.endpointCache.put(cacheKeyStr, [ { Address: """", CachePeriodInMinutes: 60 } ]); endpointRequest.send(function(err, data) { if (err) { var errorParams = { code: ""EndpointDiscoveryException"", message: ""Request cannot be fulfilled without specifying an endpoint"", retryable: false }; request.response.error = util.error(err, errorParams); AWS.endpointCache.remove(cacheKey); if (requestQueue[cacheKeyStr]) { var pendingRequests = requestQueue[cacheKeyStr]; util.arrayEach(pendingRequests, function(requestContext) { requestContext.request.response.error = util.error(err, errorParams); requestContext.callback(); }); delete requestQueue[cacheKeyStr]; } } else if (data) { AWS.endpointCache.put(cacheKeyStr, data.Endpoints); request.httpRequest.updateEndpoint(data.Endpoints[0].Address); if (requestQueue[cacheKeyStr]) { var pendingRequests = requestQueue[cacheKeyStr]; util.arrayEach(pendingRequests, function(requestContext) { requestContext.request.httpRequest.updateEndpoint(data.Endpoints[0].Address); requestContext.callback(); }); delete requestQueue[cacheKeyStr]; } } done(); }); } } function addApiVersionHeader(endpointRequest) { var api = endpointRequest.service.api; var apiVersion = api.apiVersion; if (apiVersion && !endpointRequest.httpRequest.headers[""x-amz-api-version""]) { endpointRequest.httpRequest.headers[""x-amz-api-version""] = apiVersion; } } function invalidateCachedEndpoints(response) { var error = response.error; var httpResponse = response.httpResponse; if (error && (error.code === ""InvalidEndpointException"" || httpResponse.statusCode === 421)) { var request = response.request; var operations = request.service.api.operations || {}; var inputShape = operations[request.operation] ? operations[request.operation].input : undefined; var identifiers = marshallCustomIdentifiers(request, inputShape); var cacheKey = getCacheKey(request); if (Object.keys(identifiers).length > 0) { cacheKey = util.update(cacheKey, identifiers); if (operations[request.operation]) cacheKey.operation = operations[request.operation].name; } AWS.endpointCache.remove(cacheKey); } } function hasCustomEndpoint(client) { if (client._originalConfig && client._originalConfig.endpoint && client._originalConfig.endpointDiscoveryEnabled === true) { throw util.error(new Error(), { code: ""ConfigurationException"", message: ""Custom endpoint is supplied; endpointDiscoveryEnabled must not be true."" }); } var svcConfig = AWS.config[client.serviceIdentifier] || {}; return Boolean(AWS.config.endpoint || svcConfig.endpoint || client._originalConfig && client._originalConfig.endpoint); } function isFalsy(value) { return [ ""false"", ""0"" ].indexOf(value) >= 0; } function isEndpointDiscoveryApplicable(request) { var service = request.service || {}; if (service.config.endpointDiscoveryEnabled === true) return true; if (util.isBrowser()) return false; for (var i = 0; i < endpointDiscoveryEnabledEnvs.length; i++) { var env = endpointDiscoveryEnabledEnvs[i]; if (Object.prototype.hasOwnProperty.call(process.env, env)) { if (process.env[env] === """" || process.env[env] === undefined) { throw util.error(new Error(), { code: ""ConfigurationException"", message: ""environmental variable "" + env + "" cannot be set to nothing"" }); } if (!isFalsy(process.env[env])) return true; } } var configFile = {}; try { configFile = AWS.util.iniLoader ? AWS.util.iniLoader.loadFrom({ isConfig: true, filename: process.env[AWS.util.sharedConfigFileEnv] }) : {}; } catch (e) {} var sharedFileConfig = configFile[process.env.AWS_PROFILE || AWS.util.defaultProfile] || {}; if (Object.prototype.hasOwnProperty.call(sharedFileConfig, ""endpoint_discovery_enabled"")) { if (sharedFileConfig.endpoint_discovery_enabled === undefined) { throw util.error(new Error(), { code: ""ConfigurationException"", message: ""config file entry 'endpoint_discovery_enabled' cannot be set to nothing"" }); } if (!isFalsy(sharedFileConfig.endpoint_discovery_enabled)) return true; } return false; } function discoverEndpoint(request, done) { var service = request.service || {}; if (hasCustomEndpoint(service) || request.isPresigned()) return done(); if (!isEndpointDiscoveryApplicable(request)) return done(); request.httpRequest.appendToUserAgent(""endpoint-discovery""); var operations = service.api.operations || {}; var operationModel = operations[request.operation]; var isEndpointDiscoveryRequired = operationModel ? operationModel.endpointDiscoveryRequired : ""NULL""; switch (isEndpointDiscoveryRequired) { case ""OPTIONAL"": optionalDiscoverEndpoint(request); request.addNamedListener(""INVALIDATE_CACHED_ENDPOINTS"", ""extractError"", invalidateCachedEndpoints); done(); break; case ""REQUIRED"": request.addNamedListener(""INVALIDATE_CACHED_ENDPOINTS"", ""extractError"", invalidateCachedEndpoints); requiredDiscoverEndpoint(request, done); break; case ""NULL"": default: done(); break; } } module.exports = { discoverEndpoint: discoverEndpoint, requiredDiscoverEndpoint: requiredDiscoverEndpoint, optionalDiscoverEndpoint: optionalDiscoverEndpoint, marshallCustomIdentifiers: marshallCustomIdentifiers, getCacheKey: getCacheKey, invalidateCachedEndpoint: invalidateCachedEndpoints }; }).call(this, require(""_process"")); }, { ""./core"": 38, ""./util"": 116, _process: 8 } ], 116: [ function(require, module, exports) { (function(process, setImmediate) { var AWS; var util = { environment: ""nodejs"", engine: function engine() { if (util.isBrowser() && typeof navigator !== ""undefined"") { return navigator.userAgent; } else { var engine = process.platform + ""/"" + process.version; if (process.env.AWS_EXECUTION_ENV) { engine += "" exec-env/"" + process.env.AWS_EXECUTION_ENV; } return engine; } }, userAgent: function userAgent() { var name = util.environment; var agent = ""aws-sdk-"" + name + ""/"" + require(""./core"").VERSION; if (name === ""nodejs"") agent += "" "" + util.engine(); return agent; }, uriEscape: function uriEscape(string) { var output = encodeURIComponent(string); output = output.replace(/[^A-Za-z0-9_.~\-%]+/g, escape); output = output.replace(/[*]/g, function(ch) { return ""%"" + ch.charCodeAt(0).toString(16).toUpperCase(); }); return output; }, uriEscapePath: function uriEscapePath(string) { var parts = []; util.arrayEach(string.split(""/""), function(part) { parts.push(util.uriEscape(part)); }); return parts.join(""/""); }, urlParse: function urlParse(url) { return util.url.parse(url); }, urlFormat: function urlFormat(url) { return util.url.format(url); }, queryStringParse: function queryStringParse(qs) { return util.querystring.parse(qs); }, queryParamsToString: function queryParamsToString(params) { var items = []; var escape = util.uriEscape; var sortedKeys = Object.keys(params).sort(); util.arrayEach(sortedKeys, function(name) { var value = params[name]; var ename = escape(name); var result = ename + ""=""; if (Array.isArray(value)) { var vals = []; util.arrayEach(value, function(item) { vals.push(escape(item)); }); result = ename + ""="" + vals.sort().join(""&"" + ename + ""=""); } else if (value !== undefined && value !== null) { result = ename + ""="" + escape(value); } items.push(result); }); return items.join(""&""); }, readFileSync: function readFileSync(path) { if (util.isBrowser()) return null; return require(""fs"").readFileSync(path, ""utf-8""); }, base64: { encode: function encode64(string) { if (typeof string === ""number"") { throw util.error(new Error(""Cannot base64 encode number "" + string)); } if (string === null || typeof string === ""undefined"") { return string; } var buf = util.buffer.toBuffer(string); return buf.toString(""base64""); }, decode: function decode64(string) { if (typeof string === ""number"") { throw util.error(new Error(""Cannot base64 decode number "" + string)); } if (string === null || typeof string === ""undefined"") { return string; } return util.buffer.toBuffer(string, ""base64""); } }, buffer: { toBuffer: function(data, encoding) { return typeof util.Buffer.from === ""function"" && util.Buffer.from !== Uint8Array.from ? util.Buffer.from(data, encoding) : new util.Buffer(data, encoding); }, alloc: function(size, fill, encoding) { if (typeof size !== ""number"") { throw new Error(""size passed to alloc must be a number.""); } if (typeof util.Buffer.alloc === ""function"") { return util.Buffer.alloc(size, fill, encoding); } else { var buf = new util.Buffer(size); if (fill !== undefined && typeof buf.fill === ""function"") { buf.fill(fill, undefined, undefined, encoding); } return buf; } }, toStream: function toStream(buffer) { if (!util.Buffer.isBuffer(buffer)) buffer = util.buffer.toBuffer(buffer); var readable = new util.stream.Readable(); var pos = 0; readable._read = function(size) { if (pos >= buffer.length) return readable.push(null); var end = pos + size; if (end > buffer.length) end = buffer.length; readable.push(buffer.slice(pos, end)); pos = end; }; return readable; }, concat: function(buffers) { var length = 0, offset = 0, buffer = null, i; for (i = 0; i < buffers.length; i++) { length += buffers[i].length; } buffer = util.buffer.alloc(length); for (i = 0; i < buffers.length; i++) { buffers[i].copy(buffer, offset); offset += buffers[i].length; } return buffer; } }, string: { byteLength: function byteLength(string) { if (string === null || string === undefined) return 0; if (typeof string === ""string"") string = util.buffer.toBuffer(string); if (typeof string.byteLength === ""number"") { return string.byteLength; } else if (typeof string.length === ""number"") { return string.length; } else if (typeof string.size === ""number"") { return string.size; } else if (typeof string.path === ""string"") { return require(""fs"").lstatSync(string.path).size; } else { throw util.error(new Error(""Cannot determine length of "" + string), { object: string }); } }, upperFirst: function upperFirst(string) { return string[0].toUpperCase() + string.substr(1); }, lowerFirst: function lowerFirst(string) { return string[0].toLowerCase() + string.substr(1); } }, ini: { parse: function string(ini) { var currentSection, map = {}; util.arrayEach(ini.split(/\r?\n/), function(line) { line = line.split(/(^|\s)[;#]/)[0]; var section = line.match(/^\s*\[([^\[\]]+)\]\s*$/); if (section) { currentSection = section[1]; } else if (currentSection) { var item = line.match(/^\s*(.+?)\s*=\s*(.+?)\s*$/); if (item) { map[currentSection] = map[currentSection] || {}; map[currentSection][item[1]] = item[2]; } } }); return map; } }, fn: { noop: function() {}, callback: function(err) { if (err) throw err; }, makeAsync: function makeAsync(fn, expectedArgs) { if (expectedArgs && expectedArgs <= fn.length) { return fn; } return function() { var args = Array.prototype.slice.call(arguments, 0); var callback = args.pop(); var result = fn.apply(null, args); callback(result); }; } }, date: { getDate: function getDate() { if (!AWS) AWS = require(""./core""); if (AWS.config.systemClockOffset) { return new Date(new Date().getTime() + AWS.config.systemClockOffset); } else { return new Date(); } }, iso8601: function iso8601(date) { if (date === undefined) { date = util.date.getDate(); } return date.toISOString().replace(/\.\d{3}Z$/, ""Z""); }, rfc822: function rfc822(date) { if (date === undefined) { date = util.date.getDate(); } return date.toUTCString(); }, unixTimestamp: function unixTimestamp(date) { if (date === undefined) { date = util.date.getDate(); } return date.getTime() / 1e3; }, from: function format(date) { if (typeof date === ""number"") { return new Date(date * 1e3); } else { return new Date(date); } }, format: function format(date, formatter) { if (!formatter) formatter = ""iso8601""; return util.date[formatter](util.date.from(date)); }, parseTimestamp: function parseTimestamp(value) { if (typeof value === ""number"") { return new Date(value * 1e3); } else if (value.match(/^\d+$/)) { return new Date(value * 1e3); } else if (value.match(/^\d{4}/)) { return new Date(value); } else if (value.match(/^\w{3},/)) { return new Date(value); } else { throw util.error(new Error(""unhandled timestamp format: "" + value), { code: ""TimestampParserError"" }); } } }, crypto: { crc32Table: [ 0, 1996959894, 3993919788, 2567524794, 124634137, 1886057615, 3915621685, 2657392035, 249268274, 2044508324, 3772115230, 2547177864, 162941995, 2125561021, 3887607047, 2428444049, 498536548, 1789927666, 4089016648, 2227061214, 450548861, 1843258603, 4107580753, 2211677639, 325883990, 1684777152, 4251122042, 2321926636, 335633487, 1661365465, 4195302755, 2366115317, 997073096, 1281953886, 3579855332, 2724688242, 1006888145, 1258607687, 3524101629, 2768942443, 901097722, 1119000684, 3686517206, 2898065728, 853044451, 1172266101, 3705015759, 2882616665, 651767980, 1373503546, 3369554304, 3218104598, 565507253, 1454621731, 3485111705, 3099436303, 671266974, 1594198024, 3322730930, 2970347812, 795835527, 1483230225, 3244367275, 3060149565, 1994146192, 31158534, 2563907772, 4023717930, 1907459465, 112637215, 2680153253, 3904427059, 2013776290, 251722036, 2517215374, 3775830040, 2137656763, 141376813, 2439277719, 3865271297, 1802195444, 476864866, 2238001368, 4066508878, 1812370925, 453092731, 2181625025, 4111451223, 1706088902, 314042704, 2344532202, 4240017532, 1658658271, 366619977, 2362670323, 4224994405, 1303535960, 984961486, 2747007092, 3569037538, 1256170817, 1037604311, 2765210733, 3554079995, 1131014506, 879679996, 2909243462, 3663771856, 1141124467, 855842277, 2852801631, 3708648649, 1342533948, 654459306, 3188396048, 3373015174, 1466479909, 544179635, 3110523913, 3462522015, 1591671054, 702138776, 2966460450, 3352799412, 1504918807, 783551873, 3082640443, 3233442989, 3988292384, 2596254646, 62317068, 1957810842, 3939845945, 2647816111, 81470997, 1943803523, 3814918930, 2489596804, 225274430, 2053790376, 3826175755, 2466906013, 167816743, 2097651377, 4027552580, 2265490386, 503444072, 1762050814, 4150417245, 2154129355, 426522225, 1852507879, 4275313526, 2312317920, 282753626, 1742555852, 4189708143, 2394877945, 397917763, 1622183637, 3604390888, 2714866558, 953729732, 1340076626, 3518719985, 2797360999, 1068828381, 1219638859, 3624741850, 2936675148, 906185462, 1090812512, 3747672003, 2825379669, 829329135, 1181335161, 3412177804, 3160834842, 628085408, 1382605366, 3423369109, 3138078467, 570562233, 1426400815, 3317316542, 2998733608, 733239954, 1555261956, 3268935591, 3050360625, 752459403, 1541320221, 2607071920, 3965973030, 1969922972, 40735498, 2617837225, 3943577151, 1913087877, 83908371, 2512341634, 3803740692, 2075208622, 213261112, 2463272603, 3855990285, 2094854071, 198958881, 2262029012, 4057260610, 1759359992, 534414190, 2176718541, 4139329115, 1873836001, 414664567, 2282248934, 4279200368, 1711684554, 285281116, 2405801727, 4167216745, 1634467795, 376229701, 2685067896, 3608007406, 1308918612, 956543938, 2808555105, 3495958263, 1231636301, 1047427035, 2932959818, 3654703836, 1088359270, 936918e3, 2847714899, 3736837829, 1202900863, 817233897, 3183342108, 3401237130, 1404277552, 615818150, 3134207493, 3453421203, 1423857449, 601450431, 3009837614, 3294710456, 1567103746, 711928724, 3020668471, 3272380065, 1510334235, 755167117 ], crc32: function crc32(data) { var tbl = util.crypto.crc32Table; var crc = 0 ^ -1; if (typeof data === ""string"") { data = util.buffer.toBuffer(data); } for (var i = 0; i < data.length; i++) { var code = data.readUInt8(i); crc = crc >>> 8 ^ tbl[(crc ^ code) & 255]; } return (crc ^ -1) >>> 0; }, hmac: function hmac(key, string, digest, fn) { if (!digest) digest = ""binary""; if (digest === ""buffer"") { digest = undefined; } if (!fn) fn = ""sha256""; if (typeof string === ""string"") string = util.buffer.toBuffer(string); return util.crypto.lib.createHmac(fn, key).update(string).digest(digest); }, md5: function md5(data, digest, callback) { return util.crypto.hash(""md5"", data, digest, callback); }, sha256: function sha256(data, digest, callback) { return util.crypto.hash(""sha256"", data, digest, callback); }, hash: function(algorithm, data, digest, callback) { var hash = util.crypto.createHash(algorithm); if (!digest) { digest = ""binary""; } if (digest === ""buffer"") { digest = undefined; } if (typeof data === ""string"") data = util.buffer.toBuffer(data); var sliceFn = util.arraySliceFn(data); var isBuffer = util.Buffer.isBuffer(data); if (util.isBrowser() && typeof ArrayBuffer !== ""undefined"" && data && data.buffer instanceof ArrayBuffer) isBuffer = true; if (callback && typeof data === ""object"" && typeof data.on === ""function"" && !isBuffer) { data.on(""data"", function(chunk) { hash.update(chunk); }); data.on(""error"", function(err) { callback(err); }); data.on(""end"", function() { callback(null, hash.digest(digest)); }); } else if (callback && sliceFn && !isBuffer && typeof FileReader !== ""undefined"") { var index = 0, size = 1024 * 512; var reader = new FileReader(); reader.onerror = function() { callback(new Error(""Failed to read data."")); }; reader.onload = function() { var buf = new util.Buffer(new Uint8Array(reader.result)); hash.update(buf); index += buf.length; reader._continueReading(); }; reader._continueReading = function() { if (index >= data.size) { callback(null, hash.digest(digest)); return; } var back = index + size; if (back > data.size) back = data.size; reader.readAsArrayBuffer(sliceFn.call(data, index, back)); }; reader._continueReading(); } else { if (util.isBrowser() && typeof data === ""object"" && !isBuffer) { data = new util.Buffer(new Uint8Array(data)); } var out = hash.update(data).digest(digest); if (callback) callback(null, out); return out; } }, toHex: function toHex(data) { var out = []; for (var i = 0; i < data.length; i++) { out.push((""0"" + data.charCodeAt(i).toString(16)).substr(-2, 2)); } return out.join(""""); }, createHash: function createHash(algorithm) { return util.crypto.lib.createHash(algorithm); } }, abort: {}, each: function each(object, iterFunction) { for (var key in object) { if (Object.prototype.hasOwnProperty.call(object, key)) { var ret = iterFunction.call(this, key, object[key]); if (ret === util.abort) break; } } }, arrayEach: function arrayEach(array, iterFunction) { for (var idx in array) { if (Object.prototype.hasOwnProperty.call(array, idx)) { var ret = iterFunction.call(this, array[idx], parseInt(idx, 10)); if (ret === util.abort) break; } } }, update: function update(obj1, obj2) { util.each(obj2, function iterator(key, item) { obj1[key] = item; }); return obj1; }, merge: function merge(obj1, obj2) { return util.update(util.copy(obj1), obj2); }, copy: function copy(object) { if (object === null || object === undefined) return object; var dupe = {}; for (var key in object) { dupe[key] = object[key]; } return dupe; }, isEmpty: function isEmpty(obj) { for (var prop in obj) { if (Object.prototype.hasOwnProperty.call(obj, prop)) { return false; } } return true; }, arraySliceFn: function arraySliceFn(obj) { var fn = obj.slice || obj.webkitSlice || obj.mozSlice; return typeof fn === ""function"" ? fn : null; }, isType: function isType(obj, type) { if (typeof type === ""function"") type = util.typeName(type); return Object.prototype.toString.call(obj) === ""[object "" + type + ""]""; }, typeName: function typeName(type) { if (Object.prototype.hasOwnProperty.call(type, ""name"")) return type.name; var str = type.toString(); var match = str.match(/^\s*function (.+)\(/); return match ? match[1] : str; }, error: function error(err, options) { var originalError = null; if (typeof err.message === ""string"" && err.message !== """") { if (typeof options === ""string"" || options && options.message) { originalError = util.copy(err); originalError.message = err.message; } } err.message = err.message || null; if (typeof options === ""string"") { err.message = options; } else if (typeof options === ""object"" && options !== null) { util.update(err, options); if (options.message) err.message = options.message; if (options.code || options.name) err.code = options.code || options.name; if (options.stack) err.stack = options.stack; } if (typeof Object.defineProperty === ""function"") { Object.defineProperty(err, ""name"", { writable: true, enumerable: false }); Object.defineProperty(err, ""message"", { enumerable: true }); } err.name = options && options.name || err.name || err.code || ""Error""; err.time = new Date(); if (originalError) err.originalError = originalError; return err; }, inherit: function inherit(klass, features) { var newObject = null; if (features === undefined) { features = klass; klass = Object; newObject = {}; } else { var ctor = function ConstructorWrapper() {}; ctor.prototype = klass.prototype; newObject = new ctor(); } if (features.constructor === Object) { features.constructor = function() { if (klass !== Object) { return klass.apply(this, arguments); } }; } features.constructor.prototype = newObject; util.update(features.constructor.prototype, features); features.constructor.__super__ = klass; return features.constructor; }, mixin: function mixin() { var klass = arguments[0]; for (var i = 1; i < arguments.length; i++) { for (var prop in arguments[i].prototype) { var fn = arguments[i].prototype[prop]; if (prop !== ""constructor"") { klass.prototype[prop] = fn; } } } return klass; }, hideProperties: function hideProperties(obj, props) { if (typeof Object.defineProperty !== ""function"") return; util.arrayEach(props, function(key) { Object.defineProperty(obj, key, { enumerable: false, writable: true, configurable: true }); }); }, property: function property(obj, name, value, enumerable, isValue) { var opts = { configurable: true, enumerable: enumerable !== undefined ? enumerable : true }; if (typeof value === ""function"" && !isValue) { opts.get = value; } else { opts.value = value; opts.writable = true; } Object.defineProperty(obj, name, opts); }, memoizedProperty: function memoizedProperty(obj, name, get, enumerable) { var cachedValue = null; util.property(obj, name, function() { if (cachedValue === null) { cachedValue = get(); } return cachedValue; }, enumerable); }, hoistPayloadMember: function hoistPayloadMember(resp) { var req = resp.request; var operationName = req.operation; var operation = req.service.api.operations[operationName]; var output = operation.output; if (output.payload && !operation.hasEventOutput) { var payloadMember = output.members[output.payload]; var responsePayload = resp.data[output.payload]; if (payloadMember.type === ""structure"") { util.each(responsePayload, function(key, value) { util.property(resp.data, key, value, false); }); } } }, computeSha256: function computeSha256(body, done) { if (util.isNode()) { var Stream = util.stream.Stream; var fs = require(""fs""); if (typeof Stream === ""function"" && body instanceof Stream) { if (typeof body.path === ""string"") { var settings = {}; if (typeof body.start === ""number"") { settings.start = body.start; } if (typeof body.end === ""number"") { settings.end = body.end; } body = fs.createReadStream(body.path, settings); } else { return done(new Error(""Non-file stream objects are "" + ""not supported with SigV4"")); } } } util.crypto.sha256(body, ""hex"", function(err, sha) { if (err) done(err); else done(null, sha); }); }, isClockSkewed: function isClockSkewed(serverTime) { if (serverTime) { util.property(AWS.config, ""isClockSkewed"", Math.abs(new Date().getTime() - serverTime) >= 3e5, false); return AWS.config.isClockSkewed; } }, applyClockOffset: function applyClockOffset(serverTime) { if (serverTime) AWS.config.systemClockOffset = serverTime - new Date().getTime(); }, extractRequestId: function extractRequestId(resp) { var requestId = resp.httpResponse.headers[""x-amz-request-id""] || resp.httpResponse.headers[""x-amzn-requestid""]; if (!requestId && resp.data && resp.data.ResponseMetadata) { requestId = resp.data.ResponseMetadata.RequestId; } if (requestId) { resp.requestId = requestId; } if (resp.error) { resp.error.requestId = requestId; } }, addPromises: function addPromises(constructors, PromiseDependency) { var deletePromises = false; if (PromiseDependency === undefined && AWS && AWS.config) { PromiseDependency = AWS.config.getPromisesDependency(); } if (PromiseDependency === undefined && typeof Promise !== ""undefined"") { PromiseDependency = Promise; } if (typeof PromiseDependency !== ""function"") deletePromises = true; if (!Array.isArray(constructors)) constructors = [ constructors ]; for (var ind = 0; ind < constructors.length; ind++) { var constructor = constructors[ind]; if (deletePromises) { if (constructor.deletePromisesFromClass) { constructor.deletePromisesFromClass(); } } else if (constructor.addPromisesToClass) { constructor.addPromisesToClass(PromiseDependency); } } }, promisifyMethod: function promisifyMethod(methodName, PromiseDependency) { return function promise() { var self = this; var args = Array.prototype.slice.call(arguments); return new PromiseDependency(function(resolve, reject) { args.push(function(err, data) { if (err) { reject(err); } else { resolve(data); } }); self[methodName].apply(self, args); }); }; }, isDualstackAvailable: function isDualstackAvailable(service) { if (!service) return false; var metadata = require(""../apis/metadata.json""); if (typeof service !== ""string"") service = service.serviceIdentifier; if (typeof service !== ""string"" || !metadata.hasOwnProperty(service)) return false; return !!metadata[service].dualstackAvailable; }, calculateRetryDelay: function calculateRetryDelay(retryCount, retryDelayOptions) { if (!retryDelayOptions) retryDelayOptions = {}; var customBackoff = retryDelayOptions.customBackoff || null; if (typeof customBackoff === ""function"") { return customBackoff(retryCount); } var base = typeof retryDelayOptions.base === ""number"" ? retryDelayOptions.base : 100; var delay = Math.random() * (Math.pow(2, retryCount) * base); return delay; }, handleRequestWithRetries: function handleRequestWithRetries(httpRequest, options, cb) { if (!options) options = {}; var http = AWS.HttpClient.getInstance(); var httpOptions = options.httpOptions || {}; var retryCount = 0; var errCallback = function(err) { var maxRetries = options.maxRetries || 0; if (err && err.code === ""TimeoutError"") err.retryable = true; if (err && err.retryable && retryCount < maxRetries) { retryCount++; var delay = util.calculateRetryDelay(retryCount, options.retryDelayOptions); setTimeout(sendRequest, delay + (err.retryAfter || 0)); } else { cb(err); } }; var sendRequest = function() { var data = """"; http.handleRequest(httpRequest, httpOptions, function(httpResponse) { httpResponse.on(""data"", function(chunk) { data += chunk.toString(); }); httpResponse.on(""end"", function() { var statusCode = httpResponse.statusCode; if (statusCode < 300) { cb(null, data); } else { var retryAfter = parseInt(httpResponse.headers[""retry-after""], 10) * 1e3 || 0; var err = util.error(new Error(), { retryable: statusCode >= 500 || statusCode === 429 }); if (retryAfter && err.retryable) err.retryAfter = retryAfter; errCallback(err); } }); }, errCallback); }; AWS.util.defer(sendRequest); }, uuid: { v4: function uuidV4() { return require(""uuid"").v4(); } }, convertPayloadToString: function convertPayloadToString(resp) { var req = resp.request; var operation = req.operation; var rules = req.service.api.operations[operation].output || {}; if (rules.payload && resp.data[rules.payload]) { resp.data[rules.payload] = resp.data[rules.payload].toString(); } }, defer: function defer(callback) { if (typeof process === ""object"" && typeof process.nextTick === ""function"") { process.nextTick(callback); } else if (typeof setImmediate === ""function"") { setImmediate(callback); } else { setTimeout(callback, 0); } }, getRequestPayloadShape: function getRequestPayloadShape(req) { var operations = req.service.api.operations; if (!operations) return undefined; var operation = (operations || {})[req.operation]; if (!operation || !operation.input || !operation.input.payload) return undefined; return operation.input.members[operation.input.payload]; }, getProfilesFromSharedConfig: function getProfilesFromSharedConfig(iniLoader, filename) { var profiles = {}; var profilesFromConfig = {}; if (process.env[util.configOptInEnv]) { var profilesFromConfig = iniLoader.loadFrom({ isConfig: true, filename: process.env[util.sharedConfigFileEnv] }); } var profilesFromCreds = iniLoader.loadFrom({ filename: filename || process.env[util.configOptInEnv] && process.env[util.sharedCredentialsFileEnv] }); for (var i = 0, profileNames = Object.keys(profilesFromConfig); i < profileNames.length; i++) { profiles[profileNames[i]] = profilesFromConfig[profileNames[i]]; } for (var i = 0, profileNames = Object.keys(profilesFromCreds); i < profileNames.length; i++) { profiles[profileNames[i]] = profilesFromCreds[profileNames[i]]; } return profiles; }, defaultProfile: ""default"", configOptInEnv: ""AWS_SDK_LOAD_CONFIG"", sharedCredentialsFileEnv: ""AWS_SHARED_CREDENTIALS_FILE"", sharedConfigFileEnv: ""AWS_CONFIG_FILE"", imdsDisabledEnv: ""AWS_EC2_METADATA_DISABLED"" }; module.exports = util; }).call(this, require(""_process""), require(""timers"").setImmediate); }, { ""../apis/metadata.json"": 26, ""./core"": 38, _process: 8, fs: 2, timers: 16, uuid: 21 } ], 37: [ function(require, module, exports) { var AWS = require(""./core""); require(""./credentials""); require(""./credentials/credential_provider_chain""); var PromisesDependency; AWS.Config = AWS.util.inherit({ constructor: function Config(options) { if (options === undefined) options = {}; options = this.extractCredentials(options); AWS.util.each.call(this, this.keys, function(key, value) { this.set(key, options[key], value); }); }, getCredentials: function getCredentials(callback) { var self = this; function finish(err) { callback(err, err ? null : self.credentials); } function credError(msg, err) { return new AWS.util.error(err || new Error(), { code: ""CredentialsError"", message: msg, name: ""CredentialsError"" }); } function getAsyncCredentials() { self.credentials.get(function(err) { if (err) { var msg = ""Could not load credentials from "" + self.credentials.constructor.name; err = credError(msg, err); } finish(err); }); } function getStaticCredentials() { var err = null; if (!self.credentials.accessKeyId || !self.credentials.secretAccessKey) { err = credError(""Missing credentials""); } finish(err); } if (self.credentials) { if (typeof self.credentials.get === ""function"") { getAsyncCredentials(); } else { getStaticCredentials(); } } else if (self.credentialProvider) { self.credentialProvider.resolve(function(err, creds) { if (err) { err = credError(""Could not load credentials from any providers"", err); } self.credentials = creds; finish(err); }); } else { finish(credError(""No credentials to load"")); } }, update: function update(options, allowUnknownKeys) { allowUnknownKeys = allowUnknownKeys || false; options = this.extractCredentials(options); AWS.util.each.call(this, options, function(key, value) { if (allowUnknownKeys || Object.prototype.hasOwnProperty.call(this.keys, key) || AWS.Service.hasService(key)) { this.set(key, value); } }); }, loadFromPath: function loadFromPath(path) { this.clear(); var options = JSON.parse(AWS.util.readFileSync(path)); var fileSystemCreds = new AWS.FileSystemCredentials(path); var chain = new AWS.CredentialProviderChain(); chain.providers.unshift(fileSystemCreds); chain.resolve(function(err, creds) { if (err) throw err; else options.credentials = creds; }); this.constructor(options); return this; }, clear: function clear() { AWS.util.each.call(this, this.keys, function(key) { delete this[key]; }); this.set(""credentials"", undefined); this.set(""credentialProvider"", undefined); }, set: function set(property, value, defaultValue) { if (value === undefined) { if (defaultValue === undefined) { defaultValue = this.keys[property]; } if (typeof defaultValue === ""function"") { this[property] = defaultValue.call(this); } else { this[property] = defaultValue; } } else if (property === ""httpOptions"" && this[property]) { this[property] = AWS.util.merge(this[property], value); } else { this[property] = value; } }, keys: { credentials: null, credentialProvider: null, region: null, logger: null, apiVersions: {}, apiVersion: null, endpoint: undefined, httpOptions: { timeout: 12e4 }, maxRetries: undefined, maxRedirects: 10, paramValidation: true, sslEnabled: true, s3ForcePathStyle: false, s3BucketEndpoint: false, s3DisableBodySigning: true, computeChecksums: true, convertResponseTypes: true, correctClockSkew: false, customUserAgent: null, dynamoDbCrc32: true, systemClockOffset: 0, signatureVersion: null, signatureCache: true, retryDelayOptions: {}, useAccelerateEndpoint: false, clientSideMonitoring: false, endpointDiscoveryEnabled: false, endpointCacheSize: 1e3, hostPrefixEnabled: true, stsRegionalEndpoints: null }, extractCredentials: function extractCredentials(options) { if (options.accessKeyId && options.secretAccessKey) { options = AWS.util.copy(options); options.credentials = new AWS.Credentials(options); } return options; }, setPromisesDependency: function setPromisesDependency(dep) { PromisesDependency = dep; if (dep === null && typeof Promise === ""function"") { PromisesDependency = Promise; } var constructors = [ AWS.Request, AWS.Credentials, AWS.CredentialProviderChain ]; if (AWS.S3) { constructors.push(AWS.S3); if (AWS.S3.ManagedUpload) { constructors.push(AWS.S3.ManagedUpload); } } AWS.util.addPromises(constructors, PromisesDependency); }, getPromisesDependency: function getPromisesDependency() { return PromisesDependency; } }); AWS.config = new AWS.Config(); }, { ""./core"": 38, ""./credentials"": 39, ""./credentials/credential_provider_chain"": 42 } ], 42: [ function(require, module, exports) { var AWS = require(""../core""); AWS.CredentialProviderChain = AWS.util.inherit(AWS.Credentials, { constructor: function CredentialProviderChain(providers) { if (providers) { this.providers = providers; } else { this.providers = AWS.CredentialProviderChain.defaultProviders.slice(0); } this.resolveCallbacks = []; }, resolve: function resolve(callback) { var self = this; if (self.providers.length === 0) { callback(new Error(""No providers"")); return self; } if (self.resolveCallbacks.push(callback) === 1) { var index = 0; var providers = self.providers.slice(0); function resolveNext(err, creds) { if (!err && creds || index === providers.length) { AWS.util.arrayEach(self.resolveCallbacks, function(callback) { callback(err, creds); }); self.resolveCallbacks.length = 0; return; } var provider = providers[index++]; if (typeof provider === ""function"") { creds = provider.call(); } else { creds = provider; } if (creds.get) { creds.get(function(getErr) { resolveNext(getErr, getErr ? null : creds); }); } else { resolveNext(null, creds); } } resolveNext(); } return self; } }); AWS.CredentialProviderChain.defaultProviders = []; AWS.CredentialProviderChain.addPromisesToClass = function addPromisesToClass(PromiseDependency) { this.prototype.resolvePromise = AWS.util.promisifyMethod(""resolve"", PromiseDependency); }; AWS.CredentialProviderChain.deletePromisesFromClass = function deletePromisesFromClass() { delete this.prototype.resolvePromise; }; AWS.util.addPromises(AWS.CredentialProviderChain); }, { ""../core"": 38 } ], 39: [ function(require, module, exports) { var AWS = require(""./core""); AWS.Credentials = AWS.util.inherit({ constructor: function Credentials() { AWS.util.hideProperties(this, [ ""secretAccessKey"" ]); this.expired = false; this.expireTime = null; this.refreshCallbacks = []; if (arguments.length === 1 && typeof arguments[0] === ""object"") { var creds = arguments[0].credentials || arguments[0]; this.accessKeyId = creds.accessKeyId; this.secretAccessKey = creds.secretAccessKey; this.sessionToken = creds.sessionToken; } else { this.accessKeyId = arguments[0]; this.secretAccessKey = arguments[1]; this.sessionToken = arguments[2]; } }, expiryWindow: 15, needsRefresh: function needsRefresh() { var currentTime = AWS.util.date.getDate().getTime(); var adjustedTime = new Date(currentTime + this.expiryWindow * 1e3); if (this.expireTime && adjustedTime > this.expireTime) { return true; } else { return this.expired || !this.accessKeyId || !this.secretAccessKey; } }, get: function get(callback) { var self = this; if (this.needsRefresh()) { this.refresh(function(err) { if (!err) self.expired = false; if (callback) callback(err); }); } else if (callback) { callback(); } }, refresh: function refresh(callback) { this.expired = false; callback(); }, coalesceRefresh: function coalesceRefresh(callback, sync) { var self = this; if (self.refreshCallbacks.push(callback) === 1) { self.load(function onLoad(err) { AWS.util.arrayEach(self.refreshCallbacks, function(callback) { if (sync) { callback(err); } else { AWS.util.defer(function() { callback(err); }); } }); self.refreshCallbacks.length = 0; }); } }, load: function load(callback) { callback(); } }); AWS.Credentials.addPromisesToClass = function addPromisesToClass(PromiseDependency) { this.prototype.getPromise = AWS.util.promisifyMethod(""get"", PromiseDependency); this.prototype.refreshPromise = AWS.util.promisifyMethod(""refresh"", PromiseDependency); }; AWS.Credentials.deletePromisesFromClass = function deletePromisesFromClass() { delete this.prototype.getPromise; delete this.prototype.refreshPromise; }; AWS.util.addPromises(AWS.Credentials); }, { ""./core"": 38 } ], 27: [ function(require, module, exports) { function apiLoader(svc, version) { if (!apiLoader.services.hasOwnProperty(svc)) { throw new Error(""InvalidService: Failed to load api for "" + svc); } return apiLoader.services[svc][version]; } apiLoader.services = {}; module.exports = apiLoader; }, {} ], 26: [ function(require, module, exports) { module.exports = { acm: { name: ""ACM"", cors: true }, apigateway: { name: ""APIGateway"", cors: true }, applicationautoscaling: { prefix: ""application-autoscaling"", name: ""ApplicationAutoScaling"", cors: true }, appstream: { name: ""AppStream"" }, autoscaling: { name: ""AutoScaling"", cors: true }, batch: { name: ""Batch"" }, budgets: { name: ""Budgets"" }, clouddirectory: { name: ""CloudDirectory"", versions: [ ""2016-05-10*"" ] }, cloudformation: { name: ""CloudFormation"", cors: true }, cloudfront: { name: ""CloudFront"", versions: [ ""2013-05-12*"", ""2013-11-11*"", ""2014-05-31*"", ""2014-10-21*"", ""2014-11-06*"", ""2015-04-17*"", ""2015-07-27*"", ""2015-09-17*"", ""2016-01-13*"", ""2016-01-28*"", ""2016-08-01*"", ""2016-08-20*"", ""2016-09-07*"", ""2016-09-29*"", ""2016-11-25*"", ""2017-03-25*"", ""2017-10-30*"", ""2018-06-18*"", ""2018-11-05*"" ], cors: true }, cloudhsm: { name: ""CloudHSM"", cors: true }, cloudsearch: { name: ""CloudSearch"" }, cloudsearchdomain: { name: ""CloudSearchDomain"" }, cloudtrail: { name: ""CloudTrail"", cors: true }, cloudwatch: { prefix: ""monitoring"", name: ""CloudWatch"", cors: true }, cloudwatchevents: { prefix: ""events"", name: ""CloudWatchEvents"", versions: [ ""2014-02-03*"" ], cors: true }, cloudwatchlogs: { prefix: ""logs"", name: ""CloudWatchLogs"", cors: true }, codebuild: { name: ""CodeBuild"", cors: true }, codecommit: { name: ""CodeCommit"", cors: true }, codedeploy: { name: ""CodeDeploy"", cors: true }, codepipeline: { name: ""CodePipeline"", cors: true }, cognitoidentity: { prefix: ""cognito-identity"", name: ""CognitoIdentity"", cors: true }, cognitoidentityserviceprovider: { prefix: ""cognito-idp"", name: ""CognitoIdentityServiceProvider"", cors: true }, cognitosync: { prefix: ""cognito-sync"", name: ""CognitoSync"", cors: true }, configservice: { prefix: ""config"", name: ""ConfigService"", cors: true }, cur: { name: ""CUR"", cors: true }, datapipeline: { name: ""DataPipeline"" }, devicefarm: { name: ""DeviceFarm"", cors: true }, directconnect: { name: ""DirectConnect"", cors: true }, directoryservice: { prefix: ""ds"", name: ""DirectoryService"" }, discovery: { name: ""Discovery"" }, dms: { name: ""DMS"" }, dynamodb: { name: ""DynamoDB"", cors: true }, dynamodbstreams: { prefix: ""streams.dynamodb"", name: ""DynamoDBStreams"", cors: true }, ec2: { name: ""EC2"", versions: [ ""2013-06-15*"", ""2013-10-15*"", ""2014-02-01*"", ""2014-05-01*"", ""2014-06-15*"", ""2014-09-01*"", ""2014-10-01*"", ""2015-03-01*"", ""2015-04-15*"", ""2015-10-01*"", ""2016-04-01*"", ""2016-09-15*"" ], cors: true }, ecr: { name: ""ECR"", cors: true }, ecs: { name: ""ECS"", cors: true }, efs: { prefix: ""elasticfilesystem"", name: ""EFS"", cors: true }, elasticache: { name: ""ElastiCache"", versions: [ ""2012-11-15*"", ""2014-03-24*"", ""2014-07-15*"", ""2014-09-30*"" ], cors: true }, elasticbeanstalk: { name: ""ElasticBeanstalk"", cors: true }, elb: { prefix: ""elasticloadbalancing"", name: ""ELB"", cors: true }, elbv2: { prefix: ""elasticloadbalancingv2"", name: ""ELBv2"", cors: true }, emr: { prefix: ""elasticmapreduce"", name: ""EMR"", cors: true }, es: { name: ""ES"" }, elastictranscoder: { name: ""ElasticTranscoder"", cors: true }, firehose: { name: ""Firehose"", cors: true }, gamelift: { name: ""GameLift"", cors: true }, glacier: { name: ""Glacier"" }, health: { name: ""Health"" }, iam: { name: ""IAM"", cors: true }, importexport: { name: ""ImportExport"" }, inspector: { name: ""Inspector"", versions: [ ""2015-08-18*"" ], cors: true }, iot: { name: ""Iot"", cors: true }, iotdata: { prefix: ""iot-data"", name: ""IotData"", cors: true }, kinesis: { name: ""Kinesis"", cors: true }, kinesisanalytics: { name: ""KinesisAnalytics"" }, kms: { name: ""KMS"", cors: true }, lambda: { name: ""Lambda"", cors: true }, lexruntime: { prefix: ""runtime.lex"", name: ""LexRuntime"", cors: true }, lightsail: { name: ""Lightsail"" }, machinelearning: { name: ""MachineLearning"", cors: true }, marketplacecommerceanalytics: { name: ""MarketplaceCommerceAnalytics"", cors: true }, marketplacemetering: { prefix: ""meteringmarketplace"", name: ""MarketplaceMetering"" }, mturk: { prefix: ""mturk-requester"", name: ""MTurk"", cors: true }, mobileanalytics: { name: ""MobileAnalytics"", cors: true }, opsworks: { name: ""OpsWorks"", cors: true }, opsworkscm: { name: ""OpsWorksCM"" }, organizations: { name: ""Organizations"" }, pinpoint: { name: ""Pinpoint"" }, polly: { name: ""Polly"", cors: true }, rds: { name: ""RDS"", versions: [ ""2014-09-01*"" ], cors: true }, redshift: { name: ""Redshift"", cors: true }, rekognition: { name: ""Rekognition"", cors: true }, resourcegroupstaggingapi: { name: ""ResourceGroupsTaggingAPI"" }, route53: { name: ""Route53"", cors: true }, route53domains: { name: ""Route53Domains"", cors: true }, s3: { name: ""S3"", dualstackAvailable: true, cors: true }, s3control: { name: ""S3Control"", dualstackAvailable: true }, servicecatalog: { name: ""ServiceCatalog"", cors: true }, ses: { prefix: ""email"", name: ""SES"", cors: true }, shield: { name: ""Shield"" }, simpledb: { prefix: ""sdb"", name: ""SimpleDB"" }, sms: { name: ""SMS"" }, snowball: { name: ""Snowball"" }, sns: { name: ""SNS"", cors: true }, sqs: { name: ""SQS"", cors: true }, ssm: { name: ""SSM"", cors: true }, storagegateway: { name: ""StorageGateway"", cors: true }, stepfunctions: { prefix: ""states"", name: ""StepFunctions"" }, sts: { name: ""STS"", cors: true }, support: { name: ""Support"" }, swf: { name: ""SWF"" }, xray: { name: ""XRay"", cors: true }, waf: { name: ""WAF"", cors: true }, wafregional: { prefix: ""waf-regional"", name: ""WAFRegional"" }, workdocs: { name: ""WorkDocs"", cors: true }, workspaces: { name: ""WorkSpaces"" }, codestar: { name: ""CodeStar"" }, lexmodelbuildingservice: { prefix: ""lex-models"", name: ""LexModelBuildingService"", cors: true }, marketplaceentitlementservice: { prefix: ""entitlement.marketplace"", name: ""MarketplaceEntitlementService"" }, athena: { name: ""Athena"" }, greengrass: { name: ""Greengrass"" }, dax: { name: ""DAX"" }, migrationhub: { prefix: ""AWSMigrationHub"", name: ""MigrationHub"" }, cloudhsmv2: { name: ""CloudHSMV2"" }, glue: { name: ""Glue"" }, mobile: { name: ""Mobile"" }, pricing: { name: ""Pricing"", cors: true }, costexplorer: { prefix: ""ce"", name: ""CostExplorer"", cors: true }, mediaconvert: { name: ""MediaConvert"" }, medialive: { name: ""MediaLive"" }, mediapackage: { name: ""MediaPackage"" }, mediastore: { name: ""MediaStore"" }, mediastoredata: { prefix: ""mediastore-data"", name: ""MediaStoreData"", cors: true }, appsync: { name: ""AppSync"" }, guardduty: { name: ""GuardDuty"" }, mq: { name: ""MQ"" }, comprehend: { name: ""Comprehend"", cors: true }, iotjobsdataplane: { prefix: ""iot-jobs-data"", name: ""IoTJobsDataPlane"" }, kinesisvideoarchivedmedia: { prefix: ""kinesis-video-archived-media"", name: ""KinesisVideoArchivedMedia"", cors: true }, kinesisvideomedia: { prefix: ""kinesis-video-media"", name: ""KinesisVideoMedia"", cors: true }, kinesisvideo: { name: ""KinesisVideo"", cors: true }, sagemakerruntime: { prefix: ""runtime.sagemaker"", name: ""SageMakerRuntime"" }, sagemaker: { name: ""SageMaker"" }, translate: { name: ""Translate"", cors: true }, resourcegroups: { prefix: ""resource-groups"", name: ""ResourceGroups"", cors: true }, alexaforbusiness: { name: ""AlexaForBusiness"" }, cloud9: { name: ""Cloud9"" }, serverlessapplicationrepository: { prefix: ""serverlessrepo"", name: ""ServerlessApplicationRepository"" }, servicediscovery: { name: ""ServiceDiscovery"" }, workmail: { name: ""WorkMail"" }, autoscalingplans: { prefix: ""autoscaling-plans"", name: ""AutoScalingPlans"" }, transcribeservice: { prefix: ""transcribe"", name: ""TranscribeService"" }, connect: { name: ""Connect"" }, acmpca: { prefix: ""acm-pca"", name: ""ACMPCA"" }, fms: { name: ""FMS"" }, secretsmanager: { name: ""SecretsManager"", cors: true }, iotanalytics: { name: ""IoTAnalytics"", cors: true }, iot1clickdevicesservice: { prefix: ""iot1click-devices"", name: ""IoT1ClickDevicesService"" }, iot1clickprojects: { prefix: ""iot1click-projects"", name: ""IoT1ClickProjects"" }, pi: { name: ""PI"" }, neptune: { name: ""Neptune"" }, mediatailor: { name: ""MediaTailor"" }, eks: { name: ""EKS"" }, macie: { name: ""Macie"" }, dlm: { name: ""DLM"" }, signer: { name: ""Signer"" }, chime: { name: ""Chime"" }, pinpointemail: { prefix: ""pinpoint-email"", name: ""PinpointEmail"" }, ram: { name: ""RAM"" }, route53resolver: { name: ""Route53Resolver"" }, pinpointsmsvoice: { prefix: ""sms-voice"", name: ""PinpointSMSVoice"" }, quicksight: { name: ""QuickSight"" }, rdsdataservice: { prefix: ""rds-data"", name: ""RDSDataService"" }, amplify: { name: ""Amplify"" }, datasync: { name: ""DataSync"" }, robomaker: { name: ""RoboMaker"" }, transfer: { name: ""Transfer"" }, globalaccelerator: { name: ""GlobalAccelerator"" }, comprehendmedical: { name: ""ComprehendMedical"", cors: true }, kinesisanalyticsv2: { name: ""KinesisAnalyticsV2"" }, mediaconnect: { name: ""MediaConnect"" }, fsx: { name: ""FSx"" }, securityhub: { name: ""SecurityHub"" }, appmesh: { name: ""AppMesh"", versions: [ ""2018-10-01*"" ] }, licensemanager: { prefix: ""license-manager"", name: ""LicenseManager"" }, kafka: { name: ""Kafka"" }, apigatewaymanagementapi: { name: ""ApiGatewayManagementApi"" }, apigatewayv2: { name: ""ApiGatewayV2"" }, docdb: { name: ""DocDB"" }, backup: { name: ""Backup"" }, worklink: { name: ""WorkLink"" }, textract: { name: ""Textract"" }, managedblockchain: { name: ""ManagedBlockchain"" }, mediapackagevod: { prefix: ""mediapackage-vod"", name: ""MediaPackageVod"" }, groundstation: { name: ""GroundStation"" }, iotthingsgraph: { name: ""IoTThingsGraph"" }, iotevents: { name: ""IoTEvents"" }, ioteventsdata: { prefix: ""iotevents-data"", name: ""IoTEventsData"" }, personalize: { name: ""Personalize"", cors: true }, personalizeevents: { prefix: ""personalize-events"", name: ""PersonalizeEvents"", cors: true }, personalizeruntime: { prefix: ""personalize-runtime"", name: ""PersonalizeRuntime"", cors: true }, applicationinsights: { prefix: ""application-insights"", name: ""ApplicationInsights"" }, servicequotas: { prefix: ""service-quotas"", name: ""ServiceQuotas"" }, ec2instanceconnect: { prefix: ""ec2-instance-connect"", name: ""EC2InstanceConnect"" }, eventbridge: { name: ""EventBridge"" }, lakeformation: { name: ""LakeFormation"" }, forecastservice: { prefix: ""forecast"", name: ""ForecastService"", cors: true }, forecastqueryservice: { prefix: ""forecastquery"", name: ""ForecastQueryService"", cors: true }, qldb: { name: ""QLDB"" }, qldbsession: { prefix: ""qldb-session"", name: ""QLDBSession"" }, workmailmessageflow: { name: ""WorkMailMessageFlow"" } }; }, {} ], 21: [ function(require, module, exports) { var v1 = require(""./v1""); var v4 = require(""./v4""); var uuid = v4; uuid.v1 = v1; uuid.v4 = v4; module.exports = uuid; }, { ""./v1"": 24, ""./v4"": 25 } ], 25: [ function(require, module, exports) { var rng = require(""./lib/rng""); var bytesToUuid = require(""./lib/bytesToUuid""); function v4(options, buf, offset) { var i = buf && offset || 0; if (typeof options == ""string"") { buf = options === ""binary"" ? new Array(16) : null; options = null; } options = options || {}; var rnds = options.random || (options.rng || rng)(); rnds[6] = rnds[6] & 15 | 64; rnds[8] = rnds[8] & 63 | 128; if (buf) { for (var ii = 0; ii < 16; ++ii) { buf[i + ii] = rnds[ii]; } } return buf || bytesToUuid(rnds); } module.exports = v4; }, { ""./lib/bytesToUuid"": 22, ""./lib/rng"": 23 } ], 24: [ function(require, module, exports) { var rng = require(""./lib/rng""); var bytesToUuid = require(""./lib/bytesToUuid""); var _nodeId; var _clockseq; var _lastMSecs = 0; var _lastNSecs = 0; function v1(options, buf, offset) { var i = buf && offset || 0; var b = buf || []; options = options || {}; var node = options.node || _nodeId; var clockseq = options.clockseq !== undefined ? options.clockseq : _clockseq; if (node == null || clockseq == null) { var seedBytes = rng(); if (node == null) { node = _nodeId = [ seedBytes[0] | 1, seedBytes[1], seedBytes[2], seedBytes[3], seedBytes[4], seedBytes[5] ]; } if (clockseq == null) { clockseq = _clockseq = (seedBytes[6] << 8 | seedBytes[7]) & 16383; } } var msecs = options.msecs !== undefined ? options.msecs : new Date().getTime(); var nsecs = options.nsecs !== undefined ? options.nsecs : _lastNSecs + 1; var dt = msecs - _lastMSecs + (nsecs - _lastNSecs) / 1e4; if (dt < 0 && options.clockseq === undefined) { clockseq = clockseq + 1 & 16383; } if ((dt < 0 || msecs > _lastMSecs) && options.nsecs === undefined) { nsecs = 0; } if (nsecs >= 1e4) { throw new Error(""uuid.v1(): Can't create more than 10M uuids/sec""); } _lastMSecs = msecs; _lastNSecs = nsecs; _clockseq = clockseq; msecs += 122192928e5; var tl = ((msecs & 268435455) * 1e4 + nsecs) % 4294967296; b[i++] = tl >>> 24 & 255; b[i++] = tl >>> 16 & 255; b[i++] = tl >>> 8 & 255; b[i++] = tl & 255; var tmh = msecs / 4294967296 * 1e4 & 268435455; b[i++] = tmh >>> 8 & 255; b[i++] = tmh & 255; b[i++] = tmh >>> 24 & 15 | 16; b[i++] = tmh >>> 16 & 255; b[i++] = clockseq >>> 8 | 128; b[i++] = clockseq & 255; for (var n = 0; n < 6; ++n) { b[i + n] = node[n]; } return buf ? buf : bytesToUuid(b); } module.exports = v1; }, { ""./lib/bytesToUuid"": 22, ""./lib/rng"": 23 } ], 23: [ function(require, module, exports) { var getRandomValues = typeof crypto != ""undefined"" && crypto.getRandomValues && crypto.getRandomValues.bind(crypto) || typeof msCrypto != ""undefined"" && typeof window.msCrypto.getRandomValues == ""function"" && msCrypto.getRandomValues.bind(msCrypto); if (getRandomValues) { var rnds8 = new Uint8Array(16); module.exports = function whatwgRNG() { getRandomValues(rnds8); return rnds8; }; } else { var rnds = new Array(16); module.exports = function mathRNG() { for (var i = 0, r; i < 16; i++) { if ((i & 3) === 0) r = Math.random() * 4294967296; rnds[i] = r >>> ((i & 3) << 3) & 255; } return rnds; }; } }, {} ], 22: [ function(require, module, exports) { var byteToHex = []; for (var i = 0; i < 256; ++i) { byteToHex[i] = (i + 256).toString(16).substr(1); } function bytesToUuid(buf, offset) { var i = offset || 0; var bth = byteToHex; return [ bth[buf[i++]], bth[buf[i++]], bth[buf[i++]], bth[buf[i++]], ""-"", bth[buf[i++]], bth[buf[i++]], ""-"", bth[buf[i++]], bth[buf[i++]], ""-"", bth[buf[i++]], bth[buf[i++]], ""-"", bth[buf[i++]], bth[buf[i++]], bth[buf[i++]], bth[buf[i++]], bth[buf[i++]], bth[buf[i++]] ].join(""""); } module.exports = bytesToUuid; }, {} ], 20: [ function(require, module, exports) { (function(process, global) { var formatRegExp = /%[sdj%]/g; exports.format = function(f) { if (!isString(f)) { var objects = []; for (var i = 0; i < arguments.length; i++) { objects.push(inspect(arguments[i])); } return objects.join("" ""); } var i = 1; var args = arguments; var len = args.length; var str = String(f).replace(formatRegExp, function(x) { if (x === ""%%"") return ""%""; if (i >= len) return x; switch (x) { case ""%s"": return String(args[i++]); case ""%d"": return Number(args[i++]); case ""%j"": try { return JSON.stringify(args[i++]); } catch (_) { return ""[Circular]""; } default: return x; } }); for (var x = args[i]; i < len; x = args[++i]) { if (isNull(x) || !isObject(x)) { str += "" "" + x; } else { str += "" "" + inspect(x); } } return str; }; exports.deprecate = function(fn, msg) { if (isUndefined(global.process)) { return function() { return exports.deprecate(fn, msg).apply(this, arguments); }; } if (process.noDeprecation === true) { return fn; } var warned = false; function deprecated() { if (!warned) { if (process.throwDeprecation) { throw new Error(msg); } else if (process.traceDeprecation) { console.trace(msg); } else { console.error(msg); } warned = true; } return fn.apply(this, arguments); } return deprecated; }; var debugs = {}; var debugEnviron; exports.debuglog = function(set) { if (isUndefined(debugEnviron)) debugEnviron = process.env.NODE_DEBUG || """"; set = set.toUpperCase(); if (!debugs[set]) { if (new RegExp(""\\b"" + set + ""\\b"", ""i"").test(debugEnviron)) { var pid = process.pid; debugs[set] = function() { var msg = exports.format.apply(exports, arguments); console.error(""%s %d: %s"", set, pid, msg); }; } else { debugs[set] = function() {}; } } return debugs[set]; }; function inspect(obj, opts) { var ctx = { seen: [], stylize: stylizeNoColor }; if (arguments.length >= 3) ctx.depth = arguments[2]; if (arguments.length >= 4) ctx.colors = arguments[3]; if (isBoolean(opts)) { ctx.showHidden = opts; } else if (opts) { exports._extend(ctx, opts); } if (isUndefined(ctx.showHidden)) ctx.showHidden = false; if (isUndefined(ctx.depth)) ctx.depth = 2; if (isUndefined(ctx.colors)) ctx.colors = false; if (isUndefined(ctx.customInspect)) ctx.customInspect = true; if (ctx.colors) ctx.stylize = stylizeWithColor; return formatValue(ctx, obj, ctx.depth); } exports.inspect = inspect; inspect.colors = { bold: [ 1, 22 ], italic: [ 3, 23 ], underline: [ 4, 24 ], inverse: [ 7, 27 ], white: [ 37, 39 ], grey: [ 90, 39 ], black: [ 30, 39 ], blue: [ 34, 39 ], cyan: [ 36, 39 ], green: [ 32, 39 ], magenta: [ 35, 39 ], red: [ 31, 39 ], yellow: [ 33, 39 ] }; inspect.styles = { special: ""cyan"", number: ""yellow"", boolean: ""yellow"", undefined: ""grey"", null: ""bold"", string: ""green"", date: ""magenta"", regexp: ""red"" }; function stylizeWithColor(str, styleType) { var style = inspect.styles[styleType]; if (style) { return ""["" + inspect.colors[style][0] + ""m"" + str + ""["" + inspect.colors[style][1] + ""m""; } else { return str; } } function stylizeNoColor(str, styleType) { return str; } function arrayToHash(array) { var hash = {}; array.forEach(function(val, idx) { hash[val] = true; }); return hash; } function formatValue(ctx, value, recurseTimes) { if (ctx.customInspect && value && isFunction(value.inspect) && value.inspect !== exports.inspect && !(value.constructor && value.constructor.prototype === value)) { var ret = value.inspect(recurseTimes, ctx); if (!isString(ret)) { ret = formatValue(ctx, ret, recurseTimes); } return ret; } var primitive = formatPrimitive(ctx, value); if (primitive) { return primitive; } var keys = Object.keys(value); var visibleKeys = arrayToHash(keys); if (ctx.showHidden) { keys = Object.getOwnPropertyNames(value); } if (isError(value) && (keys.indexOf(""message"") >= 0 || keys.indexOf(""description"") >= 0)) { return formatError(value); } if (keys.length === 0) { if (isFunction(value)) { var name = value.name ? "": "" + value.name : """"; return ctx.stylize(""[Function"" + name + ""]"", ""special""); } if (isRegExp(value)) { return ctx.stylize(RegExp.prototype.toString.call(value), ""regexp""); } if (isDate(value)) { return ctx.stylize(Date.prototype.toString.call(value), ""date""); } if (isError(value)) { return formatError(value); } } var base = """", array = false, braces = [ ""{"", ""}"" ]; if (isArray(value)) { array = true; braces = [ ""["", ""]"" ]; } if (isFunction(value)) { var n = value.name ? "": "" + value.name : """"; base = "" [Function"" + n + ""]""; } if (isRegExp(value)) { base = "" "" + RegExp.prototype.toString.call(value); } if (isDate(value)) { base = "" "" + Date.prototype.toUTCString.call(value); } if (isError(value)) { base = "" "" + formatError(value); } if (keys.length === 0 && (!array || value.length == 0)) { return braces[0] + base + braces[1]; } if (recurseTimes < 0) { if (isRegExp(value)) { return ctx.stylize(RegExp.prototype.toString.call(value), ""regexp""); } else { return ctx.stylize(""[Object]"", ""special""); } } ctx.seen.push(value); var output; if (array) { output = formatArray(ctx, value, recurseTimes, visibleKeys, keys); } else { output = keys.map(function(key) { return formatProperty(ctx, value, recurseTimes, visibleKeys, key, array); }); } ctx.seen.pop(); return reduceToSingleString(output, base, braces); } function formatPrimitive(ctx, value) { if (isUndefined(value)) return ctx.stylize(""undefined"", ""undefined""); if (isString(value)) { var simple = ""'"" + JSON.stringify(value).replace(/^""|""$/g, """").replace(/'/g, ""\\'"").replace(/\\""/g, '""') + ""'""; return ctx.stylize(simple, ""string""); } if (isNumber(value)) return ctx.stylize("""" + value, ""number""); if (isBoolean(value)) return ctx.stylize("""" + value, ""boolean""); if (isNull(value)) return ctx.stylize(""null"", ""null""); } function formatError(value) { return ""["" + Error.prototype.toString.call(value) + ""]""; } function formatArray(ctx, value, recurseTimes, visibleKeys, keys) { var output = []; for (var i = 0, l = value.length; i < l; ++i) { if (hasOwnProperty(value, String(i))) { output.push(formatProperty(ctx, value, recurseTimes, visibleKeys, String(i), true)); } else { output.push(""""); } } keys.forEach(function(key) { if (!key.match(/^\d+$/)) { output.push(formatProperty(ctx, value, recurseTimes, visibleKeys, key, true)); } }); return output; } function formatProperty(ctx, value, recurseTimes, visibleKeys, key, array) { var name, str, desc; desc = Object.getOwnPropertyDescriptor(value, key) || { value: value[key] }; if (desc.get) { if (desc.set) { str = ctx.stylize(""[Getter/Setter]"", ""special""); } else { str = ctx.stylize(""[Getter]"", ""special""); } } else { if (desc.set) { str = ctx.stylize(""[Setter]"", ""special""); } } if (!hasOwnProperty(visibleKeys, key)) { name = ""["" + key + ""]""; } if (!str) { if (ctx.seen.indexOf(desc.value) < 0) { if (isNull(recurseTimes)) { str = formatValue(ctx, desc.value, null); } else { str = formatValue(ctx, desc.value, recurseTimes - 1); } if (str.indexOf(""\n"") > -1) { if (array) { str = str.split(""\n"").map(function(line) { return "" "" + line; }).join(""\n"").substr(2); } else { str = ""\n"" + str.split(""\n"").map(function(line) { return "" "" + line; }).join(""\n""); } } } else { str = ctx.stylize(""[Circular]"", ""special""); } } if (isUndefined(name)) { if (array && key.match(/^\d+$/)) { return str; } name = JSON.stringify("""" + key); if (name.match(/^""([a-zA-Z_][a-zA-Z_0-9]*)""$/)) { name = name.substr(1, name.length - 2); name = ctx.stylize(name, ""name""); } else { name = name.replace(/'/g, ""\\'"").replace(/\\""/g, '""').replace(/(^""|""$)/g, ""'""); name = ctx.stylize(name, ""string""); } } return name + "": "" + str; } function reduceToSingleString(output, base, braces) { var numLinesEst = 0; var length = output.reduce(function(prev, cur) { numLinesEst++; if (cur.indexOf(""\n"") >= 0) numLinesEst++; return prev + cur.replace(/\u001b\[\d\d?m/g, """").length + 1; }, 0); if (length > 60) { return braces[0] + (base === """" ? """" : base + ""\n "") + "" "" + output.join("",\n "") + "" "" + braces[1]; } return braces[0] + base + "" "" + output.join("", "") + "" "" + braces[1]; } function isArray(ar) { return Array.isArray(ar); } exports.isArray = isArray; function isBoolean(arg) { return typeof arg === ""boolean""; } exports.isBoolean = isBoolean; function isNull(arg) { return arg === null; } exports.isNull = isNull; function isNullOrUndefined(arg) { return arg == null; } exports.isNullOrUndefined = isNullOrUndefined; function isNumber(arg) { return typeof arg === ""number""; } exports.isNumber = isNumber; function isString(arg) { return typeof arg === ""string""; } exports.isString = isString; function isSymbol(arg) { return typeof arg === ""symbol""; } exports.isSymbol = isSymbol; function isUndefined(arg) { return arg === void 0; } exports.isUndefined = isUndefined; function isRegExp(re) { return isObject(re) && objectToString(re) === ""[object RegExp]""; } exports.isRegExp = isRegExp; function isObject(arg) { return typeof arg === ""object"" && arg !== null; } exports.isObject = isObject; function isDate(d) { return isObject(d) && objectToString(d) === ""[object Date]""; } exports.isDate = isDate; function isError(e) { return isObject(e) && (objectToString(e) === ""[object Error]"" || e instanceof Error); } exports.isError = isError; function isFunction(arg) { return typeof arg === ""function""; } exports.isFunction = isFunction; function isPrimitive(arg) { return arg === null || typeof arg === ""boolean"" || typeof arg === ""number"" || typeof arg === ""string"" || typeof arg === ""symbol"" || typeof arg === ""undefined""; } exports.isPrimitive = isPrimitive; exports.isBuffer = require(""./support/isBuffer""); function objectToString(o) { return Object.prototype.toString.call(o); } function pad(n) { return n < 10 ? ""0"" + n.toString(10) : n.toString(10); } var months = [ ""Jan"", ""Feb"", ""Mar"", ""Apr"", ""May"", ""Jun"", ""Jul"", ""Aug"", ""Sep"", ""Oct"", ""Nov"", ""Dec"" ]; function timestamp() { var d = new Date(); var time = [ pad(d.getHours()), pad(d.getMinutes()), pad(d.getSeconds()) ].join("":""); return [ d.getDate(), months[d.getMonth()], time ].join("" ""); } exports.log = function() { console.log(""%s - %s"", timestamp(), exports.format.apply(exports, arguments)); }; exports.inherits = require(""inherits""); exports._extend = function(origin, add) { if (!add || !isObject(add)) return origin; var keys = Object.keys(add); var i = keys.length; while (i--) { origin[keys[i]] = add[keys[i]]; } return origin; }; function hasOwnProperty(obj, prop) { return Object.prototype.hasOwnProperty.call(obj, prop); } }).call(this, require(""_process""), typeof global !== ""undefined"" ? global : typeof self !== ""undefined"" ? self : typeof window !== ""undefined"" ? window : {}); }, { ""./support/isBuffer"": 19, _process: 8, inherits: 18 } ], 19: [ function(require, module, exports) { module.exports = function isBuffer(arg) { return arg && typeof arg === ""object"" && typeof arg.copy === ""function"" && typeof arg.fill === ""function"" && typeof arg.readUInt8 === ""function""; }; }, {} ], 18: [ function(require, module, exports) { if (typeof Object.create === ""function"") { module.exports = function inherits(ctor, superCtor) { ctor.super_ = superCtor; ctor.prototype = Object.create(superCtor.prototype, { constructor: { value: ctor, enumerable: false, writable: true, configurable: true } }); }; } else { module.exports = function inherits(ctor, superCtor) { ctor.super_ = superCtor; var TempCtor = function() {}; TempCtor.prototype = superCtor.prototype; ctor.prototype = new TempCtor(); ctor.prototype.constructor = ctor; }; } }, {} ], 16: [ function(require, module, exports) { (function(setImmediate, clearImmediate) { var nextTick = require(""process/browser.js"").nextTick; var apply = Function.prototype.apply; var slice = Array.prototype.slice; var immediateIds = {}; var nextImmediateId = 0; exports.setTimeout = function() { return new Timeout(apply.call(setTimeout, window, arguments), clearTimeout); }; exports.setInterval = function() { return new Timeout(apply.call(setInterval, window, arguments), clearInterval); }; exports.clearTimeout = exports.clearInterval = function(timeout) { timeout.close(); }; function Timeout(id, clearFn) { this._id = id; this._clearFn = clearFn; } Timeout.prototype.unref = Timeout.prototype.ref = function() {}; Timeout.prototype.close = function() { this._clearFn.call(window, this._id); }; exports.enroll = function(item, msecs) { clearTimeout(item._idleTimeoutId); item._idleTimeout = msecs; }; exports.unenroll = function(item) { clearTimeout(item._idleTimeoutId); item._idleTimeout = -1; }; exports._unrefActive = exports.active = function(item) { clearTimeout(item._idleTimeoutId); var msecs = item._idleTimeout; if (msecs >= 0) { item._idleTimeoutId = setTimeout(function onTimeout() { if (item._onTimeout) item._onTimeout(); }, msecs); } }; exports.setImmediate = typeof setImmediate === ""function"" ? setImmediate : function(fn) { var id = nextImmediateId++; var args = arguments.length < 2 ? false : slice.call(arguments, 1); immediateIds[id] = true; nextTick(function onNextTick() { if (immediateIds[id]) { if (args) { fn.apply(null, args); } else { fn.call(null); } exports.clearImmediate(id); } }); return id; }; exports.clearImmediate = typeof clearImmediate === ""function"" ? clearImmediate : function(id) { delete immediateIds[id]; }; }).call(this, require(""timers"").setImmediate, require(""timers"").clearImmediate); }, { ""process/browser.js"": 8, timers: 16 } ], 8: [ function(require, module, exports) { var process = module.exports = {}; var cachedSetTimeout; var cachedClearTimeout; function defaultSetTimout() { throw new Error(""setTimeout has not been defined""); } function defaultClearTimeout() { throw new Error(""clearTimeout has not been defined""); } (function() { try { if (typeof setTimeout === ""function"") { cachedSetTimeout = setTimeout; } else { cachedSetTimeout = defaultSetTimout; } } catch (e) { cachedSetTimeout = defaultSetTimout; } try { if (typeof clearTimeout === ""function"") { cachedClearTimeout = clearTimeout; } else { cachedClearTimeout = defaultClearTimeout; } } catch (e) { cachedClearTimeout = defaultClearTimeout; } })(); function runTimeout(fun) { if (cachedSetTimeout === setTimeout) { return setTimeout(fun, 0); } if ((cachedSetTimeout === defaultSetTimout || !cachedSetTimeout) && setTimeout) { cachedSetTimeout = setTimeout; return setTimeout(fun, 0); } try { return cachedSetTimeout(fun, 0); } catch (e) { try { return cachedSetTimeout.call(null, fun, 0); } catch (e) { return cachedSetTimeout.call(this, fun, 0); } } } function runClearTimeout(marker) { if (cachedClearTimeout === clearTimeout) { return clearTimeout(marker); } if ((cachedClearTimeout === defaultClearTimeout || !cachedClearTimeout) && clearTimeout) { cachedClearTimeout = clearTimeout; return clearTimeout(marker); } try { return cachedClearTimeout(marker); } catch (e) { try { return cachedClearTimeout.call(null, marker); } catch (e) { return cachedClearTimeout.call(this, marker); } } } var queue = []; var draining = false; var currentQueue; var queueIndex = -1; function cleanUpNextTick() { if (!draining || !currentQueue) { return; } draining = false; if (currentQueue.length) { queue = currentQueue.concat(queue); } else { queueIndex = -1; } if (queue.length) { drainQueue(); } } function drainQueue() { if (draining) { return; } var timeout = runTimeout(cleanUpNextTick); draining = true; var len = queue.length; while (len) { currentQueue = queue; queue = []; while (++queueIndex < len) { if (currentQueue) { currentQueue[queueIndex].run(); } } queueIndex = -1; len = queue.length; } currentQueue = null; draining = false; runClearTimeout(timeout); } process.nextTick = function(fun) { var args = new Array(arguments.length - 1); if (arguments.length > 1) { for (var i = 1; i < arguments.length; i++) { args[i - 1] = arguments[i]; } } queue.push(new Item(fun, args)); if (queue.length === 1 && !draining) { runTimeout(drainQueue); } }; function Item(fun, array) { this.fun = fun; this.array = array; } Item.prototype.run = function() { this.fun.apply(null, this.array); }; process.title = ""browser""; process.browser = true; process.env = {}; process.argv = []; process.version = """"; process.versions = {}; function noop() {} process.on = noop; process.addListener = noop; process.once = noop; process.off = noop; process.removeListener = noop; process.removeAllListeners = noop; process.emit = noop; process.prependListener = noop; process.prependOnceListener = noop; process.listeners = function(name) { return []; }; process.binding = function(name) { throw new Error(""process.binding is not supported""); }; process.cwd = function() { return ""/""; }; process.chdir = function(dir) { throw new Error(""process.chdir is not supported""); }; process.umask = function() { return 0; }; }, {} ], 7: [ function(require, module, exports) { (function(exports) { ""use strict""; function isArray(obj) { if (obj !== null) { return Object.prototype.toString.call(obj) === ""[object Array]""; } else { return false; } } function isObject(obj) { if (obj !== null) { return Object.prototype.toString.call(obj) === ""[object Object]""; } else { return false; } } function strictDeepEqual(first, second) { if (first === second) { return true; } var firstType = Object.prototype.toString.call(first); if (firstType !== Object.prototype.toString.call(second)) { return false; } if (isArray(first) === true) { if (first.length !== second.length) { return false; } for (var i = 0; i < first.length; i++) { if (strictDeepEqual(first[i], second[i]) === false) { return false; } } return true; } if (isObject(first) === true) { var keysSeen = {}; for (var key in first) { if (hasOwnProperty.call(first, key)) { if (strictDeepEqual(first[key], second[key]) === false) { return false; } keysSeen[key] = true; } } for (var key2 in second) { if (hasOwnProperty.call(second, key2)) { if (keysSeen[key2] !== true) { return false; } } } return true; } return false; } function isFalse(obj) { if (obj === """" || obj === false || obj === null) { return true; } else if (isArray(obj) && obj.length === 0) { return true; } else if (isObject(obj)) { for (var key in obj) { if (obj.hasOwnProperty(key)) { return false; } } return true; } else { return false; } } function objValues(obj) { var keys = Object.keys(obj); var values = []; for (var i = 0; i < keys.length; i++) { values.push(obj[keys[i]]); } return values; } function merge(a, b) { var merged = {}; for (var key in a) { merged[key] = a[key]; } for (var key2 in b) { merged[key2] = b[key2]; } return merged; } var trimLeft; if (typeof String.prototype.trimLeft === ""function"") { trimLeft = function(str) { return str.trimLeft(); }; } else { trimLeft = function(str) { return str.match(/^\s*(.*)/)[1]; }; } var TYPE_NUMBER = 0; var TYPE_ANY = 1; var TYPE_STRING = 2; var TYPE_ARRAY = 3; var TYPE_OBJECT = 4; var TYPE_BOOLEAN = 5; var TYPE_EXPREF = 6; var TYPE_NULL = 7; var TYPE_ARRAY_NUMBER = 8; var TYPE_ARRAY_STRING = 9; var TOK_EOF = ""EOF""; var TOK_UNQUOTEDIDENTIFIER = ""UnquotedIdentifier""; var TOK_QUOTEDIDENTIFIER = ""QuotedIdentifier""; var TOK_RBRACKET = ""Rbracket""; var TOK_RPAREN = ""Rparen""; var TOK_COMMA = ""Comma""; var TOK_COLON = ""Colon""; var TOK_RBRACE = ""Rbrace""; var TOK_NUMBER = ""Number""; var TOK_CURRENT = ""Current""; var TOK_EXPREF = ""Expref""; var TOK_PIPE = ""Pipe""; var TOK_OR = ""Or""; var TOK_AND = ""And""; var TOK_EQ = ""EQ""; var TOK_GT = ""GT""; var TOK_LT = ""LT""; var TOK_GTE = ""GTE""; var TOK_LTE = ""LTE""; var TOK_NE = ""NE""; var TOK_FLATTEN = ""Flatten""; var TOK_STAR = ""Star""; var TOK_FILTER = ""Filter""; var TOK_DOT = ""Dot""; var TOK_NOT = ""Not""; var TOK_LBRACE = ""Lbrace""; var TOK_LBRACKET = ""Lbracket""; var TOK_LPAREN = ""Lparen""; var TOK_LITERAL = ""Literal""; var basicTokens = { ""."": TOK_DOT, ""*"": TOK_STAR, "","": TOK_COMMA, "":"": TOK_COLON, ""{"": TOK_LBRACE, ""}"": TOK_RBRACE, ""]"": TOK_RBRACKET, ""("": TOK_LPAREN, "")"": TOK_RPAREN, ""@"": TOK_CURRENT }; var operatorStartToken = { ""<"": true, "">"": true, ""="": true, ""!"": true }; var skipChars = { "" "": true, ""\t"": true, ""\n"": true }; function isAlpha(ch) { return ch >= ""a"" && ch <= ""z"" || ch >= ""A"" && ch <= ""Z"" || ch === ""_""; } function isNum(ch) { return ch >= ""0"" && ch <= ""9"" || ch === ""-""; } function isAlphaNum(ch) { return ch >= ""a"" && ch <= ""z"" || ch >= ""A"" && ch <= ""Z"" || ch >= ""0"" && ch <= ""9"" || ch === ""_""; } function Lexer() {} Lexer.prototype = { tokenize: function(stream) { var tokens = []; this._current = 0; var start; var identifier; var token; while (this._current < stream.length) { if (isAlpha(stream[this._current])) { start = this._current; identifier = this._consumeUnquotedIdentifier(stream); tokens.push({ type: TOK_UNQUOTEDIDENTIFIER, value: identifier, start: start }); } else if (basicTokens[stream[this._current]] !== undefined) { tokens.push({ type: basicTokens[stream[this._current]], value: stream[this._current], start: this._current }); this._current++; } else if (isNum(stream[this._current])) { token = this._consumeNumber(stream); tokens.push(token); } else if (stream[this._current] === ""["") { token = this._consumeLBracket(stream); tokens.push(token); } else if (stream[this._current] === '""') { start = this._current; identifier = this._consumeQuotedIdentifier(stream); tokens.push({ type: TOK_QUOTEDIDENTIFIER, value: identifier, start: start }); } else if (stream[this._current] === ""'"") { start = this._current; identifier = this._consumeRawStringLiteral(stream); tokens.push({ type: TOK_LITERAL, value: identifier, start: start }); } else if (stream[this._current] === ""`"") { start = this._current; var literal = this._consumeLiteral(stream); tokens.push({ type: TOK_LITERAL, value: literal, start: start }); } else if (operatorStartToken[stream[this._current]] !== undefined) { tokens.push(this._consumeOperator(stream)); } else if (skipChars[stream[this._current]] !== undefined) { this._current++; } else if (stream[this._current] === ""&"") { start = this._current; this._current++; if (stream[this._current] === ""&"") { this._current++; tokens.push({ type: TOK_AND, value: ""&&"", start: start }); } else { tokens.push({ type: TOK_EXPREF, value: ""&"", start: start }); } } else if (stream[this._current] === ""|"") { start = this._current; this._current++; if (stream[this._current] === ""|"") { this._current++; tokens.push({ type: TOK_OR, value: ""||"", start: start }); } else { tokens.push({ type: TOK_PIPE, value: ""|"", start: start }); } } else { var error = new Error(""Unknown character:"" + stream[this._current]); error.name = ""LexerError""; throw error; } } return tokens; }, _consumeUnquotedIdentifier: function(stream) { var start = this._current; this._current++; while (this._current < stream.length && isAlphaNum(stream[this._current])) { this._current++; } return stream.slice(start, this._current); }, _consumeQuotedIdentifier: function(stream) { var start = this._current; this._current++; var maxLength = stream.length; while (stream[this._current] !== '""' && this._current < maxLength) { var current = this._current; if (stream[current] === ""\\"" && (stream[current + 1] === ""\\"" || stream[current + 1] === '""')) { current += 2; } else { current++; } this._current = current; } this._current++; return JSON.parse(stream.slice(start, this._current)); }, _consumeRawStringLiteral: function(stream) { var start = this._current; this._current++; var maxLength = stream.length; while (stream[this._current] !== ""'"" && this._current < maxLength) { var current = this._current; if (stream[current] === ""\\"" && (stream[current + 1] === ""\\"" || stream[current + 1] === ""'"")) { current += 2; } else { current++; } this._current = current; } this._current++; var literal = stream.slice(start + 1, this._current - 1); return literal.replace(""\\'"", ""'""); }, _consumeNumber: function(stream) { var start = this._current; this._current++; var maxLength = stream.length; while (isNum(stream[this._current]) && this._current < maxLength) { this._current++; } var value = parseInt(stream.slice(start, this._current)); return { type: TOK_NUMBER, value: value, start: start }; }, _consumeLBracket: function(stream) { var start = this._current; this._current++; if (stream[this._current] === ""?"") { this._current++; return { type: TOK_FILTER, value: ""[?"", start: start }; } else if (stream[this._current] === ""]"") { this._current++; return { type: TOK_FLATTEN, value: ""[]"", start: start }; } else { return { type: TOK_LBRACKET, value: ""["", start: start }; } }, _consumeOperator: function(stream) { var start = this._current; var startingChar = stream[start]; this._current++; if (startingChar === ""!"") { if (stream[this._current] === ""="") { this._current++; return { type: TOK_NE, value: ""!="", start: start }; } else { return { type: TOK_NOT, value: ""!"", start: start }; } } else if (startingChar === ""<"") { if (stream[this._current] === ""="") { this._current++; return { type: TOK_LTE, value: ""<="", start: start }; } else { return { type: TOK_LT, value: ""<"", start: start }; } } else if (startingChar === "">"") { if (stream[this._current] === ""="") { this._current++; return { type: TOK_GTE, value: "">="", start: start }; } else { return { type: TOK_GT, value: "">"", start: start }; } } else if (startingChar === ""="") { if (stream[this._current] === ""="") { this._current++; return { type: TOK_EQ, value: ""=="", start: start }; } } }, _consumeLiteral: function(stream) { this._current++; var start = this._current; var maxLength = stream.length; var literal; while (stream[this._current] !== ""`"" && this._current < maxLength) { var current = this._current; if (stream[current] === ""\\"" && (stream[current + 1] === ""\\"" || stream[current + 1] === ""`"")) { current += 2; } else { current++; } this._current = current; } var literalString = trimLeft(stream.slice(start, this._current)); literalString = literalString.replace(""\\`"", ""`""); if (this._looksLikeJSON(literalString)) { literal = JSON.parse(literalString); } else { literal = JSON.parse('""' + literalString + '""'); } this._current++; return literal; }, _looksLikeJSON: function(literalString) { var startingChars = '[{""'; var jsonLiterals = [ ""true"", ""false"", ""null"" ]; var numberLooking = ""-0123456789""; if (literalString === """") { return false; } else if (startingChars.indexOf(literalString[0]) >= 0) { return true; } else if (jsonLiterals.indexOf(literalString) >= 0) { return true; } else if (numberLooking.indexOf(literalString[0]) >= 0) { try { JSON.parse(literalString); return true; } catch (ex) { return false; } } else { return false; } } }; var bindingPower = {}; bindingPower[TOK_EOF] = 0; bindingPower[TOK_UNQUOTEDIDENTIFIER] = 0; bindingPower[TOK_QUOTEDIDENTIFIER] = 0; bindingPower[TOK_RBRACKET] = 0; bindingPower[TOK_RPAREN] = 0; bindingPower[TOK_COMMA] = 0; bindingPower[TOK_RBRACE] = 0; bindingPower[TOK_NUMBER] = 0; bindingPower[TOK_CURRENT] = 0; bindingPower[TOK_EXPREF] = 0; bindingPower[TOK_PIPE] = 1; bindingPower[TOK_OR] = 2; bindingPower[TOK_AND] = 3; bindingPower[TOK_EQ] = 5; bindingPower[TOK_GT] = 5; bindingPower[TOK_LT] = 5; bindingPower[TOK_GTE] = 5; bindingPower[TOK_LTE] = 5; bindingPower[TOK_NE] = 5; bindingPower[TOK_FLATTEN] = 9; bindingPower[TOK_STAR] = 20; bindingPower[TOK_FILTER] = 21; bindingPower[TOK_DOT] = 40; bindingPower[TOK_NOT] = 45; bindingPower[TOK_LBRACE] = 50; bindingPower[TOK_LBRACKET] = 55; bindingPower[TOK_LPAREN] = 60; function Parser() {} Parser.prototype = { parse: function(expression) { this._loadTokens(expression); this.index = 0; var ast = this.expression(0); if (this._lookahead(0) !== TOK_EOF) { var t = this._lookaheadToken(0); var error = new Error(""Unexpected token type: "" + t.type + "", value: "" + t.value); error.name = ""ParserError""; throw error; } return ast; }, _loadTokens: function(expression) { var lexer = new Lexer(); var tokens = lexer.tokenize(expression); tokens.push({ type: TOK_EOF, value: """", start: expression.length }); this.tokens = tokens; }, expression: function(rbp) { var leftToken = this._lookaheadToken(0); this._advance(); var left = this.nud(leftToken); var currentToken = this._lookahead(0); while (rbp < bindingPower[currentToken]) { this._advance(); left = this.led(currentToken, left); currentToken = this._lookahead(0); } return left; }, _lookahead: function(number) { return this.tokens[this.index + number].type; }, _lookaheadToken: function(number) { return this.tokens[this.index + number]; }, _advance: function() { this.index++; }, nud: function(token) { var left; var right; var expression; switch (token.type) { case TOK_LITERAL: return { type: ""Literal"", value: token.value }; case TOK_UNQUOTEDIDENTIFIER: return { type: ""Field"", name: token.value }; case TOK_QUOTEDIDENTIFIER: var node = { type: ""Field"", name: token.value }; if (this._lookahead(0) === TOK_LPAREN) { throw new Error(""Quoted identifier not allowed for function names.""); } else { return node; } break; case TOK_NOT: right = this.expression(bindingPower.Not); return { type: ""NotExpression"", children: [ right ] }; case TOK_STAR: left = { type: ""Identity"" }; right = null; if (this._lookahead(0) === TOK_RBRACKET) { right = { type: ""Identity"" }; } else { right = this._parseProjectionRHS(bindingPower.Star); } return { type: ""ValueProjection"", children: [ left, right ] }; case TOK_FILTER: return this.led(token.type, { type: ""Identity"" }); case TOK_LBRACE: return this._parseMultiselectHash(); case TOK_FLATTEN: left = { type: TOK_FLATTEN, children: [ { type: ""Identity"" } ] }; right = this._parseProjectionRHS(bindingPower.Flatten); return { type: ""Projection"", children: [ left, right ] }; case TOK_LBRACKET: if (this._lookahead(0) === TOK_NUMBER || this._lookahead(0) === TOK_COLON) { right = this._parseIndexExpression(); return this._projectIfSlice({ type: ""Identity"" }, right); } else if (this._lookahead(0) === TOK_STAR && this._lookahead(1) === TOK_RBRACKET) { this._advance(); this._advance(); right = this._parseProjectionRHS(bindingPower.Star); return { type: ""Projection"", children: [ { type: ""Identity"" }, right ] }; } else { return this._parseMultiselectList(); } break; case TOK_CURRENT: return { type: TOK_CURRENT }; case TOK_EXPREF: expression = this.expression(bindingPower.Expref); return { type: ""ExpressionReference"", children: [ expression ] }; case TOK_LPAREN: var args = []; while (this._lookahead(0) !== TOK_RPAREN) { if (this._lookahead(0) === TOK_CURRENT) { expression = { type: TOK_CURRENT }; this._advance(); } else { expression = this.expression(0); } args.push(expression); } this._match(TOK_RPAREN); return args[0]; default: this._errorToken(token); } }, led: function(tokenName, left) { var right; switch (tokenName) { case TOK_DOT: var rbp = bindingPower.Dot; if (this._lookahead(0) !== TOK_STAR) { right = this._parseDotRHS(rbp); return { type: ""Subexpression"", children: [ left, right ] }; } else { this._advance(); right = this._parseProjectionRHS(rbp); return { type: ""ValueProjection"", children: [ left, right ] }; } break; case TOK_PIPE: right = this.expression(bindingPower.Pipe); return { type: TOK_PIPE, children: [ left, right ] }; case TOK_OR: right = this.expression(bindingPower.Or); return { type: ""OrExpression"", children: [ left, right ] }; case TOK_AND: right = this.expression(bindingPower.And); return { type: ""AndExpression"", children: [ left, right ] }; case TOK_LPAREN: var name = left.name; var args = []; var expression, node; while (this._lookahead(0) !== TOK_RPAREN) { if (this._lookahead(0) === TOK_CURRENT) { expression = { type: TOK_CURRENT }; this._advance(); } else { expression = this.expression(0); } if (this._lookahead(0) === TOK_COMMA) { this._match(TOK_COMMA); } args.push(expression); } this._match(TOK_RPAREN); node = { type: ""Function"", name: name, children: args }; return node; case TOK_FILTER: var condition = this.expression(0); this._match(TOK_RBRACKET); if (this._lookahead(0) === TOK_FLATTEN) { right = { type: ""Identity"" }; } else { right = this._parseProjectionRHS(bindingPower.Filter); } return { type: ""FilterProjection"", children: [ left, right, condition ] }; case TOK_FLATTEN: var leftNode = { type: TOK_FLATTEN, children: [ left ] }; var rightNode = this._parseProjectionRHS(bindingPower.Flatten); return { type: ""Projection"", children: [ leftNode, rightNode ] }; case TOK_EQ: case TOK_NE: case TOK_GT: case TOK_GTE: case TOK_LT: case TOK_LTE: return this._parseComparator(left, tokenName); case TOK_LBRACKET: var token = this._lookaheadToken(0); if (token.type === TOK_NUMBER || token.type === TOK_COLON) { right = this._parseIndexExpression(); return this._projectIfSlice(left, right); } else { this._match(TOK_STAR); this._match(TOK_RBRACKET); right = this._parseProjectionRHS(bindingPower.Star); return { type: ""Projection"", children: [ left, right ] }; } break; default: this._errorToken(this._lookaheadToken(0)); } }, _match: function(tokenType) { if (this._lookahead(0) === tokenType) { this._advance(); } else { var t = this._lookaheadToken(0); var error = new Error(""Expected "" + tokenType + "", got: "" + t.type); error.name = ""ParserError""; throw error; } }, _errorToken: function(token) { var error = new Error(""Invalid token ("" + token.type + '): ""' + token.value + '""'); error.name = ""ParserError""; throw error; }, _parseIndexExpression: function() { if (this._lookahead(0) === TOK_COLON || this._lookahead(1) === TOK_COLON) { return this._parseSliceExpression(); } else { var node = { type: ""Index"", value: this._lookaheadToken(0).value }; this._advance(); this._match(TOK_RBRACKET); return node; } }, _projectIfSlice: function(left, right) { var indexExpr = { type: ""IndexExpression"", children: [ left, right ] }; if (right.type === ""Slice"") { return { type: ""Projection"", children: [ indexExpr, this._parseProjectionRHS(bindingPower.Star) ] }; } else { return indexExpr; } }, _parseSliceExpression: function() { var parts = [ null, null, null ]; var index = 0; var currentToken = this._lookahead(0); while (currentToken !== TOK_RBRACKET && index < 3) { if (currentToken === TOK_COLON) { index++; this._advance(); } else if (currentToken === TOK_NUMBER) { parts[index] = this._lookaheadToken(0).value; this._advance(); } else { var t = this._lookahead(0); var error = new Error(""Syntax error, unexpected token: "" + t.value + ""("" + t.type + "")""); error.name = ""Parsererror""; throw error; } currentToken = this._lookahead(0); } this._match(TOK_RBRACKET); return { type: ""Slice"", children: parts }; }, _parseComparator: function(left, comparator) { var right = this.expression(bindingPower[comparator]); return { type: ""Comparator"", name: comparator, children: [ left, right ] }; }, _parseDotRHS: function(rbp) { var lookahead = this._lookahead(0); var exprTokens = [ TOK_UNQUOTEDIDENTIFIER, TOK_QUOTEDIDENTIFIER, TOK_STAR ]; if (exprTokens.indexOf(lookahead) >= 0) { return this.expression(rbp); } else if (lookahead === TOK_LBRACKET) { this._match(TOK_LBRACKET); return this._parseMultiselectList(); } else if (lookahead === TOK_LBRACE) { this._match(TOK_LBRACE); return this._parseMultiselectHash(); } }, _parseProjectionRHS: function(rbp) { var right; if (bindingPower[this._lookahead(0)] < 10) { right = { type: ""Identity"" }; } else if (this._lookahead(0) === TOK_LBRACKET) { right = this.expression(rbp); } else if (this._lookahead(0) === TOK_FILTER) { right = this.expression(rbp); } else if (this._lookahead(0) === TOK_DOT) { this._match(TOK_DOT); right = this._parseDotRHS(rbp); } else { var t = this._lookaheadToken(0); var error = new Error(""Sytanx error, unexpected token: "" + t.value + ""("" + t.type + "")""); error.name = ""ParserError""; throw error; } return right; }, _parseMultiselectList: function() { var expressions = []; while (this._lookahead(0) !== TOK_RBRACKET) { var expression = this.expression(0); expressions.push(expression); if (this._lookahead(0) === TOK_COMMA) { this._match(TOK_COMMA); if (this._lookahead(0) === TOK_RBRACKET) { throw new Error(""Unexpected token Rbracket""); } } } this._match(TOK_RBRACKET); return { type: ""MultiSelectList"", children: expressions }; }, _parseMultiselectHash: function() { var pairs = []; var identifierTypes = [ TOK_UNQUOTEDIDENTIFIER, TOK_QUOTEDIDENTIFIER ]; var keyToken, keyName, value, node; for (;;) { keyToken = this._lookaheadToken(0); if (identifierTypes.indexOf(keyToken.type) < 0) { throw new Error(""Expecting an identifier token, got: "" + keyToken.type); } keyName = keyToken.value; this._advance(); this._match(TOK_COLON); value = this.expression(0); node = { type: ""KeyValuePair"", name: keyName, value: value }; pairs.push(node); if (this._lookahead(0) === TOK_COMMA) { this._match(TOK_COMMA); } else if (this._lookahead(0) === TOK_RBRACE) { this._match(TOK_RBRACE); break; } } return { type: ""MultiSelectHash"", children: pairs }; } }; function TreeInterpreter(runtime) { this.runtime = runtime; } TreeInterpreter.prototype = { search: function(node, value) { return this.visit(node, value); }, visit: function(node, value) { var matched, current, result, first, second, field, left, right, collected, i; switch (node.type) { case ""Field"": if (value === null) { return null; } else if (isObject(value)) { field = value[node.name]; if (field === undefined) { return null; } else { return field; } } else { return null; } break; case ""Subexpression"": result = this.visit(node.children[0], value); for (i = 1; i < node.children.length; i++) { result = this.visit(node.children[1], result); if (result === null) { return null; } } return result; case ""IndexExpression"": left = this.visit(node.children[0], value); right = this.visit(node.children[1], left); return right; case ""Index"": if (!isArray(value)) { return null; } var index = node.value; if (index < 0) { index = value.length + index; } result = value[index]; if (result === undefined) { result = null; } return result; case ""Slice"": if (!isArray(value)) { return null; } var sliceParams = node.children.slice(0); var computed = this.computeSliceParams(value.length, sliceParams); var start = computed[0]; var stop = computed[1]; var step = computed[2]; result = []; if (step > 0) { for (i = start; i < stop; i += step) { result.push(value[i]); } } else { for (i = start; i > stop; i += step) { result.push(value[i]); } } return result; case ""Projection"": var base = this.visit(node.children[0], value); if (!isArray(base)) { return null; } collected = []; for (i = 0; i < base.length; i++) { current = this.visit(node.children[1], base[i]); if (current !== null) { collected.push(current); } } return collected; case ""ValueProjection"": base = this.visit(node.children[0], value); if (!isObject(base)) { return null; } collected = []; var values = objValues(base); for (i = 0; i < values.length; i++) { current = this.visit(node.children[1], values[i]); if (current !== null) { collected.push(current); } } return collected; case ""FilterProjection"": base = this.visit(node.children[0], value); if (!isArray(base)) { return null; } var filtered = []; var finalResults = []; for (i = 0; i < base.length; i++) { matched = this.visit(node.children[2], base[i]); if (!isFalse(matched)) { filtered.push(base[i]); } } for (var j = 0; j < filtered.length; j++) { current = this.visit(node.children[1], filtered[j]); if (current !== null) { finalResults.push(current); } } return finalResults; case ""Comparator"": first = this.visit(node.children[0], value); second = this.visit(node.children[1], value); switch (node.name) { case TOK_EQ: result = strictDeepEqual(first, second); break; case TOK_NE: result = !strictDeepEqual(first, second); break; case TOK_GT: result = first > second; break; case TOK_GTE: result = first >= second; break; case TOK_LT: result = first < second; break; case TOK_LTE: result = first <= second; break; default: throw new Error(""Unknown comparator: "" + node.name); } return result; case TOK_FLATTEN: var original = this.visit(node.children[0], value); if (!isArray(original)) { return null; } var merged = []; for (i = 0; i < original.length; i++) { current = original[i]; if (isArray(current)) { merged.push.apply(merged, current); } else { merged.push(current); } } return merged; case ""Identity"": return value; case ""MultiSelectList"": if (value === null) { return null; } collected = []; for (i = 0; i < node.children.length; i++) { collected.push(this.visit(node.children[i], value)); } return collected; case ""MultiSelectHash"": if (value === null) { return null; } collected = {}; var child; for (i = 0; i < node.children.length; i++) { child = node.children[i]; collected[child.name] = this.visit(child.value, value); } return collected; case ""OrExpression"": matched = this.visit(node.children[0], value); if (isFalse(matched)) { matched = this.visit(node.children[1], value); } return matched; case ""AndExpression"": first = this.visit(node.children[0], value); if (isFalse(first) === true) { return first; } return this.visit(node.children[1], value); case ""NotExpression"": first = this.visit(node.children[0], value); return isFalse(first); case ""Literal"": return node.value; case TOK_PIPE: left = this.visit(node.children[0], value); return this.visit(node.children[1], left); case TOK_CURRENT: return value; case ""Function"": var resolvedArgs = []; for (i = 0; i < node.children.length; i++) { resolvedArgs.push(this.visit(node.children[i], value)); } return this.runtime.callFunction(node.name, resolvedArgs); case ""ExpressionReference"": var refNode = node.children[0]; refNode.jmespathType = TOK_EXPREF; return refNode; default: throw new Error(""Unknown node type: "" + node.type); } }, computeSliceParams: function(arrayLength, sliceParams) { var start = sliceParams[0]; var stop = sliceParams[1]; var step = sliceParams[2]; var computed = [ null, null, null ]; if (step === null) { step = 1; } else if (step === 0) { var error = new Error(""Invalid slice, step cannot be 0""); error.name = ""RuntimeError""; throw error; } var stepValueNegative = step < 0 ? true : false; if (start === null) { start = stepValueNegative ? arrayLength - 1 : 0; } else { start = this.capSliceRange(arrayLength, start, step); } if (stop === null) { stop = stepValueNegative ? -1 : arrayLength; } else { stop = this.capSliceRange(arrayLength, stop, step); } computed[0] = start; computed[1] = stop; computed[2] = step; return computed; }, capSliceRange: function(arrayLength, actualValue, step) { if (actualValue < 0) { actualValue += arrayLength; if (actualValue < 0) { actualValue = step < 0 ? -1 : 0; } } else if (actualValue >= arrayLength) { actualValue = step < 0 ? arrayLength - 1 : arrayLength; } return actualValue; } }; function Runtime(interpreter) { this._interpreter = interpreter; this.functionTable = { abs: { _func: this._functionAbs, _signature: [ { types: [ TYPE_NUMBER ] } ] }, avg: { _func: this._functionAvg, _signature: [ { types: [ TYPE_ARRAY_NUMBER ] } ] }, ceil: { _func: this._functionCeil, _signature: [ { types: [ TYPE_NUMBER ] } ] }, contains: { _func: this._functionContains, _signature: [ { types: [ TYPE_STRING, TYPE_ARRAY ] }, { types: [ TYPE_ANY ] } ] }, ends_with: { _func: this._functionEndsWith, _signature: [ { types: [ TYPE_STRING ] }, { types: [ TYPE_STRING ] } ] }, floor: { _func: this._functionFloor, _signature: [ { types: [ TYPE_NUMBER ] } ] }, length: { _func: this._functionLength, _signature: [ { types: [ TYPE_STRING, TYPE_ARRAY, TYPE_OBJECT ] } ] }, map: { _func: this._functionMap, _signature: [ { types: [ TYPE_EXPREF ] }, { types: [ TYPE_ARRAY ] } ] }, max: { _func: this._functionMax, _signature: [ { types: [ TYPE_ARRAY_NUMBER, TYPE_ARRAY_STRING ] } ] }, merge: { _func: this._functionMerge, _signature: [ { types: [ TYPE_OBJECT ], variadic: true } ] }, max_by: { _func: this._functionMaxBy, _signature: [ { types: [ TYPE_ARRAY ] }, { types: [ TYPE_EXPREF ] } ] }, sum: { _func: this._functionSum, _signature: [ { types: [ TYPE_ARRAY_NUMBER ] } ] }, starts_with: { _func: this._functionStartsWith, _signature: [ { types: [ TYPE_STRING ] }, { types: [ TYPE_STRING ] } ] }, min: { _func: this._functionMin, _signature: [ { types: [ TYPE_ARRAY_NUMBER, TYPE_ARRAY_STRING ] } ] }, min_by: { _func: this._functionMinBy, _signature: [ { types: [ TYPE_ARRAY ] }, { types: [ TYPE_EXPREF ] } ] }, type: { _func: this._functionType, _signature: [ { types: [ TYPE_ANY ] } ] }, keys: { _func: this._functionKeys, _signature: [ { types: [ TYPE_OBJECT ] } ] }, values: { _func: this._functionValues, _signature: [ { types: [ TYPE_OBJECT ] } ] }, sort: { _func: this._functionSort, _signature: [ { types: [ TYPE_ARRAY_STRING, TYPE_ARRAY_NUMBER ] } ] }, sort_by: { _func: this._functionSortBy, _signature: [ { types: [ TYPE_ARRAY ] }, { types: [ TYPE_EXPREF ] } ] }, join: { _func: this._functionJoin, _signature: [ { types: [ TYPE_STRING ] }, { types: [ TYPE_ARRAY_STRING ] } ] }, reverse: { _func: this._functionReverse, _signature: [ { types: [ TYPE_STRING, TYPE_ARRAY ] } ] }, to_array: { _func: this._functionToArray, _signature: [ { types: [ TYPE_ANY ] } ] }, to_string: { _func: this._functionToString, _signature: [ { types: [ TYPE_ANY ] } ] }, to_number: { _func: this._functionToNumber, _signature: [ { types: [ TYPE_ANY ] } ] }, not_null: { _func: this._functionNotNull, _signature: [ { types: [ TYPE_ANY ], variadic: true } ] } }; } Runtime.prototype = { callFunction: function(name, resolvedArgs) { var functionEntry = this.functionTable[name]; if (functionEntry === undefined) { throw new Error(""Unknown function: "" + name + ""()""); } this._validateArgs(name, resolvedArgs, functionEntry._signature); return functionEntry._func.call(this, resolvedArgs); }, _validateArgs: function(name, args, signature) { var pluralized; if (signature[signature.length - 1].variadic) { if (args.length < signature.length) { pluralized = signature.length === 1 ? "" argument"" : "" arguments""; throw new Error(""ArgumentError: "" + name + ""() "" + ""takes at least"" + signature.length + pluralized + "" but received "" + args.length); } } else if (args.length !== signature.length) { pluralized = signature.length === 1 ? "" argument"" : "" arguments""; throw new Error(""ArgumentError: "" + name + ""() "" + ""takes "" + signature.length + pluralized + "" but received "" + args.length); } var currentSpec; var actualType; var typeMatched; for (var i = 0; i < signature.length; i++) { typeMatched = false; currentSpec = signature[i].types; actualType = this._getTypeName(args[i]); for (var j = 0; j < currentSpec.length; j++) { if (this._typeMatches(actualType, currentSpec[j], args[i])) { typeMatched = true; break; } } if (!typeMatched) { throw new Error(""TypeError: "" + name + ""() "" + ""expected argument "" + (i + 1) + "" to be type "" + currentSpec + "" but received type "" + actualType + "" instead.""); } } }, _typeMatches: function(actual, expected, argValue) { if (expected === TYPE_ANY) { return true; } if (expected === TYPE_ARRAY_STRING || expected === TYPE_ARRAY_NUMBER || expected === TYPE_ARRAY) { if (expected === TYPE_ARRAY) { return actual === TYPE_ARRAY; } else if (actual === TYPE_ARRAY) { var subtype; if (expected === TYPE_ARRAY_NUMBER) { subtype = TYPE_NUMBER; } else if (expected === TYPE_ARRAY_STRING) { subtype = TYPE_STRING; } for (var i = 0; i < argValue.length; i++) { if (!this._typeMatches(this._getTypeName(argValue[i]), subtype, argValue[i])) { return false; } } return true; } } else { return actual === expected; } }, _getTypeName: function(obj) { switch (Object.prototype.toString.call(obj)) { case ""[object String]"": return TYPE_STRING; case ""[object Number]"": return TYPE_NUMBER; case ""[object Array]"": return TYPE_ARRAY; case ""[object Boolean]"": return TYPE_BOOLEAN; case ""[object Null]"": return TYPE_NULL; case ""[object Object]"": if (obj.jmespathType === TOK_EXPREF) { return TYPE_EXPREF; } else { return TYPE_OBJECT; } } }, _functionStartsWith: function(resolvedArgs) { return resolvedArgs[0].lastIndexOf(resolvedArgs[1]) === 0; }, _functionEndsWith: function(resolvedArgs) { var searchStr = resolvedArgs[0]; var suffix = resolvedArgs[1]; return searchStr.indexOf(suffix, searchStr.length - suffix.length) !== -1; }, _functionReverse: function(resolvedArgs) { var typeName = this._getTypeName(resolvedArgs[0]); if (typeName === TYPE_STRING) { var originalStr = resolvedArgs[0]; var reversedStr = """"; for (var i = originalStr.length - 1; i >= 0; i--) { reversedStr += originalStr[i]; } return reversedStr; } else { var reversedArray = resolvedArgs[0].slice(0); reversedArray.reverse(); return reversedArray; } }, _functionAbs: function(resolvedArgs) { return Math.abs(resolvedArgs[0]); }, _functionCeil: function(resolvedArgs) { return Math.ceil(resolvedArgs[0]); }, _functionAvg: function(resolvedArgs) { var sum = 0; var inputArray = resolvedArgs[0]; for (var i = 0; i < inputArray.length; i++) { sum += inputArray[i]; } return sum / inputArray.length; }, _functionContains: function(resolvedArgs) { return resolvedArgs[0].indexOf(resolvedArgs[1]) >= 0; }, _functionFloor: function(resolvedArgs) { return Math.floor(resolvedArgs[0]); }, _functionLength: function(resolvedArgs) { if (!isObject(resolvedArgs[0])) { return resolvedArgs[0].length; } else { return Object.keys(resolvedArgs[0]).length; } }, _functionMap: function(resolvedArgs) { var mapped = []; var interpreter = this._interpreter; var exprefNode = resolvedArgs[0]; var elements = resolvedArgs[1]; for (var i = 0; i < elements.length; i++) { mapped.push(interpreter.visit(exprefNode, elements[i])); } return mapped; }, _functionMerge: function(resolvedArgs) { var merged = {}; for (var i = 0; i < resolvedArgs.length; i++) { var current = resolvedArgs[i]; for (var key in current) { merged[key] = current[key]; } } return merged; }, _functionMax: function(resolvedArgs) { if (resolvedArgs[0].length > 0) { var typeName = this._getTypeName(resolvedArgs[0][0]); if (typeName === TYPE_NUMBER) { return Math.max.apply(Math, resolvedArgs[0]); } else { var elements = resolvedArgs[0]; var maxElement = elements[0]; for (var i = 1; i < elements.length; i++) { if (maxElement.localeCompare(elements[i]) < 0) { maxElement = elements[i]; } } return maxElement; } } else { return null; } }, _functionMin: function(resolvedArgs) { if (resolvedArgs[0].length > 0) { var typeName = this._getTypeName(resolvedArgs[0][0]); if (typeName === TYPE_NUMBER) { return Math.min.apply(Math, resolvedArgs[0]); } else { var elements = resolvedArgs[0]; var minElement = elements[0]; for (var i = 1; i < elements.length; i++) { if (elements[i].localeCompare(minElement) < 0) { minElement = elements[i]; } } return minElement; } } else { return null; } }, _functionSum: function(resolvedArgs) { var sum = 0; var listToSum = resolvedArgs[0]; for (var i = 0; i < listToSum.length; i++) { sum += listToSum[i]; } return sum; }, _functionType: function(resolvedArgs) { switch (this._getTypeName(resolvedArgs[0])) { case TYPE_NUMBER: return ""number""; case TYPE_STRING: return ""string""; case TYPE_ARRAY: return ""array""; case TYPE_OBJECT: return ""object""; case TYPE_BOOLEAN: return ""boolean""; case TYPE_EXPREF: return ""expref""; case TYPE_NULL: return ""null""; } }, _functionKeys: function(resolvedArgs) { return Object.keys(resolvedArgs[0]); }, _functionValues: function(resolvedArgs) { var obj = resolvedArgs[0]; var keys = Object.keys(obj); var values = []; for (var i = 0; i < keys.length; i++) { values.push(obj[keys[i]]); } return values; }, _functionJoin: function(resolvedArgs) { var joinChar = resolvedArgs[0]; var listJoin = resolvedArgs[1]; return listJoin.join(joinChar); }, _functionToArray: function(resolvedArgs) { if (this._getTypeName(resolvedArgs[0]) === TYPE_ARRAY) { return resolvedArgs[0]; } else { return [ resolvedArgs[0] ]; } }, _functionToString: function(resolvedArgs) { if (this._getTypeName(resolvedArgs[0]) === TYPE_STRING) { return resolvedArgs[0]; } else { return JSON.stringify(resolvedArgs[0]); } }, _functionToNumber: function(resolvedArgs) { var typeName = this._getTypeName(resolvedArgs[0]); var convertedValue; if (typeName === TYPE_NUMBER) { return resolvedArgs[0]; } else if (typeName === TYPE_STRING) { convertedValue = +resolvedArgs[0]; if (!isNaN(convertedValue)) { return convertedValue; } } return null; }, _functionNotNull: function(resolvedArgs) { for (var i = 0; i < resolvedArgs.length; i++) { if (this._getTypeName(resolvedArgs[i]) !== TYPE_NULL) { return resolvedArgs[i]; } } return null; }, _functionSort: function(resolvedArgs) { var sortedArray = resolvedArgs[0].slice(0); sortedArray.sort(); return sortedArray; }, _functionSortBy: function(resolvedArgs) { var sortedArray = resolvedArgs[0].slice(0); if (sortedArray.length === 0) { return sortedArray; } var interpreter = this._interpreter; var exprefNode = resolvedArgs[1]; var requiredType = this._getTypeName(interpreter.visit(exprefNode, sortedArray[0])); if ([ TYPE_NUMBER, TYPE_STRING ].indexOf(requiredType) < 0) { throw new Error(""TypeError""); } var that = this; var decorated = []; for (var i = 0; i < sortedArray.length; i++) { decorated.push([ i, sortedArray[i] ]); } decorated.sort(function(a, b) { var exprA = interpreter.visit(exprefNode, a[1]); var exprB = interpreter.visit(exprefNode, b[1]); if (that._getTypeName(exprA) !== requiredType) { throw new Error(""TypeError: expected "" + requiredType + "", received "" + that._getTypeName(exprA)); } else if (that._getTypeName(exprB) !== requiredType) { throw new Error(""TypeError: expected "" + requiredType + "", received "" + that._getTypeName(exprB)); } if (exprA > exprB) { return 1; } else if (exprA < exprB) { return -1; } else { return a[0] - b[0]; } }); for (var j = 0; j < decorated.length; j++) { sortedArray[j] = decorated[j][1]; } return sortedArray; }, _functionMaxBy: function(resolvedArgs) { var exprefNode = resolvedArgs[1]; var resolvedArray = resolvedArgs[0]; var keyFunction = this.createKeyFunction(exprefNode, [ TYPE_NUMBER, TYPE_STRING ]); var maxNumber = -Infinity; var maxRecord; var current; for (var i = 0; i < resolvedArray.length; i++) { current = keyFunction(resolvedArray[i]); if (current > maxNumber) { maxNumber = current; maxRecord = resolvedArray[i]; } } return maxRecord; }, _functionMinBy: function(resolvedArgs) { var exprefNode = resolvedArgs[1]; var resolvedArray = resolvedArgs[0]; var keyFunction = this.createKeyFunction(exprefNode, [ TYPE_NUMBER, TYPE_STRING ]); var minNumber = Infinity; var minRecord; var current; for (var i = 0; i < resolvedArray.length; i++) { current = keyFunction(resolvedArray[i]); if (current < minNumber) { minNumber = current; minRecord = resolvedArray[i]; } } return minRecord; }, createKeyFunction: function(exprefNode, allowedTypes) { var that = this; var interpreter = this._interpreter; var keyFunc = function(x) { var current = interpreter.visit(exprefNode, x); if (allowedTypes.indexOf(that._getTypeName(current)) < 0) { var msg = ""TypeError: expected one of "" + allowedTypes + "", received "" + that._getTypeName(current); throw new Error(msg); } return current; }; return keyFunc; } }; function compile(stream) { var parser = new Parser(); var ast = parser.parse(stream); return ast; } function tokenize(stream) { var lexer = new Lexer(); return lexer.tokenize(stream); } function search(data, expression) { var parser = new Parser(); var runtime = new Runtime(); var interpreter = new TreeInterpreter(runtime); runtime._interpreter = interpreter; var node = parser.parse(expression); return interpreter.search(node, data); } exports.tokenize = tokenize; exports.compile = compile; exports.search = search; exports.strictDeepEqual = strictDeepEqual; })(typeof exports === ""undefined"" ? this.jmespath = {} : exports); }, {} ], 2: [ function(require, module, exports) {}, {} ] }, {}, []); _xamzrequire = function e(t, n, r) { function s(o, u) { if (!n[o]) { if (!t[o]) { var a = typeof _xamzrequire == ""function"" && _xamzrequire; if (!u && a) return a(o, !0); if (i) return i(o, !0); var f = new Error(""Cannot find module '"" + o + ""'""); throw f.code = ""MODULE_NOT_FOUND"", f; } var l = n[o] = { exports: {} }; t[o][0].call(l.exports, function(e) { var n = t[o][1][e]; return s(n ? n : e); }, l, l.exports, e, t, n, r); } return n[o].exports; } var i = typeof _xamzrequire == ""function"" && _xamzrequire; for (var o = 0; o < r.length; o++) s(r[o]); return s; }({ 28: [ function(require, module, exports) { require(""./browser_loader""); var AWS = require(""./core""); if (typeof window !== ""undefined"") window.AWS = AWS; if (typeof module !== ""undefined"") { module.exports = AWS; } if (typeof self !== ""undefined"") self.AWS = AWS; }, { ""./browser_loader"": 35, ""./core"": 38 } ], 35: [ function(require, module, exports) { (function(process) { var util = require(""./util""); util.crypto.lib = require(""./browserCryptoLib""); util.Buffer = require(""buffer/"").Buffer; util.url = require(""url/""); util.querystring = require(""querystring/""); util.realClock = require(""./realclock/browserClock""); util.environment = ""js""; util.createEventStream = require(""./event-stream/buffered-create-event-stream"").createEventStream; util.isBrowser = function() { return true; }; util.isNode = function() { return false; }; var AWS = require(""./core""); module.exports = AWS; require(""./credentials""); require(""./credentials/credential_provider_chain""); require(""./credentials/temporary_credentials""); require(""./credentials/chainable_temporary_credentials""); require(""./credentials/web_identity_credentials""); require(""./credentials/cognito_identity_credentials""); require(""./credentials/saml_credentials""); AWS.XML.Parser = require(""./xml/browser_parser""); require(""./http/xhr""); if (typeof process === ""undefined"") { var process = { browser: true }; } }).call(this, require(""_process"")); }, { ""./browserCryptoLib"": 29, ""./core"": 38, ""./credentials"": 39, ""./credentials/chainable_temporary_credentials"": 40, ""./credentials/cognito_identity_credentials"": 41, ""./credentials/credential_provider_chain"": 42, ""./credentials/saml_credentials"": 43, ""./credentials/temporary_credentials"": 44, ""./credentials/web_identity_credentials"": 45, ""./event-stream/buffered-create-event-stream"": 53, ""./http/xhr"": 61, ""./realclock/browserClock"": 80, ""./util"": 116, ""./xml/browser_parser"": 117, _process: 8, ""buffer/"": 3, ""querystring/"": 15, ""url/"": 17 } ], 117: [ function(require, module, exports) { var util = require(""../util""); var Shape = require(""../model/shape""); function DomXmlParser() {} DomXmlParser.prototype.parse = function(xml, shape) { if (xml.replace(/^\s+/, """") === """") return {}; var result, error; try { if (window.DOMParser) { try { var parser = new DOMParser(); result = parser.parseFromString(xml, ""text/xml""); } catch (syntaxError) { throw util.error(new Error(""Parse error in document""), { originalError: syntaxError, code: ""XMLParserError"", retryable: true }); } if (result.documentElement === null) { throw util.error(new Error(""Cannot parse empty document.""), { code: ""XMLParserError"", retryable: true }); } var isError = result.getElementsByTagName(""parsererror"")[0]; if (isError && (isError.parentNode === result || isError.parentNode.nodeName === ""body"" || isError.parentNode.parentNode === result || isError.parentNode.parentNode.nodeName === ""body"")) { var errorElement = isError.getElementsByTagName(""div"")[0] || isError; throw util.error(new Error(errorElement.textContent || ""Parser error in document""), { code: ""XMLParserError"", retryable: true }); } } else if (window.ActiveXObject) { result = new window.ActiveXObject(""Microsoft.XMLDOM""); result.async = false; if (!result.loadXML(xml)) { throw util.error(new Error(""Parse error in document""), { code: ""XMLParserError"", retryable: true }); } } else { throw new Error(""Cannot load XML parser""); } } catch (e) { error = e; } if (result && result.documentElement && !error) { var data = parseXml(result.documentElement, shape); var metadata = getElementByTagName(result.documentElement, ""ResponseMetadata""); if (metadata) { data.ResponseMetadata = parseXml(metadata, {}); } return data; } else if (error) { throw util.error(error || new Error(), { code: ""XMLParserError"", retryable: true }); } else { return {}; } }; function getElementByTagName(xml, tag) { var elements = xml.getElementsByTagName(tag); for (var i = 0, iLen = elements.length; i < iLen; i++) { if (elements[i].parentNode === xml) { return elements[i]; } } } function parseXml(xml, shape) { if (!shape) shape = {}; switch (shape.type) { case ""structure"": return parseStructure(xml, shape); case ""map"": return parseMap(xml, shape); case ""list"": return parseList(xml, shape); case undefined: case null: return parseUnknown(xml); default: return parseScalar(xml, shape); } } function parseStructure(xml, shape) { var data = {}; if (xml === null) return data; util.each(shape.members, function(memberName, memberShape) { if (memberShape.isXmlAttribute) { if (Object.prototype.hasOwnProperty.call(xml.attributes, memberShape.name)) { var value = xml.attributes[memberShape.name].value; data[memberName] = parseXml({ textContent: value }, memberShape); } } else { var xmlChild = memberShape.flattened ? xml : getElementByTagName(xml, memberShape.name); if (xmlChild) { data[memberName] = parseXml(xmlChild, memberShape); } else if (!memberShape.flattened && memberShape.type === ""list"") { data[memberName] = memberShape.defaultValue; } } }); return data; } function parseMap(xml, shape) { var data = {}; var xmlKey = shape.key.name || ""key""; var xmlValue = shape.value.name || ""value""; var tagName = shape.flattened ? shape.name : ""entry""; var child = xml.firstElementChild; while (child) { if (child.nodeName === tagName) { var key = getElementByTagName(child, xmlKey).textContent; var value = getElementByTagName(child, xmlValue); data[key] = parseXml(value, shape.value); } child = child.nextElementSibling; } return data; } function parseList(xml, shape) { var data = []; var tagName = shape.flattened ? shape.name : shape.member.name || ""member""; var child = xml.firstElementChild; while (child) { if (child.nodeName === tagName) { data.push(parseXml(child, shape.member)); } child = child.nextElementSibling; } return data; } function parseScalar(xml, shape) { if (xml.getAttribute) { var encoding = xml.getAttribute(""encoding""); if (encoding === ""base64"") { shape = new Shape.create({ type: encoding }); } } var text = xml.textContent; if (text === """") text = null; if (typeof shape.toType === ""function"") { return shape.toType(text); } else { return text; } } function parseUnknown(xml) { if (xml === undefined || xml === null) return """"; if (!xml.firstElementChild) { if (xml.parentNode.parentNode === null) return {}; if (xml.childNodes.length === 0) return """"; else return xml.textContent; } var shape = { type: ""structure"", members: {} }; var child = xml.firstElementChild; while (child) { var tag = child.nodeName; if (Object.prototype.hasOwnProperty.call(shape.members, tag)) { shape.members[tag].type = ""list""; } else { shape.members[tag] = { name: tag }; } child = child.nextElementSibling; } return parseStructure(xml, shape); } module.exports = DomXmlParser; }, { ""../model/shape"": 69, ""../util"": 116 } ], 80: [ function(require, module, exports) { module.exports = { now: function now() { if (typeof performance !== ""undefined"" && typeof performance.now === ""function"") { return performance.now(); } return Date.now(); } }; }, {} ], 61: [ function(require, module, exports) { var AWS = require(""../core""); var EventEmitter = require(""events"").EventEmitter; require(""../http""); AWS.XHRClient = AWS.util.inherit({ handleRequest: function handleRequest(httpRequest, httpOptions, callback, errCallback) { var self = this; var endpoint = httpRequest.endpoint; var emitter = new EventEmitter(); var href = endpoint.protocol + ""//"" + endpoint.hostname; if (endpoint.port !== 80 && endpoint.port !== 443) { href += "":"" + endpoint.port; } href += httpRequest.path; var xhr = new XMLHttpRequest(), headersEmitted = false; httpRequest.stream = xhr; xhr.addEventListener(""readystatechange"", function() { try { if (xhr.status === 0) return; } catch (e) { return; } if (this.readyState >= this.HEADERS_RECEIVED && !headersEmitted) { emitter.statusCode = xhr.status; emitter.headers = self.parseHeaders(xhr.getAllResponseHeaders()); emitter.emit(""headers"", emitter.statusCode, emitter.headers, xhr.statusText); headersEmitted = true; } if (this.readyState === this.DONE) { self.finishRequest(xhr, emitter); } }, false); xhr.upload.addEventListener(""progress"", function(evt) { emitter.emit(""sendProgress"", evt); }); xhr.addEventListener(""progress"", function(evt) { emitter.emit(""receiveProgress"", evt); }, false); xhr.addEventListener(""timeout"", function() { errCallback(AWS.util.error(new Error(""Timeout""), { code: ""TimeoutError"" })); }, false); xhr.addEventListener(""error"", function() { errCallback(AWS.util.error(new Error(""Network Failure""), { code: ""NetworkingError"" })); }, false); xhr.addEventListener(""abort"", function() { errCallback(AWS.util.error(new Error(""Request aborted""), { code: ""RequestAbortedError"" })); }, false); callback(emitter); xhr.open(httpRequest.method, href, httpOptions.xhrAsync !== false); AWS.util.each(httpRequest.headers, function(key, value) { if (key !== ""Content-Length"" && key !== ""User-Agent"" && key !== ""Host"") { xhr.setRequestHeader(key, value); } }); if (httpOptions.timeout && httpOptions.xhrAsync !== false) { xhr.timeout = httpOptions.timeout; } if (httpOptions.xhrWithCredentials) { xhr.withCredentials = true; } try { xhr.responseType = ""arraybuffer""; } catch (e) {} try { if (httpRequest.body) { xhr.send(httpRequest.body); } else { xhr.send(); } } catch (err) { if (httpRequest.body && typeof httpRequest.body.buffer === ""object"") { xhr.send(httpRequest.body.buffer); } else { throw err; } } return emitter; }, parseHeaders: function parseHeaders(rawHeaders) { var headers = {}; AWS.util.arrayEach(rawHeaders.split(/\r?\n/), function(line) { var key = line.split("":"", 1)[0]; var value = line.substring(key.length + 2); if (key.length > 0) headers[key.toLowerCase()] = value; }); return headers; }, finishRequest: function finishRequest(xhr, emitter) { var buffer; if (xhr.responseType === ""arraybuffer"" && xhr.response) { var ab = xhr.response; buffer = new AWS.util.Buffer(ab.byteLength); var view = new Uint8Array(ab); for (var i = 0; i < buffer.length; ++i) { buffer[i] = view[i]; } } try { if (!buffer && typeof xhr.responseText === ""string"") { buffer = new AWS.util.Buffer(xhr.responseText); } } catch (e) {} if (buffer) emitter.emit(""data"", buffer); emitter.emit(""end""); } }); AWS.HttpClient.prototype = AWS.XHRClient.prototype; AWS.HttpClient.streamsApiVersion = 1; }, { ""../core"": 38, ""../http"": 60, events: 4 } ], 53: [ function(require, module, exports) { var eventMessageChunker = require(""../event-stream/event-message-chunker"").eventMessageChunker; var parseEvent = require(""./parse-event"").parseEvent; function createEventStream(body, parser, model) { var eventMessages = eventMessageChunker(body); var events = []; for (var i = 0; i < eventMessages.length; i++) { events.push(parseEvent(parser, eventMessages[i], model)); } return events; } module.exports = { createEventStream: createEventStream }; }, { ""../event-stream/event-message-chunker"": 54, ""./parse-event"": 56 } ], 56: [ function(require, module, exports) { var parseMessage = require(""./parse-message"").parseMessage; function parseEvent(parser, message, shape) { var parsedMessage = parseMessage(message); var messageType = parsedMessage.headers["":message-type""]; if (messageType) { if (messageType.value === ""error"") { throw parseError(parsedMessage); } else if (messageType.value !== ""event"") { return; } } var eventType = parsedMessage.headers["":event-type""]; var eventModel = shape.members[eventType.value]; if (!eventModel) { return; } var result = {}; var eventPayloadMemberName = eventModel.eventPayloadMemberName; if (eventPayloadMemberName) { var payloadShape = eventModel.members[eventPayloadMemberName]; if (payloadShape.type === ""binary"") { result[eventPayloadMemberName] = parsedMessage.body; } else { result[eventPayloadMemberName] = parser.parse(parsedMessage.body.toString(), payloadShape); } } var eventHeaderNames = eventModel.eventHeaderMemberNames; for (var i = 0; i < eventHeaderNames.length; i++) { var name = eventHeaderNames[i]; if (parsedMessage.headers[name]) { result[name] = eventModel.members[name].toType(parsedMessage.headers[name].value); } } var output = {}; output[eventType.value] = result; return output; } function parseError(message) { var errorCode = message.headers["":error-code""]; var errorMessage = message.headers["":error-message""]; var error = new Error(errorMessage.value || errorMessage); error.code = error.name = errorCode.value || errorCode; return error; } module.exports = { parseEvent: parseEvent }; }, { ""./parse-message"": 57 } ], 57: [ function(require, module, exports) { var Int64 = require(""./int64"").Int64; var splitMessage = require(""./split-message"").splitMessage; var BOOLEAN_TAG = ""boolean""; var BYTE_TAG = ""byte""; var SHORT_TAG = ""short""; var INT_TAG = ""integer""; var LONG_TAG = ""long""; var BINARY_TAG = ""binary""; var STRING_TAG = ""string""; var TIMESTAMP_TAG = ""timestamp""; var UUID_TAG = ""uuid""; function parseHeaders(headers) { var out = {}; var position = 0; while (position < headers.length) { var nameLength = headers.readUInt8(position++); var name = headers.slice(position, position + nameLength).toString(); position += nameLength; switch (headers.readUInt8(position++)) { case 0: out[name] = { type: BOOLEAN_TAG, value: true }; break; case 1: out[name] = { type: BOOLEAN_TAG, value: false }; break; case 2: out[name] = { type: BYTE_TAG, value: headers.readInt8(position++) }; break; case 3: out[name] = { type: SHORT_TAG, value: headers.readInt16BE(position) }; position += 2; break; case 4: out[name] = { type: INT_TAG, value: headers.readInt32BE(position) }; position += 4; break; case 5: out[name] = { type: LONG_TAG, value: new Int64(headers.slice(position, position + 8)) }; position += 8; break; case 6: var binaryLength = headers.readUInt16BE(position); position += 2; out[name] = { type: BINARY_TAG, value: headers.slice(position, position + binaryLength) }; position += binaryLength; break; case 7: var stringLength = headers.readUInt16BE(position); position += 2; out[name] = { type: STRING_TAG, value: headers.slice(position, position + stringLength).toString() }; position += stringLength; break; case 8: out[name] = { type: TIMESTAMP_TAG, value: new Date(new Int64(headers.slice(position, position + 8)).valueOf()) }; position += 8; break; case 9: var uuidChars = headers.slice(position, position + 16).toString(""hex""); position += 16; out[name] = { type: UUID_TAG, value: uuidChars.substr(0, 8) + ""-"" + uuidChars.substr(8, 4) + ""-"" + uuidChars.substr(12, 4) + ""-"" + uuidChars.substr(16, 4) + ""-"" + uuidChars.substr(20) }; break; default: throw new Error(""Unrecognized header type tag""); } } return out; } function parseMessage(message) { var parsed = splitMessage(message); return { headers: parseHeaders(parsed.headers), body: parsed.body }; } module.exports = { parseMessage: parseMessage }; }, { ""./int64"": 55, ""./split-message"": 58 } ], 58: [ function(require, module, exports) { var util = require(""../core"").util; var toBuffer = util.buffer.toBuffer; var PRELUDE_MEMBER_LENGTH = 4; var PRELUDE_LENGTH = PRELUDE_MEMBER_LENGTH * 2; var CHECKSUM_LENGTH = 4; var MINIMUM_MESSAGE_LENGTH = PRELUDE_LENGTH + CHECKSUM_LENGTH * 2; function splitMessage(message) { if (!util.Buffer.isBuffer(message)) message = toBuffer(message); if (message.length < MINIMUM_MESSAGE_LENGTH) { throw new Error(""Provided message too short to accommodate event stream message overhead""); } if (message.length !== message.readUInt32BE(0)) { throw new Error(""Reported message length does not match received message length""); } var expectedPreludeChecksum = message.readUInt32BE(PRELUDE_LENGTH); if (expectedPreludeChecksum !== util.crypto.crc32(message.slice(0, PRELUDE_LENGTH))) { throw new Error(""The prelude checksum specified in the message ("" + expectedPreludeChecksum + "") does not match the calculated CRC32 checksum.""); } var expectedMessageChecksum = message.readUInt32BE(message.length - CHECKSUM_LENGTH); if (expectedMessageChecksum !== util.crypto.crc32(message.slice(0, message.length - CHECKSUM_LENGTH))) { throw new Error(""The message checksum did not match the expected value of "" + expectedMessageChecksum); } var headersStart = PRELUDE_LENGTH + CHECKSUM_LENGTH; var headersEnd = headersStart + message.readUInt32BE(PRELUDE_MEMBER_LENGTH); return { headers: message.slice(headersStart, headersEnd), body: message.slice(headersEnd, message.length - CHECKSUM_LENGTH) }; } module.exports = { splitMessage: splitMessage }; }, { ""../core"": 38 } ], 55: [ function(require, module, exports) { var util = require(""../core"").util; var toBuffer = util.buffer.toBuffer; function Int64(bytes) { if (bytes.length !== 8) { throw new Error(""Int64 buffers must be exactly 8 bytes""); } if (!util.Buffer.isBuffer(bytes)) bytes = toBuffer(bytes); this.bytes = bytes; } Int64.fromNumber = function(number) { if (number > 0x8000000000000000 || number < -0x8000000000000000) { throw new Error(number + "" is too large (or, if negative, too small) to represent as an Int64""); } var bytes = new Uint8Array(8); for (var i = 7, remaining = Math.abs(Math.round(number)); i > -1 && remaining > 0; i--, remaining /= 256) { bytes[i] = remaining; } if (number < 0) { negate(bytes); } return new Int64(bytes); }; Int64.prototype.valueOf = function() { var bytes = this.bytes.slice(0); var negative = bytes[0] & 128; if (negative) { negate(bytes); } return parseInt(bytes.toString(""hex""), 16) * (negative ? -1 : 1); }; Int64.prototype.toString = function() { return String(this.valueOf()); }; function negate(bytes) { for (var i = 0; i < 8; i++) { bytes[i] ^= 255; } for (var i = 7; i > -1; i--) { bytes[i]++; if (bytes[i] !== 0) { break; } } } module.exports = { Int64: Int64 }; }, { ""../core"": 38 } ], 54: [ function(require, module, exports) { function eventMessageChunker(buffer) { var messages = []; var offset = 0; while (offset < buffer.length) { var totalLength = buffer.readInt32BE(offset); var message = buffer.slice(offset, totalLength + offset); offset += totalLength; messages.push(message); } return messages; } module.exports = { eventMessageChunker: eventMessageChunker }; }, {} ], 45: [ function(require, module, exports) { var AWS = require(""../core""); AWS.WebIdentityCredentials = AWS.util.inherit(AWS.Credentials, { constructor: function WebIdentityCredentials(params, clientConfig) { AWS.Credentials.call(this); this.expired = true; this.params = params; this.params.RoleSessionName = this.params.RoleSessionName || ""web-identity""; this.data = null; this._clientConfig = AWS.util.copy(clientConfig || {}); }, refresh: function refresh(callback) { this.coalesceRefresh(callback || AWS.util.fn.callback); }, load: function load(callback) { var self = this; self.createClients(); self.service.assumeRoleWithWebIdentity(function(err, data) { self.data = null; if (!err) { self.data = data; self.service.credentialsFrom(data, self); } callback(err); }); }, createClients: function() { if (!this.service) { var stsConfig = AWS.util.merge({}, this._clientConfig); stsConfig.params = this.params; this.service = new AWS.STS(stsConfig); } } }); }, { ""../core"": 38 } ], 44: [ function(require, module, exports) { var AWS = require(""../core""); AWS.TemporaryCredentials = AWS.util.inherit(AWS.Credentials, { constructor: function TemporaryCredentials(params, masterCredentials) { AWS.Credentials.call(this); this.loadMasterCredentials(masterCredentials); this.expired = true; this.params = params || {}; if (this.params.RoleArn) { this.params.RoleSessionName = this.params.RoleSessionName || ""temporary-credentials""; } }, refresh: function refresh(callback) { this.coalesceRefresh(callback || AWS.util.fn.callback); }, load: function load(callback) { var self = this; self.createClients(); self.masterCredentials.get(function() { self.service.config.credentials = self.masterCredentials; var operation = self.params.RoleArn ? self.service.assumeRole : self.service.getSessionToken; operation.call(self.service, function(err, data) { if (!err) { self.service.credentialsFrom(data, self); } callback(err); }); }); }, loadMasterCredentials: function loadMasterCredentials(masterCredentials) { this.masterCredentials = masterCredentials || AWS.config.credentials; while (this.masterCredentials.masterCredentials) { this.masterCredentials = this.masterCredentials.masterCredentials; } if (typeof this.masterCredentials.get !== ""function"") { this.masterCredentials = new AWS.Credentials(this.masterCredentials); } }, createClients: function() { this.service = this.service || new AWS.STS({ params: this.params }); } }); }, { ""../core"": 38 } ], 43: [ function(require, module, exports) { var AWS = require(""../core""); AWS.SAMLCredentials = AWS.util.inherit(AWS.Credentials, { constructor: function SAMLCredentials(params) { AWS.Credentials.call(this); this.expired = true; this.params = params; }, refresh: function refresh(callback) { this.coalesceRefresh(callback || AWS.util.fn.callback); }, load: function load(callback) { var self = this; self.createClients(); self.service.assumeRoleWithSAML(function(err, data) { if (!err) { self.service.credentialsFrom(data, self); } callback(err); }); }, createClients: function() { this.service = this.service || new AWS.STS({ params: this.params }); } }); }, { ""../core"": 38 } ], 41: [ function(require, module, exports) { var AWS = require(""../core""); AWS.CognitoIdentityCredentials = AWS.util.inherit(AWS.Credentials, { localStorageKey: { id: ""aws.cognito.identity-id."", providers: ""aws.cognito.identity-providers."" }, constructor: function CognitoIdentityCredentials(params, clientConfig) { AWS.Credentials.call(this); this.expired = true; this.params = params; this.data = null; this._identityId = null; this._clientConfig = AWS.util.copy(clientConfig || {}); this.loadCachedId(); var self = this; Object.defineProperty(this, ""identityId"", { get: function() { self.loadCachedId(); return self._identityId || self.params.IdentityId; }, set: function(identityId) { self._identityId = identityId; } }); }, refresh: function refresh(callback) { this.coalesceRefresh(callback || AWS.util.fn.callback); }, load: function load(callback) { var self = this; self.createClients(); self.data = null; self._identityId = null; self.getId(function(err) { if (!err) { if (!self.params.RoleArn) { self.getCredentialsForIdentity(callback); } else { self.getCredentialsFromSTS(callback); } } else { self.clearIdOnNotAuthorized(err); callback(err); } }); }, clearCachedId: function clearCache() { this._identityId = null; delete this.params.IdentityId; var poolId = this.params.IdentityPoolId; var loginId = this.params.LoginId || """"; delete this.storage[this.localStorageKey.id + poolId + loginId]; delete this.storage[this.localStorageKey.providers + poolId + loginId]; }, clearIdOnNotAuthorized: function clearIdOnNotAuthorized(err) { var self = this; if (err.code == ""NotAuthorizedException"") { self.clearCachedId(); } }, getId: function getId(callback) { var self = this; if (typeof self.params.IdentityId === ""string"") { return callback(null, self.params.IdentityId); } self.cognito.getId(function(err, data) { if (!err && data.IdentityId) { self.params.IdentityId = data.IdentityId; callback(null, data.IdentityId); } else { callback(err); } }); }, loadCredentials: function loadCredentials(data, credentials) { if (!data || !credentials) return; credentials.expired = false; credentials.accessKeyId = data.Credentials.AccessKeyId; credentials.secretAccessKey = data.Credentials.SecretKey; credentials.sessionToken = data.Credentials.SessionToken; credentials.expireTime = data.Credentials.Expiration; }, getCredentialsForIdentity: function getCredentialsForIdentity(callback) { var self = this; self.cognito.getCredentialsForIdentity(function(err, data) { if (!err) { self.cacheId(data); self.data = data; self.loadCredentials(self.data, self); } else { self.clearIdOnNotAuthorized(err); } callback(err); }); }, getCredentialsFromSTS: function getCredentialsFromSTS(callback) { var self = this; self.cognito.getOpenIdToken(function(err, data) { if (!err) { self.cacheId(data); self.params.WebIdentityToken = data.Token; self.webIdentityCredentials.refresh(function(webErr) { if (!webErr) { self.data = self.webIdentityCredentials.data; self.sts.credentialsFrom(self.data, self); } callback(webErr); }); } else { self.clearIdOnNotAuthorized(err); callback(err); } }); }, loadCachedId: function loadCachedId() { var self = this; if (AWS.util.isBrowser() && !self.params.IdentityId) { var id = self.getStorage(""id""); if (id && self.params.Logins) { var actualProviders = Object.keys(self.params.Logins); var cachedProviders = (self.getStorage(""providers"") || """").split("",""); var intersect = cachedProviders.filter(function(n) { return actualProviders.indexOf(n) !== -1; }); if (intersect.length !== 0) { self.params.IdentityId = id; } } else if (id) { self.params.IdentityId = id; } } }, createClients: function() { var clientConfig = this._clientConfig; this.webIdentityCredentials = this.webIdentityCredentials || new AWS.WebIdentityCredentials(this.params, clientConfig); if (!this.cognito) { var cognitoConfig = AWS.util.merge({}, clientConfig); cognitoConfig.params = this.params; this.cognito = new AWS.CognitoIdentity(cognitoConfig); } this.sts = this.sts || new AWS.STS(clientConfig); }, cacheId: function cacheId(data) { this._identityId = data.IdentityId; this.params.IdentityId = this._identityId; if (AWS.util.isBrowser()) { this.setStorage(""id"", data.IdentityId); if (this.params.Logins) { this.setStorage(""providers"", Object.keys(this.params.Logins).join("","")); } } }, getStorage: function getStorage(key) { return this.storage[this.localStorageKey[key] + this.params.IdentityPoolId + (this.params.LoginId || """")]; }, setStorage: function setStorage(key, val) { try { this.storage[this.localStorageKey[key] + this.params.IdentityPoolId + (this.params.LoginId || """")] = val; } catch (_) {} }, storage: function() { try { var storage = AWS.util.isBrowser() && window.localStorage !== null && typeof window.localStorage === ""object"" ? window.localStorage : {}; storage[""aws.test-storage""] = ""foobar""; delete storage[""aws.test-storage""]; return storage; } catch (_) { return {}; } }() }); }, { ""../core"": 38 } ], 40: [ function(require, module, exports) { var AWS = require(""../core""); AWS.ChainableTemporaryCredentials = AWS.util.inherit(AWS.Credentials, { constructor: function ChainableTemporaryCredentials(options) { AWS.Credentials.call(this); options = options || {}; this.errorCode = ""ChainableTemporaryCredentialsProviderFailure""; this.expired = true; this.tokenCodeFn = null; var params = AWS.util.copy(options.params) || {}; if (params.RoleArn) { params.RoleSessionName = params.RoleSessionName || ""temporary-credentials""; } if (params.SerialNumber) { if (!options.tokenCodeFn || typeof options.tokenCodeFn !== ""function"") { throw new AWS.util.error(new Error(""tokenCodeFn must be a function when params.SerialNumber is given""), { code: this.errorCode }); } else { this.tokenCodeFn = options.tokenCodeFn; } } var config = AWS.util.merge({ params: params, credentials: options.masterCredentials || AWS.config.credentials }, options.stsConfig || {}); this.service = new AWS.STS(config); }, refresh: function refresh(callback) { this.coalesceRefresh(callback || AWS.util.fn.callback); }, load: function load(callback) { var self = this; var operation = self.service.config.params.RoleArn ? ""assumeRole"" : ""getSessionToken""; this.getTokenCode(function(err, tokenCode) { var params = {}; if (err) { callback(err); return; } if (tokenCode) { params.TokenCode = tokenCode; } self.service[operation](params, function(err, data) { if (!err) { self.service.credentialsFrom(data, self); } callback(err); }); }); }, getTokenCode: function getTokenCode(callback) { var self = this; if (this.tokenCodeFn) { this.tokenCodeFn(this.service.config.params.SerialNumber, function(err, token) { if (err) { var message = err; if (err instanceof Error) { message = err.message; } callback(AWS.util.error(new Error(""Error fetching MFA token: "" + message), { code: self.errorCode })); return; } callback(null, token); }); } else { callback(null); } } }); }, { ""../core"": 38 } ], 29: [ function(require, module, exports) { var Hmac = require(""./browserHmac""); var Md5 = require(""./browserMd5""); var Sha1 = require(""./browserSha1""); var Sha256 = require(""./browserSha256""); module.exports = exports = { createHash: function createHash(alg) { alg = alg.toLowerCase(); if (alg === ""md5"") { return new Md5(); } else if (alg === ""sha256"") { return new Sha256(); } else if (alg === ""sha1"") { return new Sha1(); } throw new Error(""Hash algorithm "" + alg + "" is not supported in the browser SDK""); }, createHmac: function createHmac(alg, key) { alg = alg.toLowerCase(); if (alg === ""md5"") { return new Hmac(Md5, key); } else if (alg === ""sha256"") { return new Hmac(Sha256, key); } else if (alg === ""sha1"") { return new Hmac(Sha1, key); } throw new Error(""HMAC algorithm "" + alg + "" is not supported in the browser SDK""); }, createSign: function() { throw new Error(""createSign is not implemented in the browser""); } }; }, { ""./browserHmac"": 31, ""./browserMd5"": 32, ""./browserSha1"": 33, ""./browserSha256"": 34 } ], 34: [ function(require, module, exports) { var Buffer = require(""buffer/"").Buffer; var hashUtils = require(""./browserHashUtils""); var BLOCK_SIZE = 64; var DIGEST_LENGTH = 32; var KEY = new Uint32Array([ 1116352408, 1899447441, 3049323471, 3921009573, 961987163, 1508970993, 2453635748, 2870763221, 3624381080, 310598401, 607225278, 1426881987, 1925078388, 2162078206, 2614888103, 3248222580, 3835390401, 4022224774, 264347078, 604807628, 770255983, 1249150122, 1555081692, 1996064986, 2554220882, 2821834349, 2952996808, 3210313671, 3336571891, 3584528711, 113926993, 338241895, 666307205, 773529912, 1294757372, 1396182291, 1695183700, 1986661051, 2177026350, 2456956037, 2730485921, 2820302411, 3259730800, 3345764771, 3516065817, 3600352804, 4094571909, 275423344, 430227734, 506948616, 659060556, 883997877, 958139571, 1322822218, 1537002063, 1747873779, 1955562222, 2024104815, 2227730452, 2361852424, 2428436474, 2756734187, 3204031479, 3329325298 ]); var INIT = [ 1779033703, 3144134277, 1013904242, 2773480762, 1359893119, 2600822924, 528734635, 1541459225 ]; var MAX_HASHABLE_LENGTH = Math.pow(2, 53) - 1; function Sha256() { this.state = [ 1779033703, 3144134277, 1013904242, 2773480762, 1359893119, 2600822924, 528734635, 1541459225 ]; this.temp = new Int32Array(64); this.buffer = new Uint8Array(64); this.bufferLength = 0; this.bytesHashed = 0; this.finished = false; } module.exports = exports = Sha256; Sha256.BLOCK_SIZE = BLOCK_SIZE; Sha256.prototype.update = function(data) { if (this.finished) { throw new Error(""Attempted to update an already finished hash.""); } if (hashUtils.isEmptyData(data)) { return this; } data = hashUtils.convertToBuffer(data); var position = 0; var byteLength = data.byteLength; this.bytesHashed += byteLength; if (this.bytesHashed * 8 > MAX_HASHABLE_LENGTH) { throw new Error(""Cannot hash more than 2^53 - 1 bits""); } while (byteLength > 0) { this.buffer[this.bufferLength++] = data[position++]; byteLength--; if (this.bufferLength === BLOCK_SIZE) { this.hashBuffer(); this.bufferLength = 0; } } return this; }; Sha256.prototype.digest = function(encoding) { if (!this.finished) { var bitsHashed = this.bytesHashed * 8; var bufferView = new DataView(this.buffer.buffer, this.buffer.byteOffset, this.buffer.byteLength); var undecoratedLength = this.bufferLength; bufferView.setUint8(this.bufferLength++, 128); if (undecoratedLength % BLOCK_SIZE >= BLOCK_SIZE - 8) { for (var i = this.bufferLength; i < BLOCK_SIZE; i++) { bufferView.setUint8(i, 0); } this.hashBuffer(); this.bufferLength = 0; } for (var i = this.bufferLength; i < BLOCK_SIZE - 8; i++) { bufferView.setUint8(i, 0); } bufferView.setUint32(BLOCK_SIZE - 8, Math.floor(bitsHashed / 4294967296), true); bufferView.setUint32(BLOCK_SIZE - 4, bitsHashed); this.hashBuffer(); this.finished = true; } var out = new Buffer(DIGEST_LENGTH); for (var i = 0; i < 8; i++) { out[i * 4] = this.state[i] >>> 24 & 255; out[i * 4 + 1] = this.state[i] >>> 16 & 255; out[i * 4 + 2] = this.state[i] >>> 8 & 255; out[i * 4 + 3] = this.state[i] >>> 0 & 255; } return encoding ? out.toString(encoding) : out; }; Sha256.prototype.hashBuffer = function() { var _a = this, buffer = _a.buffer, state = _a.state; var state0 = state[0], state1 = state[1], state2 = state[2], state3 = state[3], state4 = state[4], state5 = state[5], state6 = state[6], state7 = state[7]; for (var i = 0; i < BLOCK_SIZE; i++) { if (i < 16) { this.temp[i] = (buffer[i * 4] & 255) << 24 | (buffer[i * 4 + 1] & 255) << 16 | (buffer[i * 4 + 2] & 255) << 8 | buffer[i * 4 + 3] & 255; } else { var u = this.temp[i - 2]; var t1_1 = (u >>> 17 | u << 15) ^ (u >>> 19 | u << 13) ^ u >>> 10; u = this.temp[i - 15]; var t2_1 = (u >>> 7 | u << 25) ^ (u >>> 18 | u << 14) ^ u >>> 3; this.temp[i] = (t1_1 + this.temp[i - 7] | 0) + (t2_1 + this.temp[i - 16] | 0); } var t1 = (((state4 >>> 6 | state4 << 26) ^ (state4 >>> 11 | state4 << 21) ^ (state4 >>> 25 | state4 << 7)) + (state4 & state5 ^ ~state4 & state6) | 0) + (state7 + (KEY[i] + this.temp[i] | 0) | 0) | 0; var t2 = ((state0 >>> 2 | state0 << 30) ^ (state0 >>> 13 | state0 << 19) ^ (state0 >>> 22 | state0 << 10)) + (state0 & state1 ^ state0 & state2 ^ state1 & state2) | 0; state7 = state6; state6 = state5; state5 = state4; state4 = state3 + t1 | 0; state3 = state2; state2 = state1; state1 = state0; state0 = t1 + t2 | 0; } state[0] += state0; state[1] += state1; state[2] += state2; state[3] += state3; state[4] += state4; state[5] += state5; state[6] += state6; state[7] += state7; }; }, { ""./browserHashUtils"": 30, ""buffer/"": 3 } ], 33: [ function(require, module, exports) { var Buffer = require(""buffer/"").Buffer; var hashUtils = require(""./browserHashUtils""); var BLOCK_SIZE = 64; var DIGEST_LENGTH = 20; var KEY = new Uint32Array([ 1518500249, 1859775393, 2400959708 | 0, 3395469782 | 0 ]); var INIT = [ 1779033703, 3144134277, 1013904242, 2773480762, 1359893119, 2600822924, 528734635, 1541459225 ]; var MAX_HASHABLE_LENGTH = Math.pow(2, 53) - 1; function Sha1() { this.h0 = 1732584193; this.h1 = 4023233417; this.h2 = 2562383102; this.h3 = 271733878; this.h4 = 3285377520; this.block = new Uint32Array(80); this.offset = 0; this.shift = 24; this.totalLength = 0; } module.exports = exports = Sha1; Sha1.BLOCK_SIZE = BLOCK_SIZE; Sha1.prototype.update = function(data) { if (this.finished) { throw new Error(""Attempted to update an already finished hash.""); } if (hashUtils.isEmptyData(data)) { return this; } data = hashUtils.convertToBuffer(data); var length = data.length; this.totalLength += length * 8; for (var i = 0; i < length; i++) { this.write(data[i]); } return this; }; Sha1.prototype.write = function write(byte) { this.block[this.offset] |= (byte & 255) << this.shift; if (this.shift) { this.shift -= 8; } else { this.offset++; this.shift = 24; } if (this.offset === 16) this.processBlock(); }; Sha1.prototype.digest = function(encoding) { this.write(128); if (this.offset > 14 || this.offset === 14 && this.shift < 24) { this.processBlock(); } this.offset = 14; this.shift = 24; this.write(0); this.write(0); this.write(this.totalLength > 0xffffffffff ? this.totalLength / 1099511627776 : 0); this.write(this.totalLength > 4294967295 ? this.totalLength / 4294967296 : 0); for (var s = 24; s >= 0; s -= 8) { this.write(this.totalLength >> s); } var out = new Buffer(DIGEST_LENGTH); var outView = new DataView(out.buffer); outView.setUint32(0, this.h0, false); outView.setUint32(4, this.h1, false); outView.setUint32(8, this.h2, false); outView.setUint32(12, this.h3, false); outView.setUint32(16, this.h4, false); return encoding ? out.toString(encoding) : out; }; Sha1.prototype.processBlock = function processBlock() { for (var i = 16; i < 80; i++) { var w = this.block[i - 3] ^ this.block[i - 8] ^ this.block[i - 14] ^ this.block[i - 16]; this.block[i] = w << 1 | w >>> 31; } var a = this.h0; var b = this.h1; var c = this.h2; var d = this.h3; var e = this.h4; var f, k; for (i = 0; i < 80; i++) { if (i < 20) { f = d ^ b & (c ^ d); k = 1518500249; } else if (i < 40) { f = b ^ c ^ d; k = 1859775393; } else if (i < 60) { f = b & c | d & (b | c); k = 2400959708; } else { f = b ^ c ^ d; k = 3395469782; } var temp = (a << 5 | a >>> 27) + f + e + k + (this.block[i] | 0); e = d; d = c; c = b << 30 | b >>> 2; b = a; a = temp; } this.h0 = this.h0 + a | 0; this.h1 = this.h1 + b | 0; this.h2 = this.h2 + c | 0; this.h3 = this.h3 + d | 0; this.h4 = this.h4 + e | 0; this.offset = 0; for (i = 0; i < 16; i++) { this.block[i] = 0; } }; }, { ""./browserHashUtils"": 30, ""buffer/"": 3 } ], 32: [ function(require, module, exports) { var hashUtils = require(""./browserHashUtils""); var Buffer = require(""buffer/"").Buffer; var BLOCK_SIZE = 64; var DIGEST_LENGTH = 16; var INIT = [ 1732584193, 4023233417, 2562383102, 271733878 ]; function Md5() { this.state = [ 1732584193, 4023233417, 2562383102, 271733878 ]; this.buffer = new DataView(new ArrayBuffer(BLOCK_SIZE)); this.bufferLength = 0; this.bytesHashed = 0; this.finished = false; } module.exports = exports = Md5; Md5.BLOCK_SIZE = BLOCK_SIZE; Md5.prototype.update = function(sourceData) { if (hashUtils.isEmptyData(sourceData)) { return this; } else if (this.finished) { throw new Error(""Attempted to update an already finished hash.""); } var data = hashUtils.convertToBuffer(sourceData); var position = 0; var byteLength = data.byteLength; this.bytesHashed += byteLength; while (byteLength > 0) { this.buffer.setUint8(this.bufferLength++, data[position++]); byteLength--; if (this.bufferLength === BLOCK_SIZE) { this.hashBuffer(); this.bufferLength = 0; } } return this; }; Md5.prototype.digest = function(encoding) { if (!this.finished) { var _a = this, buffer = _a.buffer, undecoratedLength = _a.bufferLength, bytesHashed = _a.bytesHashed; var bitsHashed = bytesHashed * 8; buffer.setUint8(this.bufferLength++, 128); if (undecoratedLength % BLOCK_SIZE >= BLOCK_SIZE - 8) { for (var i = this.bufferLength; i < BLOCK_SIZE; i++) { buffer.setUint8(i, 0); } this.hashBuffer(); this.bufferLength = 0; } for (var i = this.bufferLength; i < BLOCK_SIZE - 8; i++) { buffer.setUint8(i, 0); } buffer.setUint32(BLOCK_SIZE - 8, bitsHashed >>> 0, true); buffer.setUint32(BLOCK_SIZE - 4, Math.floor(bitsHashed / 4294967296), true); this.hashBuffer(); this.finished = true; } var out = new DataView(new ArrayBuffer(DIGEST_LENGTH)); for (var i = 0; i < 4; i++) { out.setUint32(i * 4, this.state[i], true); } var buff = new Buffer(out.buffer, out.byteOffset, out.byteLength); return encoding ? buff.toString(encoding) : buff; }; Md5.prototype.hashBuffer = function() { var _a = this, buffer = _a.buffer, state = _a.state; var a = state[0], b = state[1], c = state[2], d = state[3]; a = ff(a, b, c, d, buffer.getUint32(0, true), 7, 3614090360); d = ff(d, a, b, c, buffer.getUint32(4, true), 12, 3905402710); c = ff(c, d, a, b, buffer.getUint32(8, true), 17, 606105819); b = ff(b, c, d, a, buffer.getUint32(12, true), 22, 3250441966); a = ff(a, b, c, d, buffer.getUint32(16, true), 7, 4118548399); d = ff(d, a, b, c, buffer.getUint32(20, true), 12, 1200080426); c = ff(c, d, a, b, buffer.getUint32(24, true), 17, 2821735955); b = ff(b, c, d, a, buffer.getUint32(28, true), 22, 4249261313); a = ff(a, b, c, d, buffer.getUint32(32, true), 7, 1770035416); d = ff(d, a, b, c, buffer.getUint32(36, true), 12, 2336552879); c = ff(c, d, a, b, buffer.getUint32(40, true), 17, 4294925233); b = ff(b, c, d, a, buffer.getUint32(44, true), 22, 2304563134); a = ff(a, b, c, d, buffer.getUint32(48, true), 7, 1804603682); d = ff(d, a, b, c, buffer.getUint32(52, true), 12, 4254626195); c = ff(c, d, a, b, buffer.getUint32(56, true), 17, 2792965006); b = ff(b, c, d, a, buffer.getUint32(60, true), 22, 1236535329); a = gg(a, b, c, d, buffer.getUint32(4, true), 5, 4129170786); d = gg(d, a, b, c, buffer.getUint32(24, true), 9, 3225465664); c = gg(c, d, a, b, buffer.getUint32(44, true), 14, 643717713); b = gg(b, c, d, a, buffer.getUint32(0, true), 20, 3921069994); a = gg(a, b, c, d, buffer.getUint32(20, true), 5, 3593408605); d = gg(d, a, b, c, buffer.getUint32(40, true), 9, 38016083); c = gg(c, d, a, b, buffer.getUint32(60, true), 14, 3634488961); b = gg(b, c, d, a, buffer.getUint32(16, true), 20, 3889429448); a = gg(a, b, c, d, buffer.getUint32(36, true), 5, 568446438); d = gg(d, a, b, c, buffer.getUint32(56, true), 9, 3275163606); c = gg(c, d, a, b, buffer.getUint32(12, true), 14, 4107603335); b = gg(b, c, d, a, buffer.getUint32(32, true), 20, 1163531501); a = gg(a, b, c, d, buffer.getUint32(52, true), 5, 2850285829); d = gg(d, a, b, c, buffer.getUint32(8, true), 9, 4243563512); c = gg(c, d, a, b, buffer.getUint32(28, true), 14, 1735328473); b = gg(b, c, d, a, buffer.getUint32(48, true), 20, 2368359562); a = hh(a, b, c, d, buffer.getUint32(20, true), 4, 4294588738); d = hh(d, a, b, c, buffer.getUint32(32, true), 11, 2272392833); c = hh(c, d, a, b, buffer.getUint32(44, true), 16, 1839030562); b = hh(b, c, d, a, buffer.getUint32(56, true), 23, 4259657740); a = hh(a, b, c, d, buffer.getUint32(4, true), 4, 2763975236); d = hh(d, a, b, c, buffer.getUint32(16, true), 11, 1272893353); c = hh(c, d, a, b, buffer.getUint32(28, true), 16, 4139469664); b = hh(b, c, d, a, buffer.getUint32(40, true), 23, 3200236656); a = hh(a, b, c, d, buffer.getUint32(52, true), 4, 681279174); d = hh(d, a, b, c, buffer.getUint32(0, true), 11, 3936430074); c = hh(c, d, a, b, buffer.getUint32(12, true), 16, 3572445317); b = hh(b, c, d, a, buffer.getUint32(24, true), 23, 76029189); a = hh(a, b, c, d, buffer.getUint32(36, true), 4, 3654602809); d = hh(d, a, b, c, buffer.getUint32(48, true), 11, 3873151461); c = hh(c, d, a, b, buffer.getUint32(60, true), 16, 530742520); b = hh(b, c, d, a, buffer.getUint32(8, true), 23, 3299628645); a = ii(a, b, c, d, buffer.getUint32(0, true), 6, 4096336452); d = ii(d, a, b, c, buffer.getUint32(28, true), 10, 1126891415); c = ii(c, d, a, b, buffer.getUint32(56, true), 15, 2878612391); b = ii(b, c, d, a, buffer.getUint32(20, true), 21, 4237533241); a = ii(a, b, c, d, buffer.getUint32(48, true), 6, 1700485571); d = ii(d, a, b, c, buffer.getUint32(12, true), 10, 2399980690); c = ii(c, d, a, b, buffer.getUint32(40, true), 15, 4293915773); b = ii(b, c, d, a, buffer.getUint32(4, true), 21, 2240044497); a = ii(a, b, c, d, buffer.getUint32(32, true), 6, 1873313359); d = ii(d, a, b, c, buffer.getUint32(60, true), 10, 4264355552); c = ii(c, d, a, b, buffer.getUint32(24, true), 15, 2734768916); b = ii(b, c, d, a, buffer.getUint32(52, true), 21, 1309151649); a = ii(a, b, c, d, buffer.getUint32(16, true), 6, 4149444226); d = ii(d, a, b, c, buffer.getUint32(44, true), 10, 3174756917); c = ii(c, d, a, b, buffer.getUint32(8, true), 15, 718787259); b = ii(b, c, d, a, buffer.getUint32(36, true), 21, 3951481745); state[0] = a + state[0] & 4294967295; state[1] = b + state[1] & 4294967295; state[2] = c + state[2] & 4294967295; state[3] = d + state[3] & 4294967295; }; function cmn(q, a, b, x, s, t) { a = (a + q & 4294967295) + (x + t & 4294967295) & 4294967295; return (a << s | a >>> 32 - s) + b & 4294967295; } function ff(a, b, c, d, x, s, t) { return cmn(b & c | ~b & d, a, b, x, s, t); } function gg(a, b, c, d, x, s, t) { return cmn(b & d | c & ~d, a, b, x, s, t); } function hh(a, b, c, d, x, s, t) { return cmn(b ^ c ^ d, a, b, x, s, t); } function ii(a, b, c, d, x, s, t) { return cmn(c ^ (b | ~d), a, b, x, s, t); } }, { ""./browserHashUtils"": 30, ""buffer/"": 3 } ], 31: [ function(require, module, exports) { var hashUtils = require(""./browserHashUtils""); function Hmac(hashCtor, secret) { this.hash = new hashCtor(); this.outer = new hashCtor(); var inner = bufferFromSecret(hashCtor, secret); var outer = new Uint8Array(hashCtor.BLOCK_SIZE); outer.set(inner); for (var i = 0; i < hashCtor.BLOCK_SIZE; i++) { inner[i] ^= 54; outer[i] ^= 92; } this.hash.update(inner); this.outer.update(outer); for (var i = 0; i < inner.byteLength; i++) { inner[i] = 0; } } module.exports = exports = Hmac; Hmac.prototype.update = function(toHash) { if (hashUtils.isEmptyData(toHash) || this.error) { return this; } try { this.hash.update(hashUtils.convertToBuffer(toHash)); } catch (e) { this.error = e; } return this; }; Hmac.prototype.digest = function(encoding) { if (!this.outer.finished) { this.outer.update(this.hash.digest()); } return this.outer.digest(encoding); }; function bufferFromSecret(hashCtor, secret) { var input = hashUtils.convertToBuffer(secret); if (input.byteLength > hashCtor.BLOCK_SIZE) { var bufferHash = new hashCtor(); bufferHash.update(input); input = bufferHash.digest(); } var buffer = new Uint8Array(hashCtor.BLOCK_SIZE); buffer.set(input); return buffer; } }, { ""./browserHashUtils"": 30 } ], 30: [ function(require, module, exports) { var Buffer = require(""buffer/"").Buffer; if (typeof ArrayBuffer !== ""undefined"" && typeof ArrayBuffer.isView === ""undefined"") { ArrayBuffer.isView = function(arg) { return viewStrings.indexOf(Object.prototype.toString.call(arg)) > -1; }; } var viewStrings = [ ""[object Int8Array]"", ""[object Uint8Array]"", ""[object Uint8ClampedArray]"", ""[object Int16Array]"", ""[object Uint16Array]"", ""[object Int32Array]"", ""[object Uint32Array]"", ""[object Float32Array]"", ""[object Float64Array]"", ""[object DataView]"" ]; function isEmptyData(data) { if (typeof data === ""string"") { return data.length === 0; } return data.byteLength === 0; } function convertToBuffer(data) { if (typeof data === ""string"") { data = new Buffer(data, ""utf8""); } if (ArrayBuffer.isView(data)) { return new Uint8Array(data.buffer, data.byteOffset, data.byteLength / Uint8Array.BYTES_PER_ELEMENT); } return new Uint8Array(data); } module.exports = exports = { isEmptyData: isEmptyData, convertToBuffer: convertToBuffer }; }, { ""buffer/"": 3 } ], 17: [ function(require, module, exports) { var punycode = require(""punycode""); exports.parse = urlParse; exports.resolve = urlResolve; exports.resolveObject = urlResolveObject; exports.format = urlFormat; exports.Url = Url; function Url() { this.protocol = null; this.slashes = null; this.auth = null; this.host = null; this.port = null; this.hostname = null; this.hash = null; this.search = null; this.query = null; this.pathname = null; this.path = null; this.href = null; } var protocolPattern = /^([a-z0-9.+-]+:)/i, portPattern = /:[0-9]*$/, delims = [ ""<"", "">"", '""', ""`"", "" "", ""\r"", ""\n"", ""\t"" ], unwise = [ ""{"", ""}"", ""|"", ""\\"", ""^"", ""`"" ].concat(delims), autoEscape = [ ""'"" ].concat(unwise), nonHostChars = [ ""%"", ""/"", ""?"", "";"", ""#"" ].concat(autoEscape), hostEndingChars = [ ""/"", ""?"", ""#"" ], hostnameMaxLen = 255, hostnamePartPattern = /^[a-z0-9A-Z_-]{0,63}$/, hostnamePartStart = /^([a-z0-9A-Z_-]{0,63})(.*)$/, unsafeProtocol = { javascript: true, ""javascript:"": true }, hostlessProtocol = { javascript: true, ""javascript:"": true }, slashedProtocol = { http: true, https: true, ftp: true, gopher: true, file: true, ""http:"": true, ""https:"": true, ""ftp:"": true, ""gopher:"": true, ""file:"": true }, querystring = require(""querystring""); function urlParse(url, parseQueryString, slashesDenoteHost) { if (url && isObject(url) && url instanceof Url) return url; var u = new Url(); u.parse(url, parseQueryString, slashesDenoteHost); return u; } Url.prototype.parse = function(url, parseQueryString, slashesDenoteHost) { if (!isString(url)) { throw new TypeError(""Parameter 'url' must be a string, not "" + typeof url); } var rest = url; rest = rest.trim(); var proto = protocolPattern.exec(rest); if (proto) { proto = proto[0]; var lowerProto = proto.toLowerCase(); this.protocol = lowerProto; rest = rest.substr(proto.length); } if (slashesDenoteHost || proto || rest.match(/^\/\/[^@\/]+@[^@\/]+/)) { var slashes = rest.substr(0, 2) === ""//""; if (slashes && !(proto && hostlessProtocol[proto])) { rest = rest.substr(2); this.slashes = true; } } if (!hostlessProtocol[proto] && (slashes || proto && !slashedProtocol[proto])) { var hostEnd = -1; for (var i = 0; i < hostEndingChars.length; i++) { var hec = rest.indexOf(hostEndingChars[i]); if (hec !== -1 && (hostEnd === -1 || hec < hostEnd)) hostEnd = hec; } var auth, atSign; if (hostEnd === -1) { atSign = rest.lastIndexOf(""@""); } else { atSign = rest.lastIndexOf(""@"", hostEnd); } if (atSign !== -1) { auth = rest.slice(0, atSign); rest = rest.slice(atSign + 1); this.auth = decodeURIComponent(auth); } hostEnd = -1; for (var i = 0; i < nonHostChars.length; i++) { var hec = rest.indexOf(nonHostChars[i]); if (hec !== -1 && (hostEnd === -1 || hec < hostEnd)) hostEnd = hec; } if (hostEnd === -1) hostEnd = rest.length; this.host = rest.slice(0, hostEnd); rest = rest.slice(hostEnd); this.parseHost(); this.hostname = this.hostname || """"; var ipv6Hostname = this.hostname[0] === ""["" && this.hostname[this.hostname.length - 1] === ""]""; if (!ipv6Hostname) { var hostparts = this.hostname.split(/\./); for (var i = 0, l = hostparts.length; i < l; i++) { var part = hostparts[i]; if (!part) continue; if (!part.match(hostnamePartPattern)) { var newpart = """"; for (var j = 0, k = part.length; j < k; j++) { if (part.charCodeAt(j) > 127) { newpart += ""x""; } else { newpart += part[j]; } } if (!newpart.match(hostnamePartPattern)) { var validParts = hostparts.slice(0, i); var notHost = hostparts.slice(i + 1); var bit = part.match(hostnamePartStart); if (bit) { validParts.push(bit[1]); notHost.unshift(bit[2]); } if (notHost.length) { rest = ""/"" + notHost.join(""."") + rest; } this.hostname = validParts.join("".""); break; } } } } if (this.hostname.length > hostnameMaxLen) { this.hostname = """"; } else { this.hostname = this.hostname.toLowerCase(); } if (!ipv6Hostname) { var domainArray = this.hostname.split("".""); var newOut = []; for (var i = 0; i < domainArray.length; ++i) { var s = domainArray[i]; newOut.push(s.match(/[^A-Za-z0-9_-]/) ? ""xn--"" + punycode.encode(s) : s); } this.hostname = newOut.join("".""); } var p = this.port ? "":"" + this.port : """"; var h = this.hostname || """"; this.host = h + p; this.href += this.host; if (ipv6Hostname) { this.hostname = this.hostname.substr(1, this.hostname.length - 2); if (rest[0] !== ""/"") { rest = ""/"" + rest; } } } if (!unsafeProtocol[lowerProto]) { for (var i = 0, l = autoEscape.length; i < l; i++) { var ae = autoEscape[i]; var esc = encodeURIComponent(ae); if (esc === ae) { esc = escape(ae); } rest = rest.split(ae).join(esc); } } var hash = rest.indexOf(""#""); if (hash !== -1) { this.hash = rest.substr(hash); rest = rest.slice(0, hash); } var qm = rest.indexOf(""?""); if (qm !== -1) { this.search = rest.substr(qm); this.query = rest.substr(qm + 1); if (parseQueryString) { this.query = querystring.parse(this.query); } rest = rest.slice(0, qm); } else if (parseQueryString) { this.search = """"; this.query = {}; } if (rest) this.pathname = rest; if (slashedProtocol[lowerProto] && this.hostname && !this.pathname) { this.pathname = ""/""; } if (this.pathname || this.search) { var p = this.pathname || """"; var s = this.search || """"; this.path = p + s; } this.href = this.format(); return this; }; function urlFormat(obj) { if (isString(obj)) obj = urlParse(obj); if (!(obj instanceof Url)) return Url.prototype.format.call(obj); return obj.format(); } Url.prototype.format = function() { var auth = this.auth || """"; if (auth) { auth = encodeURIComponent(auth); auth = auth.replace(/%3A/i, "":""); auth += ""@""; } var protocol = this.protocol || """", pathname = this.pathname || """", hash = this.hash || """", host = false, query = """"; if (this.host) { host = auth + this.host; } else if (this.hostname) { host = auth + (this.hostname.indexOf("":"") === -1 ? this.hostname : ""["" + this.hostname + ""]""); if (this.port) { host += "":"" + this.port; } } if (this.query && isObject(this.query) && Object.keys(this.query).length) { query = querystring.stringify(this.query); } var search = this.search || query && ""?"" + query || """"; if (protocol && protocol.substr(-1) !== "":"") protocol += "":""; if (this.slashes || (!protocol || slashedProtocol[protocol]) && host !== false) { host = ""//"" + (host || """"); if (pathname && pathname.charAt(0) !== ""/"") pathname = ""/"" + pathname; } else if (!host) { host = """"; } if (hash && hash.charAt(0) !== ""#"") hash = ""#"" + hash; if (search && search.charAt(0) !== ""?"") search = ""?"" + search; pathname = pathname.replace(/[?#]/g, function(match) { return encodeURIComponent(match); }); search = search.replace(""#"", ""%23""); return protocol + host + pathname + search + hash; }; function urlResolve(source, relative) { return urlParse(source, false, true).resolve(relative); } Url.prototype.resolve = function(relative) { return this.resolveObject(urlParse(relative, false, true)).format(); }; function urlResolveObject(source, relative) { if (!source) return relative; return urlParse(source, false, true).resolveObject(relative); } Url.prototype.resolveObject = function(relative) { if (isString(relative)) { var rel = new Url(); rel.parse(relative, false, true); relative = rel; } var result = new Url(); Object.keys(this).forEach(function(k) { result[k] = this[k]; }, this); result.hash = relative.hash; if (relative.href === """") { result.href = result.format(); return result; } if (relative.slashes && !relative.protocol) { Object.keys(relative).forEach(function(k) { if (k !== ""protocol"") result[k] = relative[k]; }); if (slashedProtocol[result.protocol] && result.hostname && !result.pathname) { result.path = result.pathname = ""/""; } result.href = result.format(); return result; } if (relative.protocol && relative.protocol !== result.protocol) { if (!slashedProtocol[relative.protocol]) { Object.keys(relative).forEach(function(k) { result[k] = relative[k]; }); result.href = result.format(); return result; } result.protocol = relative.protocol; if (!relative.host && !hostlessProtocol[relative.protocol]) { var relPath = (relative.pathname || """").split(""/""); while (relPath.length && !(relative.host = relPath.shift())) ; if (!relative.host) relative.host = """"; if (!relative.hostname) relative.hostname = """"; if (relPath[0] !== """") relPath.unshift(""""); if (relPath.length < 2) relPath.unshift(""""); result.pathname = relPath.join(""/""); } else { result.pathname = relative.pathname; } result.search = relative.search; result.query = relative.query; result.host = relative.host || """"; result.auth = relative.auth; result.hostname = relative.hostname || relative.host; result.port = relative.port; if (result.pathname || result.search) { var p = result.pathname || """"; var s = result.search || """"; result.path = p + s; } result.slashes = result.slashes || relative.slashes; result.href = result.format(); return result; } var isSourceAbs = result.pathname && result.pathname.charAt(0) === ""/"", isRelAbs = relative.host || relative.pathname && relative.pathname.charAt(0) === ""/"", mustEndAbs = isRelAbs || isSourceAbs || result.host && relative.pathname, removeAllDots = mustEndAbs, srcPath = result.pathname && result.pathname.split(""/"") || [], relPath = relative.pathname && relative.pathname.split(""/"") || [], psychotic = result.protocol && !slashedProtocol[result.protocol]; if (psychotic) { result.hostname = """"; result.port = null; if (result.host) { if (srcPath[0] === """") srcPath[0] = result.host; else srcPath.unshift(result.host); } result.host = """"; if (relative.protocol) { relative.hostname = null; relative.port = null; if (relative.host) { if (relPath[0] === """") relPath[0] = relative.host; else relPath.unshift(relative.host); } relative.host = null; } mustEndAbs = mustEndAbs && (relPath[0] === """" || srcPath[0] === """"); } if (isRelAbs) { result.host = relative.host || relative.host === """" ? relative.host : result.host; result.hostname = relative.hostname || relative.hostname === """" ? relative.hostname : result.hostname; result.search = relative.search; result.query = relative.query; srcPath = relPath; } else if (relPath.length) { if (!srcPath) srcPath = []; srcPath.pop(); srcPath = srcPath.concat(relPath); result.search = relative.search; result.query = relative.query; } else if (!isNullOrUndefined(relative.search)) { if (psychotic) { result.hostname = result.host = srcPath.shift(); var authInHost = result.host && result.host.indexOf(""@"") > 0 ? result.host.split(""@"") : false; if (authInHost) { result.auth = authInHost.shift(); result.host = result.hostname = authInHost.shift(); } } result.search = relative.search; result.query = relative.query; if (!isNull(result.pathname) || !isNull(result.search)) { result.path = (result.pathname ? result.pathname : """") + (result.search ? result.search : """"); } result.href = result.format(); return result; } if (!srcPath.length) { result.pathname = null; if (result.search) { result.path = ""/"" + result.search; } else { result.path = null; } result.href = result.format(); return result; } var last = srcPath.slice(-1)[0]; var hasTrailingSlash = (result.host || relative.host) && (last === ""."" || last === "".."") || last === """"; var up = 0; for (var i = srcPath.length; i >= 0; i--) { last = srcPath[i]; if (last == ""."") { srcPath.splice(i, 1); } else if (last === "".."") { srcPath.splice(i, 1); up++; } else if (up) { srcPath.splice(i, 1); up--; } } if (!mustEndAbs && !removeAllDots) { for (;up--; up) { srcPath.unshift(""..""); } } if (mustEndAbs && srcPath[0] !== """" && (!srcPath[0] || srcPath[0].charAt(0) !== ""/"")) { srcPath.unshift(""""); } if (hasTrailingSlash && srcPath.join(""/"").substr(-1) !== ""/"") { srcPath.push(""""); } var isAbsolute = srcPath[0] === """" || srcPath[0] && srcPath[0].charAt(0) === ""/""; if (psychotic) { result.hostname = result.host = isAbsolute ? """" : srcPath.length ? srcPath.shift() : """"; var authInHost = result.host && result.host.indexOf(""@"") > 0 ? result.host.split(""@"") : false; if (authInHost) { result.auth = authInHost.shift(); result.host = result.hostname = authInHost.shift(); } } mustEndAbs = mustEndAbs || result.host && srcPath.length; if (mustEndAbs && !isAbsolute) { srcPath.unshift(""""); } if (!srcPath.length) { result.pathname = null; result.path = null; } else { result.pathname = srcPath.join(""/""); } if (!isNull(result.pathname) || !isNull(result.search)) { result.path = (result.pathname ? result.pathname : """") + (result.search ? result.search : """"); } result.auth = relative.auth || result.auth; result.slashes = result.slashes || relative.slashes; result.href = result.format(); return result; }; Url.prototype.parseHost = function() { var host = this.host; var port = portPattern.exec(host); if (port) { port = port[0]; if (port !== "":"") { this.port = port.substr(1); } host = host.substr(0, host.length - port.length); } if (host) this.hostname = host; }; function isString(arg) { return typeof arg === ""string""; } function isObject(arg) { return typeof arg === ""object"" && arg !== null; } function isNull(arg) { return arg === null; } function isNullOrUndefined(arg) { return arg == null; } }, { punycode: 9, querystring: 12 } ], 15: [ function(require, module, exports) { arguments[4][12][0].apply(exports, arguments); }, { ""./decode"": 13, ""./encode"": 14, dup: 12 } ], 14: [ function(require, module, exports) { ""use strict""; var stringifyPrimitive = function(v) { switch (typeof v) { case ""string"": return v; case ""boolean"": return v ? ""true"" : ""false""; case ""number"": return isFinite(v) ? v : """"; default: return """"; } }; module.exports = function(obj, sep, eq, name) { sep = sep || ""&""; eq = eq || ""=""; if (obj === null) { obj = undefined; } if (typeof obj === ""object"") { return Object.keys(obj).map(function(k) { var ks = encodeURIComponent(stringifyPrimitive(k)) + eq; if (Array.isArray(obj[k])) { return obj[k].map(function(v) { return ks + encodeURIComponent(stringifyPrimitive(v)); }).join(sep); } else { return ks + encodeURIComponent(stringifyPrimitive(obj[k])); } }).join(sep); } if (!name) return """"; return encodeURIComponent(stringifyPrimitive(name)) + eq + encodeURIComponent(stringifyPrimitive(obj)); }; }, {} ], 13: [ function(require, module, exports) { ""use strict""; function hasOwnProperty(obj, prop) { return Object.prototype.hasOwnProperty.call(obj, prop); } module.exports = function(qs, sep, eq, options) { sep = sep || ""&""; eq = eq || ""=""; var obj = {}; if (typeof qs !== ""string"" || qs.length === 0) { return obj; } var regexp = /\+/g; qs = qs.split(sep); var maxKeys = 1e3; if (options && typeof options.maxKeys === ""number"") { maxKeys = options.maxKeys; } var len = qs.length; if (maxKeys > 0 && len > maxKeys) { len = maxKeys; } for (var i = 0; i < len; ++i) { var x = qs[i].replace(regexp, ""%20""), idx = x.indexOf(eq), kstr, vstr, k, v; if (idx >= 0) { kstr = x.substr(0, idx); vstr = x.substr(idx + 1); } else { kstr = x; vstr = """"; } k = decodeURIComponent(kstr); v = decodeURIComponent(vstr); if (!hasOwnProperty(obj, k)) { obj[k] = v; } else if (Array.isArray(obj[k])) { obj[k].push(v); } else { obj[k] = [ obj[k], v ]; } } return obj; }; }, {} ], 12: [ function(require, module, exports) { ""use strict""; exports.decode = exports.parse = require(""./decode""); exports.encode = exports.stringify = require(""./encode""); }, { ""./decode"": 10, ""./encode"": 11 } ], 11: [ function(require, module, exports) { ""use strict""; var stringifyPrimitive = function(v) { switch (typeof v) { case ""string"": return v; case ""boolean"": return v ? ""true"" : ""false""; case ""number"": return isFinite(v) ? v : """"; default: return """"; } }; module.exports = function(obj, sep, eq, name) { sep = sep || ""&""; eq = eq || ""=""; if (obj === null) { obj = undefined; } if (typeof obj === ""object"") { return map(objectKeys(obj), function(k) { var ks = encodeURIComponent(stringifyPrimitive(k)) + eq; if (isArray(obj[k])) { return map(obj[k], function(v) { return ks + encodeURIComponent(stringifyPrimitive(v)); }).join(sep); } else { return ks + encodeURIComponent(stringifyPrimitive(obj[k])); } }).join(sep); } if (!name) return """"; return encodeURIComponent(stringifyPrimitive(name)) + eq + encodeURIComponent(stringifyPrimitive(obj)); }; var isArray = Array.isArray || function(xs) { return Object.prototype.toString.call(xs) === ""[object Array]""; }; function map(xs, f) { if (xs.map) return xs.map(f); var res = []; for (var i = 0; i < xs.length; i++) { res.push(f(xs[i], i)); } return res; } var objectKeys = Object.keys || function(obj) { var res = []; for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) res.push(key); } return res; }; }, {} ], 10: [ function(require, module, exports) { ""use strict""; function hasOwnProperty(obj, prop) { return Object.prototype.hasOwnProperty.call(obj, prop); } module.exports = function(qs, sep, eq, options) { sep = sep || ""&""; eq = eq || ""=""; var obj = {}; if (typeof qs !== ""string"" || qs.length === 0) { return obj; } var regexp = /\+/g; qs = qs.split(sep); var maxKeys = 1e3; if (options && typeof options.maxKeys === ""number"") { maxKeys = options.maxKeys; } var len = qs.length; if (maxKeys > 0 && len > maxKeys) { len = maxKeys; } for (var i = 0; i < len; ++i) { var x = qs[i].replace(regexp, ""%20""), idx = x.indexOf(eq), kstr, vstr, k, v; if (idx >= 0) { kstr = x.substr(0, idx); vstr = x.substr(idx + 1); } else { kstr = x; vstr = """"; } k = decodeURIComponent(kstr); v = decodeURIComponent(vstr); if (!hasOwnProperty(obj, k)) { obj[k] = v; } else if (isArray(obj[k])) { obj[k].push(v); } else { obj[k] = [ obj[k], v ]; } } return obj; }; var isArray = Array.isArray || function(xs) { return Object.prototype.toString.call(xs) === ""[object Array]""; }; }, {} ], 9: [ function(require, module, exports) { (function(global) { (function(root) { var freeExports = typeof exports == ""object"" && exports && !exports.nodeType && exports; var freeModule = typeof module == ""object"" && module && !module.nodeType && module; var freeGlobal = typeof global == ""object"" && global; if (freeGlobal.global === freeGlobal || freeGlobal.window === freeGlobal || freeGlobal.self === freeGlobal) { root = freeGlobal; } var punycode, maxInt = 2147483647, base = 36, tMin = 1, tMax = 26, skew = 38, damp = 700, initialBias = 72, initialN = 128, delimiter = ""-"", regexPunycode = /^xn--/, regexNonASCII = /[^\x20-\x7E]/, regexSeparators = /[\x2E\u3002\uFF0E\uFF61]/g, errors = { overflow: ""Overflow: input needs wider integers to process"", ""not-basic"": ""Illegal input >= 0x80 (not a basic code point)"", ""invalid-input"": ""Invalid input"" }, baseMinusTMin = base - tMin, floor = Math.floor, stringFromCharCode = String.fromCharCode, key; function error(type) { throw RangeError(errors[type]); } function map(array, fn) { var length = array.length; var result = []; while (length--) { result[length] = fn(array[length]); } return result; } function mapDomain(string, fn) { var parts = string.split(""@""); var result = """"; if (parts.length > 1) { result = parts[0] + ""@""; string = parts[1]; } string = string.replace(regexSeparators, "".""); var labels = string.split("".""); var encoded = map(labels, fn).join("".""); return result + encoded; } function ucs2decode(string) { var output = [], counter = 0, length = string.length, value, extra; while (counter < length) { value = string.charCodeAt(counter++); if (value >= 55296 && value <= 56319 && counter < length) { extra = string.charCodeAt(counter++); if ((extra & 64512) == 56320) { output.push(((value & 1023) << 10) + (extra & 1023) + 65536); } else { output.push(value); counter--; } } else { output.push(value); } } return output; } function ucs2encode(array) { return map(array, function(value) { var output = """"; if (value > 65535) { value -= 65536; output += stringFromCharCode(value >>> 10 & 1023 | 55296); value = 56320 | value & 1023; } output += stringFromCharCode(value); return output; }).join(""""); } function basicToDigit(codePoint) { if (codePoint - 48 < 10) { return codePoint - 22; } if (codePoint - 65 < 26) { return codePoint - 65; } if (codePoint - 97 < 26) { return codePoint - 97; } return base; } function digitToBasic(digit, flag) { return digit + 22 + 75 * (digit < 26) - ((flag != 0) << 5); } function adapt(delta, numPoints, firstTime) { var k = 0; delta = firstTime ? floor(delta / damp) : delta >> 1; delta += floor(delta / numPoints); for (;delta > baseMinusTMin * tMax >> 1; k += base) { delta = floor(delta / baseMinusTMin); } return floor(k + (baseMinusTMin + 1) * delta / (delta + skew)); } function decode(input) { var output = [], inputLength = input.length, out, i = 0, n = initialN, bias = initialBias, basic, j, index, oldi, w, k, digit, t, baseMinusT; basic = input.lastIndexOf(delimiter); if (basic < 0) { basic = 0; } for (j = 0; j < basic; ++j) { if (input.charCodeAt(j) >= 128) { error(""not-basic""); } output.push(input.charCodeAt(j)); } for (index = basic > 0 ? basic + 1 : 0; index < inputLength; ) { for (oldi = i, w = 1, k = base; ;k += base) { if (index >= inputLength) { error(""invalid-input""); } digit = basicToDigit(input.charCodeAt(index++)); if (digit >= base || digit > floor((maxInt - i) / w)) { error(""overflow""); } i += digit * w; t = k <= bias ? tMin : k >= bias + tMax ? tMax : k - bias; if (digit < t) { break; } baseMinusT = base - t; if (w > floor(maxInt / baseMinusT)) { error(""overflow""); } w *= baseMinusT; } out = output.length + 1; bias = adapt(i - oldi, out, oldi == 0); if (floor(i / out) > maxInt - n) { error(""overflow""); } n += floor(i / out); i %= out; output.splice(i++, 0, n); } return ucs2encode(output); } function encode(input) { var n, delta, handledCPCount, basicLength, bias, j, m, q, k, t, currentValue, output = [], inputLength, handledCPCountPlusOne, baseMinusT, qMinusT; input = ucs2decode(input); inputLength = input.length; n = initialN; delta = 0; bias = initialBias; for (j = 0; j < inputLength; ++j) { currentValue = input[j]; if (currentValue < 128) { output.push(stringFromCharCode(currentValue)); } } handledCPCount = basicLength = output.length; if (basicLength) { output.push(delimiter); } while (handledCPCount < inputLength) { for (m = maxInt, j = 0; j < inputLength; ++j) { currentValue = input[j]; if (currentValue >= n && currentValue < m) { m = currentValue; } } handledCPCountPlusOne = handledCPCount + 1; if (m - n > floor((maxInt - delta) / handledCPCountPlusOne)) { error(""overflow""); } delta += (m - n) * handledCPCountPlusOne; n = m; for (j = 0; j < inputLength; ++j) { currentValue = input[j]; if (currentValue < n && ++delta > maxInt) { error(""overflow""); } if (currentValue == n) { for (q = delta, k = base; ;k += base) { t = k <= bias ? tMin : k >= bias + tMax ? tMax : k - bias; if (q < t) { break; } qMinusT = q - t; baseMinusT = base - t; output.push(stringFromCharCode(digitToBasic(t + qMinusT % baseMinusT, 0))); q = floor(qMinusT / baseMinusT); } output.push(stringFromCharCode(digitToBasic(q, 0))); bias = adapt(delta, handledCPCountPlusOne, handledCPCount == basicLength); delta = 0; ++handledCPCount; } } ++delta; ++n; } return output.join(""""); } function toUnicode(input) { return mapDomain(input, function(string) { return regexPunycode.test(string) ? decode(string.slice(4).toLowerCase()) : string; }); } function toASCII(input) { return mapDomain(input, function(string) { return regexNonASCII.test(string) ? ""xn--"" + encode(string) : string; }); } punycode = { version: ""1.3.2"", ucs2: { decode: ucs2decode, encode: ucs2encode }, decode: decode, encode: encode, toASCII: toASCII, toUnicode: toUnicode }; if (typeof define == ""function"" && typeof define.amd == ""object"" && define.amd) { define(""punycode"", function() { return punycode; }); } else if (freeExports && freeModule) { if (module.exports == freeExports) { freeModule.exports = punycode; } else { for (key in punycode) { punycode.hasOwnProperty(key) && (freeExports[key] = punycode[key]); } } } else { root.punycode = punycode; } })(this); }).call(this, typeof global !== ""undefined"" ? global : typeof self !== ""undefined"" ? self : typeof window !== ""undefined"" ? window : {}); }, {} ], 4: [ function(require, module, exports) { function EventEmitter() { this._events = this._events || {}; this._maxListeners = this._maxListeners || undefined; } module.exports = EventEmitter; EventEmitter.EventEmitter = EventEmitter; EventEmitter.prototype._events = undefined; EventEmitter.prototype._maxListeners = undefined; EventEmitter.defaultMaxListeners = 10; EventEmitter.prototype.setMaxListeners = function(n) { if (!isNumber(n) || n < 0 || isNaN(n)) throw TypeError(""n must be a positive number""); this._maxListeners = n; return this; }; EventEmitter.prototype.emit = function(type) { var er, handler, len, args, i, listeners; if (!this._events) this._events = {}; if (type === ""error"") { if (!this._events.error || isObject(this._events.error) && !this._events.error.length) { er = arguments[1]; if (er instanceof Error) { throw er; } else { var err = new Error('Uncaught, unspecified ""error"" event. (' + er + "")""); err.context = er; throw err; } } } handler = this._events[type]; if (isUndefined(handler)) return false; if (isFunction(handler)) { switch (arguments.length) { case 1: handler.call(this); break; case 2: handler.call(this, arguments[1]); break; case 3: handler.call(this, arguments[1], arguments[2]); break; default: args = Array.prototype.slice.call(arguments, 1); handler.apply(this, args); } } else if (isObject(handler)) { args = Array.prototype.slice.call(arguments, 1); listeners = handler.slice(); len = listeners.length; for (i = 0; i < len; i++) listeners[i].apply(this, args); } return true; }; EventEmitter.prototype.addListener = function(type, listener) { var m; if (!isFunction(listener)) throw TypeError(""listener must be a function""); if (!this._events) this._events = {}; if (this._events.newListener) this.emit(""newListener"", type, isFunction(listener.listener) ? listener.listener : listener); if (!this._events[type]) this._events[type] = listener; else if (isObject(this._events[type])) this._events[type].push(listener); else this._events[type] = [ this._events[type], listener ]; if (isObject(this._events[type]) && !this._events[type].warned) { if (!isUndefined(this._maxListeners)) { m = this._maxListeners; } else { m = EventEmitter.defaultMaxListeners; } if (m && m > 0 && this._events[type].length > m) { this._events[type].warned = true; console.error(""(node) warning: possible EventEmitter memory "" + ""leak detected. %d listeners added. "" + ""Use emitter.setMaxListeners() to increase limit."", this._events[type].length); if (typeof console.trace === ""function"") { console.trace(); } } } return this; }; EventEmitter.prototype.on = EventEmitter.prototype.addListener; EventEmitter.prototype.once = function(type, listener) { if (!isFunction(listener)) throw TypeError(""listener must be a function""); var fired = false; function g() { this.removeListener(type, g); if (!fired) { fired = true; listener.apply(this, arguments); } } g.listener = listener; this.on(type, g); return this; }; EventEmitter.prototype.removeListener = function(type, listener) { var list, position, length, i; if (!isFunction(listener)) throw TypeError(""listener must be a function""); if (!this._events || !this._events[type]) return this; list = this._events[type]; length = list.length; position = -1; if (list === listener || isFunction(list.listener) && list.listener === listener) { delete this._events[type]; if (this._events.removeListener) this.emit(""removeListener"", type, listener); } else if (isObject(list)) { for (i = length; i-- > 0; ) { if (list[i] === listener || list[i].listener && list[i].listener === listener) { position = i; break; } } if (position < 0) return this; if (list.length === 1) { list.length = 0; delete this._events[type]; } else { list.splice(position, 1); } if (this._events.removeListener) this.emit(""removeListener"", type, listener); } return this; }; EventEmitter.prototype.removeAllListeners = function(type) { var key, listeners; if (!this._events) return this; if (!this._events.removeListener) { if (arguments.length === 0) this._events = {}; else if (this._events[type]) delete this._events[type]; return this; } if (arguments.length === 0) { for (key in this._events) { if (key === ""removeListener"") continue; this.removeAllListeners(key); } this.removeAllListeners(""removeListener""); this._events = {}; return this; } listeners = this._events[type]; if (isFunction(listeners)) { this.removeListener(type, listeners); } else if (listeners) { while (listeners.length) this.removeListener(type, listeners[listeners.length - 1]); } delete this._events[type]; return this; }; EventEmitter.prototype.listeners = function(type) { var ret; if (!this._events || !this._events[type]) ret = []; else if (isFunction(this._events[type])) ret = [ this._events[type] ]; else ret = this._events[type].slice(); return ret; }; EventEmitter.prototype.listenerCount = function(type) { if (this._events) { var evlistener = this._events[type]; if (isFunction(evlistener)) return 1; else if (evlistener) return evlistener.length; } return 0; }; EventEmitter.listenerCount = function(emitter, type) { return emitter.listenerCount(type); }; function isFunction(arg) { return typeof arg === ""function""; } function isNumber(arg) { return typeof arg === ""number""; } function isObject(arg) { return typeof arg === ""object"" && arg !== null; } function isUndefined(arg) { return arg === void 0; } }, {} ], 3: [ function(require, module, exports) { (function(global, Buffer) { ""use strict""; var base64 = require(""base64-js""); var ieee754 = require(""ieee754""); var isArray = require(""isarray""); exports.Buffer = Buffer; exports.SlowBuffer = SlowBuffer; exports.INSPECT_MAX_BYTES = 50; Buffer.TYPED_ARRAY_SUPPORT = global.TYPED_ARRAY_SUPPORT !== undefined ? global.TYPED_ARRAY_SUPPORT : typedArraySupport(); exports.kMaxLength = kMaxLength(); function typedArraySupport() { try { var arr = new Uint8Array(1); arr.__proto__ = { __proto__: Uint8Array.prototype, foo: function() { return 42; } }; return arr.foo() === 42 && typeof arr.subarray === ""function"" && arr.subarray(1, 1).byteLength === 0; } catch (e) { return false; } } function kMaxLength() { return Buffer.TYPED_ARRAY_SUPPORT ? 2147483647 : 1073741823; } function createBuffer(that, length) { if (kMaxLength() < length) { throw new RangeError(""Invalid typed array length""); } if (Buffer.TYPED_ARRAY_SUPPORT) { that = new Uint8Array(length); that.__proto__ = Buffer.prototype; } else { if (that === null) { that = new Buffer(length); } that.length = length; } return that; } function Buffer(arg, encodingOrOffset, length) { if (!Buffer.TYPED_ARRAY_SUPPORT && !(this instanceof Buffer)) { return new Buffer(arg, encodingOrOffset, length); } if (typeof arg === ""number"") { if (typeof encodingOrOffset === ""string"") { throw new Error(""If encoding is specified then the first argument must be a string""); } return allocUnsafe(this, arg); } return from(this, arg, encodingOrOffset, length); } Buffer.poolSize = 8192; Buffer._augment = function(arr) { arr.__proto__ = Buffer.prototype; return arr; }; function from(that, value, encodingOrOffset, length) { if (typeof value === ""number"") { throw new TypeError('""value"" argument must not be a number'); } if (typeof ArrayBuffer !== ""undefined"" && value instanceof ArrayBuffer) { return fromArrayBuffer(that, value, encodingOrOffset, length); } if (typeof value === ""string"") { return fromString(that, value, encodingOrOffset); } return fromObject(that, value); } Buffer.from = function(value, encodingOrOffset, length) { return from(null, value, encodingOrOffset, length); }; if (Buffer.TYPED_ARRAY_SUPPORT) { Buffer.prototype.__proto__ = Uint8Array.prototype; Buffer.__proto__ = Uint8Array; if (typeof Symbol !== ""undefined"" && Symbol.species && Buffer[Symbol.species] === Buffer) { Object.defineProperty(Buffer, Symbol.species, { value: null, configurable: true }); } } function assertSize(size) { if (typeof size !== ""number"") { throw new TypeError('""size"" argument must be a number'); } else if (size < 0) { throw new RangeError('""size"" argument must not be negative'); } } function alloc(that, size, fill, encoding) { assertSize(size); if (size <= 0) { return createBuffer(that, size); } if (fill !== undefined) { return typeof encoding === ""string"" ? createBuffer(that, size).fill(fill, encoding) : createBuffer(that, size).fill(fill); } return createBuffer(that, size); } Buffer.alloc = function(size, fill, encoding) { return alloc(null, size, fill, encoding); }; function allocUnsafe(that, size) { assertSize(size); that = createBuffer(that, size < 0 ? 0 : checked(size) | 0); if (!Buffer.TYPED_ARRAY_SUPPORT) { for (var i = 0; i < size; ++i) { that[i] = 0; } } return that; } Buffer.allocUnsafe = function(size) { return allocUnsafe(null, size); }; Buffer.allocUnsafeSlow = function(size) { return allocUnsafe(null, size); }; function fromString(that, string, encoding) { if (typeof encoding !== ""string"" || encoding === """") { encoding = ""utf8""; } if (!Buffer.isEncoding(encoding)) { throw new TypeError('""encoding"" must be a valid string encoding'); } var length = byteLength(string, encoding) | 0; that = createBuffer(that, length); var actual = that.write(string, encoding); if (actual !== length) { that = that.slice(0, actual); } return that; } function fromArrayLike(that, array) { var length = array.length < 0 ? 0 : checked(array.length) | 0; that = createBuffer(that, length); for (var i = 0; i < length; i += 1) { that[i] = array[i] & 255; } return that; } function fromArrayBuffer(that, array, byteOffset, length) { array.byteLength; if (byteOffset < 0 || array.byteLength < byteOffset) { throw new RangeError(""'offset' is out of bounds""); } if (array.byteLength < byteOffset + (length || 0)) { throw new RangeError(""'length' is out of bounds""); } if (byteOffset === undefined && length === undefined) { array = new Uint8Array(array); } else if (length === undefined) { array = new Uint8Array(array, byteOffset); } else { array = new Uint8Array(array, byteOffset, length); } if (Buffer.TYPED_ARRAY_SUPPORT) { that = array; that.__proto__ = Buffer.prototype; } else { that = fromArrayLike(that, array); } return that; } function fromObject(that, obj) { if (Buffer.isBuffer(obj)) { var len = checked(obj.length) | 0; that = createBuffer(that, len); if (that.length === 0) { return that; } obj.copy(that, 0, 0, len); return that; } if (obj) { if (typeof ArrayBuffer !== ""undefined"" && obj.buffer instanceof ArrayBuffer || ""length"" in obj) { if (typeof obj.length !== ""number"" || isnan(obj.length)) { return createBuffer(that, 0); } return fromArrayLike(that, obj); } if (obj.type === ""Buffer"" && isArray(obj.data)) { return fromArrayLike(that, obj.data); } } throw new TypeError(""First argument must be a string, Buffer, ArrayBuffer, Array, or array-like object.""); } function checked(length) { if (length >= kMaxLength()) { throw new RangeError(""Attempt to allocate Buffer larger than maximum "" + ""size: 0x"" + kMaxLength().toString(16) + "" bytes""); } return length | 0; } function SlowBuffer(length) { if (+length != length) { length = 0; } return Buffer.alloc(+length); } Buffer.isBuffer = function isBuffer(b) { return !!(b != null && b._isBuffer); }; Buffer.compare = function compare(a, b) { if (!Buffer.isBuffer(a) || !Buffer.isBuffer(b)) { throw new TypeError(""Arguments must be Buffers""); } if (a === b) return 0; var x = a.length; var y = b.length; for (var i = 0, len = Math.min(x, y); i < len; ++i) { if (a[i] !== b[i]) { x = a[i]; y = b[i]; break; } } if (x < y) return -1; if (y < x) return 1; return 0; }; Buffer.isEncoding = function isEncoding(encoding) { switch (String(encoding).toLowerCase()) { case ""hex"": case ""utf8"": case ""utf-8"": case ""ascii"": case ""latin1"": case ""binary"": case ""base64"": case ""ucs2"": case ""ucs-2"": case ""utf16le"": case ""utf-16le"": return true; default: return false; } }; Buffer.concat = function concat(list, length) { if (!isArray(list)) { throw new TypeError('""list"" argument must be an Array of Buffers'); } if (list.length === 0) { return Buffer.alloc(0); } var i; if (length === undefined) { length = 0; for (i = 0; i < list.length; ++i) { length += list[i].length; } } var buffer = Buffer.allocUnsafe(length); var pos = 0; for (i = 0; i < list.length; ++i) { var buf = list[i]; if (!Buffer.isBuffer(buf)) { throw new TypeError('""list"" argument must be an Array of Buffers'); } buf.copy(buffer, pos); pos += buf.length; } return buffer; }; function byteLength(string, encoding) { if (Buffer.isBuffer(string)) { return string.length; } if (typeof ArrayBuffer !== ""undefined"" && typeof ArrayBuffer.isView === ""function"" && (ArrayBuffer.isView(string) || string instanceof ArrayBuffer)) { return string.byteLength; } if (typeof string !== ""string"") { string = """" + string; } var len = string.length; if (len === 0) return 0; var loweredCase = false; for (;;) { switch (encoding) { case ""ascii"": case ""latin1"": case ""binary"": return len; case ""utf8"": case ""utf-8"": case undefined: return utf8ToBytes(string).length; case ""ucs2"": case ""ucs-2"": case ""utf16le"": case ""utf-16le"": return len * 2; case ""hex"": return len >>> 1; case ""base64"": return base64ToBytes(string).length; default: if (loweredCase) return utf8ToBytes(string).length; encoding = ("""" + encoding).toLowerCase(); loweredCase = true; } } } Buffer.byteLength = byteLength; function slowToString(encoding, start, end) { var loweredCase = false; if (start === undefined || start < 0) { start = 0; } if (start > this.length) { return """"; } if (end === undefined || end > this.length) { end = this.length; } if (end <= 0) { return """"; } end >>>= 0; start >>>= 0; if (end <= start) { return """"; } if (!encoding) encoding = ""utf8""; while (true) { switch (encoding) { case ""hex"": return hexSlice(this, start, end); case ""utf8"": case ""utf-8"": return utf8Slice(this, start, end); case ""ascii"": return asciiSlice(this, start, end); case ""latin1"": case ""binary"": return latin1Slice(this, start, end); case ""base64"": return base64Slice(this, start, end); case ""ucs2"": case ""ucs-2"": case ""utf16le"": case ""utf-16le"": return utf16leSlice(this, start, end); default: if (loweredCase) throw new TypeError(""Unknown encoding: "" + encoding); encoding = (encoding + """").toLowerCase(); loweredCase = true; } } } Buffer.prototype._isBuffer = true; function swap(b, n, m) { var i = b[n]; b[n] = b[m]; b[m] = i; } Buffer.prototype.swap16 = function swap16() { var len = this.length; if (len % 2 !== 0) { throw new RangeError(""Buffer size must be a multiple of 16-bits""); } for (var i = 0; i < len; i += 2) { swap(this, i, i + 1); } return this; }; Buffer.prototype.swap32 = function swap32() { var len = this.length; if (len % 4 !== 0) { throw new RangeError(""Buffer size must be a multiple of 32-bits""); } for (var i = 0; i < len; i += 4) { swap(this, i, i + 3); swap(this, i + 1, i + 2); } return this; }; Buffer.prototype.swap64 = function swap64() { var len = this.length; if (len % 8 !== 0) { throw new RangeError(""Buffer size must be a multiple of 64-bits""); } for (var i = 0; i < len; i += 8) { swap(this, i, i + 7); swap(this, i + 1, i + 6); swap(this, i + 2, i + 5); swap(this, i + 3, i + 4); } return this; }; Buffer.prototype.toString = function toString() { var length = this.length | 0; if (length === 0) return """"; if (arguments.length === 0) return utf8Slice(this, 0, length); return slowToString.apply(this, arguments); }; Buffer.prototype.equals = function equals(b) { if (!Buffer.isBuffer(b)) throw new TypeError(""Argument must be a Buffer""); if (this === b) return true; return Buffer.compare(this, b) === 0; }; Buffer.prototype.inspect = function inspect() { var str = """"; var max = exports.INSPECT_MAX_BYTES; if (this.length > 0) { str = this.toString(""hex"", 0, max).match(/.{2}/g).join("" ""); if (this.length > max) str += "" ... ""; } return ""<Buffer "" + str + "">""; }; Buffer.prototype.compare = function compare(target, start, end, thisStart, thisEnd) { if (!Buffer.isBuffer(target)) { throw new TypeError(""Argument must be a Buffer""); } if (start === undefined) { start = 0; } if (end === undefined) { end = target ? target.length : 0; } if (thisStart === undefined) { thisStart = 0; } if (thisEnd === undefined) { thisEnd = this.length; } if (start < 0 || end > target.length || thisStart < 0 || thisEnd > this.length) { throw new RangeError(""out of range index""); } if (thisStart >= thisEnd && start >= end) { return 0; } if (thisStart >= thisEnd) { return -1; } if (start >= end) { return 1; } start >>>= 0; end >>>= 0; thisStart >>>= 0; thisEnd >>>= 0; if (this === target) return 0; var x = thisEnd - thisStart; var y = end - start; var len = Math.min(x, y); var thisCopy = this.slice(thisStart, thisEnd); var targetCopy = target.slice(start, end); for (var i = 0; i < len; ++i) { if (thisCopy[i] !== targetCopy[i]) { x = thisCopy[i]; y = targetCopy[i]; break; } } if (x < y) return -1; if (y < x) return 1; return 0; }; function bidirectionalIndexOf(buffer, val, byteOffset, encoding, dir) { if (buffer.length === 0) return -1; if (typeof byteOffset === ""string"") { encoding = byteOffset; byteOffset = 0; } else if (byteOffset > 2147483647) { byteOffset = 2147483647; } else if (byteOffset < -2147483648) { byteOffset = -2147483648; } byteOffset = +byteOffset; if (isNaN(byteOffset)) { byteOffset = dir ? 0 : buffer.length - 1; } if (byteOffset < 0) byteOffset = buffer.length + byteOffset; if (byteOffset >= buffer.length) { if (dir) return -1; else byteOffset = buffer.length - 1; } else if (byteOffset < 0) { if (dir) byteOffset = 0; else return -1; } if (typeof val === ""string"") { val = Buffer.from(val, encoding); } if (Buffer.isBuffer(val)) { if (val.length === 0) { return -1; } return arrayIndexOf(buffer, val, byteOffset, encoding, dir); } else if (typeof val === ""number"") { val = val & 255; if (Buffer.TYPED_ARRAY_SUPPORT && typeof Uint8Array.prototype.indexOf === ""function"") { if (dir) { return Uint8Array.prototype.indexOf.call(buffer, val, byteOffset); } else { return Uint8Array.prototype.lastIndexOf.call(buffer, val, byteOffset); } } return arrayIndexOf(buffer, [ val ], byteOffset, encoding, dir); } throw new TypeError(""val must be string, number or Buffer""); } function arrayIndexOf(arr, val, byteOffset, encoding, dir) { var indexSize = 1; var arrLength = arr.length; var valLength = val.length; if (encoding !== undefined) { encoding = String(encoding).toLowerCase(); if (encoding === ""ucs2"" || encoding === ""ucs-2"" || encoding === ""utf16le"" || encoding === ""utf-16le"") { if (arr.length < 2 || val.length < 2) { return -1; } indexSize = 2; arrLength /= 2; valLength /= 2; byteOffset /= 2; } } function read(buf, i) { if (indexSize === 1) { return buf[i]; } else { return buf.readUInt16BE(i * indexSize); } } var i; if (dir) { var foundIndex = -1; for (i = byteOffset; i < arrLength; i++) { if (read(arr, i) === read(val, foundIndex === -1 ? 0 : i - foundIndex)) { if (foundIndex === -1) foundIndex = i; if (i - foundIndex + 1 === valLength) return foundIndex * indexSize; } else { if (foundIndex !== -1) i -= i - foundIndex; foundIndex = -1; } } } else { if (byteOffset + valLength > arrLength) byteOffset = arrLength - valLength; for (i = byteOffset; i >= 0; i--) { var found = true; for (var j = 0; j < valLength; j++) { if (read(arr, i + j) !== read(val, j)) { found = false; break; } } if (found) return i; } } return -1; } Buffer.prototype.includes = function includes(val, byteOffset, encoding) { return this.indexOf(val, byteOffset, encoding) !== -1; }; Buffer.prototype.indexOf = function indexOf(val, byteOffset, encoding) { return bidirectionalIndexOf(this, val, byteOffset, encoding, true); }; Buffer.prototype.lastIndexOf = function lastIndexOf(val, byteOffset, encoding) { return bidirectionalIndexOf(this, val, byteOffset, encoding, false); }; function hexWrite(buf, string, offset, length) { offset = Number(offset) || 0; var remaining = buf.length - offset; if (!length) { length = remaining; } else { length = Number(length); if (length > remaining) { length = remaining; } } var strLen = string.length; if (strLen % 2 !== 0) throw new TypeError(""Invalid hex string""); if (length > strLen / 2) { length = strLen / 2; } for (var i = 0; i < length; ++i) { var parsed = parseInt(string.substr(i * 2, 2), 16); if (isNaN(parsed)) return i; buf[offset + i] = parsed; } return i; } function utf8Write(buf, string, offset, length) { return blitBuffer(utf8ToBytes(string, buf.length - offset), buf, offset, length); } function asciiWrite(buf, string, offset, length) { return blitBuffer(asciiToBytes(string), buf, offset, length); } function latin1Write(buf, string, offset, length) { return asciiWrite(buf, string, offset, length); } function base64Write(buf, string, offset, length) { return blitBuffer(base64ToBytes(string), buf, offset, length); } function ucs2Write(buf, string, offset, length) { return blitBuffer(utf16leToBytes(string, buf.length - offset), buf, offset, length); } Buffer.prototype.write = function write(string, offset, length, encoding) { if (offset === undefined) { encoding = ""utf8""; length = this.length; offset = 0; } else if (length === undefined && typeof offset === ""string"") { encoding = offset; length = this.length; offset = 0; } else if (isFinite(offset)) { offset = offset | 0; if (isFinite(length)) { length = length | 0; if (encoding === undefined) encoding = ""utf8""; } else { encoding = length; length = undefined; } } else { throw new Error(""Buffer.write(string, encoding, offset[, length]) is no longer supported""); } var remaining = this.length - offset; if (length === undefined || length > remaining) length = remaining; if (string.length > 0 && (length < 0 || offset < 0) || offset > this.length) { throw new RangeError(""Attempt to write outside buffer bounds""); } if (!encoding) encoding = ""utf8""; var loweredCase = false; for (;;) { switch (encoding) { case ""hex"": return hexWrite(this, string, offset, length); case ""utf8"": case ""utf-8"": return utf8Write(this, string, offset, length); case ""ascii"": return asciiWrite(this, string, offset, length); case ""latin1"": case ""binary"": return latin1Write(this, string, offset, length); case ""base64"": return base64Write(this, string, offset, length); case ""ucs2"": case ""ucs-2"": case ""utf16le"": case ""utf-16le"": return ucs2Write(this, string, offset, length); default: if (loweredCase) throw new TypeError(""Unknown encoding: "" + encoding); encoding = ("""" + encoding).toLowerCase(); loweredCase = true; } } }; Buffer.prototype.toJSON = function toJSON() { return { type: ""Buffer"", data: Array.prototype.slice.call(this._arr || this, 0) }; }; function base64Slice(buf, start, end) { if (start === 0 && end === buf.length) { return base64.fromByteArray(buf); } else { return base64.fromByteArray(buf.slice(start, end)); } } function utf8Slice(buf, start, end) { end = Math.min(buf.length, end); var res = []; var i = start; while (i < end) { var firstByte = buf[i]; var codePoint = null; var bytesPerSequence = firstByte > 239 ? 4 : firstByte > 223 ? 3 : firstByte > 191 ? 2 : 1; if (i + bytesPerSequence <= end) { var secondByte, thirdByte, fourthByte, tempCodePoint; switch (bytesPerSequence) { case 1: if (firstByte < 128) { codePoint = firstByte; } break; case 2: secondByte = buf[i + 1]; if ((secondByte & 192) === 128) { tempCodePoint = (firstByte & 31) << 6 | secondByte & 63; if (tempCodePoint > 127) { codePoint = tempCodePoint; } } break; case 3: secondByte = buf[i + 1]; thirdByte = buf[i + 2]; if ((secondByte & 192) === 128 && (thirdByte & 192) === 128) { tempCodePoint = (firstByte & 15) << 12 | (secondByte & 63) << 6 | thirdByte & 63; if (tempCodePoint > 2047 && (tempCodePoint < 55296 || tempCodePoint > 57343)) { codePoint = tempCodePoint; } } break; case 4: secondByte = buf[i + 1]; thirdByte = buf[i + 2]; fourthByte = buf[i + 3]; if ((secondByte & 192) === 128 && (thirdByte & 192) === 128 && (fourthByte & 192) === 128) { tempCodePoint = (firstByte & 15) << 18 | (secondByte & 63) << 12 | (thirdByte & 63) << 6 | fourthByte & 63; if (tempCodePoint > 65535 && tempCodePoint < 1114112) { codePoint = tempCodePoint; } } } } if (codePoint === null) { codePoint = 65533; bytesPerSequence = 1; } else if (codePoint > 65535) { codePoint -= 65536; res.push(codePoint >>> 10 & 1023 | 55296); codePoint = 56320 | codePoint & 1023; } res.push(codePoint); i += bytesPerSequence; } return decodeCodePointsArray(res); } var MAX_ARGUMENTS_LENGTH = 4096; function decodeCodePointsArray(codePoints) { var len = codePoints.length; if (len <= MAX_ARGUMENTS_LENGTH) { return String.fromCharCode.apply(String, codePoints); } var res = """"; var i = 0; while (i < len) { res += String.fromCharCode.apply(String, codePoints.slice(i, i += MAX_ARGUMENTS_LENGTH)); } return res; } function asciiSlice(buf, start, end) { var ret = """"; end = Math.min(buf.length, end); for (var i = start; i < end; ++i) { ret += String.fromCharCode(buf[i] & 127); } return ret; } function latin1Slice(buf, start, end) { var ret = """"; end = Math.min(buf.length, end); for (var i = start; i < end; ++i) { ret += String.fromCharCode(buf[i]); } return ret; } function hexSlice(buf, start, end) { var len = buf.length; if (!start || start < 0) start = 0; if (!end || end < 0 || end > len) end = len; var out = """"; for (var i = start; i < end; ++i) { out += toHex(buf[i]); } return out; } function utf16leSlice(buf, start, end) { var bytes = buf.slice(start, end); var res = """"; for (var i = 0; i < bytes.length; i += 2) { res += String.fromCharCode(bytes[i] + bytes[i + 1] * 256); } return res; } Buffer.prototype.slice = function slice(start, end) { var len = this.length; start = ~~start; end = end === undefined ? len : ~~end; if (start < 0) { start += len; if (start < 0) start = 0; } else if (start > len) { start = len; } if (end < 0) { end += len; if (end < 0) end = 0; } else if (end > len) { end = len; } if (end < start) end = start; var newBuf; if (Buffer.TYPED_ARRAY_SUPPORT) { newBuf = this.subarray(start, end); newBuf.__proto__ = Buffer.prototype; } else { var sliceLen = end - start; newBuf = new Buffer(sliceLen, undefined); for (var i = 0; i < sliceLen; ++i) { newBuf[i] = this[i + start]; } } return newBuf; }; function checkOffset(offset, ext, length) { if (offset % 1 !== 0 || offset < 0) throw new RangeError(""offset is not uint""); if (offset + ext > length) throw new RangeError(""Trying to access beyond buffer length""); } Buffer.prototype.readUIntLE = function readUIntLE(offset, byteLength, noAssert) { offset = offset | 0; byteLength = byteLength | 0; if (!noAssert) checkOffset(offset, byteLength, this.length); var val = this[offset]; var mul = 1; var i = 0; while (++i < byteLength && (mul *= 256)) { val += this[offset + i] * mul; } return val; }; Buffer.prototype.readUIntBE = function readUIntBE(offset, byteLength, noAssert) { offset = offset | 0; byteLength = byteLength | 0; if (!noAssert) { checkOffset(offset, byteLength, this.length); } var val = this[offset + --byteLength]; var mul = 1; while (byteLength > 0 && (mul *= 256)) { val += this[offset + --byteLength] * mul; } return val; }; Buffer.prototype.readUInt8 = function readUInt8(offset, noAssert) { if (!noAssert) checkOffset(offset, 1, this.length); return this[offset]; }; Buffer.prototype.readUInt16LE = function readUInt16LE(offset, noAssert) { if (!noAssert) checkOffset(offset, 2, this.length); return this[offset] | this[offset + 1] << 8; }; Buffer.prototype.readUInt16BE = function readUInt16BE(offset, noAssert) { if (!noAssert) checkOffset(offset, 2, this.length); return this[offset] << 8 | this[offset + 1]; }; Buffer.prototype.readUInt32LE = function readUInt32LE(offset, noAssert) { if (!noAssert) checkOffset(offset, 4, this.length); return (this[offset] | this[offset + 1] << 8 | this[offset + 2] << 16) + this[offset + 3] * 16777216; }; Buffer.prototype.readUInt32BE = function readUInt32BE(offset, noAssert) { if (!noAssert) checkOffset(offset, 4, this.length); return this[offset] * 16777216 + (this[offset + 1] << 16 | this[offset + 2] << 8 | this[offset + 3]); }; Buffer.prototype.readIntLE = function readIntLE(offset, byteLength, noAssert) { offset = offset | 0; byteLength = byteLength | 0; if (!noAssert) checkOffset(offset, byteLength, this.length); var val = this[offset]; var mul = 1; var i = 0; while (++i < byteLength && (mul *= 256)) { val += this[offset + i] * mul; } mul *= 128; if (val >= mul) val -= Math.pow(2, 8 * byteLength); return val; }; Buffer.prototype.readIntBE = function readIntBE(offset, byteLength, noAssert) { offset = offset | 0; byteLength = byteLength | 0; if (!noAssert) checkOffset(offset, byteLength, this.length); var i = byteLength; var mul = 1; var val = this[offset + --i]; while (i > 0 && (mul *= 256)) { val += this[offset + --i] * mul; } mul *= 128; if (val >= mul) val -= Math.pow(2, 8 * byteLength); return val; }; Buffer.prototype.readInt8 = function readInt8(offset, noAssert) { if (!noAssert) checkOffset(offset, 1, this.length); if (!(this[offset] & 128)) return this[offset]; return (255 - this[offset] + 1) * -1; }; Buffer.prototype.readInt16LE = function readInt16LE(offset, noAssert) { if (!noAssert) checkOffset(offset, 2, this.length); var val = this[offset] | this[offset + 1] << 8; return val & 32768 ? val | 4294901760 : val; }; Buffer.prototype.readInt16BE = function readInt16BE(offset, noAssert) { if (!noAssert) checkOffset(offset, 2, this.length); var val = this[offset + 1] | this[offset] << 8; return val & 32768 ? val | 4294901760 : val; }; Buffer.prototype.readInt32LE = function readInt32LE(offset, noAssert) { if (!noAssert) checkOffset(offset, 4, this.length); return this[offset] | this[offset + 1] << 8 | this[offset + 2] << 16 | this[offset + 3] << 24; }; Buffer.prototype.readInt32BE = function readInt32BE(offset, noAssert) { if (!noAssert) checkOffset(offset, 4, this.length); return this[offset] << 24 | this[offset + 1] << 16 | this[offset + 2] << 8 | this[offset + 3]; }; Buffer.prototype.readFloatLE = function readFloatLE(offset, noAssert) { if (!noAssert) checkOffset(offset, 4, this.length); return ieee754.read(this, offset, true, 23, 4); }; Buffer.prototype.readFloatBE = function readFloatBE(offset, noAssert) { if (!noAssert) checkOffset(offset, 4, this.length); return ieee754.read(this, offset, false, 23, 4); }; Buffer.prototype.readDoubleLE = function readDoubleLE(offset, noAssert) { if (!noAssert) checkOffset(offset, 8, this.length); return ieee754.read(this, offset, true, 52, 8); }; Buffer.prototype.readDoubleBE = function readDoubleBE(offset, noAssert) { if (!noAssert) checkOffset(offset, 8, this.length); return ieee754.read(this, offset, false, 52, 8); }; function checkInt(buf, value, offset, ext, max, min) { if (!Buffer.isBuffer(buf)) throw new TypeError('""buffer"" argument must be a Buffer instance'); if (value > max || value < min) throw new RangeError('""value"" argument is out of bounds'); if (offset + ext > buf.length) throw new RangeError(""Index out of range""); } Buffer.prototype.writeUIntLE = function writeUIntLE(value, offset, byteLength, noAssert) { value = +value; offset = offset | 0; byteLength = byteLength | 0; if (!noAssert) { var maxBytes = Math.pow(2, 8 * byteLength) - 1; checkInt(this, value, offset, byteLength, maxBytes, 0); } var mul = 1; var i = 0; this[offset] = value & 255; while (++i < byteLength && (mul *= 256)) { this[offset + i] = value / mul & 255; } return offset + byteLength; }; Buffer.prototype.writeUIntBE = function writeUIntBE(value, offset, byteLength, noAssert) { value = +value; offset = offset | 0; byteLength = byteLength | 0; if (!noAssert) { var maxBytes = Math.pow(2, 8 * byteLength) - 1; checkInt(this, value, offset, byteLength, maxBytes, 0); } var i = byteLength - 1; var mul = 1; this[offset + i] = value & 255; while (--i >= 0 && (mul *= 256)) { this[offset + i] = value / mul & 255; } return offset + byteLength; }; Buffer.prototype.writeUInt8 = function writeUInt8(value, offset, noAssert) { value = +value; offset = offset | 0; if (!noAssert) checkInt(this, value, offset, 1, 255, 0); if (!Buffer.TYPED_ARRAY_SUPPORT) value = Math.floor(value); this[offset] = value & 255; return offset + 1; }; function objectWriteUInt16(buf, value, offset, littleEndian) { if (value < 0) value = 65535 + value + 1; for (var i = 0, j = Math.min(buf.length - offset, 2); i < j; ++i) { buf[offset + i] = (value & 255 << 8 * (littleEndian ? i : 1 - i)) >>> (littleEndian ? i : 1 - i) * 8; } } Buffer.prototype.writeUInt16LE = function writeUInt16LE(value, offset, noAssert) { value = +value; offset = offset | 0; if (!noAssert) checkInt(this, value, offset, 2, 65535, 0); if (Buffer.TYPED_ARRAY_SUPPORT) { this[offset] = value & 255; this[offset + 1] = value >>> 8; } else { objectWriteUInt16(this, value, offset, true); } return offset + 2; }; Buffer.prototype.writeUInt16BE = function writeUInt16BE(value, offset, noAssert) { value = +value; offset = offset | 0; if (!noAssert) checkInt(this, value, offset, 2, 65535, 0); if (Buffer.TYPED_ARRAY_SUPPORT) { this[offset] = value >>> 8; this[offset + 1] = value & 255; } else { objectWriteUInt16(this, value, offset, false); } return offset + 2; }; function objectWriteUInt32(buf, value, offset, littleEndian) { if (value < 0) value = 4294967295 + value + 1; for (var i = 0, j = Math.min(buf.length - offset, 4); i < j; ++i) { buf[offset + i] = value >>> (littleEndian ? i : 3 - i) * 8 & 255; } } Buffer.prototype.writeUInt32LE = function writeUInt32LE(value, offset, noAssert) { value = +value; offset = offset | 0; if (!noAssert) checkInt(this, value, offset, 4, 4294967295, 0); if (Buffer.TYPED_ARRAY_SUPPORT) { this[offset + 3] = value >>> 24; this[offset + 2] = value >>> 16; this[offset + 1] = value >>> 8; this[offset] = value & 255; } else { objectWriteUInt32(this, value, offset, true); } return offset + 4; }; Buffer.prototype.writeUInt32BE = function writeUInt32BE(value, offset, noAssert) { value = +value; offset = offset | 0; if (!noAssert) checkInt(this, value, offset, 4, 4294967295, 0); if (Buffer.TYPED_ARRAY_SUPPORT) { this[offset] = value >>> 24; this[offset + 1] = value >>> 16; this[offset + 2] = value >>> 8; this[offset + 3] = value & 255; } else { objectWriteUInt32(this, value, offset, false); } return offset + 4; }; Buffer.prototype.writeIntLE = function writeIntLE(value, offset, byteLength, noAssert) { value = +value; offset = offset | 0; if (!noAssert) { var limit = Math.pow(2, 8 * byteLength - 1); checkInt(this, value, offset, byteLength, limit - 1, -limit); } var i = 0; var mul = 1; var sub = 0; this[offset] = value & 255; while (++i < byteLength && (mul *= 256)) { if (value < 0 && sub === 0 && this[offset + i - 1] !== 0) { sub = 1; } this[offset + i] = (value / mul >> 0) - sub & 255; } return offset + byteLength; }; Buffer.prototype.writeIntBE = function writeIntBE(value, offset, byteLength, noAssert) { value = +value; offset = offset | 0; if (!noAssert) { var limit = Math.pow(2, 8 * byteLength - 1); checkInt(this, value, offset, byteLength, limit - 1, -limit); } var i = byteLength - 1; var mul = 1; var sub = 0; this[offset + i] = value & 255; while (--i >= 0 && (mul *= 256)) { if (value < 0 && sub === 0 && this[offset + i + 1] !== 0) { sub = 1; } this[offset + i] = (value / mul >> 0) - sub & 255; } return offset + byteLength; }; Buffer.prototype.writeInt8 = function writeInt8(value, offset, noAssert) { value = +value; offset = offset | 0; if (!noAssert) checkInt(this, value, offset, 1, 127, -128); if (!Buffer.TYPED_ARRAY_SUPPORT) value = Math.floor(value); if (value < 0) value = 255 + value + 1; this[offset] = value & 255; return offset + 1; }; Buffer.prototype.writeInt16LE = function writeInt16LE(value, offset, noAssert) { value = +value; offset = offset | 0; if (!noAssert) checkInt(this, value, offset, 2, 32767, -32768); if (Buffer.TYPED_ARRAY_SUPPORT) { this[offset] = value & 255; this[offset + 1] = value >>> 8; } else { objectWriteUInt16(this, value, offset, true); } return offset + 2; }; Buffer.prototype.writeInt16BE = function writeInt16BE(value, offset, noAssert) { value = +value; offset = offset | 0; if (!noAssert) checkInt(this, value, offset, 2, 32767, -32768); if (Buffer.TYPED_ARRAY_SUPPORT) { this[offset] = value >>> 8; this[offset + 1] = value & 255; } else { objectWriteUInt16(this, value, offset, false); } return offset + 2; }; Buffer.prototype.writeInt32LE = function writeInt32LE(value, offset, noAssert) { value = +value; offset = offset | 0; if (!noAssert) checkInt(this, value, offset, 4, 2147483647, -2147483648); if (Buffer.TYPED_ARRAY_SUPPORT) { this[offset] = value & 255; this[offset + 1] = value >>> 8; this[offset + 2] = value >>> 16; this[offset + 3] = value >>> 24; } else { objectWriteUInt32(this, value, offset, true); } return offset + 4; }; Buffer.prototype.writeInt32BE = function writeInt32BE(value, offset, noAssert) { value = +value; offset = offset | 0; if (!noAssert) checkInt(this, value, offset, 4, 2147483647, -2147483648); if (value < 0) value = 4294967295 + value + 1; if (Buffer.TYPED_ARRAY_SUPPORT) { this[offset] = value >>> 24; this[offset + 1] = value >>> 16; this[offset + 2] = value >>> 8; this[offset + 3] = value & 255; } else { objectWriteUInt32(this, value, offset, false); } return offset + 4; }; function checkIEEE754(buf, value, offset, ext, max, min) { if (offset + ext > buf.length) throw new RangeError(""Index out of range""); if (offset < 0) throw new RangeError(""Index out of range""); } function writeFloat(buf, value, offset, littleEndian, noAssert) { if (!noAssert) { checkIEEE754(buf, value, offset, 4, 3.4028234663852886e38, -3.4028234663852886e38); } ieee754.write(buf, value, offset, littleEndian, 23, 4); return offset + 4; } Buffer.prototype.writeFloatLE = function writeFloatLE(value, offset, noAssert) { return writeFloat(this, value, offset, true, noAssert); }; Buffer.prototype.writeFloatBE = function writeFloatBE(value, offset, noAssert) { return writeFloat(this, value, offset, false, noAssert); }; function writeDouble(buf, value, offset, littleEndian, noAssert) { if (!noAssert) { checkIEEE754(buf, value, offset, 8, 1.7976931348623157e308, -1.7976931348623157e308); } ieee754.write(buf, value, offset, littleEndian, 52, 8); return offset + 8; } Buffer.prototype.writeDoubleLE = function writeDoubleLE(value, offset, noAssert) { return writeDouble(this, value, offset, true, noAssert); }; Buffer.prototype.writeDoubleBE = function writeDoubleBE(value, offset, noAssert) { return writeDouble(this, value, offset, false, noAssert); }; Buffer.prototype.copy = function copy(target, targetStart, start, end) { if (!start) start = 0; if (!end && end !== 0) end = this.length; if (targetStart >= target.length) targetStart = target.length; if (!targetStart) targetStart = 0; if (end > 0 && end < start) end = start; if (end === start) return 0; if (target.length === 0 || this.length === 0) return 0; if (targetStart < 0) { throw new RangeError(""targetStart out of bounds""); } if (start < 0 || start >= this.length) throw new RangeError(""sourceStart out of bounds""); if (end < 0) throw new RangeError(""sourceEnd out of bounds""); if (end > this.length) end = this.length; if (target.length - targetStart < end - start) { end = target.length - targetStart + start; } var len = end - start; var i; if (this === target && start < targetStart && targetStart < end) { for (i = len - 1; i >= 0; --i) { target[i + targetStart] = this[i + start]; } } else if (len < 1e3 || !Buffer.TYPED_ARRAY_SUPPORT) { for (i = 0; i < len; ++i) { target[i + targetStart] = this[i + start]; } } else { Uint8Array.prototype.set.call(target, this.subarray(start, start + len), targetStart); } return len; }; Buffer.prototype.fill = function fill(val, start, end, encoding) { if (typeof val === ""string"") { if (typeof start === ""string"") { encoding = start; start = 0; end = this.length; } else if (typeof end === ""string"") { encoding = end; end = this.length; } if (val.length === 1) { var code = val.charCodeAt(0); if (code < 256) { val = code; } } if (encoding !== undefined && typeof encoding !== ""string"") { throw new TypeError(""encoding must be a string""); } if (typeof encoding === ""string"" && !Buffer.isEncoding(encoding)) { throw new TypeError(""Unknown encoding: "" + encoding); } } else if (typeof val === ""number"") { val = val & 255; } if (start < 0 || this.length < start || this.length < end) { throw new RangeError(""Out of range index""); } if (end <= start) { return this; } start = start >>> 0; end = end === undefined ? this.length : end >>> 0; if (!val) val = 0; var i; if (typeof val === ""number"") { for (i = start; i < end; ++i) { this[i] = val; } } else { var bytes = Buffer.isBuffer(val) ? val : utf8ToBytes(new Buffer(val, encoding).toString()); var len = bytes.length; for (i = 0; i < end - start; ++i) { this[i + start] = bytes[i % len]; } } return this; }; var INVALID_BASE64_RE = /[^+\/0-9A-Za-z-_]/g; function base64clean(str) { str = stringtrim(str).replace(INVALID_BASE64_RE, """"); if (str.length < 2) return """"; while (str.length % 4 !== 0) { str = str + ""=""; } return str; } function stringtrim(str) { if (str.trim) return str.trim(); return str.replace(/^\s+|\s+$/g, """"); } function toHex(n) { if (n < 16) return ""0"" + n.toString(16); return n.toString(16); } function utf8ToBytes(string, units) { units = units || Infinity; var codePoint; var length = string.length; var leadSurrogate = null; var bytes = []; for (var i = 0; i < length; ++i) { codePoint = string.charCodeAt(i); if (codePoint > 55295 && codePoint < 57344) { if (!leadSurrogate) { if (codePoint > 56319) { if ((units -= 3) > -1) bytes.push(239, 191, 189); continue; } else if (i + 1 === length) { if ((units -= 3) > -1) bytes.push(239, 191, 189); continue; } leadSurrogate = codePoint; continue; } if (codePoint < 56320) { if ((units -= 3) > -1) bytes.push(239, 191, 189); leadSurrogate = codePoint; continue; } codePoint = (leadSurrogate - 55296 << 10 | codePoint - 56320) + 65536; } else if (leadSurrogate) { if ((units -= 3) > -1) bytes.push(239, 191, 189); } leadSurrogate = null; if (codePoint < 128) { if ((units -= 1) < 0) break; bytes.push(codePoint); } else if (codePoint < 2048) { if ((units -= 2) < 0) break; bytes.push(codePoint >> 6 | 192, codePoint & 63 | 128); } else if (codePoint < 65536) { if ((units -= 3) < 0) break; bytes.push(codePoint >> 12 | 224, codePoint >> 6 & 63 | 128, codePoint & 63 | 128); } else if (codePoint < 1114112) { if ((units -= 4) < 0) break; bytes.push(codePoint >> 18 | 240, codePoint >> 12 & 63 | 128, codePoint >> 6 & 63 | 128, codePoint & 63 | 128); } else { throw new Error(""Invalid code point""); } } return bytes; } function asciiToBytes(str) { var byteArray = []; for (var i = 0; i < str.length; ++i) { byteArray.push(str.charCodeAt(i) & 255); } return byteArray; } function utf16leToBytes(str, units) { var c, hi, lo; var byteArray = []; for (var i = 0; i < str.length; ++i) { if ((units -= 2) < 0) break; c = str.charCodeAt(i); hi = c >> 8; lo = c % 256; byteArray.push(lo); byteArray.push(hi); } return byteArray; } function base64ToBytes(str) { return base64.toByteArray(base64clean(str)); } function blitBuffer(src, dst, offset, length) { for (var i = 0; i < length; ++i) { if (i + offset >= dst.length || i >= src.length) break; dst[i + offset] = src[i]; } return i; } function isnan(val) { return val !== val; } }).call(this, typeof global !== ""undefined"" ? global : typeof self !== ""undefined"" ? self : typeof window !== ""undefined"" ? window : {}, require(""buffer"").Buffer); }, { ""base64-js"": 1, buffer: 3, ieee754: 5, isarray: 6 } ], 6: [ function(require, module, exports) { var toString = {}.toString; module.exports = Array.isArray || function(arr) { return toString.call(arr) == ""[object Array]""; }; }, {} ], 5: [ function(require, module, exports) { exports.read = function(buffer, offset, isLE, mLen, nBytes) { var e, m; var eLen = nBytes * 8 - mLen - 1; var eMax = (1 << eLen) - 1; var eBias = eMax >> 1; var nBits = -7; var i = isLE ? nBytes - 1 : 0; var d = isLE ? -1 : 1; var s = buffer[offset + i]; i += d; e = s & (1 << -nBits) - 1; s >>= -nBits; nBits += eLen; for (;nBits > 0; e = e * 256 + buffer[offset + i], i += d, nBits -= 8) {} m = e & (1 << -nBits) - 1; e >>= -nBits; nBits += mLen; for (;nBits > 0; m = m * 256 + buffer[offset + i], i += d, nBits -= 8) {} if (e === 0) { e = 1 - eBias; } else if (e === eMax) { return m ? NaN : (s ? -1 : 1) * Infinity; } else { m = m + Math.pow(2, mLen); e = e - eBias; } return (s ? -1 : 1) * m * Math.pow(2, e - mLen); }; exports.write = function(buffer, value, offset, isLE, mLen, nBytes) { var e, m, c; var eLen = nBytes * 8 - mLen - 1; var eMax = (1 << eLen) - 1; var eBias = eMax >> 1; var rt = mLen === 23 ? Math.pow(2, -24) - Math.pow(2, -77) : 0; var i = isLE ? 0 : nBytes - 1; var d = isLE ? 1 : -1; var s = value < 0 || value === 0 && 1 / value < 0 ? 1 : 0; value = Math.abs(value); if (isNaN(value) || value === Infinity) { m = isNaN(value) ? 1 : 0; e = eMax; } else { e = Math.floor(Math.log(value) / Math.LN2); if (value * (c = Math.pow(2, -e)) < 1) { e--; c *= 2; } if (e + eBias >= 1) { value += rt / c; } else { value += rt * Math.pow(2, 1 - eBias); } if (value * c >= 2) { e++; c /= 2; } if (e + eBias >= eMax) { m = 0; e = eMax; } else if (e + eBias >= 1) { m = (value * c - 1) * Math.pow(2, mLen); e = e + eBias; } else { m = value * Math.pow(2, eBias - 1) * Math.pow(2, mLen); e = 0; } } for (;mLen >= 8; buffer[offset + i] = m & 255, i += d, m /= 256, mLen -= 8) {} e = e << mLen | m; eLen += mLen; for (;eLen > 0; buffer[offset + i] = e & 255, i += d, e /= 256, eLen -= 8) {} buffer[offset + i - d] |= s * 128; }; }, {} ], 1: [ function(require, module, exports) { ""use strict""; exports.byteLength = byteLength; exports.toByteArray = toByteArray; exports.fromByteArray = fromByteArray; var lookup = []; var revLookup = []; var Arr = typeof Uint8Array !== ""undefined"" ? Uint8Array : Array; var code = ""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/""; for (var i = 0, len = code.length; i < len; ++i) { lookup[i] = code[i]; revLookup[code.charCodeAt(i)] = i; } revLookup[""-"".charCodeAt(0)] = 62; revLookup[""_"".charCodeAt(0)] = 63; function getLens(b64) { var len = b64.length; if (len % 4 > 0) { throw new Error(""Invalid string. Length must be a multiple of 4""); } var validLen = b64.indexOf(""=""); if (validLen === -1) validLen = len; var placeHoldersLen = validLen === len ? 0 : 4 - validLen % 4; return [ validLen, placeHoldersLen ]; } function byteLength(b64) { var lens = getLens(b64); var validLen = lens[0]; var placeHoldersLen = lens[1]; return (validLen + placeHoldersLen) * 3 / 4 - placeHoldersLen; } function _byteLength(b64, validLen, placeHoldersLen) { return (validLen + placeHoldersLen) * 3 / 4 - placeHoldersLen; } function toByteArray(b64) { var tmp; var lens = getLens(b64); var validLen = lens[0]; var placeHoldersLen = lens[1]; var arr = new Arr(_byteLength(b64, validLen, placeHoldersLen)); var curByte = 0; var len = placeHoldersLen > 0 ? validLen - 4 : validLen; var i; for (i = 0; i < len; i += 4) { tmp = revLookup[b64.charCodeAt(i)] << 18 | revLookup[b64.charCodeAt(i + 1)] << 12 | revLookup[b64.charCodeAt(i + 2)] << 6 | revLookup[b64.charCodeAt(i + 3)]; arr[curByte++] = tmp >> 16 & 255; arr[curByte++] = tmp >> 8 & 255; arr[curByte++] = tmp & 255; } if (placeHoldersLen === 2) { tmp = revLookup[b64.charCodeAt(i)] << 2 | revLookup[b64.charCodeAt(i + 1)] >> 4; arr[curByte++] = tmp & 255; } if (placeHoldersLen === 1) { tmp = revLookup[b64.charCodeAt(i)] << 10 | revLookup[b64.charCodeAt(i + 1)] << 4 | revLookup[b64.charCodeAt(i + 2)] >> 2; arr[curByte++] = tmp >> 8 & 255; arr[curByte++] = tmp & 255; } return arr; } function tripletToBase64(num) { return lookup[num >> 18 & 63] + lookup[num >> 12 & 63] + lookup[num >> 6 & 63] + lookup[num & 63]; } function encodeChunk(uint8, start, end) { var tmp; var output = []; for (var i = start; i < end; i += 3) { tmp = (uint8[i] << 16 & 16711680) + (uint8[i + 1] << 8 & 65280) + (uint8[i + 2] & 255); output.push(tripletToBase64(tmp)); } return output.join(""""); } function fromByteArray(uint8) { var tmp; var len = uint8.length; var extraBytes = len % 3; var parts = []; var maxChunkLength = 16383; for (var i = 0, len2 = len - extraBytes; i < len2; i += maxChunkLength) { parts.push(encodeChunk(uint8, i, i + maxChunkLength > len2 ? len2 : i + maxChunkLength)); } if (extraBytes === 1) { tmp = uint8[len - 1]; parts.push(lookup[tmp >> 2] + lookup[tmp << 4 & 63] + ""==""); } else if (extraBytes === 2) { tmp = (uint8[len - 2] << 8) + uint8[len - 1]; parts.push(lookup[tmp >> 10] + lookup[tmp >> 4 & 63] + lookup[tmp << 2 & 63] + ""=""); } return parts.join(""""); } }, {} ] }, {}, [ 28 ]);AWS.apiLoader.services[""s3""] = {}; AWS.S3 = AWS.Service.defineService(""s3"", [ ""2006-03-01"" ]); _xamzrequire = function e(t, n, r) { function s(o, u) { if (!n[o]) { if (!t[o]) { var a = typeof _xamzrequire == ""function"" && _xamzrequire; if (!u && a) return a(o, !0); if (i) return i(o, !0); var f = new Error(""Cannot find module '"" + o + ""'""); throw f.code = ""MODULE_NOT_FOUND"", f; } var l = n[o] = { exports: {} }; t[o][0].call(l.exports, function(e) { var n = t[o][1][e]; return s(n ? n : e); }, l, l.exports, e, t, n, r); } return n[o].exports; } var i = typeof _xamzrequire == ""function"" && _xamzrequire; for (var o = 0; o < r.length; o++) s(r[o]); return s; }({ 102: [ function(require, module, exports) { var AWS = require(""../core""); var v4Credentials = require(""../signers/v4_credentials""); require(""../s3/managed_upload""); var operationsWith200StatusCodeError = { completeMultipartUpload: true, copyObject: true, uploadPartCopy: true }; var regionRedirectErrorCodes = [ ""AuthorizationHeaderMalformed"", ""BadRequest"", ""PermanentRedirect"", 301 ]; AWS.util.update(AWS.S3.prototype, { getSignatureVersion: function getSignatureVersion(request) { var defaultApiVersion = this.api.signatureVersion; var userDefinedVersion = this._originalConfig ? this._originalConfig.signatureVersion : null; var regionDefinedVersion = this.config.signatureVersion; var isPresigned = request ? request.isPresigned() : false; if (userDefinedVersion) { userDefinedVersion = userDefinedVersion === ""v2"" ? ""s3"" : userDefinedVersion; return userDefinedVersion; } if (isPresigned !== true) { defaultApiVersion = ""v4""; } else if (regionDefinedVersion) { defaultApiVersion = regionDefinedVersion; } return defaultApiVersion; }, getSignerClass: function getSignerClass(request) { var signatureVersion = this.getSignatureVersion(request); return AWS.Signers.RequestSigner.getVersion(signatureVersion); }, validateService: function validateService() { var msg; var messages = []; if (!this.config.region) this.config.region = ""us-east-1""; if (!this.config.endpoint && this.config.s3BucketEndpoint) { messages.push(""An endpoint must be provided when configuring "" + ""`s3BucketEndpoint` to true.""); } if (messages.length === 1) { msg = messages[0]; } else if (messages.length > 1) { msg = ""Multiple configuration errors:\n"" + messages.join(""\n""); } if (msg) { throw AWS.util.error(new Error(), { name: ""InvalidEndpoint"", message: msg }); } }, shouldDisableBodySigning: function shouldDisableBodySigning(request) { var signerClass = this.getSignerClass(); if (this.config.s3DisableBodySigning === true && signerClass === AWS.Signers.V4 && request.httpRequest.endpoint.protocol === ""https:"") { return true; } return false; }, setupRequestListeners: function setupRequestListeners(request) { var prependListener = true; request.addListener(""validate"", this.validateScheme); request.addListener(""validate"", this.validateBucketEndpoint); request.addListener(""validate"", this.correctBucketRegionFromCache); request.addListener(""validate"", this.validateBucketName, prependListener); request.addListener(""build"", this.addContentType); request.addListener(""build"", this.populateURI); request.addListener(""build"", this.computeContentMd5); request.addListener(""build"", this.computeSseCustomerKeyMd5); request.addListener(""afterBuild"", this.addExpect100Continue); request.removeListener(""validate"", AWS.EventListeners.Core.VALIDATE_REGION); request.addListener(""extractError"", this.extractError); request.onAsync(""extractError"", this.requestBucketRegion); request.addListener(""extractData"", this.extractData); request.addListener(""extractData"", AWS.util.hoistPayloadMember); request.addListener(""beforePresign"", this.prepareSignedUrl); if (AWS.util.isBrowser()) { request.onAsync(""retry"", this.reqRegionForNetworkingError); } if (this.shouldDisableBodySigning(request)) { request.removeListener(""afterBuild"", AWS.EventListeners.Core.COMPUTE_SHA256); request.addListener(""afterBuild"", this.disableBodySigning); } }, validateScheme: function(req) { var params = req.params, scheme = req.httpRequest.endpoint.protocol, sensitive = params.SSECustomerKey || params.CopySourceSSECustomerKey; if (sensitive && scheme !== ""https:"") { var msg = ""Cannot send SSE keys over HTTP. Set 'sslEnabled'"" + ""to 'true' in your configuration""; throw AWS.util.error(new Error(), { code: ""ConfigError"", message: msg }); } }, validateBucketEndpoint: function(req) { if (!req.params.Bucket && req.service.config.s3BucketEndpoint) { var msg = ""Cannot send requests to root API with `s3BucketEndpoint` set.""; throw AWS.util.error(new Error(), { code: ""ConfigError"", message: msg }); } }, validateBucketName: function validateBucketName(req) { var service = req.service; var signatureVersion = service.getSignatureVersion(req); var bucket = req.params && req.params.Bucket; var key = req.params && req.params.Key; var slashIndex = bucket && bucket.indexOf(""/""); if (bucket && slashIndex >= 0) { if (typeof key === ""string"" && slashIndex > 0) { req.params = AWS.util.copy(req.params); var prefix = bucket.substr(slashIndex + 1) || """"; req.params.Key = prefix + ""/"" + key; req.params.Bucket = bucket.substr(0, slashIndex); } else if (signatureVersion === ""v4"") { var msg = ""Bucket names cannot contain forward slashes. Bucket: "" + bucket; throw AWS.util.error(new Error(), { code: ""InvalidBucket"", message: msg }); } } }, isValidAccelerateOperation: function isValidAccelerateOperation(operation) { var invalidOperations = [ ""createBucket"", ""deleteBucket"", ""listBuckets"" ]; return invalidOperations.indexOf(operation) === -1; }, populateURI: function populateURI(req) { var httpRequest = req.httpRequest; var b = req.params.Bucket; var service = req.service; var endpoint = httpRequest.endpoint; if (b) { if (!service.pathStyleBucketName(b)) { if (service.config.useAccelerateEndpoint && service.isValidAccelerateOperation(req.operation)) { if (service.config.useDualstack) { endpoint.hostname = b + "".s3-accelerate.dualstack.amazonaws.com""; } else { endpoint.hostname = b + "".s3-accelerate.amazonaws.com""; } } else if (!service.config.s3BucketEndpoint) { endpoint.hostname = b + ""."" + endpoint.hostname; } var port = endpoint.port; if (port !== 80 && port !== 443) { endpoint.host = endpoint.hostname + "":"" + endpoint.port; } else { endpoint.host = endpoint.hostname; } httpRequest.virtualHostedBucket = b; service.removeVirtualHostedBucketFromPath(req); } } }, removeVirtualHostedBucketFromPath: function removeVirtualHostedBucketFromPath(req) { var httpRequest = req.httpRequest; var bucket = httpRequest.virtualHostedBucket; if (bucket && httpRequest.path) { if (req.params && req.params.Key) { var encodedS3Key = ""/"" + AWS.util.uriEscapePath(req.params.Key); if (httpRequest.path.indexOf(encodedS3Key) === 0 && (httpRequest.path.length === encodedS3Key.length || httpRequest.path[encodedS3Key.length] === ""?"")) { return; } } httpRequest.path = httpRequest.path.replace(new RegExp(""/"" + bucket), """"); if (httpRequest.path[0] !== ""/"") { httpRequest.path = ""/"" + httpRequest.path; } } }, addExpect100Continue: function addExpect100Continue(req) { var len = req.httpRequest.headers[""Content-Length""]; if (AWS.util.isNode() && (len >= 1024 * 1024 || req.params.Body instanceof AWS.util.stream.Stream)) { req.httpRequest.headers[""Expect""] = ""100-continue""; } }, addContentType: function addContentType(req) { var httpRequest = req.httpRequest; if (httpRequest.method === ""GET"" || httpRequest.method === ""HEAD"") { delete httpRequest.headers[""Content-Type""]; return; } if (!httpRequest.headers[""Content-Type""]) { httpRequest.headers[""Content-Type""] = ""application/octet-stream""; } var contentType = httpRequest.headers[""Content-Type""]; if (AWS.util.isBrowser()) { if (typeof httpRequest.body === ""string"" && !contentType.match(/;\s*charset=/)) { var charset = ""; charset=UTF-8""; httpRequest.headers[""Content-Type""] += charset; } else { var replaceFn = function(_, prefix, charsetName) { return prefix + charsetName.toUpperCase(); }; httpRequest.headers[""Content-Type""] = contentType.replace(/(;\s*charset=)(.+)$/, replaceFn); } } }, computableChecksumOperations: { putBucketCors: true, putBucketLifecycle: true, putBucketLifecycleConfiguration: true, putBucketTagging: true, deleteObjects: true, putBucketReplication: true, putObjectLegalHold: true, putObjectRetention: true, putObjectLockConfiguration: true }, willComputeChecksums: function willComputeChecksums(req) { if (this.computableChecksumOperations[req.operation]) return true; if (!this.config.computeChecksums) return false; if (!AWS.util.Buffer.isBuffer(req.httpRequest.body) && typeof req.httpRequest.body !== ""string"") { return false; } var rules = req.service.api.operations[req.operation].input.members; if (req.service.shouldDisableBodySigning(req) && !Object.prototype.hasOwnProperty.call(req.httpRequest.headers, ""presigned-expires"")) { if (rules.ContentMD5 && !req.params.ContentMD5) { return true; } } if (req.service.getSignerClass(req) === AWS.Signers.V4) { if (rules.ContentMD5 && !rules.ContentMD5.required) return false; } if (rules.ContentMD5 && !req.params.ContentMD5) return true; }, computeContentMd5: function computeContentMd5(req) { if (req.service.willComputeChecksums(req)) { var md5 = AWS.util.crypto.md5(req.httpRequest.body, ""base64""); req.httpRequest.headers[""Content-MD5""] = md5; } }, computeSseCustomerKeyMd5: function computeSseCustomerKeyMd5(req) { var keys = { SSECustomerKey: ""x-amz-server-side-encryption-customer-key-MD5"", CopySourceSSECustomerKey: ""x-amz-copy-source-server-side-encryption-customer-key-MD5"" }; AWS.util.each(keys, function(key, header) { if (req.params[key]) { var value = AWS.util.crypto.md5(req.params[key], ""base64""); req.httpRequest.headers[header] = value; } }); }, pathStyleBucketName: function pathStyleBucketName(bucketName) { if (this.config.s3ForcePathStyle) return true; if (this.config.s3BucketEndpoint) return false; if (this.dnsCompatibleBucketName(bucketName)) { return this.config.sslEnabled && bucketName.match(/\./) ? true : false; } else { return true; } }, dnsCompatibleBucketName: function dnsCompatibleBucketName(bucketName) { var b = bucketName; var domain = new RegExp(/^[a-z0-9][a-z0-9\.\-]{1,61}[a-z0-9]$/); var ipAddress = new RegExp(/(\d+\.){3}\d+/); var dots = new RegExp(/\.\./); return b.match(domain) && !b.match(ipAddress) && !b.match(dots) ? true : false; }, successfulResponse: function successfulResponse(resp) { var req = resp.request; var httpResponse = resp.httpResponse; if (operationsWith200StatusCodeError[req.operation] && httpResponse.body.toString().match(""<Error>"")) { return false; } else { return httpResponse.statusCode < 300; } }, retryableError: function retryableError(error, request) { if (operationsWith200StatusCodeError[request.operation] && error.statusCode === 200) { return true; } else if (request._requestRegionForBucket && request.service.bucketRegionCache[request._requestRegionForBucket]) { return false; } else if (error && error.code === ""RequestTimeout"") { return true; } else if (error && regionRedirectErrorCodes.indexOf(error.code) != -1 && error.region && error.region != request.httpRequest.region) { request.httpRequest.region = error.region; if (error.statusCode === 301) { request.service.updateReqBucketRegion(request); } return true; } else { var _super = AWS.Service.prototype.retryableError; return _super.call(this, error, request); } }, updateReqBucketRegion: function updateReqBucketRegion(request, region) { var httpRequest = request.httpRequest; if (typeof region === ""string"" && region.length) { httpRequest.region = region; } if (!httpRequest.endpoint.host.match(/s3(?!-accelerate).*\.amazonaws\.com$/)) { return; } var service = request.service; var s3Config = service.config; var s3BucketEndpoint = s3Config.s3BucketEndpoint; if (s3BucketEndpoint) { delete s3Config.s3BucketEndpoint; } var newConfig = AWS.util.copy(s3Config); delete newConfig.endpoint; newConfig.region = httpRequest.region; httpRequest.endpoint = new AWS.S3(newConfig).endpoint; service.populateURI(request); s3Config.s3BucketEndpoint = s3BucketEndpoint; httpRequest.headers.Host = httpRequest.endpoint.host; if (request._asm.currentState === ""validate"") { request.removeListener(""build"", service.populateURI); request.addListener(""build"", service.removeVirtualHostedBucketFromPath); } }, extractData: function extractData(resp) { var req = resp.request; if (req.operation === ""getBucketLocation"") { var match = resp.httpResponse.body.toString().match(/>(.+)<\/Location/); delete resp.data[""_""]; if (match) { resp.data.LocationConstraint = match[1]; } else { resp.data.LocationConstraint = """"; } } var bucket = req.params.Bucket || null; if (req.operation === ""deleteBucket"" && typeof bucket === ""string"" && !resp.error) { req.service.clearBucketRegionCache(bucket); } else { var headers = resp.httpResponse.headers || {}; var region = headers[""x-amz-bucket-region""] || null; if (!region && req.operation === ""createBucket"" && !resp.error) { var createBucketConfiguration = req.params.CreateBucketConfiguration; if (!createBucketConfiguration) { region = ""us-east-1""; } else if (createBucketConfiguration.LocationConstraint === ""EU"") { region = ""eu-west-1""; } else { region = createBucketConfiguration.LocationConstraint; } } if (region) { if (bucket && region !== req.service.bucketRegionCache[bucket]) { req.service.bucketRegionCache[bucket] = region; } } } req.service.extractRequestIds(resp); }, extractError: function extractError(resp) { var codes = { 304: ""NotModified"", 403: ""Forbidden"", 400: ""BadRequest"", 404: ""NotFound"" }; var req = resp.request; var code = resp.httpResponse.statusCode; var body = resp.httpResponse.body || """"; var headers = resp.httpResponse.headers || {}; var region = headers[""x-amz-bucket-region""] || null; var bucket = req.params.Bucket || null; var bucketRegionCache = req.service.bucketRegionCache; if (region && bucket && region !== bucketRegionCache[bucket]) { bucketRegionCache[bucket] = region; } var cachedRegion; if (codes[code] && body.length === 0) { if (bucket && !region) { cachedRegion = bucketRegionCache[bucket] || null; if (cachedRegion !== req.httpRequest.region) { region = cachedRegion; } } resp.error = AWS.util.error(new Error(), { code: codes[code], message: null, region: region }); } else { var data = new AWS.XML.Parser().parse(body.toString()); if (data.Region && !region) { region = data.Region; if (bucket && region !== bucketRegionCache[bucket]) { bucketRegionCache[bucket] = region; } } else if (bucket && !region && !data.Region) { cachedRegion = bucketRegionCache[bucket] || null; if (cachedRegion !== req.httpRequest.region) { region = cachedRegion; } } resp.error = AWS.util.error(new Error(), { code: data.Code || code, message: data.Message || null, region: region }); } req.service.extractRequestIds(resp); }, requestBucketRegion: function requestBucketRegion(resp, done) { var error = resp.error; var req = resp.request; var bucket = req.params.Bucket || null; if (!error || !bucket || error.region || req.operation === ""listObjects"" || AWS.util.isNode() && req.operation === ""headBucket"" || error.statusCode === 400 && req.operation !== ""headObject"" || regionRedirectErrorCodes.indexOf(error.code) === -1) { return done(); } var reqOperation = AWS.util.isNode() ? ""headBucket"" : ""listObjects""; var reqParams = { Bucket: bucket }; if (reqOperation === ""listObjects"") reqParams.MaxKeys = 0; var regionReq = req.service[reqOperation](reqParams); regionReq._requestRegionForBucket = bucket; regionReq.send(function() { var region = req.service.bucketRegionCache[bucket] || null; error.region = region; done(); }); }, reqRegionForNetworkingError: function reqRegionForNetworkingError(resp, done) { if (!AWS.util.isBrowser()) { return done(); } var error = resp.error; var request = resp.request; var bucket = request.params.Bucket; if (!error || error.code !== ""NetworkingError"" || !bucket || request.httpRequest.region === ""us-east-1"") { return done(); } var service = request.service; var bucketRegionCache = service.bucketRegionCache; var cachedRegion = bucketRegionCache[bucket] || null; if (cachedRegion && cachedRegion !== request.httpRequest.region) { service.updateReqBucketRegion(request, cachedRegion); done(); } else if (!service.dnsCompatibleBucketName(bucket)) { service.updateReqBucketRegion(request, ""us-east-1""); if (bucketRegionCache[bucket] !== ""us-east-1"") { bucketRegionCache[bucket] = ""us-east-1""; } done(); } else if (request.httpRequest.virtualHostedBucket) { var getRegionReq = service.listObjects({ Bucket: bucket, MaxKeys: 0 }); service.updateReqBucketRegion(getRegionReq, ""us-east-1""); getRegionReq._requestRegionForBucket = bucket; getRegionReq.send(function() { var region = service.bucketRegionCache[bucket] || null; if (region && region !== request.httpRequest.region) { service.updateReqBucketRegion(request, region); } done(); }); } else { done(); } }, bucketRegionCache: {}, clearBucketRegionCache: function(buckets) { var bucketRegionCache = this.bucketRegionCache; if (!buckets) { buckets = Object.keys(bucketRegionCache); } else if (typeof buckets === ""string"") { buckets = [ buckets ]; } for (var i = 0; i < buckets.length; i++) { delete bucketRegionCache[buckets[i]]; } return bucketRegionCache; }, correctBucketRegionFromCache: function correctBucketRegionFromCache(req) { var bucket = req.params.Bucket || null; if (bucket) { var service = req.service; var requestRegion = req.httpRequest.region; var cachedRegion = service.bucketRegionCache[bucket]; if (cachedRegion && cachedRegion !== requestRegion) { service.updateReqBucketRegion(req, cachedRegion); } } }, extractRequestIds: function extractRequestIds(resp) { var extendedRequestId = resp.httpResponse.headers ? resp.httpResponse.headers[""x-amz-id-2""] : null; var cfId = resp.httpResponse.headers ? resp.httpResponse.headers[""x-amz-cf-id""] : null; resp.extendedRequestId = extendedRequestId; resp.cfId = cfId; if (resp.error) { resp.error.requestId = resp.requestId || null; resp.error.extendedRequestId = extendedRequestId; resp.error.cfId = cfId; } }, getSignedUrl: function getSignedUrl(operation, params, callback) { params = AWS.util.copy(params || {}); var expires = params.Expires || 900; if (typeof expires !== ""number"") { throw AWS.util.error(new Error(), { code: ""InvalidParameterException"", message: ""The expiration must be a number, received "" + typeof expires }); } delete params.Expires; var request = this.makeRequest(operation, params); if (callback) { AWS.util.defer(function() { request.presign(expires, callback); }); } else { return request.presign(expires, callback); } }, createPresignedPost: function createPresignedPost(params, callback) { if (typeof params === ""function"" && callback === undefined) { callback = params; params = null; } params = AWS.util.copy(params || {}); var boundParams = this.config.params || {}; var bucket = params.Bucket || boundParams.Bucket, self = this, config = this.config, endpoint = AWS.util.copy(this.endpoint); if (!config.s3BucketEndpoint) { endpoint.pathname = ""/"" + bucket; } function finalizePost() { return { url: AWS.util.urlFormat(endpoint), fields: self.preparePostFields(config.credentials, config.region, bucket, params.Fields, params.Conditions, params.Expires) }; } if (callback) { config.getCredentials(function(err) { if (err) { callback(err); } callback(null, finalizePost()); }); } else { return finalizePost(); } }, preparePostFields: function preparePostFields(credentials, region, bucket, fields, conditions, expiresInSeconds) { var now = this.getSkewCorrectedDate(); if (!credentials || !region || !bucket) { throw new Error(""Unable to create a POST object policy without a bucket,"" + "" region, and credentials""); } fields = AWS.util.copy(fields || {}); conditions = (conditions || []).slice(0); expiresInSeconds = expiresInSeconds || 3600; var signingDate = AWS.util.date.iso8601(now).replace(/[:\-]|\.\d{3}/g, """"); var shortDate = signingDate.substr(0, 8); var scope = v4Credentials.createScope(shortDate, region, ""s3""); var credential = credentials.accessKeyId + ""/"" + scope; fields[""bucket""] = bucket; fields[""X-Amz-Algorithm""] = ""AWS4-HMAC-SHA256""; fields[""X-Amz-Credential""] = credential; fields[""X-Amz-Date""] = signingDate; if (credentials.sessionToken) { fields[""X-Amz-Security-Token""] = credentials.sessionToken; } for (var field in fields) { if (fields.hasOwnProperty(field)) { var condition = {}; condition[field] = fields[field]; conditions.push(condition); } } fields.Policy = this.preparePostPolicy(new Date(now.valueOf() + expiresInSeconds * 1e3), conditions); fields[""X-Amz-Signature""] = AWS.util.crypto.hmac(v4Credentials.getSigningKey(credentials, shortDate, region, ""s3"", true), fields.Policy, ""hex""); return fields; }, preparePostPolicy: function preparePostPolicy(expiration, conditions) { return AWS.util.base64.encode(JSON.stringify({ expiration: AWS.util.date.iso8601(expiration), conditions: conditions })); }, prepareSignedUrl: function prepareSignedUrl(request) { request.addListener(""validate"", request.service.noPresignedContentLength); request.removeListener(""build"", request.service.addContentType); if (!request.params.Body) { request.removeListener(""build"", request.service.computeContentMd5); } else { request.addListener(""afterBuild"", AWS.EventListeners.Core.COMPUTE_SHA256); } }, disableBodySigning: function disableBodySigning(request) { var headers = request.httpRequest.headers; if (!Object.prototype.hasOwnProperty.call(headers, ""presigned-expires"")) { headers[""X-Amz-Content-Sha256""] = ""UNSIGNED-PAYLOAD""; } }, noPresignedContentLength: function noPresignedContentLength(request) { if (request.params.ContentLength !== undefined) { throw AWS.util.error(new Error(), { code: ""UnexpectedParameter"", message: ""ContentLength is not supported in pre-signed URLs."" }); } }, createBucket: function createBucket(params, callback) { if (typeof params === ""function"" || !params) { callback = callback || params; params = {}; } var hostname = this.endpoint.hostname; if (hostname !== this.api.globalEndpoint && !params.CreateBucketConfiguration) { params.CreateBucketConfiguration = { LocationConstraint: this.config.region }; } return this.makeRequest(""createBucket"", params, callback); }, upload: function upload(params, options, callback) { if (typeof options === ""function"" && callback === undefined) { callback = options; options = null; } options = options || {}; options = AWS.util.merge(options || {}, { service: this, params: params }); var uploader = new AWS.S3.ManagedUpload(options); if (typeof callback === ""function"") uploader.send(callback); return uploader; } }); AWS.S3.addPromisesToClass = function addPromisesToClass(PromiseDependency) { this.prototype.getSignedUrlPromise = AWS.util.promisifyMethod(""getSignedUrl"", PromiseDependency); }; AWS.S3.deletePromisesFromClass = function deletePromisesFromClass() { delete this.prototype.getSignedUrlPromise; }; AWS.util.addPromises(AWS.S3); }, { ""../core"": 38, ""../s3/managed_upload"": 86, ""../signers/v4_credentials"": 114 } ], 86: [ function(require, module, exports) { var AWS = require(""../core""); var byteLength = AWS.util.string.byteLength; var Buffer = AWS.util.Buffer; AWS.S3.ManagedUpload = AWS.util.inherit({ constructor: function ManagedUpload(options) { var self = this; AWS.SequentialExecutor.call(self); self.body = null; self.sliceFn = null; self.callback = null; self.parts = {}; self.completeInfo = []; self.fillQueue = function() { self.callback(new Error(""Unsupported body payload "" + typeof self.body)); }; self.configure(options); }, configure: function configure(options) { options = options || {}; this.partSize = this.minPartSize; if (options.queueSize) this.queueSize = options.queueSize; if (options.partSize) this.partSize = options.partSize; if (options.leavePartsOnError) this.leavePartsOnError = true; if (options.tags) { if (!Array.isArray(options.tags)) { throw new Error(""Tags must be specified as an array; "" + typeof options.tags + "" provided.""); } this.tags = options.tags; } if (this.partSize < this.minPartSize) { throw new Error(""partSize must be greater than "" + this.minPartSize); } this.service = options.service; this.bindServiceObject(options.params); this.validateBody(); this.adjustTotalBytes(); }, leavePartsOnError: false, queueSize: 4, partSize: null, minPartSize: 1024 * 1024 * 5, maxTotalParts: 1e4, send: function(callback) { var self = this; self.failed = false; self.callback = callback || function(err) { if (err) throw err; }; var runFill = true; if (self.sliceFn) { self.fillQueue = self.fillBuffer; } else if (AWS.util.isNode()) { var Stream = AWS.util.stream.Stream; if (self.body instanceof Stream) { runFill = false; self.fillQueue = self.fillStream; self.partBuffers = []; self.body.on(""error"", function(err) { self.cleanup(err); }).on(""readable"", function() { self.fillQueue(); }).on(""end"", function() { self.isDoneChunking = true; self.numParts = self.totalPartNumbers; self.fillQueue.call(self); if (self.isDoneChunking && self.totalPartNumbers >= 1 && self.doneParts === self.numParts) { self.finishMultiPart(); } }); } } if (runFill) self.fillQueue.call(self); }, abort: function() { var self = this; if (self.isDoneChunking === true && self.totalPartNumbers === 1 && self.singlePart) { self.singlePart.abort(); } else { self.cleanup(AWS.util.error(new Error(""Request aborted by user""), { code: ""RequestAbortedError"", retryable: false })); } }, validateBody: function validateBody() { var self = this; self.body = self.service.config.params.Body; if (typeof self.body === ""string"") { self.body = AWS.util.buffer.toBuffer(self.body); } else if (!self.body) { throw new Error(""params.Body is required""); } self.sliceFn = AWS.util.arraySliceFn(self.body); }, bindServiceObject: function bindServiceObject(params) { params = params || {}; var self = this; if (!self.service) { self.service = new AWS.S3({ params: params }); } else { var service = self.service; var config = AWS.util.copy(service.config); config.signatureVersion = service.getSignatureVersion(); self.service = new service.constructor.__super__(config); self.service.config.params = AWS.util.merge(self.service.config.params || {}, params); } }, adjustTotalBytes: function adjustTotalBytes() { var self = this; try { self.totalBytes = byteLength(self.body); } catch (e) {} if (self.totalBytes) { var newPartSize = Math.ceil(self.totalBytes / self.maxTotalParts); if (newPartSize > self.partSize) self.partSize = newPartSize; } else { self.totalBytes = undefined; } }, isDoneChunking: false, partPos: 0, totalChunkedBytes: 0, totalUploadedBytes: 0, totalBytes: undefined, numParts: 0, totalPartNumbers: 0, activeParts: 0, doneParts: 0, parts: null, completeInfo: null, failed: false, multipartReq: null, partBuffers: null, partBufferLength: 0, fillBuffer: function fillBuffer() { var self = this; var bodyLen = byteLength(self.body); if (bodyLen === 0) { self.isDoneChunking = true; self.numParts = 1; self.nextChunk(self.body); return; } while (self.activeParts < self.queueSize && self.partPos < bodyLen) { var endPos = Math.min(self.partPos + self.partSize, bodyLen); var buf = self.sliceFn.call(self.body, self.partPos, endPos); self.partPos += self.partSize; if (byteLength(buf) < self.partSize || self.partPos === bodyLen) { self.isDoneChunking = true; self.numParts = self.totalPartNumbers + 1; } self.nextChunk(buf); } }, fillStream: function fillStream() { var self = this; if (self.activeParts >= self.queueSize) return; var buf = self.body.read(self.partSize - self.partBufferLength) || self.body.read(); if (buf) { self.partBuffers.push(buf); self.partBufferLength += buf.length; self.totalChunkedBytes += buf.length; } if (self.partBufferLength >= self.partSize) { var pbuf = self.partBuffers.length === 1 ? self.partBuffers[0] : Buffer.concat(self.partBuffers); self.partBuffers = []; self.partBufferLength = 0; if (pbuf.length > self.partSize) { var rest = pbuf.slice(self.partSize); self.partBuffers.push(rest); self.partBufferLength += rest.length; pbuf = pbuf.slice(0, self.partSize); } self.nextChunk(pbuf); } if (self.isDoneChunking && !self.isDoneSending) { pbuf = self.partBuffers.length === 1 ? self.partBuffers[0] : Buffer.concat(self.partBuffers); self.partBuffers = []; self.partBufferLength = 0; self.totalBytes = self.totalChunkedBytes; self.isDoneSending = true; if (self.numParts === 0 || pbuf.length > 0) { self.numParts++; self.nextChunk(pbuf); } } self.body.read(0); }, nextChunk: function nextChunk(chunk) { var self = this; if (self.failed) return null; var partNumber = ++self.totalPartNumbers; if (self.isDoneChunking && partNumber === 1) { var params = { Body: chunk }; if (this.tags) { params.Tagging = this.getTaggingHeader(); } var req = self.service.putObject(params); req._managedUpload = self; req.on(""httpUploadProgress"", self.progress).send(self.finishSinglePart); self.singlePart = req; return null; } else if (self.service.config.params.ContentMD5) { var err = AWS.util.error(new Error(""The Content-MD5 you specified is invalid for multi-part uploads.""), { code: ""InvalidDigest"", retryable: false }); self.cleanup(err); return null; } if (self.completeInfo[partNumber] && self.completeInfo[partNumber].ETag !== null) { return null; } self.activeParts++; if (!self.service.config.params.UploadId) { if (!self.multipartReq) { self.multipartReq = self.service.createMultipartUpload(); self.multipartReq.on(""success"", function(resp) { self.service.config.params.UploadId = resp.data.UploadId; self.multipartReq = null; }); self.queueChunks(chunk, partNumber); self.multipartReq.on(""error"", function(err) { self.cleanup(err); }); self.multipartReq.send(); } else { self.queueChunks(chunk, partNumber); } } else { self.uploadPart(chunk, partNumber); } }, getTaggingHeader: function getTaggingHeader() { var kvPairStrings = []; for (var i = 0; i < this.tags.length; i++) { kvPairStrings.push(AWS.util.uriEscape(this.tags[i].Key) + ""="" + AWS.util.uriEscape(this.tags[i].Value)); } return kvPairStrings.join(""&""); }, uploadPart: function uploadPart(chunk, partNumber) { var self = this; var partParams = { Body: chunk, ContentLength: AWS.util.string.byteLength(chunk), PartNumber: partNumber }; var partInfo = { ETag: null, PartNumber: partNumber }; self.completeInfo[partNumber] = partInfo; var req = self.service.uploadPart(partParams); self.parts[partNumber] = req; req._lastUploadedBytes = 0; req._managedUpload = self; req.on(""httpUploadProgress"", self.progress); req.send(function(err, data) { delete self.parts[partParams.PartNumber]; self.activeParts--; if (!err && (!data || !data.ETag)) { var message = ""No access to ETag property on response.""; if (AWS.util.isBrowser()) { message += "" Check CORS configuration to expose ETag header.""; } err = AWS.util.error(new Error(message), { code: ""ETagMissing"", retryable: false }); } if (err) return self.cleanup(err); if (self.completeInfo[partNumber] && self.completeInfo[partNumber].ETag !== null) return null; partInfo.ETag = data.ETag; self.doneParts++; if (self.isDoneChunking && self.doneParts === self.numParts) { self.finishMultiPart(); } else { self.fillQueue.call(self); } }); }, queueChunks: function queueChunks(chunk, partNumber) { var self = this; self.multipartReq.on(""success"", function() { self.uploadPart(chunk, partNumber); }); }, cleanup: function cleanup(err) { var self = this; if (self.failed) return; if (typeof self.body.removeAllListeners === ""function"" && typeof self.body.resume === ""function"") { self.body.removeAllListeners(""readable""); self.body.removeAllListeners(""end""); self.body.resume(); } if (self.multipartReq) { self.multipartReq.removeAllListeners(""success""); self.multipartReq.removeAllListeners(""error""); self.multipartReq.removeAllListeners(""complete""); delete self.multipartReq; } if (self.service.config.params.UploadId && !self.leavePartsOnError) { self.service.abortMultipartUpload().send(); } else if (self.leavePartsOnError) { self.isDoneChunking = false; } AWS.util.each(self.parts, function(partNumber, part) { part.removeAllListeners(""complete""); part.abort(); }); self.activeParts = 0; self.partPos = 0; self.numParts = 0; self.totalPartNumbers = 0; self.parts = {}; self.failed = true; self.callback(err); }, finishMultiPart: function finishMultiPart() { var self = this; var completeParams = { MultipartUpload: { Parts: self.completeInfo.slice(1) } }; self.service.completeMultipartUpload(completeParams, function(err, data) { if (err) { return self.cleanup(err); } if (data && typeof data.Location === ""string"") { data.Location = data.Location.replace(/%2F/g, ""/""); } if (Array.isArray(self.tags)) { for (var i = 0; i < self.tags.length; i++) { self.tags[i].Value = String(self.tags[i].Value); } self.service.putObjectTagging({ Tagging: { TagSet: self.tags } }, function(e, d) { if (e) { self.callback(e); } else { self.callback(e, data); } }); } else { self.callback(err, data); } }); }, finishSinglePart: function finishSinglePart(err, data) { var upload = this.request._managedUpload; var httpReq = this.request.httpRequest; var endpoint = httpReq.endpoint; if (err) return upload.callback(err); data.Location = [ endpoint.protocol, ""//"", endpoint.host, httpReq.path ].join(""""); data.key = this.request.params.Key; data.Key = this.request.params.Key; data.Bucket = this.request.params.Bucket; upload.callback(err, data); }, progress: function progress(info) { var upload = this._managedUpload; if (this.operation === ""putObject"") { info.part = 1; info.key = this.params.Key; } else { upload.totalUploadedBytes += info.loaded - this._lastUploadedBytes; this._lastUploadedBytes = info.loaded; info = { loaded: upload.totalUploadedBytes, total: upload.totalBytes, part: this.params.PartNumber, key: this.params.Key }; } upload.emit(""httpUploadProgress"", [ info ]); } }); AWS.util.mixin(AWS.S3.ManagedUpload, AWS.SequentialExecutor); AWS.S3.ManagedUpload.addPromisesToClass = function addPromisesToClass(PromiseDependency) { this.prototype.promise = AWS.util.promisifyMethod(""send"", PromiseDependency); }; AWS.S3.ManagedUpload.deletePromisesFromClass = function deletePromisesFromClass() { delete this.prototype.promise; }; AWS.util.addPromises(AWS.S3.ManagedUpload); module.exports = AWS.S3.ManagedUpload; }, { ""../core"": 38 } ] }, {}, [ 102 ]);AWS.apiLoader.services[""s3""][""2006-03-01""] = { version: ""2.0"", metadata: { apiVersion: ""2006-03-01"", checksumFormat: ""md5"", endpointPrefix: ""s3"", globalEndpoint: ""s3.amazonaws.com"", protocol: ""rest-xml"", serviceAbbreviation: ""Amazon S3"", serviceFullName: ""Amazon Simple Storage Service"", serviceId: ""S3"", signatureVersion: ""s3"", uid: ""s3-2006-03-01"" }, operations: { AbortMultipartUpload: { http: { method: ""DELETE"", requestUri: ""/{Bucket}/{Key+}"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"", ""UploadId"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, UploadId: { location: ""querystring"", locationName: ""uploadId"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" } } }, output: { type: ""structure"", members: { RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } } } }, CompleteMultipartUpload: { http: { requestUri: ""/{Bucket}/{Key+}"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"", ""UploadId"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, MultipartUpload: { locationName: ""CompleteMultipartUpload"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", members: { Parts: { locationName: ""Part"", type: ""list"", member: { type: ""structure"", members: { ETag: {}, PartNumber: { type: ""integer"" } } }, flattened: true } } }, UploadId: { location: ""querystring"", locationName: ""uploadId"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" } }, payload: ""MultipartUpload"" }, output: { type: ""structure"", members: { Location: {}, Bucket: {}, Key: {}, Expiration: { location: ""header"", locationName: ""x-amz-expiration"" }, ETag: {}, ServerSideEncryption: { location: ""header"", locationName: ""x-amz-server-side-encryption"" }, VersionId: { location: ""header"", locationName: ""x-amz-version-id"" }, SSEKMSKeyId: { shape: ""Sj"", location: ""header"", locationName: ""x-amz-server-side-encryption-aws-kms-key-id"" }, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } } } }, CopyObject: { http: { method: ""PUT"", requestUri: ""/{Bucket}/{Key+}"" }, input: { type: ""structure"", required: [ ""Bucket"", ""CopySource"", ""Key"" ], members: { ACL: { location: ""header"", locationName: ""x-amz-acl"" }, Bucket: { location: ""uri"", locationName: ""Bucket"" }, CacheControl: { location: ""header"", locationName: ""Cache-Control"" }, ContentDisposition: { location: ""header"", locationName: ""Content-Disposition"" }, ContentEncoding: { location: ""header"", locationName: ""Content-Encoding"" }, ContentLanguage: { location: ""header"", locationName: ""Content-Language"" }, ContentType: { location: ""header"", locationName: ""Content-Type"" }, CopySource: { location: ""header"", locationName: ""x-amz-copy-source"" }, CopySourceIfMatch: { location: ""header"", locationName: ""x-amz-copy-source-if-match"" }, CopySourceIfModifiedSince: { location: ""header"", locationName: ""x-amz-copy-source-if-modified-since"", type: ""timestamp"" }, CopySourceIfNoneMatch: { location: ""header"", locationName: ""x-amz-copy-source-if-none-match"" }, CopySourceIfUnmodifiedSince: { location: ""header"", locationName: ""x-amz-copy-source-if-unmodified-since"", type: ""timestamp"" }, Expires: { location: ""header"", locationName: ""Expires"", type: ""timestamp"" }, GrantFullControl: { location: ""header"", locationName: ""x-amz-grant-full-control"" }, GrantRead: { location: ""header"", locationName: ""x-amz-grant-read"" }, GrantReadACP: { location: ""header"", locationName: ""x-amz-grant-read-acp"" }, GrantWriteACP: { location: ""header"", locationName: ""x-amz-grant-write-acp"" }, Key: { location: ""uri"", locationName: ""Key"" }, Metadata: { shape: ""S11"", location: ""headers"", locationName: ""x-amz-meta-"" }, MetadataDirective: { location: ""header"", locationName: ""x-amz-metadata-directive"" }, TaggingDirective: { location: ""header"", locationName: ""x-amz-tagging-directive"" }, ServerSideEncryption: { location: ""header"", locationName: ""x-amz-server-side-encryption"" }, StorageClass: { location: ""header"", locationName: ""x-amz-storage-class"" }, WebsiteRedirectLocation: { location: ""header"", locationName: ""x-amz-website-redirect-location"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKey: { shape: ""S19"", location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, SSEKMSKeyId: { shape: ""Sj"", location: ""header"", locationName: ""x-amz-server-side-encryption-aws-kms-key-id"" }, SSEKMSEncryptionContext: { shape: ""S1b"", location: ""header"", locationName: ""x-amz-server-side-encryption-context"" }, CopySourceSSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-copy-source-server-side-encryption-customer-algorithm"" }, CopySourceSSECustomerKey: { shape: ""S1d"", location: ""header"", locationName: ""x-amz-copy-source-server-side-encryption-customer-key"" }, CopySourceSSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-copy-source-server-side-encryption-customer-key-MD5"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" }, Tagging: { location: ""header"", locationName: ""x-amz-tagging"" }, ObjectLockMode: { location: ""header"", locationName: ""x-amz-object-lock-mode"" }, ObjectLockRetainUntilDate: { shape: ""S1h"", location: ""header"", locationName: ""x-amz-object-lock-retain-until-date"" }, ObjectLockLegalHoldStatus: { location: ""header"", locationName: ""x-amz-object-lock-legal-hold"" } } }, output: { type: ""structure"", members: { CopyObjectResult: { type: ""structure"", members: { ETag: {}, LastModified: { type: ""timestamp"" } } }, Expiration: { location: ""header"", locationName: ""x-amz-expiration"" }, CopySourceVersionId: { location: ""header"", locationName: ""x-amz-copy-source-version-id"" }, VersionId: { location: ""header"", locationName: ""x-amz-version-id"" }, ServerSideEncryption: { location: ""header"", locationName: ""x-amz-server-side-encryption"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, SSEKMSKeyId: { shape: ""Sj"", location: ""header"", locationName: ""x-amz-server-side-encryption-aws-kms-key-id"" }, SSEKMSEncryptionContext: { shape: ""S1b"", location: ""header"", locationName: ""x-amz-server-side-encryption-context"" }, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } }, payload: ""CopyObjectResult"" }, alias: ""PutObjectCopy"" }, CreateBucket: { http: { method: ""PUT"", requestUri: ""/{Bucket}"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { ACL: { location: ""header"", locationName: ""x-amz-acl"" }, Bucket: { location: ""uri"", locationName: ""Bucket"" }, CreateBucketConfiguration: { locationName: ""CreateBucketConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", members: { LocationConstraint: {} } }, GrantFullControl: { location: ""header"", locationName: ""x-amz-grant-full-control"" }, GrantRead: { location: ""header"", locationName: ""x-amz-grant-read"" }, GrantReadACP: { location: ""header"", locationName: ""x-amz-grant-read-acp"" }, GrantWrite: { location: ""header"", locationName: ""x-amz-grant-write"" }, GrantWriteACP: { location: ""header"", locationName: ""x-amz-grant-write-acp"" }, ObjectLockEnabledForBucket: { location: ""header"", locationName: ""x-amz-bucket-object-lock-enabled"", type: ""boolean"" } }, payload: ""CreateBucketConfiguration"" }, output: { type: ""structure"", members: { Location: { location: ""header"", locationName: ""Location"" } } }, alias: ""PutBucket"" }, CreateMultipartUpload: { http: { requestUri: ""/{Bucket}/{Key+}?uploads"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { ACL: { location: ""header"", locationName: ""x-amz-acl"" }, Bucket: { location: ""uri"", locationName: ""Bucket"" }, CacheControl: { location: ""header"", locationName: ""Cache-Control"" }, ContentDisposition: { location: ""header"", locationName: ""Content-Disposition"" }, ContentEncoding: { location: ""header"", locationName: ""Content-Encoding"" }, ContentLanguage: { location: ""header"", locationName: ""Content-Language"" }, ContentType: { location: ""header"", locationName: ""Content-Type"" }, Expires: { location: ""header"", locationName: ""Expires"", type: ""timestamp"" }, GrantFullControl: { location: ""header"", locationName: ""x-amz-grant-full-control"" }, GrantRead: { location: ""header"", locationName: ""x-amz-grant-read"" }, GrantReadACP: { location: ""header"", locationName: ""x-amz-grant-read-acp"" }, GrantWriteACP: { location: ""header"", locationName: ""x-amz-grant-write-acp"" }, Key: { location: ""uri"", locationName: ""Key"" }, Metadata: { shape: ""S11"", location: ""headers"", locationName: ""x-amz-meta-"" }, ServerSideEncryption: { location: ""header"", locationName: ""x-amz-server-side-encryption"" }, StorageClass: { location: ""header"", locationName: ""x-amz-storage-class"" }, WebsiteRedirectLocation: { location: ""header"", locationName: ""x-amz-website-redirect-location"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKey: { shape: ""S19"", location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, SSEKMSKeyId: { shape: ""Sj"", location: ""header"", locationName: ""x-amz-server-side-encryption-aws-kms-key-id"" }, SSEKMSEncryptionContext: { shape: ""S1b"", location: ""header"", locationName: ""x-amz-server-side-encryption-context"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" }, Tagging: { location: ""header"", locationName: ""x-amz-tagging"" }, ObjectLockMode: { location: ""header"", locationName: ""x-amz-object-lock-mode"" }, ObjectLockRetainUntilDate: { shape: ""S1h"", location: ""header"", locationName: ""x-amz-object-lock-retain-until-date"" }, ObjectLockLegalHoldStatus: { location: ""header"", locationName: ""x-amz-object-lock-legal-hold"" } } }, output: { type: ""structure"", members: { AbortDate: { location: ""header"", locationName: ""x-amz-abort-date"", type: ""timestamp"" }, AbortRuleId: { location: ""header"", locationName: ""x-amz-abort-rule-id"" }, Bucket: { locationName: ""Bucket"" }, Key: {}, UploadId: {}, ServerSideEncryption: { location: ""header"", locationName: ""x-amz-server-side-encryption"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, SSEKMSKeyId: { shape: ""Sj"", location: ""header"", locationName: ""x-amz-server-side-encryption-aws-kms-key-id"" }, SSEKMSEncryptionContext: { shape: ""S1b"", location: ""header"", locationName: ""x-amz-server-side-encryption-context"" }, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } } }, alias: ""InitiateMultipartUpload"" }, DeleteBucket: { http: { method: ""DELETE"", requestUri: ""/{Bucket}"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } } }, DeleteBucketAnalyticsConfiguration: { http: { method: ""DELETE"", requestUri: ""/{Bucket}?analytics"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"", ""Id"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Id: { location: ""querystring"", locationName: ""id"" } } } }, DeleteBucketCors: { http: { method: ""DELETE"", requestUri: ""/{Bucket}?cors"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } } }, DeleteBucketEncryption: { http: { method: ""DELETE"", requestUri: ""/{Bucket}?encryption"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } } }, DeleteBucketInventoryConfiguration: { http: { method: ""DELETE"", requestUri: ""/{Bucket}?inventory"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"", ""Id"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Id: { location: ""querystring"", locationName: ""id"" } } } }, DeleteBucketLifecycle: { http: { method: ""DELETE"", requestUri: ""/{Bucket}?lifecycle"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } } }, DeleteBucketMetricsConfiguration: { http: { method: ""DELETE"", requestUri: ""/{Bucket}?metrics"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"", ""Id"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Id: { location: ""querystring"", locationName: ""id"" } } } }, DeleteBucketPolicy: { http: { method: ""DELETE"", requestUri: ""/{Bucket}?policy"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } } }, DeleteBucketReplication: { http: { method: ""DELETE"", requestUri: ""/{Bucket}?replication"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } } }, DeleteBucketTagging: { http: { method: ""DELETE"", requestUri: ""/{Bucket}?tagging"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } } }, DeleteBucketWebsite: { http: { method: ""DELETE"", requestUri: ""/{Bucket}?website"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } } }, DeleteObject: { http: { method: ""DELETE"", requestUri: ""/{Bucket}/{Key+}"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, MFA: { location: ""header"", locationName: ""x-amz-mfa"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" }, BypassGovernanceRetention: { location: ""header"", locationName: ""x-amz-bypass-governance-retention"", type: ""boolean"" } } }, output: { type: ""structure"", members: { DeleteMarker: { location: ""header"", locationName: ""x-amz-delete-marker"", type: ""boolean"" }, VersionId: { location: ""header"", locationName: ""x-amz-version-id"" }, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } } } }, DeleteObjectTagging: { http: { method: ""DELETE"", requestUri: ""/{Bucket}/{Key+}?tagging"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" } } }, output: { type: ""structure"", members: { VersionId: { location: ""header"", locationName: ""x-amz-version-id"" } } } }, DeleteObjects: { http: { requestUri: ""/{Bucket}?delete"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Delete"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Delete: { locationName: ""Delete"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", required: [ ""Objects"" ], members: { Objects: { locationName: ""Object"", type: ""list"", member: { type: ""structure"", required: [ ""Key"" ], members: { Key: {}, VersionId: {} } }, flattened: true }, Quiet: { type: ""boolean"" } } }, MFA: { location: ""header"", locationName: ""x-amz-mfa"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" }, BypassGovernanceRetention: { location: ""header"", locationName: ""x-amz-bypass-governance-retention"", type: ""boolean"" } }, payload: ""Delete"" }, output: { type: ""structure"", members: { Deleted: { type: ""list"", member: { type: ""structure"", members: { Key: {}, VersionId: {}, DeleteMarker: { type: ""boolean"" }, DeleteMarkerVersionId: {} } }, flattened: true }, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" }, Errors: { locationName: ""Error"", type: ""list"", member: { type: ""structure"", members: { Key: {}, VersionId: {}, Code: {}, Message: {} } }, flattened: true } } }, alias: ""DeleteMultipleObjects"" }, DeletePublicAccessBlock: { http: { method: ""DELETE"", requestUri: ""/{Bucket}?publicAccessBlock"", responseCode: 204 }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } } }, GetBucketAccelerateConfiguration: { http: { method: ""GET"", requestUri: ""/{Bucket}?accelerate"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { Status: {} } } }, GetBucketAcl: { http: { method: ""GET"", requestUri: ""/{Bucket}?acl"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { Owner: { shape: ""S32"" }, Grants: { shape: ""S35"", locationName: ""AccessControlList"" } } } }, GetBucketAnalyticsConfiguration: { http: { method: ""GET"", requestUri: ""/{Bucket}?analytics"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Id"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Id: { location: ""querystring"", locationName: ""id"" } } }, output: { type: ""structure"", members: { AnalyticsConfiguration: { shape: ""S3e"" } }, payload: ""AnalyticsConfiguration"" } }, GetBucketCors: { http: { method: ""GET"", requestUri: ""/{Bucket}?cors"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { CORSRules: { shape: ""S3u"", locationName: ""CORSRule"" } } } }, GetBucketEncryption: { http: { method: ""GET"", requestUri: ""/{Bucket}?encryption"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { ServerSideEncryptionConfiguration: { shape: ""S47"" } }, payload: ""ServerSideEncryptionConfiguration"" } }, GetBucketInventoryConfiguration: { http: { method: ""GET"", requestUri: ""/{Bucket}?inventory"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Id"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Id: { location: ""querystring"", locationName: ""id"" } } }, output: { type: ""structure"", members: { InventoryConfiguration: { shape: ""S4d"" } }, payload: ""InventoryConfiguration"" } }, GetBucketLifecycle: { http: { method: ""GET"", requestUri: ""/{Bucket}?lifecycle"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { Rules: { shape: ""S4t"", locationName: ""Rule"" } } }, deprecated: true }, GetBucketLifecycleConfiguration: { http: { method: ""GET"", requestUri: ""/{Bucket}?lifecycle"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { Rules: { shape: ""S58"", locationName: ""Rule"" } } } }, GetBucketLocation: { http: { method: ""GET"", requestUri: ""/{Bucket}?location"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { LocationConstraint: {} } } }, GetBucketLogging: { http: { method: ""GET"", requestUri: ""/{Bucket}?logging"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { LoggingEnabled: { shape: ""S5i"" } } } }, GetBucketMetricsConfiguration: { http: { method: ""GET"", requestUri: ""/{Bucket}?metrics"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Id"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Id: { location: ""querystring"", locationName: ""id"" } } }, output: { type: ""structure"", members: { MetricsConfiguration: { shape: ""S5q"" } }, payload: ""MetricsConfiguration"" } }, GetBucketNotification: { http: { method: ""GET"", requestUri: ""/{Bucket}?notification"" }, input: { shape: ""S5t"" }, output: { shape: ""S5u"" }, deprecated: true }, GetBucketNotificationConfiguration: { http: { method: ""GET"", requestUri: ""/{Bucket}?notification"" }, input: { shape: ""S5t"" }, output: { shape: ""S65"" } }, GetBucketPolicy: { http: { method: ""GET"", requestUri: ""/{Bucket}?policy"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { Policy: {} }, payload: ""Policy"" } }, GetBucketPolicyStatus: { http: { method: ""GET"", requestUri: ""/{Bucket}?policyStatus"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { PolicyStatus: { type: ""structure"", members: { IsPublic: { locationName: ""IsPublic"", type: ""boolean"" } } } }, payload: ""PolicyStatus"" } }, GetBucketReplication: { http: { method: ""GET"", requestUri: ""/{Bucket}?replication"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { ReplicationConfiguration: { shape: ""S6s"" } }, payload: ""ReplicationConfiguration"" } }, GetBucketRequestPayment: { http: { method: ""GET"", requestUri: ""/{Bucket}?requestPayment"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { Payer: {} } } }, GetBucketTagging: { http: { method: ""GET"", requestUri: ""/{Bucket}?tagging"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", required: [ ""TagSet"" ], members: { TagSet: { shape: ""S3k"" } } } }, GetBucketVersioning: { http: { method: ""GET"", requestUri: ""/{Bucket}?versioning"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { Status: {}, MFADelete: { locationName: ""MfaDelete"" } } } }, GetBucketWebsite: { http: { method: ""GET"", requestUri: ""/{Bucket}?website"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { RedirectAllRequestsTo: { shape: ""S7l"" }, IndexDocument: { shape: ""S7o"" }, ErrorDocument: { shape: ""S7q"" }, RoutingRules: { shape: ""S7r"" } } } }, GetObject: { http: { method: ""GET"", requestUri: ""/{Bucket}/{Key+}"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, IfMatch: { location: ""header"", locationName: ""If-Match"" }, IfModifiedSince: { location: ""header"", locationName: ""If-Modified-Since"", type: ""timestamp"" }, IfNoneMatch: { location: ""header"", locationName: ""If-None-Match"" }, IfUnmodifiedSince: { location: ""header"", locationName: ""If-Unmodified-Since"", type: ""timestamp"" }, Key: { location: ""uri"", locationName: ""Key"" }, Range: { location: ""header"", locationName: ""Range"" }, ResponseCacheControl: { location: ""querystring"", locationName: ""response-cache-control"" }, ResponseContentDisposition: { location: ""querystring"", locationName: ""response-content-disposition"" }, ResponseContentEncoding: { location: ""querystring"", locationName: ""response-content-encoding"" }, ResponseContentLanguage: { location: ""querystring"", locationName: ""response-content-language"" }, ResponseContentType: { location: ""querystring"", locationName: ""response-content-type"" }, ResponseExpires: { location: ""querystring"", locationName: ""response-expires"", type: ""timestamp"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKey: { shape: ""S19"", location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" }, PartNumber: { location: ""querystring"", locationName: ""partNumber"", type: ""integer"" } } }, output: { type: ""structure"", members: { Body: { streaming: true, type: ""blob"" }, DeleteMarker: { location: ""header"", locationName: ""x-amz-delete-marker"", type: ""boolean"" }, AcceptRanges: { location: ""header"", locationName: ""accept-ranges"" }, Expiration: { location: ""header"", locationName: ""x-amz-expiration"" }, Restore: { location: ""header"", locationName: ""x-amz-restore"" }, LastModified: { location: ""header"", locationName: ""Last-Modified"", type: ""timestamp"" }, ContentLength: { location: ""header"", locationName: ""Content-Length"", type: ""long"" }, ETag: { location: ""header"", locationName: ""ETag"" }, MissingMeta: { location: ""header"", locationName: ""x-amz-missing-meta"", type: ""integer"" }, VersionId: { location: ""header"", locationName: ""x-amz-version-id"" }, CacheControl: { location: ""header"", locationName: ""Cache-Control"" }, ContentDisposition: { location: ""header"", locationName: ""Content-Disposition"" }, ContentEncoding: { location: ""header"", locationName: ""Content-Encoding"" }, ContentLanguage: { location: ""header"", locationName: ""Content-Language"" }, ContentRange: { location: ""header"", locationName: ""Content-Range"" }, ContentType: { location: ""header"", locationName: ""Content-Type"" }, Expires: { location: ""header"", locationName: ""Expires"", type: ""timestamp"" }, WebsiteRedirectLocation: { location: ""header"", locationName: ""x-amz-website-redirect-location"" }, ServerSideEncryption: { location: ""header"", locationName: ""x-amz-server-side-encryption"" }, Metadata: { shape: ""S11"", location: ""headers"", locationName: ""x-amz-meta-"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, SSEKMSKeyId: { shape: ""Sj"", location: ""header"", locationName: ""x-amz-server-side-encryption-aws-kms-key-id"" }, StorageClass: { location: ""header"", locationName: ""x-amz-storage-class"" }, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" }, ReplicationStatus: { location: ""header"", locationName: ""x-amz-replication-status"" }, PartsCount: { location: ""header"", locationName: ""x-amz-mp-parts-count"", type: ""integer"" }, TagCount: { location: ""header"", locationName: ""x-amz-tagging-count"", type: ""integer"" }, ObjectLockMode: { location: ""header"", locationName: ""x-amz-object-lock-mode"" }, ObjectLockRetainUntilDate: { shape: ""S1h"", location: ""header"", locationName: ""x-amz-object-lock-retain-until-date"" }, ObjectLockLegalHoldStatus: { location: ""header"", locationName: ""x-amz-object-lock-legal-hold"" } }, payload: ""Body"" } }, GetObjectAcl: { http: { method: ""GET"", requestUri: ""/{Bucket}/{Key+}?acl"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" } } }, output: { type: ""structure"", members: { Owner: { shape: ""S32"" }, Grants: { shape: ""S35"", locationName: ""AccessControlList"" }, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } } } }, GetObjectLegalHold: { http: { method: ""GET"", requestUri: ""/{Bucket}/{Key+}?legal-hold"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" } } }, output: { type: ""structure"", members: { LegalHold: { shape: ""S8q"" } }, payload: ""LegalHold"" } }, GetObjectLockConfiguration: { http: { method: ""GET"", requestUri: ""/{Bucket}?object-lock"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { ObjectLockConfiguration: { shape: ""S8t"" } }, payload: ""ObjectLockConfiguration"" } }, GetObjectRetention: { http: { method: ""GET"", requestUri: ""/{Bucket}/{Key+}?retention"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" } } }, output: { type: ""structure"", members: { Retention: { shape: ""S91"" } }, payload: ""Retention"" } }, GetObjectTagging: { http: { method: ""GET"", requestUri: ""/{Bucket}/{Key+}?tagging"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" } } }, output: { type: ""structure"", required: [ ""TagSet"" ], members: { VersionId: { location: ""header"", locationName: ""x-amz-version-id"" }, TagSet: { shape: ""S3k"" } } } }, GetObjectTorrent: { http: { method: ""GET"", requestUri: ""/{Bucket}/{Key+}?torrent"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" } } }, output: { type: ""structure"", members: { Body: { streaming: true, type: ""blob"" }, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } }, payload: ""Body"" } }, GetPublicAccessBlock: { http: { method: ""GET"", requestUri: ""/{Bucket}?publicAccessBlock"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, output: { type: ""structure"", members: { PublicAccessBlockConfiguration: { shape: ""S98"" } }, payload: ""PublicAccessBlockConfiguration"" } }, HeadBucket: { http: { method: ""HEAD"", requestUri: ""/{Bucket}"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } } }, HeadObject: { http: { method: ""HEAD"", requestUri: ""/{Bucket}/{Key+}"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, IfMatch: { location: ""header"", locationName: ""If-Match"" }, IfModifiedSince: { location: ""header"", locationName: ""If-Modified-Since"", type: ""timestamp"" }, IfNoneMatch: { location: ""header"", locationName: ""If-None-Match"" }, IfUnmodifiedSince: { location: ""header"", locationName: ""If-Unmodified-Since"", type: ""timestamp"" }, Key: { location: ""uri"", locationName: ""Key"" }, Range: { location: ""header"", locationName: ""Range"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKey: { shape: ""S19"", location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" }, PartNumber: { location: ""querystring"", locationName: ""partNumber"", type: ""integer"" } } }, output: { type: ""structure"", members: { DeleteMarker: { location: ""header"", locationName: ""x-amz-delete-marker"", type: ""boolean"" }, AcceptRanges: { location: ""header"", locationName: ""accept-ranges"" }, Expiration: { location: ""header"", locationName: ""x-amz-expiration"" }, Restore: { location: ""header"", locationName: ""x-amz-restore"" }, LastModified: { location: ""header"", locationName: ""Last-Modified"", type: ""timestamp"" }, ContentLength: { location: ""header"", locationName: ""Content-Length"", type: ""long"" }, ETag: { location: ""header"", locationName: ""ETag"" }, MissingMeta: { location: ""header"", locationName: ""x-amz-missing-meta"", type: ""integer"" }, VersionId: { location: ""header"", locationName: ""x-amz-version-id"" }, CacheControl: { location: ""header"", locationName: ""Cache-Control"" }, ContentDisposition: { location: ""header"", locationName: ""Content-Disposition"" }, ContentEncoding: { location: ""header"", locationName: ""Content-Encoding"" }, ContentLanguage: { location: ""header"", locationName: ""Content-Language"" }, ContentType: { location: ""header"", locationName: ""Content-Type"" }, Expires: { location: ""header"", locationName: ""Expires"", type: ""timestamp"" }, WebsiteRedirectLocation: { location: ""header"", locationName: ""x-amz-website-redirect-location"" }, ServerSideEncryption: { location: ""header"", locationName: ""x-amz-server-side-encryption"" }, Metadata: { shape: ""S11"", location: ""headers"", locationName: ""x-amz-meta-"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, SSEKMSKeyId: { shape: ""Sj"", location: ""header"", locationName: ""x-amz-server-side-encryption-aws-kms-key-id"" }, StorageClass: { location: ""header"", locationName: ""x-amz-storage-class"" }, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" }, ReplicationStatus: { location: ""header"", locationName: ""x-amz-replication-status"" }, PartsCount: { location: ""header"", locationName: ""x-amz-mp-parts-count"", type: ""integer"" }, ObjectLockMode: { location: ""header"", locationName: ""x-amz-object-lock-mode"" }, ObjectLockRetainUntilDate: { shape: ""S1h"", location: ""header"", locationName: ""x-amz-object-lock-retain-until-date"" }, ObjectLockLegalHoldStatus: { location: ""header"", locationName: ""x-amz-object-lock-legal-hold"" } } } }, ListBucketAnalyticsConfigurations: { http: { method: ""GET"", requestUri: ""/{Bucket}?analytics"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContinuationToken: { location: ""querystring"", locationName: ""continuation-token"" } } }, output: { type: ""structure"", members: { IsTruncated: { type: ""boolean"" }, ContinuationToken: {}, NextContinuationToken: {}, AnalyticsConfigurationList: { locationName: ""AnalyticsConfiguration"", type: ""list"", member: { shape: ""S3e"" }, flattened: true } } } }, ListBucketInventoryConfigurations: { http: { method: ""GET"", requestUri: ""/{Bucket}?inventory"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContinuationToken: { location: ""querystring"", locationName: ""continuation-token"" } } }, output: { type: ""structure"", members: { ContinuationToken: {}, InventoryConfigurationList: { locationName: ""InventoryConfiguration"", type: ""list"", member: { shape: ""S4d"" }, flattened: true }, IsTruncated: { type: ""boolean"" }, NextContinuationToken: {} } } }, ListBucketMetricsConfigurations: { http: { method: ""GET"", requestUri: ""/{Bucket}?metrics"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContinuationToken: { location: ""querystring"", locationName: ""continuation-token"" } } }, output: { type: ""structure"", members: { IsTruncated: { type: ""boolean"" }, ContinuationToken: {}, NextContinuationToken: {}, MetricsConfigurationList: { locationName: ""MetricsConfiguration"", type: ""list"", member: { shape: ""S5q"" }, flattened: true } } } }, ListBuckets: { http: { method: ""GET"" }, output: { type: ""structure"", members: { Buckets: { type: ""list"", member: { locationName: ""Bucket"", type: ""structure"", members: { Name: {}, CreationDate: { type: ""timestamp"" } } } }, Owner: { shape: ""S32"" } } }, alias: ""GetService"" }, ListMultipartUploads: { http: { method: ""GET"", requestUri: ""/{Bucket}?uploads"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Delimiter: { location: ""querystring"", locationName: ""delimiter"" }, EncodingType: { location: ""querystring"", locationName: ""encoding-type"" }, KeyMarker: { location: ""querystring"", locationName: ""key-marker"" }, MaxUploads: { location: ""querystring"", locationName: ""max-uploads"", type: ""integer"" }, Prefix: { location: ""querystring"", locationName: ""prefix"" }, UploadIdMarker: { location: ""querystring"", locationName: ""upload-id-marker"" } } }, output: { type: ""structure"", members: { Bucket: {}, KeyMarker: {}, UploadIdMarker: {}, NextKeyMarker: {}, Prefix: {}, Delimiter: {}, NextUploadIdMarker: {}, MaxUploads: { type: ""integer"" }, IsTruncated: { type: ""boolean"" }, Uploads: { locationName: ""Upload"", type: ""list"", member: { type: ""structure"", members: { UploadId: {}, Key: {}, Initiated: { type: ""timestamp"" }, StorageClass: {}, Owner: { shape: ""S32"" }, Initiator: { shape: ""Sa5"" } } }, flattened: true }, CommonPrefixes: { shape: ""Sa6"" }, EncodingType: {} } } }, ListObjectVersions: { http: { method: ""GET"", requestUri: ""/{Bucket}?versions"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Delimiter: { location: ""querystring"", locationName: ""delimiter"" }, EncodingType: { location: ""querystring"", locationName: ""encoding-type"" }, KeyMarker: { location: ""querystring"", locationName: ""key-marker"" }, MaxKeys: { location: ""querystring"", locationName: ""max-keys"", type: ""integer"" }, Prefix: { location: ""querystring"", locationName: ""prefix"" }, VersionIdMarker: { location: ""querystring"", locationName: ""version-id-marker"" } } }, output: { type: ""structure"", members: { IsTruncated: { type: ""boolean"" }, KeyMarker: {}, VersionIdMarker: {}, NextKeyMarker: {}, NextVersionIdMarker: {}, Versions: { locationName: ""Version"", type: ""list"", member: { type: ""structure"", members: { ETag: {}, Size: { type: ""integer"" }, StorageClass: {}, Key: {}, VersionId: {}, IsLatest: { type: ""boolean"" }, LastModified: { type: ""timestamp"" }, Owner: { shape: ""S32"" } } }, flattened: true }, DeleteMarkers: { locationName: ""DeleteMarker"", type: ""list"", member: { type: ""structure"", members: { Owner: { shape: ""S32"" }, Key: {}, VersionId: {}, IsLatest: { type: ""boolean"" }, LastModified: { type: ""timestamp"" } } }, flattened: true }, Name: {}, Prefix: {}, Delimiter: {}, MaxKeys: { type: ""integer"" }, CommonPrefixes: { shape: ""Sa6"" }, EncodingType: {} } }, alias: ""GetBucketObjectVersions"" }, ListObjects: { http: { method: ""GET"", requestUri: ""/{Bucket}"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Delimiter: { location: ""querystring"", locationName: ""delimiter"" }, EncodingType: { location: ""querystring"", locationName: ""encoding-type"" }, Marker: { location: ""querystring"", locationName: ""marker"" }, MaxKeys: { location: ""querystring"", locationName: ""max-keys"", type: ""integer"" }, Prefix: { location: ""querystring"", locationName: ""prefix"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" } } }, output: { type: ""structure"", members: { IsTruncated: { type: ""boolean"" }, Marker: {}, NextMarker: {}, Contents: { shape: ""Sao"" }, Name: {}, Prefix: {}, Delimiter: {}, MaxKeys: { type: ""integer"" }, CommonPrefixes: { shape: ""Sa6"" }, EncodingType: {} } }, alias: ""GetBucket"" }, ListObjectsV2: { http: { method: ""GET"", requestUri: ""/{Bucket}?list-type=2"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Delimiter: { location: ""querystring"", locationName: ""delimiter"" }, EncodingType: { location: ""querystring"", locationName: ""encoding-type"" }, MaxKeys: { location: ""querystring"", locationName: ""max-keys"", type: ""integer"" }, Prefix: { location: ""querystring"", locationName: ""prefix"" }, ContinuationToken: { location: ""querystring"", locationName: ""continuation-token"" }, FetchOwner: { location: ""querystring"", locationName: ""fetch-owner"", type: ""boolean"" }, StartAfter: { location: ""querystring"", locationName: ""start-after"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" } } }, output: { type: ""structure"", members: { IsTruncated: { type: ""boolean"" }, Contents: { shape: ""Sao"" }, Name: {}, Prefix: {}, Delimiter: {}, MaxKeys: { type: ""integer"" }, CommonPrefixes: { shape: ""Sa6"" }, EncodingType: {}, KeyCount: { type: ""integer"" }, ContinuationToken: {}, NextContinuationToken: {}, StartAfter: {} } } }, ListParts: { http: { method: ""GET"", requestUri: ""/{Bucket}/{Key+}"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"", ""UploadId"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, MaxParts: { location: ""querystring"", locationName: ""max-parts"", type: ""integer"" }, PartNumberMarker: { location: ""querystring"", locationName: ""part-number-marker"", type: ""integer"" }, UploadId: { location: ""querystring"", locationName: ""uploadId"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" } } }, output: { type: ""structure"", members: { AbortDate: { location: ""header"", locationName: ""x-amz-abort-date"", type: ""timestamp"" }, AbortRuleId: { location: ""header"", locationName: ""x-amz-abort-rule-id"" }, Bucket: {}, Key: {}, UploadId: {}, PartNumberMarker: { type: ""integer"" }, NextPartNumberMarker: { type: ""integer"" }, MaxParts: { type: ""integer"" }, IsTruncated: { type: ""boolean"" }, Parts: { locationName: ""Part"", type: ""list"", member: { type: ""structure"", members: { PartNumber: { type: ""integer"" }, LastModified: { type: ""timestamp"" }, ETag: {}, Size: { type: ""integer"" } } }, flattened: true }, Initiator: { shape: ""Sa5"" }, Owner: { shape: ""S32"" }, StorageClass: {}, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } } } }, PutBucketAccelerateConfiguration: { http: { method: ""PUT"", requestUri: ""/{Bucket}?accelerate"" }, input: { type: ""structure"", required: [ ""Bucket"", ""AccelerateConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, AccelerateConfiguration: { locationName: ""AccelerateConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", members: { Status: {} } } }, payload: ""AccelerateConfiguration"" } }, PutBucketAcl: { http: { method: ""PUT"", requestUri: ""/{Bucket}?acl"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { ACL: { location: ""header"", locationName: ""x-amz-acl"" }, AccessControlPolicy: { shape: ""Sb6"", locationName: ""AccessControlPolicy"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } }, Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, GrantFullControl: { location: ""header"", locationName: ""x-amz-grant-full-control"" }, GrantRead: { location: ""header"", locationName: ""x-amz-grant-read"" }, GrantReadACP: { location: ""header"", locationName: ""x-amz-grant-read-acp"" }, GrantWrite: { location: ""header"", locationName: ""x-amz-grant-write"" }, GrantWriteACP: { location: ""header"", locationName: ""x-amz-grant-write-acp"" } }, payload: ""AccessControlPolicy"" } }, PutBucketAnalyticsConfiguration: { http: { method: ""PUT"", requestUri: ""/{Bucket}?analytics"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Id"", ""AnalyticsConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Id: { location: ""querystring"", locationName: ""id"" }, AnalyticsConfiguration: { shape: ""S3e"", locationName: ""AnalyticsConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } } }, payload: ""AnalyticsConfiguration"" } }, PutBucketCors: { http: { method: ""PUT"", requestUri: ""/{Bucket}?cors"" }, input: { type: ""structure"", required: [ ""Bucket"", ""CORSConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, CORSConfiguration: { locationName: ""CORSConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", required: [ ""CORSRules"" ], members: { CORSRules: { shape: ""S3u"", locationName: ""CORSRule"" } } }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" } }, payload: ""CORSConfiguration"" } }, PutBucketEncryption: { http: { method: ""PUT"", requestUri: ""/{Bucket}?encryption"" }, input: { type: ""structure"", required: [ ""Bucket"", ""ServerSideEncryptionConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, ServerSideEncryptionConfiguration: { shape: ""S47"", locationName: ""ServerSideEncryptionConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } } }, payload: ""ServerSideEncryptionConfiguration"" } }, PutBucketInventoryConfiguration: { http: { method: ""PUT"", requestUri: ""/{Bucket}?inventory"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Id"", ""InventoryConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Id: { location: ""querystring"", locationName: ""id"" }, InventoryConfiguration: { shape: ""S4d"", locationName: ""InventoryConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } } }, payload: ""InventoryConfiguration"" } }, PutBucketLifecycle: { http: { method: ""PUT"", requestUri: ""/{Bucket}?lifecycle"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, LifecycleConfiguration: { locationName: ""LifecycleConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", required: [ ""Rules"" ], members: { Rules: { shape: ""S4t"", locationName: ""Rule"" } } } }, payload: ""LifecycleConfiguration"" }, deprecated: true }, PutBucketLifecycleConfiguration: { http: { method: ""PUT"", requestUri: ""/{Bucket}?lifecycle"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, LifecycleConfiguration: { locationName: ""LifecycleConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", required: [ ""Rules"" ], members: { Rules: { shape: ""S58"", locationName: ""Rule"" } } } }, payload: ""LifecycleConfiguration"" } }, PutBucketLogging: { http: { method: ""PUT"", requestUri: ""/{Bucket}?logging"" }, input: { type: ""structure"", required: [ ""Bucket"", ""BucketLoggingStatus"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, BucketLoggingStatus: { locationName: ""BucketLoggingStatus"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", members: { LoggingEnabled: { shape: ""S5i"" } } }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" } }, payload: ""BucketLoggingStatus"" } }, PutBucketMetricsConfiguration: { http: { method: ""PUT"", requestUri: ""/{Bucket}?metrics"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Id"", ""MetricsConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Id: { location: ""querystring"", locationName: ""id"" }, MetricsConfiguration: { shape: ""S5q"", locationName: ""MetricsConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } } }, payload: ""MetricsConfiguration"" } }, PutBucketNotification: { http: { method: ""PUT"", requestUri: ""/{Bucket}?notification"" }, input: { type: ""structure"", required: [ ""Bucket"", ""NotificationConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, NotificationConfiguration: { shape: ""S5u"", locationName: ""NotificationConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } } }, payload: ""NotificationConfiguration"" }, deprecated: true }, PutBucketNotificationConfiguration: { http: { method: ""PUT"", requestUri: ""/{Bucket}?notification"" }, input: { type: ""structure"", required: [ ""Bucket"", ""NotificationConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, NotificationConfiguration: { shape: ""S65"", locationName: ""NotificationConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } } }, payload: ""NotificationConfiguration"" } }, PutBucketPolicy: { http: { method: ""PUT"", requestUri: ""/{Bucket}?policy"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Policy"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, ConfirmRemoveSelfBucketAccess: { location: ""header"", locationName: ""x-amz-confirm-remove-self-bucket-access"", type: ""boolean"" }, Policy: {} }, payload: ""Policy"" } }, PutBucketReplication: { http: { method: ""PUT"", requestUri: ""/{Bucket}?replication"" }, input: { type: ""structure"", required: [ ""Bucket"", ""ReplicationConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, ReplicationConfiguration: { shape: ""S6s"", locationName: ""ReplicationConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } }, Token: { location: ""header"", locationName: ""x-amz-bucket-object-lock-token"" } }, payload: ""ReplicationConfiguration"" } }, PutBucketRequestPayment: { http: { method: ""PUT"", requestUri: ""/{Bucket}?requestPayment"" }, input: { type: ""structure"", required: [ ""Bucket"", ""RequestPaymentConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, RequestPaymentConfiguration: { locationName: ""RequestPaymentConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", required: [ ""Payer"" ], members: { Payer: {} } } }, payload: ""RequestPaymentConfiguration"" } }, PutBucketTagging: { http: { method: ""PUT"", requestUri: ""/{Bucket}?tagging"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Tagging"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, Tagging: { shape: ""Sbt"", locationName: ""Tagging"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } } }, payload: ""Tagging"" } }, PutBucketVersioning: { http: { method: ""PUT"", requestUri: ""/{Bucket}?versioning"" }, input: { type: ""structure"", required: [ ""Bucket"", ""VersioningConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, MFA: { location: ""header"", locationName: ""x-amz-mfa"" }, VersioningConfiguration: { locationName: ""VersioningConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", members: { MFADelete: { locationName: ""MfaDelete"" }, Status: {} } } }, payload: ""VersioningConfiguration"" } }, PutBucketWebsite: { http: { method: ""PUT"", requestUri: ""/{Bucket}?website"" }, input: { type: ""structure"", required: [ ""Bucket"", ""WebsiteConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, WebsiteConfiguration: { locationName: ""WebsiteConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", members: { ErrorDocument: { shape: ""S7q"" }, IndexDocument: { shape: ""S7o"" }, RedirectAllRequestsTo: { shape: ""S7l"" }, RoutingRules: { shape: ""S7r"" } } } }, payload: ""WebsiteConfiguration"" } }, PutObject: { http: { method: ""PUT"", requestUri: ""/{Bucket}/{Key+}"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { ACL: { location: ""header"", locationName: ""x-amz-acl"" }, Body: { streaming: true, type: ""blob"" }, Bucket: { location: ""uri"", locationName: ""Bucket"" }, CacheControl: { location: ""header"", locationName: ""Cache-Control"" }, ContentDisposition: { location: ""header"", locationName: ""Content-Disposition"" }, ContentEncoding: { location: ""header"", locationName: ""Content-Encoding"" }, ContentLanguage: { location: ""header"", locationName: ""Content-Language"" }, ContentLength: { location: ""header"", locationName: ""Content-Length"", type: ""long"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, ContentType: { location: ""header"", locationName: ""Content-Type"" }, Expires: { location: ""header"", locationName: ""Expires"", type: ""timestamp"" }, GrantFullControl: { location: ""header"", locationName: ""x-amz-grant-full-control"" }, GrantRead: { location: ""header"", locationName: ""x-amz-grant-read"" }, GrantReadACP: { location: ""header"", locationName: ""x-amz-grant-read-acp"" }, GrantWriteACP: { location: ""header"", locationName: ""x-amz-grant-write-acp"" }, Key: { location: ""uri"", locationName: ""Key"" }, Metadata: { shape: ""S11"", location: ""headers"", locationName: ""x-amz-meta-"" }, ServerSideEncryption: { location: ""header"", locationName: ""x-amz-server-side-encryption"" }, StorageClass: { location: ""header"", locationName: ""x-amz-storage-class"" }, WebsiteRedirectLocation: { location: ""header"", locationName: ""x-amz-website-redirect-location"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKey: { shape: ""S19"", location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, SSEKMSKeyId: { shape: ""Sj"", location: ""header"", locationName: ""x-amz-server-side-encryption-aws-kms-key-id"" }, SSEKMSEncryptionContext: { shape: ""S1b"", location: ""header"", locationName: ""x-amz-server-side-encryption-context"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" }, Tagging: { location: ""header"", locationName: ""x-amz-tagging"" }, ObjectLockMode: { location: ""header"", locationName: ""x-amz-object-lock-mode"" }, ObjectLockRetainUntilDate: { shape: ""S1h"", location: ""header"", locationName: ""x-amz-object-lock-retain-until-date"" }, ObjectLockLegalHoldStatus: { location: ""header"", locationName: ""x-amz-object-lock-legal-hold"" } }, payload: ""Body"" }, output: { type: ""structure"", members: { Expiration: { location: ""header"", locationName: ""x-amz-expiration"" }, ETag: { location: ""header"", locationName: ""ETag"" }, ServerSideEncryption: { location: ""header"", locationName: ""x-amz-server-side-encryption"" }, VersionId: { location: ""header"", locationName: ""x-amz-version-id"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, SSEKMSKeyId: { shape: ""Sj"", location: ""header"", locationName: ""x-amz-server-side-encryption-aws-kms-key-id"" }, SSEKMSEncryptionContext: { shape: ""S1b"", location: ""header"", locationName: ""x-amz-server-side-encryption-context"" }, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } } } }, PutObjectAcl: { http: { method: ""PUT"", requestUri: ""/{Bucket}/{Key+}?acl"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { ACL: { location: ""header"", locationName: ""x-amz-acl"" }, AccessControlPolicy: { shape: ""Sb6"", locationName: ""AccessControlPolicy"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } }, Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, GrantFullControl: { location: ""header"", locationName: ""x-amz-grant-full-control"" }, GrantRead: { location: ""header"", locationName: ""x-amz-grant-read"" }, GrantReadACP: { location: ""header"", locationName: ""x-amz-grant-read-acp"" }, GrantWrite: { location: ""header"", locationName: ""x-amz-grant-write"" }, GrantWriteACP: { location: ""header"", locationName: ""x-amz-grant-write-acp"" }, Key: { location: ""uri"", locationName: ""Key"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" } }, payload: ""AccessControlPolicy"" }, output: { type: ""structure"", members: { RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } } } }, PutObjectLegalHold: { http: { method: ""PUT"", requestUri: ""/{Bucket}/{Key+}?legal-hold"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, LegalHold: { shape: ""S8q"", locationName: ""LegalHold"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" } }, payload: ""LegalHold"" }, output: { type: ""structure"", members: { RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } } } }, PutObjectLockConfiguration: { http: { method: ""PUT"", requestUri: ""/{Bucket}?object-lock"" }, input: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ObjectLockConfiguration: { shape: ""S8t"", locationName: ""ObjectLockConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" }, Token: { location: ""header"", locationName: ""x-amz-bucket-object-lock-token"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" } }, payload: ""ObjectLockConfiguration"" }, output: { type: ""structure"", members: { RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } } } }, PutObjectRetention: { http: { method: ""PUT"", requestUri: ""/{Bucket}/{Key+}?retention"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, Retention: { shape: ""S91"", locationName: ""Retention"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" }, BypassGovernanceRetention: { location: ""header"", locationName: ""x-amz-bypass-governance-retention"", type: ""boolean"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" } }, payload: ""Retention"" }, output: { type: ""structure"", members: { RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } } } }, PutObjectTagging: { http: { method: ""PUT"", requestUri: ""/{Bucket}/{Key+}?tagging"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"", ""Tagging"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, Tagging: { shape: ""Sbt"", locationName: ""Tagging"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } } }, payload: ""Tagging"" }, output: { type: ""structure"", members: { VersionId: { location: ""header"", locationName: ""x-amz-version-id"" } } } }, PutPublicAccessBlock: { http: { method: ""PUT"", requestUri: ""/{Bucket}?publicAccessBlock"" }, input: { type: ""structure"", required: [ ""Bucket"", ""PublicAccessBlockConfiguration"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, PublicAccessBlockConfiguration: { shape: ""S98"", locationName: ""PublicAccessBlockConfiguration"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" } } }, payload: ""PublicAccessBlockConfiguration"" } }, RestoreObject: { http: { requestUri: ""/{Bucket}/{Key+}?restore"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, VersionId: { location: ""querystring"", locationName: ""versionId"" }, RestoreRequest: { locationName: ""RestoreRequest"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", members: { Days: { type: ""integer"" }, GlacierJobParameters: { type: ""structure"", required: [ ""Tier"" ], members: { Tier: {} } }, Type: {}, Tier: {}, Description: {}, SelectParameters: { type: ""structure"", required: [ ""InputSerialization"", ""ExpressionType"", ""Expression"", ""OutputSerialization"" ], members: { InputSerialization: { shape: ""Scj"" }, ExpressionType: {}, Expression: {}, OutputSerialization: { shape: ""Scy"" } } }, OutputLocation: { type: ""structure"", members: { S3: { type: ""structure"", required: [ ""BucketName"", ""Prefix"" ], members: { BucketName: {}, Prefix: {}, Encryption: { type: ""structure"", required: [ ""EncryptionType"" ], members: { EncryptionType: {}, KMSKeyId: { shape: ""Sj"" }, KMSContext: {} } }, CannedACL: {}, AccessControlList: { shape: ""S35"" }, Tagging: { shape: ""Sbt"" }, UserMetadata: { type: ""list"", member: { locationName: ""MetadataEntry"", type: ""structure"", members: { Name: {}, Value: {} } } }, StorageClass: {} } } } } } }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" } }, payload: ""RestoreRequest"" }, output: { type: ""structure"", members: { RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" }, RestoreOutputPath: { location: ""header"", locationName: ""x-amz-restore-output-path"" } } }, alias: ""PostObjectRestore"" }, SelectObjectContent: { http: { requestUri: ""/{Bucket}/{Key+}?select&select-type=2"" }, input: { locationName: ""SelectObjectContentRequest"", xmlNamespace: { uri: ""http://s3.amazonaws.com/doc/2006-03-01/"" }, type: ""structure"", required: [ ""Bucket"", ""Key"", ""Expression"", ""ExpressionType"", ""InputSerialization"", ""OutputSerialization"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, Key: { location: ""uri"", locationName: ""Key"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKey: { shape: ""S19"", location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, Expression: {}, ExpressionType: {}, RequestProgress: { type: ""structure"", members: { Enabled: { type: ""boolean"" } } }, InputSerialization: { shape: ""Scj"" }, OutputSerialization: { shape: ""Scy"" } } }, output: { type: ""structure"", members: { Payload: { type: ""structure"", members: { Records: { type: ""structure"", members: { Payload: { eventpayload: true, type: ""blob"" } }, event: true }, Stats: { type: ""structure"", members: { Details: { eventpayload: true, type: ""structure"", members: { BytesScanned: { type: ""long"" }, BytesProcessed: { type: ""long"" }, BytesReturned: { type: ""long"" } } } }, event: true }, Progress: { type: ""structure"", members: { Details: { eventpayload: true, type: ""structure"", members: { BytesScanned: { type: ""long"" }, BytesProcessed: { type: ""long"" }, BytesReturned: { type: ""long"" } } } }, event: true }, Cont: { type: ""structure"", members: {}, event: true }, End: { type: ""structure"", members: {}, event: true } }, eventstream: true } }, payload: ""Payload"" } }, UploadPart: { http: { method: ""PUT"", requestUri: ""/{Bucket}/{Key+}"" }, input: { type: ""structure"", required: [ ""Bucket"", ""Key"", ""PartNumber"", ""UploadId"" ], members: { Body: { streaming: true, type: ""blob"" }, Bucket: { location: ""uri"", locationName: ""Bucket"" }, ContentLength: { location: ""header"", locationName: ""Content-Length"", type: ""long"" }, ContentMD5: { location: ""header"", locationName: ""Content-MD5"" }, Key: { location: ""uri"", locationName: ""Key"" }, PartNumber: { location: ""querystring"", locationName: ""partNumber"", type: ""integer"" }, UploadId: { location: ""querystring"", locationName: ""uploadId"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKey: { shape: ""S19"", location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" } }, payload: ""Body"" }, output: { type: ""structure"", members: { ServerSideEncryption: { location: ""header"", locationName: ""x-amz-server-side-encryption"" }, ETag: { location: ""header"", locationName: ""ETag"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, SSEKMSKeyId: { shape: ""Sj"", location: ""header"", locationName: ""x-amz-server-side-encryption-aws-kms-key-id"" }, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } } } }, UploadPartCopy: { http: { method: ""PUT"", requestUri: ""/{Bucket}/{Key+}"" }, input: { type: ""structure"", required: [ ""Bucket"", ""CopySource"", ""Key"", ""PartNumber"", ""UploadId"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" }, CopySource: { location: ""header"", locationName: ""x-amz-copy-source"" }, CopySourceIfMatch: { location: ""header"", locationName: ""x-amz-copy-source-if-match"" }, CopySourceIfModifiedSince: { location: ""header"", locationName: ""x-amz-copy-source-if-modified-since"", type: ""timestamp"" }, CopySourceIfNoneMatch: { location: ""header"", locationName: ""x-amz-copy-source-if-none-match"" }, CopySourceIfUnmodifiedSince: { location: ""header"", locationName: ""x-amz-copy-source-if-unmodified-since"", type: ""timestamp"" }, CopySourceRange: { location: ""header"", locationName: ""x-amz-copy-source-range"" }, Key: { location: ""uri"", locationName: ""Key"" }, PartNumber: { location: ""querystring"", locationName: ""partNumber"", type: ""integer"" }, UploadId: { location: ""querystring"", locationName: ""uploadId"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKey: { shape: ""S19"", location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, CopySourceSSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-copy-source-server-side-encryption-customer-algorithm"" }, CopySourceSSECustomerKey: { shape: ""S1d"", location: ""header"", locationName: ""x-amz-copy-source-server-side-encryption-customer-key"" }, CopySourceSSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-copy-source-server-side-encryption-customer-key-MD5"" }, RequestPayer: { location: ""header"", locationName: ""x-amz-request-payer"" } } }, output: { type: ""structure"", members: { CopySourceVersionId: { location: ""header"", locationName: ""x-amz-copy-source-version-id"" }, CopyPartResult: { type: ""structure"", members: { ETag: {}, LastModified: { type: ""timestamp"" } } }, ServerSideEncryption: { location: ""header"", locationName: ""x-amz-server-side-encryption"" }, SSECustomerAlgorithm: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-algorithm"" }, SSECustomerKeyMD5: { location: ""header"", locationName: ""x-amz-server-side-encryption-customer-key-MD5"" }, SSEKMSKeyId: { shape: ""Sj"", location: ""header"", locationName: ""x-amz-server-side-encryption-aws-kms-key-id"" }, RequestCharged: { location: ""header"", locationName: ""x-amz-request-charged"" } }, payload: ""CopyPartResult"" } } }, shapes: { Sj: { type: ""string"", sensitive: true }, S11: { type: ""map"", key: {}, value: {} }, S19: { type: ""blob"", sensitive: true }, S1b: { type: ""string"", sensitive: true }, S1d: { type: ""blob"", sensitive: true }, S1h: { type: ""timestamp"", timestampFormat: ""iso8601"" }, S32: { type: ""structure"", members: { DisplayName: {}, ID: {} } }, S35: { type: ""list"", member: { locationName: ""Grant"", type: ""structure"", members: { Grantee: { shape: ""S37"" }, Permission: {} } } }, S37: { type: ""structure"", required: [ ""Type"" ], members: { DisplayName: {}, EmailAddress: {}, ID: {}, Type: { locationName: ""xsi:type"", xmlAttribute: true }, URI: {} }, xmlNamespace: { prefix: ""xsi"", uri: ""http://www.w3.org/2001/XMLSchema-instance"" } }, S3e: { type: ""structure"", required: [ ""Id"", ""StorageClassAnalysis"" ], members: { Id: {}, Filter: { type: ""structure"", members: { Prefix: {}, Tag: { shape: ""S3h"" }, And: { type: ""structure"", members: { Prefix: {}, Tags: { shape: ""S3k"", flattened: true, locationName: ""Tag"" } } } } }, StorageClassAnalysis: { type: ""structure"", members: { DataExport: { type: ""structure"", required: [ ""OutputSchemaVersion"", ""Destination"" ], members: { OutputSchemaVersion: {}, Destination: { type: ""structure"", required: [ ""S3BucketDestination"" ], members: { S3BucketDestination: { type: ""structure"", required: [ ""Format"", ""Bucket"" ], members: { Format: {}, BucketAccountId: {}, Bucket: {}, Prefix: {} } } } } } } } } } }, S3h: { type: ""structure"", required: [ ""Key"", ""Value"" ], members: { Key: {}, Value: {} } }, S3k: { type: ""list"", member: { shape: ""S3h"", locationName: ""Tag"" } }, S3u: { type: ""list"", member: { type: ""structure"", required: [ ""AllowedMethods"", ""AllowedOrigins"" ], members: { AllowedHeaders: { locationName: ""AllowedHeader"", type: ""list"", member: {}, flattened: true }, AllowedMethods: { locationName: ""AllowedMethod"", type: ""list"", member: {}, flattened: true }, AllowedOrigins: { locationName: ""AllowedOrigin"", type: ""list"", member: {}, flattened: true }, ExposeHeaders: { locationName: ""ExposeHeader"", type: ""list"", member: {}, flattened: true }, MaxAgeSeconds: { type: ""integer"" } } }, flattened: true }, S47: { type: ""structure"", required: [ ""Rules"" ], members: { Rules: { locationName: ""Rule"", type: ""list"", member: { type: ""structure"", members: { ApplyServerSideEncryptionByDefault: { type: ""structure"", required: [ ""SSEAlgorithm"" ], members: { SSEAlgorithm: {}, KMSMasterKeyID: { shape: ""Sj"" } } } } }, flattened: true } } }, S4d: { type: ""structure"", required: [ ""Destination"", ""IsEnabled"", ""Id"", ""IncludedObjectVersions"", ""Schedule"" ], members: { Destination: { type: ""structure"", required: [ ""S3BucketDestination"" ], members: { S3BucketDestination: { type: ""structure"", required: [ ""Bucket"", ""Format"" ], members: { AccountId: {}, Bucket: {}, Format: {}, Prefix: {}, Encryption: { type: ""structure"", members: { SSES3: { locationName: ""SSE-S3"", type: ""structure"", members: {} }, SSEKMS: { locationName: ""SSE-KMS"", type: ""structure"", required: [ ""KeyId"" ], members: { KeyId: { shape: ""Sj"" } } } } } } } } }, IsEnabled: { type: ""boolean"" }, Filter: { type: ""structure"", required: [ ""Prefix"" ], members: { Prefix: {} } }, Id: {}, IncludedObjectVersions: {}, OptionalFields: { type: ""list"", member: { locationName: ""Field"" } }, Schedule: { type: ""structure"", required: [ ""Frequency"" ], members: { Frequency: {} } } } }, S4t: { type: ""list"", member: { type: ""structure"", required: [ ""Prefix"", ""Status"" ], members: { Expiration: { shape: ""S4v"" }, ID: {}, Prefix: {}, Status: {}, Transition: { shape: ""S50"" }, NoncurrentVersionTransition: { shape: ""S52"" }, NoncurrentVersionExpiration: { shape: ""S53"" }, AbortIncompleteMultipartUpload: { shape: ""S54"" } } }, flattened: true }, S4v: { type: ""structure"", members: { Date: { shape: ""S4w"" }, Days: { type: ""integer"" }, ExpiredObjectDeleteMarker: { type: ""boolean"" } } }, S4w: { type: ""timestamp"", timestampFormat: ""iso8601"" }, S50: { type: ""structure"", members: { Date: { shape: ""S4w"" }, Days: { type: ""integer"" }, StorageClass: {} } }, S52: { type: ""structure"", members: { NoncurrentDays: { type: ""integer"" }, StorageClass: {} } }, S53: { type: ""structure"", members: { NoncurrentDays: { type: ""integer"" } } }, S54: { type: ""structure"", members: { DaysAfterInitiation: { type: ""integer"" } } }, S58: { type: ""list"", member: { type: ""structure"", required: [ ""Status"" ], members: { Expiration: { shape: ""S4v"" }, ID: {}, Prefix: { deprecated: true }, Filter: { type: ""structure"", members: { Prefix: {}, Tag: { shape: ""S3h"" }, And: { type: ""structure"", members: { Prefix: {}, Tags: { shape: ""S3k"", flattened: true, locationName: ""Tag"" } } } } }, Status: {}, Transitions: { locationName: ""Transition"", type: ""list"", member: { shape: ""S50"" }, flattened: true }, NoncurrentVersionTransitions: { locationName: ""NoncurrentVersionTransition"", type: ""list"", member: { shape: ""S52"" }, flattened: true }, NoncurrentVersionExpiration: { shape: ""S53"" }, AbortIncompleteMultipartUpload: { shape: ""S54"" } } }, flattened: true }, S5i: { type: ""structure"", required: [ ""TargetBucket"", ""TargetPrefix"" ], members: { TargetBucket: {}, TargetGrants: { type: ""list"", member: { locationName: ""Grant"", type: ""structure"", members: { Grantee: { shape: ""S37"" }, Permission: {} } } }, TargetPrefix: {} } }, S5q: { type: ""structure"", required: [ ""Id"" ], members: { Id: {}, Filter: { type: ""structure"", members: { Prefix: {}, Tag: { shape: ""S3h"" }, And: { type: ""structure"", members: { Prefix: {}, Tags: { shape: ""S3k"", flattened: true, locationName: ""Tag"" } } } } } } }, S5t: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: { location: ""uri"", locationName: ""Bucket"" } } }, S5u: { type: ""structure"", members: { TopicConfiguration: { type: ""structure"", members: { Id: {}, Events: { shape: ""S5x"", locationName: ""Event"" }, Event: { deprecated: true }, Topic: {} } }, QueueConfiguration: { type: ""structure"", members: { Id: {}, Event: { deprecated: true }, Events: { shape: ""S5x"", locationName: ""Event"" }, Queue: {} } }, CloudFunctionConfiguration: { type: ""structure"", members: { Id: {}, Event: { deprecated: true }, Events: { shape: ""S5x"", locationName: ""Event"" }, CloudFunction: {}, InvocationRole: {} } } } }, S5x: { type: ""list"", member: {}, flattened: true }, S65: { type: ""structure"", members: { TopicConfigurations: { locationName: ""TopicConfiguration"", type: ""list"", member: { type: ""structure"", required: [ ""TopicArn"", ""Events"" ], members: { Id: {}, TopicArn: { locationName: ""Topic"" }, Events: { shape: ""S5x"", locationName: ""Event"" }, Filter: { shape: ""S68"" } } }, flattened: true }, QueueConfigurations: { locationName: ""QueueConfiguration"", type: ""list"", member: { type: ""structure"", required: [ ""QueueArn"", ""Events"" ], members: { Id: {}, QueueArn: { locationName: ""Queue"" }, Events: { shape: ""S5x"", locationName: ""Event"" }, Filter: { shape: ""S68"" } } }, flattened: true }, LambdaFunctionConfigurations: { locationName: ""CloudFunctionConfiguration"", type: ""list"", member: { type: ""structure"", required: [ ""LambdaFunctionArn"", ""Events"" ], members: { Id: {}, LambdaFunctionArn: { locationName: ""CloudFunction"" }, Events: { shape: ""S5x"", locationName: ""Event"" }, Filter: { shape: ""S68"" } } }, flattened: true } } }, S68: { type: ""structure"", members: { Key: { locationName: ""S3Key"", type: ""structure"", members: { FilterRules: { locationName: ""FilterRule"", type: ""list"", member: { type: ""structure"", members: { Name: {}, Value: {} } }, flattened: true } } } } }, S6s: { type: ""structure"", required: [ ""Role"", ""Rules"" ], members: { Role: {}, Rules: { locationName: ""Rule"", type: ""list"", member: { type: ""structure"", required: [ ""Status"", ""Destination"" ], members: { ID: {}, Priority: { type: ""integer"" }, Prefix: { deprecated: true }, Filter: { type: ""structure"", members: { Prefix: {}, Tag: { shape: ""S3h"" }, And: { type: ""structure"", members: { Prefix: {}, Tags: { shape: ""S3k"", flattened: true, locationName: ""Tag"" } } } } }, Status: {}, SourceSelectionCriteria: { type: ""structure"", members: { SseKmsEncryptedObjects: { type: ""structure"", required: [ ""Status"" ], members: { Status: {} } } } }, Destination: { type: ""structure"", required: [ ""Bucket"" ], members: { Bucket: {}, Account: {}, StorageClass: {}, AccessControlTranslation: { type: ""structure"", required: [ ""Owner"" ], members: { Owner: {} } }, EncryptionConfiguration: { type: ""structure"", members: { ReplicaKmsKeyID: {} } } } }, DeleteMarkerReplication: { type: ""structure"", members: { Status: {} } } } }, flattened: true } } }, S7l: { type: ""structure"", required: [ ""HostName"" ], members: { HostName: {}, Protocol: {} } }, S7o: { type: ""structure"", required: [ ""Suffix"" ], members: { Suffix: {} } }, S7q: { type: ""structure"", required: [ ""Key"" ], members: { Key: {} } }, S7r: { type: ""list"", member: { locationName: ""RoutingRule"", type: ""structure"", required: [ ""Redirect"" ], members: { Condition: { type: ""structure"", members: { HttpErrorCodeReturnedEquals: {}, KeyPrefixEquals: {} } }, Redirect: { type: ""structure"", members: { HostName: {}, HttpRedirectCode: {}, Protocol: {}, ReplaceKeyPrefixWith: {}, ReplaceKeyWith: {} } } } } }, S8q: { type: ""structure"", members: { Status: {} } }, S8t: { type: ""structure"", members: { ObjectLockEnabled: {}, Rule: { type: ""structure"", members: { DefaultRetention: { type: ""structure"", members: { Mode: {}, Days: { type: ""integer"" }, Years: { type: ""integer"" } } } } } } }, S91: { type: ""structure"", members: { Mode: {}, RetainUntilDate: { shape: ""S4w"" } } }, S98: { type: ""structure"", members: { BlockPublicAcls: { locationName: ""BlockPublicAcls"", type: ""boolean"" }, IgnorePublicAcls: { locationName: ""IgnorePublicAcls"", type: ""boolean"" }, BlockPublicPolicy: { locationName: ""BlockPublicPolicy"", type: ""boolean"" }, RestrictPublicBuckets: { locationName: ""RestrictPublicBuckets"", type: ""boolean"" } } }, Sa5: { type: ""structure"", members: { ID: {}, DisplayName: {} } }, Sa6: { type: ""list"", member: { type: ""structure"", members: { Prefix: {} } }, flattened: true }, Sao: { type: ""list"", member: { type: ""structure"", members: { Key: {}, LastModified: { type: ""timestamp"" }, ETag: {}, Size: { type: ""integer"" }, StorageClass: {}, Owner: { shape: ""S32"" } } }, flattened: true }, Sb6: { type: ""structure"", members: { Grants: { shape: ""S35"", locationName: ""AccessControlList"" }, Owner: { shape: ""S32"" } } }, Sbt: { type: ""structure"", required: [ ""TagSet"" ], members: { TagSet: { shape: ""S3k"" } } }, Scj: { type: ""structure"", members: { CSV: { type: ""structure"", members: { FileHeaderInfo: {}, Comments: {}, QuoteEscapeCharacter: {}, RecordDelimiter: {}, FieldDelimiter: {}, QuoteCharacter: {}, AllowQuotedRecordDelimiter: { type: ""boolean"" } } }, CompressionType: {}, JSON: { type: ""structure"", members: { Type: {} } }, Parquet: { type: ""structure"", members: {} } } }, Scy: { type: ""structure"", members: { CSV: { type: ""structure"", members: { QuoteFields: {}, QuoteEscapeCharacter: {}, RecordDelimiter: {}, FieldDelimiter: {}, QuoteCharacter: {} } }, JSON: { type: ""structure"", members: { RecordDelimiter: {} } } } } }, paginators: { ListBuckets: { result_key: ""Buckets"" }, ListMultipartUploads: { input_token: [ ""KeyMarker"", ""UploadIdMarker"" ], limit_key: ""MaxUploads"", more_results: ""IsTruncated"", output_token: [ ""NextKeyMarker"", ""NextUploadIdMarker"" ], result_key: [ ""Uploads"", ""CommonPrefixes"" ] }, ListObjectVersions: { input_token: [ ""KeyMarker"", ""VersionIdMarker"" ], limit_key: ""MaxKeys"", more_results: ""IsTruncated"", output_token: [ ""NextKeyMarker"", ""NextVersionIdMarker"" ], result_key: [ ""Versions"", ""DeleteMarkers"", ""CommonPrefixes"" ] }, ListObjects: { input_token: ""Marker"", limit_key: ""MaxKeys"", more_results: ""IsTruncated"", output_token: ""NextMarker || Contents[-1].Key"", result_key: [ ""Contents"", ""CommonPrefixes"" ] }, ListObjectsV2: { input_token: ""ContinuationToken"", limit_key: ""MaxKeys"", output_token: ""NextContinuationToken"", result_key: [ ""Contents"", ""CommonPrefixes"" ] }, ListParts: { input_token: ""PartNumberMarker"", limit_key: ""MaxParts"", more_results: ""IsTruncated"", output_token: ""NextPartNumberMarker"", result_key: ""Parts"" } }, waiters: { BucketExists: { delay: 5, operation: ""HeadBucket"", maxAttempts: 20, acceptors: [ { expected: 200, matcher: ""status"", state: ""success"" }, { expected: 301, matcher: ""status"", state: ""success"" }, { expected: 403, matcher: ""status"", state: ""success"" }, { expected: 404, matcher: ""status"", state: ""retry"" } ] }, BucketNotExists: { delay: 5, operation: ""HeadBucket"", maxAttempts: 20, acceptors: [ { expected: 404, matcher: ""status"", state: ""success"" } ] }, ObjectExists: { delay: 5, operation: ""HeadObject"", maxAttempts: 20, acceptors: [ { expected: 200, matcher: ""status"", state: ""success"" }, { expected: 404, matcher: ""status"", state: ""retry"" } ] }, ObjectNotExists: { delay: 5, operation: ""HeadObject"", maxAttempts: 20, acceptors: [ { expected: 404, matcher: ""status"", state: ""success"" } ] } } };AWS.apiLoader.services[""sts""] = {}; AWS.STS = AWS.Service.defineService(""sts"", [ ""2011-06-15"" ]); _xamzrequire = function e(t, n, r) { function s(o, u) { if (!n[o]) { if (!t[o]) { var a = typeof _xamzrequire == ""function"" && _xamzrequire; if (!u && a) return a(o, !0); if (i) return i(o, !0); var f = new Error(""Cannot find module '"" + o + ""'""); throw f.code = ""MODULE_NOT_FOUND"", f; } var l = n[o] = { exports: {} }; t[o][0].call(l.exports, function(e) { var n = t[o][1][e]; return s(n ? n : e); }, l, l.exports, e, t, n, r); } return n[o].exports; } var i = typeof _xamzrequire == ""function"" && _xamzrequire; for (var o = 0; o < r.length; o++) s(r[o]); return s; }({ 105: [ function(require, module, exports) { (function(process) { var AWS = require(""../core""); var regionConfig = require(""../region_config""); var ENV_REGIONAL_ENDPOINT_ENABLED = ""AWS_STS_REGIONAL_ENDPOINTS""; var CONFIG_REGIONAL_ENDPOINT_ENABLED = ""sts_regional_endpoints""; AWS.util.update(AWS.STS.prototype, { credentialsFrom: function credentialsFrom(data, credentials) { if (!data) return null; if (!credentials) credentials = new AWS.TemporaryCredentials(); credentials.expired = false; credentials.accessKeyId = data.Credentials.AccessKeyId; credentials.secretAccessKey = data.Credentials.SecretAccessKey; credentials.sessionToken = data.Credentials.SessionToken; credentials.expireTime = data.Credentials.Expiration; return credentials; }, assumeRoleWithWebIdentity: function assumeRoleWithWebIdentity(params, callback) { return this.makeUnauthenticatedRequest(""assumeRoleWithWebIdentity"", params, callback); }, assumeRoleWithSAML: function assumeRoleWithSAML(params, callback) { return this.makeUnauthenticatedRequest(""assumeRoleWithSAML"", params, callback); }, validateRegionalEndpointsFlagValue: function validateRegionalEndpointsFlagValue(configValue, errorOptions) { if (typeof configValue === ""string"" && [ ""legacy"", ""regional"" ].indexOf(configValue.toLowerCase()) >= 0) { this.config.stsRegionalEndpoints = configValue.toLowerCase(); return; } else { throw AWS.util.error(new Error(), errorOptions); } }, validateRegionalEndpointsFlag: function validateRegionalEndpointsFlag() { var config = this.config; if (config.stsRegionalEndpoints) { this.validateRegionalEndpointsFlagValue(config.stsRegionalEndpoints, { code: ""InvalidConfiguration"", message: 'invalid ""stsRegionalEndpoints"" configuration. Expect ""legacy"" ' + ' or ""regional"". Got ""' + config.stsRegionalEndpoints + '"".' }); } if (!AWS.util.isNode()) return; if (Object.prototype.hasOwnProperty.call(process.env, ENV_REGIONAL_ENDPOINT_ENABLED)) { var envFlag = process.env[ENV_REGIONAL_ENDPOINT_ENABLED]; this.validateRegionalEndpointsFlagValue(envFlag, { code: ""InvalidEnvironmentalVariable"", message: ""invalid "" + ENV_REGIONAL_ENDPOINT_ENABLED + ' environmental variable. Expect ""legacy"" ' + ' or ""regional"". Got ""' + process.env[ENV_REGIONAL_ENDPOINT_ENABLED] + '"".' }); } var profile = {}; try { var profiles = AWS.util.getProfilesFromSharedConfig(AWS.util.iniLoader); profile = profiles[process.env.AWS_PROFILE || AWS.util.defaultProfile]; } catch (e) {} if (profile && Object.prototype.hasOwnProperty.call(profile, CONFIG_REGIONAL_ENDPOINT_ENABLED)) { var fileFlag = profile[CONFIG_REGIONAL_ENDPOINT_ENABLED]; this.validateRegionalEndpointsFlagValue(fileFlag, { code: ""InvalidConfiguration"", message: ""invalid "" + CONFIG_REGIONAL_ENDPOINT_ENABLED + ' profile config. Expect ""legacy"" ' + ' or ""regional"". Got ""' + profile[CONFIG_REGIONAL_ENDPOINT_ENABLED] + '"".' }); } }, optInRegionalEndpoint: function optInRegionalEndpoint() { this.validateRegionalEndpointsFlag(); var config = this.config; if (config.stsRegionalEndpoints === ""regional"") { regionConfig(this); if (!this.isGlobalEndpoint) return; this.isGlobalEndpoint = false; if (!config.region) { throw AWS.util.error(new Error(), { code: ""ConfigError"", message: ""Missing region in config"" }); } var insertPoint = config.endpoint.indexOf("".amazonaws.com""); config.endpoint = config.endpoint.substring(0, insertPoint) + ""."" + config.region + config.endpoint.substring(insertPoint); } }, validateService: function validateService() { this.optInRegionalEndpoint(); } }); }).call(this, require(""_process"")); }, { ""../core"": 38, ""../region_config"": 81, _process: 8 } ] }, {}, [ 105 ]);AWS.apiLoader.services[""sts""][""2011-06-15""] = { version: ""2.0"", metadata: { apiVersion: ""2011-06-15"", endpointPrefix: ""sts"", globalEndpoint: ""sts.amazonaws.com"", protocol: ""query"", serviceAbbreviation: ""AWS STS"", serviceFullName: ""AWS Security Token Service"", serviceId: ""STS"", signatureVersion: ""v4"", uid: ""sts-2011-06-15"", xmlNamespace: ""https://sts.amazonaws.com/doc/2011-06-15/"" }, operations: { AssumeRole: { input: { type: ""structure"", required: [ ""RoleArn"", ""RoleSessionName"" ], members: { RoleArn: {}, RoleSessionName: {}, PolicyArns: { shape: ""S4"" }, Policy: {}, DurationSeconds: { type: ""integer"" }, ExternalId: {}, SerialNumber: {}, TokenCode: {} } }, output: { resultWrapper: ""AssumeRoleResult"", type: ""structure"", members: { Credentials: { shape: ""Sc"" }, AssumedRoleUser: { shape: ""Sh"" }, PackedPolicySize: { type: ""integer"" } } } }, AssumeRoleWithSAML: { input: { type: ""structure"", required: [ ""RoleArn"", ""PrincipalArn"", ""SAMLAssertion"" ], members: { RoleArn: {}, PrincipalArn: {}, SAMLAssertion: {}, PolicyArns: { shape: ""S4"" }, Policy: {}, DurationSeconds: { type: ""integer"" } } }, output: { resultWrapper: ""AssumeRoleWithSAMLResult"", type: ""structure"", members: { Credentials: { shape: ""Sc"" }, AssumedRoleUser: { shape: ""Sh"" }, PackedPolicySize: { type: ""integer"" }, Subject: {}, SubjectType: {}, Issuer: {}, Audience: {}, NameQualifier: {} } } }, AssumeRoleWithWebIdentity: { input: { type: ""structure"", required: [ ""RoleArn"", ""RoleSessionName"", ""WebIdentityToken"" ], members: { RoleArn: {}, RoleSessionName: {}, WebIdentityToken: {}, ProviderId: {}, PolicyArns: { shape: ""S4"" }, Policy: {}, DurationSeconds: { type: ""integer"" } } }, output: { resultWrapper: ""AssumeRoleWithWebIdentityResult"", type: ""structure"", members: { Credentials: { shape: ""Sc"" }, SubjectFromWebIdentityToken: {}, AssumedRoleUser: { shape: ""Sh"" }, PackedPolicySize: { type: ""integer"" }, Provider: {}, Audience: {} } } }, DecodeAuthorizationMessage: { input: { type: ""structure"", required: [ ""EncodedMessage"" ], members: { EncodedMessage: {} } }, output: { resultWrapper: ""DecodeAuthorizationMessageResult"", type: ""structure"", members: { DecodedMessage: {} } } }, GetAccessKeyInfo: { input: { type: ""structure"", required: [ ""AccessKeyId"" ], members: { AccessKeyId: {} } }, output: { resultWrapper: ""GetAccessKeyInfoResult"", type: ""structure"", members: { Account: {} } } }, GetCallerIdentity: { input: { type: ""structure"", members: {} }, output: { resultWrapper: ""GetCallerIdentityResult"", type: ""structure"", members: { UserId: {}, Account: {}, Arn: {} } } }, GetFederationToken: { input: { type: ""structure"", required: [ ""Name"" ], members: { Name: {}, Policy: {}, PolicyArns: { shape: ""S4"" }, DurationSeconds: { type: ""integer"" } } }, output: { resultWrapper: ""GetFederationTokenResult"", type: ""structure"", members: { Credentials: { shape: ""Sc"" }, FederatedUser: { type: ""structure"", required: [ ""FederatedUserId"", ""Arn"" ], members: { FederatedUserId: {}, Arn: {} } }, PackedPolicySize: { type: ""integer"" } } } }, GetSessionToken: { input: { type: ""structure"", members: { DurationSeconds: { type: ""integer"" }, SerialNumber: {}, TokenCode: {} } }, output: { resultWrapper: ""GetSessionTokenResult"", type: ""structure"", members: { Credentials: { shape: ""Sc"" } } } } }, shapes: { S4: { type: ""list"", member: { type: ""structure"", members: { arn: {} } } }, Sc: { type: ""structure"", required: [ ""AccessKeyId"", ""SecretAccessKey"", ""SessionToken"", ""Expiration"" ], members: { AccessKeyId: {}, SecretAccessKey: {}, SessionToken: {}, Expiration: { type: ""timestamp"" } } }, Sh: { type: ""structure"", required: [ ""AssumedRoleId"", ""Arn"" ], members: { AssumedRoleId: {}, Arn: {} } } }, paginators: {} }; ",,20196,0
openstack%2Fneutron~master~I544d82d7d0794450f3835d1af76bbaf3fd91f0ff,openstack/neutron,master,I544d82d7d0794450f3835d1af76bbaf3fd91f0ff,DevStack: do not overshadow the default of geneve max_header_size,ABANDONED,2020-03-20 08:46:43.000000000,2020-03-20 19:13:39.000000000,,"[{'_account_id': 22348}, {'_account_id': 26622}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-20 08:46:43.000000000', 'files': ['devstack/lib/ovn_agent'], 'web_link': 'https://opendev.org/openstack/neutron/commit/6fc531406a8f584af7b644be380ce1313b32c422', 'message': 'DevStack: do not overshadow the default of geneve max_header_size\n\nThis is to test the real default as set by\nGENEVE_ENCAP_MIN_OVERHEAD from neutron-lib.\n\nChange-Id: I544d82d7d0794450f3835d1af76bbaf3fd91f0ff\nRelated-bug: #1868137\n'}]",0,714050,6fc531406a8f584af7b644be380ce1313b32c422,5,3,1,30491,,,0,"DevStack: do not overshadow the default of geneve max_header_size

This is to test the real default as set by
GENEVE_ENCAP_MIN_OVERHEAD from neutron-lib.

Change-Id: I544d82d7d0794450f3835d1af76bbaf3fd91f0ff
Related-bug: #1868137
",git fetch https://review.opendev.org/openstack/neutron refs/changes/50/714050/1 && git format-patch -1 --stdout FETCH_HEAD,['devstack/lib/ovn_agent'],1,6fc531406a8f584af7b644be380ce1313b32c422,bug/1868137-related,"OVN_GENEVE_OVERHEAD=${OVN_GENEVE_OVERHEAD:-} if [ ! -z ""$OVN_GENEVE_OVERHEAD"" ] ; then populate_ml2_config /$Q_PLUGIN_CONF_FILE ml2_type_geneve max_header_size=$OVN_GENEVE_OVERHEAD fi ",OVN_GENEVE_OVERHEAD=${OVN_GENEVE_OVERHEAD:-38} populate_ml2_config /$Q_PLUGIN_CONF_FILE ml2_type_geneve max_header_size=$OVN_GENEVE_OVERHEAD,6,2
openstack%2Foslo.policy~master~I8e9f28b0a15d1ed092d72b983be74fe281708fbe,openstack/oslo.policy,master,I8e9f28b0a15d1ed092d72b983be74fe281708fbe,Don't parse cli args on the global object in sphinxpolicygen,MERGED,2020-03-20 16:09:58.000000000,2020-03-20 19:13:27.000000000,2020-03-20 19:10:25.000000000,"[{'_account_id': 11904}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-03-20 16:09:58.000000000', 'files': ['oslo_policy/tests/test_sphinxpolicygen.py', 'oslo_policy/sphinxpolicygen.py'], 'web_link': 'https://opendev.org/openstack/oslo.policy/commit/a0d99e1046f99607609f5b0e75942f90e85946ad', 'message': ""Don't parse cli args on the global object in sphinxpolicygen\n\nsphinxpolicygen is calling the generate_sample cli entrypoint when\nwe aren't actually the command being run. This can cause problems\nif the consuming project has cli args that get registered on import\nof their modules because we may have parsed args before those modules\nget imported. This results in an exception because oslo.config won't\nallow cli args to be registered after they've been parsed once.\n\nThis change makes use of the existing parameter to generate_sample\nthat allows us to pass in a local config object on which to register\nthe cli args. This way we can parse them without affecting the\nglobal config object.\n\nThis was the only place I could find that we were doing something\nlike this so I believe it should eliminate the problem.\n\nChange-Id: I8e9f28b0a15d1ed092d72b983be74fe281708fbe\n""}]",0,714161,a0d99e1046f99607609f5b0e75942f90e85946ad,8,3,1,6928,,,0,"Don't parse cli args on the global object in sphinxpolicygen

sphinxpolicygen is calling the generate_sample cli entrypoint when
we aren't actually the command being run. This can cause problems
if the consuming project has cli args that get registered on import
of their modules because we may have parsed args before those modules
get imported. This results in an exception because oslo.config won't
allow cli args to be registered after they've been parsed once.

This change makes use of the existing parameter to generate_sample
that allows us to pass in a local config object on which to register
the cli args. This way we can parse them without affecting the
global config object.

This was the only place I could find that we were doing something
like this so I believe it should eliminate the problem.

Change-Id: I8e9f28b0a15d1ed092d72b983be74fe281708fbe
",git fetch https://review.opendev.org/openstack/oslo.policy refs/changes/61/714161/1 && git format-patch -1 --stdout FETCH_HEAD,"['oslo_policy/tests/test_sphinxpolicygen.py', 'oslo_policy/sphinxpolicygen.py']",2,a0d99e1046f99607609f5b0e75942f90e85946ad,no-sphinx-global-cli,"from oslo_config import cfg # NOTE(bnemec): We don't want to do cli parsing on the global object here # because that can break consumers who do cli arg registration on import # in their documented modules. It's not allowed to register a cli arg after # the args have been parsed once. conf = cfg.ConfigOpts() '--output-file', out_file], conf=conf)"," '--output-file', out_file])",16,5
openstack%2Fneutron~master~Ie119693854aa283b863a1eac2bdae3330c2b6a9d,openstack/neutron,master,Ie119693854aa283b863a1eac2bdae3330c2b6a9d,Switch to use cast method in dhcp_ready_on_ports method,MERGED,2019-10-31 22:18:50.000000000,2020-03-20 19:07:17.000000000,2019-11-15 14:47:24.000000000,"[{'_account_id': 1131}, {'_account_id': 2733}, {'_account_id': 4694}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 13995}, {'_account_id': 16376}, {'_account_id': 16688}, {'_account_id': 22348}, {'_account_id': 26622}, {'_account_id': 26970}]","[{'number': 1, 'created': '2019-10-31 22:18:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/74de0cf5b23e4b0347098ffde76665795956efd4', 'message': ""Switch to use cast method in dhcp_ready_on_ports method\n\nWhen DHCP agent reports to the neutron-server which ports are\nready, it was using call() method from rpc client.\nThat caused blocking dhcp agent's dhcp_ready_ports_loop thread which\nblocks to send info about other ready ports if there are any.\n\nMethod call() should be used when RPC caller returns a value to the\ncaller but that's not the case here. On neutron server side this\nRPC method is only calling provisioning_complete() method to\nfinish provisioning of ports. And is not returning anything.\n\nSo to make sending dhcp ready ports to neutron-server much faster\nthis patch switch to use cast() method from rpc client.\nThis method don't block to wait for return value from RPC caller.\n\nChange-Id: Ie119693854aa283b863a1eac2bdae3330c2b6a9d\nCloses-Bug: #1850864\n""}, {'number': 2, 'created': '2019-11-12 15:59:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/a9b5d4844eecad2338eed7eda4508a760a97b451', 'message': ""Switch to use cast method in dhcp_ready_on_ports method\n\nWhen DHCP agent reports to the neutron-server which ports are\nready, it was using call() method from rpc client.\nThat caused blocking dhcp agent's dhcp_ready_ports_loop thread which\nblocks to send info about other ready ports if there are any.\n\nMethod call() should be used when RPC caller returns a value to the\ncaller but that's not the case here. On neutron server side this\nRPC method is only calling provisioning_complete() method to\nfinish provisioning of ports. And is not returning anything.\n\nSo to make sending dhcp ready ports to neutron-server much faster\nthis patch switch to use cast() method from rpc client.\nThis method don't block to wait for return value from RPC caller.\n\nChange-Id: Ie119693854aa283b863a1eac2bdae3330c2b6a9d\nCloses-Bug: #1850864\n""}, {'number': 3, 'created': '2019-11-13 12:12:55.000000000', 'files': ['neutron/agent/dhcp/agent.py', 'neutron/tests/unit/agent/dhcp/test_agent.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/1a686fb401eca1843b81292fc88c13a2e6fd274d', 'message': ""Switch to use cast method in dhcp_ready_on_ports method\n\nWhen DHCP agent reports to the neutron-server which ports are\nready, it was using call() method from rpc client.\nThat caused blocking dhcp agent's dhcp_ready_ports_loop thread which\nblocks to send info about other ready ports if there are any.\n\nMethod call() should be used when RPC caller returns a value to the\ncaller but that's not the case here. On neutron server side this\nRPC method is only calling provisioning_complete() method to\nfinish provisioning of ports. And is not returning anything.\n\nSo to make sending dhcp ready ports to neutron-server much faster\nthis patch switch to use cast() method from rpc client.\nThis method don't block to wait for return value from RPC caller.\n\nChange-Id: Ie119693854aa283b863a1eac2bdae3330c2b6a9d\nCloses-Bug: #1850864\n""}]",0,692459,1a686fb401eca1843b81292fc88c13a2e6fd274d,61,11,3,11975,,,0,"Switch to use cast method in dhcp_ready_on_ports method

When DHCP agent reports to the neutron-server which ports are
ready, it was using call() method from rpc client.
That caused blocking dhcp agent's dhcp_ready_ports_loop thread which
blocks to send info about other ready ports if there are any.

Method call() should be used when RPC caller returns a value to the
caller but that's not the case here. On neutron server side this
RPC method is only calling provisioning_complete() method to
finish provisioning of ports. And is not returning anything.

So to make sending dhcp ready ports to neutron-server much faster
this patch switch to use cast() method from rpc client.
This method don't block to wait for return value from RPC caller.

Change-Id: Ie119693854aa283b863a1eac2bdae3330c2b6a9d
Closes-Bug: #1850864
",git fetch https://review.opendev.org/openstack/neutron refs/changes/59/692459/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/agent/dhcp/agent.py'],1,74de0cf5b23e4b0347098ffde76665795956efd4,bug/1850864," cctxt.cast(self.context, 'dhcp_ready_on_ports', port_ids=port_ids)"," return cctxt.call(self.context, 'dhcp_ready_on_ports', port_ids=port_ids)",2,2
openstack%2Fpython-openstackclient~master~I858e004c3b29c687b6a39c8a1ed5fb029eb19c67,openstack/python-openstackclient,master,I858e004c3b29c687b6a39c8a1ed5fb029eb19c67,Now we can add description for role creation in OSC,MERGED,2017-07-17 12:37:51.000000000,2020-03-20 19:02:10.000000000,2020-03-20 18:59:44.000000000,"[{'_account_id': 970}, {'_account_id': 6482}, {'_account_id': 11904}, {'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 25553}, {'_account_id': 25695}]","[{'number': 1, 'created': '2017-07-17 12:37:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/3e86d19ba75747809bdf69a1ee886291ff439083', 'message': ""now we can add description for role creation in OSC\n\nNow user can add the description when user create's the role using OSC\n``openstack role create`` command. User can add the description by adding\n`--description <Description>` to OSC ``openstack role create`` command.\n\nChange-Id: I858e004c3b29c687b6a39c8a1ed5fb029eb19c67\nDepends-on: I230af9cc833af13064636b5d9a7ce6334c3f6e9a\nCloses-Bug: #1669080\n""}, {'number': 2, 'created': '2017-07-18 06:27:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/ec8959d7939957900f409a3c30057a82b4be6651', 'message': ""now we can add description for role creation in OSC\n\nNow user can add the description when user create's the role using OSC\n``openstack role create`` command. User can add the description by adding\n`--description <Description>` to OSC ``openstack role create`` command.\n\nChange-Id: I858e004c3b29c687b6a39c8a1ed5fb029eb19c67\nDepends-on: I230af9cc833af13064636b5d9a7ce6334c3f6e9a\nCloses-Bug: #1669080\n""}, {'number': 3, 'created': '2017-07-18 10:15:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/feb907ba0a69aa1be75da0c8cf890d9b7386d940', 'message': ""now we can add description for role creation in OSC\n\nNow user can add the description when user create's the role using OSC\n``openstack role create`` command. User can add the description by adding\n`--description <Description>` to OSC ``openstack role create`` command.\n\nChange-Id: I858e004c3b29c687b6a39c8a1ed5fb029eb19c67\nDepends-on: I230af9cc833af13064636b5d9a7ce6334c3f6e9a\nCloses-Bug: #1669080\n""}, {'number': 4, 'created': '2018-08-20 11:28:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/3c12d327bd9e85afd1769ad68908f606eade2371', 'message': ""now we can add description for role creation in OSC\n\nNow user can add the description when user create's the role using OSC\n``openstack role create`` command. User can add the description by adding\n`--description <Description>` to OSC ``openstack role create`` command.\n\nChange-Id: I858e004c3b29c687b6a39c8a1ed5fb029eb19c67\nDepends-on: I230af9cc833af13064636b5d9a7ce6334c3f6e9a\nCloses-Bug: #1669080\n""}, {'number': 5, 'created': '2018-08-23 04:56:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/c188fb681fe86bc95c120a5f651f783cb28c16db', 'message': ""Now we can add description for role creation in OSC\n\nNow user can add the description when user create's the role using OSC\n``openstack role create`` command. User can add the description by adding\n`--description <Description>` to OSC ``openstack role create`` command.\n\nChange-Id: I858e004c3b29c687b6a39c8a1ed5fb029eb19c67\nDepends-on: I230af9cc833af13064636b5d9a7ce6334c3f6e9a\nCloses-Bug: #1669080\n""}, {'number': 6, 'created': '2018-08-23 09:56:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/29c707fdd29ac8823319ff977aadf059e0057c49', 'message': ""Now we can add description for role creation in OSC\n\nNow user can add the description when user create's the role using OSC\n``openstack role create`` command. User can add the description by adding\n`--description <Description>` to OSC ``openstack role create`` command.\n\nCo-Authored-By: Deepak Mourya<deepakmoriya7@gmail.com>\nChange-Id: I858e004c3b29c687b6a39c8a1ed5fb029eb19c67\nDepends-on: I230af9cc833af13064636b5d9a7ce6334c3f6e9a\nCloses-Bug: #1669080\n""}, {'number': 7, 'created': '2018-09-14 05:08:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/a6913eca462d60d284b314bcc58dc3dc72e7d283', 'message': ""Now we can add description for role creation in OSC\n\nNow user can add the description when user create's the role using OSC\n``openstack role create`` command. User can add the description by adding\n`--description <Description>` to OSC ``openstack role create`` command.\n\nCo-Authored-By: Deepak Mourya<deepakmoriya7@gmail.com>\nChange-Id: I858e004c3b29c687b6a39c8a1ed5fb029eb19c67\nDepends-on: I230af9cc833af13064636b5d9a7ce6334c3f6e9a\nCloses-Bug: #1669080\n""}, {'number': 8, 'created': '2018-09-17 06:28:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/bb5100c45a3ff345d87c410216dfac046f068126', 'message': ""Now we can add description for role creation in OSC\n\nNow user can add the description when user create's the role using OSC\n``openstack role create`` command. User can add the description by adding\n`--description <Description>` to OSC ``openstack role create`` command.\n\nCo-Authored-By: Deepak Mourya<deepakmoriya7@gmail.com>\nChange-Id: I858e004c3b29c687b6a39c8a1ed5fb029eb19c67\nDepends-on: I230af9cc833af13064636b5d9a7ce6334c3f6e9a\nCloses-Bug: #1669080\n""}, {'number': 9, 'created': '2018-09-18 03:26:35.000000000', 'files': ['releasenotes/notes/add-description-to-role-afe7b6ff668df261.yaml', 'openstackclient/tests/functional/identity/v3/test_role.py', 'doc/source/cli/command-objects/role.rst', 'openstackclient/identity/v3/role.py', 'openstackclient/tests/unit/identity/v3/test_role.py', 'openstackclient/tests/unit/identity/v3/fakes.py', 'openstackclient/tests/functional/identity/v3/common.py'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/eb001733fd3c1a98027f7439b84e952f1eb2a406', 'message': ""Now we can add description for role creation in OSC\n\nNow user can add the description when user create's the role using OSC\n``openstack role create`` command. User can add the description by adding\n`--description <Description>` to OSC ``openstack role create`` command.\n\nCo-Authored-By: Deepak Mourya<deepakmoriya7@gmail.com>\nChange-Id: I858e004c3b29c687b6a39c8a1ed5fb029eb19c67\nDepends-on: I230af9cc833af13064636b5d9a7ce6334c3f6e9a\nCloses-Bug: #1669080\n""}]",7,484355,eb001733fd3c1a98027f7439b84e952f1eb2a406,43,7,9,20401,,,0,"Now we can add description for role creation in OSC

Now user can add the description when user create's the role using OSC
``openstack role create`` command. User can add the description by adding
`--description <Description>` to OSC ``openstack role create`` command.

Co-Authored-By: Deepak Mourya<deepakmoriya7@gmail.com>
Change-Id: I858e004c3b29c687b6a39c8a1ed5fb029eb19c67
Depends-on: I230af9cc833af13064636b5d9a7ce6334c3f6e9a
Closes-Bug: #1669080
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/55/484355/9 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/add-description-to-role-afe7b6ff668df261.yaml', 'doc/source/cli/command-objects/role.rst', 'openstackclient/identity/v3/role.py', 'openstackclient/tests/unit/identity/v3/test_role.py', 'openstackclient/tests/unit/identity/v3/fakes.py']",5,3e86d19ba75747809bdf69a1ee886291ff439083,bug/1669080,role_description = 'role description',,112,4
openstack%2Fkayobe~stable%2Ftrain~I7c1a885b36445e33d4db1b1c8533db28a644b4a1,openstack/kayobe,stable/train,I7c1a885b36445e33d4db1b1c8533db28a644b4a1,CentOS 8: Add seed and overcloud CI jobs,MERGED,2020-03-03 17:19:20.000000000,2020-03-20 18:52:19.000000000,2020-03-20 18:49:30.000000000,"[{'_account_id': 14826}, {'_account_id': 15197}, {'_account_id': 17669}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-03 17:19:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/76d63be4210c61b116ad241da2a672a336809560', 'message': 'CentOS 8: Add overcloud CI job\n\nCentOS 8 removes interfaces from their bridge during ifdown, and removes\nthe bridge if there are no interfaces left. When Kayobe bounces veth\nlinks plugged into the bridge, it causes the bridge which has the IP we\nare using for SSH to be removed. Use a dummy interface in CI to avoid\nthis problem.\n\nDepends-On: https://review.opendev.org/695881\n\nChange-Id: I7c1a885b36445e33d4db1b1c8533db28a644b4a1\nStory: 2006574\nTask: 38870\n(cherry picked from commit 4c390bb74355ca8d200b9854373d6ebafeece99b)\n'}, {'number': 2, 'created': '2020-03-04 12:51:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/15e18d433bbb5daad27bd93255bf616099e9e2ea', 'message': 'CentOS 8: Add overcloud CI job\n\nCentOS 8 removes interfaces from their bridge during ifdown, and removes\nthe bridge if there are no interfaces left. When Kayobe bounces veth\nlinks plugged into the bridge, it causes the bridge which has the IP we\nare using for SSH to be removed. Use a dummy interface in CI to avoid\nthis problem.\n\nDepends-On: https://review.opendev.org/695881\n\nChange-Id: I7c1a885b36445e33d4db1b1c8533db28a644b4a1\nStory: 2006574\nTask: 38870\n(cherry picked from commit 4c390bb74355ca8d200b9854373d6ebafeece99b)\n'}, {'number': 3, 'created': '2020-03-12 17:18:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/72ed6b9ff918d0719c1ef1e12cdc8de9d9951b6c', 'message': 'CentOS 8: Add seed and overcloud CI jobs\n\nCentOS 8 removes interfaces from their bridge during ifdown, and removes\nthe bridge if there are no interfaces left. When Kayobe bounces veth\nlinks plugged into the bridge, it causes the bridge which has the IP we\nare using for SSH to be removed. Use a dummy interface in CI to avoid\nthis problem.\n\nUnlike the original version of this commit on master, CentOS 7 jobs are\nretained.\n\nDepends-On: https://review.opendev.org/695881\n\nChange-Id: I7c1a885b36445e33d4db1b1c8533db28a644b4a1\nStory: 2006574\nTask: 38870\n(cherry picked from commit b0359e37dff7d3207627cfa39f28c167a81426b5)\n'}, {'number': 4, 'created': '2020-03-13 10:44:51.000000000', 'files': ['ansible/baremetal-compute-inspect.yml', 'playbooks/kayobe-overcloud-base/pre.yml', 'playbooks/kayobe-seed-base/pre.yml', 'zuul.d/project.yaml', 'playbooks/kayobe-overcloud-base/overrides.yml.j2', 'zuul.d/nodesets.yaml', 'playbooks/kayobe-seed-base/overrides.yml.j2', 'zuul.d/jobs.yaml'], 'web_link': 'https://opendev.org/openstack/kayobe/commit/13f5fa486bfc946c701eb89dd8f88544d11bb0f1', 'message': 'CentOS 8: Add seed and overcloud CI jobs\n\nCentOS 8 removes interfaces from their bridge during ifdown, and removes\nthe bridge if there are no interfaces left. When Kayobe bounces veth\nlinks plugged into the bridge, it causes the bridge which has the IP we\nare using for SSH to be removed. Use a dummy interface in CI to avoid\nthis problem.\n\nUnlike the original version of this commit on master, CentOS 7 jobs are\nretained.\n\nDepends-On: https://review.opendev.org/695881\n\nChange-Id: I7c1a885b36445e33d4db1b1c8533db28a644b4a1\nStory: 2006574\nTask: 38870\n(cherry picked from commit b0359e37dff7d3207627cfa39f28c167a81426b5)\n'}]",1,711067,13f5fa486bfc946c701eb89dd8f88544d11bb0f1,18,4,4,14826,,,0,"CentOS 8: Add seed and overcloud CI jobs

CentOS 8 removes interfaces from their bridge during ifdown, and removes
the bridge if there are no interfaces left. When Kayobe bounces veth
links plugged into the bridge, it causes the bridge which has the IP we
are using for SSH to be removed. Use a dummy interface in CI to avoid
this problem.

Unlike the original version of this commit on master, CentOS 7 jobs are
retained.

Depends-On: https://review.opendev.org/695881

Change-Id: I7c1a885b36445e33d4db1b1c8533db28a644b4a1
Story: 2006574
Task: 38870
(cherry picked from commit b0359e37dff7d3207627cfa39f28c167a81426b5)
",git fetch https://review.opendev.org/openstack/kayobe refs/changes/67/711067/4 && git format-patch -1 --stdout FETCH_HEAD,"['ansible/baremetal-compute-inspect.yml', 'playbooks/kayobe-overcloud-base/pre.yml', 'zuul.d/project.yaml', 'playbooks/kayobe-overcloud-base/overrides.yml.j2', 'zuul.d/nodesets.yaml', 'zuul.d/jobs.yaml']",6,76d63be4210c61b116ad241da2a672a336809560,, name: kayobe-overcloud-centos8 parent: kayobe-overcloud-base nodeset: kayobe-centos8 - job:,,29,1
openstack%2Fkayobe~stable%2Ftrain~I96d8df005780e8d14dbdb80fe4557e8b0d037699,openstack/kayobe,stable/train,I96d8df005780e8d14dbdb80fe4557e8b0d037699,CentOS 8: Use same local python version for kolla-ansible,MERGED,2020-03-04 12:51:20.000000000,2020-03-20 18:50:59.000000000,2020-03-20 18:49:26.000000000,"[{'_account_id': 14826}, {'_account_id': 15197}, {'_account_id': 17669}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-04 12:51:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/8b924414746886a1a098082ee6d59042d1ba4a23', 'message': 'CentOS 8: Use same local python version for kolla-ansible\n\nTrain only.\n\nThe master branch switched to always use Python 3 for local\nkolla-ansible execution. For Train it is more appropriate to use the\nsame version used to execute ansible.\n\nChange-Id: I96d8df005780e8d14dbdb80fe4557e8b0d037699\nStory: 2004959\nTask: 38930\n'}, {'number': 2, 'created': '2020-03-12 17:18:33.000000000', 'files': ['ansible/roles/kolla-ansible/defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/kayobe/commit/047f84ad99598bb134b879b5a85fbcb6403752ab', 'message': 'CentOS 8: Use same local python version for kolla-ansible\n\nTrain only.\n\nThe master branch switched to always use Python 3 for local\nkolla-ansible execution. For Train it is more appropriate to use the\nsame version used to execute ansible.\n\nChange-Id: I96d8df005780e8d14dbdb80fe4557e8b0d037699\nStory: 2004959\nTask: 38930\n'}]",0,711219,047f84ad99598bb134b879b5a85fbcb6403752ab,15,4,2,14826,,,0,"CentOS 8: Use same local python version for kolla-ansible

Train only.

The master branch switched to always use Python 3 for local
kolla-ansible execution. For Train it is more appropriate to use the
same version used to execute ansible.

Change-Id: I96d8df005780e8d14dbdb80fe4557e8b0d037699
Story: 2004959
Task: 38930
",git fetch https://review.opendev.org/openstack/kayobe refs/changes/19/711219/1 && git format-patch -1 --stdout FETCH_HEAD,['ansible/roles/kolla-ansible/defaults/main.yml'],1,8b924414746886a1a098082ee6d59042d1ba4a23,,"kolla_ansible_venv_python_major_version: ""{{ ansible_python.version.major }}""",# FIXME(mgoddard): Use ansible_python when Kayobe supports Python 3.# FIXME(mgoddard): Use ansible_python when Kayobe supports Python 3. kolla_ansible_venv_python_major_version: 2,1,3
openstack%2Fkayobe~stable%2Ftrain~If5230854d7565c8b3c91a46da4795c63edf095e4,openstack/kayobe,stable/train,If5230854d7565c8b3c91a46da4795c63edf095e4,"CentOS 8: Disable ntpd, enable chrony container",MERGED,2020-03-03 17:19:20.000000000,2020-03-20 18:44:53.000000000,2020-03-20 18:43:23.000000000,"[{'_account_id': 14826}, {'_account_id': 17669}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-03 17:19:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/b33f394b1b4d7bd121376576de14943ca322880b', 'message': 'CentOS 8: Disable ntpd, enable chrony container\n\nCentOS 8 does not provide an ntp package. Instead fall back to using the\nchrony container provided by Kolla Ansible by default.\n\nChange-Id: If5230854d7565c8b3c91a46da4795c63edf095e4\nStory: 2006574\nTask: 38866\n(cherry picked from commit dc8cd9d8a893be6121a846f2b0106f4222489d0e)\n'}, {'number': 2, 'created': '2020-03-12 17:18:33.000000000', 'files': ['releasenotes/notes/centos-8-chrony-bec9d7bc8b346363.yaml', 'ansible/group_vars/all/kolla', 'doc/source/configuration/hosts.rst', 'etc/kayobe/kolla.yml', 'ansible/group_vars/all/ntp', 'etc/kayobe/ntp.yml'], 'web_link': 'https://opendev.org/openstack/kayobe/commit/612575a4a82a471fcd2e4c2233982c62efa496b3', 'message': 'CentOS 8: Disable ntpd, enable chrony container\n\nCentOS 8 does not provide an ntp package. Instead fall back to using the\nchrony container provided by Kolla Ansible by default.\n\nDepends-On: https://review.opendev.org/711666\n\nChange-Id: If5230854d7565c8b3c91a46da4795c63edf095e4\nStory: 2006574\nTask: 38866\n(cherry picked from commit 71d36cbe5e00be6e94ec28fdc1fc28126756e557)\n'}]",1,711066,612575a4a82a471fcd2e4c2233982c62efa496b3,14,3,2,14826,,,0,"CentOS 8: Disable ntpd, enable chrony container

CentOS 8 does not provide an ntp package. Instead fall back to using the
chrony container provided by Kolla Ansible by default.

Depends-On: https://review.opendev.org/711666

Change-Id: If5230854d7565c8b3c91a46da4795c63edf095e4
Story: 2006574
Task: 38866
(cherry picked from commit 71d36cbe5e00be6e94ec28fdc1fc28126756e557)
",git fetch https://review.opendev.org/openstack/kayobe refs/changes/66/711066/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/centos-8-chrony-bec9d7bc8b346363.yaml', 'ansible/group_vars/all/kolla', 'ansible/ntp.yml', 'doc/source/configuration/hosts.rst', 'etc/kayobe/kolla.yml', 'ansible/group_vars/all/ntp', 'etc/kayobe/ntp.yml']",7,b33f394b1b4d7bd121376576de14943ca322880b,,"# Whether to enable the NTP daemon on the host. On CentOS 7 the default is true # unless 'kolla_enable_chrony' has been set to true on overcloud hosts. On # CentOS 8 the host NTP daemon is not supported, and kolla_enable_chrony is set # to true by default.",# Whether to enable the NTP daemon on the host. Default is true unless # 'kolla_enable_chrony' has been set to true on overcloud hosts.,41,13
openstack%2Fkolla-ansible~master~I5abe46948992121a11a36f941d4f8fac1caa92b1,openstack/kolla-ansible,master,I5abe46948992121a11a36f941d4f8fac1caa92b1,Do not enforce vxlan in init-runonce,MERGED,2020-03-20 13:53:06.000000000,2020-03-20 18:31:17.000000000,2020-03-20 18:29:50.000000000,"[{'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 24072}]","[{'number': 1, 'created': '2020-03-20 13:53:06.000000000', 'files': ['tools/init-runonce'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/e03f06c081edc82e5d4ad614cce9a093fc2d1863', 'message': 'Do not enforce vxlan in init-runonce\n\nThe affected command was meant to create a tenant network, so let\nus really test it this way.\n\nNot marking CI, because someone may be using this script.\n\nChange-Id: I5abe46948992121a11a36f941d4f8fac1caa92b1\n'}]",0,714115,e03f06c081edc82e5d4ad614cce9a093fc2d1863,9,3,1,30491,,,0,"Do not enforce vxlan in init-runonce

The affected command was meant to create a tenant network, so let
us really test it this way.

Not marking CI, because someone may be using this script.

Change-Id: I5abe46948992121a11a36f941d4f8fac1caa92b1
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/15/714115/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/init-runonce'],1,e03f06c081edc82e5d4ad614cce9a093fc2d1863,do-not-enforce-vxlan,$KOLLA_OPENSTACK_COMMAND network create demo-net,$KOLLA_OPENSTACK_COMMAND network create --provider-network-type vxlan demo-net,1,1
openstack%2Ftripleo-ansible~master~I3090106fd24a82373ab308a1ba7f8bcd0eddecad,openstack/tripleo-ansible,master,I3090106fd24a82373ab308a1ba7f8bcd0eddecad,Add playbook for update deployment plan,MERGED,2020-03-13 10:20:23.000000000,2020-03-20 18:27:55.000000000,2020-03-20 18:24:45.000000000,"[{'_account_id': 7353}, {'_account_id': 8833}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25877}]","[{'number': 1, 'created': '2020-03-13 10:20:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/57f11e36894ad5fa4b83b0e614e504c838afcafc', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}, {'number': 2, 'created': '2020-03-13 14:17:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/f6c18f09c506b61ff6355f96b575e46bf0800521', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}, {'number': 3, 'created': '2020-03-14 02:47:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/37b5979f68ab5ee12048369e3dd98433183f7367', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}, {'number': 4, 'created': '2020-03-16 02:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/947a556541a1c446094339e000ec54a401063e8f', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}, {'number': 5, 'created': '2020-03-16 13:15:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/9982cb90ad9afa247860247c14a2ec810fd4aaa2', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}, {'number': 6, 'created': '2020-03-18 06:43:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/df9317c8c3cdc1bda08d46310c32384477180f95', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}, {'number': 7, 'created': '2020-03-18 10:22:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/e9d06ffcc1448eb4806576a68e12ad7de86eb3ae', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}, {'number': 8, 'created': '2020-03-18 10:32:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/0ecf558d39e44592ac2e5f12e922450fa995a675', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}, {'number': 9, 'created': '2020-03-18 12:37:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/b096945a28637113bb7ad04218c093c6293a91df', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}, {'number': 10, 'created': '2020-03-18 15:23:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/74cf5add363949ce6b8b7135c165f1892b03ac06', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}, {'number': 11, 'created': '2020-03-19 19:34:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/5dd22dc553fd45e6294d320a9ad49e10568965ab', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}, {'number': 12, 'created': '2020-03-19 21:52:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/cc90f77723175c8535cf33a88221a35258c2edd5', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}, {'number': 13, 'created': '2020-03-20 02:52:06.000000000', 'files': ['tripleo_ansible/ansible_plugins/modules/tripleo_swift_tempurl.py', 'tripleo_ansible/playbooks/cli-update-deployment-plan.yaml', 'doc/source/modules/modules-tripleo_swift_tempurl.rst'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/ac40b0f5ef2279564252b3434cd6689e63ce0513', 'message': 'Add playbook for update deployment plan\n\nThis adds the playbook and required modules.\n\nDepends-On: https://review.opendev.org/712896\nChange-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad\n'}]",7,712899,ac40b0f5ef2279564252b3434cd6689e63ce0513,44,6,13,8833,,,0,"Add playbook for update deployment plan

This adds the playbook and required modules.

Depends-On: https://review.opendev.org/712896
Change-Id: I3090106fd24a82373ab308a1ba7f8bcd0eddecad
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/99/712899/1 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_ansible/ansible_plugins/modules/tripleo_swift_tempurl.py', 'tripleo_ansible/playbooks/cli-update-deployment-plan.yaml', 'doc/source/modules/modules-tripleo_swift_tempurl.rst']",3,57f11e36894ad5fa4b83b0e614e504c838afcafc,mistral_to_ansible,============================== Module - tripleo_swift_tempurl ============================== This module provides for the following ansible plugin: * tripleo_swift_tempurl .. ansibleautoplugin:: :module: tripleo_ansible/ansible_plugins/modules/tripleo_swift_tempurl.py :documentation: true :examples: true ,,260,0
openstack%2Freleases~master~I2e9fda88faeacbb7ecad583182b83e608c832c91,openstack/releases,master,I2e9fda88faeacbb7ecad583182b83e608c832c91,[cloudkitty] Transition Rocky to EM,MERGED,2020-02-26 01:09:37.000000000,2020-03-20 18:21:18.000000000,2020-03-20 18:21:18.000000000,"[{'_account_id': 8556}, {'_account_id': 11904}, {'_account_id': 17068}, {'_account_id': 22348}, {'_account_id': 28522}, {'_account_id': 29503}]","[{'number': 1, 'created': '2020-02-26 01:09:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/7ec74d077425a73ea27bdeb5455b4db6c8a52b20', 'message': '[cloudkitty] Transition Rocky to EM\n\nThis transition the rocky branch to extended maintenance.\nChanges for bugfixes and things the team deems important are\nstill encouraged, but there will no longer be official releases\noff of the branch.\n\nPlease +1 if the team is ready for us to proceed with this\ntransition, or -1 if there are any final backports currently in\nflight that we should wait for. For the latter case, please\nupdate the patch with the new commit hash after doing a final\nrelease to get those changes out so we know to proceed with the\ntransition.\n\nChange-Id: I2e9fda88faeacbb7ecad583182b83e608c832c91\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}, {'number': 2, 'created': '2020-02-26 17:31:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/753b2d8782cbecd98d912466389f56e92fd67ba0', 'message': '[cloudkitty] Transition Rocky to EM\n\nThis transition the rocky branch to extended maintenance.\nChanges for bugfixes and things the team deems important are\nstill encouraged, but there will no longer be official releases\noff of the branch.\n\nPlease +1 if the team is ready for us to proceed with this\ntransition, or -1 if there are any final backports currently in\nflight that we should wait for. For the latter case, please\nupdate the patch with the new commit hash after doing a final\nrelease to get those changes out so we know to proceed with the\ntransition.\n\nChange-Id: I2e9fda88faeacbb7ecad583182b83e608c832c91\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}, {'number': 3, 'created': '2020-02-29 14:30:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/70ead4c58b48384a22411d5a91a7d5bcf7c55039', 'message': '[cloudkitty] Transition Rocky to EM\n\nThis transition the rocky branch to extended maintenance.\nChanges for bugfixes and things the team deems important are\nstill encouraged, but there will no longer be official releases\noff of the branch.\n\nPlease +1 if the team is ready for us to proceed with this\ntransition, or -1 if there are any final backports currently in\nflight that we should wait for. For the latter case, please\nupdate the patch with the new commit hash after doing a final\nrelease to get those changes out so we know to proceed with the\ntransition.\n\nChange-Id: I2e9fda88faeacbb7ecad583182b83e608c832c91\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}, {'number': 4, 'created': '2020-03-20 16:13:47.000000000', 'files': ['deliverables/rocky/cloudkitty.yaml', 'deliverables/rocky/python-cloudkittyclient.yaml', 'deliverables/rocky/cloudkitty-dashboard.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/5b301e7ae3630b5c206947b2b47667bedffa7bef', 'message': '[cloudkitty] Transition Rocky to EM\n\nThis transition the rocky branch to extended maintenance.\nChanges for bugfixes and things the team deems important are\nstill encouraged, but there will no longer be official releases\noff of the branch.\n\nPlease +1 if the team is ready for us to proceed with this\ntransition, or -1 if there are any final backports currently in\nflight that we should wait for. For the latter case, please\nupdate the patch with the new commit hash after doing a final\nrelease to get those changes out so we know to proceed with the\ntransition.\n\nChange-Id: I2e9fda88faeacbb7ecad583182b83e608c832c91\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}]",4,709920,5b301e7ae3630b5c206947b2b47667bedffa7bef,23,6,4,11904,,,0,"[cloudkitty] Transition Rocky to EM

This transition the rocky branch to extended maintenance.
Changes for bugfixes and things the team deems important are
still encouraged, but there will no longer be official releases
off of the branch.

Please +1 if the team is ready for us to proceed with this
transition, or -1 if there are any final backports currently in
flight that we should wait for. For the latter case, please
update the patch with the new commit hash after doing a final
release to get those changes out so we know to proceed with the
transition.

Change-Id: I2e9fda88faeacbb7ecad583182b83e608c832c91
Signed-off-by: Sean McGinnis <sean.mcginnis@gmail.com>
",git fetch https://review.opendev.org/openstack/releases refs/changes/20/709920/1 && git format-patch -1 --stdout FETCH_HEAD,"['deliverables/rocky/cloudkitty-tempest-plugin.yaml', 'deliverables/rocky/cloudkitty.yaml', 'deliverables/rocky/cloudkitty-dashboard.yaml']",3,7ec74d077425a73ea27bdeb5455b4db6c8a52b20,rocky-em, - version: rocky-em projects: - repo: openstack/cloudkitty-dashboard hash: 071392642480fb700399340ba6b4eb13b31a05fa,,12,0
openstack%2Fpuppet-tripleo~master~I7894c51d18961c5cab7ac62e5eec5d515e2667c8,openstack/puppet-tripleo,master,I7894c51d18961c5cab7ac62e5eec5d515e2667c8,Fix grafana haproxy frontend ip variable,MERGED,2020-03-19 14:27:57.000000000,2020-03-20 17:52:32.000000000,2020-03-20 14:21:42.000000000,"[{'_account_id': 3153}, {'_account_id': 6796}, {'_account_id': 14985}, {'_account_id': 18002}, {'_account_id': 20182}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25402}]","[{'number': 1, 'created': '2020-03-19 14:27:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/814fd0fd57540c9c96135e1d71ed92686753ef77', 'message': ""Fix grafana haproxy frontend ip variable\n\nGrafana could be exposed along with ceph dashboard\nbut it's actually embedded by in a view created for\nthis purpose.\nFor this reason the ceph-dashboard component should\nbe able to reach grafana or the requests will fail.\n\nChange-Id: I7894c51d18961c5cab7ac62e5eec5d515e2667c8\n""}, {'number': 2, 'created': '2020-03-19 15:36:24.000000000', 'files': ['manifests/haproxy.pp'], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/b6175ece1ac77647e78e38f69648a8ac6c8795ee', 'message': ""Fix grafana haproxy frontend ip variable\n\nGrafana could be exposed along with ceph dashboard\nbut it's actually embedded by in a view created for\nthis purpose.\nFor this reason the ceph-dashboard component should\nbe able to reach grafana or the requests will fail.\n\nCloses-Bug: #1868118\nChange-Id: I7894c51d18961c5cab7ac62e5eec5d515e2667c8\n""}]",2,713892,b6175ece1ac77647e78e38f69648a8ac6c8795ee,15,8,2,25402,,,0,"Fix grafana haproxy frontend ip variable

Grafana could be exposed along with ceph dashboard
but it's actually embedded by in a view created for
this purpose.
For this reason the ceph-dashboard component should
be able to reach grafana or the requests will fail.

Closes-Bug: #1868118
Change-Id: I7894c51d18961c5cab7ac62e5eec5d515e2667c8
",git fetch https://review.opendev.org/openstack/puppet-tripleo refs/changes/92/713892/1 && git format-patch -1 --stdout FETCH_HEAD,['manifests/haproxy.pp'],1,814fd0fd57540c9c96135e1d71ed92686753ef77,lp1868118," internal_ip => hiera('ceph_dashboard_vip', $controller_virtual_ip),"," internal_ip => hiera('ceph_grafana_vip', $controller_virtual_ip),",1,1
openstack%2Ftripleo-ansible~stable%2Ftrain~I530e257f343ffc551db9e984f9a27b20c397bfb1,openstack/tripleo-ansible,stable/train,I530e257f343ffc551db9e984f9a27b20c397bfb1,Filter out wrapper commands from the ps output,MERGED,2020-03-20 12:03:39.000000000,2020-03-20 17:45:40.000000000,2020-03-20 17:45:40.000000000,"[{'_account_id': 3153}, {'_account_id': 8655}, {'_account_id': 14287}, {'_account_id': 20172}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-20 12:03:39.000000000', 'files': ['tripleo_ansible/roles/tripleo-systemd-wrapper/templates/service_sync.j2'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/546bc0d0f14c52114a31dceb47f7501682731ef0', 'message': ""Filter out wrapper commands from the ps output\n\nThis patch is filtering out the wrapper execution from the ps output\nin the sync script. By doing this, it'll effectively detect when\nthe target process is not running and start it. Otherwise, there might\nbe cases where the process start is postponed until next iteration\nof the sync script (1 minute) and it may be already too late.\n\nThis is causing tests to fail as the metadata service is not provisioned\nin time for instances to fetch their SSH keys.\n\nChange-Id: I530e257f343ffc551db9e984f9a27b20c397bfb1\nCo-Authored-By: Jakub Libosvar <jlibosva@redhat.com>\nCloses-Bug: #1868082\nSigned-off-by: Daniel Alvarez <dalvarez@redhat.com>\n(cherry-picked from 0df3ebd7592eac90f76650770f59c89fba4ebe39)\n""}]",0,714099,546bc0d0f14c52114a31dceb47f7501682731ef0,10,6,1,23804,,,0,"Filter out wrapper commands from the ps output

This patch is filtering out the wrapper execution from the ps output
in the sync script. By doing this, it'll effectively detect when
the target process is not running and start it. Otherwise, there might
be cases where the process start is postponed until next iteration
of the sync script (1 minute) and it may be already too late.

This is causing tests to fail as the metadata service is not provisioned
in time for instances to fetch their SSH keys.

Change-Id: I530e257f343ffc551db9e984f9a27b20c397bfb1
Co-Authored-By: Jakub Libosvar <jlibosva@redhat.com>
Closes-Bug: #1868082
Signed-off-by: Daniel Alvarez <dalvarez@redhat.com>
(cherry-picked from 0df3ebd7592eac90f76650770f59c89fba4ebe39)
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/99/714099/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_ansible/roles/tripleo-systemd-wrapper/templates/service_sync.j2'],1,546bc0d0f14c52114a31dceb47f7501682731ef0,bug/1868082," if ! ps -e -o pid,command | grep ""$(echo $NETNS | sed 's|^[^-]*\-||')"" | egrep -v ""grep | netns exec"" &> /dev/null; then"," if ! ps -e -o pid,command | grep ""$(echo $NETNS | sed 's|^[^-]*\-||')"" | grep -v grep &> /dev/null; then",1,1
openstack%2Ftripleo-ansible~master~I60f4ed38584a6b55a09590d9a012d2bfb010f9bc,openstack/tripleo-ansible,master,I60f4ed38584a6b55a09590d9a012d2bfb010f9bc,Add tripleo_plan_parameters_update module,MERGED,2020-03-12 02:21:39.000000000,2020-03-20 17:41:28.000000000,2020-03-20 17:39:26.000000000,"[{'_account_id': 7353}, {'_account_id': 9712}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25877}]","[{'number': 1, 'created': '2020-03-12 02:21:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/18b55046a53620cfd4c41c2c48cdd05531ded234', 'message': 'Add tripleo_plan_parameters_update module\n\nThis adds ansible module to update plan parameters and optionally\nvalidate the stack.\n\nChange-Id: I60f4ed38584a6b55a09590d9a012d2bfb010f9bc\n'}, {'number': 2, 'created': '2020-03-16 13:15:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/9369004419b23a7b8ca3bbc39eb082ee4fac1e4d', 'message': 'Add tripleo_plan_parameters_update module\n\nThis adds ansible module to update plan parameters and optionally\nvalidate the stack.\n\nChange-Id: I60f4ed38584a6b55a09590d9a012d2bfb010f9bc\n'}, {'number': 3, 'created': '2020-03-18 06:43:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/d9666c8a6a6c038f0bbf2e290284523ded103ec8', 'message': 'Add tripleo_plan_parameters_update module\n\nThis adds ansible module to update plan parameters and optionally\nvalidate the stack.\n\nChange-Id: I60f4ed38584a6b55a09590d9a012d2bfb010f9bc\n'}, {'number': 4, 'created': '2020-03-18 10:32:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/ac6d9bfff353fb81c65b774936c2372a6588972b', 'message': 'Add tripleo_plan_parameters_update module\n\nThis adds ansible module to update plan parameters and optionally\nvalidate the stack.\n\nChange-Id: I60f4ed38584a6b55a09590d9a012d2bfb010f9bc\n'}, {'number': 5, 'created': '2020-03-18 12:37:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/7916cbeff11b0166560692fd3853c4b94afca6b5', 'message': 'Add tripleo_plan_parameters_update module\n\nThis adds ansible module to update plan parameters and optionally\nvalidate the stack.\n\nChange-Id: I60f4ed38584a6b55a09590d9a012d2bfb010f9bc\n'}, {'number': 6, 'created': '2020-03-19 19:34:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/ef632e1570d33a4e73688bc6de5f23bc06443f58', 'message': 'Add tripleo_plan_parameters_update module\n\nThis adds ansible module to update plan parameters and optionally\nvalidate the stack.\n\nChange-Id: I60f4ed38584a6b55a09590d9a012d2bfb010f9bc\n'}, {'number': 7, 'created': '2020-03-19 21:52:31.000000000', 'files': ['tripleo_ansible/ansible_plugins/modules/tripleo_plan_parameters_update.py', 'doc/source/modules/modules-tripleo_plan_parameters_update.rst'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/ce16399264a7b9d99e68b649d403b87cca6fb19a', 'message': 'Add tripleo_plan_parameters_update module\n\nThis adds ansible module to update plan parameters and optionally\nvalidate the stack.\n\nChange-Id: I60f4ed38584a6b55a09590d9a012d2bfb010f9bc\n'}]",2,712604,ce16399264a7b9d99e68b649d403b87cca6fb19a,24,5,7,8833,,,0,"Add tripleo_plan_parameters_update module

This adds ansible module to update plan parameters and optionally
validate the stack.

Change-Id: I60f4ed38584a6b55a09590d9a012d2bfb010f9bc
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/04/712604/6 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_ansible/ansible_plugins/modules/tripleo_plan_parameters_update.py', 'doc/source/modules/modules-tripleo_plan_parameters_update.rst']",2,18b55046a53620cfd4c41c2c48cdd05531ded234,mistral_to_ansible,======================================= Module - tripleo_plan_parameters_update ======================================= This module provides for the following ansible plugin: * tripleo_plan_parameters_update .. ansibleautoplugin:: :module: tripleo_ansible/ansible_plugins/modules/tripleo_plan_parameters_update.py :documentation: true :examples: true ,,168,0
openstack%2Ftripleo-ansible~master~Ib97d654fac3dd2ce74a50add6aedb2d0f7368dce,openstack/tripleo-ansible,master,Ib97d654fac3dd2ce74a50add6aedb2d0f7368dce,Add playbook for create deployment plan,MERGED,2020-03-12 02:21:39.000000000,2020-03-20 17:39:27.000000000,2020-03-20 17:39:27.000000000,"[{'_account_id': 7353}, {'_account_id': 8833}, {'_account_id': 9712}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25877}]","[{'number': 1, 'created': '2020-03-12 02:21:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/4d9c7404d5b597266d4e85d580b5ce4302261df4', 'message': 'Add playbook for create deployment plan\n\nThis adds a playbook for creating deployment plan which would be\ncalled from python-tripleoclient\n\nChange-Id: Ib97d654fac3dd2ce74a50add6aedb2d0f7368dce\n'}, {'number': 2, 'created': '2020-03-14 02:47:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/0c6c5dfd34306f232fc1fa5539112e71e125e341', 'message': 'Add playbook for create deployment plan\n\nThis adds a playbook for creating deployment plan which would be\ncalled from python-tripleoclient\n\nChange-Id: Ib97d654fac3dd2ce74a50add6aedb2d0f7368dce\n'}, {'number': 3, 'created': '2020-03-16 02:35:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/70101782c5e5ebc227eee09f4b23b47cb5cbc30a', 'message': 'Add playbook for create deployment plan\n\nThis adds a playbook for creating deployment plan which would be\ncalled from python-tripleoclient\n\nChange-Id: Ib97d654fac3dd2ce74a50add6aedb2d0f7368dce\n'}, {'number': 4, 'created': '2020-03-16 13:15:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/a891a0f6d4a7b44ee76a294b3185a6af89d3475c', 'message': 'Add playbook for create deployment plan\n\nThis adds a playbook for creating deployment plan which would be\ncalled from python-tripleoclient\n\nChange-Id: Ib97d654fac3dd2ce74a50add6aedb2d0f7368dce\n'}, {'number': 5, 'created': '2020-03-18 06:43:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/a66aa9d75dd60dce5cecc8ffa5a6fc9063402ba6', 'message': 'Add playbook for create deployment plan\n\nThis adds a playbook for creating deployment plan which would be\ncalled from python-tripleoclient\n\nChange-Id: Ib97d654fac3dd2ce74a50add6aedb2d0f7368dce\n'}, {'number': 6, 'created': '2020-03-18 10:32:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/637a50a5d9cf7bb56b544b579a961b65d303d622', 'message': 'Add playbook for create deployment plan\n\nThis adds a playbook for creating deployment plan which would be\ncalled from python-tripleoclient\n\nChange-Id: Ib97d654fac3dd2ce74a50add6aedb2d0f7368dce\n'}, {'number': 7, 'created': '2020-03-18 12:37:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/9bc943ecbac1be7760ab089ea958b613b2f9e15d', 'message': 'Add playbook for create deployment plan\n\nThis adds a playbook for creating deployment plan which would be\ncalled from python-tripleoclient\n\nChange-Id: Ib97d654fac3dd2ce74a50add6aedb2d0f7368dce\n'}, {'number': 8, 'created': '2020-03-19 19:34:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/c6957ef7a1c6984b1be8ec47ea55e31a22cd5996', 'message': 'Add playbook for create deployment plan\n\nThis adds a playbook for creating deployment plan which would be\ncalled from python-tripleoclient\n\nChange-Id: Ib97d654fac3dd2ce74a50add6aedb2d0f7368dce\n'}, {'number': 9, 'created': '2020-03-19 21:52:41.000000000', 'files': ['tripleo_ansible/playbooks/cli-create-deployment-plan.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/4a988ab1d83fc8be2691510a3fdde823bbe0a487', 'message': 'Add playbook for create deployment plan\n\nThis adds a playbook for creating deployment plan which would be\ncalled from python-tripleoclient\n\nChange-Id: Ib97d654fac3dd2ce74a50add6aedb2d0f7368dce\n'}]",9,712605,4a988ab1d83fc8be2691510a3fdde823bbe0a487,36,6,9,8833,,,0,"Add playbook for create deployment plan

This adds a playbook for creating deployment plan which would be
called from python-tripleoclient

Change-Id: Ib97d654fac3dd2ce74a50add6aedb2d0f7368dce
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/05/712605/5 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_ansible/playbooks/cli-create-deployment-plan.yaml'],1,4d9c7404d5b597266d4e85d580b5ce4302261df4,mistral_to_ansible,"--- # Copyright 2019 Red Hat, Inc. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. - name: Create Deployment Plan connection: ""{{ (tripleo_target_host is defined) | ternary('ssh', 'local') }}"" hosts: ""{{ tripleo_target_host | default('undercloud') }}"" remote_user: ""{{ tripleo_target_user | default(lookup('env', 'USER')) }}"" gather_facts: ""{{ (tripleo_target_host is defined) | ternary(true, false) }}"" any_errors_fatal: true vars: container: overcloud source_url: null generate_passwords: true plan_environment: plan-environment.yaml default_templates_dir: '/usr/share/openstack-tripleo-heat-templates/' use_default_templates: false validate_stack: true handlers: - name: Cleanup temp directory file: path: ""{{ temp_dir.path }}"" state: absent when: temp_dir is defined tasks: - name: crate container and upload templates block: - name: Create container if does not exist os_object: container: ""{{ container }}"" state: present - name: Create temp directory tempfile: state: directory suffix: ""{{ '_' + container + '_import' }}"" register: temp_dir notify: - ""Cleanup temp directory"" when: source_url is not none - name: Git clone remote url git: repo: ""{{ source_url }}"" dest: ""{{ temp_dir }}"" when: source_url is not none - name: Upload templates tripleo_templates_upload: container: ""{{ container }}"" templates_dir: ""{{ temp_dir.path }}"" when: source_url is not none - name: Upload default templates tripleo_templates_upload: container: ""{{ container }}"" templates_dir: ""{{ default_templates_dir }}"" when: use_default_templates|bool when: use_default_templates or source_url is not none - name: Create plan environment if does not exit os_object: container: ""{{ container }}"" name: plan-environment.yaml filename: ""{{ default_templates_dir + plan_environment }}"" state: present - name: Generate passwords and update plan tripleo_passwords_rotate: container: ""{{ container }}"" when: generate_passwords|bool - name: Prepare Container images and update plan tripleo_image_params_prepare: container: ""{{ container }}"" with_roledata: false - name: Add root stack name tripleo_plan_parameters_update: container: ""{{ container }}"" parameters: RootStackName: ""{{ container }}"" parameter_key: parameter_defaults validate: false ",,99,0
openstack%2Ftripleo-ansible~master~I7ac4c328ec7f4dd0fb4aa7495945fbc57feb4bbc,openstack/tripleo-ansible,master,I7ac4c328ec7f4dd0fb4aa7495945fbc57feb4bbc,Add tripleo_templates_upload module,MERGED,2020-03-12 02:21:39.000000000,2020-03-20 17:39:17.000000000,2020-03-20 17:35:53.000000000,"[{'_account_id': 7353}, {'_account_id': 8833}, {'_account_id': 9712}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25877}]","[{'number': 1, 'created': '2020-03-12 02:21:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/dd1899a0c187e0474ecb46513be3e8397e779938', 'message': 'Add tripleo_templates_upload module\n\nThis adds a new module to upload templates to plan as a tar\nfile.\n\nChange-Id: I7ac4c328ec7f4dd0fb4aa7495945fbc57feb4bbc\nDepends-On: https://review.opendev.org/712599\n'}, {'number': 2, 'created': '2020-03-16 13:15:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/3c5dbb86f837c3ab44c4b09319151b3c98637763', 'message': 'Add tripleo_templates_upload module\n\nThis adds a new module to upload templates to plan as a tar\nfile.\n\nChange-Id: I7ac4c328ec7f4dd0fb4aa7495945fbc57feb4bbc\nDepends-On: https://review.opendev.org/712599\n'}, {'number': 3, 'created': '2020-03-18 06:43:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/e13156681483e562f1438f2f4224401580ed4758', 'message': 'Add tripleo_templates_upload module\n\nThis adds a new module to upload templates to plan as a tar\nfile.\n\nChange-Id: I7ac4c328ec7f4dd0fb4aa7495945fbc57feb4bbc\nDepends-On: https://review.opendev.org/712599\n'}, {'number': 4, 'created': '2020-03-19 19:34:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/a003cb03367cd59b7a33e94c04379b8df24c1069', 'message': 'Add tripleo_templates_upload module\n\nThis adds a new module to upload templates to plan as a tar\nfile.\n\nChange-Id: I7ac4c328ec7f4dd0fb4aa7495945fbc57feb4bbc\nDepends-On: https://review.opendev.org/712599\n'}, {'number': 5, 'created': '2020-03-19 21:51:07.000000000', 'files': ['tripleo_ansible/ansible_plugins/modules/tripleo_templates_upload.py', 'tox.ini', 'doc/source/modules/modules-tripleo_templates_upload.rst'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/e8c9b421f9d82c85b00d42821cb766efc0685d93', 'message': 'Add tripleo_templates_upload module\n\nThis adds a new module to upload templates to plan as a tar\nfile.\n\nChange-Id: I7ac4c328ec7f4dd0fb4aa7495945fbc57feb4bbc\nDepends-On: https://review.opendev.org/712599\nSigned-off-by: Kevin Carter <kecarter@redhat.com>\n'}]",6,712603,e8c9b421f9d82c85b00d42821cb766efc0685d93,29,6,5,8833,,,0,"Add tripleo_templates_upload module

This adds a new module to upload templates to plan as a tar
file.

Change-Id: I7ac4c328ec7f4dd0fb4aa7495945fbc57feb4bbc
Depends-On: https://review.opendev.org/712599
Signed-off-by: Kevin Carter <kecarter@redhat.com>
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/03/712603/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/build/html/.gitkeep', 'tripleo_ansible/ansible_plugins/modules/tripleo_templates_upload.py', 'doc/source/modules/modules-tripleo_templates_upload.rst']",3,dd1899a0c187e0474ecb46513be3e8397e779938,mistral_to_ansible,================================= Module - tripleo_templates_upload ================================= This module provides for the following ansible plugin: * tripleo_templates_upload .. ansibleautoplugin:: :module: tripleo_ansible/ansible_plugins/modules/tripleo_templates_upload.py :documentation: true :examples: true ,,144,0
openstack%2Fproject-config~master~If64a8e91adaa4d4c3e23ba961eaeed7bd1ae9f9f,openstack/project-config,master,If64a8e91adaa4d4c3e23ba961eaeed7bd1ae9f9f,Update jobs for infra-manual,MERGED,2020-03-20 16:30:53.000000000,2020-03-20 17:29:15.000000000,2020-03-20 17:29:15.000000000,"[{'_account_id': 1}, {'_account_id': 2}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 16:30:53.000000000', 'files': ['zuul.d/projects.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/97b61fbfc25c63b291a890a06c5af7df642830e1', 'message': 'Update jobs for infra-manual\n\nTo be able to use opendev-promote-docs, we need the opendev-tox-docs\njob, update the configuration for infra-manual.\n\nChange-Id: If64a8e91adaa4d4c3e23ba961eaeed7bd1ae9f9f\n'}]",0,714166,97b61fbfc25c63b291a890a06c5af7df642830e1,7,3,1,6547,,,0,"Update jobs for infra-manual

To be able to use opendev-promote-docs, we need the opendev-tox-docs
job, update the configuration for infra-manual.

Change-Id: If64a8e91adaa4d4c3e23ba961eaeed7bd1ae9f9f
",git fetch https://review.opendev.org/openstack/project-config refs/changes/66/714166/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/projects.yaml'],1,97b61fbfc25c63b291a890a06c5af7df642830e1,fix-infra-manual, check: jobs: - opendev-tox-docs gate: jobs: - opendev-tox-docs, templates: - build-tox-docs,6,2
openstack%2Fproject-config~master~I815c71fc7e924bd7fcfbe7851caad92d370da812,openstack/project-config,master,I815c71fc7e924bd7fcfbe7851caad92d370da812,Don't load zuul objects from opendev/gerrit,MERGED,2020-03-20 16:05:55.000000000,2020-03-20 17:29:12.000000000,2020-03-20 17:29:12.000000000,"[{'_account_id': 1}, {'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 16:05:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/3cd459f4e472cd943dfd93155261add8c5242c37', 'message': ""Stop tracking gerrit upstream\n\nWe're not doing local dev any more and dont' need this. Plus,\nit's breaking zuul.\n\nChange-Id: I815c71fc7e924bd7fcfbe7851caad92d370da812\n""}, {'number': 2, 'created': '2020-03-20 16:12:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/7d8432d83ae1a582b3cfecc5e8ca16fead610572', 'message': ""Don't load zuul objects from opendev/gerrit\n\nWe added config to gerrit upstream, which is now flowing down\nand confusing our zuul. We don't have in-repo job config anyway\nsince we're building images in system-config.\n\nChange-Id: I815c71fc7e924bd7fcfbe7851caad92d370da812\n""}, {'number': 3, 'created': '2020-03-20 16:21:08.000000000', 'files': ['zuul/main.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/775266b6b680d6bcd36f4c0fb4431b74202017b3', 'message': ""Don't load zuul objects from opendev/gerrit\n\nWe added config to gerrit upstream, which is now flowing down\nand confusing our zuul. We don't have in-repo job config anyway\nsince we're building images in system-config.\n\nChange-Id: I815c71fc7e924bd7fcfbe7851caad92d370da812\n""}]",2,714160,775266b6b680d6bcd36f4c0fb4431b74202017b3,11,3,3,2,,,0,"Don't load zuul objects from opendev/gerrit

We added config to gerrit upstream, which is now flowing down
and confusing our zuul. We don't have in-repo job config anyway
since we're building images in system-config.

Change-Id: I815c71fc7e924bd7fcfbe7851caad92d370da812
",git fetch https://review.opendev.org/openstack/project-config refs/changes/60/714160/1 && git format-patch -1 --stdout FETCH_HEAD,['gerrit/projects.yaml'],1,3cd459f4e472cd943dfd93155261add8c5242c37,,, options: - track-upstream,0,2
openstack%2Fnova~master~Id3066c198b616fd5a82ccacde49776f1ec6cf221,openstack/nova,master,Id3066c198b616fd5a82ccacde49776f1ec6cf221,Introduce scope_types in os-flavor-access,MERGED,2020-03-18 00:46:16.000000000,2020-03-20 17:25:58.000000000,2020-03-20 17:15:59.000000000,"[{'_account_id': 782}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-03-18 00:46:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/828ced9f2e98ae0dba2b71a975a0810dec2332eb', 'message': ""Introduce scope_types in os-flavor-access\n\noslo.policy introduced the scope_type feature which can\ncontrol the access level at system-level and project-level.\n - https://docs.openstack.org/oslo.policy/latest/user/usage.html#setting-scope\n - http://specs.openstack.org/openstack/keystone-specs/specs/keystone/queens/system-scope.html\n\nAppropriate scope_type for nova case:\n- https://specs.openstack.org/openstack/nova-specs/specs/ussuri/approved/policy-defaults-refresh.html#scope\n\nThis commit introduce scope_type for os-flavor-access API policies\nas 'system'. List flavor access policy is allowed for everyone even\nit is default as admin_or_owner we can correct it to admin only\nwhile intriducing the new defaults. Details: Bug#1867840\n\nAlso adds the test case with scope_type enabled and verify we\npass and fail the policy check with expected context.\n\nPartial implement blueprint policy-defaults-refresh\n\nChange-Id: Id3066c198b616fd5a82ccacde49776f1ec6cf221\n""}, {'number': 2, 'created': '2020-03-18 00:48:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/7098b32056002e8ee4c2ceacc86c9d21910bfd8b', 'message': ""Introduce scope_types in os-flavor-access\n\noslo.policy introduced the scope_type feature which can\ncontrol the access level at system-level and project-level.\n - https://docs.openstack.org/oslo.policy/latest/user/usage.html#setting-scope\n - http://specs.openstack.org/openstack/keystone-specs/specs/keystone/queens/system-scope.html\n\nAppropriate scope_type for nova case:\n- https://specs.openstack.org/openstack/nova-specs/specs/ussuri/approved/policy-defaults-refresh.html#scope\n\nThis commit introduce scope_type for os-flavor-access API policies\nas 'system'. List flavor access policy is allowed for everyone even\nit is default as admin_or_owner we can correct it to admin only\nwhile intriducing the new defaults. Details: Bug#1867840\n\nAlso adds the test case with scope_type enabled and verify we\npass and fail the policy check with expected context.\n\nPartial implement blueprint policy-defaults-refresh\n\nChange-Id: Id3066c198b616fd5a82ccacde49776f1ec6cf221\n""}, {'number': 3, 'created': '2020-03-18 01:05:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/2aafb26907737608d9ab9681a5db4fe665275831', 'message': ""Introduce scope_types in os-flavor-access\n\noslo.policy introduced the scope_type feature which can\ncontrol the access level at system-level and project-level.\n - https://docs.openstack.org/oslo.policy/latest/user/usage.html#setting-scope\n - http://specs.openstack.org/openstack/keystone-specs/specs/keystone/queens/system-scope.html\n\nAppropriate scope_type for nova case:\n- https://specs.openstack.org/openstack/nova-specs/specs/ussuri/approved/policy-defaults-refresh.html#scope\n\nThis commit introduce scope_type for os-flavor-access API policies\nas 'system'. List flavor access policy is allowed for everyone even\nit is default as admin_or_owner we can correct it to admin only\nwhile intriducing the new defaults. Details: Bug#1867840\n\nAlso adds the test case with scope_type enabled and verify we\npass and fail the policy check with expected context.\n\nPartial implement blueprint policy-defaults-refresh\n\nChange-Id: Id3066c198b616fd5a82ccacde49776f1ec6cf221\n""}, {'number': 4, 'created': '2020-03-18 01:06:14.000000000', 'files': ['nova/policies/flavor_access.py', 'nova/tests/unit/policies/test_flavor_access.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/741c0f60bde282c5cb7b94f4574c3f6be697a207', 'message': ""Introduce scope_types in os-flavor-access\n\noslo.policy introduced the scope_type feature which can\ncontrol the access level at system-level and project-level.\n - https://docs.openstack.org/oslo.policy/latest/user/usage.html#setting-scope\n - http://specs.openstack.org/openstack/keystone-specs/specs/keystone/queens/system-scope.html\n\nAppropriate scope_type for nova case:\n- https://specs.openstack.org/openstack/nova-specs/specs/ussuri/approved/policy-defaults-refresh.html#scope\n\nThis commit introduce scope_type for os-flavor-access API policies\nas 'system'. List flavor access policy is allowed for everyone even\nit is default as admin_or_owner we can correct it to admin only\nwhile intriducing the new defaults. Details: Bug#1867840\n\nAlso adds the test case with scope_type enabled and verify we\npass and fail the policy check with expected context.\n\nPartial implement blueprint policy-defaults-refresh\n\nChange-Id: Id3066c198b616fd5a82ccacde49776f1ec6cf221\n""}]",1,713559,741c0f60bde282c5cb7b94f4574c3f6be697a207,33,11,4,8556,,,0,"Introduce scope_types in os-flavor-access

oslo.policy introduced the scope_type feature which can
control the access level at system-level and project-level.
 - https://docs.openstack.org/oslo.policy/latest/user/usage.html#setting-scope
 - http://specs.openstack.org/openstack/keystone-specs/specs/keystone/queens/system-scope.html

Appropriate scope_type for nova case:
- https://specs.openstack.org/openstack/nova-specs/specs/ussuri/approved/policy-defaults-refresh.html#scope

This commit introduce scope_type for os-flavor-access API policies
as 'system'. List flavor access policy is allowed for everyone even
it is default as admin_or_owner we can correct it to admin only
while intriducing the new defaults. Details: Bug#1867840

Also adds the test case with scope_type enabled and verify we
pass and fail the policy check with expected context.

Partial implement blueprint policy-defaults-refresh

Change-Id: Id3066c198b616fd5a82ccacde49776f1ec6cf221
",git fetch https://review.opendev.org/openstack/nova refs/changes/59/713559/3 && git format-patch -1 --stdout FETCH_HEAD,"['nova/policies/flavor_access.py', 'nova/tests/unit/policies/test_flavor_access.py']",2,828ced9f2e98ae0dba2b71a975a0810dec2332eb,bp/policy-defaults-refresh," # Check that system admin is able to add/remove flavor access # to a tenant. self.admin_authorized_contexts = [ self.system_admin_context] # Check that non-system-admin is not able to add/remove flavor access # to a tenant. self.admin_unauthorized_contexts = [ self.legacy_admin_context, self.system_member_context, self.system_reader_context, self.project_admin_context, self.system_foo_context, self.project_member_context, self.other_project_member_context, self.project_foo_context, self.project_reader_context ] # Check that system user is able to list flavor access # information. self.admin_or_owner_authorized_contexts = [ self.system_admin_context, self.system_member_context, self.system_reader_context, self.system_foo_cotext] # Check that non-system is not able to list flavor access # information. self.admin_or_owner_unauthorized_contexts = [ self.legacy_admin_context, self.other_project_member_context, self.project_admin_context, self.project_member_context, self.project_reader_context, self.project_foo_context]",,51,15
openstack%2Fopenstack-zuul-jobs~master~I9df35e3e7093fb9d00774775fb50f2dda246899f,openstack/openstack-zuul-jobs,master,I9df35e3e7093fb9d00774775fb50f2dda246899f,Follow devstack-plugin-nfs rename,MERGED,2020-03-20 15:58:12.000000000,2020-03-20 17:22:38.000000000,2020-03-20 17:13:21.000000000,"[{'_account_id': 2}, {'_account_id': 1004}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 15:58:12.000000000', 'files': ['playbooks/legacy/tempest-dsvm-full-devstack-plugin-nfs/run.yaml', 'zuul.d/zuul-legacy-jobs.yaml', 'playbooks/legacy/tempest-dsvm-multibackend-matrix/run.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-zuul-jobs/commit/18a9713d90dfad376c680c4259291f193e40aa52', 'message': 'Follow devstack-plugin-nfs rename\n\nThe repo was renamed from x/devstack-plugin-nfs to\nopenstack/devstack-plugin-nfs - follow rename.\n\nChange-Id: I9df35e3e7093fb9d00774775fb50f2dda246899f\n'}]",0,714157,18a9713d90dfad376c680c4259291f193e40aa52,8,3,1,6547,,,0,"Follow devstack-plugin-nfs rename

The repo was renamed from x/devstack-plugin-nfs to
openstack/devstack-plugin-nfs - follow rename.

Change-Id: I9df35e3e7093fb9d00774775fb50f2dda246899f
",git fetch https://review.opendev.org/openstack/openstack-zuul-jobs refs/changes/57/714157/1 && git format-patch -1 --stdout FETCH_HEAD,"['playbooks/legacy/tempest-dsvm-full-devstack-plugin-nfs/run.yaml', 'zuul.d/zuul-legacy-jobs.yaml', 'playbooks/legacy/tempest-dsvm-multibackend-matrix/run.yaml']",3,18a9713d90dfad376c680c4259291f193e40aa52,devstack-nfs-rename," export PROJECTS=""openstack/devstack-plugin-nfs $PROJECTS"""," export PROJECTS=""x/devstack-plugin-nfs $PROJECTS""",5,5
openstack%2Fkolla~master~Id67c58580d1eaad86cacc5094ee80a734ac1eb26,openstack/kolla,master,Id67c58580d1eaad86cacc5094ee80a734ac1eb26,CentOS 8: enable monasca-grafana image,MERGED,2020-03-13 19:42:34.000000000,2020-03-20 17:18:40.000000000,2020-03-20 15:57:07.000000000,"[{'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 22629}, {'_account_id': 23181}, {'_account_id': 24072}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-13 19:42:34.000000000', 'files': ['kolla/image/build.py'], 'web_link': 'https://opendev.org/openstack/kolla/commit/b0c1647352dda3e785612c6c66e07b97290ae85d', 'message': 'CentOS 8: enable monasca-grafana image\n\nChange-Id: Id67c58580d1eaad86cacc5094ee80a734ac1eb26\nPartially-Implements: blueprint centos-rhel-8\n'}]",0,713054,b0c1647352dda3e785612c6c66e07b97290ae85d,14,6,1,14826,,,0,"CentOS 8: enable monasca-grafana image

Change-Id: Id67c58580d1eaad86cacc5094ee80a734ac1eb26
Partially-Implements: blueprint centos-rhel-8
",git fetch https://review.opendev.org/openstack/kolla refs/changes/54/713054/1 && git format-patch -1 --stdout FETCH_HEAD,['kolla/image/build.py'],1,b0c1647352dda3e785612c6c66e07b97290ae85d,bp/centos-rhel-8,," ""monasca-grafana"", # Using python2",0,1
openstack%2Fneutron-tempest-plugin~master~I54c9f041f2f971219c32005b3fa573c06f0110ef,openstack/neutron-tempest-plugin,master,I54c9f041f2f971219c32005b3fa573c06f0110ef,Fix how nc is called in qos test,MERGED,2020-03-16 12:18:11.000000000,2020-03-20 17:15:56.000000000,2020-03-20 17:15:56.000000000,"[{'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 13995}, {'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-16 12:18:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/68b12e6d014d2dfb241da738462fbfde3f81037e', 'message': ""Fix how nc is called in qos test\n\nIt is similar change to what was done in [1] for port forwarding\ntests.\nIn QoS test, when ncat is run on guest vm, it's also send in the\nbackground so when using ssh_client.exec_command() can block on\nwaiting for fd from remote host.\nUsing ssh_client.execute_script() method solves this issue.\n\n[1] https://review.opendev.org/#/c/705926/\n\nChange-Id: I54c9f041f2f971219c32005b3fa573c06f0110ef\n""}, {'number': 2, 'created': '2020-03-16 13:33:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/178c3e8e9e64fc456f27d7130384bae3ca8c9124', 'message': ""Fix how nc is called in qos test\n\nIt is similar change to what was done in [1] for port forwarding\ntests.\nIn QoS test, when ncat is run on guest vm, it's also send in the\nbackground so when using ssh_client.exec_command() can block on\nwaiting for fd from remote host.\nUsing ssh_client.execute_script() method solves this issue.\n\n[1] https://review.opendev.org/#/c/705926/\n\nChange-Id: I54c9f041f2f971219c32005b3fa573c06f0110ef\n""}, {'number': 3, 'created': '2020-03-17 15:27:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/70eb43d15620fa1592c82f26d2f07395c493b8fd', 'message': ""Fix how nc is called in qos test\n\nIt is similar change to what was done in [1] for port forwarding\ntests.\nIn QoS test, when ncat is run on guest vm, it's also send in the\nbackground so when using ssh_client.exec_command() can block on\nwaiting for fd from remote host.\nUsing ssh_client.execute_script() method solves this issue.\n\n[1] https://review.opendev.org/#/c/705926/\n\nChange-Id: I54c9f041f2f971219c32005b3fa573c06f0110ef\n""}, {'number': 4, 'created': '2020-03-18 12:02:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/b8932caf2e73b98a51e2ccd154650ff6ea38e9d3', 'message': 'Fix how nc is called in qos test\n\nWe have already nc_listen method in base scenario tests class.\nIt was since now used only in port_forwarding tests but we can\nreuse it in QoS tests also.\n\nThis patch also makes ""server"" attribute to be optional in nc_listen\nmethod. It is used only to log console output in case when ssh to the\nserver wouldn\'t work as expected. And if server is not given,\n_log_console_output() method will list all servers which belongs to\ntenant and log console for each of them.\n\nChange-Id: I54c9f041f2f971219c32005b3fa573c06f0110ef\n'}, {'number': 5, 'created': '2020-03-18 13:26:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/7f1bbf2c268b883737b2ce009c2cb6826a095201', 'message': 'Fix how nc is called in qos test\n\nWe have already nc_listen method in base scenario tests class.\nIt was since now used only in port_forwarding tests but we can\nreuse it in QoS tests also.\n\nThis patch also makes ""server"" attribute to be optional in nc_listen\nmethod. It is used only to log console output in case when ssh to the\nserver wouldn\'t work as expected. And if server is not given,\n_log_console_output() method will list all servers which belongs to\ntenant and log console for each of them.\n\nChange-Id: I54c9f041f2f971219c32005b3fa573c06f0110ef\n'}, {'number': 6, 'created': '2020-03-19 14:38:16.000000000', 'files': ['neutron_tempest_plugin/scenario/test_qos.py', 'neutron_tempest_plugin/common/utils.py', 'neutron_tempest_plugin/scenario/test_port_forwardings.py', 'neutron_tempest_plugin/scenario/base.py'], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/fd4141f2015d25f1b009d7cf2ebdd2907cd8e81a', 'message': 'Fix how nc is called in qos test\n\nWe have already nc_listen method in base scenario tests class.\nIt was since now used only in port_forwarding tests but we can\nreuse it in QoS tests also.\n\nThere was also problem with spawning ncat process, that sometimes,\nwithout any clear reason for me, process wasn\'t spawned at all.\nThat caused failure of test.\n\nSo this patch adds new method ensure_nc_listen() which spawns ncat\nprocess on remote host and checkes if process is really spawned. That\nway we can avoid problems with not spawned ncat process.\n\nThis patch also makes ""server"" attribute to be optional in nc_listen\nmethod. It is used only to log console output in case when ssh to the\nserver wouldn\'t work as expected. And if server is not given,\n_log_console_output() method will list all servers which belongs to\ntenant and log console for each of them.\n\nCloses-Bug: #1868100\n\nChange-Id: I54c9f041f2f971219c32005b3fa573c06f0110ef\n'}]",5,713208,fd4141f2015d25f1b009d7cf2ebdd2907cd8e81a,30,6,6,11975,,,0,"Fix how nc is called in qos test

We have already nc_listen method in base scenario tests class.
It was since now used only in port_forwarding tests but we can
reuse it in QoS tests also.

There was also problem with spawning ncat process, that sometimes,
without any clear reason for me, process wasn't spawned at all.
That caused failure of test.

So this patch adds new method ensure_nc_listen() which spawns ncat
process on remote host and checkes if process is really spawned. That
way we can avoid problems with not spawned ncat process.

This patch also makes ""server"" attribute to be optional in nc_listen
method. It is used only to log console output in case when ssh to the
server wouldn't work as expected. And if server is not given,
_log_console_output() method will list all servers which belongs to
tenant and log console for each of them.

Closes-Bug: #1868100

Change-Id: I54c9f041f2f971219c32005b3fa573c06f0110ef
",git fetch https://review.opendev.org/openstack/neutron-tempest-plugin refs/changes/08/713208/3 && git format-patch -1 --stdout FETCH_HEAD,['neutron_tempest_plugin/scenario/test_qos.py'],1,68b12e6d014d2dfb241da738462fbfde3f81037e,bz/1812059," ssh_client.execute_script(cmd, timeout=5)"," ssh_client.exec_command(cmd, timeout=5)",1,1
openstack%2Fpython-openstackclient~master~I4faabbb107f912c7ed1cc5d3467ea5a94197d4a0,openstack/python-openstackclient,master,I4faabbb107f912c7ed1cc5d3467ea5a94197d4a0,"Replace port 35357 with 5000 for ""auth_url""",MERGED,2018-07-18 08:51:27.000000000,2020-03-20 17:15:49.000000000,2020-03-20 17:07:25.000000000,"[{'_account_id': 6482}, {'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 25564}]","[{'number': 1, 'created': '2018-07-18 08:51:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/5a2a1d46bd1fcb04d64be82b0623115c6b61c3ea', 'message': 'Replace port 35357 with 5000 for ""auth_url""\n\nBased on the change in Keystone Install Guide [1],\nthis patch replace port 35357 with 5000 for ""auth_url"".\n\nFor more details, please check similar changes which have been done\non other projects: Nova [2], Neutron [3], Cinder [4], Glance [5].\n\n[1] https://review.openstack.org/#/c/541857\n[2] https://review.openstack.org/#/c/562812\n[3] https://review.openstack.org/#/c/566491\n[4] https://review.openstack.org/#/c/565464\n[5] https://review.openstack.org/#/c/558932\n\nChange-Id: I4faabbb107f912c7ed1cc5d3467ea5a94197d4a0\n'}, {'number': 2, 'created': '2018-09-17 04:16:13.000000000', 'files': ['doc/source/cli/man/openstack.rst', 'doc/source/configuration/index.rst'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/eb4f839ec7c1a5e791d6f798c5efb705d55b1e01', 'message': 'Replace port 35357 with 5000 for ""auth_url""\n\nBased on the change in Keystone Install Guide [1],\nthis patch replace port 35357 with 5000 for ""auth_url"".\n\nFor more details, please check similar changes which have been done\non other projects: Nova [2], Neutron [3], Cinder [4], Glance [5].\n\n[1] https://review.openstack.org/#/c/541857\n[2] https://review.openstack.org/#/c/562812\n[3] https://review.openstack.org/#/c/566491\n[4] https://review.openstack.org/#/c/565464\n[5] https://review.openstack.org/#/c/558932\n\nChange-Id: I4faabbb107f912c7ed1cc5d3467ea5a94197d4a0\n'}]",0,583514,eb4f839ec7c1a5e791d6f798c5efb705d55b1e01,11,4,2,28278,,,0,"Replace port 35357 with 5000 for ""auth_url""

Based on the change in Keystone Install Guide [1],
this patch replace port 35357 with 5000 for ""auth_url"".

For more details, please check similar changes which have been done
on other projects: Nova [2], Neutron [3], Cinder [4], Glance [5].

[1] https://review.openstack.org/#/c/541857
[2] https://review.openstack.org/#/c/562812
[3] https://review.openstack.org/#/c/566491
[4] https://review.openstack.org/#/c/565464
[5] https://review.openstack.org/#/c/558932

Change-Id: I4faabbb107f912c7ed1cc5d3467ea5a94197d4a0
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/14/583514/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/cli/man/openstack.rst', 'doc/source/configuration/index.rst']",2,5a2a1d46bd1fcb04d64be82b0623115c6b61c3ea,auth_url, auth_url: http://192.168.122.10:5000/ auth_url: http://192.168.122.10:5000/ auth_url: http://192.168.122.10:5000/ auth_url: http://192.168.122.10:5000/ auth_url: http://192.168.122.10:5000/, auth_url: http://192.168.122.10:35357/ auth_url: http://192.168.122.10:35357/ auth_url: http://192.168.122.10:35357/ auth_url: http://192.168.122.10:35357/ auth_url: http://192.168.122.10:35357/,7,7
openstack%2Fpython-openstackclient~master~I2f32d773c12d484e8c0e435a78a3fe16d0eeae03,openstack/python-openstackclient,master,I2f32d773c12d484e8c0e435a78a3fe16d0eeae03,Update the content about Import Format,MERGED,2018-05-22 07:45:00.000000000,2020-03-20 17:10:51.000000000,2020-03-20 17:07:24.000000000,"[{'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 26311}]","[{'number': 1, 'created': '2018-05-22 07:45:00.000000000', 'files': ['doc/source/contributor/developing.rst'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/819663d1343a5025e2f677803b0faa91acc83318', 'message': 'Update the content about Import Format\n\nFollowing by\nhttps://git.openstack.org/cgit/openstack-dev/hacking/tree/HACKING.rst#n71\n\nChange-Id: I2f32d773c12d484e8c0e435a78a3fe16d0eeae03\n'}]",0,569931,819663d1343a5025e2f677803b0faa91acc83318,8,3,1,19779,,,0,"Update the content about Import Format

Following by
https://git.openstack.org/cgit/openstack-dev/hacking/tree/HACKING.rst#n71

Change-Id: I2f32d773c12d484e8c0e435a78a3fe16d0eeae03
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/31/569931/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/contributor/developing.rst'],1,819663d1343a5025e2f677803b0faa91acc83318,rst_convention,"More information about Import Format, see `Import Order Guide <https://docs.openstack.org/hacking/latest/user/hacking.html#imports>`__... code-block:: none {{stdlib imports in human alphabetical order}} \n {{third-party lib imports in human alphabetical order}} \n {{project imports in human alphabetical order}} \n \n {{begin your code}}",.. _`Import Order Guide`: https://docs.openstack.org/hacking/latest/user/hacking.html#imports* {{stdlib imports in human alphabetical order}} * \n * {{third-party lib imports in human alphabetical order}} * \n * {{project imports in human alphabetical order}} * \n * \n * {{begin your code}},12,9
openstack%2Fpython-openstackclient~master~If00f2a31735b658c4c3267cacd406d2c6c7c91a0,openstack/python-openstackclient,master,If00f2a31735b658c4c3267cacd406d2c6c7c91a0,fix tox python3 overrides,ABANDONED,2018-10-03 02:32:45.000000000,2020-03-20 16:59:00.000000000,,"[{'_account_id': 841}, {'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 26285}, {'_account_id': 28543}]","[{'number': 1, 'created': '2018-10-03 02:32:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/bc5d46248fced6511146278e1264ca67cc062a51', 'message': 'fix tox python3 overrides\n\nWe want to default to running all tox environments under python 3, so\nset the basepython value in each environment.\n\nWe do not want to specify a minor version number, because we do not\nwant to have to update the file every time we upgrade python.\n\nWe do not want to set the override once in testenv, because that\nbreaks the more specific versions used in default environments like\npy35 and py36.\n\nChange-Id: If00f2a31735b658c4c3267cacd406d2c6c7c91a0\n'}, {'number': 2, 'created': '2018-11-05 06:54:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/e1397df66b8252988016d1211ef2edaae62842c3', 'message': 'fix tox python3 overrides\n\nWe want to default to running all tox environments under python 3, so\nset the basepython value in each environment.\n\nWe do not want to specify a minor version number, because we do not\nwant to have to update the file every time we upgrade python.\n\nWe do not want to set the override once in testenv, because that\nbreaks the more specific versions used in default environments like\npy35 and py36.\n\nChange-Id: If00f2a31735b658c4c3267cacd406d2c6c7c91a0\nCloses-Bug:  #1801661\n'}, {'number': 3, 'created': '2019-01-12 03:21:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/e572f95a19fbc28ad0032f56d9f0c658cb2a51ea', 'message': 'fix tox python3 overrides\n\nWe want to default to running all tox environments under python 3, so\nset the basepython value in each environment.\n\nWe do not want to specify a minor version number, because we do not\nwant to have to update the file every time we upgrade python.\n\nWe do not want to set the override once in testenv, because that\nbreaks the more specific versions used in default environments like\npy35 and py36.\n\nChange-Id: If00f2a31735b658c4c3267cacd406d2c6c7c91a0\nCloses-Bug:  #1801661\n'}, {'number': 4, 'created': '2019-01-15 06:21:29.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/df2741491ddfd3ba9bed140ecb7a0c5e6a58e710', 'message': 'fix tox python3 overrides\n\nWe want to default to running all tox environments under python 3, so\nset the basepython value in each environment.\n\nWe do not want to specify a minor version number, because we do not\nwant to have to update the file every time we upgrade python.\n\nWe do not want to set the override once in testenv, because that\nbreaks the more specific versions used in default environments like\npy35 and py36.\n\nChange-Id: If00f2a31735b658c4c3267cacd406d2c6c7c91a0\nCloses-Bug:  #1801661\n'}]",2,607453,df2741491ddfd3ba9bed140ecb7a0c5e6a58e710,14,5,4,26297,,,0,"fix tox python3 overrides

We want to default to running all tox environments under python 3, so
set the basepython value in each environment.

We do not want to specify a minor version number, because we do not
want to have to update the file every time we upgrade python.

We do not want to set the override once in testenv, because that
breaks the more specific versions used in default environments like
py35 and py36.

Change-Id: If00f2a31735b658c4c3267cacd406d2c6c7c91a0
Closes-Bug:  #1801661
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/53/607453/4 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,bc5d46248fced6511146278e1264ca67cc062a51,bug/1801661,basepython = python3basepython = python3basepython = python3basepython = python3,,4,0
openstack%2Freleases~master~I4fe4521b3c4891109e3bf702cd4cbd92ed29a25a,openstack/releases,master,I4fe4521b3c4891109e3bf702cd4cbd92ed29a25a,Switch karbor to cycle-with-rc,ABANDONED,2020-02-25 21:52:07.000000000,2020-03-20 16:57:37.000000000,,"[{'_account_id': 11904}, {'_account_id': 17068}, {'_account_id': 21224}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-02-25 21:52:07.000000000', 'files': ['deliverables/ussuri/karbor.yaml', 'deliverables/ussuri/karbor-dashboard.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/cb98adb2c10dd742ad3d10b7f2c03b54b0af4375', 'message': 'Switch karbor to cycle-with-rc\n\nDeliverables following the cycle-with-intermediary model are expected\nto release multiple times throughout the development cycle. Since\nkarbor only released once last cycle, and has not done a release\nyet this cycle, this proposes changing the release model to cycle-with-rc\nso the expectation is only that it will release an RC by the end of\nthe cycle prior to doing the final release.\n\nFor the team, please respond to this patch in one of the following ways:\n\n* -1 this patch and follow up with an intermediary release\n* +1 this patch and we will switch over the release model\n* -1 this patch, but acknowledge a release will be done prior to the\n  RC1 deadline.\n\nChange-Id: I4fe4521b3c4891109e3bf702cd4cbd92ed29a25a\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}]",0,709845,cb98adb2c10dd742ad3d10b7f2c03b54b0af4375,9,5,1,11904,,,0,"Switch karbor to cycle-with-rc

Deliverables following the cycle-with-intermediary model are expected
to release multiple times throughout the development cycle. Since
karbor only released once last cycle, and has not done a release
yet this cycle, this proposes changing the release model to cycle-with-rc
so the expectation is only that it will release an RC by the end of
the cycle prior to doing the final release.

For the team, please respond to this patch in one of the following ways:

* -1 this patch and follow up with an intermediary release
* +1 this patch and we will switch over the release model
* -1 this patch, but acknowledge a release will be done prior to the
  RC1 deadline.

Change-Id: I4fe4521b3c4891109e3bf702cd4cbd92ed29a25a
Signed-off-by: Sean McGinnis <sean.mcginnis@gmail.com>
",git fetch https://review.opendev.org/openstack/releases refs/changes/45/709845/1 && git format-patch -1 --stdout FETCH_HEAD,"['deliverables/ussuri/karbor.yaml', 'deliverables/ussuri/karbor-dashboard.yaml']",2,cb98adb2c10dd742ad3d10b7f2c03b54b0af4375,ussuri-cwi,release-model: cycle-with-rc,release-model: cycle-with-intermediary,2,2
openstack%2Ftripleo-quickstart~master~I594235ac06df98c1f9615cc6307d43ff7fce93c6,openstack/tripleo-quickstart,master,I594235ac06df98c1f9615cc6307d43ff7fce93c6,Set overcloud_container_cli to podman for c8,MERGED,2020-03-10 04:31:26.000000000,2020-03-20 16:57:04.000000000,2020-03-20 16:51:47.000000000,"[{'_account_id': 8449}, {'_account_id': 9592}, {'_account_id': 9976}, {'_account_id': 10969}, {'_account_id': 12393}, {'_account_id': 18846}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-03-10 04:31:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart/commit/879fa8804e28bc9811955de1b2e9bbe3f06dfec7', 'message': 'Set overcloud_container_cli to podman for c8\n\nChange-Id: I594235ac06df98c1f9615cc6307d43ff7fce93c6\nSigned-off-by: Chandan Kumar (raukadah) <chkumar@redhat.com>\n'}, {'number': 2, 'created': '2020-03-11 06:32:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart/commit/05af8a1b0613d0ae73a5a4dfee5744088d34593a', 'message': 'Set overcloud_container_cli to podman for c8\n\n\nDepends-On: https://review.opendev.org/#/c/710876/\n\nChange-Id: I594235ac06df98c1f9615cc6307d43ff7fce93c6\nSigned-off-by: Chandan Kumar (raukadah) <chkumar@redhat.com>\n'}, {'number': 3, 'created': '2020-03-11 09:30:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart/commit/14697d7d726a7e9d131a8f4dba0cb11f6de2fb3b', 'message': 'Set overcloud_container_cli to podman for c8\n\n\nDepends-On: https://review.opendev.org/712294\n\nChange-Id: I594235ac06df98c1f9615cc6307d43ff7fce93c6\nSigned-off-by: Chandan Kumar (raukadah) <chkumar@redhat.com>\n'}, {'number': 4, 'created': '2020-03-12 06:11:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart/commit/05ed747a2bfc84b02c6d5a46d17962f4e5a5344e', 'message': 'Set overcloud_container_cli to podman for c8\n\nIt also runs c8 multinode on fs030 changes.\n\nDepends-On: https://review.opendev.org/712294\n\nChange-Id: I594235ac06df98c1f9615cc6307d43ff7fce93c6\nSigned-off-by: Chandan Kumar (raukadah) <chkumar@redhat.com>\n'}, {'number': 5, 'created': '2020-03-12 09:44:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart/commit/e0fe168a1890906b864527bd0f156884a430a719', 'message': 'Set overcloud_container_cli to podman for c8\n\nIt also runs c8 multinode on fs030 changes.\n\nDepends-On: https://review.opendev.org/712294\n\nChange-Id: I594235ac06df98c1f9615cc6307d43ff7fce93c6\nSigned-off-by: Chandan Kumar (raukadah) <chkumar@redhat.com>\n'}, {'number': 6, 'created': '2020-03-16 17:54:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart/commit/f7609abf8e0abac815fe72defb44c40938bec644', 'message': 'Set overcloud_container_cli to podman for c8\n\nIt also runs c8 multinode on fs030 changes.\n\nDepends-On: https://review.opendev.org/712294\n\nChange-Id: I594235ac06df98c1f9615cc6307d43ff7fce93c6\nSigned-off-by: Chandan Kumar (raukadah) <chkumar@redhat.com>\n'}, {'number': 7, 'created': '2020-03-17 13:14:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart/commit/f24a0360f9568bde04473994266df7126c3c9063', 'message': 'Set overcloud_container_cli to podman for c8\n\nIt also runs c8 multinode on fs030 changes.\n\nDepends-On: https://review.opendev.org/713397\n\nChange-Id: I594235ac06df98c1f9615cc6307d43ff7fce93c6\nSigned-off-by: Chandan Kumar (raukadah) <chkumar@redhat.com>\n'}, {'number': 8, 'created': '2020-03-20 04:49:46.000000000', 'files': ['zuul.d/layout.yaml', 'config/general_config/featureset030.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart/commit/638a5296d52d4082fc59fcafff0430ff1153ce13', 'message': 'Set overcloud_container_cli to podman for c8\n\nIt also runs c8 multinode on fs030 changes.\n\nDepends-On: https://review.opendev.org/713397\n\nChange-Id: I594235ac06df98c1f9615cc6307d43ff7fce93c6\nSigned-off-by: Chandan Kumar (raukadah) <chkumar@redhat.com>\n'}]",4,712013,638a5296d52d4082fc59fcafff0430ff1153ce13,68,9,8,12393,,,0,"Set overcloud_container_cli to podman for c8

It also runs c8 multinode on fs030 changes.

Depends-On: https://review.opendev.org/713397

Change-Id: I594235ac06df98c1f9615cc6307d43ff7fce93c6
Signed-off-by: Chandan Kumar (raukadah) <chkumar@redhat.com>
",git fetch https://review.opendev.org/openstack/tripleo-quickstart refs/changes/13/712013/8 && git format-patch -1 --stdout FETCH_HEAD,['config/general_config/featureset030.yml'],1,879fa8804e28bc9811955de1b2e9bbe3f06dfec7,,"overcloud_container_cli: >- {% if release not in ['pike', 'queens', 'rocky', 'stein', 'rocky', 'train'] -%} podman {%- else -%} docker {%- endif -%}",overcloud_container_cli: docker,6,1
openstack%2Fcharm-cinder-purestorage~master~I824a47a4f441e8a4a6de0b68ae75e5eed9d9a858,openstack/charm-cinder-purestorage,master,I824a47a4f441e8a4a6de0b68ae75e5eed9d9a858,Rebuild for updates to charms.openstack,ABANDONED,2020-03-11 08:37:21.000000000,2020-03-20 16:50:53.000000000,,"[{'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-11 08:37:21.000000000', 'files': ['rebuild'], 'web_link': 'https://opendev.org/openstack/charm-cinder-purestorage/commit/e531070eeb3b6b3009fd2630039da4fb90785811', 'message': 'Rebuild for updates to charms.openstack\n\nChange-Id: I824a47a4f441e8a4a6de0b68ae75e5eed9d9a858\n'}]",0,712261,e531070eeb3b6b3009fd2630039da4fb90785811,4,2,1,13686,,,0,"Rebuild for updates to charms.openstack

Change-Id: I824a47a4f441e8a4a6de0b68ae75e5eed9d9a858
",git fetch https://review.opendev.org/openstack/charm-cinder-purestorage refs/changes/61/712261/1 && git format-patch -1 --stdout FETCH_HEAD,['rebuild'],1,e531070eeb3b6b3009fd2630039da4fb90785811,rebuild,3fef9e9b-3b33-4c1d-9fde-d2a783bbfa92,d1c16faa-4837-11ea-810e-97e1f362d969,1,1
openstack%2Fpython-openstackclient~master~I6634e4a3e32d8e982e5bdb42af6c04cdb68ae222,openstack/python-openstackclient,master,I6634e4a3e32d8e982e5bdb42af6c04cdb68ae222,Replace Chinese quotes to English quotes,ABANDONED,2018-02-05 10:28:27.000000000,2020-03-20 16:49:06.000000000,,"[{'_account_id': 841}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2018-02-05 10:28:27.000000000', 'files': ['releasenotes/notes/image_set_visibility-babf4ff2f687d465.yaml'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/67cff87c6ef9b57c45b5d627bad902fda85486c2', 'message': 'Replace Chinese quotes to English quotes\n\nChange-Id: I6634e4a3e32d8e982e5bdb42af6c04cdb68ae222\n'}]",0,540814,67cff87c6ef9b57c45b5d627bad902fda85486c2,5,3,1,27549,,,0,"Replace Chinese quotes to English quotes

Change-Id: I6634e4a3e32d8e982e5bdb42af6c04cdb68ae222
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/14/540814/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/notes/image_set_visibility-babf4ff2f687d465.yaml'],1,67cff87c6ef9b57c45b5d627bad902fda85486c2,fix0205," ""Community images"" will not appear in user's default", “Community images” will not appear in user’s default,1,1
openstack%2Fcharm-cinder-backup-swift-proxy~master~Id508110bc5d935886bb1542757c8dc0570218c2b,openstack/charm-cinder-backup-swift-proxy,master,Id508110bc5d935886bb1542757c8dc0570218c2b,Rebuild for updates to charms.openstack,ABANDONED,2020-03-11 08:36:58.000000000,2020-03-20 16:48:26.000000000,,"[{'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-11 08:36:58.000000000', 'files': ['rebuild'], 'web_link': 'https://opendev.org/openstack/charm-cinder-backup-swift-proxy/commit/2ab593e9a47baf7cff5f056f6e76e423ac4ab0b4', 'message': 'Rebuild for updates to charms.openstack\n\nChange-Id: Id508110bc5d935886bb1542757c8dc0570218c2b\n'}]",0,712260,2ab593e9a47baf7cff5f056f6e76e423ac4ab0b4,4,2,1,13686,,,0,"Rebuild for updates to charms.openstack

Change-Id: Id508110bc5d935886bb1542757c8dc0570218c2b
",git fetch https://review.opendev.org/openstack/charm-cinder-backup-swift-proxy refs/changes/60/712260/1 && git format-patch -1 --stdout FETCH_HEAD,['rebuild'],1,2ab593e9a47baf7cff5f056f6e76e423ac4ab0b4,rebuild,3fef9e9b-3b33-4c1d-9fde-d2a783bbfa92,d1c16faa-4837-11ea-810e-97e1f362d969,1,1
openstack%2Fcharm-manila-generic~master~I13f70173e905729c630c692429c7f9dcd809ee48,openstack/charm-manila-generic,master,I13f70173e905729c630c692429c7f9dcd809ee48,Rebuild for updates to charms.openstack,ABANDONED,2020-03-11 08:39:38.000000000,2020-03-20 16:47:42.000000000,,"[{'_account_id': 13686}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-11 08:39:38.000000000', 'files': ['rebuild'], 'web_link': 'https://opendev.org/openstack/charm-manila-generic/commit/04f18d328aba56c54de5f74e781055544f80a1f8', 'message': 'Rebuild for updates to charms.openstack\n\nChange-Id: I13f70173e905729c630c692429c7f9dcd809ee48\n'}]",0,712269,04f18d328aba56c54de5f74e781055544f80a1f8,7,3,1,13686,,,0,"Rebuild for updates to charms.openstack

Change-Id: I13f70173e905729c630c692429c7f9dcd809ee48
",git fetch https://review.opendev.org/openstack/charm-manila-generic refs/changes/69/712269/1 && git format-patch -1 --stdout FETCH_HEAD,['rebuild'],1,04f18d328aba56c54de5f74e781055544f80a1f8,rebuild,3fef9e9b-3b33-4c1d-9fde-d2a783bbfa92,d1c16faa-4837-11ea-810e-97e1f362d969,1,1
openstack%2Fpython-openstackclient~master~Ib0e41286082560e7bf1a823c562f986feafc5faa,openstack/python-openstackclient,master,Ib0e41286082560e7bf1a823c562f986feafc5faa,Use general py3 env in default tox run,ABANDONED,2018-01-22 10:48:08.000000000,2020-03-20 16:47:34.000000000,,"[{'_account_id': 6482}, {'_account_id': 22348}]","[{'number': 1, 'created': '2018-01-22 10:48:08.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/bcea36135f0b42d6b61af6c6dc7282d504f0efef', 'message': ""Use general py3 env in default tox run\n\nthere are already distros out there that ship with Python 3.6\n(e.g. Fedora 26, Ubuntu 17.10).\nTo ease developers life, make the default env list for bare 'tox' run\nuse whatever Python3 interpreter is avalable.\n\nThis does not preclude from runing tox with specific Python version\n(as in -epy3.5) as all those pyXY envs are autogenerated by tox.\nThis also does not affect CI as it always runs specific tox env\nas part of the job and never uses bare 'tox' with default env list.\n\nChange-Id: Ib0e41286082560e7bf1a823c562f986feafc5faa\n""}]",0,536304,bcea36135f0b42d6b61af6c6dc7282d504f0efef,4,2,1,9542,,,0,"Use general py3 env in default tox run

there are already distros out there that ship with Python 3.6
(e.g. Fedora 26, Ubuntu 17.10).
To ease developers life, make the default env list for bare 'tox' run
use whatever Python3 interpreter is avalable.

This does not preclude from runing tox with specific Python version
(as in -epy3.5) as all those pyXY envs are autogenerated by tox.
This also does not affect CI as it always runs specific tox env
as part of the job and never uses bare 'tox' with default env list.

Change-Id: Ib0e41286082560e7bf1a823c562f986feafc5faa
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/04/536304/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,bcea36135f0b42d6b61af6c6dc7282d504f0efef,py3-default,"envlist = py3,py27,pep8","envlist = py35,py27,pep8",1,1
openstack%2Fironic-python-agent~master~I1151945d3dc6a685d62ad49183919ef4a81962f8,openstack/ironic-python-agent,master,I1151945d3dc6a685d62ad49183919ef4a81962f8,Fix agent token vmedia use,MERGED,2020-03-19 21:38:05.000000000,2020-03-20 16:45:32.000000000,2020-03-20 14:24:51.000000000,"[{'_account_id': 10239}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 23851}]","[{'number': 1, 'created': '2020-03-19 21:38:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/1e18b2ece5d4bb83bccf8fba9a0d6107ba0b4c3f', 'message': 'Fix agent token vmedia use\n\nThe agent token was being passed through upon class invocation\nas opposed through direct configuration loading due to how IPA\nis designed to load items upon start-up.\n\nAs a result, we somewhere forgot to save the token to the api\nclient so heartbeats were being rejected when being forced\nto be enabled by default.\n\nThis has been corrected an an additional test class has been\nadded to help ensure that this operates as expected.\n\nChange-Id: I1151945d3dc6a685d62ad49183919ef4a81962f8\n'}, {'number': 2, 'created': '2020-03-20 08:54:01.000000000', 'files': ['ironic_python_agent/agent.py', 'ironic_python_agent/tests/unit/test_agent.py'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/c97a71d6f37f54563081bcfa71bf15be7894f3b5', 'message': 'Fix agent token vmedia use\n\nThe agent token was being passed through upon class invocation\nas opposed through direct configuration loading due to how IPA\nis designed to load items upon start-up.\n\nAs a result, we somewhere forgot to save the token to the api\nclient so heartbeats were being rejected when being forced\nto be enabled by default.\n\nThis has been corrected an an additional test class has been\nadded to help ensure that this operates as expected.\n\nChange-Id: I1151945d3dc6a685d62ad49183919ef4a81962f8\n'}]",0,713989,c97a71d6f37f54563081bcfa71bf15be7894f3b5,13,4,2,11655,,,0,"Fix agent token vmedia use

The agent token was being passed through upon class invocation
as opposed through direct configuration loading due to how IPA
is designed to load items upon start-up.

As a result, we somewhere forgot to save the token to the api
client so heartbeats were being rejected when being forced
to be enabled by default.

This has been corrected an an additional test class has been
added to help ensure that this operates as expected.

Change-Id: I1151945d3dc6a685d62ad49183919ef4a81962f8
",git fetch https://review.opendev.org/openstack/ironic-python-agent refs/changes/89/713989/2 && git format-patch -1 --stdout FETCH_HEAD,"['ironic_python_agent/agent.py', 'ironic_python_agent/tests/unit/test_agent.py']",2,1e18b2ece5d4bb83bccf8fba9a0d6107ba0b4c3f,," False, None) @mock.patch.object(hardware, '_md_scan_and_assemble', lambda: None) @mock.patch.object(hardware, '_check_for_iscsi', lambda: None) @mock.patch.object(hardware.GenericHardwareManager, 'wait_for_disks', lambda self: None) class TestBaseAgentVMediaToken(ironic_agent_base.IronicAgentTest): def setUp(self): super(TestBaseAgentVMediaToken, self).setUp() self.encoder = encoding.RESTJSONEncoder(indent=4) self.agent = agent.IronicPythonAgent('https://fake_api.example.' 'org:8081/', agent.Host('203.0.113.1', 9990), agent.Host('192.0.2.1', 9999), 3, 10, 'eth0', 300, 1, False, '1' * 128) self.agent.ext_mgr = extension.ExtensionManager.\ make_test_instance([extension.Extension('fake', None, FakeExtension, FakeExtension())]) self.sample_nw_iface = hardware.NetworkInterface( ""eth9"", ""AA:BB:CC:DD:EE:FF"", ""1.2.3.4"", True) hardware.NODE = None @mock.patch( 'ironic_python_agent.hardware_managers.cna._detect_cna_card', mock.Mock()) @mock.patch.object(hardware, 'dispatch_to_managers', autospec=True) @mock.patch.object(agent.IronicPythonAgent, '_wait_for_interface', autospec=True) @mock.patch('oslo_service.wsgi.Server', autospec=True) @mock.patch.object(hardware, 'get_managers', autospec=True) def test_run_agent_token_vmedia(self, mock_get_managers, mock_wsgi, mock_wait, mock_dispatch): CONF.set_override('inspection_callback_url', '') wsgi_server = mock_wsgi.return_value def set_serve_api(): self.agent.serve_api = False wsgi_server.start.side_effect = set_serve_api self.agent.heartbeater = mock.Mock() self.agent.api_client.lookup_node = mock.Mock() self.agent.api_client.lookup_node.return_value = { 'node': { 'uuid': 'deadbeef-dabb-ad00-b105-f00d00bab10c' }, 'config': { 'heartbeat_timeout': 300, 'agent_token': '********', 'agent_token_required': True } } self.agent.run() mock_wsgi.assert_called_once_with(CONF, 'ironic-python-agent', app=self.agent.api, host=mock.ANY, port=9999) wsgi_server.start.assert_called_once_with() mock_wait.assert_called_once_with(mock.ANY) self.assertEqual([mock.call('list_hardware_info'), mock.call('wait_for_disks')], mock_dispatch.call_args_list) self.agent.heartbeater.start.assert_called_once_with() self.assertEqual('1' * 128, self.agent.agent_token) self.assertEqual('1' * 128, self.agent.api_client.agent_token)"," None, False)",87,4
openstack%2Freleases~master~If4add04307cf459c92b8ee563a3e3e4a512e2fdc,openstack/releases,master,If4add04307cf459c92b8ee563a3e3e4a512e2fdc,Release cloudkitty deliverables for stable/rocky,MERGED,2020-03-20 15:57:06.000000000,2020-03-20 16:43:48.000000000,2020-03-20 16:43:48.000000000,"[{'_account_id': 11904}, {'_account_id': 22348}, {'_account_id': 29503}]","[{'number': 1, 'created': '2020-03-20 15:57:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/c51376f330bcdeb2c8c98be66b9c11221de5061f', 'message': 'Release cloudkitty deliverables for stable/rocky\n\nChange-Id: If4add04307cf459c92b8ee563a3e3e4a512e2fdc\n'}, {'number': 2, 'created': '2020-03-20 16:04:37.000000000', 'files': ['deliverables/rocky/cloudkitty.yaml', 'deliverables/rocky/python-cloudkittyclient.yaml', 'deliverables/rocky/cloudkitty-dashboard.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/1002abbc240c866756fc8c1ef09a7528f405fbe4', 'message': 'Release cloudkitty deliverables for stable/rocky\n\nChange-Id: If4add04307cf459c92b8ee563a3e3e4a512e2fdc\n'}]",3,714155,1002abbc240c866756fc8c1ef09a7528f405fbe4,9,3,2,29503,,,0,"Release cloudkitty deliverables for stable/rocky

Change-Id: If4add04307cf459c92b8ee563a3e3e4a512e2fdc
",git fetch https://review.opendev.org/openstack/releases refs/changes/55/714155/1 && git format-patch -1 --stdout FETCH_HEAD,"['deliverables/rocky/cloudkitty.yaml', 'deliverables/rocky/python-cloudkittyclient.yaml', 'deliverables/rocky/cloudkitty-dashboard.yaml']",3,c51376f330bcdeb2c8c98be66b9c11221de5061f,, - version: 8.0.0 projects: - repo: openstack/cloudkitty-dashboard hash: 6e7db6c425c00cec907cd1cd88e5af78bfaa8898,,12,0
openstack%2Fopenstack-helm~master~If142b0810cfffcd9e986ba1483a20ca2db7ce078,openstack/openstack-helm,master,If142b0810cfffcd9e986ba1483a20ca2db7ce078,Remove Ceph references when configured with NFS,ABANDONED,2019-10-29 22:32:11.000000000,2020-03-20 16:40:41.000000000,,"[{'_account_id': 22348}, {'_account_id': 22636}]","[{'number': 1, 'created': '2019-10-29 22:32:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/3fb9c6139e1c495d133e00f6df24410efc464406', 'message': 'Remove Ceph references when configured with NFS\n\nThis patchset removes references in the Nova chart to Ceph when\nbeing tested by a job that uses NFS instead of Ceph. To do that,\nthis patch uses the feature gate and associated values overrides\nto ensure that ceph is not configured when running NFS.\n\nChange-Id: If142b0810cfffcd9e986ba1483a20ca2db7ce078\n'}, {'number': 2, 'created': '2019-11-07 21:12:13.000000000', 'files': ['zuul.d/jobs-openstack-helm.yaml', 'nova/values_overrides/nfs.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/23b5bade42ceeb6e42387b429dd11a510cf46c8d', 'message': 'Remove Ceph references when configured with NFS\n\nThis patchset removes references in the Nova chart to Ceph when\nbeing tested by a job that uses NFS instead of Ceph. To do that,\nthis patch uses the feature gate and associated values overrides\nto ensure that ceph is not configured when running NFS.\n\nChange-Id: If142b0810cfffcd9e986ba1483a20ca2db7ce078\n'}]",0,691999,23b5bade42ceeb6e42387b429dd11a510cf46c8d,7,2,2,22636,,,0,"Remove Ceph references when configured with NFS

This patchset removes references in the Nova chart to Ceph when
being tested by a job that uses NFS instead of Ceph. To do that,
this patch uses the feature gate and associated values overrides
to ensure that ceph is not configured when running NFS.

Change-Id: If142b0810cfffcd9e986ba1483a20ca2db7ce078
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/99/691999/2 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/jobs-openstack-helm.yaml', 'nova/values_overrides/nfs.yaml']",2,3fb9c6139e1c495d133e00f6df24410efc464406,,conf: ceph: enabled: false ,,12,1
openstack%2Fkayobe~master~I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643,openstack/kayobe,master,I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643,CentOS 8: Support DNF,MERGED,2020-03-03 16:00:13.000000000,2020-03-20 16:38:12.000000000,2020-03-19 18:34:10.000000000,"[{'_account_id': 14826}, {'_account_id': 15197}, {'_account_id': 17669}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-03 16:00:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/bd1025e41c111b1c2ea69eb9d8035394d85d8009', 'message': 'WIP: CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 2, 'created': '2020-03-03 16:08:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/464c3643de1ffd26079bdbc7d72ea1dc5c8b39da', 'message': 'WIP: CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 3, 'created': '2020-03-03 16:15:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/15351d3a109ce215d3153af68ab22c50fc93068b', 'message': 'WIP: CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 4, 'created': '2020-03-03 17:33:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/1282e35db1e70b507fe035931ad9575cdb7d365d', 'message': 'WIP: CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 5, 'created': '2020-03-03 17:46:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/2044f5144c1faa6f2742d6befbe9462c76f8c271', 'message': 'WIP: CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 6, 'created': '2020-03-04 10:13:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/97b7466a3016f399c4b9e28d640d0a0ec5cd19ae', 'message': 'WIP: CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 7, 'created': '2020-03-04 11:58:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/f9ff92adbf773d7e292b3e918b10a95e003369ec', 'message': 'WIP: CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 8, 'created': '2020-03-04 13:24:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/38af14573602c1c88c97faa28950f0145fe6ebde', 'message': 'WIP: CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 9, 'created': '2020-03-04 13:48:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/5ad1740cef0dbebf575e9bbc5992622c83dd498d', 'message': 'WIP: CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 10, 'created': '2020-03-04 14:50:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/c0184f2784a25e25913296b75805723403ee5b68', 'message': 'WIP: CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 11, 'created': '2020-03-04 16:31:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/eba73c50d94d168168addab39290d5afcb399c09', 'message': 'WIP: CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 12, 'created': '2020-03-04 18:03:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/7fa3d5bb9556d8d945036d7401908038e20f2fa6', 'message': 'CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 13, 'created': '2020-03-05 12:02:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/5cff6d6c5a2d393b7d18ec7731cb94e10aac19a7', 'message': 'CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 14, 'created': '2020-03-06 12:28:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/ae48fbb0edbfd89f4f11dfd2590c16e8f9ede60d', 'message': 'CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 15, 'created': '2020-03-12 16:49:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kayobe/commit/8adca0598ccbeaab5aa052f0727e2666a9c7c0c2', 'message': 'CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-crom.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}, {'number': 16, 'created': '2020-03-19 11:23:53.000000000', 'files': ['ansible/roles/dnf/templates/CentOS-Base.repo.j2', 'kayobe/tests/unit/cli/test_commands.py', 'etc/kayobe/yum.yml', 'releasenotes/notes/dnf-2071fc40b0d783b6.yaml', 'ansible/roles/dnf/defaults/main.yml', 'ansible/roles/dnf/templates/CentOS-AppStream.repo.j2', 'doc/source/configuration/hosts.rst', 'ansible/roles/dnf-automatic/defaults/main.yml', 'ansible/group_vars/all/dnf', 'ansible/roles/dnf/tasks/custom-repo.yml', 'ansible/roles/dnf/tasks/local-mirror.yml', 'ansible/dnf.yml', 'ansible/group_vars/all/yum-cron', 'ansible/roles/dnf/tasks/main.yml', 'ansible/roles/dnf/templates/epel-modular.repo.j2', 'ansible/yum.yml', 'etc/kayobe/yum-cron.yml', 'kayobe/cli/commands.py', 'ansible/roles/dnf-automatic/tasks/main.yml', 'ansible/roles/dnf/templates/CentOS-Extras.repo.j2', 'ansible/roles/dnf/templates/epel.repo.j2', 'ansible/group_vars/all/yum', 'etc/kayobe/dnf.yml'], 'web_link': 'https://opendev.org/openstack/kayobe/commit/dc32b52f08da551f23070d0915d78da0627aec3c', 'message': 'CentOS 8: Support DNF\n\nAdds support for configuration of DNF repo mirrors for CentOS and EPEL\nrepositories, as well as custom repositories.\n\nAdds support for DNF automatic, which is a replacement for yum-cron.\n\nConfiguration is backwards compatible, falling back to the equivalent\nyum variables when DNF variables have not been overridden.\n\nChange-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643\nStory: 2006574\nTask: 38922\n'}]",4,711044,dc32b52f08da551f23070d0915d78da0627aec3c,49,4,16,14826,,,0,"CentOS 8: Support DNF

Adds support for configuration of DNF repo mirrors for CentOS and EPEL
repositories, as well as custom repositories.

Adds support for DNF automatic, which is a replacement for yum-cron.

Configuration is backwards compatible, falling back to the equivalent
yum variables when DNF variables have not been overridden.

Change-Id: I8bef5e9c8e1c77c25d6077ff690da8f2cde6a643
Story: 2006574
Task: 38922
",git fetch https://review.opendev.org/openstack/kayobe refs/changes/44/711044/10 && git format-patch -1 --stdout FETCH_HEAD,"['ansible/roles/dnf/templates/CentOS-Base.repo.j2', 'etc/kayobe/yum.yml', 'releasenotes/notes/dnf-2071fc40b0d783b6.yaml', 'ansible/roles/dnf/defaults/main.yml', 'ansible/roles/dnf/templates/CentOS-AppStream.repo.j2', 'doc/source/configuration/hosts.rst', 'ansible/roles/dnf-automatic/defaults/main.yml', 'ansible/group_vars/all/dnf', 'ansible/roles/dnf/tasks/custom-repo.yml', 'ansible/roles/dnf/tasks/local-mirror.yml', 'playbooks/kayobe-overcloud-host-configure-base/tests/test_overcloud_host_configure.py', 'ansible/dnf.yml', 'ansible/roles/dnf/tasks/main.yml', 'ansible/roles/dnf/templates/epel-modular.repo.j2', 'ansible/yum.yml', 'playbooks/kayobe-overcloud-host-configure-base/overrides.yml.j2', 'etc/kayobe/yum-cron.yml', 'ansible/roles/dnf-automatic/tasks/main.yml', 'ansible/roles/dnf/templates/CentOS-Extras.repo.j2', 'ansible/roles/dnf/templates/epel.repo.j2', 'ansible/group_vars/all/yum', 'etc/kayobe/dnf.yml']",22,bd1025e41c111b1c2ea69eb9d8035394d85d8009,dnf,"--- # DNF configuration. ############################################################################### # DNF repository configuration. # For backwards compatibility, all variables in this section default to the # equivalently named variables starting with 'yum_' instead of 'dnf_'. # The yum variables will be removed in a future release. # Yum configuration. Dict mapping Yum config option names to their values. # dnf_config: # proxy: http://proxy.example.com #dnf_config: # Whether or not to use a local Yum mirror. Default value is 'false'. #dnf_use_local_mirror: # Mirror FQDN for Yum repos. Default value is 'mirror.centos.org'. #dnf_centos_mirror_host: # Mirror directory for Yum CentOS repos. Default value is 'centos'. #dnf_centos_mirror_directory: # Mirror FQDN for Yum EPEL repos. Default value is # 'download.fedoraproject.org'. #dnf_epel_mirror_host: # Mirror directory for Yum EPEL repos. Default value is 'pub/epel'. #dnf_epel_mirror_directory: # A dict of custom repositories. # You can see params on # http://docs.ansible.com/ansible/latest/modules/yum_repository_module.html. # For example: # dnf_custom_repos: # reponame: # baseurl: http://repo # file: myrepo # gpgkey: http://gpgkey # gpgcheck: yes #dnf_custom_repos: # Whether to install the epel-release package. This affects RedHat-based # systems only. Default value is 'true'. #dnf_install_epel: ############################################################################### # DNF Automatic configuration. # For backwards compatibility, all variables in this section default to the # equivalently named variables starting with 'yum_cron' instead of # 'dnf_automatic'. # The yum-cron variables will be removed in a future # release. # Whether DNF Automatic is enabled. This can be used to regularly apply # security updates. Default value is 'false'. #dnf_automatic_enabled: # DNF Automatic upgrade type. Default value is 'security'. Note that the # equivalent yum-cron variable is named slightly differently - # 'yum_cron_update_cmd'. #dnf_automatic_upgrade_type: ############################################################################### # Dummy variable to allow Ansible to accept this file. workaround_ansible_issue_8743: yes ",,602,10
openstack%2Fopenstack-helm~master~I818b54290abe723628ad6e7dfece3a83ecfc90a1,openstack/openstack-helm,master,I818b54290abe723628ad6e7dfece3a83ecfc90a1,Fix daemonset container names in apparmor annotations,ABANDONED,2019-11-07 16:33:47.000000000,2020-03-20 16:37:56.000000000,,"[{'_account_id': 17591}, {'_account_id': 18236}, {'_account_id': 20466}, {'_account_id': 21420}, {'_account_id': 22348}, {'_account_id': 23928}, {'_account_id': 24780}, {'_account_id': 26686}, {'_account_id': 28618}, {'_account_id': 28849}, {'_account_id': 28875}]","[{'number': 1, 'created': '2019-11-07 16:33:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/584796f69a546c60bcf912fbf0dbee0f47c0787f', 'message': 'Fix daemonset container names in apparmor annotations\n\nThere was a recent update (0) which changed the way the container names\nare generated for daemonsets. The ""-default"" will no longer be the default\nsuffix on the container names for these daemonsets. This patchset changes\nthe container names specified in the apparmor annotations and apparmor\nconfiguration to match what is generated for the affected daemonsets.\n\n(0) https://review.opendev.org/#/c/645958\n\nChange-Id: I818b54290abe723628ad6e7dfece3a83ecfc90a1\n'}, {'number': 2, 'created': '2019-12-04 12:45:44.000000000', 'files': ['tools/deployment/apparmor/compute-kit.sh', 'nova/templates/daemonset-compute.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/b0317f23a43e2693f2d725bd2af0682c853dec71', 'message': 'Fix daemonset container names in apparmor annotations\n\nThere was a recent update (0) which changed the way the container names\nare generated for daemonsets. The ""-default"" will no longer be the default\nsuffix on the container names for these daemonsets. This patchset changes\nthe container names specified in the apparmor annotations and apparmor\nconfiguration to match what is generated for the affected daemonsets.\n\n(0) https://review.opendev.org/#/c/645958\n\nChange-Id: I818b54290abe723628ad6e7dfece3a83ecfc90a1\n'}]",0,693385,b0317f23a43e2693f2d725bd2af0682c853dec71,14,11,2,22636,,,0,"Fix daemonset container names in apparmor annotations

There was a recent update (0) which changed the way the container names
are generated for daemonsets. The ""-default"" will no longer be the default
suffix on the container names for these daemonsets. This patchset changes
the container names specified in the apparmor annotations and apparmor
configuration to match what is generated for the affected daemonsets.

(0) https://review.opendev.org/#/c/645958

Change-Id: I818b54290abe723628ad6e7dfece3a83ecfc90a1
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/85/693385/1 && git format-patch -1 --stdout FETCH_HEAD,"['tools/deployment/apparmor/compute-kit.sh', 'nova/templates/daemonset-compute.yaml']",2,584796f69a546c60bcf912fbf0dbee0f47c0787f,,"{{ dict ""envAll"" $envAll ""podName"" ""nova-compute-default"" ""containerNames"" (list ""nova-compute"") | include ""helm-toolkit.snippets.kubernetes_mandatory_access_control_annotation"" | indent 8 }}","{{ dict ""envAll"" $envAll ""podName"" ""nova-compute-default"" ""containerNames"" (list ""nova-compute-default"") | include ""helm-toolkit.snippets.kubernetes_mandatory_access_control_annotation"" | indent 8 }}",8,8
openstack%2Fopenstack-helm~master~I18ec89da703d64c46401e09e861f4976b6d9d024,openstack/openstack-helm,master,I18ec89da703d64c46401e09e861f4976b6d9d024,Add apparmor annotation to Nova and Neutron,ABANDONED,2018-12-14 00:01:52.000000000,2020-03-20 16:33:34.000000000,,"[{'_account_id': 7769}, {'_account_id': 8898}, {'_account_id': 17591}, {'_account_id': 17887}, {'_account_id': 17966}, {'_account_id': 22348}, {'_account_id': 22477}, {'_account_id': 22636}, {'_account_id': 23928}, {'_account_id': 28701}]","[{'number': 1, 'created': '2018-12-14 00:01:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/c8364ced6e29dd54d557a8ba7eff0bb312d27b52', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\nThe daemonsets in particular specify that the pods run in privileged\nmode. The docker-default profile will not load on a privileged\ncontainer, so there is a custom profile specified for these pods,\nwhich is the same as the docker-default profile.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 2, 'created': '2018-12-14 00:03:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/9f57b7b05caaf721ba54a4a3dd51f7adbd9a4ff4', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\nThe daemonsets in particular specify that the pods run in privileged\nmode. The docker-default profile will not load on a privileged\ncontainer, so there is a custom profile specified for these pods,\nwhich is the same as the docker-default profile.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 3, 'created': '2018-12-14 00:14:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/207fd21162f9f2ee794f032c25b31f5e34ada5c7', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\nThe daemonsets in particular specify that the pods run in privileged\nmode. The docker-default profile will not load on a privileged\ncontainer, so there is a custom profile specified for these pods,\nwhich is the same as the docker-default profile.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 4, 'created': '2018-12-14 00:39:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/3400356144e76f5a19f0f592be88577a2218c463', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 5, 'created': '2018-12-14 00:41:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/72d97c424a08c50c54b76f3e42a0214860e8f26d', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 6, 'created': '2018-12-14 16:03:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/96f8deff10234fa87df07d1cd2b1c218ead46751', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 7, 'created': '2018-12-14 16:03:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/c4591b837f9acb2151488f71bf5de2c9a7916a97', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 8, 'created': '2018-12-14 16:05:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/9c525eb9ae71ad5a9aaa14cad91560ad3211d7aa', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 9, 'created': '2018-12-14 19:57:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/40959ee3d08b2c879d3563612ff88e1b1f433975', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 10, 'created': '2019-01-02 16:05:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/a7231845b2a392006afb8acda71f51e67ae69d23', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 11, 'created': '2019-01-11 19:23:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/d9cf862015c72a48e4e3033a360e94ae838ce8c0', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 12, 'created': '2019-01-11 19:23:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/ed80975cf162d9dcd416e46e753f1f65cfa1187d', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 13, 'created': '2019-02-13 19:12:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/9d9e3ad9ff900a818a827161c8cf7309d9abec44', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}, {'number': 14, 'created': '2019-02-13 20:28:46.000000000', 'files': ['neutron/templates/deployment-server.yaml', 'neutron/templates/daemonset-ovs-agent.yaml', 'nova/templates/deployment-conductor.yaml', 'nova/templates/deployment-api-osapi.yaml', 'neutron/templates/daemonset-dhcp-agent.yaml', 'neutron/templates/daemonset-sriov-agent.yaml', 'nova/templates/deployment-placement.yaml', 'nova/templates/deployment-novncproxy.yaml', 'neutron/templates/daemonset-lb-agent.yaml', 'nova/templates/deployment-spiceproxy.yaml', 'nova/values.yaml', 'nova/templates/deployment-consoleauth.yaml', 'neutron/templates/daemonset-l3-agent.yaml', 'neutron/values.yaml', 'nova/templates/daemonset-compute.yaml', 'neutron/templates/daemonset-metadata-agent.yaml', 'nova/templates/deployment-api-metadata.yaml', 'tools/deployment/common/confirm-mac.sh', 'nova/templates/deployment-scheduler.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/8f8335097fc368f9d862db4402fa2cf2352a138e', 'message': 'Add apparmor annotation to Nova and Neutron\n\nThis patch set adds the apparmor security annotations to the\ndeployments and daemonsets found in the Nova and Neutron charts.\n\nChange-Id: I18ec89da703d64c46401e09e861f4976b6d9d024\n'}]",0,625133,8f8335097fc368f9d862db4402fa2cf2352a138e,26,10,14,22636,,,0,"Add apparmor annotation to Nova and Neutron

This patch set adds the apparmor security annotations to the
deployments and daemonsets found in the Nova and Neutron charts.

Change-Id: I18ec89da703d64c46401e09e861f4976b6d9d024
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/33/625133/6 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/templates/deployment-server.yaml', 'nova/templates/configmap-apparmor.yaml', 'neutron/templates/daemonset-ovs-agent.yaml', 'nova/templates/deployment-conductor.yaml', 'nova/templates/deployment-api-osapi.yaml', 'neutron/templates/daemonset-dhcp-agent.yaml', 'neutron/templates/daemonset-sriov-agent.yaml', 'nova/templates/deployment-placement.yaml', 'nova/templates/deployment-novncproxy.yaml', 'neutron/templates/daemonset-lb-agent.yaml', 'nova/templates/deployment-spiceproxy.yaml', 'nova/values.yaml', 'nova/templates/deployment-consoleauth.yaml', 'neutron/templates/daemonset-l3-agent.yaml', 'neutron/values.yaml', 'nova/templates/daemonset-compute.yaml', 'neutron/templates/daemonset-metadata-agent.yaml', 'nova/templates/deployment-api-metadata.yaml', 'nova/templates/deployment-scheduler.yaml']",19,c8364ced6e29dd54d557a8ba7eff0bb312d27b52,,"{{- dict ""envAll"" $envAll ""podName"" ""nova-scheduler"" ""containerNames"" (list ""nova-scheduler"" ""init"") | include ""helm-toolkit.snippets.kubernetes_mandatory_access_control_annotation"" | indent 8 }}",,198,0
openstack%2Fpaunch~stable%2Fstein~I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da,openstack/paunch,stable/stein,I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da,Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all',MERGED,2020-03-19 21:27:33.000000000,2020-03-20 16:31:30.000000000,2020-03-20 16:29:20.000000000,"[{'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 21:27:33.000000000', 'files': ['paunch/builder/podman.py', 'paunch/tests/test_builder_compose1.py', 'paunch/utils/common.py', 'paunch/builder/compose1.py', 'paunch/tests/test_builder_base.py', 'paunch/tests/test_utils_common.py'], 'web_link': 'https://opendev.org/openstack/paunch/commit/01d9f1c7348963ea5778a4a89ddb6c93f6365a21', 'message': ""Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all'\n\nIn the case of nova_libvirt container, we want to use all CPUs that are\nreported online.\nRather than computing the list with Python (which has proven to be\nproblematic on PPC), let the container engine figuring it out by itself\nlike it was the case before.\n\nChange-Id: I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da\nCloses-Bug: #1868135\n(cherry picked from commit 9b6276512141693bddd064c915b08c1d027e5985)\n""}]",0,713987,01d9f1c7348963ea5778a4a89ddb6c93f6365a21,8,3,1,3153,,,0,"Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all'

In the case of nova_libvirt container, we want to use all CPUs that are
reported online.
Rather than computing the list with Python (which has proven to be
problematic on PPC), let the container engine figuring it out by itself
like it was the case before.

Change-Id: I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da
Closes-Bug: #1868135
(cherry picked from commit 9b6276512141693bddd064c915b08c1d027e5985)
",git fetch https://review.opendev.org/openstack/paunch refs/changes/87/713987/1 && git format-patch -1 --stdout FETCH_HEAD,"['paunch/builder/podman.py', 'paunch/tests/test_builder_compose1.py', 'paunch/utils/common.py', 'paunch/builder/compose1.py', 'paunch/tests/test_builder_base.py', 'paunch/tests/test_utils_common.py']",6,01d9f1c7348963ea5778a4a89ddb6c93f6365a21,stein/1868135,," @mock.patch(""psutil.cpu_count"", return_value=4) def test_get_all_cpus(self, mock_cpu): expected_list = '0-3' actual_list = common.get_all_cpus() self.assertEqual(actual_list, expected_list)",13,26
openstack%2Fpaunch~stable%2Ftrain~I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da,openstack/paunch,stable/train,I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da,Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all',MERGED,2020-03-19 21:22:03.000000000,2020-03-20 16:31:06.000000000,2020-03-20 16:29:19.000000000,"[{'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 21:22:03.000000000', 'files': ['paunch/builder/podman.py', 'paunch/tests/test_builder_compose1.py', 'paunch/utils/common.py', 'paunch/builder/compose1.py', 'paunch/tests/test_builder_base.py', 'paunch/tests/test_utils_common.py'], 'web_link': 'https://opendev.org/openstack/paunch/commit/ebc49c444f2fa3077d60dd79c221a7dcd1f443ec', 'message': ""Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all'\n\nIn the case of nova_libvirt container, we want to use all CPUs that are\nreported online.\nRather than computing the list with Python (which has proven to be\nproblematic on PPC), let the container engine figuring it out by itself\nlike it was the case before.\n\nChange-Id: I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da\nCloses-Bug: #1868135\n(cherry picked from commit 9b6276512141693bddd064c915b08c1d027e5985)\n""}]",0,713985,ebc49c444f2fa3077d60dd79c221a7dcd1f443ec,8,3,1,3153,,,0,"Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all'

In the case of nova_libvirt container, we want to use all CPUs that are
reported online.
Rather than computing the list with Python (which has proven to be
problematic on PPC), let the container engine figuring it out by itself
like it was the case before.

Change-Id: I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da
Closes-Bug: #1868135
(cherry picked from commit 9b6276512141693bddd064c915b08c1d027e5985)
",git fetch https://review.opendev.org/openstack/paunch refs/changes/85/713985/1 && git format-patch -1 --stdout FETCH_HEAD,"['paunch/builder/podman.py', 'paunch/tests/test_builder_compose1.py', 'paunch/utils/common.py', 'paunch/builder/compose1.py', 'paunch/tests/test_builder_base.py', 'paunch/tests/test_utils_common.py']",6,ebc49c444f2fa3077d60dd79c221a7dcd1f443ec,train/1868135,," @mock.patch(""psutil.cpu_count"", return_value=4) def test_get_all_cpus(self, mock_cpu): expected_list = '0-3' actual_list = common.get_all_cpus() self.assertEqual(actual_list, expected_list) ",13,26
openstack%2Frpm-packaging~stable%2Fstein~I680462f028fb8606079f0c6f20eb30625dbd0ba7,openstack/rpm-packaging,stable/stein,I680462f028fb8606079f0c6f20eb30625dbd0ba7,Update keystonemiddleware to 6.0.1,MERGED,2020-03-19 20:38:09.000000000,2020-03-20 16:29:54.000000000,2020-03-20 16:29:54.000000000,"[{'_account_id': 2062}, {'_account_id': 6593}, {'_account_id': 6679}, {'_account_id': 7102}, {'_account_id': 13294}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 20:38:09.000000000', 'files': ['openstack/keystonemiddleware/keystonemiddleware.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/aa39b78193c5187ae1587c31e488b163afdf6552', 'message': 'Update keystonemiddleware to 6.0.1\n\nDepends-on: https://review.opendev.org/713954\nChange-Id: I680462f028fb8606079f0c6f20eb30625dbd0ba7\n'}]",0,713977,aa39b78193c5187ae1587c31e488b163afdf6552,18,8,1,8482,,,0,"Update keystonemiddleware to 6.0.1

Depends-on: https://review.opendev.org/713954
Change-Id: I680462f028fb8606079f0c6f20eb30625dbd0ba7
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/77/713977/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/keystonemiddleware/keystonemiddleware.spec.j2'],1,aa39b78193c5187ae1587c31e488b163afdf6552,keystonemiddleware-6.0.1,Version: 6.0.1,Version: 6.0.0,1,1
openstack%2Fcharms.ceph~master~I66359963b5e8e792d23b7989f07b7ac67afcd9b0,openstack/charms.ceph,master,I66359963b5e8e792d23b7989f07b7ac67afcd9b0,Maintain OSD state on upgrade,MERGED,2020-03-18 21:31:45.000000000,2020-03-20 16:24:37.000000000,2020-03-20 16:24:37.000000000,"[{'_account_id': 20634}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-18 21:31:45.000000000', 'files': ['charms_ceph/utils.py', 'unit_tests/test_osd_upgrade_roll.py'], 'web_link': 'https://opendev.org/openstack/charms.ceph/commit/26114b2a544b4b2e3bb2b43839fd18e3d395d95a', 'message': ""Maintain OSD state on upgrade\n\nEnsure each OSD reaches its pre-restart state before proceeding\nafter restart. This prevents the charm from finalizing the upgrade\nprior to OSDs recovering after upgrade. For example, if the state\nis 'active' prior to restart, then it must reach 'active' after\nrestart, at which point the upgrade will be allowed to complete.\n\nChange-Id: I66359963b5e8e792d23b7989f07b7ac67afcd9b0\nCloses-Bug: #1821028\n""}]",0,713743,26114b2a544b4b2e3bb2b43839fd18e3d395d95a,6,2,1,11805,,,0,"Maintain OSD state on upgrade

Ensure each OSD reaches its pre-restart state before proceeding
after restart. This prevents the charm from finalizing the upgrade
prior to OSDs recovering after upgrade. For example, if the state
is 'active' prior to restart, then it must reach 'active' after
restart, at which point the upgrade will be allowed to complete.

Change-Id: I66359963b5e8e792d23b7989f07b7ac67afcd9b0
Closes-Bug: #1821028
",git fetch https://review.opendev.org/openstack/charms.ceph refs/changes/43/713743/1 && git format-patch -1 --stdout FETCH_HEAD,"['charms_ceph/utils.py', 'unit_tests/test_osd_upgrade_roll.py']",2,26114b2a544b4b2e3bb2b43839fd18e3d395d95a,bug/1821028,"import subprocess @patch.object(charms_ceph.utils, 'get_osd_state') _determine_packages, get_osd_state): get_osd_state.side_effect = ['active'] * 6 get_osd_state.assert_has_calls([ call(0), call(1), call(2), call(0, osd_goal_state='active'), call(1, osd_goal_state='active'), call(2, osd_goal_state='active'), ]) @patch.object(charms_ceph.utils, 'get_osd_state') _determine_packages, get_osd_state): get_osd_state.side_effect = ['active'] * 6 get_osd_state.assert_has_calls([ call(0), call(1), call(2), call(0, osd_goal_state='active'), call(1, osd_goal_state='active'), call(2, osd_goal_state='active'), ]) @patch.object(charms_ceph.utils, 'get_osd_state') disable_osd, stop_osd, get_osd_state): get_osd_state.side_effect = ['active'] * 2 get_osd_state.assert_has_calls([ call(1), call(1, osd_goal_state='active'), ]) @patch('subprocess.check_output') @patch.object(charms_ceph.utils, 'log') def test_get_osd_state(self, log, check_output): check_output.side_effect = [ subprocess.CalledProcessError(returncode=2, cmd=[""bad""]), ValueError(""bad value""), '{""state"":""active""}'.encode()] * 2 osd_state = charms_ceph.utils.get_osd_state(2) check_output.assert_called_with( ['ceph', 'daemon', '/var/run/ceph/ceph-osd.2.asok', 'status']) log.assert_has_calls([ call(""Command '['bad']' returned non-zero exit status 2."", level='DEBUG'), call('bad value', level='DEBUG'), call('OSD 2 state: active, goal state: None', level='DEBUG')]) self.assertEqual(osd_state, 'active') osd_state = charms_ceph.utils.get_osd_state(2, osd_goal_state='active') check_output.assert_called_with( ['ceph', 'daemon', '/var/run/ceph/ceph-osd.2.asok', 'status']) log.assert_has_calls([ call(""Command '['bad']' returned non-zero exit status 2."", level='DEBUG'), call('bad value', level='DEBUG'), call('OSD 2 state: active, goal state: None', level='DEBUG')]) self.assertEqual(osd_state, 'active') "," _determine_packages): _determine_packages): disable_osd, stop_osd):",158,12
openstack%2Fopenstack-helm-infra~master~Ie106490b2fb8d61660663f39a992bf4dc1a61222,openstack/openstack-helm-infra,master,Ie106490b2fb8d61660663f39a992bf4dc1a61222,Adjust RabbitMQ Exporter Probes,MERGED,2020-03-18 16:58:49.000000000,2020-03-20 16:23:16.000000000,2020-03-20 16:20:07.000000000,"[{'_account_id': 8898}, {'_account_id': 21420}, {'_account_id': 22348}, {'_account_id': 22636}, {'_account_id': 24780}]","[{'number': 1, 'created': '2020-03-18 16:58:49.000000000', 'files': ['rabbitmq/templates/monitoring/prometheus/exporter-deployment.yaml', 'rabbitmq/values.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/72afe093aa0b45ecf5f7600bdb0b768967afda0d', 'message': 'Adjust RabbitMQ Exporter Probes\n\nThe currently defined RabbitMQ Exporter probes make a call to the\n""/metrics"" path of the exporter service, which downloads a huge file and\ntakes a very long time to download. An http probe should be based on a very\nsimple and short url response from the service. So this changes the\nprobes to just call the base path ""/"" of the url and set the timeout to\nsomething reasonable like 5 seconds.\n\nChange-Id: Ie106490b2fb8d61660663f39a992bf4dc1a61222\n'}]",2,713696,72afe093aa0b45ecf5f7600bdb0b768967afda0d,10,5,1,22636,,,0,"Adjust RabbitMQ Exporter Probes

The currently defined RabbitMQ Exporter probes make a call to the
""/metrics"" path of the exporter service, which downloads a huge file and
takes a very long time to download. An http probe should be based on a very
simple and short url response from the service. So this changes the
probes to just call the base path ""/"" of the url and set the timeout to
something reasonable like 5 seconds.

Change-Id: Ie106490b2fb8d61660663f39a992bf4dc1a61222
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/96/713696/1 && git format-patch -1 --stdout FETCH_HEAD,"['rabbitmq/templates/monitoring/prometheus/exporter-deployment.yaml', 'rabbitmq/values.yaml']",2,72afe093aa0b45ecf5f7600bdb0b768967afda0d,, timeoutSeconds: 5 timeoutSeconds: 5, timeoutSeconds: 20 timeoutSeconds: 70,3,3
openstack%2Fpython-glanceclient~master~Ie57d7de5822264a5ea8a5f4587ab8cfb4afb79de,openstack/python-glanceclient,master,Ie57d7de5822264a5ea8a5f4587ab8cfb4afb79de,Delete image from specific store,MERGED,2019-12-18 09:59:09.000000000,2020-03-20 16:18:34.000000000,2020-03-20 16:14:48.000000000,"[{'_account_id': 5202}, {'_account_id': 5314}, {'_account_id': 9303}, {'_account_id': 11904}, {'_account_id': 22348}]","[{'number': 1, 'created': '2019-12-18 09:59:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-glanceclient/commit/2b8a16d1931c0c5a928ee48fa59859a0189804cd', 'message': 'Delete image from specific store\n\nWIP: Missing tests\n\nAdd support to delete image from specific store.\n\nbp: delete-from-store\nDepends-On: https://review.opendev.org/#/c/698049\nChange-Id: Ie57d7de5822264a5ea8a5f4587ab8cfb4afb79de\n'}, {'number': 2, 'created': '2019-12-18 10:52:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-glanceclient/commit/40687027a3ba3c4676eb883902171fac1a9fe877', 'message': 'Delete image from specific store\n\nWIP: Missing tests\n\nAdd support to delete image from specific store.\n\nbp: delete-from-store\nDepends-On: https://review.opendev.org/#/c/698049\nChange-Id: Ie57d7de5822264a5ea8a5f4587ab8cfb4afb79de\n'}, {'number': 3, 'created': '2019-12-18 14:15:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-glanceclient/commit/71847ca1ef205ac53c2a427526584deb719a1923', 'message': 'Delete image from specific store\n\nAdd support to delete image from specific store.\n\nbp: delete-from-store\nDepends-On: https://review.opendev.org/#/c/698049\nChange-Id: Ie57d7de5822264a5ea8a5f4587ab8cfb4afb79de\n'}, {'number': 4, 'created': '2019-12-18 15:20:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-glanceclient/commit/88904051f5650d08cdc745a1ecace9591805dd60', 'message': 'Delete image from specific store\n\nAdd support to delete image from specific store.\n\nbp: delete-from-store\nDepends-On: https://review.opendev.org/#/c/698049\nChange-Id: Ie57d7de5822264a5ea8a5f4587ab8cfb4afb79de\n'}, {'number': 5, 'created': '2020-03-16 04:28:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-glanceclient/commit/763e7409243c12da3c38857e42f3b285a551c900', 'message': 'Delete image from specific store\n\nAdd support to delete image from specific store.\n\nbp: delete-from-store\nDepends-On: https://review.opendev.org/#/c/698049\nChange-Id: Ie57d7de5822264a5ea8a5f4587ab8cfb4afb79de\n'}, {'number': 6, 'created': '2020-03-20 07:22:12.000000000', 'files': ['glanceclient/tests/unit/v2/test_shell_v2.py', 'glanceclient/v2/images.py', 'glanceclient/v2/shell.py', 'releasenotes/notes/del_from_store-2d807c3038283907.yaml'], 'web_link': 'https://opendev.org/openstack/python-glanceclient/commit/d91bcae8a509235ec17a60233e2c3e252b9a317d', 'message': 'Delete image from specific store\n\nAdd support to delete image from specific store.\n\nbp: delete-from-store\nChange-Id: Ie57d7de5822264a5ea8a5f4587ab8cfb4afb79de\n'}]",6,699644,d91bcae8a509235ec17a60233e2c3e252b9a317d,24,5,6,5202,,,0,"Delete image from specific store

Add support to delete image from specific store.

bp: delete-from-store
Change-Id: Ie57d7de5822264a5ea8a5f4587ab8cfb4afb79de
",git fetch https://review.opendev.org/openstack/python-glanceclient refs/changes/44/699644/4 && git format-patch -1 --stdout FETCH_HEAD,"['glanceclient/v2/images.py', 'glanceclient/v2/shell.py']",2,2b8a16d1931c0c5a928ee48fa59859a0189804cd,del_from_store,"@utils.arg('id', metavar='<IMAGE_ID>', help=_('ID of image to update.')) @utils.arg('--store', metavar='<STORE_ID>', required=True, help=_('Store to delete image from.')) def do_stores_delete(gc, args): """"""Delete image from specific store."""""" try: stores_info = gc.images.del_from_store(args.store, args.id) except exc.HTTPNotFound: utils.exit('Multi Backend support is not enabled or Image/store not ' 'found.') except (exc.HTTPForbidden, exc.HTTPException) as e: msg = ""Unable to delete image '%s' from store '%s'. (%s)"" % ( args.id, args.store, e) utils.exit(msg) ",,26,0
openstack%2Fopenstack-helm-images~master~I1acdb3fd69e2172de9ecf6bfcf2d64bc9b1ad266,openstack/openstack-helm-images,master,I1acdb3fd69e2172de9ecf6bfcf2d64bc9b1ad266,Update package to python3,ABANDONED,2020-01-13 19:13:44.000000000,2020-03-20 16:16:37.000000000,,"[{'_account_id': 16881}, {'_account_id': 18256}, {'_account_id': 20466}, {'_account_id': 21420}, {'_account_id': 22348}, {'_account_id': 30777}]","[{'number': 1, 'created': '2020-01-13 19:13:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/f665501c508551d5826359c3041ddbcaf390828c', 'message': 'Update package to python3\n\nThis patch set updates the python packages to be python 3 compatible\nas python 2 is deprecated.\n\nChange-Id: I1acdb3fd69e2172de9ecf6bfcf2d64bc9b1ad266\nSigned-off-by: Tin Lam <tin@irrational.io>\n'}, {'number': 2, 'created': '2020-01-13 19:32:56.000000000', 'files': ['openvswitch/Dockerfile.ubuntu_bionic-dpdk', 'openvswitch/Dockerfile.debian'], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/2e584cdedc49a9bc854c5decda495f0f3ed1ff71', 'message': 'Update package to python3\n\nThis patch set updates the python packages to be python 3 compatible\nas python 2 is deprecated.\n\nChange-Id: I1acdb3fd69e2172de9ecf6bfcf2d64bc9b1ad266\nSigned-off-by: Tin Lam <tin@irrational.io>\n'}]",4,702282,2e584cdedc49a9bc854c5decda495f0f3ed1ff71,10,6,2,20466,,,0,"Update package to python3

This patch set updates the python packages to be python 3 compatible
as python 2 is deprecated.

Change-Id: I1acdb3fd69e2172de9ecf6bfcf2d64bc9b1ad266
Signed-off-by: Tin Lam <tin@irrational.io>
",git fetch https://review.opendev.org/openstack/openstack-helm-images refs/changes/82/702282/2 && git format-patch -1 --stdout FETCH_HEAD,"['openvswitch/Dockerfile.ubuntu_bionic-dpdk', 'openvswitch/Dockerfile.debian']",2,f665501c508551d5826359c3041ddbcaf390828c,py2-3, python3-all \ python3-six \ python3-all \ python3-six \ python-zopeinterface ;\, python-all \ python-six \ python-all \ python-six \ python-zopeinterface ;\,10,10
openstack%2Fvalidations-common~master~I3db72ef186f650ff6485b805ed794f0defbcafa6,openstack/validations-common,master,I3db72ef186f650ff6485b805ed794f0defbcafa6,Use Zuul template for lower-constraints,MERGED,2020-03-20 15:03:13.000000000,2020-03-20 16:15:32.000000000,2020-03-20 16:15:32.000000000,"[{'_account_id': 11491}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 15:03:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-common/commit/45d48967ad61dc93f1e43d76f70074beabdf0c27', 'message': ""Use Zuul template for lower-constraints\n\nIt's better to use templates so that we can change configuration\nglobally. Use the lower-constraints template.\n\nAlso, sort list of templates and indent items by two spaces as we do\nnormally.\n\nChange-Id: I3db72ef186f650ff6485b805ed794f0defbcafa6\n""}, {'number': 2, 'created': '2020-03-20 15:13:23.000000000', 'files': ['zuul.d/layout.yaml'], 'web_link': 'https://opendev.org/openstack/validations-common/commit/99bbfbebe1d76a4141449e1862972d619e4a84e9', 'message': ""Use Zuul template for lower-constraints\n\nIt's better to use templates so that we can change configuration\nglobally. Use the lower-constraints template.\n\nAlso, sort list of templates and indent items by two spaces as we do\nnormally.\n\nChange-Id: I3db72ef186f650ff6485b805ed794f0defbcafa6\n""}]",1,714136,99bbfbebe1d76a4141449e1862972d619e4a84e9,8,2,2,6547,,,0,"Use Zuul template for lower-constraints

It's better to use templates so that we can change configuration
globally. Use the lower-constraints template.

Also, sort list of templates and indent items by two spaces as we do
normally.

Change-Id: I3db72ef186f650ff6485b805ed794f0defbcafa6
",git fetch https://review.opendev.org/openstack/validations-common refs/changes/36/714136/2 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/layout.yaml'],1,45d48967ad61dc93f1e43d76f70074beabdf0c27,templates, - check-requirements - openstack-python3-ussuri-jobs - openstack-tox-lower-constraints - release-notes-jobs-python3 - validations-common-molecule-jobs - openstack-tox-linters - openstack-tox-linters, - openstack-python3-ussuri-jobs - validations-common-molecule-jobs - check-requirements - release-notes-jobs-python3 - openstack-tox-linters - openstack-tox-lower-constraints - openstack-tox-linters - openstack-tox-lower-constraints,7,8
openstack%2Fpaunch~stable%2Fqueens~I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da,openstack/paunch,stable/queens,I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da,Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all',MERGED,2020-03-19 21:40:20.000000000,2020-03-20 16:04:58.000000000,2020-03-20 16:04:57.000000000,"[{'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 21:40:20.000000000', 'files': ['paunch/tests/test_builder_compose1.py', 'paunch/utils/common.py', 'paunch/builder/compose1.py', 'paunch/tests/test_utils_common.py'], 'web_link': 'https://opendev.org/openstack/paunch/commit/341e5f6337f8260c9ee2304a8b6562f8aff98cd9', 'message': ""Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all'\n\nIn the case of nova_libvirt container, we want to use all CPUs that are\nreported online.\nRather than computing the list with Python (which has proven to be\nproblematic on PPC), let the container engine figuring it out by itself\nlike it was the case before.\n\nChange-Id: I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da\nCloses-Bug: #1868135\n(cherry picked from commit 9b6276512141693bddd064c915b08c1d027e5985)\n""}]",0,713991,341e5f6337f8260c9ee2304a8b6562f8aff98cd9,7,3,1,3153,,,0,"Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all'

In the case of nova_libvirt container, we want to use all CPUs that are
reported online.
Rather than computing the list with Python (which has proven to be
problematic on PPC), let the container engine figuring it out by itself
like it was the case before.

Change-Id: I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da
Closes-Bug: #1868135
(cherry picked from commit 9b6276512141693bddd064c915b08c1d027e5985)
",git fetch https://review.opendev.org/openstack/paunch refs/changes/91/713991/1 && git format-patch -1 --stdout FETCH_HEAD,"['paunch/tests/test_builder_compose1.py', 'paunch/utils/common.py', 'paunch/builder/compose1.py', 'paunch/tests/test_utils_common.py']",4,341e5f6337f8260c9ee2304a8b6562f8aff98cd9,queens/1868135,," @mock.patch(""psutil.cpu_count"", return_value=4) def test_get_all_cpus(self, mock_cpu): expected_list = '0-3' actual_list = common.get_all_cpus() self.assertEqual(actual_list, expected_list)",8,20
openstack%2Frpm-packaging~stable%2Frocky~I7a3a11e8c3c499c7a22ebd4aea53fec7b07d1920,openstack/rpm-packaging,stable/rocky,I7a3a11e8c3c499c7a22ebd4aea53fec7b07d1920,Make unittests pass for keystoneclient,MERGED,2020-03-19 11:58:43.000000000,2020-03-20 16:04:58.000000000,2020-03-20 16:04:58.000000000,"[{'_account_id': 6593}, {'_account_id': 7102}, {'_account_id': 8482}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 11:58:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/e1ea6f1c5eed71439441de29d76bbad3e4004aea', 'message': 'Make unittests pass for keystoneclient\n\nUnfortunately this was not released as a new keystoneclient release\nbefore the end of the maintenance, so we need to add the patch\nthat was merged in git\n\nChange-Id: I7a3a11e8c3c499c7a22ebd4aea53fec7b07d1920\n'}, {'number': 2, 'created': '2020-03-19 16:35:03.000000000', 'files': ['openstack/python-keystoneclient/python-keystoneclient.spec.j2', 'openstack/python-keystoneclient/0001-Make-tests-pass-in-2020.patch'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/d01a3d46a9322c31b3853e62e859f39ed00190c4', 'message': 'Make unittests pass for keystoneclient\n\nUnfortunately this was not released as a new keystoneclient release\nbefore the end of the maintenance, so we need to add the patch\nthat was merged in git\n\nChange-Id: I7a3a11e8c3c499c7a22ebd4aea53fec7b07d1920\n'}]",0,713857,d01a3d46a9322c31b3853e62e859f39ed00190c4,14,6,2,6593,,,0,"Make unittests pass for keystoneclient

Unfortunately this was not released as a new keystoneclient release
before the end of the maintenance, so we need to add the patch
that was merged in git

Change-Id: I7a3a11e8c3c499c7a22ebd4aea53fec7b07d1920
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/57/713857/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/python-keystoneclient/python-keystoneclient.spec.j2'],1,e1ea6f1c5eed71439441de29d76bbad3e4004aea,,Patch1: 0001-Make-tests-pass-in-2020.patch,,1,0
openstack%2Ftripleo-heat-templates~master~Ib2fe159d6e2503db834ae68d9a60c627a15b7b5e,openstack/tripleo-heat-templates,master,Ib2fe159d6e2503db834ae68d9a60c627a15b7b5e,"Use internal interface for keystone in ""wait for placement"" script",MERGED,2019-01-22 16:48:49.000000000,2020-03-20 16:03:43.000000000,2019-01-22 23:08:46.000000000,"[{'_account_id': 3153}, {'_account_id': 14250}, {'_account_id': 14985}, {'_account_id': 17216}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2019-01-22 16:48:49.000000000', 'files': ['docker_config_scripts/nova_wait_for_placement_service.py'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/55188215d9745fd8da5ae360b9ff26fb1d34253c', 'message': 'Use internal interface for keystone in ""wait for placement"" script\n\nInstead of trying to go to the ""admin"" endpoint, which is\nkeystoneclient\'s default. We instead try to use the internal endpoint.\n\nThis reduces our usage of the keystone admin endpoint.\n\nChange-Id: Ib2fe159d6e2503db834ae68d9a60c627a15b7b5e\n'}]",0,632511,55188215d9745fd8da5ae360b9ff26fb1d34253c,10,6,1,10873,,,0,"Use internal interface for keystone in ""wait for placement"" script

Instead of trying to go to the ""admin"" endpoint, which is
keystoneclient's default. We instead try to use the internal endpoint.

This reduces our usage of the keystone admin endpoint.

Change-Id: Ib2fe159d6e2503db834ae68d9a60c627a15b7b5e
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/11/632511/1 && git format-patch -1 --stdout FETCH_HEAD,['docker_config_scripts/nova_wait_for_placement_service.py'],1,55188215d9745fd8da5ae360b9ff26fb1d34253c,," keystone = client.Client(session=sess, interface='internal')", keystone = client.Client(session=sess),1,1
openstack%2Fpython-openstackclient~master~Ica375357332605b97b45d211f48c26360f28dff1,openstack/python-openstackclient,master,Ica375357332605b97b45d211f48c26360f28dff1,"added fix for pep8, releasenote, test case for --force flag",ABANDONED,2020-03-20 15:51:02.000000000,2020-03-20 15:58:26.000000000,,[],"[{'number': 1, 'created': '2020-03-20 15:51:02.000000000', 'files': ['openstackclient/tests/unit/common/test_quota.py', 'releasenotes/notes/force-flag-openstackclient-c172de2717e5cfac.yaml', 'openstackclient/common/quota.py'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/53b5b7138939d65d43f6544786a7e43769b951db', 'message': 'added fix for pep8, releasenote, test case for --force flag\n\nChange-Id: Ica375357332605b97b45d211f48c26360f28dff1\n'}]",8,714153,53b5b7138939d65d43f6544786a7e43769b951db,3,0,1,31733,,,0,"added fix for pep8, releasenote, test case for --force flag

Change-Id: Ica375357332605b97b45d211f48c26360f28dff1
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/53/714153/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstackclient/tests/unit/common/test_quota.py', 'releasenotes/notes/force-flag-openstackclient-c172de2717e5cfac.yaml', 'openstackclient/common/quota.py']",3,53b5b7138939d65d43f6544786a7e43769b951db,bug/2007440," parser.add_argument( '--force', action='store_true', help=_('Force quota update for compute'), ) if parsed_args.force: compute_kwargs['force'] = True if parsed_args.force: if volume_kwargs: sys.stderr.write(""--foce is only supported by compute. \n"") if network_kwargs: sys.stderr.write(""--force is only supported by compute. \n"") ",,51,0
openstack%2Fneutron-tempest-plugin~master~I519e2685dbc9d0ae0011b7b9f8a04c705f31ef3b,openstack/neutron-tempest-plugin,master,I519e2685dbc9d0ae0011b7b9f8a04c705f31ef3b,Test connectivity after migration of the server with trunk,MERGED,2020-03-05 09:32:09.000000000,2020-03-20 15:53:28.000000000,2020-03-20 15:53:28.000000000,"[{'_account_id': 11975}, {'_account_id': 13995}, {'_account_id': 16688}, {'_account_id': 18894}, {'_account_id': 19118}, {'_account_id': 22348}, {'_account_id': 28609}, {'_account_id': 29350}, {'_account_id': 31450}]","[{'number': 1, 'created': '2020-03-05 09:32:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/fa5e74bef66e42e6b98f7a04b03bf12594696fe9', 'message': 'Test connectivity after migration of the server with trunk\n\nChange-Id: I519e2685dbc9d0ae0011b7b9f8a04c705f31ef3b\n'}, {'number': 2, 'created': '2020-03-17 11:07:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/68053174d1a3df9582e9307cd0272ae34bb3cfe0', 'message': 'Test connectivity after migration of the server with trunk\n\nVerify that server with VLAN aware port can be correctly\nmigrated to another compute node. The original issue has\nbeen mentioned in the following bugzilla\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1694624\n\nChange-Id: I519e2685dbc9d0ae0011b7b9f8a04c705f31ef3b\n'}, {'number': 3, 'created': '2020-03-17 14:12:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/744c6042eed71ed5c70573f20cba03ad83b314cc', 'message': 'Test connectivity after migration of the server with trunk\n\nVerify that server with VLAN aware port can be correctly\nmigrated to another compute node. The original issue has\nbeen mentioned in the following bugzilla\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1694624\n\nChange-Id: I519e2685dbc9d0ae0011b7b9f8a04c705f31ef3b\n'}, {'number': 4, 'created': '2020-03-19 12:29:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/dc72c720a90a4332870dfc9b69b7f494ce97e728', 'message': 'Test connectivity after migration of the server with trunk\n\nVerify that server with VLAN aware port can be correctly\nmigrated to another compute node. The original issue has\nbeen mentioned in the following bugzilla\n\n+ Fixed the issue when security group rule that allows icmp\ntraffic is not removed after the test\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1694624\n\nChange-Id: I519e2685dbc9d0ae0011b7b9f8a04c705f31ef3b\n'}, {'number': 5, 'created': '2020-03-19 16:40:34.000000000', 'files': ['neutron_tempest_plugin/scenario/test_trunk.py'], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/baf14a5d92ed87656444d2d55d1f7622e9640805', 'message': 'Test connectivity after migration of the server with trunk\n\nVerify that server with VLAN aware port can be correctly\nmigrated to another compute node. The original issue has\nbeen mentioned in the following bugzilla\n\n+ Fixed the issue when security group rule that allows icmp\ntraffic is not removed after the test\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1694624\n\nChange-Id: I519e2685dbc9d0ae0011b7b9f8a04c705f31ef3b\n'}]",9,711409,baf14a5d92ed87656444d2d55d1f7622e9640805,27,9,5,31450,,,0,"Test connectivity after migration of the server with trunk

Verify that server with VLAN aware port can be correctly
migrated to another compute node. The original issue has
been mentioned in the following bugzilla

+ Fixed the issue when security group rule that allows icmp
traffic is not removed after the test

https://bugzilla.redhat.com/show_bug.cgi?id=1694624

Change-Id: I519e2685dbc9d0ae0011b7b9f8a04c705f31ef3b
",git fetch https://review.opendev.org/openstack/neutron-tempest-plugin refs/changes/09/711409/3 && git format-patch -1 --stdout FETCH_HEAD,['neutron_tempest_plugin/scenario/test_trunk.py'],1,fa5e74bef66e42e6b98f7a04b03bf12594696fe9,vlan_aware_vm_migration,"from tempest.common import waiters credentials = ['primary', 'admin'] @testtools.skipUnless(CONF.compute_feature_enabled.cold_migration, 'Cold migration is not available.') @testtools.skipUnless(CONF.compute.min_compute_nodes > 1, 'Less than 2 compute nodes, skipping multinode ' 'tests.') @testtools.skipUnless( (CONF.neutron_plugin_options.advanced_image_ref or CONF.neutron_plugin_options.default_image_is_advanced), ""Advanced image is required to run this test."") @decorators.attr(type='slow') @decorators.idempotent_id('ecd7de30-1c90-4280-b97c-1bed776d5d07') def test_trunk_vm_migration(self): '''Test connectivity after migration of the server with trunk A successfully migrated server shows a VERIFY_RESIZE status that requires confirmation. Need to reconfigure VLAN interface on server side after migration is finished as the configuration doesn't survive the reboot. ''' vlan_tag = 10 vlan_network = self.create_network() vlan_subnet = self.create_subnet(vlan_network) self.create_pingable_secgroup_rule(self.security_group['id']) use_advanced_image = ( not CONF.neutron_plugin_options.default_image_is_advanced) servers = {} for role in ['migrate', 'connection_test']: servers[role] = self._create_server_with_trunk_port( subport_network=vlan_network, segmentation_id=vlan_tag, use_advanced_image=use_advanced_image) self._configure_vlan_subport(vm=servers[role], vlan_tag=vlan_tag, vlan_subnet=vlan_subnet) client = self.os_admin.compute.ServersClient() client.migrate_server(servers['migrate'].server['id']) waiters.wait_for_server_status(client, servers['migrate'].server['id'], 'VERIFY_RESIZE') client.confirm_resize_server(servers['migrate'].server['id']) self._configure_vlan_subport(vm=servers['migrate'], vlan_tag=vlan_tag, vlan_subnet=vlan_subnet) self.check_remote_connectivity( servers['connection_test'].ssh_client, servers['migrate'].subport['fixed_ips'][0]['ip_address']) ", credentials = ['primary'],52,1
openstack%2Fvalidations-libs~master~I64197cd887e9af5e0c9a811bb7e75b1e87e96ded,openstack/validations-libs,master,I64197cd887e9af5e0c9a811bb7e75b1e87e96ded,Use Zuul template for lower-constraints,MERGED,2020-03-20 15:03:21.000000000,2020-03-20 15:38:15.000000000,2020-03-20 15:38:15.000000000,"[{'_account_id': 11491}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 15:03:21.000000000', 'files': ['zuul.d/layout.yaml'], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/142dbe9aa5838475437bc82ba7113ab47efa3d24', 'message': ""Use Zuul template for lower-constraints\n\nIt's better to use templates so that we can change configuration\nglobally. Use the lower-constraints template.\n\nAlso, sort list of templates and indent items by two spaces as we do\nnormally.\n\nChange-Id: I64197cd887e9af5e0c9a811bb7e75b1e87e96ded\n""}]",0,714137,142dbe9aa5838475437bc82ba7113ab47efa3d24,6,2,1,6547,,,0,"Use Zuul template for lower-constraints

It's better to use templates so that we can change configuration
globally. Use the lower-constraints template.

Also, sort list of templates and indent items by two spaces as we do
normally.

Change-Id: I64197cd887e9af5e0c9a811bb7e75b1e87e96ded
",git fetch https://review.opendev.org/openstack/validations-libs refs/changes/37/714137/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/layout.yaml'],1,142dbe9aa5838475437bc82ba7113ab47efa3d24,templates, - openstack-python3-ussuri-jobs - check-requirements - openstack-lower-constraints-jobs - openstack-tox-linters - openstack-tox-linters, - openstack-python3-ussuri-jobs - check-requirements - openstack-tox-linters - openstack-tox-lower-constraints - openstack-tox-linters - openstack-tox-lower-constraints,5,6
openstack%2Ftap-as-a-service~master~I701c89ecc890a1d866815b5401cd4c022c51c9c4,openstack/tap-as-a-service,master,I701c89ecc890a1d866815b5401cd4c022c51c9c4,Switch to hacking 2.0,ABANDONED,2020-03-20 14:48:57.000000000,2020-03-20 15:37:45.000000000,,[],"[{'number': 1, 'created': '2020-03-20 14:48:57.000000000', 'files': ['neutron_taas/db/migration/alembic_migration/versions/newton/contract/2ecce0368a62_add_foreign_key_constraint_on_tap_id_association.py', 'neutron_taas/services/taas/drivers/linux/sriov_nic_utils.py', 'test-requirements.txt', 'neutron_taas/db/migration/alembic_migration/versions/newton/expand/04625466c6fa_initial_newton_no_op_expand_script.py', 'neutron_taas/db/migration/alembic_migration/versions/newton/contract/80c85b675b6e_initial_newton_no_op_contract_script.py', 'lower-constraints.txt', 'neutron_taas/db/taas_db.py', 'neutron_taas/services/taas/drivers/linux/ovs_taas.py', 'neutron_taas/tests/unit/db/test_migrations.py', 'neutron_taas/db/migration/alembic_migration/env.py', 'neutron_taas/db/migration/alembic_migration/versions/newton/contract/1817af933379_remove_network_id_from_tap_service.py', 'neutron_taas/db/migration/alembic_migration/versions/stein/expand/ccbcc559d175_add_vlan_filter_to_tap_flow.py', 'neutron_taas/tests/unit/services/taas/test_taas_plugin.py', 'neutron_taas/db/migration/alembic_migration/versions/start_neutron_taas.py', 'neutron_taas/db/migration/alembic_migration/versions/newton/expand/fddbdec8711a_add_status.py', 'neutron_taas/db/migration/alembic_migration/versions/newton/contract/4086b3cffc01_rename_tenant_to_project.py', 'tox.ini', 'neutron_taas/db/migration/alembic_migration/versions/pike/contract/bac61f603e39_alter_tap_id_associations_to_support_tap_id_reuse.py', 'releasenotes/source/conf.py'], 'web_link': 'https://opendev.org/openstack/tap-as-a-service/commit/d78d241f4072760ebdb279f683c773bcf84339b5', 'message': 'Switch to hacking 2.0\n\nMake the code compatible with hacking rules:\n* import order problems\n* line break before/after binary operator (for this added ignore for\nW503, line break before binary operator, as if I understand well the\nsuggested is to have the line break before the operator)\n* indentation problems\n\nI removed the rule N530 (Direct neutron imports not allowed), as that is\na longer story.\n\nChange-Id: I701c89ecc890a1d866815b5401cd4c022c51c9c4\nDepends-On: https://review.opendev.org/#/c/708965/\n'}]",0,714132,d78d241f4072760ebdb279f683c773bcf84339b5,2,0,1,8313,,,0,"Switch to hacking 2.0

Make the code compatible with hacking rules:
* import order problems
* line break before/after binary operator (for this added ignore for
W503, line break before binary operator, as if I understand well the
suggested is to have the line break before the operator)
* indentation problems

I removed the rule N530 (Direct neutron imports not allowed), as that is
a longer story.

Change-Id: I701c89ecc890a1d866815b5401cd4c022c51c9c4
Depends-On: https://review.opendev.org/#/c/708965/
",git fetch https://review.opendev.org/openstack/tap-as-a-service refs/changes/32/714132/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron_taas/db/migration/alembic_migration/versions/newton/contract/2ecce0368a62_add_foreign_key_constraint_on_tap_id_association.py', 'neutron_taas/services/taas/drivers/linux/sriov_nic_utils.py', 'test-requirements.txt', 'neutron_taas/db/migration/alembic_migration/versions/newton/expand/04625466c6fa_initial_newton_no_op_expand_script.py', 'neutron_taas/db/migration/alembic_migration/versions/newton/contract/80c85b675b6e_initial_newton_no_op_contract_script.py', 'lower-constraints.txt', 'neutron_taas/db/taas_db.py', 'neutron_taas/services/taas/drivers/linux/ovs_taas.py', 'neutron_taas/tests/unit/db/test_migrations.py', 'neutron_taas/db/migration/alembic_migration/env.py', 'neutron_taas/db/migration/alembic_migration/versions/newton/contract/1817af933379_remove_network_id_from_tap_service.py', 'neutron_taas/db/migration/alembic_migration/versions/stein/expand/ccbcc559d175_add_vlan_filter_to_tap_flow.py', 'neutron_taas/tests/unit/services/taas/test_taas_plugin.py', 'neutron_taas/db/migration/alembic_migration/versions/start_neutron_taas.py', 'neutron_taas/db/migration/alembic_migration/versions/newton/expand/fddbdec8711a_add_status.py', 'neutron_taas/db/migration/alembic_migration/versions/newton/contract/4086b3cffc01_rename_tenant_to_project.py', 'tox.ini', 'neutron_taas/db/migration/alembic_migration/versions/pike/contract/bac61f603e39_alter_tap_id_associations_to_support_tap_id_reuse.py', 'releasenotes/source/conf.py']",19,d78d241f4072760ebdb279f683c773bcf84339b5,hacking2, import pbr.version ,#import pbr.version,75,49
openstack%2Fpython-cloudkittyclient~stable%2Frocky~Ie2de0162311c2d162c1573042187ac4e628bd966,openstack/python-cloudkittyclient,stable/rocky,Ie2de0162311c2d162c1573042187ac4e628bd966,Fix the rating.get_quotation method,MERGED,2020-03-20 10:29:44.000000000,2020-03-20 15:22:03.000000000,2020-03-20 15:22:03.000000000,"[{'_account_id': 22348}, {'_account_id': 29503}]","[{'number': 1, 'created': '2020-03-20 10:29:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cloudkittyclient/commit/3f77d0e84f2b478318e60d9e4894ff1c29044d35', 'message': 'Fix the rating.get_quotation method\n\nThis updates the rating.get_quotation method of the client. Tests on this\nmethod have been added.\n\nDepends-On: https://review.openstack.org/#/c/648062/\nChange-Id: Ie2de0162311c2d162c1573042187ac4e628bd966\n'}, {'number': 2, 'created': '2020-03-20 10:39:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cloudkittyclient/commit/455231234dedd49e3ec72778977e53e751b11357', 'message': 'Fix the rating.get_quotation method\n\nThis updates the rating.get_quotation method of the client. Tests on this\nmethod have been added.\n\nDepends-On: https://review.openstack.org/#/c/648062/\nChange-Id: Ie2de0162311c2d162c1573042187ac4e628bd966\n(cherry picked from commit de96c61985a978a5c1ca6ddf5fe58b3f601c054f)\n'}, {'number': 3, 'created': '2020-03-20 13:44:27.000000000', 'files': ['cloudkittyclient/v1/rating/__init__.py', 'releasenotes/notes/fix-get-quotation-1d2c18a979f85fe6.yaml', 'cloudkittyclient/tests/functional/v1/base.py', 'cloudkittyclient/tests/unit/v1/test_rating.py'], 'web_link': 'https://opendev.org/openstack/python-cloudkittyclient/commit/37c6bb0c8b2fd9a67a8d15830f545949657e74eb', 'message': 'Fix the rating.get_quotation method\n\nThis updates the rating.get_quotation method of the client. Tests on this\nmethod have been added.\n\nDepends-On: https://review.openstack.org/#/c/648062/\nChange-Id: Ie2de0162311c2d162c1573042187ac4e628bd966\n(cherry picked from commit de96c61985a978a5c1ca6ddf5fe58b3f601c054f)\n'}]",0,714067,37c6bb0c8b2fd9a67a8d15830f545949657e74eb,11,2,3,29503,,,0,"Fix the rating.get_quotation method

This updates the rating.get_quotation method of the client. Tests on this
method have been added.

Depends-On: https://review.openstack.org/#/c/648062/
Change-Id: Ie2de0162311c2d162c1573042187ac4e628bd966
(cherry picked from commit de96c61985a978a5c1ca6ddf5fe58b3f601c054f)
",git fetch https://review.opendev.org/openstack/python-cloudkittyclient refs/changes/67/714067/3 && git format-patch -1 --stdout FETCH_HEAD,"['cloudkittyclient/v1/rating/__init__.py', 'releasenotes/notes/fix-get-quotation-1d2c18a979f85fe6.yaml', 'cloudkittyclient/tests/unit/v1/test_rating.py']",3,3f77d0e84f2b478318e60d9e4894ff1c29044d35,,"# Copyright 2019 Objectif Libre # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # import decimal from cloudkittyclient import exc from cloudkittyclient.tests.unit.v1 import base class TestRating(base.BaseAPIEndpointTestCase): def test_quote_request(self): res_data = [{'usage': { 'instance': [{ 'vol': {'unit': 'undef', 'qty': '1'}, 'rating': {'price': decimal.Decimal(1)}, 'desc': { 'disk_total_display': 1, 'image_id': 'c43a3e7d-c4e6-45d6-8c8d-e2832a45bc0a', 'ram': 64, 'ephemeral': 0, 'vcpus': 1, 'source_type': 'image', 'disk_total': 1, 'flavor_id': '42', 'flavor': 'm1.nano', 'disk': 1, 'source_val': 'c43a3e7d-c4e6-45d6-8c8d-e2832a45bc0a'} }] }}] self.rating.get_quotation(res_data=res_data) self.api_client.post.assert_called_once_with( '/v1/rating/quote/', json={'resources': res_data}) def test_get_quotation_no_res_data(self): self.assertRaises(exc.ArgumentRequired, self.rating.get_quotation) ",,57,2
openstack%2Ftripleo-heat-templates~master~I263133c046c8c48f838da6d506b5dd046fbf7048,openstack/tripleo-heat-templates,master,I263133c046c8c48f838da6d506b5dd046fbf7048,Fix dashboard_frontend_vip parameter,MERGED,2020-03-19 19:00:56.000000000,2020-03-20 15:18:46.000000000,2020-03-20 15:01:50.000000000,"[{'_account_id': 3153}, {'_account_id': 6796}, {'_account_id': 14985}, {'_account_id': 18002}, {'_account_id': 20182}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25402}]","[{'number': 1, 'created': '2020-03-19 19:00:56.000000000', 'files': ['deployment/ceph-ansible/ceph-mgr.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/a6ff7e2eacf27536b98ee10d511281b3d19a2b0a', 'message': 'Fix dashboard_frontend_vip parameter\n\nThis patch fixes the dashboard_frontend_vip network\nused to properly configure grafana in HA, reflecting\nthe change we have in haproxy.pp\nThis change also exposes a new variable to give the\noperators the chance to configure a read-only admin\nuser (e.g., no pools can be created through the\ndashboard).\n\nDepends-On: https://review.opendev.org/#/c/713892\nChange-Id: I263133c046c8c48f838da6d506b5dd046fbf7048\nCloses-Bug: #1868118\n'}]",0,713967,a6ff7e2eacf27536b98ee10d511281b3d19a2b0a,12,8,1,25402,,,0,"Fix dashboard_frontend_vip parameter

This patch fixes the dashboard_frontend_vip network
used to properly configure grafana in HA, reflecting
the change we have in haproxy.pp
This change also exposes a new variable to give the
operators the chance to configure a read-only admin
user (e.g., no pools can be created through the
dashboard).

Depends-On: https://review.opendev.org/#/c/713892
Change-Id: I263133c046c8c48f838da6d506b5dd046fbf7048
Closes-Bug: #1868118
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/67/713967/1 && git format-patch -1 --stdout FETCH_HEAD,['deployment/ceph-ansible/ceph-mgr.yaml'],1,a6ff7e2eacf27536b98ee10d511281b3d19a2b0a,lp1868118," CephDashboardAdminRO: type: boolean default: true description: Parameter used to set a read-only admin user. dashboard_frontend_vip: {get_param: [EndpointMap, CephDashboardInternal, host]} dashboard_admin_user_ro: {get_param: CephDashboardAdminRO}"," dashboard_frontend_vip: {get_param: [EndpointMap, CephGrafanaInternal, host]}",6,1
openstack%2Fzun~master~I2ce108f6bfc91e2aff9857593243a4c0412660bc,openstack/zun,master,I2ce108f6bfc91e2aff9857593243a4c0412660bc,Failed to format message,MERGED,2020-03-20 07:28:14.000000000,2020-03-20 15:17:51.000000000,2020-03-20 15:01:58.000000000,"[{'_account_id': 11536}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 07:28:14.000000000', 'files': ['zun/common/exception.py'], 'web_link': 'https://opendev.org/openstack/zun/commit/5dc4ae5a0d8f9cd8509f17c8dab9d738d47bc7dd', 'message': 'Failed to format message\n\nFile ""/usr/lib/python2.7/site-packages/oslo_privsep/daemon.py"", line 204, in remote_call\n  raise exc_type(*result[2])\nFile ""/usr/lib/python2.7/site-packages/zun/common/exception.py"", line 204, in __init__\n  self.message = str(self.message) % kwargs\nValueError: unsupported format character \')\' (0x29) at index 345\n\nChange-Id: I2ce108f6bfc91e2aff9857593243a4c0412660bc\n'}]",0,714042,5dc4ae5a0d8f9cd8509f17c8dab9d738d47bc7dd,7,2,1,23365,,,0,"Failed to format message

File ""/usr/lib/python2.7/site-packages/oslo_privsep/daemon.py"", line 204, in remote_call
  raise exc_type(*result[2])
File ""/usr/lib/python2.7/site-packages/zun/common/exception.py"", line 204, in __init__
  self.message = str(self.message) % kwargs
ValueError: unsupported format character ')' (0x29) at index 345

Change-Id: I2ce108f6bfc91e2aff9857593243a4c0412660bc
",git fetch https://review.opendev.org/openstack/zun refs/changes/42/714042/1 && git format-patch -1 --stdout FETCH_HEAD,['zun/common/exception.py'],1,5dc4ae5a0d8f9cd8509f17c8dab9d738d47bc7dd,, self.message = self.message % kwargs, self.message = str(self.message) % kwargs,1,1
openstack%2Fvalidations-libs~master~Ie3366f4f32dd87871315cc9f95f7c2335df25e4b,openstack/validations-libs,master,Ie3366f4f32dd87871315cc9f95f7c2335df25e4b,Implement Validation Show Parameters,MERGED,2020-03-18 23:23:26.000000000,2020-03-20 15:14:36.000000000,2020-03-20 15:14:36.000000000,"[{'_account_id': 11491}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-18 23:23:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/933ec5c89d29f615fd73f4e3ab9f1757abc31e58', 'message': 'Implement Validation Show Parameters\n\nChange-Id: Ie3366f4f32dd87871315cc9f95f7c2335df25e4b\n'}, {'number': 2, 'created': '2020-03-20 09:19:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/bdd5c042ec67f832086651b11af706cf02f53b34', 'message': 'Implement Validation Show Parameters\n\nChange-Id: Ie3366f4f32dd87871315cc9f95f7c2335df25e4b\n'}, {'number': 3, 'created': '2020-03-20 13:45:12.000000000', 'files': ['validations_libs/validation_actions.py', 'validations_libs/utils.py', 'validations_libs/validation.py'], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/fd95a175b725bf047027413d7ef581d1ec2dbcb1', 'message': 'Implement Validation Show Parameters\n\nChange-Id: Ie3366f4f32dd87871315cc9f95f7c2335df25e4b\n'}]",0,713756,fd95a175b725bf047027413d7ef581d1ec2dbcb1,11,2,3,16515,,,0,"Implement Validation Show Parameters

Change-Id: Ie3366f4f32dd87871315cc9f95f7c2335df25e4b
",git fetch https://review.opendev.org/openstack/validations-libs refs/changes/56/713756/1 && git format-patch -1 --stdout FETCH_HEAD,"['validations_libs/validation_actions.py', 'validations_libs/utils.py', 'validations_libs/validation.py']",3,933ec5c89d29f615fd73f4e3ab9f1757abc31e58,validation/show/param, if self.dict['vars'].get('metadata'): self.metadata = {'id': self.id} self.metadata.update(self.dict['vars'].get('metadata')) return self.dict['vars']['metadata'].get('groups'), self.metadata = {'id': self.id} self.metadata.update(self.dict['vars']['metadata']) return self.dict['vars']['metadata']['groups'],26,3
openstack%2Fpython-openstackclient~master~I2bc8b5f48a7c43ef327d0daf06a96154a020b994,openstack/python-openstackclient,master,I2bc8b5f48a7c43ef327d0daf06a96154a020b994,pep8 fix on test_quota.py,ABANDONED,2020-03-20 14:58:52.000000000,2020-03-20 15:12:17.000000000,,"[{'_account_id': 11904}, {'_account_id': 31733}]","[{'number': 1, 'created': '2020-03-20 14:58:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/e90d2711bdf5619c006efe6d9cc8dd3420788f8d', 'message': 'pep8 fix on test_quota.py\n\nChange-Id: I2bc8b5f48a7c43ef327d0daf06a96154a020b994\n'}, {'number': 2, 'created': '2020-03-20 15:10:24.000000000', 'files': ['openstackclient/tests/unit/common/test_quota.py', 'releasenotes/notes/force-flag-openstackclient-c172de2717e5cfac.yaml', 'openstackclient/common/quota.py'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/1bbd547fa7203ae34f1ad15b5597dd65370502fb', 'message': 'pep8 fix on test_quota.py\n\nChange-Id: I2bc8b5f48a7c43ef327d0daf06a96154a020b994\n'}]",1,714135,1bbd547fa7203ae34f1ad15b5597dd65370502fb,5,2,2,31733,,,0,"pep8 fix on test_quota.py

Change-Id: I2bc8b5f48a7c43ef327d0daf06a96154a020b994
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/35/714135/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstackclient/tests/unit/common/test_quota.py', 'releasenotes/notes/force-flag-openstackclient-c172de2717e5cfac.yaml', 'openstackclient/common/quota.py']",3,e90d2711bdf5619c006efe6d9cc8dd3420788f8d,bug/2007440," parser.add_argument( '--force', action='store_true', help=_('Force quota update'), ) if parsed_args.force: compute_kwargs['force'] = True if parsed_args.force: if volume_kwargs: sys.stderr.write(""--foce is only supported by compute. \n"") if network_kwargs: sys.stderr.write(""--force is only supported by compute. \n"") ",,51,0
openstack%2Fvalidations-libs~master~Idc7a55e26de20968f0a6a90f2a005d21a30c9e70,openstack/validations-libs,master,Idc7a55e26de20968f0a6a90f2a005d21a30c9e70,Add validation_actions class and group info implementation,MERGED,2020-03-18 22:13:53.000000000,2020-03-20 15:11:21.000000000,2020-03-20 14:08:41.000000000,"[{'_account_id': 11491}, {'_account_id': 16515}, {'_account_id': 22348}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-03-18 22:13:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/4f3fbb0243800bcaf956585028551f397a96423b', 'message': 'Add validation_actions class and group info implementation\n\nMove all validation actions into a ValidationActions class\nin order to have one object with several validations actions\n\nAdd group info implementation\nAnd add group object like validation object to have\na simpler representation of the validation group.\n\nChange-Id: Idc7a55e26de20968f0a6a90f2a005d21a30c9e70\n'}, {'number': 2, 'created': '2020-03-18 22:39:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/b71a2bb48917ba56652b931306f36dd71ccc1bcb', 'message': 'Add validation_actions class and group info implementation\n\nMove all validation actions into a ValidationActions class\nin order to have one object with several validations actions\n\nAdd group info implementation\nAnd add group object like validation object to have\na simpler representation of the validation group.\n\nChange-Id: Idc7a55e26de20968f0a6a90f2a005d21a30c9e70\n'}, {'number': 3, 'created': '2020-03-18 22:40:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/7aa1383741ee5e301e5e08c2e8912301c4ef0d32', 'message': 'Add validation_actions class and group info implementation\n\nMove all validation actions into a ValidationActions class\nin order to have one object with several validations actions\n\nAdd group info implementation\nAnd add group object like validation object to have\na simpler representation of the validation group.\n\nChange-Id: Idc7a55e26de20968f0a6a90f2a005d21a30c9e70\n'}, {'number': 4, 'created': '2020-03-20 09:19:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/7a4fcb9a86c16710b10f205d928ef329ea31aedb', 'message': 'Add validation_actions class and group info implementation\n\nMove all validation actions into a ValidationActions class\nin order to have one object with several validations actions\n\nAdd group info implementation\nAnd add group object like validation object to have\na simpler representation of the validation group.\n\nChange-Id: Idc7a55e26de20968f0a6a90f2a005d21a30c9e70\n'}, {'number': 5, 'created': '2020-03-20 13:44:53.000000000', 'files': ['validations_libs/tests/test_validations_run.py', 'validations_libs/tests/test_validations_show.py', 'validations_libs/validation_actions.py', 'validations_libs/tests/test_validations_list.py', 'validations_libs/list.py', 'validations_libs/show.py', 'validations_libs/group.py', 'validations_libs/group_info.py', 'validations_libs/utils.py'], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/491a27b53ca7cceaf959c2e55cc206fdefddaaab', 'message': 'Add validation_actions class and group info implementation\n\nMove all validation actions into a ValidationActions class\nin order to have one object with several validations actions\n\nAdd group info implementation\nAnd add group object like validation object to have\na simpler representation of the validation group.\n\nChange-Id: Idc7a55e26de20968f0a6a90f2a005d21a30c9e70\n'}]",0,713748,491a27b53ca7cceaf959c2e55cc206fdefddaaab,24,4,5,16515,,,0,"Add validation_actions class and group info implementation

Move all validation actions into a ValidationActions class
in order to have one object with several validations actions

Add group info implementation
And add group object like validation object to have
a simpler representation of the validation group.

Change-Id: Idc7a55e26de20968f0a6a90f2a005d21a30c9e70
",git fetch https://review.opendev.org/openstack/validations-libs refs/changes/48/713748/4 && git format-patch -1 --stdout FETCH_HEAD,"['validations_libs/tests/test_validations_run.py', 'validations_libs/tests/test_validations_show.py', 'validations_libs/validation_actions.py', 'validations_libs/tests/test_validations_list.py', 'validations_libs/list.py', 'validations_libs/show.py', 'validations_libs/group.py', 'validations_libs/group_info.py']",8,4f3fbb0243800bcaf956585028551f397a96423b,validation/group,,,94,88
openstack%2Fproject-config~master~I761615fd7edf4e9209d06e56aee6bd2988395376,openstack/project-config,master,I761615fd7edf4e9209d06e56aee6bd2988395376,Rename x/devstack-plugin-nfs to openstack/devstack-plugin-nfs,MERGED,2020-03-08 02:53:46.000000000,2020-03-20 15:01:18.000000000,2020-03-20 15:01:18.000000000,"[{'_account_id': 5263}, {'_account_id': 5314}, {'_account_id': 6547}, {'_account_id': 8556}, {'_account_id': 22348}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-08 02:53:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/31f5922ddc4ee84a18a622222700f206a4e61497', 'message': 'Add devstack-plugin-nfs project under QA\n\ndevstack-plugin-nfs is devstack plugin to\nconfigure the NFS backend.\n\nWe are going to bring this plugin under QA\nproject.\n\nDetail discussion: http://lists.openstack.org/pipermail/openstack-discuss/2020-March/013048.html\n\nChange-Id: I761615fd7edf4e9209d06e56aee6bd2988395376\n'}, {'number': 2, 'created': '2020-03-08 02:58:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/9dde617c68a972caa7a5eb4b94644abb741034ee', 'message': 'Add devstack-plugin-nfs project under QA\n\ndevstack-plugin-nfs is devstack plugin to\nconfigure the NFS backend.\n\nWe are going to bring this plugin under QA\nproject.\n\nDetail discussion: http://lists.openstack.org/pipermail/openstack-discuss/2020-March/013048.html\n\nNeeded-By: https://review.opendev.org/#/c/711835/\n\nChange-Id: I761615fd7edf4e9209d06e56aee6bd2988395376\n'}, {'number': 3, 'created': '2020-03-10 01:24:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/9b7dbd0d4fc0a9a989caf44ceb28de83be857e36', 'message': 'Rename x/devstack-plugin-nfs to openstack/devstack-plugin-nfs\n\ndevstack-plugin-nfs is devstack plugin to\nconfigure the NFS backend.\n\nWe are going to bring this plugin under QA\ngovernance.\n\nDetail discussion: http://lists.openstack.org/pipermail/openstack-discuss/2020-March/013048.html\n\nNeeded-By: https://review.opendev.org/#/c/711835/\n\nChange-Id: I761615fd7edf4e9209d06e56aee6bd2988395376\n'}, {'number': 4, 'created': '2020-03-20 14:54:41.000000000', 'files': ['gerrit/acls/openstack/devstack-plugin-nfs.config', 'gerritbot/channels.yaml', 'zuul.d/projects.yaml', 'zuul/main.yaml', 'gerrit/projects.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/11bd536edc698c9beecebe0d3ca365046024f235', 'message': 'Rename x/devstack-plugin-nfs to openstack/devstack-plugin-nfs\n\ndevstack-plugin-nfs is devstack plugin to\nconfigure the NFS backend.\n\nWe are going to bring this plugin under QA\ngovernance.\n\nDetail discussion: http://lists.openstack.org/pipermail/openstack-discuss/2020-March/013048.html\n\nNeeded-By: https://review.opendev.org/#/c/711835/\n\nChange-Id: I761615fd7edf4e9209d06e56aee6bd2988395376\n'}]",10,711834,11bd536edc698c9beecebe0d3ca365046024f235,28,6,4,8556,,,0,"Rename x/devstack-plugin-nfs to openstack/devstack-plugin-nfs

devstack-plugin-nfs is devstack plugin to
configure the NFS backend.

We are going to bring this plugin under QA
governance.

Detail discussion: http://lists.openstack.org/pipermail/openstack-discuss/2020-March/013048.html

Needed-By: https://review.opendev.org/#/c/711835/

Change-Id: I761615fd7edf4e9209d06e56aee6bd2988395376
",git fetch https://review.opendev.org/openstack/project-config refs/changes/34/711834/3 && git format-patch -1 --stdout FETCH_HEAD,"['gerrit/acls/openstack/devstack-plugin-nfs.config', 'gerritbot/channels.yaml', 'gerrit/projects.yaml', 'zuul/main.yaml']",4,31f5922ddc4ee84a18a622222700f206a4e61497,rename-infra-manual, - openstack/devstack-plugin-nfs, - x/devstack-plugin-nfs,16,1
openstack%2Fproject-config~master~I96d95781805826b1060523a3c616a9a2f1dc51dd,openstack/project-config,master,I96d95781805826b1060523a3c616a9a2f1dc51dd,Move openstack/infra-manuals to opendev,MERGED,2020-03-10 18:22:07.000000000,2020-03-20 14:59:31.000000000,2020-03-20 14:59:28.000000000,"[{'_account_id': 5263}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-10 18:22:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/f0b534310be2a161f792ec54bde12955076e323f', 'message': 'Move openstack/infra-manuals to opendev\n\nThe infra-manual is now the OpenDev manual, move it to opendev\nnamespace.\n\nRemove also the publish job, we can publish now to docs.opendev.org.\n\nChange-Id: I96d95781805826b1060523a3c616a9a2f1dc51dd\n'}, {'number': 2, 'created': '2020-03-10 18:24:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/22f46d6a71d204b02b8732e08146d201f4b32d29', 'message': 'Move openstack/infra-manuals to opendev\n\nThe infra-manual is now the OpenDev manual, move it to opendev\nnamespace.\n\nRemove also the publish job, we can publish now to docs.opendev.org.\n\nChange-Id: I96d95781805826b1060523a3c616a9a2f1dc51dd\n'}, {'number': 3, 'created': '2020-03-19 16:38:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/9b79bead302710ff8e7e0a65429b330c8b65922f', 'message': 'Move openstack/infra-manuals to opendev\n\nThe infra-manual is now the OpenDev manual, move it to opendev\nnamespace.\n\nRemove also the publish job, we can publish now to docs.opendev.org.\n\nChange-Id: I96d95781805826b1060523a3c616a9a2f1dc51dd\n'}, {'number': 4, 'created': '2020-03-20 14:51:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/e2bead3bc72fbf6ad8090fae9c2ba6d8524c97bd', 'message': 'Move openstack/infra-manuals to opendev\n\nThe infra-manual is now the OpenDev manual, move it to opendev\nnamespace.\n\nRemove also the publish job, we can publish now to docs.opendev.org.\n\nChange-Id: I96d95781805826b1060523a3c616a9a2f1dc51dd\n'}, {'number': 5, 'created': '2020-03-20 14:54:41.000000000', 'files': ['gerritbot/channels.yaml', 'gerrit/acls/opendev/infra-manual.config', 'zuul.d/projects.yaml', 'zuul/main.yaml', 'gerrit/projects.yaml', 'zuul.d/jobs.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/bd2cec54e3337885ed53d57ba9ccd1f08e864a46', 'message': 'Move openstack/infra-manuals to opendev\n\nThe infra-manual is now the OpenDev manual, move it to opendev\nnamespace.\n\nRemove also the publish job, we can publish now to docs.opendev.org.\n\nChange-Id: I96d95781805826b1060523a3c616a9a2f1dc51dd\n'}]",4,712149,bd2cec54e3337885ed53d57ba9ccd1f08e864a46,11,2,5,6547,,,0,"Move openstack/infra-manuals to opendev

The infra-manual is now the OpenDev manual, move it to opendev
namespace.

Remove also the publish job, we can publish now to docs.opendev.org.

Change-Id: I96d95781805826b1060523a3c616a9a2f1dc51dd
",git fetch https://review.opendev.org/openstack/project-config refs/changes/49/712149/4 && git format-patch -1 --stdout FETCH_HEAD,"['gerritbot/channels.yaml', 'gerrit/acls/opendev/infra-manual.config', 'gerrit/projects.yaml', 'zuul.d/projects.yaml', 'zuul.d/jobs.yaml']",5,f0b534310be2a161f792ec54bde12955076e323f,rename-infra-manual,," name: promote-infra-manual parent: promote-tox-docs-special-base description: | Promote infra-manual documents to https://docs.openstack.org/infra/manual allowed-projects: - openstack/infra-manual final: true branches: master vars: download_artifact_job: tox-docs special_publish_directory: ""infra/manual"" - job:",14,29
openstack%2Fpython-openstackclient~master~Ic4d70794a76045bfbd0b64191edc569751c5be53,openstack/python-openstackclient,master,Ic4d70794a76045bfbd0b64191edc569751c5be53,"added tests code for --force, also added check for neutron and cinder",ABANDONED,2020-03-20 13:48:44.000000000,2020-03-20 14:46:05.000000000,,[],"[{'number': 1, 'created': '2020-03-20 13:48:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/f45549d569ca3ba2772d3b7731cf26fbcf7ab7ec', 'message': 'added tests code for --force, also added check for neutron and cinder\n\nChange-Id: Ic4d70794a76045bfbd0b64191edc569751c5be53\n'}, {'number': 2, 'created': '2020-03-20 14:45:03.000000000', 'files': ['openstackclient/tests/unit/common/test_quota.py', 'openstackclient/common/quota.py'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/717018d5d7ffbeee6369e24a1e1fbd1753dc8fc2', 'message': 'added tests code for --force, also added check for neutron and cinder\n\nChange-Id: Ic4d70794a76045bfbd0b64191edc569751c5be53\n'}]",4,714114,717018d5d7ffbeee6369e24a1e1fbd1753dc8fc2,4,0,2,31733,,,0,"added tests code for --force, also added check for neutron and cinder

Change-Id: Ic4d70794a76045bfbd0b64191edc569751c5be53
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/14/714114/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstackclient/tests/unit/common/test_quota.py', 'openstackclient/common/quota.py']",2,f45549d569ca3ba2772d3b7731cf26fbcf7ab7ec,bug/2007440," parser.add_argument( '--force', action='store_true', help=_('Force quota update'), ) if parsed_args.force: compute_kwargs['force'] = True if parsed_args.force: if volume_kwargs: sys.stderr.write(""--foce is only supported by compute. \n"") if network_kwargs: sys.stderr.write(""--force is only supported by compute. \n"") ",,47,0
openstack%2Frpm-packaging~stable%2Ftrain~Iaf45fb9d96b9b62220a8a21a8d65be66378e2507,openstack/rpm-packaging,stable/train,Iaf45fb9d96b9b62220a8a21a8d65be66378e2507,heat-dashboard: fix python3 reqs for -test subpackage,MERGED,2020-03-20 09:49:40.000000000,2020-03-20 14:42:50.000000000,2020-03-20 14:42:49.000000000,"[{'_account_id': 2062}, {'_account_id': 6593}, {'_account_id': 7102}, {'_account_id': 8482}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-20 09:49:40.000000000', 'files': ['openstack/heat-dashboard/heat-dashboard.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/5935d8b9345084012c6ff212564c223fe5e9f1fa', 'message': 'heat-dashboard: fix python3 reqs for -test subpackage\n\nThey were still using the Python 2 version of the packages, instead\nof the Python 3 one.\n\nChange-Id: Iaf45fb9d96b9b62220a8a21a8d65be66378e2507\n(cherry picked from commit 9787a667a9944544a033889f0eaca87c117f08c9)\n'}]",0,714063,5935d8b9345084012c6ff212564c223fe5e9f1fa,9,7,1,13294,,,0,"heat-dashboard: fix python3 reqs for -test subpackage

They were still using the Python 2 version of the packages, instead
of the Python 3 one.

Change-Id: Iaf45fb9d96b9b62220a8a21a8d65be66378e2507
(cherry picked from commit 9787a667a9944544a033889f0eaca87c117f08c9)
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/63/714063/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/heat-dashboard/heat-dashboard.spec.j2'],1,5935d8b9345084012c6ff212564c223fe5e9f1fa,remove-python2-stable/train,Requires: {{ py3('mock') }} Requires: {{ py3('testtools') }},Requires: {{ py2pkg('mock') }} Requires: {{ py2pkg('testtools') }},2,2
openstack%2Fnova~master~I0c3f14100a18107f7e416293f3d4fcc641ce5e55,openstack/nova,master,I0c3f14100a18107f7e416293f3d4fcc641ce5e55,libvirt: Correctly resize encrypted LUKSv1 volumes,MERGED,2020-02-10 16:29:30.000000000,2020-03-20 14:30:16.000000000,2020-03-20 14:26:44.000000000,"[{'_account_id': 9008}, {'_account_id': 9708}, {'_account_id': 9732}, {'_account_id': 10118}, {'_account_id': 10135}, {'_account_id': 10379}, {'_account_id': 11604}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-02-10 16:29:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ccf962c014bed3a19b64ee0ee21cb72772a714a9', 'message': 'WIP libvirt: Fix attached encrypted volume extension\n\nDepends-On: https://review.opendev.org/706893\nDepends-On: https://review.opendev.org/706880\n\nCloses-Bug: #1861071\nChange-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55\n'}, {'number': 2, 'created': '2020-02-11 18:11:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/04a34f7bbf4db5547cdbc679f8bea8e4f09ba76c', 'message': 'WIP libvirt: Fix attached encrypted LUKSv1 volume extension\n\nCloses-Bug: #1861071\nDepends-On: https://review.opendev.org/706880\nChange-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55\n'}, {'number': 3, 'created': '2020-02-12 09:38:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9bf86d6686ec1b4680c42f0f5f66f28540e0858e', 'message': 'WIP libvirt: Fix attached encrypted LUKSv1 volume extension\n\nCloses-Bug: #1861071\nDepends-On: https://review.opendev.org/706880\nChange-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55\n'}, {'number': 4, 'created': '2020-02-12 18:05:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f7707e318bd1a2f8a316d5ab28b1a0166a6145cc', 'message': ""libvirt: Correctly resize encrypted LUKSv1 volumes\n\nWhen extending volumes the Libvirt virt driver first calls os-brick to\nrefresh any host block devices so they show their new extended size\nbefore calling down into Libvirt's virDomainBlockResize API to do the\nsame within the instance.\n\nWhen extending encrypted LUKSv1 volumes the new size of the volume\nprovided to the virDomainBlockResize API did not previously take the\nspace already consumed by the LUKSv1 header into account. As a result\nany attempt to extend a host block device based encrypted LUKSv1 volume\nwould fail as Libvirt would need to grow the device to allow room for\nthe LUKSv1 header.\n\nThis change corrects this by inspecting the host block device or remote\nRBD volume using ``qemu-img info``. The output from this call now\nincludes LUKSv1 specific data such as the offset of the encrypted\npayload, essentially where the users data actually starts within the\nvolume.\n\nBy subtracting this offset from the new overall size of the extended\nvolume we can ensure calls to virDomainBlockResize will now succeed as\nno attempt will be made by Libvirt to grow the device.\n\nFuture work will also need to handle this extend_volume use case for\n``dm-crypt`` based encryption providers ``plain`` and ``luks2``.\n\nCloses-Bug: #1861071\nDepends-On: https://review.opendev.org/706880\nChange-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55\n""}, {'number': 5, 'created': '2020-02-19 15:53:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9d2f70d96d1514bee6970cf4867a4aeaf3abe5da', 'message': ""libvirt: Correctly resize encrypted LUKSv1 volumes\n\nWhen extending volumes the Libvirt virt driver first calls os-brick to\nrefresh any host block devices so they show their new extended size\nbefore calling down into Libvirt's virDomainBlockResize API to do the\nsame within the instance.\n\nWhen extending encrypted LUKSv1 volumes the new size of the volume\nprovided to the virDomainBlockResize API did not previously take the\nspace already consumed by the LUKSv1 header into account. As a result\nany attempt to extend a host block device based encrypted LUKSv1 volume\nwould fail as Libvirt would need to grow the device to allow room for\nthe LUKSv1 header.\n\nThis change corrects this by inspecting the host block device or remote\nRBD volume using ``qemu-img info``. The output from this call now\nincludes LUKSv1 specific data such as the offset of the encrypted\npayload, essentially where the users data actually starts within the\nvolume.\n\nBy subtracting this offset from the new overall size of the extended\nvolume we can ensure calls to virDomainBlockResize will now succeed as\nno attempt will be made by Libvirt to grow the device.\n\nFuture work will also need to handle this extend_volume use case for\n``dm-crypt`` based encryption providers ``plain`` and ``luks2``.\n\nCloses-Bug: #1861071\nDepends-On: https://review.opendev.org/706880\nChange-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55\n""}, {'number': 6, 'created': '2020-03-02 10:05:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/070996ff69cf53533bbc3987ce697278c55e1974', 'message': ""libvirt: Correctly resize encrypted LUKSv1 volumes\n\nWhen extending volumes the Libvirt virt driver first calls os-brick to\nrefresh any host block devices so they show their new extended size\nbefore calling down into Libvirt's virDomainBlockResize API to do the\nsame within the instance.\n\nWhen extending encrypted LUKSv1 volumes the new size of the volume\nprovided to the virDomainBlockResize API did not previously take the\nspace already consumed by the LUKSv1 header into account. As a result\nany attempt to extend a host block device based encrypted LUKSv1 volume\nwould fail as Libvirt would need to grow the device to allow room for\nthe LUKSv1 header.\n\nThis change corrects this by inspecting the host block device or remote\nRBD volume using ``qemu-img info``. Since oslo.utils 4.1.0 the returned\nQemuImgInfo object that encapsulates the output of this call now\nincludes LUKSv1 specific data such as the offset of the encrypted\npayload, essentially where the users data actually starts within the\nvolume.\n\nBy subtracting this offset from the new overall size of the extended\nvolume we can ensure calls to virDomainBlockResize will now succeed as\nno attempt will be made by Libvirt to grow the device.\n\nFuture work will also need to handle this extend_volume use case for\n``dm-crypt`` based encryption providers ``plain`` and ``luks2``.\n\nCloses-Bug: #1861071\nDepends-On: https://review.opendev.org/706880\nChange-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55\n""}, {'number': 7, 'created': '2020-03-02 14:46:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f86ea8bd62ebc4202346146136f5c80ce6b3fc7d', 'message': ""libvirt: Correctly resize encrypted LUKSv1 volumes\n\nWhen extending volumes the Libvirt virt driver first calls os-brick to\nrefresh any host block devices so they show their new extended size\nbefore calling down into Libvirt's virDomainBlockResize API to do the\nsame within the instance.\n\nWhen extending encrypted LUKSv1 volumes the new size of the volume\nprovided to the virDomainBlockResize API did not previously take the\nspace already consumed by the LUKSv1 header into account. As a result\nany attempt to extend a host block device based encrypted LUKSv1 volume\nwould fail as Libvirt would need to grow the device to allow room for\nthe LUKSv1 header.\n\nThis change corrects this by inspecting the host block device or remote\nRBD volume using ``qemu-img info``. Since oslo.utils 4.1.0 the returned\nQemuImgInfo object that encapsulates the output of this call now\nincludes LUKSv1 specific data such as the offset of the encrypted\npayload, essentially where the users data actually starts within the\nvolume.\n\nBy subtracting this offset from the new overall size of the extended\nvolume we can ensure calls to virDomainBlockResize will now succeed as\nno attempt will be made by Libvirt to grow the device.\n\nFuture work will also need to handle this extend_volume use case for\n``dm-crypt`` based encryption providers ``plain`` and ``luks2``.\n\nCloses-Bug: #1861071\nDepends-On: https://review.opendev.org/706880\nChange-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55\n""}, {'number': 8, 'created': '2020-03-03 09:09:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d3f5d7884e7d17230600bbd1ee778c6c2051e954', 'message': ""libvirt: Correctly resize encrypted LUKSv1 volumes\n\nWhen extending volumes the Libvirt virt driver first calls os-brick to\nrefresh any host block devices so they show their new extended size\nbefore calling down into Libvirt's virDomainBlockResize API to do the\nsame within the instance.\n\nWhen extending encrypted LUKSv1 volumes the new size of the volume\nprovided to the virDomainBlockResize API did not previously take the\nspace already consumed by the LUKSv1 header into account. As a result\nany attempt to extend a host block device based encrypted LUKSv1 volume\nwould fail as Libvirt would need to grow the device to allow room for\nthe LUKSv1 header.\n\nThis change corrects this by inspecting the host block device or remote\nRBD volume using ``qemu-img info``. Since oslo.utils 4.1.0 the returned\nQemuImgInfo object that encapsulates the output of this call now\nincludes LUKSv1 specific data such as the offset of the encrypted\npayload, essentially where the users data actually starts within the\nvolume.\n\nBy subtracting this offset from the new overall size of the extended\nvolume we can ensure calls to virDomainBlockResize will now succeed as\nno attempt will be made by Libvirt to grow the device.\n\nFuture work will also need to handle this extend_volume use case for\n``dm-crypt`` based encryption providers ``plain`` and ``luks2``.\n\nCloses-Bug: #1861071\nDepends-On: https://review.opendev.org/706880\nChange-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55\n""}, {'number': 9, 'created': '2020-03-09 15:03:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/19887328a5aa46edfbdb0c051926dfdb912718a7', 'message': ""libvirt: Correctly resize encrypted LUKSv1 volumes\n\nWhen extending volumes the Libvirt virt driver first calls os-brick to\nrefresh any host block devices so they show their new extended size\nbefore calling down into Libvirt's virDomainBlockResize API to do the\nsame within the instance.\n\nWhen extending encrypted LUKSv1 volumes the new size of the volume\nprovided to the virDomainBlockResize API did not previously take the\nspace already consumed by the LUKSv1 header into account. As a result\nany attempt to extend a host block device based encrypted LUKSv1 volume\nwould fail as Libvirt would need to grow the device to allow room for\nthe LUKSv1 header.\n\nThis change corrects this by inspecting the host block device or remote\nRBD volume using ``qemu-img info``. Since oslo.utils 4.1.0 the returned\nQemuImgInfo object that encapsulates the output of this call now\nincludes LUKSv1 specific data such as the offset of the encrypted\npayload, essentially where the users data actually starts within the\nvolume.\n\nBy subtracting this offset from the new overall size of the extended\nvolume we can ensure calls to virDomainBlockResize will now succeed as\nno attempt will be made by Libvirt to grow the device.\n\nFuture work will also need to handle this extend_volume use case for\n``dm-crypt`` based encryption providers ``plain`` and ``luks2``.\n\nCloses-Bug: #1861071\nDepends-On: https://review.opendev.org/706880\nChange-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55\n""}, {'number': 10, 'created': '2020-03-09 19:19:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f90c494989ee65cc97eb61b5522b3fb0bc4d1628', 'message': ""libvirt: Correctly resize encrypted LUKSv1 volumes\n\nWhen extending volumes the Libvirt virt driver first calls os-brick to\nrefresh any host block devices so they show their new extended size\nbefore calling down into Libvirt's virDomainBlockResize API to do the\nsame within the instance.\n\nWhen extending encrypted LUKSv1 volumes the new size of the volume\nprovided to the virDomainBlockResize API did not previously take the\nspace already consumed by the LUKSv1 header into account. As a result\nany attempt to extend a host block device based encrypted LUKSv1 volume\nwould fail as Libvirt would need to grow the device to allow room for\nthe LUKSv1 header.\n\nThis change corrects this by inspecting the host block device or remote\nRBD volume using ``qemu-img info``. Since oslo.utils 4.1.0 the returned\nQemuImgInfo object that encapsulates the output of this call now\nincludes LUKSv1 specific data such as the offset of the encrypted\npayload, essentially where the users data actually starts within the\nvolume.\n\nBy subtracting this offset from the new overall size of the extended\nvolume we can ensure calls to virDomainBlockResize will now succeed as\nno attempt will be made by Libvirt to grow the device.\n\nFuture work will also need to handle this extend_volume use case for\n``dm-crypt`` based encryption providers ``plain`` and ``luks2``.\n\nCloses-Bug: #1861071\nDepends-On: https://review.opendev.org/706880\nChange-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55\n""}, {'number': 11, 'created': '2020-03-11 10:52:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/018fabeb8535919ad1f47dabacae48ac392cb277', 'message': ""libvirt: Correctly resize encrypted LUKSv1 volumes\n\nWhen extending volumes the Libvirt virt driver first calls os-brick to\nrefresh any host block devices so they show their new extended size\nbefore calling down into Libvirt's virDomainBlockResize API to do the\nsame within the instance.\n\nWhen extending encrypted LUKSv1 volumes the new size of the volume\nprovided to the virDomainBlockResize API did not previously take the\nspace already consumed by the LUKSv1 header into account. As a result\nany attempt to extend a host block device based encrypted LUKSv1 volume\nwould fail as Libvirt would need to grow the device to allow room for\nthe LUKSv1 header.\n\nThis change corrects this by inspecting the host block device or remote\nRBD volume using ``qemu-img info``. Since oslo.utils 4.1.0 the returned\nQemuImgInfo object that encapsulates the output of this call now\nincludes LUKSv1 specific data such as the offset of the encrypted\npayload, essentially where the users data actually starts within the\nvolume.\n\nBy subtracting this offset from the new overall size of the extended\nvolume we can ensure calls to virDomainBlockResize will now succeed as\nno attempt will be made by Libvirt to grow the device.\n\nFuture work will also need to handle this extend_volume use case for\n``dm-crypt`` based encryption providers ``plain`` and ``luks2``.\n\nCloses-Bug: #1861071\nDepends-On: https://review.opendev.org/706880\nChange-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55\n""}, {'number': 12, 'created': '2020-03-16 09:45:47.000000000', 'files': ['nova/virt/libvirt/driver.py', 'nova/tests/unit/virt/libvirt/test_driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/961df123934046b67fb95d1d18501913d5c20f29', 'message': ""libvirt: Correctly resize encrypted LUKSv1 volumes\n\nWhen extending volumes the Libvirt virt driver first calls os-brick to\nrefresh any host block devices so they show their new extended size\nbefore calling down into Libvirt's virDomainBlockResize API to do the\nsame within the instance.\n\nWhen extending encrypted LUKSv1 volumes the new size of the volume\nprovided to the virDomainBlockResize API did not previously take the\nspace already consumed by the LUKSv1 header into account. As a result\nany attempt to extend a host block device based encrypted LUKSv1 volume\nwould fail as Libvirt would need to grow the device to allow room for\nthe LUKSv1 header.\n\nThis change corrects this by inspecting the host block device or remote\nRBD volume using ``qemu-img info``. Since oslo.utils 4.1.0 the returned\nQemuImgInfo object that encapsulates the output of this call now\nincludes LUKSv1 specific data such as the offset of the encrypted\npayload, essentially where the users data actually starts within the\nvolume.\n\nBy subtracting this offset from the new overall size of the extended\nvolume we can ensure calls to virDomainBlockResize will now succeed as\nno attempt will be made by Libvirt to grow the device.\n\nFuture work will also need to handle this extend_volume use case for\n``dm-crypt`` based encryption providers ``plain`` and ``luks2``.\n\nCloses-Bug: #1861071\nDepends-On: https://review.opendev.org/706880\nChange-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55\n""}]",19,706900,961df123934046b67fb95d1d18501913d5c20f29,159,15,12,10135,,,0,"libvirt: Correctly resize encrypted LUKSv1 volumes

When extending volumes the Libvirt virt driver first calls os-brick to
refresh any host block devices so they show their new extended size
before calling down into Libvirt's virDomainBlockResize API to do the
same within the instance.

When extending encrypted LUKSv1 volumes the new size of the volume
provided to the virDomainBlockResize API did not previously take the
space already consumed by the LUKSv1 header into account. As a result
any attempt to extend a host block device based encrypted LUKSv1 volume
would fail as Libvirt would need to grow the device to allow room for
the LUKSv1 header.

This change corrects this by inspecting the host block device or remote
RBD volume using ``qemu-img info``. Since oslo.utils 4.1.0 the returned
QemuImgInfo object that encapsulates the output of this call now
includes LUKSv1 specific data such as the offset of the encrypted
payload, essentially where the users data actually starts within the
volume.

By subtracting this offset from the new overall size of the extended
volume we can ensure calls to virDomainBlockResize will now succeed as
no attempt will be made by Libvirt to grow the device.

Future work will also need to handle this extend_volume use case for
``dm-crypt`` based encryption providers ``plain`` and ``luks2``.

Closes-Bug: #1861071
Depends-On: https://review.opendev.org/706880
Change-Id: I0c3f14100a18107f7e416293f3d4fcc641ce5e55
",git fetch https://review.opendev.org/openstack/nova refs/changes/00/706900/10 && git format-patch -1 --stdout FETCH_HEAD,['nova/virt/libvirt/driver.py'],1,ccf962c014bed3a19b64ee0ee21cb72772a714a9,bug/1861071," def _resize_attached_volume(self, new_size, block_device, instance): LOG.debug('Resizing target device %(dev)s to %(size)u kb', {'dev': block_device._disk, 'size': new_size}, instance=instance) block_device.resize(new_size // units.Ki) def _resize_attached_encrypted_volume(self, new_size, block_device, instance, connection_info, encryption): if self._use_native_luks(encryption): # NOTE(lyarwood): new_size currently refers to the total size of # the extended volume, with natively decrypted LUKSv1 volumes we # need to ensure this now takes the LUKSv1 header into account # before we ask QEMU to resize the device within the instance. payload_offset_kb = 0 if 'device_path' in connection_info['data']: info = disk_api.get_disk_info( connection_info['data']['device_path'], run_as_root=True) format_specific_data = info.format_specific['data'] payload_offset_kb = format_specific_data['payload-offset'] new_size = new_size - payload_offset_kb else: # NOTE(lyarwood): With plain encrypted volumes we need to first # call down to the encryptor to resize the dm-crypt device before # resizing the device within the instance via QEMU. encryptor = self._get_volume_encryptor(connection_info, encryption) encryptor.resize() self._resize_attached_volume(new_size, block_device, instance) volume_id = driver_block_device.get_volume_id(connection_info) encryption = encryptors.get_encryption_metadata( context, self._volume_api, volume_id, connection_info) if encryption: self._resize_attached_encrypted_volume( new_size, dev, instance, connection_info, encryption) else: self._resize_attached_volume( new_size, dev, instance)"," volume_id = driver_block_device.get_volume_id( connection_info) LOG.debug('resizing block device %(dev)s to %(size)u kb', {'dev': disk_path, 'size': new_size}) dev.resize(new_size // units.Ki)",41,6
openstack%2Ftripleo-ansible~master~I530e257f343ffc551db9e984f9a27b20c397bfb1,openstack/tripleo-ansible,master,I530e257f343ffc551db9e984f9a27b20c397bfb1,Filter out wrapper commands from the ps output,MERGED,2020-03-19 11:24:49.000000000,2020-03-20 13:46:49.000000000,2020-03-20 13:46:49.000000000,"[{'_account_id': 6681}, {'_account_id': 6926}, {'_account_id': 14287}, {'_account_id': 20172}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 11:24:49.000000000', 'files': ['tripleo_ansible/roles/tripleo_systemd_wrapper/templates/service_sync.j2'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/0df3ebd7592eac90f76650770f59c89fba4ebe39', 'message': ""Filter out wrapper commands from the ps output\n\nThis patch is filtering out the wrapper execution from the ps output\nin the sync script. By doing this, it'll effectively detect when\nthe target process is not running and start it. Otherwise, there might\nbe cases where the process start is postponed until next iteration\nof the sync script (1 minute) and it may be already too late.\n\nThis is causing tests to fail as the metadata service is not provisioned\nin time for instances to fetch their SSH keys.\n\nChange-Id: I530e257f343ffc551db9e984f9a27b20c397bfb1\nCo-Authored-By: Jakub Libosvar <jlibosva@redhat.com>\nCloses-Bug: #1868082\nSigned-off-by: Daniel Alvarez <dalvarez@redhat.com>\n""}]",1,713852,0df3ebd7592eac90f76650770f59c89fba4ebe39,17,7,1,23804,,,0,"Filter out wrapper commands from the ps output

This patch is filtering out the wrapper execution from the ps output
in the sync script. By doing this, it'll effectively detect when
the target process is not running and start it. Otherwise, there might
be cases where the process start is postponed until next iteration
of the sync script (1 minute) and it may be already too late.

This is causing tests to fail as the metadata service is not provisioned
in time for instances to fetch their SSH keys.

Change-Id: I530e257f343ffc551db9e984f9a27b20c397bfb1
Co-Authored-By: Jakub Libosvar <jlibosva@redhat.com>
Closes-Bug: #1868082
Signed-off-by: Daniel Alvarez <dalvarez@redhat.com>
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/52/713852/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_ansible/roles/tripleo_systemd_wrapper/templates/service_sync.j2'],1,0df3ebd7592eac90f76650770f59c89fba4ebe39,bug/1868082," if ! ps -e -o pid,command | grep ""$(echo $NETNS | sed 's|^[^-]*\-||')"" | egrep -v ""grep | netns exec"" &> /dev/null; then"," if ! ps -e -o pid,command | grep ""$(echo $NETNS | sed 's|^[^-]*\-||')"" | grep -v grep &> /dev/null; then",1,1
openstack%2Fvalidations-libs~master~I1b82f3ef24b80d55f96bd76c17278b988e4b611d,openstack/validations-libs,master,I1b82f3ef24b80d55f96bd76c17278b988e4b611d,Move validation data as object,MERGED,2020-03-18 20:04:16.000000000,2020-03-20 13:41:06.000000000,2020-03-20 13:41:04.000000000,"[{'_account_id': 11491}, {'_account_id': 16515}, {'_account_id': 22348}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-03-18 20:04:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/8ae0cb550b7d77cc2d79de4b1ab254a7b5e0e570', 'message': 'Move validation data as object\n\nChange-Id: I1b82f3ef24b80d55f96bd76c17278b988e4b611d\n'}, {'number': 2, 'created': '2020-03-18 20:38:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/4ee240be762c7ffac82227b901e7dec4175bebce', 'message': 'Move validation data as object\n\nChange-Id: I1b82f3ef24b80d55f96bd76c17278b988e4b611d\n'}, {'number': 3, 'created': '2020-03-19 16:19:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/18310c58719ff511bcf9cfd20265aa2afcd137c7', 'message': 'Move validation data as object\n\nChange-Id: I1b82f3ef24b80d55f96bd76c17278b988e4b611d\n'}, {'number': 4, 'created': '2020-03-19 16:38:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/77ffbfc18ae09920191c274014c603e65d184af4', 'message': 'Move validation data as object\n\nChange-Id: I1b82f3ef24b80d55f96bd76c17278b988e4b611d\n'}, {'number': 5, 'created': '2020-03-19 16:51:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/47aac0b5c44d71e8fbeca4b763d49cd5721a38f6', 'message': 'Move validation data as object\n\nChange-Id: I1b82f3ef24b80d55f96bd76c17278b988e4b611d\n'}, {'number': 6, 'created': '2020-03-20 08:50:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/44bb1309b7d5f8b6c47d4b6467bd6e5c10aeb20c', 'message': 'Move validation data as object\n\nChange-Id: I1b82f3ef24b80d55f96bd76c17278b988e4b611d\n'}, {'number': 7, 'created': '2020-03-20 09:11:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/3aa8684e1626b49cd79b860c8496aaf5e0c572ec', 'message': 'Move validation data as object\n\nDepends-On: I11f0e43023c8348c761a266f6396590c3bf44fc2\nChange-Id: I1b82f3ef24b80d55f96bd76c17278b988e4b611d\n'}, {'number': 8, 'created': '2020-03-20 13:21:18.000000000', 'files': ['validations_libs/tests/test_utils.py', 'validations_libs/tests/test_validations_show.py', 'validations_libs/show.py', 'validations_libs/tests/fakes.py', 'validations_libs/utils.py', 'validations_libs/validation.py'], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/95edef7a8c8f4fd85c51068db2e260c7343b323a', 'message': 'Move validation data as object\n\nChange-Id: I1b82f3ef24b80d55f96bd76c17278b988e4b611d\n'}]",0,713730,95edef7a8c8f4fd85c51068db2e260c7343b323a,32,4,8,16515,,,0,"Move validation data as object

Change-Id: I1b82f3ef24b80d55f96bd76c17278b988e4b611d
",git fetch https://review.opendev.org/openstack/validations-libs refs/changes/30/713730/6 && git format-patch -1 --stdout FETCH_HEAD,"['validations_libs/tests/test_utils.py', 'validations_libs/tests/test_validations_show.py', 'validations_libs/show.py', 'validations_libs/utils.py', 'validations_libs/validation.py']",5,8ae0cb550b7d77cc2d79de4b1ab254a7b5e0e570,refactor/utils,"# Copyright 2020 Red Hat, Inc. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # import logging import os import yaml from collections import OrderedDict LOG = logging.getLogger(__name__ + "".validation"") class Validation(object): _col_keys = ['ID', 'Name', 'Description', 'Groups'] def __init__(self, validation_path): self.dict = self._get_content(validation_path) self.id = os.path.splitext(os.path.basename(validation_path))[0] def _get_content(self, val_path): with open(val_path, 'r') as val_playbook: return yaml.safe_load(val_playbook)[0] @property def get_metadata(self): self.metadata = {'id': self.id} self.metadata.update(self.dict['vars']['metadata']) return self.metadata @property def get_vars(self): vars = self.dict['vars'].copy() if vars.get('metadata'): vars.pop('metadata') return vars @property def get_data(self): return self.dict @property def groups(self): return self.dict['vars']['metadata']['groups'] @property def get_id(self): return self.id @property def get_ordered_dict(self): data = OrderedDict() data.update(self.dict) return data @property def get_formated_data(self): data = {} for key in self.get_metadata.keys(): if key in map(str.lower, self._col_keys): for k in self._col_keys: if key == k.lower(): output_key = k data[output_key] = self.get_metadata.get(key) else: # Get all other values: data[key] = self.get_metadata.get(key) return data ",,102,62
openstack%2Fpuppet-tripleo~master~I9b04c1bb5a2389201b99815f17d6242baaf856d9,openstack/puppet-tripleo,master,I9b04c1bb5a2389201b99815f17d6242baaf856d9,pacemaker-remote is broken on CentOS/RHEL < 8,ABANDONED,2020-03-18 16:11:38.000000000,2020-03-20 13:24:15.000000000,,"[{'_account_id': 17823}, {'_account_id': 20172}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 30598}]","[{'number': 1, 'created': '2020-03-18 16:11:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/f04961a446441fa9ef969b9d6b25c607fbcc1f22', 'message': ""pacemaker-remote is broken on CentOS/RHEL < 8\n\nhttps://opendev.org/openstack/puppet-tripleo/src/commit/16c5f1692580d7fc42985d8895828eb71433af09/manifests/profile/base/pacemaker_remote.pp#L63\nBy setting use_pcsd to true, the new pcs 0.10 remote addition is always used,\nwhich doesn't work if not running CentOS/RHEL 8.\n\nChange-Id: I9b04c1bb5a2389201b99815f17d6242baaf856d9\nCloses-bug: #1850058\n""}, {'number': 2, 'created': '2020-03-18 16:15:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/b974d35dd1d6d347126a5a415a86fe25c49e6297', 'message': ""pacemaker-remote is broken on CentOS/RHEL < 8\n\nBroken since commit 16c5f1692580d7fc42985d8895828eb71433af09\nThe problem is in manifests/profile/base/pacemaker_remote.pp#L63\nBy setting use_pcsd to true, the new pcs 0.10 remote addition is\nalways used, which doesn't work if not running CentOS/RHEL 8.\n\nChange-Id: I9b04c1bb5a2389201b99815f17d6242baaf856d9\nCloses-bug: #1850058\n""}, {'number': 3, 'created': '2020-03-18 17:37:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/266a82741463a9773814f7362ce45f55a922c765', 'message': ""pacemaker-remote is broken on CentOS/RHEL < 8\n\nBroken since commit 16c5f1692580d7fc42985d8895828eb71433af09\nThe problem is in manifests/profile/base/pacemaker_remote.pp#L63\nBy setting use_pcsd to true, the new pcs 0.10 remote addition is\nalways used, which doesn't work if not running CentOS/RHEL 8.\n\nChange-Id: I9b04c1bb5a2389201b99815f17d6242baaf856d9\nCloses-bug: #1850058\n""}, {'number': 4, 'created': '2020-03-19 12:59:55.000000000', 'files': ['manifests/profile/base/pacemaker_remote.pp'], 'web_link': 'https://opendev.org/openstack/puppet-tripleo/commit/b15e4f431c055a61c055c4cf57822a0dadef04fa', 'message': ""pacemaker-remote is broken on CentOS/RHEL < 8\n\nBroken since commit 16c5f1692580d7fc42985d8895828eb71433af09\nThe problem is in manifests/profile/base/pacemaker_remote.pp#L63\nBy setting use_pcsd to true, the new pcs 0.10 remote addition is\nalways used, which doesn't work if not running CentOS/RHEL 8.\n\nChange-Id: I9b04c1bb5a2389201b99815f17d6242baaf856d9\nCloses-bug: #1850058\n""}]",1,713689,b15e4f431c055a61c055c4cf57822a0dadef04fa,23,5,4,30598,,,0,"pacemaker-remote is broken on CentOS/RHEL < 8

Broken since commit 16c5f1692580d7fc42985d8895828eb71433af09
The problem is in manifests/profile/base/pacemaker_remote.pp#L63
By setting use_pcsd to true, the new pcs 0.10 remote addition is
always used, which doesn't work if not running CentOS/RHEL 8.

Change-Id: I9b04c1bb5a2389201b99815f17d6242baaf856d9
Closes-bug: #1850058
",git fetch https://review.opendev.org/openstack/puppet-tripleo refs/changes/89/713689/4 && git format-patch -1 --stdout FETCH_HEAD,['manifests/profile/base/pacemaker_remote.pp'],1,f04961a446441fa9ef969b9d6b25c607fbcc1f22,bug/1850058," use_pcsd => $::pacemaker::pcs_010,"," use_pcsd => true,",1,1
openstack%2Frequirements~master~I11f0e43023c8348c761a266f6396590c3bf44fc2,openstack/requirements,master,I11f0e43023c8348c761a266f6396590c3bf44fc2,Bump virtualenv===20.0.12,ABANDONED,2020-03-19 18:53:15.000000000,2020-03-20 13:15:04.000000000,,"[{'_account_id': 9592}, {'_account_id': 10022}, {'_account_id': 11491}, {'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 16515}, {'_account_id': 22348}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-03-19 18:53:15.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/31d7723b88d5bd467f2390acc97271c58214c8a8', 'message': 'Bump virtualenv===20.0.12\n\nThis should avoid https://github.com/pypa/virtualenv/issues/1740\n\nChange-Id: I11f0e43023c8348c761a266f6396590c3bf44fc2\n'}]",0,713963,31d7723b88d5bd467f2390acc97271c58214c8a8,6,8,1,24162,,,0,"Bump virtualenv===20.0.12

This should avoid https://github.com/pypa/virtualenv/issues/1740

Change-Id: I11f0e43023c8348c761a266f6396590c3bf44fc2
",git fetch https://review.opendev.org/openstack/requirements refs/changes/63/713963/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,31d7723b88d5bd467f2390acc97271c58214c8a8,,virtualenv===20.0.12,virtualenv===20.0.10,1,1
openstack%2Fcinder~master~I5854a475efeced9941a4ab3cea8dd6f9a303ae37,openstack/cinder,master,I5854a475efeced9941a4ab3cea8dd6f9a303ae37,Skip common tests on translation files,ABANDONED,2020-03-19 12:12:20.000000000,2020-03-20 12:57:33.000000000,,"[{'_account_id': 1736}, {'_account_id': 6547}, {'_account_id': 10118}, {'_account_id': 11904}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 14384}, {'_account_id': 15386}, {'_account_id': 15670}, {'_account_id': 15941}, {'_account_id': 18120}, {'_account_id': 18883}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23613}, {'_account_id': 24921}, {'_account_id': 25243}, {'_account_id': 25678}, {'_account_id': 26077}, {'_account_id': 26537}, {'_account_id': 29705}, {'_account_id': 29716}]","[{'number': 1, 'created': '2020-03-19 12:12:20.000000000', 'files': ['.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/cinder/commit/71463f87eae0c130caf7989a84c8e3b4e0eb5019', 'message': 'Skip common tests on translation files\n\nChange-Id: I5854a475efeced9941a4ab3cea8dd6f9a303ae37\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}]",3,713858,71463f87eae0c130caf7989a84c8e3b4e0eb5019,32,24,1,11904,,,0,"Skip common tests on translation files

Change-Id: I5854a475efeced9941a4ab3cea8dd6f9a303ae37
Signed-off-by: Sean McGinnis <sean.mcginnis@gmail.com>
",git fetch https://review.opendev.org/openstack/cinder refs/changes/58/713858/1 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.yaml'],1,71463f87eae0c130caf7989a84c8e3b4e0eb5019,translations, - ^.*\.po$ - ^.*\.po$,,2,0
openstack%2Fapi-sig~master~I88380fe647864e9a7108d5dd76bdf501f289b229,openstack/api-sig,master,I88380fe647864e9a7108d5dd76bdf501f289b229,Verify JSON files in the CI,MERGED,2020-03-19 10:50:42.000000000,2020-03-20 12:47:06.000000000,2020-03-20 12:45:20.000000000,"[{'_account_id': 10670}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 10:50:42.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/api-sig/commit/393406bbef789dc98a46c29ccbf19cbb6713a780', 'message': 'Verify JSON files in the CI\n\nChange-Id: I88380fe647864e9a7108d5dd76bdf501f289b229\n'}]",0,713832,393406bbef789dc98a46c29ccbf19cbb6713a780,7,2,1,10239,,,0,"Verify JSON files in the CI

Change-Id: I88380fe647864e9a7108d5dd76bdf501f289b229
",git fetch https://review.opendev.org/openstack/api-sig refs/changes/32/713832/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,393406bbef789dc98a46c29ccbf19cbb6713a780,gate,"whitelist_externals = bash # Check the JSON files bash -c 'for f in guidelines/*.json; do echo Checking ""$f""; python -m json.tool ""$f"" /dev/null || exit 1; done'",,3,0
openstack%2Fvalidations-common~master~I2832475a131fae6f7fb95158bacdd9f615ca891f,openstack/validations-common,master,I2832475a131fae6f7fb95158bacdd9f615ca891f,Add reportentry.py custom module,MERGED,2020-03-17 15:12:02.000000000,2020-03-20 12:37:24.000000000,2020-03-20 12:37:24.000000000,"[{'_account_id': 11491}, {'_account_id': 16515}, {'_account_id': 22348}, {'_account_id': 28223}]","[{'number': 1, 'created': '2020-03-17 15:12:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/validations-common/commit/bc3a85011ad28ae8ecbe61d7e1d3106dcb22a50d', 'message': 'Add reportentry.py custom module\n\nChange-Id: I2832475a131fae6f7fb95158bacdd9f615ca891f\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n'}, {'number': 2, 'created': '2020-03-18 14:04:21.000000000', 'files': ['validations_common/tests/library/test_reportentry.py', 'validations_common/library/reportentry.py'], 'web_link': 'https://opendev.org/openstack/validations-common/commit/f2b62f98f2216dfc94923c46b7a385d3a4829589', 'message': 'Add reportentry.py custom module\n\nChange-Id: I2832475a131fae6f7fb95158bacdd9f615ca891f\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n'}]",0,713466,f2b62f98f2216dfc94923c46b7a385d3a4829589,11,4,2,11491,,,0,"Add reportentry.py custom module

Change-Id: I2832475a131fae6f7fb95158bacdd9f615ca891f
Signed-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>
",git fetch https://review.opendev.org/openstack/validations-common refs/changes/66/713466/1 && git format-patch -1 --stdout FETCH_HEAD,"['validations_common/tests/library/test_reportentry.py', 'validations_common/library/reportentry.py']",2,bc3a85011ad28ae8ecbe61d7e1d3106dcb22a50d,,"# -*- coding: utf-8 -*- # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. from ansible.module_utils.basic import AnsibleModule from yaml import safe_load as yaml_safe_load DOCUMENTATION = ''' --- module: reportentry short_description: Print a custom report description: - Print a custom report options: report_status: required: true description: - The report status. Should be 'OK', 'ERROR' or 'SKIPPED'. choices: - 'OK' - 'ERROR' - 'SKIPPED' type: str report_reason: required: true description: - The reason of the report type: str report_recommendations: required: true description: - A list of recommendations to do. type: list author: ""Gael Chamoulaud"" ''' EXAMPLES = ''' - hosts: undercloud tasks: - name: Report DNS setup in undercloud.conf reportentry: report_status: ""ERROR"" report_reason: ""DNS is not setup correctly in undercloud.conf"" report_recommendations: - ""Please set the 'undercloud_nameservers' param in undercloud.conf"" ''' def format_msg_report(status, reason, recommendations): msg = (""[{}] '{}'\n"".format(status, reason)) if recommendations: for rec in recommendations: msg += "" - RECOMMENDATION: {}\n"".format(rec) return msg def main(): module = AnsibleModule( argument_spec=yaml_safe_load(DOCUMENTATION)['options'] ) status = module.params.get('report_status') msg = format_msg_report(module.params.get('report_status'), module.params.get('report_reason'), module.params.get('report_recommendations')) if status == 'ERROR': module.fail_json(msg=msg) elif status == ""SKIPPED"": module.exit_json(changed=False, warnings=msg) else: module.exit_json(changed=False, msg=msg) if __name__ == '__main__': main() ",,148,0
openstack%2Ftripleo-operator-ansible~master~I945a0964d5ff333936aea346cf3c05f031fbf66c,openstack/tripleo-operator-ansible,master,I945a0964d5ff333936aea346cf3c05f031fbf66c,Fix wrong metadata,MERGED,2020-03-17 20:11:43.000000000,2020-03-20 11:59:33.000000000,2020-03-20 11:59:33.000000000,"[{'_account_id': 3153}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 20:11:43.000000000', 'files': ['roles/tripleo_overcloud_update_run/tests/test.yml', 'roles/tripleo_overcloud_update_run/meta/main.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-operator-ansible/commit/41553ffbea68d441219900cc27c8c741510263fa', 'message': 'Fix wrong metadata\n\nForgot to update these files when I copied the prepare role for the run.\n\nChange-Id: I945a0964d5ff333936aea346cf3c05f031fbf66c\n'}]",0,713526,41553ffbea68d441219900cc27c8c741510263fa,10,2,1,14985,,,0,"Fix wrong metadata

Forgot to update these files when I copied the prepare role for the run.

Change-Id: I945a0964d5ff333936aea346cf3c05f031fbf66c
",git fetch https://review.opendev.org/openstack/tripleo-operator-ansible refs/changes/26/713526/1 && git format-patch -1 --stdout FETCH_HEAD,"['roles/tripleo_overcloud_update_run/tests/test.yml', 'roles/tripleo_overcloud_update_run/meta/main.yml']",2,41553ffbea68d441219900cc27c8c741510263fa,fix-typos, description: TripleO Operator Role -- tripleo_overcloud_update_run, description: TripleO Operator Role -- tripleo_overcloud_update_prepare,2,2
openstack%2Fneutron~stable%2Fpike~I6798f3c634865de9431737ceabe62aaece8413d8,openstack/neutron,stable/pike,I6798f3c634865de9431737ceabe62aaece8413d8,Wrong status change for port when restart neutron-linuxbridge-agent Pike,ABANDONED,2020-03-19 03:44:51.000000000,2020-03-20 11:58:46.000000000,,"[{'_account_id': 9732}, {'_account_id': 12898}, {'_account_id': 22348}, {'_account_id': 26622}]","[{'number': 1, 'created': '2020-03-19 03:44:51.000000000', 'files': ['neutron/plugins/ml2/rpc.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/13d0ebe158c0e99febe034614ebd314a9124373f', 'message': 'Wrong status change for port when restart neutron-linuxbridge-agent\nPike\n\nthe port need not change when restart neutron-linuxbridge-agent on\ncompute nodes, the judgment of host and conext host for ports was\nwrong here, this will trigger all compute node to refrash firewall\nand make the rabbitmq and neutron-server service will be High CPU\nload. Openstack version is Pike.\n\nChange-Id: I6798f3c634865de9431737ceabe62aaece8413d8\nRelated-Bug: #1866743\n'}]",0,713774,13d0ebe158c0e99febe034614ebd314a9124373f,7,4,1,19714,,,0,"Wrong status change for port when restart neutron-linuxbridge-agent
Pike

the port need not change when restart neutron-linuxbridge-agent on
compute nodes, the judgment of host and conext host for ports was
wrong here, this will trigger all compute node to refrash firewall
and make the rabbitmq and neutron-server service will be High CPU
load. Openstack version is Pike.

Change-Id: I6798f3c634865de9431737ceabe62aaece8413d8
Related-Bug: #1866743
",git fetch https://review.opendev.org/openstack/neutron refs/changes/74/713774/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/plugins/ml2/rpc.py'],1,13d0ebe158c0e99febe034614ebd314a9124373f,pike_Bug1866743, if not host or host != port_context.host:, if not host or host == port_context.host:,1,1
openstack%2Fblazar-specs~master~I759fcb2d65b6cc7132486088e1e3f1388a9a2e6d,openstack/blazar-specs,master,I759fcb2d65b6cc7132486088e1e3f1388a9a2e6d,Small Cleanups,MERGED,2020-03-19 13:42:20.000000000,2020-03-20 11:41:22.000000000,2020-03-20 11:25:44.000000000,"[{'_account_id': 1894}, {'_account_id': 3012}, {'_account_id': 6547}, {'_account_id': 7075}, {'_account_id': 7166}, {'_account_id': 7535}, {'_account_id': 8878}, {'_account_id': 9331}, {'_account_id': 13192}, {'_account_id': 15197}, {'_account_id': 22348}, {'_account_id': 23840}, {'_account_id': 25625}]","[{'number': 1, 'created': '2020-03-19 13:42:20.000000000', 'files': ['doc/source/conf.py', 'doc/requirements.txt', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/blazar-specs/commit/8120c0a6a37d5e7d140cbd0669c2163f1ca0f2fa', 'message': 'Small Cleanups\n\nRemove install_command, the default is just fine.\n\nUpdate openstackdocstheme version, remove obsolete setting of\nhtml_last_updated_fmt.\n\nUpdate pbr version.\n\nChange-Id: I759fcb2d65b6cc7132486088e1e3f1388a9a2e6d\n'}]",0,713884,8120c0a6a37d5e7d140cbd0669c2163f1ca0f2fa,11,13,1,6547,,,0,"Small Cleanups

Remove install_command, the default is just fine.

Update openstackdocstheme version, remove obsolete setting of
html_last_updated_fmt.

Update pbr version.

Change-Id: I759fcb2d65b6cc7132486088e1e3f1388a9a2e6d
",git fetch https://review.opendev.org/openstack/blazar-specs refs/changes/84/713884/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/conf.py', 'doc/requirements.txt', 'tox.ini']",3,8120c0a6a37d5e7d140cbd0669c2163f1ca0f2fa,cleanup,,install_command = pip install -U {opts} {packages},2,3
openstack%2Fnova~stable%2Fstein~Icd7ab2ca4ddbed92c7e883a63a23245920d961e7,openstack/nova,stable/stein,Icd7ab2ca4ddbed92c7e883a63a23245920d961e7,nova-live-migration: Wait for n-cpu services to come up after configuring Ceph,MERGED,2020-03-19 11:04:11.000000000,2020-03-20 11:23:52.000000000,2020-03-20 11:20:32.000000000,"[{'_account_id': 4690}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-03-19 11:04:11.000000000', 'files': ['nova/tests/live_migration/hooks/ceph.sh'], 'web_link': 'https://opendev.org/openstack/nova/commit/373c4ffde2053c7ff11bd38339b88d144cd442f2', 'message': 'nova-live-migration: Wait for n-cpu services to come up after configuring Ceph\n\nPreviously the ceph.sh script used during the nova-live-migration job\nwould only grep for a `compute` process when checking if the services\nhad been restarted. This check was bogus and would always return 0 as it\nwould always match itself. For example:\n\n2020-03-13 21:06:47.682073 | primary | 2020-03-13 21:06:47.681 | root\n29529  0.0  0.0   4500   736 pts/0    S+   21:06   0:00 /bin/sh -c ps\n       aux | grep compute\n2020-03-13 21:06:47.683964 | primary | 2020-03-13 21:06:47.683 | root\n29531  0.0  0.0  14616   944 pts/0    S+   21:06   0:00 grep compute\n\nFailures of this job were seen on the stable/pike branch where slower CI\nnodes appeared to struggle to allow Libvirt to report to n-cpu in time\nbefore Tempest was started. This in-turn caused instance build failures\nand the overall failure of the job.\n\nThis change resolves this issue by switching to pgrep and ensuring\nn-cpu services are reported as fully up after a cold restart before\nstarting the Tempest test run.\n\nCloses-Bug: 1867380\nChange-Id: Icd7ab2ca4ddbed92c7e883a63a23245920d961e7\n(cherry picked from commit e23c3c2c8df3843c5853c87ef684bd21c4af95d8)\n(cherry picked from commit 70447bca2f4f33c6872eaf94a2e4351bb257c22a)\n'}]",0,713837,373c4ffde2053c7ff11bd38339b88d144cd442f2,11,5,1,10135,,,0,"nova-live-migration: Wait for n-cpu services to come up after configuring Ceph

Previously the ceph.sh script used during the nova-live-migration job
would only grep for a `compute` process when checking if the services
had been restarted. This check was bogus and would always return 0 as it
would always match itself. For example:

2020-03-13 21:06:47.682073 | primary | 2020-03-13 21:06:47.681 | root
29529  0.0  0.0   4500   736 pts/0    S+   21:06   0:00 /bin/sh -c ps
       aux | grep compute
2020-03-13 21:06:47.683964 | primary | 2020-03-13 21:06:47.683 | root
29531  0.0  0.0  14616   944 pts/0    S+   21:06   0:00 grep compute

Failures of this job were seen on the stable/pike branch where slower CI
nodes appeared to struggle to allow Libvirt to report to n-cpu in time
before Tempest was started. This in-turn caused instance build failures
and the overall failure of the job.

This change resolves this issue by switching to pgrep and ensuring
n-cpu services are reported as fully up after a cold restart before
starting the Tempest test run.

Closes-Bug: 1867380
Change-Id: Icd7ab2ca4ddbed92c7e883a63a23245920d961e7
(cherry picked from commit e23c3c2c8df3843c5853c87ef684bd21c4af95d8)
(cherry picked from commit 70447bca2f4f33c6872eaf94a2e4351bb257c22a)
",git fetch https://review.opendev.org/openstack/nova refs/changes/37/713837/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/live_migration/hooks/ceph.sh'],1,373c4ffde2053c7ff11bd38339b88d144cd442f2,bug/1867380,"function _wait_for_nova_compute_service_state { source $BASE/new/devstack/openrc admin admin local status=$1 local attempt=1 local max_attempts=24 local attempt_sleep=5 local computes_count=$(openstack compute service list | grep -c nova-compute) local computes_ready=$(openstack compute service list | grep nova-compute | grep $status | wc -l) echo ""Waiting for $computes_count computes to report as $status"" while [ ""$computes_ready"" -ne ""$computes_count"" ]; do if [ ""$attempt"" -eq ""$max_attempts"" ]; then echo ""Failed waiting for computes to report as ${status}, ${computes_ready}/${computes_count} ${status} after ${max_attempts} attempts"" exit 4 fi echo ""Waiting ${attempt_sleep} seconds for ${computes_count} computes to report as ${status}, ${computes_ready}/${computes_count} ${status} after ${attempt}/${max_attempts} attempts"" sleep $attempt_sleep attempt=$((attempt+1)) computes_ready=$(openstack compute service list | grep nova-compute | grep $status | wc -l) done echo ""All computes are now reporting as ${status} after ${attempt} attempts"" } function configure_and_start_nova { echo ""Checking all n-cpu services"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""pgrep -u stack -a nova-compute"" # stop nova-compute echo ""Stopping all n-cpu services"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""systemctl stop devstack@n-cpu"" # Wait for the service to be marked as down _wait_for_nova_compute_service_state ""down"" # start nova-compute echo ""Starting all n-cpu services"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""systemctl start devstack@n-cpu"" echo ""Checking all n-cpu services"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""pgrep -u stack -a nova-compute"" # Wait for the service to be marked as up _wait_for_nova_compute_service_state ""up""","function configure_and_start_nova { echo 'check compute processes before restart' $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""ps aux | grep compute"" # restart nova-compute $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""systemctl restart devstack@n-cpu"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""ps aux | grep compute"" ",42,5
openstack%2Fkuryr-kubernetes~master~Iad40167d28078b2d6811d3afed58b8da4b41cd42,openstack/kuryr-kubernetes,master,Iad40167d28078b2d6811d3afed58b8da4b41cd42,Fix IPv6 enabled devstack and namespace subnet plugin.,MERGED,2020-03-13 13:06:04.000000000,2020-03-20 11:21:29.000000000,2020-03-20 11:19:11.000000000,"[{'_account_id': 11600}, {'_account_id': 13692}, {'_account_id': 14352}, {'_account_id': 22348}, {'_account_id': 23567}, {'_account_id': 27032}]","[{'number': 1, 'created': '2020-03-13 13:06:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr-kubernetes/commit/b04bfe06bb6e627f54c9e9f0e1f67475951e47d8', 'message': ""Fix IPv6 enabled devstack and namespace subnet plugin.\n\nIn case of setting subnet driver to 'namespace', and making use of IPv6\nin kuryr devstack‚ we experienced a clash with devstack route, since we\nused to use shared subnet pool crated by devstack.\n\nTo avoid such clash, for IPv6 we simply create our own IPv6 shared\nsubnet pool, and subnets within it.\n\nChange-Id: Iad40167d28078b2d6811d3afed58b8da4b41cd42\n""}, {'number': 2, 'created': '2020-03-13 19:08:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr-kubernetes/commit/cb56a7034f5705d275c77cf0098aa49909d9af72', 'message': ""Fix IPv6 enabled devstack and namespace subnet plugin.\n\nIn case of setting subnet driver to 'namespace', and making use of IPv6\nin kuryr devstack‚ we experienced a clash with devstack route, since we\nused to use shared subnet pool crated by devstack.\n\nTo avoid such clash, for IPv6 we simply create our own IPv6 shared\nsubnet pool, and subnets within it.\n\nChange-Id: Iad40167d28078b2d6811d3afed58b8da4b41cd42\n""}, {'number': 3, 'created': '2020-03-19 12:04:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kuryr-kubernetes/commit/5bda6b45d1d450364ee8cdf30eee7bc6db3989e0', 'message': ""Fix IPv6 enabled devstack and namespace subnet plugin.\n\nIn case of setting subnet driver to 'namespace', and making use of IPv6\nin kuryr devstack‚ we experienced a clash with devstack route, since we\nused to use shared subnet pool crated by devstack.\n\nTo avoid such clash, for IPv6 we simply create our own IPv6 shared\nsubnet pool, and subnets within it.\n\nChange-Id: Iad40167d28078b2d6811d3afed58b8da4b41cd42\n""}, {'number': 4, 'created': '2020-03-19 12:06:30.000000000', 'files': ['devstack/plugin.sh', 'devstack/lib/kuryr_kubernetes', 'devstack/settings'], 'web_link': 'https://opendev.org/openstack/kuryr-kubernetes/commit/ec88d2aabf53bd21cf111b05824c4cf7c54c66ab', 'message': ""Fix IPv6 enabled devstack and namespace subnet plugin.\n\nIn case of setting subnet driver to 'namespace', and making use of IPv6\nin kuryr devstack‚ we experienced a clash with devstack route, since we\nused to use shared subnet pool crated by devstack.\n\nTo avoid such clash, for IPv6 we simply create our own IPv6 shared\nsubnet pool, and subnets within it.\n\nChange-Id: Iad40167d28078b2d6811d3afed58b8da4b41cd42\n""}]",8,712941,ec88d2aabf53bd21cf111b05824c4cf7c54c66ab,24,6,4,13692,,,0,"Fix IPv6 enabled devstack and namespace subnet plugin.

In case of setting subnet driver to 'namespace', and making use of IPv6
in kuryr devstack‚ we experienced a clash with devstack route, since we
used to use shared subnet pool crated by devstack.

To avoid such clash, for IPv6 we simply create our own IPv6 shared
subnet pool, and subnets within it.

Change-Id: Iad40167d28078b2d6811d3afed58b8da4b41cd42
",git fetch https://review.opendev.org/openstack/kuryr-kubernetes refs/changes/41/712941/4 && git format-patch -1 --stdout FETCH_HEAD,"['devstack/plugin.sh', 'devstack/lib/kuryr_kubernetes']",2,b04bfe06bb6e627f54c9e9f0e1f67475951e47d8,devstack_ipv6, subnetpool_id=${KURYR_NEUTRON_DEFAULT_SUBNETPOOL_ID:-${SUBNETPOOL_KURYR_V6_ID}}, subnetpool_id=${KURYR_NEUTRON_DEFAULT_SUBNETPOOL_ID:-${SUBNETPOOL_V6_ID}},23,7
openstack%2Fneutron~master~Iaaab299298c6528ea56e4b212674f492dfe517b7,openstack/neutron,master,Iaaab299298c6528ea56e4b212674f492dfe517b7,"Use ""datetime.datetime.isoformat"" instead of ""timeutils.isotime""",MERGED,2020-03-13 15:46:22.000000000,2020-03-20 11:18:07.000000000,2020-03-20 11:14:55.000000000,"[{'_account_id': 6773}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 22348}, {'_account_id': 26622}]","[{'number': 1, 'created': '2020-03-13 15:46:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/51452ecce21708602d4af1d06537b06c229e6e9e', 'message': 'Use ""datetime.datetime.isoformat"" instead of ""timeutils.isotime""\n\nChange-Id: Iaaab299298c6528ea56e4b212674f492dfe517b7\n'}, {'number': 2, 'created': '2020-03-14 18:29:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/19de26a3ff9a7b2b9833f5fad2e58a7aae2eec74', 'message': 'Use ""datetime.datetime.isoformat"" instead of ""timeutils.isotime""\n\nChange-Id: Iaaab299298c6528ea56e4b212674f492dfe517b7\n'}, {'number': 3, 'created': '2020-03-19 09:35:59.000000000', 'files': ['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/e1804e6f5f6b12c93cfa79d933c3de6cf71378e4', 'message': 'Use ""datetime.datetime.isoformat"" instead of ""timeutils.isotime""\n\noslo-utils ""isotime"" is deprecated [1]. ""datetime.datetime.isoformat""\nshould be used instead.\n\n[1]https://github.com/openstack/oslo.utils/blob/382370781bef900fae16687cc83d5e72d6769193/oslo_utils/timeutils.py#L45-L49\n\nChange-Id: Iaaab299298c6528ea56e4b212674f492dfe517b7\n'}]",2,712986,e1804e6f5f6b12c93cfa79d933c3de6cf71378e4,20,6,3,16688,,,0,"Use ""datetime.datetime.isoformat"" instead of ""timeutils.isotime""

oslo-utils ""isotime"" is deprecated [1]. ""datetime.datetime.isoformat""
should be used instead.

[1]https://github.com/openstack/oslo.utils/blob/382370781bef900fae16687cc83d5e72d6769193/oslo_utils/timeutils.py#L45-L49

Change-Id: Iaaab299298c6528ea56e4b212674f492dfe517b7
",git fetch https://review.opendev.org/openstack/neutron refs/changes/86/712986/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py'],1,51452ecce21708602d4af1d06537b06c229e6e9e,datetime.datetime.isoformat, datetime.datetime.isoformat(updated_at)} datetime.datetime.isoformat(updated_at)}), timeutils.isotime(updated_at)} timeutils.isotime(updated_at)}),2,2
openstack%2Fpython-openstackclient~master~I6cf70a590a7afe8a18d6d31c7cf6f9f71cdd57b6,openstack/python-openstackclient,master,I6cf70a590a7afe8a18d6d31c7cf6f9f71cdd57b6,FIX for openstack quota set --force,ABANDONED,2020-03-20 09:46:51.000000000,2020-03-20 11:11:23.000000000,,[{'_account_id': 15334}],"[{'number': 1, 'created': '2020-03-20 09:46:51.000000000', 'files': ['openstackclient/common/quota.py'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/e059eef09ebff2304491dc50e745d23db429ffcc', 'message': 'FIX for openstack quota set --force\n\nChange-Id: I6cf70a590a7afe8a18d6d31c7cf6f9f71cdd57b6\n'}]",0,714062,e059eef09ebff2304491dc50e745d23db429ffcc,3,1,1,31733,,,0,"FIX for openstack quota set --force

Change-Id: I6cf70a590a7afe8a18d6d31c7cf6f9f71cdd57b6
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/62/714062/1 && git format-patch -1 --stdout FETCH_HEAD,['openstackclient/common/quota.py'],1,e059eef09ebff2304491dc50e745d23db429ffcc,bug/2007440," '--force', action='store_true', help=_('Force quota update'), compute_kwargs['force'] = True if parsed_args.force: network_kwargs['force'] = True if parsed_args.force: network_kwargs['force'] = True"," '--force', action='store_true', help=_('Force quota update'), compute_kwargs['force'] = True",8,4
openstack%2Fzun~master~I03a3208b031c49070749359f32d22a06c95dcb57,openstack/zun,master,I03a3208b031c49070749359f32d22a06c95dcb57,Specify network for capsules,NEW,2020-02-13 07:59:32.000000000,2020-03-20 10:56:38.000000000,,"[{'_account_id': 11536}, {'_account_id': 22348}, {'_account_id': 23365}]","[{'number': 1, 'created': '2020-02-13 07:59:32.000000000', 'files': ['zun/api/controllers/v1/capsules.py', 'zun/api/controllers/v1/schemas/parameter_types.py'], 'web_link': 'https://opendev.org/openstack/zun/commit/89829a92cc49bbca0be2be60d8fc8b73851bdf06', 'message': 'Specify network for capsules\n\nChange-Id: I03a3208b031c49070749359f32d22a06c95dcb57\n'}]",0,707578,89829a92cc49bbca0be2be60d8fc8b73851bdf06,7,3,1,23365,,,0,"Specify network for capsules

Change-Id: I03a3208b031c49070749359f32d22a06c95dcb57
",git fetch https://review.opendev.org/openstack/zun refs/changes/78/707578/1 && git format-patch -1 --stdout FETCH_HEAD,"['zun/api/controllers/v1/capsules.py', 'zun/api/controllers/v1/schemas/parameter_types.py']",2,89829a92cc49bbca0be2be60d8fc8b73851bdf06,," ""nets"": nets,",,2,1
openstack%2Fkolla~stable%2Frocky~I29028976acb884e799a035fcb76cb998e40e1171,openstack/kolla,stable/rocky,I29028976acb884e799a035fcb76cb998e40e1171,Bump glance for Rocky,MERGED,2020-03-19 17:54:35.000000000,2020-03-20 10:50:15.000000000,2020-03-20 10:50:15.000000000,"[{'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-19 17:54:35.000000000', 'files': ['kolla/common/config.py'], 'web_link': 'https://opendev.org/openstack/kolla/commit/703dedbb7f34c2b7923847943ba88e08b6b69809', 'message': 'Bump glance for Rocky\n\nChange-Id: I29028976acb884e799a035fcb76cb998e40e1171\n'}]",0,713952,703dedbb7f34c2b7923847943ba88e08b6b69809,7,3,1,14826,,,0,"Bump glance for Rocky

Change-Id: I29028976acb884e799a035fcb76cb998e40e1171
",git fetch https://review.opendev.org/openstack/kolla refs/changes/52/713952/1 && git format-patch -1 --stdout FETCH_HEAD,['kolla/common/config.py'],1,703dedbb7f34c2b7923847943ba88e08b6b69809,," 'glance-17.0.1.tar.gz')},"," 'glance-17.0.0.tar.gz')},",1,1
openstack%2Fcharm-specs~master~I89c668cb44fd6308d8819afba19db42a52e8dbd0,openstack/charm-specs,master,I89c668cb44fd6308d8819afba19db42a52e8dbd0,Add spec for Mellanox hardware offload support,MERGED,2019-09-11 09:12:08.000000000,2020-03-20 10:50:05.000000000,2020-03-20 10:48:01.000000000,"[{'_account_id': 935}, {'_account_id': 13686}, {'_account_id': 22348}]","[{'number': 1, 'created': '2019-09-11 09:12:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-specs/commit/46d6adf8f0a05b36a214a96f4da550b8d54488a5', 'message': 'Add spec for Mellanox hardware offload support\n\nNew spec for delivery of support for hardware offload of\nnetworking to supported Mellanox Connect-X 5+ networking\ncards using Open vSwitch.\n\nChange-Id: I89c668cb44fd6308d8819afba19db42a52e8dbd0\n'}, {'number': 2, 'created': '2019-09-11 09:13:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-specs/commit/06543d41a1e8fd209df323599593a87bb37a8493', 'message': 'Add spec for Mellanox hardware offload support\n\nNew spec for delivery of support for hardware offload of\nnetworking to supported Mellanox Connect-X 5+ networking\ncards using Open vSwitch.\n\nChange-Id: I89c668cb44fd6308d8819afba19db42a52e8dbd0\n'}, {'number': 3, 'created': '2019-11-01 13:30:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-specs/commit/3ed23c11fa098a74678bf4af738a24716cc9d81e', 'message': 'Add spec for Mellanox hardware offload support\n\nNew spec for delivery of support for hardware offload of\nnetworking to supported Mellanox Connect-X 5+ networking\ncards using Open vSwitch.\n\nChange-Id: I89c668cb44fd6308d8819afba19db42a52e8dbd0\n'}, {'number': 4, 'created': '2019-12-20 11:24:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-specs/commit/99f07b1005acb6872454cf1fd4d20b4f68b451b3', 'message': 'Add spec for Mellanox hardware offload support\n\nNew spec for delivery of support for hardware offload of\nnetworking to supported Mellanox Connect-X 5+ networking\ncards using Open vSwitch.\n\nChange-Id: I89c668cb44fd6308d8819afba19db42a52e8dbd0\n'}, {'number': 5, 'created': '2020-03-20 10:30:16.000000000', 'files': ['tests/test_titles.py', 'test-requirements.txt', 'specs/ussuri/approved/mellanox-hw-offload.rst'], 'web_link': 'https://opendev.org/openstack/charm-specs/commit/678e889fc8f2b34c85f5652a96e72b9eeeb48208', 'message': 'Add spec for Mellanox hardware offload support\n\nNew spec for delivery of support for hardware offload of\nnetworking to supported Mellanox Connect-X 5+ networking\ncards using Open vSwitch.\n\nIncluded drive-by fixes for pep8 failures with Py3.8.\n\nChange-Id: I89c668cb44fd6308d8819afba19db42a52e8dbd0\n'}]",3,681414,678e889fc8f2b34c85f5652a96e72b9eeeb48208,23,3,5,935,,,0,"Add spec for Mellanox hardware offload support

New spec for delivery of support for hardware offload of
networking to supported Mellanox Connect-X 5+ networking
cards using Open vSwitch.

Included drive-by fixes for pep8 failures with Py3.8.

Change-Id: I89c668cb44fd6308d8819afba19db42a52e8dbd0
",git fetch https://review.opendev.org/openstack/charm-specs refs/changes/14/681414/5 && git format-patch -1 --stdout FETCH_HEAD,['specs/train/approved/mellanox-hw-offload.rst'],1,46d6adf8f0a05b36a214a96f4da550b8d54488a5,mlnx-hardware-offload,".. Copyright 2019, Canonical Ltd This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode .. This template should be in ReSTructured text. Please do not delete any of the sections in this template. If you have nothing to say for a whole section, just write: ""None"". For help with syntax, see http://sphinx-doc.org/rest.html To test out your formatting, see http://www.tele3.cz/jbar/rest/rest.html ================================= ML2/OVS Mellanox Hardware Offload ================================= High end networking adapters such as the Mellanox Connect-X series support up to 200Gbps networking; instances using traditional software virtualized networking via OVS and virtio-net cannot achieve networking throughput of more than 10Gbps with a high CPU load; it’s possible to use SR-IOV to increase performance to nearer 100Gbps but that has limitations in terms of the flexibility of networking options (no support for overlay networks) and no support for security groups. The Mellanox Connect-X 5+ supports hardware offload via a feature called ASAP2; integrated with Open vSwitch (OVS), this pushes the network data path from the kernel directly onto a high performance embedded switch on the network card. This will allow instances to achieve much higher network performance with very low CPU overheads as well as utilizing as much of the overall network capacity to the hypervisor as possible. Problem Description =================== Hardware offload support is not currently enabled in the OpenStack charms to enable this feature to be used. Proposed Change =============== The neutron-openvswitch charm will be updated to support configuration of Mellanox Connect-X 5+ network adapters to support hardware offloading (card needs to be placed in switchdev mode on boot); OVS will be configured to make use of hardware offloading where possible. A new tool (mlnx-switchdev-boot) will be developed to configure the Connect-X card in the right mode on boot prior to it being used by OVS for networking. This may be superseded at some future date by features in networkd and/or netplan. This feature does make use of a number of leading edge features across the hypervisor software stack: - Linux Kernel >= 5.2 - OVS >= 2.11.0 This limits the scope of support to OpenStack Stein or later in-conjunction with the Ubuntu LTS hardware enablement kernel from Ubuntu 19.10 (not yet released). The Connect-X series of cards also supports Virtual Function (VF) Link Aggregation (LAG) providing upstream switch/cable/port resilience for hardware offloaded ports which are supported using a VF on the underlying network card - this is driven by the normal configuration process for bonding network devices via Linux - in this case the Physical Functions (PF’s) for the network cards. .. note:: The most recent kernel and OVS versions don’t yet support connection tracking offload which means that security groups cannot be offloaded to the network card switch; this is under development and will land in a later release of OVS and Linux. .. note:: The total number of hardware offloaded ports a hypervisor can support s limited by the total number of VF’s that the underlying network cards can support. Alternatives ------------ It’s worth noting that for VLAN or flat networking requirements, it’s possible to achieve high throughput networking to instances by using SR-IOV which is already supported by the neutron-openvswitch charm. Implementation ============== Assignee(s) ----------- Primary assignees: wgrant james-page Gerrit Topic ------------ Use Gerrit topic ""<topic_name>"" for all patches related to this spec. .. code-block:: bash git-review -t hardware-offload Work Items ---------- Develop and test tool for configuration of Connect-X adapters for hardware offload support. Update neutron-openvswitch to support configuration of OVS in hardware offload mode. Add appendix to charm deployment guide to detail configuration and use of OpenStack with Mellanox Connect-X 5+ cards in hardware offload mode. Repositories ------------ A new repository (mlnx-switchdev-boot) will be required for tooling to configure the Mellanox Connect-X networking cards on boot. Documentations ------------- Documentation of this feature will be provided in the charm deployment guide. Security -------- No additional security risks are introduced by this feature. Testing ------- Testing of this feature requires specific hardware in the form of Mellanox Connect-X 5+ networking cards. Dependencies ============ - Linux >= 5.2 - OVS >= 2.11.0 - Mellanox Firmware >= XX.YY.ZZ ",,150,0
openstack%2Fcyborg~master~I4fd12a7d34594a63bd80bdc188ddc977d0a094e7,openstack/cyborg,master,I4fd12a7d34594a63bd80bdc188ddc977d0a094e7,Deployable V2 API implementation,MERGED,2020-03-13 05:18:52.000000000,2020-03-20 10:49:47.000000000,2020-03-20 10:48:23.000000000,"[{'_account_id': 21672}, {'_account_id': 22348}, {'_account_id': 23168}, {'_account_id': 24872}, {'_account_id': 25738}, {'_account_id': 26458}, {'_account_id': 28748}, {'_account_id': 30759}]","[{'number': 1, 'created': '2020-03-13 05:18:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/59a39d8b7aed877cf1b854aee714bc7c7b6b5156', 'message': ""Deployable V2 API implementation\n\nThis patch aims at implementing basic function of v2/deployable API.\nIt contains:\n1. implement deployable list/show API based on v1 deployable API.\n2. remove 'patch' method, because Cyborg V2 API provides other\nAPI resource(ARQ) to let caller to binding, not based on deployable.\n3. remove 'delete' method, because deployable can not actually be\ndeleted, it is reporter by each driver on the node.\n4. Add related UT.\n\nAs for further implementation like program API, it will be done in\nanther patch.\n\nChange-Id: I4fd12a7d34594a63bd80bdc188ddc977d0a094e7\nTask: 2007427\n""}, {'number': 2, 'created': '2020-03-13 06:33:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/7cf88998bf3db8a7beaa9c383e62174cb5643fd3', 'message': ""Deployable V2 API implementation\n\nThis patch aims at implementing basic functions of v2/deployable API.\nIt contains:\n1. implement deployable list/show API based on v1 deployable API.\n2. remove 'patch' method, because Cyborg V2 API provides other\nAPI resource(ARQ) to let caller to do binding, not based on deployable.\n3. remove 'delete' method, because deployable can not actually be\ndeleted, it is reported by each driver on the node.\n4. Add related UT.\n\nAs for further implementation like program API, it will be done in\nanother patch.\n\nChange-Id: I4fd12a7d34594a63bd80bdc188ddc977d0a094e7\nTask: 2007427\n""}, {'number': 3, 'created': '2020-03-16 06:47:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cyborg/commit/059841b955faebebd020929b1bd6565b7fa4e8ca', 'message': ""Deployable V2 API implementation\n\nThis patch aims at implementing basic functions of v2/deployable API.\nIt contains:\n1. implement deployable list/show API based on v1 deployable API.\n2. remove 'patch' method, because Cyborg V2 API provides other\nAPI resource(ARQ) to let caller to do binding, not based on deployable.\n3. remove 'delete' method, because deployable can not actually be\ndeleted, it is reported by each driver on the node.\n4. Add related UT.\n\nAs for further implementation like program API, it will be done in\nanother patch.\n\nChange-Id: I4fd12a7d34594a63bd80bdc188ddc977d0a094e7\nTask: 2007427\n""}, {'number': 4, 'created': '2020-03-19 02:49:43.000000000', 'files': ['cyborg/api/controllers/types.py', 'cyborg/api/controllers/v2/__init__.py', 'cyborg/tests/unit/api/controllers/v2/test_deployables.py', 'cyborg/api/controllers/v2/deployables.py', 'cyborg/common/policy.py', 'cyborg/tests/unit/fake_deployable.py'], 'web_link': 'https://opendev.org/openstack/cyborg/commit/a9ca2c5ece998b088314aa2ab38668a851e3b52a', 'message': ""Deployable V2 API implementation\n\nThis patch aims at implementing basic functions of v2/deployable API.\nIt contains:\n1. Implement deployable list/show API based on v1 deployable API.\n2. Remove 'patch' method, because Cyborg V2 API provides other\nAPI resource(ARQ) to let caller to do binding, not based on deployable.\n3. Remove 'delete' method, because deployable can not actually be\ndeleted, it is reported by each driver on the node.\n4. Add related UT.\n\nAs for further implementation like program API, it will be done in\nanother patch.\n\nChange-Id: I4fd12a7d34594a63bd80bdc188ddc977d0a094e7\nTask: 2007427\n""}]",30,712835,a9ca2c5ece998b088314aa2ab38668a851e3b52a,29,8,4,25738,,,0,"Deployable V2 API implementation

This patch aims at implementing basic functions of v2/deployable API.
It contains:
1. Implement deployable list/show API based on v1 deployable API.
2. Remove 'patch' method, because Cyborg V2 API provides other
API resource(ARQ) to let caller to do binding, not based on deployable.
3. Remove 'delete' method, because deployable can not actually be
deleted, it is reported by each driver on the node.
4. Add related UT.

As for further implementation like program API, it will be done in
another patch.

Change-Id: I4fd12a7d34594a63bd80bdc188ddc977d0a094e7
Task: 2007427
",git fetch https://review.opendev.org/openstack/cyborg refs/changes/35/712835/4 && git format-patch -1 --stdout FETCH_HEAD,"['cyborg/api/controllers/types.py', 'cyborg/api/controllers/v2/__init__.py', 'cyborg/tests/unit/api/controllers/v2/test_deployables.py', 'cyborg/api/controllers/v2/deployables.py', 'cyborg/common/policy.py', 'cyborg/tests/unit/fake_deployable.py']",6,59a39d8b7aed877cf1b854aee714bc7c7b6b5156,deployable-v2-api,,"import json bdf = {""domain"": ""0000"", ""bus"": ""00"", ""device"": ""01"", ""function"": ""1""} ""cpid_info"": json.dumps(bdf).encode('utf-8')",247,5
openstack%2Fironic-python-agent~master~I63fca663940e44924a201b166bdd79d8f7710bee,openstack/ironic-python-agent,master,I63fca663940e44924a201b166bdd79d8f7710bee,Use crypt to generate salt,MERGED,2020-03-18 16:15:56.000000000,2020-03-20 10:47:38.000000000,2020-03-20 10:45:47.000000000,"[{'_account_id': 10239}, {'_account_id': 15519}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 23851}, {'_account_id': 24828}]","[{'number': 1, 'created': '2020-03-18 16:15:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/497ba2aeeef701627b0a73ac3c370e4147b625d4', 'message': ""Use secrets to generate salt\n\nTo avoid problems with FIPS 140-2 let's use the secrets\nmodule instead of random to generate the salt.\n\nChange-Id: I63fca663940e44924a201b166bdd79d8f7710bee\n""}, {'number': 2, 'created': '2020-03-19 12:14:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/9548a724653f14cb44ef418141c750a2a6e3d3ea', 'message': ""Use secrets to generate salt\n\nTo avoid problems with FIPS 140-2 let's use the secrets\nmodule instead of random to generate the salt.\n\nChange-Id: I63fca663940e44924a201b166bdd79d8f7710bee\n""}, {'number': 3, 'created': '2020-03-19 12:19:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/85ff9e974afc26235f76e6f98b55f1c15074cef2', 'message': ""Use crypt to generate salt\n\nLet's use the crypt to generate the salt, the crypt.crypt\ncan handle the generation of the salt if we don't pass.\n\nChange-Id: I63fca663940e44924a201b166bdd79d8f7710bee\n""}, {'number': 4, 'created': '2020-03-19 14:18:47.000000000', 'files': ['ironic_python_agent/tests/unit/extensions/test_rescue.py', 'releasenotes/notes/let_crypt_generate_the_salt-99876591325275a1.yaml', 'ironic_python_agent/extensions/rescue.py'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/7f8afac092e44ca3c93350a73c7b9ff29a0642ba', 'message': ""Use crypt to generate salt\n\nLet's use the crypt to generate the salt, the crypt.crypt\ncan handle the generation of the salt if we don't pass.\n\nChange-Id: I63fca663940e44924a201b166bdd79d8f7710bee\nStory: 2007443\nTask: 39103\n""}]",2,713690,7f8afac092e44ca3c93350a73c7b9ff29a0642ba,17,6,4,15519,,,0,"Use crypt to generate salt

Let's use the crypt to generate the salt, the crypt.crypt
can handle the generation of the salt if we don't pass.

Change-Id: I63fca663940e44924a201b166bdd79d8f7710bee
Story: 2007443
Task: 39103
",git fetch https://review.opendev.org/openstack/ironic-python-agent refs/changes/90/713690/3 && git format-patch -1 --stdout FETCH_HEAD,['ironic_python_agent/extensions/rescue.py'],1,497ba2aeeef701627b0a73ac3c370e4147b625d4,use_secrets,"import secrets """"""Generate a secret salt for hashing the rescue password. return secrets.choice(allowed_chars) + secrets.choice(allowed_chars)","import random """"""Generate a random salt for hashing the rescue password. return random.choice(allowed_chars) + random.choice(allowed_chars)",3,3
openstack%2Freleases~master~I30eff07200e674f664b2139e0065c3aba494e527,openstack/releases,master,I30eff07200e674f664b2139e0065c3aba494e527,Release python-keystoneclient 3.19.1 for stable/stein,MERGED,2020-03-19 17:11:44.000000000,2020-03-20 10:45:37.000000000,2020-03-20 10:45:37.000000000,"[{'_account_id': 308}, {'_account_id': 8482}, {'_account_id': 11904}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 17:11:44.000000000', 'files': ['deliverables/stein/python-keystoneclient.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/709146f3b4381dfd69ec7c8725c60c6daaf6cd5a', 'message': 'Release python-keystoneclient 3.19.1 for stable/stein\n\nThis release would include [1], which makes life easier for packagers\nwho want to run unit tests.\n\n[1] - https://review.opendev.org/701425\n\nChange-Id: I30eff07200e674f664b2139e0065c3aba494e527\n'}]",0,713938,709146f3b4381dfd69ec7c8725c60c6daaf6cd5a,9,4,1,13294,,,0,"Release python-keystoneclient 3.19.1 for stable/stein

This release would include [1], which makes life easier for packagers
who want to run unit tests.

[1] - https://review.opendev.org/701425

Change-Id: I30eff07200e674f664b2139e0065c3aba494e527
",git fetch https://review.opendev.org/openstack/releases refs/changes/38/713938/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/stein/python-keystoneclient.yaml'],1,709146f3b4381dfd69ec7c8725c60c6daaf6cd5a,, - projects: - hash: abe757aa70b262068f26b2b634b8fff06a8fbfdf repo: openstack/python-keystoneclient version: 3.19.1,,4,0
openstack%2Freleases~master~Ic49c04559ff4e5664e8ed50c8ebced3c7f879403,openstack/releases,master,Ic49c04559ff4e5664e8ed50c8ebced3c7f879403,Release keystonemiddleware 6.0.1,MERGED,2020-03-19 18:05:02.000000000,2020-03-20 10:45:14.000000000,2020-03-20 10:45:14.000000000,"[{'_account_id': 308}, {'_account_id': 11904}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 18:05:02.000000000', 'files': ['deliverables/stein/keystonemiddleware.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/fc72e3cb32adbf672a0c4ca4b15223d720e2cfe7', 'message': 'Release keystonemiddleware 6.0.1\n\nChange-Id: Ic49c04559ff4e5664e8ed50c8ebced3c7f879403\n'}]",0,713954,fc72e3cb32adbf672a0c4ca4b15223d720e2cfe7,8,3,1,8482,,,0,"Release keystonemiddleware 6.0.1

Change-Id: Ic49c04559ff4e5664e8ed50c8ebced3c7f879403
",git fetch https://review.opendev.org/openstack/releases refs/changes/54/713954/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/stein/keystonemiddleware.yaml'],1,fc72e3cb32adbf672a0c4ca4b15223d720e2cfe7,keystonemiddleware-6.0.1, - projects: - hash: 89af8f0192a3446e416efe2215b47c4e3a89809d repo: openstack/keystonemiddleware version: 6.0.1,,4,0
openstack%2Freleases~master~I5b94d3633aaa422c9f0b4e36ed1534060faad480,openstack/releases,master,I5b94d3633aaa422c9f0b4e36ed1534060faad480,Release karbor ussuri deliverables,MERGED,2020-03-19 17:35:21.000000000,2020-03-20 10:42:58.000000000,2020-03-20 10:42:58.000000000,"[{'_account_id': 308}, {'_account_id': 21224}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 17:35:21.000000000', 'files': ['deliverables/ussuri/karbor.yaml', 'deliverables/ussuri/karbor-dashboard.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/e54005d24d35e31ec92ce83b9021cd209eee0064', 'message': 'Release karbor ussuri deliverables\n\nChange-Id: I5b94d3633aaa422c9f0b4e36ed1534060faad480\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}]",0,713946,e54005d24d35e31ec92ce83b9021cd209eee0064,8,3,1,11904,,,0,"Release karbor ussuri deliverables

Change-Id: I5b94d3633aaa422c9f0b4e36ed1534060faad480
Signed-off-by: Sean McGinnis <sean.mcginnis@gmail.com>
",git fetch https://review.opendev.org/openstack/releases refs/changes/46/713946/1 && git format-patch -1 --stdout FETCH_HEAD,"['deliverables/ussuri/karbor.yaml', 'deliverables/ussuri/karbor-dashboard.yaml']",2,e54005d24d35e31ec92ce83b9021cd209eee0064,karbor,releases: - version: 1.4.0 projects: - repo: openstack/karbor-dashboard hash: af0f54f7e490bc88505754d30be90eeb2aa5792e flags: - forced,,14,0
openstack%2Fneutron~master~Idc7819340b37bee8ae7841d14d0143fb18ac362a,openstack/neutron,master,Idc7819340b37bee8ae7841d14d0143fb18ac362a,Reno only - Make stateless allocation segment aware,MERGED,2020-03-04 10:04:15.000000000,2020-03-20 10:24:57.000000000,2020-03-20 10:23:00.000000000,"[{'_account_id': 1131}, {'_account_id': 4694}, {'_account_id': 11975}, {'_account_id': 13995}, {'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 24245}, {'_account_id': 26622}]","[{'number': 1, 'created': '2020-03-04 10:04:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c491fc000c61301dfeb9d663d5a6c619dccc0213', 'message': ""Reno only - Make stateless allocation segment aware\n\nThis add's a releasenote for changes:\n * https://review.opendev.org/709444\n * https://review.opendev.org/710546\n * https://review.opendev.org/710547\n\nRelated-Bug: #1864225\nRelated-Bug: #1864333\nRelated-Bug: #1865138\nChange-Id: Idc7819340b37bee8ae7841d14d0143fb18ac362a\n""}, {'number': 2, 'created': '2020-03-05 06:59:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/2ffed7a2e42c5c4a02018a1eecf2d8137b54bce5', 'message': ""Reno only - Make stateless allocation segment aware\n\nThis add's a releasenote for changes:\n * https://review.opendev.org/709444\n * https://review.opendev.org/710546\n * https://review.opendev.org/710547\n\nRelated-Bug: #1864225\nRelated-Bug: #1864333\nRelated-Bug: #1865138\nChange-Id: Idc7819340b37bee8ae7841d14d0143fb18ac362a\n""}, {'number': 3, 'created': '2020-03-19 23:40:33.000000000', 'files': ['releasenotes/notes/fix-ipv6-auto-allocation-with-segments-b90e99a30d096c9d.yaml'], 'web_link': 'https://opendev.org/openstack/neutron/commit/c8f2a309836e152c4f08cc8e5735409f992177af', 'message': ""Reno only - Make stateless allocation segment aware\n\nThis add's a releasenote for changes:\n * https://review.opendev.org/709444\n * https://review.opendev.org/710546\n * https://review.opendev.org/710547\n\nRelated-Bug: #1864225\nRelated-Bug: #1864333\nRelated-Bug: #1865138\nChange-Id: Idc7819340b37bee8ae7841d14d0143fb18ac362a\n""}]",4,711192,c8f2a309836e152c4f08cc8e5735409f992177af,23,9,3,24245,,,0,"Reno only - Make stateless allocation segment aware

This add's a releasenote for changes:
 * https://review.opendev.org/709444
 * https://review.opendev.org/710546
 * https://review.opendev.org/710547

Related-Bug: #1864225
Related-Bug: #1864333
Related-Bug: #1865138
Change-Id: Idc7819340b37bee8ae7841d14d0143fb18ac362a
",git fetch https://review.opendev.org/openstack/neutron refs/changes/92/711192/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/notes/fix-ipv6-auto-allocation-with-segments-b90e99a30d096c9d.yaml'],1,c491fc000c61301dfeb9d663d5a6c619dccc0213,bug/1864225,"--- fixes: - | Fixed an issue where IP allocation for IPv6 stateless subnets would allocate on invalid subnets when segments are used. Auto-addressing now filter on segment ids when allocating IP addresses. (See bugs: `#1864225 <https://bugs.launchpad.net/neutron/+bug/1864225>`_, `#1864333 <https://bugs.launchpad.net/neutron/+bug/1864333>`_, `#1865138 <https://bugs.launchpad.net/neutron/+bug/1865138>`_.) ",,9,0
openstack%2Fmanila-ui~master~I7300746fb7cd00a4df4635a7bbd796194de48b5d,openstack/manila-ui,master,I7300746fb7cd00a4df4635a7bbd796194de48b5d,Add availability zone field for public shares,ABANDONED,2020-03-20 06:33:16.000000000,2020-03-20 10:17:01.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2020-03-20 06:33:16.000000000', 'files': ['manila_ui/dashboards/admin/shares/templates/shares/_detail.html'], 'web_link': 'https://opendev.org/openstack/manila-ui/commit/319757389876f3172bd0d9affd66101d38e51ff3', 'message': 'Add availability zone field for public shares\n\nIn manila_ui/dashboards/admin/shares/templates/shares/_detail.html there\nis empty field for availability zone for public share.So add\navailability zone for public share(line :41)\n\nChange-Id: I7300746fb7cd00a4df4635a7bbd796194de48b5d\nCloses-Bug:#1498433\n'}]",0,714036,319757389876f3172bd0d9affd66101d38e51ff3,3,1,1,31449,,,0,"Add availability zone field for public shares

In manila_ui/dashboards/admin/shares/templates/shares/_detail.html there
is empty field for availability zone for public share.So add
availability zone for public share(line :41)

Change-Id: I7300746fb7cd00a4df4635a7bbd796194de48b5d
Closes-Bug:#1498433
",git fetch https://review.opendev.org/openstack/manila-ui refs/changes/36/714036/1 && git format-patch -1 --stdout FETCH_HEAD,['manila_ui/dashboards/admin/shares/templates/shares/_detail.html'],1,319757389876f3172bd0d9affd66101d38e51ff3,bug/1498433, </div> , {% if share.is_public == True %} <dd>{{ 'public' }}</dd> {% else %} <dd>{{ 'private' }}</dd> {% endif %}</div>,2,6
openstack%2Fnova~master~I64fbfc5cb323cdf1c586f59bd13fbafaaeb3e5c3,openstack/nova,master,I64fbfc5cb323cdf1c586f59bd13fbafaaeb3e5c3,Add test coverage of existing flavor_access policies,MERGED,2020-03-18 00:20:01.000000000,2020-03-20 10:09:52.000000000,2020-03-20 10:07:07.000000000,"[{'_account_id': 782}, {'_account_id': 8556}, {'_account_id': 14384}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-03-18 00:20:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/673806dd67591c8ae0f6fab9c832057d0672ff3a', 'message': 'Add test coverage of existing flavor_access policies\n\nCurrent tests do not have good test coverage of existing policies.\nEither tests for policies do not exist or if they exist then they\ndo not cover the actual negative and positive testing.\n\nFor Example, if any policy with default rule as admin only then\ntest should verify:\n- policy check pass with context having admin or server owner\n- policy check fail with context having non-admin and not server owner\n\nAs discussed in policy-defaults-refresh [1], to change the policies\nwith new default roles and scope_type, we need to have the enough\ntesting coverage of existing policy behavior.\n\nWhen we will add the scope_type in policies or new default roles,\nthen these test coverage will be extended to adopt the new changes\nand also make sure we do not break the existing behavior.\n\nThis commit covers the testing coverage of existing flavor_access policies.\n\nPartial implement blueprint policy-defaults-refresh\n\n[1] https://specs.openstack.org/openstack/nova-specs/specs/ussuri/approved/policy-defaults-refresh.html#testing\n\nChange-Id: I64fbfc5cb323cdf1c586f59bd13fbafaaeb3e5c3\n'}, {'number': 2, 'created': '2020-03-18 00:35:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a2ecc7d9476ba11bceb641eebbdcbf9f9e4bde3a', 'message': 'Add test coverage of existing flavor_access policies\n\nCurrent tests do not have good test coverage of existing policies.\nEither tests for policies do not exist or if they exist then they\ndo not cover the actual negative and positive testing.\n\nFor Example, if any policy with default rule as admin only then\ntest should verify:\n- policy check pass with context having admin or server owner\n- policy check fail with context having non-admin and not server owner\n\nAs discussed in policy-defaults-refresh [1], to change the policies\nwith new default roles and scope_type, we need to have the enough\ntesting coverage of existing policy behavior.\n\nWhen we will add the scope_type in policies or new default roles,\nthen these test coverage will be extended to adopt the new changes\nand also make sure we do not break the existing behavior.\n\nThis commit covers the testing coverage of existing flavor_access policies.\n\nPartial implement blueprint policy-defaults-refresh\n\n[1] https://specs.openstack.org/openstack/nova-specs/specs/ussuri/approved/policy-defaults-refresh.html#testing\n\nChange-Id: I64fbfc5cb323cdf1c586f59bd13fbafaaeb3e5c3\n'}, {'number': 3, 'created': '2020-03-18 00:48:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8ae48db559d8e11b00ae0f15644782a9e04c6955', 'message': 'Add test coverage of existing flavor_access policies\n\nCurrent tests do not have good test coverage of existing policies.\nEither tests for policies do not exist or if they exist then they\ndo not cover the actual negative and positive testing.\n\nFor Example, if any policy with default rule as admin only then\ntest should verify:\n- policy check pass with context having admin or server owner\n- policy check fail with context having non-admin and not server owner\n\nAs discussed in policy-defaults-refresh [1], to change the policies\nwith new default roles and scope_type, we need to have the enough\ntesting coverage of existing policy behavior.\n\nWhen we will add the scope_type in policies or new default roles,\nthen these test coverage will be extended to adopt the new changes\nand also make sure we do not break the existing behavior.\n\nThis commit covers the testing coverage of existing flavor_access policies.\n\nPartial implement blueprint policy-defaults-refresh\n\n[1] https://specs.openstack.org/openstack/nova-specs/specs/ussuri/approved/policy-defaults-refresh.html#testing\n\nChange-Id: I64fbfc5cb323cdf1c586f59bd13fbafaaeb3e5c3\n'}, {'number': 4, 'created': '2020-03-18 01:05:23.000000000', 'files': ['nova/tests/unit/api/openstack/compute/test_flavor_access.py', 'nova/tests/unit/policies/test_flavor_access.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/3393d998203be0cfb1e09f17316177b73343cfdc', 'message': 'Add test coverage of existing flavor_access policies\n\nCurrent tests do not have good test coverage of existing policies.\nEither tests for policies do not exist or if they exist then they\ndo not cover the actual negative and positive testing.\n\nFor Example, if any policy with default rule as admin only then\ntest should verify:\n- policy check pass with context having admin or server owner\n- policy check fail with context having non-admin and not server owner\n\nAs discussed in policy-defaults-refresh [1], to change the policies\nwith new default roles and scope_type, we need to have the enough\ntesting coverage of existing policy behavior.\n\nWhen we will add the scope_type in policies or new default roles,\nthen these test coverage will be extended to adopt the new changes\nand also make sure we do not break the existing behavior.\n\nThis commit covers the testing coverage of existing flavor_access policies.\n\nPartial implement blueprint policy-defaults-refresh\n\n[1] https://specs.openstack.org/openstack/nova-specs/specs/ussuri/approved/policy-defaults-refresh.html#testing\n\nChange-Id: I64fbfc5cb323cdf1c586f59bd13fbafaaeb3e5c3\n'}]",3,713556,3393d998203be0cfb1e09f17316177b73343cfdc,21,9,4,8556,,,0,"Add test coverage of existing flavor_access policies

Current tests do not have good test coverage of existing policies.
Either tests for policies do not exist or if they exist then they
do not cover the actual negative and positive testing.

For Example, if any policy with default rule as admin only then
test should verify:
- policy check pass with context having admin or server owner
- policy check fail with context having non-admin and not server owner

As discussed in policy-defaults-refresh [1], to change the policies
with new default roles and scope_type, we need to have the enough
testing coverage of existing policy behavior.

When we will add the scope_type in policies or new default roles,
then these test coverage will be extended to adopt the new changes
and also make sure we do not break the existing behavior.

This commit covers the testing coverage of existing flavor_access policies.

Partial implement blueprint policy-defaults-refresh

[1] https://specs.openstack.org/openstack/nova-specs/specs/ussuri/approved/policy-defaults-refresh.html#testing

Change-Id: I64fbfc5cb323cdf1c586f59bd13fbafaaeb3e5c3
",git fetch https://review.opendev.org/openstack/nova refs/changes/56/713556/4 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/api/openstack/compute/test_flavor_access.py', 'nova/tests/unit/policies/test_flavor_access.py']",2,673806dd67591c8ae0f6fab9c832057d0672ff3a,bp/policy-defaults-refresh,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import fixtures import mock from oslo_utils.fixture import uuidsentinel as uuids from nova.api.openstack.compute import flavor_access from nova.tests.unit.api.openstack import fakes from nova.tests.unit import fake_flavor from nova.tests.unit.policies import base class FlavorAccessPolicyTest(base.BasePolicyTest): """"""Test Flavor Access APIs policies with all possible context. This class defines the set of context with different roles which are allowed and not allowed to pass the policy checks. With those set of context, it will call the API operation and verify the expected behaviour. """""" def setUp(self): super(FlavorAccessPolicyTest, self).setUp() self.controller = flavor_access.FlavorActionController() self.controller_index = flavor_access.FlavorAccessController() self.req = fakes.HTTPRequest.blank('') self.mock_get = self.useFixture( fixtures.MockPatch('nova.api.openstack.common.get_flavor')).mock uuid = uuids.fake_id self.flavor = fake_flavor.fake_flavor_obj( self.project_member_context, id=1, uuid=uuid, project_id=self.project_id, is_public=False) self.mock_get.return_value = self.flavor self.stub_out('nova.api.openstack.identity.verify_project_id', lambda ctx, project_id: True) self.stub_out('nova.objects.flavor._get_projects_from_db', lambda context, flavorid: []) # Check that admin is able to add/remove flavor access # to a tenant. self.admin_authorized_contexts = [ self.legacy_admin_context, self.system_admin_context, self.project_admin_context] # Check that non-admin is not able to add/remove flavor access # to a tenant. self.admin_unauthorized_contexts = [ self.system_member_context, self.system_reader_context, self.system_foo_context, self.project_member_context, self.other_project_member_context, self.project_foo_context, self.project_reader_context ] # Check that admin or flavor owner is able to list flavor access # information. self.admin_or_owner_authorized_contexts = [ self.legacy_admin_context, self.system_admin_context, self.project_admin_context, self.project_member_context, self.project_reader_context, self.project_foo_context, self.system_member_context, self.system_reader_context, self.system_foo_context, self.other_project_member_context] # Check that non-admin is not able to list flavor access # information. self.admin_or_owner_unauthorized_contexts = [ ] def test_list_flavor_access_policy(self): rule_name = ""os_compute_api:os-flavor-access"" self.common_policy_check(self.admin_or_owner_authorized_contexts, self.admin_or_owner_unauthorized_contexts, rule_name, self.controller_index.index, self.req, '1') @mock.patch('nova.objects.Flavor.add_access') def test_add_tenant_access_policy(self, mock_add): rule_name = ""os_compute_api:os-flavor-access:add_tenant_access"" self.common_policy_check(self.admin_authorized_contexts, self.admin_unauthorized_contexts, rule_name, self.controller._add_tenant_access, self.req, '1', body={'addTenantAccess': {'tenant': 't1'}}) @mock.patch('nova.objects.Flavor.remove_access') def test_remove_tenant_access_policy(self, mock_remove): rule_name = ""os_compute_api:os-flavor-access:remove_tenant_access"" self.common_policy_check(self.admin_authorized_contexts, self.admin_unauthorized_contexts, rule_name, self.controller._remove_tenant_access, self.req, '1', body={'removeTenantAccess': {'tenant': 't1'}}) class FlavorAccessScopeTypePolicyTest(FlavorAccessPolicyTest): """"""Test Flavor Access APIs policies with system scope enabled. This class set the nova.conf [oslo_policy] enforce_scope to True so that we can switch on the scope checking on oslo policy side. It defines the set of context with scopped token which are allowed and not allowed to pass the policy checks. With those set of context, it will run the API operation and verify the expected behaviour. """""" def setUp(self): super(FlavorAccessScopeTypePolicyTest, self).setUp() self.flags(enforce_scope=True, group=""oslo_policy"") ",,119,44
openstack%2Fcharm-neutron-api~master~I1f59012ad2d16af18ca310906f6c6b537bb7ec72,openstack/charm-neutron-api,master,I1f59012ad2d16af18ca310906f6c6b537bb7ec72,Enable hardware offload support,MERGED,2019-10-03 13:43:27.000000000,2020-03-20 10:09:06.000000000,2020-03-20 10:09:06.000000000,"[{'_account_id': 935}, {'_account_id': 13686}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2019-10-03 13:43:27.000000000', 'files': ['unit_tests/test_neutron_api_hooks.py', 'hooks/neutron_api_hooks.py', 'config.yaml'], 'web_link': 'https://opendev.org/openstack/charm-neutron-api/commit/c7d9e9ab427e8971b65d8881f010d6aeedd607fd', 'message': 'Enable hardware offload support\n\nFollowing the style of the SR-IOV enablement, add a new config\noption to this charm to enable hardware offload support.\n\nThis is mainly used to signal to the nova-cloud-controller charm\nto enable the PCI Passthrough Filter which is used in this type\nof deployment.\n\nChange-Id: I1f59012ad2d16af18ca310906f6c6b537bb7ec72\n'}]",1,686404,c7d9e9ab427e8971b65d8881f010d6aeedd607fd,14,4,1,935,,,0,"Enable hardware offload support

Following the style of the SR-IOV enablement, add a new config
option to this charm to enable hardware offload support.

This is mainly used to signal to the nova-cloud-controller charm
to enable the PCI Passthrough Filter which is used in this type
of deployment.

Change-Id: I1f59012ad2d16af18ca310906f6c6b537bb7ec72
",git fetch https://review.opendev.org/openstack/charm-neutron-api refs/changes/04/686404/1 && git format-patch -1 --stdout FETCH_HEAD,"['unit_tests/test_neutron_api_hooks.py', 'hooks/neutron_api_hooks.py', 'config.yaml']",3,c7d9e9ab427e8971b65d8881f010d6aeedd607fd,mlnx-hardware-offload, enable-hardware-offload: type: boolean default: False description: | Enable OVS hardware offload support across Neutron and Nova.,,8,0
openstack%2Frequirements~master~I663b3562b2668e99a84063babd6b9006acfeff34,openstack/requirements,master,I663b3562b2668e99a84063babd6b9006acfeff34,Updated from generate-constraints,MERGED,2020-03-20 06:24:06.000000000,2020-03-20 10:08:45.000000000,2020-03-20 10:07:02.000000000,"[{'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 06:24:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/8b291ea4505b421aa326e78ce3c463e133f05d7f', 'message': 'Updated from generate-constraints\n\nChange-Id: I663b3562b2668e99a84063babd6b9006acfeff34\n'}, {'number': 2, 'created': '2020-03-20 07:16:41.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/02df513aa8c6de4469c5d5214b1dd2591aeebe1a', 'message': 'Updated from generate-constraints\n\nChange-Id: I663b3562b2668e99a84063babd6b9006acfeff34\nSigned-off-by: Matthew Thode <mthode@mthode.org>\n'}]",1,714035,02df513aa8c6de4469c5d5214b1dd2591aeebe1a,9,2,2,11131,,,0,"Updated from generate-constraints

Change-Id: I663b3562b2668e99a84063babd6b9006acfeff34
Signed-off-by: Matthew Thode <mthode@mthode.org>
",git fetch https://review.opendev.org/openstack/requirements refs/changes/35/714035/2 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,8b291ea4505b421aa326e78ce3c463e133f05d7f,openstack/requirements/constraints/noclob,ddt===1.3.1 XStatic-Graphlib===1.0.7.0pyghmi===1.5.13gnocchiclient===7.0.6XStatic-lodash===4.16.4.1mock===3.0.5;python_version=='2.7' mock===4.0.2;python_version=='3.6' mock===4.0.2;python_version=='3.7' PyYAML===5.3.1pytest-html===2.1.1;python_version=='3.6' pytest-html===2.1.1;python_version=='3.7'SQLAlchemy-Utils===0.36.3docutils===0.16XStatic-Dagre===0.6.4.0boto3===1.12.25pyScss2===1.4.0XStatic-moment===2.8.4.2XStatic-Dagre-D3===0.4.17.0numpy===1.18.2;python_version=='3.6' numpy===1.18.2;python_version=='3.7' msgpack===1.0.0XStatic-Moment-Timezone===0.5.22.0botocore===1.15.25oslo.policy===2.4.1;python_version=='2.7' oslo.policy===3.0.1;python_version=='3.6' oslo.policy===3.0.1;python_version=='3.7'importlib-resources===1.4.0alembic===1.4.2django-pyscss2===3.0.0sqlalchemy-filters===0.10.0virtualenv===20.0.13,pyScss2===1.4.0ddt===1.3.0pyghmi===1.5.12gnocchiclient===7.0.5mock===3.0.5 PyYAML===5.3pytest-html===2.1.0;python_version=='3.6' pytest-html===2.1.0;python_version=='3.7'SQLAlchemy-Utils===0.36.2django-pyscss2===3.0.0docutils===0.15.2boto3===1.12.22sqlalchemy-filters===0.10.0numpy===1.18.1;python_version=='3.6' numpy===1.18.1;python_version=='3.7' msgpack===0.6.2botocore===1.15.22oslo.policy===3.0.1importlib-resources===1.3.1alembic===1.3.3virtualenv===20.0.10XStatic-Dagre===0.6.4.0 XStatic-Dagre-D3===0.4.17.0 XStatic-Graphlib===1.0.7.0 XStatic-lodash===4.16.4.1 XStatic-moment===2.8.4.1 XStatic-Moment-Timezone===0.5.22.0,31,27
openstack%2Fdevstack~master~I86601bc3bf114583ccad7a301af4e5b71e3ba2bf,openstack/devstack,master,I86601bc3bf114583ccad7a301af4e5b71e3ba2bf,Updated from generate-devstack-plugins-list,MERGED,2020-03-19 06:16:34.000000000,2020-03-20 10:08:42.000000000,2020-03-20 10:07:03.000000000,"[{'_account_id': 22348}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-19 06:16:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/0a0855afd3bbf35b9e2425c01682a71cd398254f', 'message': 'Updated from generate-devstack-plugins-list\n\nChange-Id: I86601bc3bf114583ccad7a301af4e5b71e3ba2bf\n'}, {'number': 2, 'created': '2020-03-20 08:33:15.000000000', 'files': ['doc/source/plugin-registry.rst'], 'web_link': 'https://opendev.org/openstack/devstack/commit/4af912d88953f406f4c3275de8c6ad370aade8ab', 'message': 'Updated from generate-devstack-plugins-list\n\nChange-Id: I86601bc3bf114583ccad7a301af4e5b71e3ba2bf\n'}]",0,713786,4af912d88953f406f4c3275de8c6ad370aade8ab,10,2,2,11131,,,0,"Updated from generate-devstack-plugins-list

Change-Id: I86601bc3bf114583ccad7a301af4e5b71e3ba2bf
",git fetch https://review.opendev.org/openstack/devstack refs/changes/86/713786/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/plugin-registry.rst'],1,0a0855afd3bbf35b9e2425c01682a71cd398254f,openstack/devstack/plugins,openstack/devstack-plugin-open-cas `https://opendev.org/openstack/devstack-plugin-open-cas <https://opendev.org/openstack/devstack-plugin-open-cas>`__,,1,0
openstack%2Fcharm-neutron-openvswitch~master~I4ce47b1712e79bfbed9ac708cc521840b3709724,openstack/charm-neutron-openvswitch,master,I4ce47b1712e79bfbed9ac708cc521840b3709724,Enable support for hardware offload,MERGED,2019-10-03 13:45:57.000000000,2020-03-20 10:07:50.000000000,2020-03-20 10:07:49.000000000,"[{'_account_id': 935}, {'_account_id': 12549}, {'_account_id': 13686}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2019-10-03 13:45:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/c00f5b5d54116edb22ea386fdf4f454e62b37f11', 'message': 'Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later inconjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n'}, {'number': 2, 'created': '2019-10-30 09:41:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/35cc5358729ea7dfd523254adcee68d9703b1a9b', 'message': 'Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later inconjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n'}, {'number': 3, 'created': '2019-10-30 14:26:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/4638004ab79eb6effab1b24bdd7426c7fff0c98f', 'message': 'Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later inconjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n'}, {'number': 4, 'created': '2019-11-01 11:34:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/1c3493c6f83cca177e0c0dcf866f169be91aece5', 'message': ""Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later inconjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nThis commit also includes a refactoring of the SR-IOV VF configuration\nsupport via this charm - sriov-netplan-shim is used to configure\nVF's on PF's so the charm simply writes out the required interfaces.yaml\nfile and restarts the sriov-netplan-shim service which is fully\nidempotent.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n""}, {'number': 5, 'created': '2019-11-01 13:02:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/79ed501145cc363d9f4571a0816dbd378fa3c34c', 'message': ""Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later inconjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nThis commit also includes a refactoring of the SR-IOV VF configuration\nsupport via this charm - sriov-netplan-shim is used to configure\nVF's on PF's so the charm simply writes out the required interfaces.yaml\nfile and restarts the sriov-netplan-shim service which is fully\nidempotent.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n""}, {'number': 6, 'created': '2019-11-04 11:20:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/5b9fb97557b319b54b8b7c25cca69f9ed2f0dc40', 'message': ""Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later in conjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nThis commit also includes a refactoring of the SR-IOV VF configuration\nsupport via this charm - sriov-netplan-shim is used to configure\nVF's on PF's so the charm simply writes out the required interfaces.yaml\nfile and restarts the sriov-netplan-shim service which is fully\nidempotent.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n""}, {'number': 7, 'created': '2019-11-04 11:30:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/a3ac33033a875c02d0afa89cb90c4cae1d614146', 'message': ""Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later in conjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nThis commit also includes a refactoring of the SR-IOV VF configuration\nsupport via this charm - sriov-netplan-shim is used to configure\nVF's on PF's so the charm simply writes out the required interfaces.yaml\nfile and restarts the sriov-netplan-shim service which is fully\nidempotent.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n""}, {'number': 8, 'created': '2019-11-04 13:46:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/6e015bbe3b40149739a4fb92c5a684971e8d002f', 'message': ""Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later in conjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nThis commit also includes a refactoring of the SR-IOV VF configuration\nsupport via this charm - sriov-netplan-shim is used to configure\nVF's on PF's so the charm simply writes out the required interfaces.yaml\nfile and restarts the sriov-netplan-shim service which is fully\nidempotent.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n""}, {'number': 9, 'created': '2020-01-27 11:01:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/704de47dfd863761ab64a82528f0a2df72a1a70f', 'message': ""Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later in conjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nThis commit also includes a refactoring of the SR-IOV VF configuration\nsupport via this charm - sriov-netplan-shim is used to configure\nVF's on PF's so the charm simply writes out the required interfaces.yaml\nfile and restarts the sriov-netplan-shim service which is fully\nidempotent.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n""}, {'number': 10, 'created': '2020-02-07 15:07:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/6af2e21372dc673955967ab31ebc895c47edb30a', 'message': ""Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later in conjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nThis commit also includes a refactoring of the SR-IOV VF configuration\nsupport via this charm - sriov-netplan-shim is used to configure\nVF's on PF's so the charm simply writes out the required interfaces.yaml\nfile and restarts the sriov-netplan-shim service which is fully\nidempotent.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n""}, {'number': 11, 'created': '2020-03-18 10:59:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/33abe0487895305d4c950e30c7cbe71b27e0fba1', 'message': 'Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later in conjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n'}, {'number': 12, 'created': '2020-03-18 11:01:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/d103e1ab7a1e8db29e9e09ed05ac36404d5ec87e', 'message': 'Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later in conjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n'}, {'number': 13, 'created': '2020-03-19 11:51:26.000000000', 'files': ['unit_tests/test_neutron_ovs_utils.py', 'hooks/neutron_ovs_utils.py', 'config.yaml'], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/ab5de869720d944be604490dd3cd569c61ad93db', 'message': 'Enable support for hardware offload\n\nEnable support for use of hardware offload via OVS; this requires\nOpenStack Stein or later in conjunction with the latest HWE kernel\nfor Ubuntu 18.04 LTS.\n\nChange-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724\n'}]",2,686405,ab5de869720d944be604490dd3cd569c61ad93db,45,5,13,935,,,0,"Enable support for hardware offload

Enable support for use of hardware offload via OVS; this requires
OpenStack Stein or later in conjunction with the latest HWE kernel
for Ubuntu 18.04 LTS.

Change-Id: I4ce47b1712e79bfbed9ac708cc521840b3709724
",git fetch https://review.opendev.org/openstack/charm-neutron-openvswitch refs/changes/05/686405/3 && git format-patch -1 --stdout FETCH_HEAD,"['unit_tests/test_neutron_ovs_utils.py', 'hooks/neutron_ovs_utils.py', 'config.yaml']",3,c00f5b5d54116edb22ea386fdf4f454e62b37f11,mlnx-hardware-offload, enable-hardware-offload: type: boolean default: false description: | Enable support for hardware offload of flows from Open vSwitch to supported network adapters. This requires use of OpenStack Stein or later and has only been tested on Mellanox ConnectX 5 adapters. . Enabling this option will make use of the sriov-numvfs option to configure the VF functions of the physical network adapters detected in each unit. . Enabling this option also requires that the firewall-driver option be set to 'openvswitch'; this allows security groups to be applied to hardware offloaded ports. . This option must not be enabled with either enable-sriov or enable-dpdk. hardware-offload-source: type: string default: ppa:openstack-charmers/mlnx-switchdev-mode description: | Package archive source to use for utilities associated with configuring hardware offloading in Mellanox network adapters.,,93,13
openstack%2Fcharm-neutron-openvswitch~master~I7a3ddf91d4b2ae6aa0806d97c45b59e8a951f67f,openstack/charm-neutron-openvswitch,master,I7a3ddf91d4b2ae6aa0806d97c45b59e8a951f67f,Refactor SR-IOV support,MERGED,2020-03-18 10:54:00.000000000,2020-03-20 10:06:34.000000000,2020-03-20 10:06:33.000000000,"[{'_account_id': 13686}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-18 10:54:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/cde705c09169d59115f97fbe9613da74b51f3e56', 'message': ""Refactor SR-IOV support\n\nRefactor SR-IOV VF configuration support to use sriov-netplan-shim\nto configure VF's on PF's so the charm simply writes out the required\ninterfaces.yaml file and restarts the sriov-netplan-shim service\nwhich is fully idempotent.\n\nChange-Id: I7a3ddf91d4b2ae6aa0806d97c45b59e8a951f67f\n""}, {'number': 2, 'created': '2020-03-18 11:01:30.000000000', 'files': ['unit_tests/test_neutron_ovs_utils.py', 'unit_tests/test_neutron_ovs_hooks.py', 'unit_tests/test_pci.py', 'hooks/pci.py', 'hooks/neutron_ovs_hooks.py', 'hooks/neutron_ovs_utils.py', 'config.yaml'], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/7ba64f9412b3b143054811b311cd37986e1d1df1', 'message': ""Refactor SR-IOV support\n\nRefactor SR-IOV VF configuration support to use sriov-netplan-shim\nto configure VF's on PF's so the charm simply writes out the required\ninterfaces.yaml file and restarts the sriov-netplan-shim service\nwhich is fully idempotent.\n\nChange-Id: I7a3ddf91d4b2ae6aa0806d97c45b59e8a951f67f\n""}]",0,713624,7ba64f9412b3b143054811b311cd37986e1d1df1,9,3,2,935,,,0,"Refactor SR-IOV support

Refactor SR-IOV VF configuration support to use sriov-netplan-shim
to configure VF's on PF's so the charm simply writes out the required
interfaces.yaml file and restarts the sriov-netplan-shim service
which is fully idempotent.

Change-Id: I7a3ddf91d4b2ae6aa0806d97c45b59e8a951f67f
",git fetch https://review.opendev.org/openstack/charm-neutron-openvswitch refs/changes/24/713624/2 && git format-patch -1 --stdout FETCH_HEAD,"['unit_tests/test_neutron_ovs_utils.py', 'unit_tests/test_neutron_ovs_hooks.py', 'unit_tests/test_pci.py', 'hooks/pci.py', 'hooks/neutron_ovs_hooks.py', 'hooks/neutron_ovs_utils.py', 'config.yaml']",7,cde705c09169d59115f97fbe9613da74b51f3e56,mlnx-hardware-offload, networking-tools-source: type: string default: ppa:openstack-charmers/networking-tools description: | Package archive source to use for utilities associated with configuring SR-IOV VF's and switchdev mode in Mellanox network adapters. . This PPA can be mirrored for offline deployments.,,214,204
openstack%2Fcharm-neutron-openvswitch~master~I5f0c57dbf0e11a7d2746f289f60cbf8cd1df44e6,openstack/charm-neutron-openvswitch,master,I5f0c57dbf0e11a7d2746f289f60cbf8cd1df44e6,Resync charmhelpers,MERGED,2020-03-18 10:44:27.000000000,2020-03-20 10:03:39.000000000,2020-03-20 10:03:39.000000000,"[{'_account_id': 13686}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-18 10:44:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/c95d66cd5503199ac656c682ed0ce527f0c7fe3a', 'message': 'Resync charmhelpers\n\nAnd enable Python 3.8 tox target.\n\nChange-Id: I5f0c57dbf0e11a7d2746f289f60cbf8cd1df44e6\n'}, {'number': 2, 'created': '2020-03-18 10:51:29.000000000', 'files': ['hooks/charmhelpers/contrib/openstack/policyd.py', 'hooks/charmhelpers/contrib/openstack/vaultlocker.py', 'test-requirements.txt', 'hooks/charmhelpers/contrib/openstack/utils.py', 'unit_tests/test_utils.py', 'hooks/charmhelpers/contrib/storage/linux/ceph.py', 'hooks/charmhelpers/osplatform.py', 'hooks/charmhelpers/contrib/python/__init__.py', 'hooks/charmhelpers/core/host_factory/ubuntu.py', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/charm-neutron-openvswitch/commit/91b86cb9ebc0809670f8236808b5cc372d73bc5e', 'message': 'Resync charmhelpers\n\nAnd enable Python 3.8 tox target.\n\nUncap flake8, tidy any essential lint.\n\nChange-Id: I5f0c57dbf0e11a7d2746f289f60cbf8cd1df44e6\n'}]",0,713623,91b86cb9ebc0809670f8236808b5cc372d73bc5e,8,3,2,935,,,0,"Resync charmhelpers

And enable Python 3.8 tox target.

Uncap flake8, tidy any essential lint.

Change-Id: I5f0c57dbf0e11a7d2746f289f60cbf8cd1df44e6
",git fetch https://review.opendev.org/openstack/charm-neutron-openvswitch refs/changes/23/713623/2 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/charmhelpers/contrib/openstack/policyd.py', 'hooks/charmhelpers/contrib/openstack/vaultlocker.py', 'hooks/charmhelpers/contrib/openstack/utils.py', 'hooks/charmhelpers/contrib/storage/linux/ceph.py', 'hooks/charmhelpers/osplatform.py', 'hooks/charmhelpers/contrib/python/__init__.py', 'hooks/charmhelpers/core/host_factory/ubuntu.py', 'tox.ini']",8,c95d66cd5503199ac656c682ed0ce527f0c7fe3a,ch-resync,[testenv:py38] basepython = python3.8 deps = -r{toxinidir}/requirements.txt -r{toxinidir}/test-requirements.txt ,,159,23
openstack%2Fkolla-ansible~stable%2Fstein~Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec,openstack/kolla-ansible,stable/stein,Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec,Enable buffering to file for Monasca logs,ABANDONED,2019-07-16 15:24:32.000000000,2020-03-20 09:57:07.000000000,,"[{'_account_id': 14826}, {'_account_id': 17669}, {'_account_id': 22348}, {'_account_id': 30491}]","[{'number': 1, 'created': '2019-07-16 15:24:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/ddb4a0817d1f6ceef18d4f8eb2f3ebeeb74b7948', 'message': 'Enable buffering to file for Monasca logs\n\nThis enables buffering to file, rather than memory for Monasca logs.\nA dedicated docker volume is used for the file buffer. If a post\nto the Monasca Log API fails, retries will be made using an exponential\nbackoff algorithm with a maximum retry interval of 30mins. The maximum\ninterval is set relatively low to try and reduce the risk of large\nbuffers accumulating, and therefore the risk of overloading the Monasca\nLog API.\n\nChange-Id: Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec\n'}, {'number': 2, 'created': '2019-08-03 14:56:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/8cf0c8a61c51192a6b4d4134473d7b684f644a6e', 'message': 'Enable buffering to file for Monasca logs\n\nThis enables buffering to file, rather than memory for Monasca logs.\nA dedicated docker volume is used for the file buffer. If a post\nto the Monasca Log API fails, retries will be made using an exponential\nbackoff algorithm with a maximum retry interval of 30mins. The maximum\ninterval is set relatively low to try and reduce the risk of large\nbuffers accumulating, and therefore the risk of overloading the Monasca\nLog API.\n\nChange-Id: Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec\n'}, {'number': 3, 'created': '2020-03-11 13:35:10.000000000', 'files': ['ansible/roles/common/templates/conf/output/02-monasca.conf.j2', 'ansible/roles/common/templates/conf/output/00-local.conf.j2', 'releasenotes/notes/enable-buffering-to-file-for-monasca-logs-88ca66cc4d6cda3b.yaml'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/11bac51dd8cb4693627824fb839a2626668f6a69', 'message': 'Enable buffering to file for Monasca logs\n\nThis enables buffering to file, rather than memory for Monasca logs.\nA dedicated docker volume is used for the file buffer. If a post\nto the Monasca Log API fails, retries will be made using an exponential\nbackoff algorithm with a maximum retry interval of 30mins. The maximum\ninterval is set relatively low to try and reduce the risk of large\nbuffers accumulating, and therefore the risk of overloading the Monasca\nLog API.\n\nCloses-Bug: #1855700\nChange-Id: Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec\n(cherry picked from commit 5293b1294f5a67107f247f462840ef145bf2ef44)\n'}]",0,671081,11bac51dd8cb4693627824fb839a2626668f6a69,14,4,3,28048,,,0,"Enable buffering to file for Monasca logs

This enables buffering to file, rather than memory for Monasca logs.
A dedicated docker volume is used for the file buffer. If a post
to the Monasca Log API fails, retries will be made using an exponential
backoff algorithm with a maximum retry interval of 30mins. The maximum
interval is set relatively low to try and reduce the risk of large
buffers accumulating, and therefore the risk of overloading the Monasca
Log API.

Closes-Bug: #1855700
Change-Id: Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec
(cherry picked from commit 5293b1294f5a67107f247f462840ef145bf2ef44)
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/81/671081/1 && git format-patch -1 --stdout FETCH_HEAD,"['ansible/roles/common/templates/conf/output/02-monasca.conf.j2', 'ansible/roles/common/templates/conf/output/00-local.conf.j2']",2,ddb4a0817d1f6ceef18d4f8eb2f3ebeeb74b7948,, buffer_type file buffer_path /var/lib/fluentd/data/syslog-swift-monasca.*.buffer max_retry_wait 1800s buffer_type file buffer_path /var/lib/fluentd/data/syslog-haproxy-monasca.*.buffer max_retry_wait 1800s,,9,0
openstack%2Fkolla-ansible~stable%2Frocky~Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec,openstack/kolla-ansible,stable/rocky,Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec,Enable buffering to file for Monasca logs,ABANDONED,2020-03-11 13:35:18.000000000,2020-03-20 09:56:57.000000000,,"[{'_account_id': 17669}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-11 13:35:18.000000000', 'files': ['ansible/roles/common/templates/conf/output/02-monasca.conf.j2', 'ansible/roles/common/templates/conf/output/00-local.conf.j2', 'releasenotes/notes/enable-buffering-to-file-for-monasca-logs-88ca66cc4d6cda3b.yaml'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/4cabea0ebbd5cf52bd9a93cc905917a91a423de0', 'message': 'Enable buffering to file for Monasca logs\n\nThis enables buffering to file, rather than memory for Monasca logs.\nA dedicated docker volume is used for the file buffer. If a post\nto the Monasca Log API fails, retries will be made using an exponential\nbackoff algorithm with a maximum retry interval of 30mins. The maximum\ninterval is set relatively low to try and reduce the risk of large\nbuffers accumulating, and therefore the risk of overloading the Monasca\nLog API.\n\nCloses-Bug: #1855700\nChange-Id: Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec\n(cherry picked from commit 5293b1294f5a67107f247f462840ef145bf2ef44)\n'}]",0,712427,4cabea0ebbd5cf52bd9a93cc905917a91a423de0,4,2,1,14826,,,0,"Enable buffering to file for Monasca logs

This enables buffering to file, rather than memory for Monasca logs.
A dedicated docker volume is used for the file buffer. If a post
to the Monasca Log API fails, retries will be made using an exponential
backoff algorithm with a maximum retry interval of 30mins. The maximum
interval is set relatively low to try and reduce the risk of large
buffers accumulating, and therefore the risk of overloading the Monasca
Log API.

Closes-Bug: #1855700
Change-Id: Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec
(cherry picked from commit 5293b1294f5a67107f247f462840ef145bf2ef44)
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/27/712427/1 && git format-patch -1 --stdout FETCH_HEAD,"['ansible/roles/common/templates/conf/output/02-monasca.conf.j2', 'ansible/roles/common/templates/conf/output/00-local.conf.j2', 'releasenotes/notes/enable-buffering-to-file-for-monasca-logs-88ca66cc4d6cda3b.yaml']",3,4cabea0ebbd5cf52bd9a93cc905917a91a423de0,bug/1855702-stable/rocky,--- features: - | Fluentd now buffers logs locally to file when the Monasca API is unreachable. ,,14,0
openstack%2Fkolla-ansible~stable%2Ftrain~Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec,openstack/kolla-ansible,stable/train,Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec,Enable buffering to file for Monasca logs,ABANDONED,2020-03-11 13:35:03.000000000,2020-03-20 09:56:52.000000000,,"[{'_account_id': 17669}, {'_account_id': 22348}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-11 13:35:03.000000000', 'files': ['ansible/roles/common/templates/conf/output/02-monasca.conf.j2', 'ansible/roles/common/templates/conf/output/00-local.conf.j2', 'releasenotes/notes/enable-buffering-to-file-for-monasca-logs-88ca66cc4d6cda3b.yaml'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/3c8c498e76f1f2485c7880230bf50fa11d7d9ab4', 'message': 'Enable buffering to file for Monasca logs\n\nThis enables buffering to file, rather than memory for Monasca logs.\nA dedicated docker volume is used for the file buffer. If a post\nto the Monasca Log API fails, retries will be made using an exponential\nbackoff algorithm with a maximum retry interval of 30mins. The maximum\ninterval is set relatively low to try and reduce the risk of large\nbuffers accumulating, and therefore the risk of overloading the Monasca\nLog API.\n\nCloses-Bug: #1855700\nChange-Id: Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec\n(cherry picked from commit 5293b1294f5a67107f247f462840ef145bf2ef44)\n'}]",0,712426,3c8c498e76f1f2485c7880230bf50fa11d7d9ab4,4,3,1,14826,,,0,"Enable buffering to file for Monasca logs

This enables buffering to file, rather than memory for Monasca logs.
A dedicated docker volume is used for the file buffer. If a post
to the Monasca Log API fails, retries will be made using an exponential
backoff algorithm with a maximum retry interval of 30mins. The maximum
interval is set relatively low to try and reduce the risk of large
buffers accumulating, and therefore the risk of overloading the Monasca
Log API.

Closes-Bug: #1855700
Change-Id: Ib5286e9dbaf2bc92d2f4960b2131223ab5dbdbec
(cherry picked from commit 5293b1294f5a67107f247f462840ef145bf2ef44)
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/26/712426/1 && git format-patch -1 --stdout FETCH_HEAD,"['ansible/roles/common/templates/conf/output/02-monasca.conf.j2', 'ansible/roles/common/templates/conf/output/00-local.conf.j2', 'releasenotes/notes/enable-buffering-to-file-for-monasca-logs-88ca66cc4d6cda3b.yaml']",3,3c8c498e76f1f2485c7880230bf50fa11d7d9ab4,bug/1855702-stable/train,--- features: - | Fluentd now buffers logs locally to file when the Monasca API is unreachable. ,,14,0
openstack%2Ftripleo-common~master~I2fc2072f0ab8b761a2b8079f746c3b91cacc8733,openstack/tripleo-common,master,I2fc2072f0ab8b761a2b8079f746c3b91cacc8733,Refactor registry request actions,MERGED,2020-03-19 15:51:28.000000000,2020-03-20 09:55:52.000000000,2020-03-20 01:21:18.000000000,"[{'_account_id': 3153}, {'_account_id': 7353}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 15:51:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/6df0d1649b0f50642acfa5730ec1162557f8a99a', 'message': 'Refactor registry request actions\n\nIn order to handle re-authentication at request time rather than\nletting tenancity trigger it, this change creates a session helper class\nthat wraps the various get/patch/push/put actions that we use in the\nimage uploader to perform a single re-autentication action when we get a\n401 from a registry.  Previously we would let the reauthentication occur\nwhen tenancity would retry various functions.  There are several\nfunctions which perform muiltple requests that could exceed the TTL for\nauthentication tokens. This leads to failures that could have been\nprevented if we re-auth at the time of the request rather than retrying\nthe entire action.\n\nChange-Id: I2fc2072f0ab8b761a2b8079f746c3b91cacc8733\nRelated-Bug: #1867981\n'}, {'number': 2, 'created': '2020-03-19 15:53:31.000000000', 'files': ['tripleo_common/tests/image/test_image_uploader.py', 'tripleo_common/image/image_uploader.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/19dba3f385139ff1e37a9561d62b5f4d9eccedac', 'message': 'Refactor registry request actions\n\nIn order to handle re-authentication at request time rather than\nletting tenancity trigger it, this change creates a session helper class\nthat wraps the various get/patch/push/put actions that we use in the\nimage uploader to perform a single re-autentication action when we get a\n401 from a registry.  Previously we would let the reauthentication occur\nwhen tenancity would retry various functions.  There are several\nfunctions which perform muiltple requests that could exceed the TTL for\nauthentication tokens. This leads to failures that could have been\nprevented if we re-auth at the time of the request rather than retrying\nthe entire action.\n\nChange-Id: I2fc2072f0ab8b761a2b8079f746c3b91cacc8733\nRelated-Bug: #1867981\n'}]",0,713918,19dba3f385139ff1e37a9561d62b5f4d9eccedac,14,5,2,14985,,,0,"Refactor registry request actions

In order to handle re-authentication at request time rather than
letting tenancity trigger it, this change creates a session helper class
that wraps the various get/patch/push/put actions that we use in the
image uploader to perform a single re-autentication action when we get a
401 from a registry.  Previously we would let the reauthentication occur
when tenancity would retry various functions.  There are several
functions which perform muiltple requests that could exceed the TTL for
authentication tokens. This leads to failures that could have been
prevented if we re-auth at the time of the request rather than retrying
the entire action.

Change-Id: I2fc2072f0ab8b761a2b8079f746c3b91cacc8733
Related-Bug: #1867981
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/18/713918/2 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_common/tests/image/test_image_uploader.py', 'tripleo_common/image/image_uploader.py']",2,6df0d1649b0f50642acfa5730ec1162557f8a99a,bug/1867981,"class RegistrySessionHelper(object): """""" Class with various registry session helpers This class contains a bunch of static methods to be used when making session requests against a container registry. The methods are primarily used to handle authentication/reauthentication for the requests against registries that require auth. """""" @staticmethod def check_status(session, request, allow_reauth=True): """""" Check request status and trigger reauth This function can be used to check if we need to perform authentication for a container registry request because we've gotten a 401. """""" hash_request_id = hashlib.sha1(str(request.url).encode()) request_id = hash_request_id.hexdigest() text = getattr(request, 'text', 'unknown') reason = getattr(request, 'reason', 'unknown') status_code = getattr(request, 'status_code', None) headers = getattr(request, 'headers', {}) session_headers = getattr(session, 'headers', {}) if status_code >= 300: LOG.info( 'Non-2xx: id {}, status {}, reason {}, text {}'.format( request_id, status_code, reason, text ) ) if status_code == 401: LOG.warning( 'Failure: id {}, status {}, reason {} text {}'.format( request_id, status_code, reason, text ) ) LOG.debug( 'Request headers after 401: id {}, headers {}'.format( request_id, headers ) ) LOG.debug( 'Session headers after 401: id {}, headers {}'.format( request_id, session_headers ) ) www_auth = headers.get( 'www-authenticate', headers.get( 'Www-Authenticate' ) ) if www_auth: error = None # Handle docker.io shenanigans. docker.io will return 401 # for 403 and 404 but provide an error string. Other registries # like registry.redhat.io and quay.io do not do this. So if # we find an error string, check to see if we should reauth. do_reauth = allow_reauth if 'error=' in www_auth: error = re.search('error=""(.*?)""', www_auth).group(1) LOG.warning( 'Error detected in auth headers: error {}'.format( error ) ) do_reauth = (error == 'invalid_token' and allow_reauth) if do_reauth: if hasattr(session, 'reauthenticate'): reauth = int(session.headers.get('_TripleOReAuth', 0)) reauth += 1 session.headers['_TripleOReAuth'] = str(reauth) LOG.warning( 'Re-authenticating: id {}, count {}'.format( request_id, reauth ) ) session.reauthenticate(**session.auth_args) request.raise_for_status() @staticmethod def _action(action, request_session, *args, **kwargs): """""" Perform a get and retry if auth fails This function dynamically performs a specific type of call using the provided session (get, patch, post, etc). It will attempt a single re-authentication if the initial request fails with a 401. """""" _action = getattr(request_session, action) try: req = _action(*args, **kwargs) RegistrySessionHelper.check_status(session=request_session, request=req) except requests.exceptions.HTTPError as e: if e.response.status_code == 401: req = _action(*args, **kwargs) RegistrySessionHelper.check_status(session=request_session, request=req) else: raise return req @staticmethod def get(request_session, *args, **kwargs): """""" Perform a get and retry if auth fails This function is designed to be used when we perform a get to an authenticated source. This function will attempt a single re-authentication request if the first one fails. """""" return RegistrySessionHelper._action('get', request_session, *args, **kwargs) @staticmethod def patch(request_session, *args, **kwargs): """""" Perform a put and retry if auth fails This function is designed to be used when we perform a path to an authenticated source. This function will attempt a single re-authentication request if the first one fails. """""" return RegistrySessionHelper._action('patch', request_session, *args, **kwargs) @staticmethod def post(request_session, *args, **kwargs): """""" Perform a post and retry if auth fails This function is designed to be used when we perform a post to an authenticated source. This function will attempt a single re-authentication request if the first one fails. """""" return RegistrySessionHelper._action('post', request_session, *args, **kwargs) @staticmethod def put(request_session, *args, **kwargs): """""" Perform a put and retry if auth fails This function is designed to be used when we perform a put to an authenticated source. This function will attempt a single re-authentication request if the first one fails. """""" return RegistrySessionHelper._action('put', request_session, *args, **kwargs) try: manifest_r = RegistrySessionHelper.get( session, manifest_url, headers=manifest_headers, timeout=30 ) except requests.exceptions.HTTPError as e: if e.response.status_code in (403, 404): raise ImageNotFoundException('Not found image: %s' % image_url.geturl()) else: raise tags_r = RegistrySessionHelper.get(session, tags_url, timeout=30) config_r = RegistrySessionHelper.get( session, config_url, headers=config_headers, timeout=30 ) r = RegistrySessionHelper.post(session, url, data=data, timeout=30) try: RegistrySessionHelper.post( session, upload_req_url, timeout=30 ) except requests.exceptions.HTTPError as e: if e.response.status_code in (501, 403, 404, 405): cls.export_registries.add(image_url.netloc) return True else: raise try: r = RegistrySessionHelper.get( session, url, headers=manifest_headers, timeout=30 ) except requests.exceptions.HTTPError as e: if e.response.status_code in (403, 404): raise ImageNotFoundException('Not found image: %s' % url) else: raise r = RegistrySessionHelper.post( session, upload_req_url, timeout=30 ) RegistrySessionHelper.check_status(session=session, request=blob_req) r = RegistrySessionHelper.get( source_session, source_config_url, timeout=30 ) r = RegistrySessionHelper.put( target_session, try: r = RegistrySessionHelper.put( target_session, manifest_url, timeout=30, data=manifest_str.encode('utf-8'), headers={ 'Content-Type': manifest_type } ) except requests.exceptions.HTTPError as e: if e.response.status_code == 400: LOG.error(cls._get_response_text(r)) raise ImageUploaderException('Pushing manifest failed') else: raise upload_resp = RegistrySessionHelper.patch( session, upload_resp = RegistrySessionHelper.put( session,"," @staticmethod def check_status(session, request, allow_reauth=True): hash_request_id = hashlib.sha1(str(request.url).encode()) request_id = hash_request_id.hexdigest() text = getattr(request, 'text', 'unknown') reason = getattr(request, 'reason', 'unknown') status_code = getattr(request, 'status_code', None) headers = getattr(request, 'headers', {}) session_headers = getattr(session, 'headers', {}) if status_code >= 300: LOG.info( 'Non-2xx: id {}, status {}, reason {}, text {}'.format( request_id, status_code, reason, text ) ) if status_code == 401: LOG.warning( 'Failure: id {}, status {}, reason {} text {}'.format( request_id, status_code, reason, text ) ) LOG.debug( 'Request headers after 401: id {}, headers {}'.format( request_id, headers ) ) LOG.debug( 'Session headers after 401: id {}, headers {}'.format( request_id, session_headers ) ) www_auth = headers.get( 'www-authenticate', headers.get( 'Www-Authenticate' ) ) if www_auth: error = None # Handle docker.io shenanigans. docker.io will return 401 # for 403 and 404 but provide an error string. Other registries # like registry.redhat.io and quay.io do not do this. So if # we find an error string, check to see if we should reauth. do_reauth = allow_reauth if 'error=' in www_auth: error = re.search('error=""(.*?)""', www_auth).group(1) LOG.warning( 'Error detected in auth headers: error {}'.format( error ) ) do_reauth = (error == 'invalid_token' and allow_reauth) if do_reauth: if hasattr(session, 'reauthenticate'): reauth = int(session.headers.get('_TripleOReAuth', 0)) reauth += 1 session.headers['_TripleOReAuth'] = str(reauth) LOG.warning( 'Re-authenticating: id {}, count {}'.format( request_id, reauth ) ) session.reauthenticate(**session.auth_args) request.raise_for_status() manifest_r = session.get(manifest_url, headers=manifest_headers, timeout=30) if manifest_r.status_code in (403, 404): raise ImageNotFoundException('Not found image: %s' % image_url.geturl()) cls.check_status(session=session, request=manifest_r) tags_r = session.get(tags_url, timeout=30) cls.check_status(session=session, request=tags_r) config_r = session.get(config_url, headers=config_headers, timeout=30) cls.check_status(session=session, request=config_r) r = session.post(url, data=data, timeout=30) cls.check_status(session=session, request=r) r = session.post(upload_req_url, timeout=30) if r.status_code in (501, 403, 404, 405): cls.export_registries.add(image_url.netloc) return True cls.check_status(session=session, request=r) r = session.get(url, headers=manifest_headers, timeout=30) if r.status_code in (403, 404): raise ImageNotFoundException('Not found image: %s' % url) cls.check_status(session=session, request=r) r = session.post(upload_req_url, timeout=30) cls.check_status(session=session, request=r) cls.check_status(session=session, request=blob_req) # Because the image layer fetching can exceed the auth # token lifetime, we may have a bad token here and don't want # to retry all of the layer fetching to just fetch the config # data. Let's try a single retry here (as check_status with # reauth by default). try: r = source_session.get(source_config_url, timeout=30) cls.check_status( session=source_session, request=r ) except requests.exceptions.HTTPError as e: LOG.debug('[%s] Config fetch failed, retrying: %s' % (image, source_config_url)) if e.response.status_code == 401: # check_status should have reauthed so try on more # time and raise again if we still have problems. r = source_session.get(source_config_url, timeout=30) cls.check_status( session=source_session, request=r ) else: raise r = target_session.put( cls.check_status(session=target_session, request=r) r = target_session.put( manifest_url, timeout=30, data=manifest_str.encode('utf-8'), headers={ 'Content-Type': manifest_type } ) if r.status_code == 400: LOG.error(cls._get_response_text(r)) raise ImageUploaderException('Pushing manifest failed') cls.check_status(session=target_session, request=r) upload_resp = session.patch( cls.check_status(session=session, request=upload_resp) upload_resp = session.put( cls.check_status(session=session, request=upload_resp)",400,189
openstack%2Fopenstack-zuul-jobs~master~I5d5b84a67f0bb87de923d53326676d70f5fe8b16,openstack/openstack-zuul-jobs,master,I5d5b84a67f0bb87de923d53326676d70f5fe8b16,Add add-build-sshkey tests,ABANDONED,2019-02-11 04:31:48.000000000,2020-03-20 09:51:24.000000000,,"[{'_account_id': 7069}, {'_account_id': 22348}]","[{'number': 1, 'created': '2019-02-11 04:31:48.000000000', 'files': ['zuul.d/jobs.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-zuul-jobs/commit/443d9751c50ec8d6a24d83086335e2ee5265282f', 'message': 'Add add-build-sshkey tests\n\nChange-Id: I5d5b84a67f0bb87de923d53326676d70f5fe8b16\nDepends-On: https://review.openstack.org/632620\n'}]",0,636085,443d9751c50ec8d6a24d83086335e2ee5265282f,4,2,1,9311,,,0,"Add add-build-sshkey tests

Change-Id: I5d5b84a67f0bb87de923d53326676d70f5fe8b16
Depends-On: https://review.openstack.org/632620
",git fetch https://review.opendev.org/openstack/openstack-zuul-jobs refs/changes/85/636085/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/jobs.yaml'],1,443d9751c50ec8d6a24d83086335e2ee5265282f,build-sshkey-static-node, - ^roles/add-build-sshkey/.* - ^roles/add-build-sshkey/.*,,2,0
openstack%2Frpm-packaging~master~Iaf45fb9d96b9b62220a8a21a8d65be66378e2507,openstack/rpm-packaging,master,Iaf45fb9d96b9b62220a8a21a8d65be66378e2507,heat-dashboard: fix python3 reqs for -test subpackage,MERGED,2020-02-20 10:53:38.000000000,2020-03-20 09:49:40.000000000,2020-02-20 13:20:52.000000000,"[{'_account_id': 6593}, {'_account_id': 19648}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-02-20 10:53:38.000000000', 'files': ['openstack/heat-dashboard/heat-dashboard.spec.j2'], 'web_link': 'https://opendev.org/openstack/rpm-packaging/commit/9787a667a9944544a033889f0eaca87c117f08c9', 'message': 'heat-dashboard: fix python3 reqs for -test subpackage\n\nThey were still using the Python 2 version of the packages, instead\nof the Python 3 one.\n\nChange-Id: Iaf45fb9d96b9b62220a8a21a8d65be66378e2507\n'}]",0,708848,9787a667a9944544a033889f0eaca87c117f08c9,9,4,1,13294,,,0,"heat-dashboard: fix python3 reqs for -test subpackage

They were still using the Python 2 version of the packages, instead
of the Python 3 one.

Change-Id: Iaf45fb9d96b9b62220a8a21a8d65be66378e2507
",git fetch https://review.opendev.org/openstack/rpm-packaging refs/changes/48/708848/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/heat-dashboard/heat-dashboard.spec.j2'],1,9787a667a9944544a033889f0eaca87c117f08c9,remove-python2,Requires: {{ py3('mock') }} Requires: {{ py3('testtools') }},Requires: {{ py2pkg('mock') }} Requires: {{ py2pkg('testtools') }},2,2
openstack%2Fopenstack-zuul-jobs~master~I4fa75ddcf13876326454ed7158b5bd1549429ad2,openstack/openstack-zuul-jobs,master,I4fa75ddcf13876326454ed7158b5bd1549429ad2,DNM: test fetch-zuul-cloner on base-test,ABANDONED,2017-09-28 22:28:21.000000000,2020-03-20 09:49:33.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2017-09-28 22:28:21.000000000', 'files': ['zuul.d/jobs.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-zuul-jobs/commit/9c3f01120d72386f53fe77945b5d4308b7b65ff0', 'message': 'DNM: test fetch-zuul-cloner on base-test\n\nChange-Id: I4fa75ddcf13876326454ed7158b5bd1549429ad2\n'}]",0,508338,9c3f01120d72386f53fe77945b5d4308b7b65ff0,4,1,1,1,,,0,"DNM: test fetch-zuul-cloner on base-test

Change-Id: I4fa75ddcf13876326454ed7158b5bd1549429ad2
",git fetch https://review.opendev.org/openstack/openstack-zuul-jobs refs/changes/38/508338/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/jobs.yaml'],1,9c3f01120d72386f53fe77945b5d4308b7b65ff0,, parent: base-test,,1,0
openstack%2Foperations-guide~master~I6e452910434766718981f46705569339f3d1d66f,openstack/operations-guide,master,I6e452910434766718981f46705569339f3d1d66f,Imported Translations from Zanata,MERGED,2020-03-20 09:01:22.000000000,2020-03-20 09:44:21.000000000,2020-03-20 09:42:32.000000000,"[{'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-20 09:01:22.000000000', 'files': ['doc/source/locale/de/LC_MESSAGES/doc-common.po', 'doc/source/locale/id/LC_MESSAGES/doc-common.po', 'doc/source/locale/ja/LC_MESSAGES/doc-common.po'], 'web_link': 'https://opendev.org/openstack/operations-guide/commit/b43b9ddb93cb22ce504b2b3e824032d3ae062096', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I6e452910434766718981f46705569339f3d1d66f\n'}]",0,714052,b43b9ddb93cb22ce504b2b3e824032d3ae062096,7,2,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: I6e452910434766718981f46705569339f3d1d66f
",git fetch https://review.opendev.org/openstack/operations-guide refs/changes/52/714052/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/locale/de/LC_MESSAGES/doc-common.po', 'doc/source/locale/id/LC_MESSAGES/doc-common.po', 'doc/source/locale/ja/LC_MESSAGES/doc-common.po']",3,b43b9ddb93cb22ce504b2b3e824032d3ae062096,zanata/translations,"""POT-Creation-Date: 2020-03-07 09:43+0000\n""","""POT-Creation-Date: 2019-08-23 12:35+0000\n""# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary msgid """" ""An authentication and authorization service for Object Storage, implemented "" ""through WSGI middleware; uses Object Storage itself as the persistent "" ""backing store."" msgstr """" ""Object Storage の認証と認可のサービス。WSGI ミドルウェア経由で実装される。"" ""バックエンドの永続的なデータストアとして、Object Storage 自身を使用する。"" # auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/install-guide/source/locale/install-guide# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary""Each OpenStack release has a code name. Code names ascend in alphabetical "" ""order: Austin, Bexar, Cactus, Diablo, Essex, Folsom, Grizzly, Havana, "" ""Icehouse, Juno, Kilo, Liberty, Mitaka, Newton, Ocata, Pike, Queens, Rocky, "" ""Stein, and Train. Code names are cities or counties near where the "" ""corresponding OpenStack design summit took place. An exception, called the "" ""Waldon exception, is granted to elements of the state flag that sound "" ""especially cool. Code names are chosen by popular vote."" msgstr """" ""各 OpenStack リリースはコード名を持つ。コード名はアルファベット順である。"" ""Austin, Bexar, Cactus, Diablo, Essex, Folsom, Grizzly, Havana, Icehouse, "" ""Juno, Kilo, Liberty, Mitaka, Newton, Ocata, Pike, Queens, Rocky, Stein, "" ""Train。コード名は、対応する OpenStack デザインサミットが開催された場所の近く"" ""にある都市または国である。唯一の例外は Waldon 例外と呼ばれる例外で、州の旗の"" ""一部であり、非常に格好いい響きだったことから認められた。コード名は、一般の投"" ""票により選択される。"" # auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary msgid """"# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: horizon, version: stable-rocky, DocId: openstack_dashboard/locale/django# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: horizon, version: master, DocId: openstack_dashboard/locale/django# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/arch-design/source/locale/arch-design# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-user-survey, version: openstack-user-survey, DocId: survey_template# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-user-survey, version: openstack-user-survey, DocId: survey_template# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/ha-guide/source/locale/ha-guide# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-newton, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/source/locale/doc# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/admin-guide/source/locale/admin-guide# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: stable-liberty, DocId: doc/common-rst/source/locale/common-rst# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/source/locale/doc# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: nova, version: master, DocId: nova/locale/nova# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master_2016, DocId: doc/openstack-ops/locale/openstack-ops# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common msgid ""swauth"" msgstr ""swauth"" # auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: openstack-manuals, version: master, DocId: doc/common/source/locale/common# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary# auto translated by TM merge from project: operations-guide, version: master, DocId: doc/glossary/locale/glossary",68,1640
openstack%2Fdevstack~master~I1222a7e1ff706afdf3b2628e5f7ea3f357737b86,openstack/devstack,master,I1222a7e1ff706afdf3b2628e5f7ea3f357737b86,DNM: test fedora build for /opt/cache/issues,ABANDONED,2020-03-19 15:28:28.000000000,2020-03-20 09:38:34.000000000,,"[{'_account_id': 22348}, {'_account_id': 30491}]","[{'number': 1, 'created': '2020-03-19 15:28:28.000000000', 'files': ['lib/neutron_plugins/ovs_base'], 'web_link': 'https://opendev.org/openstack/devstack/commit/64701a9965b3fee26a3cc0d5bd4f5ed95d6a67dc', 'message': 'DNM: test fedora build for /opt/cache/issues\n\nChange-Id: I1222a7e1ff706afdf3b2628e5f7ea3f357737b86\n'}]",0,713902,64701a9965b3fee26a3cc0d5bd4f5ed95d6a67dc,3,2,1,21798,,,0,"DNM: test fedora build for /opt/cache/issues

Change-Id: I1222a7e1ff706afdf3b2628e5f7ea3f357737b86
",git fetch https://review.opendev.org/openstack/devstack refs/changes/02/713902/1 && git format-patch -1 --stdout FETCH_HEAD,['lib/neutron_plugins/ovs_base'],1,64701a9965b3fee26a3cc0d5bd4f5ed95d6a67dc,bug/1868076, # Added comment,,1,0
openstack%2Ftripleo-heat-templates~master~Ifae2f25b55aae6cac48b3b5e8803c74dc00fe448,openstack/tripleo-heat-templates,master,Ifae2f25b55aae6cac48b3b5e8803c74dc00fe448,DNM: is a test change to exercise https://review.opendev.org/#/c/711333/      that change introduces ovn_nb_connection in a way that does not break      scenario010,ABANDONED,2020-03-09 13:45:24.000000000,2020-03-20 09:38:34.000000000,,"[{'_account_id': 6926}, {'_account_id': 11952}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-09 13:45:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/86e2b189914de3fd33146425433052346a8f3a5c', 'message': 'DNM: is a test change to exercise https://review.opendev.org/#/c/711333/\n     that change introduces ovn_nb_connection in a way that does not break\n     scenario010\n\nDepends-On: https://review.opendev.org/#/c/711333/\nChange-Id: Ifae2f25b55aae6cac48b3b5e8803c74dc00fe448\nCo-Authored-By: Brent Eagles <beagles@redhat.com>\nRelated-Bug: #1861886\n'}, {'number': 2, 'created': '2020-03-17 17:31:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/2488059d0a397fb8e22623eba1271cfa48fd762c', 'message': 'DNM: is a test change to exercise https://review.opendev.org/#/c/711333/\n     that change introduces ovn_nb_connection in a way that does not break\n     scenario010\n\nDepends-On: https://review.opendev.org/#/c/711333/\nChange-Id: Ifae2f25b55aae6cac48b3b5e8803c74dc00fe448\nCo-Authored-By: Brent Eagles <beagles@redhat.com>\nRelated-Bug: #1861886\n'}, {'number': 3, 'created': '2020-03-18 18:41:31.000000000', 'files': ['ci/environments/scenario010-standalone.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/019c84de3711837ddbfabe1bfa462379f37ba2a3', 'message': 'DNM: is a test change to exercise https://review.opendev.org/#/c/711333/\n     that change introduces ovn_nb_connection in a way that does not break\n     scenario010\n\nDepends-On: https://review.opendev.org/#/c/705728/\nDepends-On: https://review.opendev.org/#/c/711333/\n\nChange-Id: Ifae2f25b55aae6cac48b3b5e8803c74dc00fe448\nRelated-Bug: #1861886\n'}]",0,711921,019c84de3711837ddbfabe1bfa462379f37ba2a3,14,4,3,11952,,,0,"DNM: is a test change to exercise https://review.opendev.org/#/c/711333/
     that change introduces ovn_nb_connection in a way that does not break
     scenario010

Depends-On: https://review.opendev.org/#/c/705728/
Depends-On: https://review.opendev.org/#/c/711333/

Change-Id: Ifae2f25b55aae6cac48b3b5e8803c74dc00fe448
Related-Bug: #1861886
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/21/711921/3 && git format-patch -1 --stdout FETCH_HEAD,['ci/environments/scenario010-standalone.yaml'],1,86e2b189914de3fd33146425433052346a8f3a5c,bug/1861886, # DNM: flaviof this is a test change to exercise https://review.opendev.org/#/c/711333/ # that change introduces ovn_nb_connection in a way that does not break # scenario010,,3,0
openstack%2Fcharm-octavia-diskimage-retrofit~master~I80809dbe0ae43a87986b33c902e0eb53563cb90d,openstack/charm-octavia-diskimage-retrofit,master,I80809dbe0ae43a87986b33c902e0eb53563cb90d,Enable Ussuri and Focal,MERGED,2020-03-17 11:31:35.000000000,2020-03-20 09:34:02.000000000,2020-03-20 09:34:02.000000000,"[{'_account_id': 935}, {'_account_id': 13686}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-17 11:31:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-octavia-diskimage-retrofit/commit/3170098376d7c3daaf13f06d0fd1b7c521e1af9c', 'message': 'Enable Ussuri and Focal\n\nAdd support for selecting release of image to use as base for\nretrofitting process, defaults to current LTS.\n\nChange-Id: I80809dbe0ae43a87986b33c902e0eb53563cb90d\n'}, {'number': 2, 'created': '2020-03-17 15:06:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-octavia-diskimage-retrofit/commit/ebad64a8b2a7d5d494a682a6d13fda5e57fba03f', 'message': 'Enable Ussuri and Focal\n\nAdd support for selecting release of image to use as base for\nretrofitting process, defaults to current LTS.\n\nKeep Focal as dev-bundle pending required updates to neighbouring\ncharms.\n\nChange-Id: I80809dbe0ae43a87986b33c902e0eb53563cb90d\n'}, {'number': 3, 'created': '2020-03-17 15:11:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-octavia-diskimage-retrofit/commit/484309f626ff3ba3e00fdb1461674eb192d4a3b1', 'message': 'Enable Ussuri and Focal\n\nAdd support for selecting release of image to use as base for\nretrofitting process, defaults to current LTS.\n\nKeep Focal as dev-bundle pending required updates to neighbouring\ncharms.\n\nUnpin flake8 to allow running lint on Python 3.8 systems.\n\nChange-Id: I80809dbe0ae43a87986b33c902e0eb53563cb90d\n'}, {'number': 4, 'created': '2020-03-17 16:07:13.000000000', 'files': ['src/tests/bundles/eoan.yaml', 'src/tests/bundles/focal.yaml', 'test-requirements.txt', 'src/metadata.yaml', 'unit_tests/test_reactive_octavia_diskimage_retrofit_handlers.py', 'src/reactive/octavia_diskimage_retrofit_handlers.py', 'src/tests/tests.yaml', 'src/tests/bundles/bionic-train.yaml', 'src/tests/bundles/bionic-ussuri.yaml', 'src/config.yaml', 'src/lib/charm/openstack/octavia_diskimage_retrofit.py', 'unit_tests/test_lib_charm_openstack_glance_retrofitter.py', 'src/tests/bundles/bionic-stein.yaml', 'unit_tests/test_lib_charm_openstack_octavia_diskimage_retrofit.py', 'src/lib/charm/openstack/glance_retrofitter.py', 'src/tests/bundles/bionic-rocky.yaml'], 'web_link': 'https://opendev.org/openstack/charm-octavia-diskimage-retrofit/commit/c3b3efc05c41f4bdf519e14e92928c87e33a61ed', 'message': 'Enable Ussuri and Focal\n\nAdd support for selecting release of image to use as base for\nretrofitting process, defaults to current LTS.\n\nKeep Focal as dev-bundle pending required updates to neighbouring\ncharms.\n\nUnpin flake8 to allow running lint on Python 3.8 systems.\n\nChange-Id: I80809dbe0ae43a87986b33c902e0eb53563cb90d\n'}]",0,713409,c3b3efc05c41f4bdf519e14e92928c87e33a61ed,16,4,4,13686,,,0,"Enable Ussuri and Focal

Add support for selecting release of image to use as base for
retrofitting process, defaults to current LTS.

Keep Focal as dev-bundle pending required updates to neighbouring
charms.

Unpin flake8 to allow running lint on Python 3.8 systems.

Change-Id: I80809dbe0ae43a87986b33c902e0eb53563cb90d
",git fetch https://review.opendev.org/openstack/charm-octavia-diskimage-retrofit refs/changes/09/713409/4 && git format-patch -1 --stdout FETCH_HEAD,"['src/tests/bundles/eoan.yaml', 'src/tests/bundles/focal.yaml', 'test-requirements.txt', 'src/metadata.yaml', 'src/tests/tests.yaml', 'src/tests/bundles/bionic-train.yaml', 'src/tests/bundles/bionic-ussuri.yaml', 'src/config.yaml', 'src/lib/charm/openstack/octavia_diskimage_retrofit.py', 'unit_tests/test_lib_charm_openstack_glance_retrofitter.py', 'src/tests/bundles/bionic-stein.yaml', 'unit_tests/test_lib_charm_openstack_octavia_diskimage_retrofit.py', 'src/lib/charm/openstack/glance_retrofitter.py', 'src/tests/bundles/bionic-rocky.yaml']",14,3170098376d7c3daaf13f06d0fd1b7c521e1af9c,enable-ussuri, options: retrofit-release: 18.04 retrofit-uca-pocket: rocky,,292,47
openstack%2Fnova~master~I678722b3cf295c89110967d5ad8c0c964df4cb42,openstack/nova,master,I678722b3cf295c89110967d5ad8c0c964df4cb42,Support unshelve with qos ports,MERGED,2020-01-29 09:48:19.000000000,2020-03-20 09:31:42.000000000,2020-03-20 09:29:18.000000000,"[{'_account_id': 4393}, {'_account_id': 9008}, {'_account_id': 9708}, {'_account_id': 10118}, {'_account_id': 14384}, {'_account_id': 14595}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26458}, {'_account_id': 26515}, {'_account_id': 29963}, {'_account_id': 30112}]","[{'number': 1, 'created': '2020-01-29 09:48:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b040579531ea6ce6738f9b0c0aae2d5af695b904', 'message': 'Support unshelve with qos ports\n\nTODO:\n* finish func test coverage\n* unit test\n\nChange-Id: I678722b3cf295c89110967d5ad8c0c964df4cb42\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 2, 'created': '2020-02-03 10:55:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/0e11be7771427b049be62ea58e06e88916189a9f', 'message': 'Support unshelve with qos ports\n\nTODO:\n* finish func test coverage\n* unit test\n\nChange-Id: I678722b3cf295c89110967d5ad8c0c964df4cb42\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 3, 'created': '2020-02-03 12:36:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/da1f1b5faaa161478edeff90bd5c9d6fb2221f92', 'message': 'Support unshelve with qos ports\n\nThis patch adds support for unshelving an offloaded server with qos ports.\nTo do that this patch:\n* collects the port resource requests from neutron before the scheduler\n  is called to select the target of the unshelve.\n* calculate the request group - provider mapping after the scheduler\n  selected the target host\n* update the InstancePCIRequest to drive the pci_claim to allocate VFs\n  from the same PF as the bandwidth is allocated from by the scheduler\n* update the binding profile of the qos ports to so that the allocation\n  key of the binding profile points to the RPs the port is allocated\n  from.\n\nNote that unshelve does not have a re-schedule loop and all the RPC\nchanges was commited in Queens.\n\nTODO:\n* finish func test coverage\n* unit test\n\nChange-Id: I678722b3cf295c89110967d5ad8c0c964df4cb42\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 4, 'created': '2020-02-03 16:11:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/7df68e69b34d71275e96fe6aa2deaaf99eab3049', 'message': 'Support unshelve with qos ports\n\nThis patch adds support for unshelving an offloaded server with qos ports.\nTo do that this patch:\n* collects the port resource requests from neutron before the scheduler\n  is called to select the target of the unshelve.\n* calculate the request group - provider mapping after the scheduler\n  selected the target host\n* update the InstancePCIRequest to drive the pci_claim to allocate VFs\n  from the same PF as the bandwidth is allocated from by the scheduler\n* update the binding profile of the qos ports to so that the allocation\n  key of the binding profile points to the RPs the port is allocated\n  from.\n\nNote that unshelve does not have a re-schedule loop and all the RPC\nchanges was committed in Queens.\n\nTwo error cases needs special care by rolling back allocations before\nputting the instance back to SHELVED_OFFLOADED state:\n\n* if the IntancePCIRequest cannot be updated according to the new target\nhost of unshelve\n* if updating port binding fails in neutron during unshelve\n\nChange-Id: I678722b3cf295c89110967d5ad8c0c964df4cb42\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 5, 'created': '2020-02-10 09:31:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a95f1cb39d2e5917be9c3253aac0d41bb5524c63', 'message': 'Support unshelve with qos ports\n\nThis patch adds support for unshelving an offloaded server with qos ports.\nTo do that this patch:\n* collects the port resource requests from neutron before the scheduler\n  is called to select the target of the unshelve.\n* calculate the request group - provider mapping after the scheduler\n  selected the target host\n* update the InstancePCIRequest to drive the pci_claim to allocate VFs\n  from the same PF as the bandwidth is allocated from by the scheduler\n* update the binding profile of the qos ports to so that the allocation\n  key of the binding profile points to the RPs the port is allocated\n  from.\n\nNote that unshelve does not have a re-schedule loop and all the RPC\nchanges was committed in Queens.\n\nTwo error cases needs special care by rolling back allocations before\nputting the instance back to SHELVED_OFFLOADED state:\n\n* if the IntancePCIRequest cannot be updated according to the new target\nhost of unshelve\n* if updating port binding fails in neutron during unshelve\n\nChange-Id: I678722b3cf295c89110967d5ad8c0c964df4cb42\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 6, 'created': '2020-02-11 12:24:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a59fd1f5b3a758f849a391734718195b176d830c', 'message': 'Support unshelve with qos ports\n\nThis patch adds support for unshelving an offloaded server with qos ports.\nTo do that this patch:\n* collects the port resource requests from neutron before the scheduler\n  is called to select the target of the unshelve.\n* calculate the request group - provider mapping after the scheduler\n  selected the target host\n* update the InstancePCIRequest to drive the pci_claim to allocate VFs\n  from the same PF as the bandwidth is allocated from by the scheduler\n* update the binding profile of the qos ports to so that the allocation\n  key of the binding profile points to the RPs the port is allocated\n  from.\n\nNote that unshelve does not have a re-schedule loop and all the RPC\nchanges was committed in Queens.\n\nTwo error cases needs special care by rolling back allocations before\nputting the instance back to SHELVED_OFFLOADED state:\n\n* if the IntancePCIRequest cannot be updated according to the new target\nhost of unshelve\n* if updating port binding fails in neutron during unshelve\n\nChange-Id: I678722b3cf295c89110967d5ad8c0c964df4cb42\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 7, 'created': '2020-02-21 15:29:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3db615fd7567a823cae2f2b4ee7abbdd7054df38', 'message': 'Support unshelve with qos ports\n\nThis patch adds support for unshelving an offloaded server with qos ports.\nTo do that this patch:\n* collects the port resource requests from neutron before the scheduler\n  is called to select the target of the unshelve.\n* calculate the request group - provider mapping after the scheduler\n  selected the target host\n* update the InstancePCIRequest to drive the pci_claim to allocate VFs\n  from the same PF as the bandwidth is allocated from by the scheduler\n* update the binding profile of the qos ports to so that the allocation\n  key of the binding profile points to the RPs the port is allocated\n  from.\n\nNote that unshelve does not have a re-schedule loop and all the RPC\nchanges was committed in Queens.\n\nTwo error cases needs special care by rolling back allocations before\nputting the instance back to SHELVED_OFFLOADED state:\n\n* if the IntancePCIRequest cannot be updated according to the new target\nhost of unshelve\n* if updating port binding fails in neutron during unshelve\n\nChange-Id: I678722b3cf295c89110967d5ad8c0c964df4cb42\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 8, 'created': '2020-03-18 10:05:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/889a45bdab039e1dc5bc5b926ec35f6910d3da8e', 'message': 'Support unshelve with qos ports\n\nThis patch adds support for unshelving an offloaded server with qos ports.\nTo do that this patch:\n* collects the port resource requests from neutron before the scheduler\n  is called to select the target of the unshelve.\n* calculate the request group - provider mapping after the scheduler\n  selected the target host\n* update the InstancePCIRequest to drive the pci_claim to allocate VFs\n  from the same PF as the bandwidth is allocated from by the scheduler\n* update the binding profile of the qos ports to so that the allocation\n  key of the binding profile points to the RPs the port is allocated\n  from.\n\nNote that unshelve does not have a re-schedule loop and all the RPC\nchanges was committed in Queens.\n\nTwo error cases needs special care by rolling back allocations before\nputting the instance back to SHELVED_OFFLOADED state:\n\n* if the IntancePCIRequest cannot be updated according to the new target\nhost of unshelve\n* if updating port binding fails in neutron during unshelve\n\nChange-Id: I678722b3cf295c89110967d5ad8c0c964df4cb42\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 9, 'created': '2020-03-18 14:38:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f5a25fd6cdc0dc386ecb3665287ba30fa7aa353b', 'message': 'Support unshelve with qos ports\n\nThis patch adds support for unshelving an offloaded server with qos ports.\nTo do that this patch:\n* collects the port resource requests from neutron before the scheduler\n  is called to select the target of the unshelve.\n* calculate the request group - provider mapping after the scheduler\n  selected the target host\n* update the InstancePCIRequest to drive the pci_claim to allocate VFs\n  from the same PF as the bandwidth is allocated from by the scheduler\n* update the binding profile of the qos ports to so that the allocation\n  key of the binding profile points to the RPs the port is allocated\n  from.\n\nAs this was the last move operation to be supported the compute service\nversion is bumped to indicate such support. This will be used in a later\npatches to implement a global service level check in the API.\n\nNote that unshelve does not have a re-schedule loop and all the RPC\nchanges was committed in Queens.\n\nTwo error cases needs special care by rolling back allocations before\nputting the instance back to SHELVED_OFFLOADED state:\n\n* if the IntancePCIRequest cannot be updated according to the new target\nhost of unshelve\n* if updating port binding fails in neutron during unshelve\n\nChange-Id: I678722b3cf295c89110967d5ad8c0c964df4cb42\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}, {'number': 10, 'created': '2020-03-18 18:10:50.000000000', 'files': ['nova/tests/unit/network/test_neutron.py', 'nova/network/neutron.py', 'nova/objects/service.py', 'nova/tests/unit/conductor/test_conductor.py', 'nova/tests/unit/compute/test_shelve.py', 'nova/conductor/manager.py', 'nova/compute/manager.py', 'nova/tests/functional/test_servers.py', 'nova/tests/unit/compute/test_compute_utils.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/94c7e7ad4333286ac7730048cacf2a6837d1c6ec', 'message': 'Support unshelve with qos ports\n\nThis patch adds support for unshelving an offloaded server with qos ports.\nTo do that this patch:\n* collects the port resource requests from neutron before the scheduler\n  is called to select the target of the unshelve.\n* calculate the request group - provider mapping after the scheduler\n  selected the target host\n* update the InstancePCIRequest to drive the pci_claim to allocate VFs\n  from the same PF as the bandwidth is allocated from by the scheduler\n* update the binding profile of the qos ports to so that the allocation\n  key of the binding profile points to the RPs the port is allocated\n  from.\n\nAs this was the last move operation to be supported the compute service\nversion is bumped to indicate such support. This will be used in a later\npatches to implement a global service level check in the API.\n\nNote that unshelve does not have a re-schedule loop and all the RPC\nchanges was committed in Queens.\n\nTwo error cases needs special care by rolling back allocations before\nputting the instance back to SHELVED_OFFLOADED state:\n\n* if the IntancePCIRequest cannot be updated according to the new target\nhost of unshelve\n* if updating port binding fails in neutron during unshelve\n\nChange-Id: I678722b3cf295c89110967d5ad8c0c964df4cb42\nblueprint: support-move-ops-with-qos-ports-ussuri\n'}]",32,704759,94c7e7ad4333286ac7730048cacf2a6837d1c6ec,142,14,10,9708,,,0,"Support unshelve with qos ports

This patch adds support for unshelving an offloaded server with qos ports.
To do that this patch:
* collects the port resource requests from neutron before the scheduler
  is called to select the target of the unshelve.
* calculate the request group - provider mapping after the scheduler
  selected the target host
* update the InstancePCIRequest to drive the pci_claim to allocate VFs
  from the same PF as the bandwidth is allocated from by the scheduler
* update the binding profile of the qos ports to so that the allocation
  key of the binding profile points to the RPs the port is allocated
  from.

As this was the last move operation to be supported the compute service
version is bumped to indicate such support. This will be used in a later
patches to implement a global service level check in the API.

Note that unshelve does not have a re-schedule loop and all the RPC
changes was committed in Queens.

Two error cases needs special care by rolling back allocations before
putting the instance back to SHELVED_OFFLOADED state:

* if the IntancePCIRequest cannot be updated according to the new target
host of unshelve
* if updating port binding fails in neutron during unshelve

Change-Id: I678722b3cf295c89110967d5ad8c0c964df4cb42
blueprint: support-move-ops-with-qos-ports-ussuri
",git fetch https://review.opendev.org/openstack/nova refs/changes/59/704759/8 && git format-patch -1 --stdout FETCH_HEAD,"['nova/network/neutron.py', 'nova/conductor/manager.py', 'nova/compute/manager.py', 'nova/tests/functional/test_servers.py']",4,b040579531ea6ce6738f9b0c0aae2d5af695b904,bp/support-move-ops-with-qos-ports-ussuri," def _turn_off_api_check(self): # The API actively rejecting the move operations with resource # request so we have to turn off that check. # TODO(gibi): Remove this when the move operations are supported and # the API check is removed. patcher = mock.patch( 'nova.api.openstack.common.' 'supports_port_resource_request_during_move', return_value=True) self.addCleanup(patcher.stop) patcher.start() def _assert_no_allocation_and_no_binding_when_offloaded(self): # TODO: pass def test_unshelve_offloaded_server_with_qos_port(self): # TODO(gibi): remove this when live migration is fully supported and # therefore the check is removed from the api self._turn_off_api_check() non_qos_normal_port = self.neutron.port_1 qos_normal_port = self.neutron.port_with_resource_request qos_sriov_port = self.neutron.port_with_sriov_resource_request server = self._create_server_with_ports_and_check_allocation( non_qos_normal_port, qos_normal_port, qos_sriov_port) # with default config shelve means immediate offload as well req = { 'shelve': {} } self.api.post_server_action(server['id'], req) self._wait_for_server_parameter( server, {'status': 'SHELVED_OFFLOADED'}) self._assert_no_allocation_and_no_binding_when_offloaded() self.api.post_server_action(server['id'], {'unshelve': None}) self._wait_for_server_parameter( server, {'OS-EXT-SRV-ATTR:host': 'host1', 'status': 'ACTIVE'}) self._check_allocation( server, self.compute1_rp_uuid, non_qos_normal_port, qos_normal_port, qos_sriov_port, self.flavor_with_group_policy) self._assert_pci_request_pf_device_name(server, 'host1-ens2') # shelve offload again and then make host1 unusable so the subsequent # unshelve needs to select host2 # with default config shelve means immediate offload as well req = { 'shelve': {} } self.api.post_server_action(server['id'], req) self._wait_for_server_parameter( server, {'status': 'SHELVED_OFFLOADED'}) self._assert_no_allocation_and_no_binding_when_offloaded() self.admin_api.put_service( self.compute1_service_id, {""status"": ""disabled""}) self.api.post_server_action(server['id'], {'unshelve': None}) self._wait_for_server_parameter( server, {'OS-EXT-SRV-ATTR:host': 'host2', 'status': 'ACTIVE'}) self._check_allocation( server, self.compute2_rp_uuid, non_qos_normal_port, qos_normal_port, qos_sriov_port, self.flavor_with_group_policy) self._assert_pci_request_pf_device_name(server, 'host2-ens2') self._delete_server_and_check_allocations( server, qos_normal_port, qos_sriov_port) # unshelve does not have reschedule loop # RPC exteded for in Queens so no old compute check def test_unshelve_offloaded_server_with_qos_port_pci_update_fails(self): # TODO(gibi): remove this when live migration is fully supported and # therefore the check is removed from the api self._turn_off_api_check() # Update the name of the network device RP of PF2 on host2 to something # unexpected. This will cause # update_pci_request_spec_with_allocated_interface_name() to raise # when the instance is live migrated to the host2. rsp = self.placement_api.put( '/resource_providers/%s' % self.sriov_dev_rp_per_host[self.compute2_rp_uuid][self.PF2], {""name"": ""invalid-device-rp-name""}) self.assertEqual(200, rsp.status) non_qos_normal_port = self.neutron.port_1 qos_normal_port = self.neutron.port_with_resource_request qos_sriov_port = self.neutron.port_with_sriov_resource_request server = self._create_server_with_ports_and_check_allocation( non_qos_normal_port, qos_normal_port, qos_sriov_port) # with default config shelve means immediate offload as well req = { 'shelve': {} } self.api.post_server_action(server['id'], req) self._wait_for_server_parameter( server, {'status': 'SHELVED_OFFLOADED'}) self._assert_no_allocation_and_no_binding_when_offloaded() # make host1 unusable so the subsequent unshelve needs to select host2 self.admin_api.put_service( self.compute1_service_id, {""status"": ""disabled""}) server = self._wait_for_server_parameter( server, {'OS-EXT-SRV-ATTR:host': None, 'status': 'SHELVED_OFFLOADED'}) self._assert_no_allocation_and_no_binding_when_offloaded() self._delete_server_and_check_allocations( server, qos_normal_port, qos_sriov_port) ",,159,9
openstack%2Fproject-config~master~I06bff5e368819d9c05513300615b6bf5404b53b6,openstack/project-config,master,I06bff5e368819d9c05513300615b6bf5404b53b6,rename x/osops- to openstack-operators/,ABANDONED,2019-05-30 19:11:16.000000000,2020-03-20 09:28:11.000000000,,"[{'_account_id': 1004}, {'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2019-05-30 19:11:16.000000000', 'files': ['gerritbot/channels.yaml', 'zuul.d/projects.yaml', 'zuul/main.yaml', 'gerrit/projects.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/93aecc800d17740c0e6353ece6bdf727a4be46bc', 'message': 'rename x/osops- to openstack-operators/\n\nChange-Id: I06bff5e368819d9c05513300615b6bf5404b53b6\n'}]",7,662300,93aecc800d17740c0e6353ece6bdf727a4be46bc,7,3,1,1004,,,0,"rename x/osops- to openstack-operators/

Change-Id: I06bff5e368819d9c05513300615b6bf5404b53b6
",git fetch https://review.opendev.org/openstack/project-config refs/changes/00/662300/1 && git format-patch -1 --stdout FETCH_HEAD,"['gerritbot/channels.yaml', 'gerrit/projects.yaml', 'zuul.d/projects.yaml', 'zuul/main.yaml']",4,93aecc800d17740c0e6353ece6bdf727a4be46bc,project-rename, - openstack-operators/coda - openstack-operators/example-configs - openstack-operators/tools-contrib - openstack-operators/tools-generic - openstack-operators/tools-logging - openstack-operators/tools-monitoring, - x/osops-coda - x/osops-example-configs - x/osops-tools-contrib - x/osops-tools-generic - x/osops-tools-logging - x/osops-tools-monitoring,24,24
openstack%2Fproject-config~master~I4015c16ee85ca67357f0dcb10d4bd76f171bb356,openstack/project-config,master,I4015c16ee85ca67357f0dcb10d4bd76f171bb356,Upload artifacts in base-test job,ABANDONED,2017-10-13 14:09:43.000000000,2020-03-20 09:26:47.000000000,,"[{'_account_id': 4162}, {'_account_id': 9061}, {'_account_id': 22348}]","[{'number': 1, 'created': '2017-10-13 14:09:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/f2efeb454d1ed7f0669e7a41b93e5b9ca8a49dd1', 'message': 'Handle basic artifacts in base-test job\n\nDepends-On: Iadb4033f076057859967b8085f209cc4b7da34c1\nChange-Id: I4015c16ee85ca67357f0dcb10d4bd76f171bb356\n'}, {'number': 2, 'created': '2018-07-10 20:30:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/74e85c1814071b34e4f48f197cd5e2d8334868cc', 'message': 'Handle basic artifacts in base-test job\n\nDepends-On: Iadb4033f076057859967b8085f209cc4b7da34c1\nChange-Id: I4015c16ee85ca67357f0dcb10d4bd76f171bb356\n'}, {'number': 3, 'created': '2019-01-09 17:31:06.000000000', 'files': ['playbooks/base-test/post.yaml', 'zuul.d/jobs.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/08c8c7a0ba879d11280eec8d54056d65dc79d2b8', 'message': 'Upload artifacts in base-test job\n\nAdd the artifact upload role to the base-test job. This should\nresult in artifacts being uploaded to the fileserver if any are in\nthe artifacts directory.\n\nThe merge-output-to-logs role will empty out the artifacts dir on\npre-merge jobs, so this should be a no-op for check and gate jobs.\n\nDepends-On: Iadb4033f076057859967b8085f209cc4b7da34c1\nChange-Id: I4015c16ee85ca67357f0dcb10d4bd76f171bb356\n'}]",0,511860,08c8c7a0ba879d11280eec8d54056d65dc79d2b8,10,3,3,2,,,0,"Upload artifacts in base-test job

Add the artifact upload role to the base-test job. This should
result in artifacts being uploaded to the fileserver if any are in
the artifacts directory.

The merge-output-to-logs role will empty out the artifacts dir on
pre-merge jobs, so this should be a no-op for check and gate jobs.

Depends-On: Iadb4033f076057859967b8085f209cc4b7da34c1
Change-Id: I4015c16ee85ca67357f0dcb10d4bd76f171bb356
",git fetch https://review.opendev.org/openstack/project-config refs/changes/60/511860/3 && git format-patch -1 --stdout FETCH_HEAD,"['playbooks/base-test/post-artifacts.yaml', 'zuul.d/jobs.yaml']",2,f2efeb454d1ed7f0669e7a41b93e5b9ca8a49dd1,zuulv3-output, - playbooks/base-test/post-artifacts - site_tarballs,,11,0
openstack%2Fvalidations-libs~master~I642327567e069d2d6947f7e20676accdb027afac,openstack/validations-libs,master,I642327567e069d2d6947f7e20676accdb027afac,DNM: Testing pep8 job,ABANDONED,2020-03-20 09:06:18.000000000,2020-03-20 09:18:09.000000000,,[],"[{'number': 1, 'created': '2020-03-20 09:06:18.000000000', 'files': ['tox.ini', '.pre-commit-config.yaml'], 'web_link': 'https://opendev.org/openstack/validations-libs/commit/ed5fdefbb15e910b4faba881c157889e87843edc', 'message': 'DNM: Testing pep8 job\n\nChange-Id: I642327567e069d2d6947f7e20676accdb027afac\nSigned-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>\n'}]",0,714054,ed5fdefbb15e910b4faba881c157889e87843edc,2,0,1,11491,,,0,"DNM: Testing pep8 job

Change-Id: I642327567e069d2d6947f7e20676accdb027afac
Signed-off-by: Gael Chamoulaud (Strider) <gchamoul@redhat.com>
",git fetch https://review.opendev.org/openstack/validations-libs refs/changes/54/714054/1 && git format-patch -1 --stdout FETCH_HEAD,"['tox.ini', '.pre-commit-config.yaml']",2,ed5fdefbb15e910b4faba881c157889e87843edc,fix_flake8_issue," - repo: https://gitlab.com/pycqa/flake8 rev: 3.7.9 hooks: - id: flake8 #entry: flake8 --ignore=E24,E121,E122,E123,E124,E126,E226,E265,E305,E402,F401,F405,E501,E704,F403,F841,W503,W605"," - id: flake8 #entry: flake8 --ignore=E24,E121,E122,E123,E124,E126,E226,E265,E305,E402,F401,F405,E501,E704,F403,F841,W503,W605",6,3
openstack%2Fproject-config~master~I84dfb8dee2540e0a56e0aee2048ec3eec6affd38,openstack/project-config,master,I84dfb8dee2540e0a56e0aee2048ec3eec6affd38,Allow push and push merge commit for tripleo-quickstart,ABANDONED,2018-09-13 15:30:08.000000000,2020-03-20 09:15:12.000000000,,"[{'_account_id': 4146}, {'_account_id': 22348}]","[{'number': 1, 'created': '2018-09-13 15:30:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/738f6af9213d8bd36ec347798500c9eb69b5d78c', 'message': ""Allow push and push merge commit for tripleo-quickstart\n\ntripleo-quickstart and tripleo-quickstart-extras repositories are going\nto be merged with all their unrelated history.\nWe need push permission so we don't have to upload and approve the 2000\ncommit merged from tripleo-quickstart-extras to gerrit.\nWe are going to push the merge to a dedicated branch, then push a merge\ncommit to master when we are ready to go.\n\nChange-Id: I84dfb8dee2540e0a56e0aee2048ec3eec6affd38\n""}, {'number': 2, 'created': '2018-09-13 17:07:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/31f4fb512657f2acf8a6f45afe4c826af9b5702e', 'message': ""Allow push and push merge commit for tripleo-quickstart\n\ntripleo-quickstart and tripleo-quickstart-extras repositories are going\nto be merged with all their unrelated history.\nWe need push permission so we don't have to upload and approve the 2000\ncommit merged from tripleo-quickstart-extras to gerrit.\nWe are going to push the merge to a dedicated branch, then push a merge\ncommit to master when we are ready to go.\n\nChange-Id: I84dfb8dee2540e0a56e0aee2048ec3eec6affd38\n""}, {'number': 3, 'created': '2018-09-19 11:59:27.000000000', 'files': ['gerrit/acls/openstack/tripleo-quickstart.config'], 'web_link': 'https://opendev.org/openstack/project-config/commit/862bebe4b0a0bb42ddf33531237d25dcacdb2d3c', 'message': ""Allow push and push merge commit for tripleo-quickstart\n\ntripleo-quickstart and tripleo-quickstart-extras repositories are going\nto be merged with all their unrelated history.\nWe need push permission so we don't have to upload and approve the 2000\ncommit merged from tripleo-quickstart-extras to gerrit.\nWe are going to push the merge to a dedicated branch, then push a merge\ncommit to master when we are ready to go.\nThere is no easy way to update the branch without redoing the merge from\nscratch, so the force option is needed to iterate and test on the branch\n\nChange-Id: I84dfb8dee2540e0a56e0aee2048ec3eec6affd38\n""}]",1,602377,862bebe4b0a0bb42ddf33531237d25dcacdb2d3c,8,2,3,10022,,,0,"Allow push and push merge commit for tripleo-quickstart

tripleo-quickstart and tripleo-quickstart-extras repositories are going
to be merged with all their unrelated history.
We need push permission so we don't have to upload and approve the 2000
commit merged from tripleo-quickstart-extras to gerrit.
We are going to push the merge to a dedicated branch, then push a merge
commit to master when we are ready to go.
There is no easy way to update the branch without redoing the merge from
scratch, so the force option is needed to iterate and test on the branch

Change-Id: I84dfb8dee2540e0a56e0aee2048ec3eec6affd38
",git fetch https://review.opendev.org/openstack/project-config refs/changes/77/602377/1 && git format-patch -1 --stdout FETCH_HEAD,['gerrit/acls/openstack/tripleo-quickstart.config'],1,738f6af9213d8bd36ec347798500c9eb69b5d78c,allow-tq-push,pushMerge = group tripleo-quickstart-core push = group tripleo-quickstart-core,,2,0
openstack%2Fswift~feature%2Flosf~I58b26d63649820600808257d8ff3eb14e96fbf10,openstack/swift,feature/losf,I58b26d63649820600808257d8ff3eb14e96fbf10,Make a new timeseries for LOSF object header mismatches,MERGED,2020-03-19 15:47:50.000000000,2020-03-20 08:28:15.000000000,2020-03-20 08:26:21.000000000,"[{'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 15:47:50.000000000', 'files': ['swift/obj/vfile.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/f737857f33718edd4c2a19ceec84400795736f71', 'message': ""Make a new timeseries for LOSF object header mismatches\n\nAdd a new timeseries 'vfile.wrong_object_header_name', to track when\nwe have an object header mismatch: the name in the object header does\nnot match what's in the levelDB database.\n\nTwo cases are excluded from this count:\n  - object header is fully blank\n  - object is a fragment and has been renamed to durable (#d)\n\nThese two cases are known and handled in code.\n\nChange-Id: I58b26d63649820600808257d8ff3eb14e96fbf10\n""}]",0,713913,f737857f33718edd4c2a19ceec84400795736f71,11,2,1,25251,,,0,"Make a new timeseries for LOSF object header mismatches

Add a new timeseries 'vfile.wrong_object_header_name', to track when
we have an object header mismatch: the name in the object header does
not match what's in the levelDB database.

Two cases are excluded from this count:
  - object header is fully blank
  - object is a fragment and has been renamed to durable (#d)

These two cases are known and handled in code.

Change-Id: I58b26d63649820600808257d8ff3eb14e96fbf10
",git fetch https://review.opendev.org/openstack/swift refs/changes/13/713913/1 && git format-patch -1 --stdout FETCH_HEAD,['swift/obj/vfile.py'],1,f737857f33718edd4c2a19ceec84400795736f71,dev/alecuyer/feature-branch-catchup," increment(logger, 'vfile.wrong_object_header_name') ""Wrong object header name. Header: {} Expected:\"," ""Wrong header name. Header: {} Expected:\",2,1
openstack%2Ftripleo-common~master~I469b4f51c57e1651f42064b0134aa3916af72d80,openstack/tripleo-common,master,I469b4f51c57e1651f42064b0134aa3916af72d80,Add libcgroup-tools to libvirt container,MERGED,2020-03-18 09:44:27.000000000,2020-03-20 08:12:19.000000000,2020-03-19 19:35:39.000000000,"[{'_account_id': 360}, {'_account_id': 6926}, {'_account_id': 7144}, {'_account_id': 11082}, {'_account_id': 11090}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-18 09:44:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/722e6b88b4a1f009dbec9d2d03b457f6877dac50', 'message': 'Add libcgroup-tools to libvirt container\n\nTo reuse the tripleo containers when running libvirtd container\non OCP we need to run it via a wrapper script and create our own\ncgroups and run libvirtd using cgexec to break out the default\nOCP cgroup of the pod. Otherwise if the libvirtd pod gets\ndeleted/recreated all VMs get shutdown as OCP cleans up the\nresources owned by the pod.\n\nChange-Id: I469b4f51c57e1651f42064b0134aa3916af72d80\n'}, {'number': 2, 'created': '2020-03-18 11:48:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/bc46f327ab4a8d6df8e45d4e4322709ac7b0a35f', 'message': 'Add libcgroup-tools to libvirt container\n\nTo reuse the tripleo containers when running libvirtd container\non OCP we need to run it via a wrapper script and create our own\ncgroups and run libvirtd using cgexec to break out the default\nOCP cgroup of the pod. Otherwise if the libvirtd pod gets\ndeleted/recreated all VMs get shutdown as OCP cleans up the\nresources owned by the pod.\n\nChange-Id: I469b4f51c57e1651f42064b0134aa3916af72d80\n'}, {'number': 3, 'created': '2020-03-19 12:22:46.000000000', 'files': ['container-images/tripleo_kolla_template_overrides.j2'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/dde8702afe1e867c27097c3acbdfbb1078905d73', 'message': 'Add libcgroup-tools to libvirt container\n\nTo reuse the tripleo containers when running libvirtd container\non OCP we need to run it via a wrapper script and create our own\ncgroups and run libvirtd using cgexec to break out the default\nOCP cgroup of the pod. Otherwise if the libvirtd pod gets\ndeleted/recreated all VMs get shutdown as OCP cleans up the\nresources owned by the pod.\n\nChange-Id: I469b4f51c57e1651f42064b0134aa3916af72d80\n'}]",0,713599,dde8702afe1e867c27097c3acbdfbb1078905d73,19,7,3,17216,,,0,"Add libcgroup-tools to libvirt container

To reuse the tripleo containers when running libvirtd container
on OCP we need to run it via a wrapper script and create our own
cgroups and run libvirtd using cgexec to break out the default
OCP cgroup of the pod. Otherwise if the libvirtd pod gets
deleted/recreated all VMs get shutdown as OCP cleans up the
resources owned by the pod.

Change-Id: I469b4f51c57e1651f42064b0134aa3916af72d80
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/99/713599/1 && git format-patch -1 --stdout FETCH_HEAD,['container-images/tripleo_kolla_template_overrides.j2'],1,722e6b88b4a1f009dbec9d2d03b457f6877dac50,libcgroup-tools," 'libcgroup-tools,",,1,0
openstack%2Foctavia~master~I48315f8f293811e1d99584ea36da05c4211cf275,openstack/octavia,master,I48315f8f293811e1d99584ea36da05c4211cf275,Fix load balancer update with provider filtered params,MERGED,2019-11-21 21:52:46.000000000,2020-03-20 08:09:42.000000000,2020-03-20 08:06:31.000000000,"[{'_account_id': 1131}, {'_account_id': 2245}, {'_account_id': 6469}, {'_account_id': 7249}, {'_account_id': 10273}, {'_account_id': 11628}, {'_account_id': 22348}, {'_account_id': 29244}]","[{'number': 1, 'created': '2019-11-21 21:52:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/5df77fa69cbfa06531cdf08de0668773b45214e2', 'message': 'Fix load balancer update with provider filtere params\n\nWhen only setting tags to an existing load balancer, the amphora driver\nwould try to apply QoS policy on VRRP ports even if no policy is\ndefined. This raised an NetworkException that led load balancers to go\ninto ERROR.\n\nTask: 37589\nStory: 2006922\n\nChange-Id: I48315f8f293811e1d99584ea36da05c4211cf275\n'}, {'number': 2, 'created': '2019-11-21 22:05:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/32b6c4c40aec3b2e52e6f71172253a7ae87d3a5d', 'message': 'Fix load balancer update with provider filtere params\n\nWhen only setting tags to an existing load balancer, the amphora driver\nwould try to apply QoS policy on VRRP ports even if no policy is\ndefined. This raised an NetworkException that led load balancers to go\ninto ERROR.\n\nTask: 37589\nStory: 2006922\n\nChange-Id: I48315f8f293811e1d99584ea36da05c4211cf275\n'}, {'number': 3, 'created': '2019-11-28 19:19:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/7d8363e91d25356c216eb566556b3ff08f8e3aca', 'message': 'Fix load balancer update with provider filtered params\n\nWhen only setting tags to an existing load balancer, the amphora driver\nwould try to apply QoS policy on VRRP ports even if no policy is\ndefined. This raised an NetworkException that led load balancers to go\ninto ERROR.\n\nTask: 37589\nStory: 2006922\n\nChange-Id: I48315f8f293811e1d99584ea36da05c4211cf275\n'}, {'number': 4, 'created': '2020-02-26 03:23:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/4850654df865e12d776fb5455057869fd459ceb7', 'message': 'Fix load balancer update with provider filtered params\n\nWhen only setting tags to an existing load balancer, the amphora driver\nwould try to apply QoS policy on VRRP ports even if no policy is\ndefined. This raised an NetworkException that led load balancers to go\ninto ERROR.\n\nTask: 37589\nStory: 2006922\n\nChange-Id: I48315f8f293811e1d99584ea36da05c4211cf275\n'}, {'number': 5, 'created': '2020-02-26 17:10:01.000000000', 'files': ['octavia/tests/unit/controller/worker/v2/tasks/test_network_tasks.py', 'octavia/tests/unit/controller/worker/v1/tasks/test_network_tasks.py', 'releasenotes/notes/fix-lb-update-with-no-data-abefe7860b8fb4c7.yaml', 'octavia/controller/worker/v2/tasks/network_tasks.py', 'octavia/controller/worker/v1/tasks/network_tasks.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/2cfcd71a8659583d91cda3e488c60ded078c9677', 'message': 'Fix load balancer update with provider filtered params\n\nWhen only setting tags to an existing load balancer, the amphora driver\nwould try to apply QoS policy on VRRP ports even if no policy is\ndefined. This raised an NetworkException that led load balancers to go\ninto ERROR.\n\nTask: 37589\nStory: 2006922\n\nChange-Id: I48315f8f293811e1d99584ea36da05c4211cf275\n'}]",7,695589,2cfcd71a8659583d91cda3e488c60ded078c9677,41,8,5,6469,,,0,"Fix load balancer update with provider filtered params

When only setting tags to an existing load balancer, the amphora driver
would try to apply QoS policy on VRRP ports even if no policy is
defined. This raised an NetworkException that led load balancers to go
into ERROR.

Task: 37589
Story: 2006922

Change-Id: I48315f8f293811e1d99584ea36da05c4211cf275
",git fetch https://review.opendev.org/openstack/octavia refs/changes/89/695589/2 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/tests/unit/controller/worker/v2/tasks/test_network_tasks.py', 'octavia/tests/unit/controller/worker/v1/tasks/test_network_tasks.py', 'octavia/controller/worker/v2/tasks/network_tasks.py', 'releasenotes/notes/fix-lb-update-with-no-data-abefe7860b8fb4c7.yaml', 'octavia/controller/worker/v1/tasks/network_tasks.py']",5,5df77fa69cbfa06531cdf08de0668773b45214e2,695589," isinstance(update_dict, dict) and (", update_dict and (,39,2
openstack%2Fneutron~master~If8edd29dd741f1688ffcac341fd58173539ba000,openstack/neutron,master,If8edd29dd741f1688ffcac341fd58173539ba000,[OvS] Handle re_added multi ports,MERGED,2020-02-25 10:26:18.000000000,2020-03-20 08:09:36.000000000,2020-03-15 21:58:31.000000000,"[{'_account_id': 4694}, {'_account_id': 9732}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 13995}, {'_account_id': 16688}, {'_account_id': 22348}, {'_account_id': 26622}, {'_account_id': 28691}, {'_account_id': 31662}]","[{'number': 1, 'created': '2020-02-25 10:26:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e1247df748528480a7f195addbfccc24b5200574', 'message': ""[OvS] Handle re_added multi ports\n\nMultiple ports are located in ports_re_added. Assume we have port_one\nand port_two. It will loop through the ports. Port_one is iterated\nfirst, events ['re_added'] is assigned port_one, events ['removed']\nis assigned port_two. In the second loop, events ['re_added'] is set\nto port_two instead of adding port_two to list. So after the loop,\nonly port_two is left in events ['re_added'].\n\nhttps://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py#L1630\n\nChange-Id: If8edd29dd741f1688ffcac341fd58173539ba000\nCloses-Bug: #1864630\n""}, {'number': 2, 'created': '2020-02-27 03:23:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/a4eb195f1e7a7f1a4cd54ca82fbcbba56cce8f4f', 'message': ""[OvS] Handle re_added multi ports\n\nMultiple ports are located in ports_re_added. Assume we have port_one\nand port_two. It will loop through the ports. Port_one is iterated\nfirst, events ['re_added'] is assigned port_one, events ['removed']\nis assigned port_two. In the second loop, events ['re_added'] is set\nto port_two instead of adding port_two to list. So after the loop,\nonly port_two is left in events ['re_added'].\n\nChange-Id: If8edd29dd741f1688ffcac341fd58173539ba000\nCloses-Bug: #1864630\n""}, {'number': 3, 'created': '2020-02-27 06:11:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8d2ee6a0b804332f4c2004be62a441b2ea69341d', 'message': ""[OvS] Handle re_added multi ports\n\nMultiple ports are located in ports_re_added. Assume we have port_one\nand port_two. It will loop through the ports. Port_one is iterated\nfirst, events ['re_added'] is assigned port_one, events ['removed']\nis assigned port_two. In the second loop, events ['re_added'] is set\nto port_two instead of adding port_two to list. So after the loop,\nonly port_two is left in events ['re_added'].\n\nChange-Id: If8edd29dd741f1688ffcac341fd58173539ba000\nCloses-Bug: #1864630\n""}, {'number': 4, 'created': '2020-02-27 08:46:46.000000000', 'files': ['neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/test_ovs_neutron_agent.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/5600163e9b4fb89b1525e9e415b1fedbe9526501', 'message': ""[OvS] Handle re_added multi ports\n\nMultiple ports are located in ports_re_added. Assume we have port_one\nand port_two. It will loop through the ports. Port_one is iterated\nfirst, events ['re_added'] is assigned port_one, events ['removed']\nis assigned port_two. In the second loop, events ['re_added'] is set\nto port_two instead of adding port_two to list. So after the loop,\nonly port_two is left in events ['re_added'].\n\nChange-Id: If8edd29dd741f1688ffcac341fd58173539ba000\nCloses-Bug: #1864630\n""}]",8,709687,5600163e9b4fb89b1525e9e415b1fedbe9526501,82,10,4,31662,,,0,"[OvS] Handle re_added multi ports

Multiple ports are located in ports_re_added. Assume we have port_one
and port_two. It will loop through the ports. Port_one is iterated
first, events ['re_added'] is assigned port_one, events ['removed']
is assigned port_two. In the second loop, events ['re_added'] is set
to port_two instead of adding port_two to list. So after the loop,
only port_two is left in events ['re_added'].

Change-Id: If8edd29dd741f1688ffcac341fd58173539ba000
Closes-Bug: #1864630
",git fetch https://review.opendev.org/openstack/neutron refs/changes/87/709687/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py'],1,e1247df748528480a7f195addbfccc24b5200574,bug/1864630, if 're_added' not in events: events['re_added'] = [e for e in events['removed'] if e['name'] == p] else: events['re_added'].extend([e for e in events['removed'] if e['name'] == p]), events['re_added'] = [e for e in events['removed'] if e['name'] == p],6,2
openstack%2Ftripleo-ansible~master~If8df9863a3b3f18e00767d095849b48eb08486fd,openstack/tripleo-ansible,master,If8df9863a3b3f18e00767d095849b48eb08486fd,Add tripleo_image_params_prepare ansible module,MERGED,2020-03-12 02:21:39.000000000,2020-03-20 08:03:24.000000000,2020-03-20 08:02:00.000000000,"[{'_account_id': 7353}, {'_account_id': 8833}, {'_account_id': 9712}, {'_account_id': 10969}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25877}]","[{'number': 1, 'created': '2020-03-12 02:21:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/28fd1c4e335b158cb82d3cb04a57d36ba08b3391', 'message': 'Add tripleo_image_params_prepare ansible module\n\nUses tripleo-common library to build image params and update\nthe plan.\n\nDepends-On: https://review.opendev.org/712598\nChange-Id: If8df9863a3b3f18e00767d095849b48eb08486fd\n'}, {'number': 2, 'created': '2020-03-18 06:43:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/b8e7ffaeac2360dd37cc88e67e8495a29493b007', 'message': 'Add tripleo_image_params_prepare ansible module\n\nUses tripleo-common library to build image params and update\nthe plan.\n\nDepends-On: https://review.opendev.org/712598\nChange-Id: If8df9863a3b3f18e00767d095849b48eb08486fd\n'}, {'number': 3, 'created': '2020-03-19 19:33:45.000000000', 'files': ['doc/source/modules/modules-tripleo_image_params_prepare.rst', 'tripleo_ansible/ansible_plugins/modules/tripleo_image_params_prepare.py'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/d1e0e371468fa618a7542fe3cb75a8ba9ba93326', 'message': 'Add tripleo_image_params_prepare ansible module\n\nUses tripleo-common library to build image params and update\nthe plan.\n\nDepends-On: https://review.opendev.org/712598\nChange-Id: If8df9863a3b3f18e00767d095849b48eb08486fd\n'}]",16,712602,d1e0e371468fa618a7542fe3cb75a8ba9ba93326,33,7,3,8833,,,0,"Add tripleo_image_params_prepare ansible module

Uses tripleo-common library to build image params and update
the plan.

Depends-On: https://review.opendev.org/712598
Change-Id: If8df9863a3b3f18e00767d095849b48eb08486fd
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/02/712602/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/modules/modules-tripleo_image_params_prepare.rst', 'tripleo_ansible/ansible_plugins/modules/tripleo_image_params_prepare.py']",2,28fd1c4e335b158cb82d3cb04a57d36ba08b3391,mistral_to_ansible,"#!/usr/bin/python # -*- coding: utf-8 -*- # Copyright (c) 2018 OpenStack Foundation # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from __future__ import absolute_import from __future__ import division from __future__ import print_function import yaml from ansible.module_utils.basic import AnsibleModule from ansible.module_utils.openstack import openstack_full_argument_spec from ansible.module_utils.openstack import openstack_module_kwargs from ansible.module_utils.openstack import openstack_cloud_from_module # NOTE: This is still using the legacy clients. We've not # changed to using the OpenStackSDK fully because # tripleo-common expects the legacy clients. Once # we've updated tripleo-common to use the SDK we # should revise this. from swiftclient import client as swift_client from tripleo_common.utils import plan as plan_utils ANSIBLE_METADATA = { 'metadata_version': '1.1', 'status': ['preview'], 'supported_by': 'community' } DOCUMENTATION = ''' --- module: tripleo_image_params_prepare short_description: Update plan image params version_added: ""2.8"" description: - ""Prepare Image params and update plan"" options: container: description: - Overcloud plan container name default: overcloud with_roledata: description: - With role data type: bool default: false author: - Rabi Mishra (@ramishra) ''' EXAMPLES = ''' - name: Prepare image params and update plan tripleo_image_params_prepare: container: overcloud with_roledata: true ''' def run_module(): result = dict( success=False, error="""", ) argument_spec = openstack_full_argument_spec( **yaml.safe_load(DOCUMENTATION)['options'] ) module = AnsibleModule( argument_spec, supports_check_mode=True, **openstack_module_kwargs() ) def get_object_client(session): return swift_client.Connection( session=session, retries=10, starting_backoff=3, max_backoff=120) try: container = module.params.get('container') with_roledata = module.params.get('with_roledata') _, conn = openstack_cloud_from_module(module) session = conn.session # if the user is working with this module in only check mode we do not # want to make any changes to the environment, just return the current # state with no modifications if module.check_mode: module.exit_json(**result) swift = get_object_client(session) plan_utils.update_plan_environment_with_image_parameters( swift, container, with_roledata=with_roledata) result['success'] = True except Exception as err: result['error'] = str(err) result['msg'] = (""Error updating image parms for plan %s: %s"" % ( container, err)) module.fail_json(**result) # in the event of a successful module execution, you will want to # simple AnsibleModule.exit_json(), passing the key/value results module.exit_json(**result) def main(): run_module() if __name__ == '__main__': main() ",,145,0
openstack%2Fswift~feature%2Flosf~I3731b691d197e755501236960033bde85ed1c871,openstack/swift,feature/losf,I3731b691d197e755501236960033bde85ed1c871,Fix RPC error checking in losf volume check,MERGED,2020-03-19 15:47:50.000000000,2020-03-20 07:53:46.000000000,2020-03-20 07:51:55.000000000,"[{'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 15:47:50.000000000', 'files': ['bin/swift-losf-volume-check'], 'web_link': 'https://opendev.org/openstack/swift/commit/53d43df8345c08e2d35c748461e4b984895054dd', 'message': 'Fix RPC error checking in losf volume check\n\nWith the switch from gRPC to HTTP, the error code was changed to\nan int. Caller in volume check was not updated.\n\nChange-Id: I3731b691d197e755501236960033bde85ed1c871\n'}]",0,713912,53d43df8345c08e2d35c748461e4b984895054dd,13,2,1,25251,,,0,"Fix RPC error checking in losf volume check

With the switch from gRPC to HTTP, the error code was changed to
an int. Caller in volume check was not updated.

Change-Id: I3731b691d197e755501236960033bde85ed1c871
",git fetch https://review.opendev.org/openstack/swift refs/changes/12/713912/1 && git format-patch -1 --stdout FETCH_HEAD,['bin/swift-losf-volume-check'],1,53d43df8345c08e2d35c748461e4b984895054dd,dev/alecuyer/feature-branch-catchup, if e.code == StatusCode.NotFound: if e.code == StatusCode.NotFound:, if e.code() == StatusCode.NotFound: if e.code() == StatusCode.NotFound:,2,2
openstack%2Fopenstack-helm~master~Iea52d94a1b5cf1db41ce694349e49e6311a47333,openstack/openstack-helm,master,Iea52d94a1b5cf1db41ce694349e49e6311a47333,Add netpol value overrides for placement chart,MERGED,2020-03-16 08:37:35.000000000,2020-03-20 07:30:49.000000000,2020-03-20 07:27:55.000000000,"[{'_account_id': 8898}, {'_account_id': 20466}, {'_account_id': 21420}, {'_account_id': 22348}, {'_account_id': 29397}]","[{'number': 1, 'created': '2020-03-16 08:37:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/bd55c1f80081d1ea9be9ac21773975067378457d', 'message': '[WIP] netpol test\n\nChange-Id: Iea52d94a1b5cf1db41ce694349e49e6311a47333\n'}, {'number': 2, 'created': '2020-03-16 08:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/e8ba02ad04e54ef4104c8b5a3906431f9572be76', 'message': '[WIP] netpol test\n\nChange-Id: Iea52d94a1b5cf1db41ce694349e49e6311a47333\n'}, {'number': 3, 'created': '2020-03-16 10:01:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/91ce5d5707ad9d0cb337737ef2364bf4152905ef', 'message': '[DNM] netpol test\n\nChange-Id: Iea52d94a1b5cf1db41ce694349e49e6311a47333\n'}, {'number': 4, 'created': '2020-03-16 13:02:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/0e29ea6d244e67afdfcf0556d4e7e3d32757c23b', 'message': '[DNM] netpol test\n\nChange-Id: Iea52d94a1b5cf1db41ce694349e49e6311a47333\n'}, {'number': 5, 'created': '2020-03-16 15:18:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/9aaf9d993f4cd37c9bd4634ee65344692d67d0a2', 'message': '[DNM] netpol test\n\nChange-Id: Iea52d94a1b5cf1db41ce694349e49e6311a47333\n'}, {'number': 6, 'created': '2020-03-16 18:01:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/75067a9f9a2a67c06d6b595cd378095ec87d65b3', 'message': 'Add netpol value overrides for placement chart\n\nChange-Id: Iea52d94a1b5cf1db41ce694349e49e6311a47333\n'}, {'number': 7, 'created': '2020-03-17 07:43:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/2c4f24717430cdede3dc8b2e3857f80d684c12cb', 'message': 'Add netpol value overrides for placement chart\n\nChange-Id: Iea52d94a1b5cf1db41ce694349e49e6311a47333\n'}, {'number': 8, 'created': '2020-03-17 10:50:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/517ae51d2c4c2700db8f44bcf3204fb94fbbd177', 'message': 'Add netpol value overrides for placement chart\n\nChange-Id: Iea52d94a1b5cf1db41ce694349e49e6311a47333\n'}, {'number': 9, 'created': '2020-03-18 07:21:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/ce2d357d9eb08046a71971ae0b2ad7f801158eb2', 'message': 'Add netpol value overrides for placement chart\n\nChange-Id: Iea52d94a1b5cf1db41ce694349e49e6311a47333\n'}, {'number': 10, 'created': '2020-03-19 16:53:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/6b37d48ab6bf9bd4ad90ca11f7e73d249a0b8cd8', 'message': 'Add netpol value overrides for placement chart\n\nChange-Id: Iea52d94a1b5cf1db41ce694349e49e6311a47333\n'}, {'number': 11, 'created': '2020-03-19 16:53:37.000000000', 'files': ['placement/values_overrides/netpol.yaml', 'zuul.d/project.yaml', 'zuul.d/jobs-openstack-helm.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/78d1624ad3ca7b61d9b9735794290bd9f6c9163d', 'message': 'Add netpol value overrides for placement chart\n\nChange-Id: Iea52d94a1b5cf1db41ce694349e49e6311a47333\n'}]",2,713172,78d1624ad3ca7b61d9b9735794290bd9f6c9163d,29,5,11,29397,,,0,"Add netpol value overrides for placement chart

Change-Id: Iea52d94a1b5cf1db41ce694349e49e6311a47333
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/72/713172/1 && git format-patch -1 --stdout FETCH_HEAD,['placement/values_overrides/netpol.yaml'],1,bd55c1f80081d1ea9be9ac21773975067378457d,netpol,manifests: network_policy: true network_policy: placement: egress: - to: - ipBlock: cidr: 172.17.0.1/32 ports: - protocol: TCP port: 6443 ,,11,0
openstack%2Fliberasurecode~master~I065ca37e5d69bb6ee3c7f00374af2e9c3da7739f,openstack/liberasurecode,master,I065ca37e5d69bb6ee3c7f00374af2e9c3da7739f,Use ceph's GitHub mirrors for gf-complete/jerasure,MERGED,2020-03-13 06:52:28.000000000,2020-03-20 07:01:37.000000000,2020-03-20 07:01:37.000000000,"[{'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-13 06:52:28.000000000', 'files': ['roles/install_jerasure/tasks/main.yaml'], 'web_link': 'https://opendev.org/openstack/liberasurecode/commit/423004247648f06dc34be23cc63ad8507bb74035', 'message': ""Use ceph's GitHub mirrors for gf-complete/jerasure\n\nApparently upstream now requires that you log in?\n\nChange-Id: I065ca37e5d69bb6ee3c7f00374af2e9c3da7739f\n""}]",0,712842,423004247648f06dc34be23cc63ad8507bb74035,7,2,1,15343,,,0,"Use ceph's GitHub mirrors for gf-complete/jerasure

Apparently upstream now requires that you log in?

Change-Id: I065ca37e5d69bb6ee3c7f00374af2e9c3da7739f
",git fetch https://review.opendev.org/openstack/liberasurecode refs/changes/42/712842/1 && git format-patch -1 --stdout FETCH_HEAD,['roles/install_jerasure/tasks/main.yaml'],1,423004247648f06dc34be23cc63ad8507bb74035,, git clone https://github.com/ceph/gf-complete.git git clone https://github.com/ceph/jerasure.git, git clone http://lab.jerasure.org/jerasure/gf-complete.git git clone http://lab.jerasure.org/jerasure/jerasure.git,2,2
openstack%2Fmanila-specs~master~Ia4715276117c54cd7cca7f997639a6f92328266a,openstack/manila-specs,master,Ia4715276117c54cd7cca7f997639a6f92328266a,Spec changes for Ussuri and Train,MERGED,2020-03-19 05:14:12.000000000,2020-03-20 06:43:43.000000000,2020-03-20 06:41:52.000000000,"[{'_account_id': 16643}, {'_account_id': 22348}, {'_account_id': 29632}, {'_account_id': 30002}]","[{'number': 1, 'created': '2020-03-19 05:14:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/e23e4bb70bc9f8960c33f29482e5756fe43fc392', 'message': ""Spec changes for Ussuri and Train\n\n- Add Ussuri release to index\n- move unimplemented Train specs\n  - OSC spec is release independent\n  - share capability enhancements wasn't implemented\n\nChange-Id: Ia4715276117c54cd7cca7f997639a6f92328266a\n""}, {'number': 2, 'created': '2020-03-19 14:33:57.000000000', 'files': ['doc/source/index.rst', 'specs/unimplemented/share-and-backend-capabilities-improvements.rst', 'specs/release_independent/manila-support-openstackclient.rst'], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/9213f28dc82685352ada2d9c15ccdb929f4f5223', 'message': ""Spec changes for Ussuri and Train\n\n- Add Ussuri release to index\n- move unimplemented Train specs\n  - OSC spec is release independent\n  - share capability enhancements wasn't implemented\n\nChange-Id: Ia4715276117c54cd7cca7f997639a6f92328266a\nSigned-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>\n""}]",0,713780,9213f28dc82685352ada2d9c15ccdb929f4f5223,11,4,2,16643,,,0,"Spec changes for Ussuri and Train

- Add Ussuri release to index
- move unimplemented Train specs
  - OSC spec is release independent
  - share capability enhancements wasn't implemented

Change-Id: Ia4715276117c54cd7cca7f997639a6f92328266a
Signed-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>
",git fetch https://review.opendev.org/openstack/manila-specs refs/changes/80/713780/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/index.rst', 'specs/release_independent/manila-support-openstackclient.rst', 'specs/unimplemented/share-and-backend-capabilities-improvements.rst']",3,e23e4bb70bc9f8960c33f29482e5756fe43fc392,spec-changes,,,10,0
openstack%2Frequirements~master~I015d823f9574002bf5ad118668d50e09ce0ba465,openstack/requirements,master,I015d823f9574002bf5ad118668d50e09ce0ba465,Updated from generate-constraints,MERGED,2020-03-18 06:19:17.000000000,2020-03-20 06:34:00.000000000,2020-03-20 06:32:23.000000000,"[{'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-18 06:19:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/760c32e49283f15f9769253a0ee673b150ef010f', 'message': 'Updated from generate-constraints\n\nChange-Id: I015d823f9574002bf5ad118668d50e09ce0ba465\n'}, {'number': 2, 'created': '2020-03-19 06:48:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/bd89f93ed422ede30fd5ed52e9f0bf03d05060f6', 'message': 'Updated from generate-constraints\n\nChange-Id: I015d823f9574002bf5ad118668d50e09ce0ba465\n'}, {'number': 3, 'created': '2020-03-19 07:20:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/66dfc68652ac3d193261ed37b76b1ab9d861e996', 'message': 'Updated from generate-constraints\n\nhold back mock for nova/neutron failures\nhold back docutils because boto likes to cap things\nhold back msgpack because salt likes to cap things\nhold back alembic because nova fails tests\n\nChange-Id: I015d823f9574002bf5ad118668d50e09ce0ba465\nSigned-off-by: Matthew Thode <mthode@mthode.org>\n'}, {'number': 4, 'created': '2020-03-19 20:10:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/753688217749dd98282f6179e162db78b9a0bc54', 'message': 'Updated from generate-constraints\n\nhold back mock for nova/neutron failures\nhold back docutils because boto likes to cap things\nhold back msgpack because salt likes to cap things\nhold back alembic because nova/neutron fails tests\n\nChange-Id: I015d823f9574002bf5ad118668d50e09ce0ba465\nSigned-off-by: Matthew Thode <mthode@mthode.org>\n'}, {'number': 5, 'created': '2020-03-20 00:52:31.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/c300b8917c13b56ad4e75bfac0e9f3f50c28586a', 'message': 'Updated from generate-constraints\n\nhold back mock for nova/neutron failures\nhold back docutils because boto likes to cap things\nhold back msgpack because salt likes to cap things\nhold back alembic because nova/neutron fails tests\n\nChange-Id: I015d823f9574002bf5ad118668d50e09ce0ba465\nSigned-off-by: Matthew Thode <mthode@mthode.org>\n'}]",0,713573,c300b8917c13b56ad4e75bfac0e9f3f50c28586a,27,2,5,11131,,,0,"Updated from generate-constraints

hold back mock for nova/neutron failures
hold back docutils because boto likes to cap things
hold back msgpack because salt likes to cap things
hold back alembic because nova/neutron fails tests

Change-Id: I015d823f9574002bf5ad118668d50e09ce0ba465
Signed-off-by: Matthew Thode <mthode@mthode.org>
",git fetch https://review.opendev.org/openstack/requirements refs/changes/73/713573/3 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,760c32e49283f15f9769253a0ee673b150ef010f,openstack/requirements/constraints/noclob,ddt===1.3.1mock===3.0.5;python_version=='2.7' mock===4.0.2;python_version=='3.6' mock===4.0.2;python_version=='3.7'docutils===0.16boto3===1.12.23pyScss2===1.4.0numpy===1.18.2;python_version=='3.6' numpy===1.18.2;python_version=='3.7' msgpack===1.0.0botocore===1.15.23alembic===1.4.1django-pyscss2===3.0.0,pyScss2===1.4.0ddt===1.3.0mock===3.0.5django-pyscss2===3.0.0docutils===0.15.2boto3===1.12.22numpy===1.18.1;python_version=='3.6' numpy===1.18.1;python_version=='3.7' msgpack===0.6.2botocore===1.15.22alembic===1.3.3,13,11
openstack%2Fheat-agents~master~Ib9819e87a8f39112ef6d1b49109a45554ed741a7,openstack/heat-agents,master,Ib9819e87a8f39112ef6d1b49109a45554ed741a7,Add atomic hook support,NEW,2020-03-05 07:13:22.000000000,2020-03-20 06:09:01.000000000,,"[{'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-05 07:13:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-agents/commit/53b5a0c36a30bc7c94c3d4c183a6cf7852ab3d85', 'message': 'Add atomic hook support\n\nCopy from https://review.opendev.org/#/c/585420/\n\nChange-Id: Ib9819e87a8f39112ef6d1b49109a45554ed741a7\n'}, {'number': 2, 'created': '2020-03-05 10:33:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-agents/commit/6e0fb8b36afe4dcb8a7a95dd4a1bb4bbc5aab5db', 'message': 'Add atomic hook support\n\nCopy from https://review.opendev.org/#/c/585420/\n\nChange-Id: Ib9819e87a8f39112ef6d1b49109a45554ed741a7\n'}, {'number': 3, 'created': '2020-03-20 05:48:37.000000000', 'files': ['heat-config-atomic/element-deps', 'heat-config-atomic/install.d/50-heat-config-hook-atomic', 'Makefile', 'heat-config-atomic/install.d/hook-atomic.py'], 'web_link': 'https://opendev.org/openstack/heat-agents/commit/9c642da37140ce9d8258fd834db3a8736a4fb6a9', 'message': 'Add atomic hook support\n\nCopy from https://review.opendev.org/#/c/585420/\n\nChange-Id: Ib9819e87a8f39112ef6d1b49109a45554ed741a7\n'}]",0,711389,9c642da37140ce9d8258fd834db3a8736a4fb6a9,9,2,3,12404,,,0,"Add atomic hook support

Copy from https://review.opendev.org/#/c/585420/

Change-Id: Ib9819e87a8f39112ef6d1b49109a45554ed741a7
",git fetch https://review.opendev.org/openstack/heat-agents refs/changes/89/711389/1 && git format-patch -1 --stdout FETCH_HEAD,"['heat-config-atomic/element-deps', 'heat-config-atomic/install.d/50-heat-config-hook-atomic', 'Makefile', 'heat-config-atomic/install.d/hook-atomic.py']",4,53b5a0c36a30bc7c94c3d4c183a6cf7852ab3d85,build-container-agent,"#!/usr/bin/env python # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import json import logging import os import select import subprocess import sys WORKING_DIR = os.environ.get('HEAT_SCRIPT_WORKING', '/var/lib/heat-config/heat-config-script') OUTPUTS_DIR = os.environ.get('HEAT_SCRIPT_OUTPUTS', '/var/run/heat-config/heat-config-script') LOGS_DIR = os.environ.get('HEAT_SCRIPT_LOGS', '/var/log/heat-config/heat-config-script') def prepare_dir(path, mode=0o700): if not os.path.isdir(path): os.makedirs(path, mode) def main(argv=sys.argv): log = logging.getLogger('heat-config') handler = logging.StreamHandler(sys.stderr) handler.setFormatter( logging.Formatter( '[%(asctime)s] (%(name)s) [%(levelname)s] %(message)s')) log.addHandler(handler) log.setLevel('DEBUG') prepare_dir(OUTPUTS_DIR) prepare_dir(WORKING_DIR) prepare_dir(LOGS_DIR, mode=0o644) os.chdir(WORKING_DIR) c = json.load(sys.stdin) env = os.environ.copy() for input in c['inputs']: input_name = input['name'] value = input.get('value', '') if isinstance(value, dict) or isinstance(value, list): env[input_name] = json.dumps(value) else: env[input_name] = value log.info('%s=%s' % (input_name, env[input_name])) fn = os.path.join(WORKING_DIR, c['id']) suffix = c.get('name', '') suffix = '-%s' % suffix if suffix else '' lp = os.path.join(LOGS_DIR, '%s%s.log' % (c['id'], suffix)) heat_outputs_path = os.path.join(OUTPUTS_DIR, c['id']) env['heat_outputs_path'] = heat_outputs_path with os.fdopen(os.open(fn, os.O_CREAT | os.O_WRONLY, 0o700), 'w') as f: f.write(c.get('config', '')) log.debug('Running %s, logging to %s' % (fn, lp)) subproc = subprocess.Popen([fn], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env) with open(lp, 'w') as f: while subproc.poll() is None: read, _, _ = select.select([subproc.stdout], [], []) for stream in read: if stream: line = stream.readline().decode('utf-8', 'replace') f.write(line) f.flush() if subproc.returncode: log.error(""Error running %s. [%s]\n"" % (fn, subproc.returncode)) else: log.info('Completed %s' % fn) response = {} for output in c.get('outputs') or []: output_name = output['name'] try: with open('%s.%s' % (heat_outputs_path, output_name)) as out: response[output_name] = out.read() except IOError: pass response.update({ 'deploy_status_code': subproc.returncode, }) json.dump(response, sys.stdout) if __name__ == '__main__': sys.exit(main(sys.argv)) ",,113,1
openstack%2Fmistral~master~I3dbf26e3fdf2d2bd17d99b678172bd9ac02a7323,openstack/mistral,master,I3dbf26e3fdf2d2bd17d99b678172bd9ac02a7323,Keep openstack mapping_path option,MERGED,2020-03-19 13:11:40.000000000,2020-03-20 05:59:00.000000000,2020-03-20 05:57:35.000000000,"[{'_account_id': 8367}, {'_account_id': 8731}, {'_account_id': 9712}, {'_account_id': 15895}, {'_account_id': 19134}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 13:11:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/5921b064d8a3a7b88862e64d2632945df7aaf5b2', 'message': 'Keep openstack_actions_mapping_path option to keep backwards compatibility\n\nWhen we moved the openstack actions to mistral-extra this option was removed\nThis adds it back if mistral-extra is installed otherwise it keeps the same\n\nChange-Id: I3dbf26e3fdf2d2bd17d99b678172bd9ac02a7323\nCloses-bug: #1866621\n'}, {'number': 2, 'created': '2020-03-19 13:12:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/20c6633050deacee05517d6243f2841c8cd1aa6e', 'message': 'Keep openstack mapping_path option to keep backwards compatibility\n\nWhen we moved the openstack actions to mistral-extra this\noption was removed. This adds it back if mistral-extra is\ninstalled otherwise it keeps the same\n\nChange-Id: I3dbf26e3fdf2d2bd17d99b678172bd9ac02a7323\nCloses-bug: #1866621\n'}, {'number': 3, 'created': '2020-03-19 13:16:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/f22461af361934d1eb12568549d6d17d83c09423', 'message': 'Keep openstack mapping_path option\n\nRemoval of this option breaks backwards compatibility\n\nWhen we moved the openstack actions to mistral-extra this\noption was removed. This adds it back if mistral-extra is\ninstalled otherwise it keeps the same\n\nChange-Id: I3dbf26e3fdf2d2bd17d99b678172bd9ac02a7323\nCloses-bug: #1866621\n'}, {'number': 4, 'created': '2020-03-19 15:35:53.000000000', 'files': ['mistral/db/sqlalchemy/migration/cli.py'], 'web_link': 'https://opendev.org/openstack/mistral/commit/a594a2523e0b19100f3699f502cf617405698fc7', 'message': 'Keep openstack mapping_path option\n\nRemoval of this option breaks backwards compatibility\n\nWhen we moved the openstack actions to mistral-extra this\noption was removed. This adds it back if mistral-extra is\ninstalled otherwise it keeps the same\n\nChange-Id: I3dbf26e3fdf2d2bd17d99b678172bd9ac02a7323\nCloses-bug: #1866621\n'}]",2,713873,a594a2523e0b19100f3699f502cf617405698fc7,15,6,4,15895,,,0,"Keep openstack mapping_path option

Removal of this option breaks backwards compatibility

When we moved the openstack actions to mistral-extra this
option was removed. This adds it back if mistral-extra is
installed otherwise it keeps the same

Change-Id: I3dbf26e3fdf2d2bd17d99b678172bd9ac02a7323
Closes-bug: #1866621
",git fetch https://review.opendev.org/openstack/mistral refs/changes/73/713873/4 && git format-patch -1 --stdout FETCH_HEAD,['mistral/db/sqlalchemy/migration/cli.py'],1,5921b064d8a3a7b88862e64d2632945df7aaf5b2,fix_missing_map_option,# To Keep backwards compatibility we need to accept mapping path # from mistral-extra if present try: import mistral_extra.config as extra_conf CONF.register_cli_opt(extra_conf.os_actions_mapping_path) except ImportError: pass ,,8,0
openstack%2Fopenstack-helm-infra~master~I1690035c7e35887245bbdcfdc97b19409fd6ab8a,openstack/openstack-helm-infra,master,I1690035c7e35887245bbdcfdc97b19409fd6ab8a,Add more fields to daemonjob crd spec.,MERGED,2020-03-11 17:00:10.000000000,2020-03-20 05:51:45.000000000,2020-03-20 05:50:20.000000000,"[{'_account_id': 8898}, {'_account_id': 21420}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-11 17:00:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/85e0ea4a52c4004ddce123431bc22333c35bd46d', 'message': 'Add metadata and spec fields to daemonjob crd.\n\nChange-Id: I1690035c7e35887245bbdcfdc97b19409fd6ab8a\n'}, {'number': 2, 'created': '2020-03-11 18:22:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/fb0eef54359f94fdda2f4fb373dba676faee92b4', 'message': 'Add metadata and spec fields to daemonjob crd.\n\nChange-Id: I1690035c7e35887245bbdcfdc97b19409fd6ab8a\n'}, {'number': 3, 'created': '2020-03-12 11:39:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/68f474af1f4a72d1a25026241b3802615005b6f6', 'message': 'Add more fields to daemonjob crd spec.\n\nChange-Id: I1690035c7e35887245bbdcfdc97b19409fd6ab8a\n'}, {'number': 4, 'created': '2020-03-16 14:53:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/62f289de440e6ff9d9a37ed367b45692f765aaef', 'message': 'Add more fields to daemonjob crd spec.\n\nChange-Id: I1690035c7e35887245bbdcfdc97b19409fd6ab8a\n'}, {'number': 5, 'created': '2020-03-16 15:33:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/692c319ba432f061e3d2e628a78a3128ba6fffea', 'message': 'Add more fields to daemonjob crd spec.\n\nChange-Id: I1690035c7e35887245bbdcfdc97b19409fd6ab8a\n'}, {'number': 6, 'created': '2020-03-20 04:03:13.000000000', 'files': ['daemonjob-controller/values.yaml', 'tools/deployment/common/daemonjob-controller.sh', 'daemonjob-controller/templates/crd.yaml', 'daemonjob-controller/templates/deployment.yaml', 'metacontroller/templates/statefulset.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/55beab680f18daeb79f6cf48a66aef3a572ababb', 'message': 'Add more fields to daemonjob crd spec.\n\nChange-Id: I1690035c7e35887245bbdcfdc97b19409fd6ab8a\n'}]",0,712506,55beab680f18daeb79f6cf48a66aef3a572ababb,17,3,6,31482,,,0,"Add more fields to daemonjob crd spec.

Change-Id: I1690035c7e35887245bbdcfdc97b19409fd6ab8a
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/06/712506/5 && git format-patch -1 --stdout FETCH_HEAD,"['tools/deployment/common/daemonjob-controller.sh', 'daemonjob-controller/templates/crd.yaml']",2,85e0ea4a52c4004ddce123431bc22333c35bd46d,, metadata: type: object properties: name: type: string annotations: type: object additionalProperties: type: string labels: type: object additionalProperties: type: string args: type: array items: type: string hostNetwork: type: boolean,,27,0
openstack%2Fproject-config~master~I0fa0ca21220a5e9c8b3e8c26c31daa4326701e19,openstack/project-config,master,I0fa0ca21220a5e9c8b3e8c26c31daa4326701e19,Revise gerrit ACLs for devstack-plugin-open-cas,MERGED,2020-03-19 20:27:32.000000000,2020-03-20 05:51:20.000000000,2020-03-20 05:51:19.000000000,"[{'_account_id': 5263}, {'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 20:27:32.000000000', 'files': ['gerrit/acls/openstack/devstack-plugin-open-cas.config'], 'web_link': 'https://opendev.org/openstack/project-config/commit/d435ae141aaa0d5410819d4f991d7655ca900275', 'message': ""Revise gerrit ACLs for devstack-plugin-open-cas\n\nThe devstack-plugin-open-cas repo was set up with cinder-core and\ndevstack-core having core permissions for the project.  Change this\nto a project-specific gerrit group 'devstack-plugin-open-cas-core'.\nThen cinder-core and devstack-core can be added as included groups\nin gerrit, which will allow us to add cores to the project who\naren't in cinder-core or devstack-core (i.e., it will basically make\ndevstack-plugin-open-cas-core self-maintaining, as is the usual\nsituation).\n\nChange-Id: I0fa0ca21220a5e9c8b3e8c26c31daa4326701e19\n""}]",0,713974,d435ae141aaa0d5410819d4f991d7655ca900275,7,3,1,5314,,,0,"Revise gerrit ACLs for devstack-plugin-open-cas

The devstack-plugin-open-cas repo was set up with cinder-core and
devstack-core having core permissions for the project.  Change this
to a project-specific gerrit group 'devstack-plugin-open-cas-core'.
Then cinder-core and devstack-core can be added as included groups
in gerrit, which will allow us to add cores to the project who
aren't in cinder-core or devstack-core (i.e., it will basically make
devstack-plugin-open-cas-core self-maintaining, as is the usual
situation).

Change-Id: I0fa0ca21220a5e9c8b3e8c26c31daa4326701e19
",git fetch https://review.opendev.org/openstack/project-config refs/changes/74/713974/1 && git format-patch -1 --stdout FETCH_HEAD,['gerrit/acls/openstack/devstack-plugin-open-cas.config'],1,d435ae141aaa0d5410819d4f991d7655ca900275,revise-open-cas-acls,abandon = group devstack-plugin-open-cas-core label-Code-Review = -2..+2 group devstack-plugin-open-cas-core label-Workflow = -1..+1 group devstack-plugin-open-cas-core,abandon = group cinder-core abandon = group devstack-core label-Code-Review = -2..+2 group cinder-core label-Code-Review = -2..+2 group devstack-core label-Workflow = -1..+1 group cinder-core label-Workflow = -1..+1 group devstack-core,3,6
openstack%2Fopenstack-helm-infra~master~I59982558c4a29c6611a28191206b1c8400b6a8c9,openstack/openstack-helm-infra,master,I59982558c4a29c6611a28191206b1c8400b6a8c9,gnocchi: Add metadata labels to CronJob,MERGED,2020-03-17 01:28:20.000000000,2020-03-20 04:59:29.000000000,2020-03-20 04:57:02.000000000,"[{'_account_id': 8898}, {'_account_id': 20466}, {'_account_id': 22348}, {'_account_id': 22636}, {'_account_id': 24780}, {'_account_id': 28701}]","[{'number': 1, 'created': '2020-03-17 01:28:20.000000000', 'files': ['gnocchi/templates/cron-job-resources-cleaner.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/44175bba47a2f367808a9f0c549422dca671b897', 'message': 'gnocchi: Add metadata labels to CronJob\n\nThis change adds the same helm-toolkit-generated metadata labels to\nthe CronJob itself that are applied to the Jobs it creates.\n\nChange-Id: I59982558c4a29c6611a28191206b1c8400b6a8c9\n'}]",0,713362,44175bba47a2f367808a9f0c549422dca671b897,11,6,1,28719,,,0,"gnocchi: Add metadata labels to CronJob

This change adds the same helm-toolkit-generated metadata labels to
the CronJob itself that are applied to the Jobs it creates.

Change-Id: I59982558c4a29c6611a28191206b1c8400b6a8c9
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/62/713362/1 && git format-patch -1 --stdout FETCH_HEAD,['gnocchi/templates/cron-job-resources-cleaner.yaml'],1,44175bba47a2f367808a9f0c549422dca671b897,cronjob-labels," labels: {{ tuple $envAll ""gnocchi"" ""resources-cleaner"" | include ""helm-toolkit.snippets.kubernetes_metadata_labels"" | indent 4 }}",,2,0
openstack%2Fopenstack-helm-infra~master~I888ca6f25c97e3deb6710e2e6be5a87a6133604b,openstack/openstack-helm-infra,master,I888ca6f25c97e3deb6710e2e6be5a87a6133604b,postgresql: Add metadata labels to CronJob,MERGED,2020-03-17 01:26:50.000000000,2020-03-20 04:58:25.000000000,2020-03-20 04:57:01.000000000,"[{'_account_id': 8898}, {'_account_id': 20466}, {'_account_id': 22348}, {'_account_id': 22636}, {'_account_id': 23140}, {'_account_id': 24780}, {'_account_id': 28701}, {'_account_id': 30746}]","[{'number': 1, 'created': '2020-03-17 01:26:50.000000000', 'files': ['postgresql/templates/cron-job-backup-postgres.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/3860dedef387e910d2628a725699a00443144f66', 'message': 'postgresql: Add metadata labels to CronJob\n\nThis change adds the same helm-toolkit-generated metadata labels to\nthe CronJob itself that are applied to the Jobs it creates.\n\nChange-Id: I888ca6f25c97e3deb6710e2e6be5a87a6133604b\n'}]",0,713358,3860dedef387e910d2628a725699a00443144f66,13,8,1,28719,,,0,"postgresql: Add metadata labels to CronJob

This change adds the same helm-toolkit-generated metadata labels to
the CronJob itself that are applied to the Jobs it creates.

Change-Id: I888ca6f25c97e3deb6710e2e6be5a87a6133604b
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/58/713358/1 && git format-patch -1 --stdout FETCH_HEAD,['postgresql/templates/cron-job-backup-postgres.yaml'],1,3860dedef387e910d2628a725699a00443144f66,cronjob-labels," labels: {{ tuple $envAll ""postgresql-backup"" ""backup"" | include ""helm-toolkit.snippets.kubernetes_metadata_labels"" | indent 4 }}",,2,0
openstack%2Fopenstack-helm-addons~master~I5350fd44714da5a14c0058dc96635126a7d2ba4f,openstack/openstack-helm-addons,master,I5350fd44714da5a14c0058dc96635126a7d2ba4f,Remove monasca from OpenStack-Helm-Addons repo,MERGED,2020-03-10 15:33:30.000000000,2020-03-20 04:46:11.000000000,2020-03-20 04:46:11.000000000,"[{'_account_id': 8898}, {'_account_id': 21420}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-10 15:33:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-addons/commit/3a8eafae2ff53a9015360ce6c4da99ff2febdb5a', 'message': 'Deprecate monasca from OpenStack-Helm-Repo\n\nBased on weekly IRC meeting, monasca and its related charts are\ndeprecated/removed from the OpenStack-Helm-AddOns repository.\n\n[0] http://eavesdrop.openstack.org/meetings/openstack_helm/2020/openstack_helm.2020-03-10-15.00.html\n\nChange-Id: I5350fd44714da5a14c0058dc96635126a7d2ba4f\nSigned-off-by: Tin Lam <tin@irrational.io>\n'}, {'number': 2, 'created': '2020-03-10 15:34:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-addons/commit/3f53a046d72858e867f60bc9a9e14b28565cd42e', 'message': 'Deprecate monasca from OpenStack-Helm-Addons repo\n\nBased on weekly IRC meeting, monasca and its related charts are\ndeprecated/removed from the OpenStack-Helm-AddOns repository.\n\n[0] http://eavesdrop.openstack.org/meetings/openstack_helm/2020/openstack_helm.2020-03-10-15.00.html\n\nChange-Id: I5350fd44714da5a14c0058dc96635126a7d2ba4f\nSigned-off-by: Tin Lam <tin@irrational.io>\n'}, {'number': 3, 'created': '2020-03-10 15:36:18.000000000', 'files': ['storm/templates/_helpers.tpl', 'monasca/values.yaml', 'monasca-agent/values.yaml', 'monasca-agent/Chart.yaml', 'storm/templates/nimbus-deployment.yaml', 'monasca-alarms/README.md', 'monasca/templates/mysql-api-secret.yaml', 'keystone-init/Chart.yaml', 'zookeeper/.helmignore', 'zookeeper/templates/zookeeper-deployment.yaml', 'monasca/templates/influx-init-job.yaml', 'monasca-agent/templates/_helpers.tpl', 'monasca-agent/templates/role.yaml', 'monasca/templates/mysql-keystone-secret.yaml', 'monasca/templates/agent-deployment.yaml', 'monasca/Chart.yaml', 'monasca/templates/static-keystone-svc.yaml', 'monasca/templates/cleanup-role.yaml', 'storm/Chart.yaml', 'keystone-init/templates/cleanup-hook.yaml', 'keystone-init/templates/_helpers.tpl', 'monasca/templates/_secret_env.tpl', 'mysql-users-init/templates/mysql-users-init-rolebinding.yaml', 'monasca/templates/persister-deployment.yaml', 'zookeeper/templates/_helpers.tpl', 'monasca/templates/agent-daemonset.yaml', 'monasca/templates/agent-configmap.yaml', 'monasca/templates/agent-serviceaccount.yaml', 'keystone-init/.helmignore', 'storm/values.yaml', 'zookeeper/templates/zookeeper-configmap.yaml', 'monasca/templates/keystone-configmap.yaml', 'monasca-alarms/templates/_helpers.tpl', 'monasca/templates/memcached-svc.yaml', 'mysql-users-init/templates/_helpers.tpl', 'mysql-users-init/templates/cleanup-rolebinding.yaml', 'zookeeper/templates/zookeeper-svc.yaml', 'monasca/templates/mysql-thresh-secret.yaml', 'mysql-users-init/templates/mysql-users-preload-configmap.yaml', 'zookeeper/templates/zookeeper-static-svc.yaml', 'monasca/templates/cleanup-hook.yaml', 'monasca/templates/mysql-grafana-secret.yaml', 'mysql-users-init/values.yaml', 'monasca/templates/smoke-test-pod.yaml', 'monasca-agent/.helmignore', 'monasca/templates/_helpers.tpl', 'monasca/templates/notification-hipchat-configmap.yaml', 'monasca/templates/agent-clusterrolebinding.yaml', 'mysql-users-init/Chart.yaml', 'monasca/templates/keystone-svc.yaml', 'monasca/.helmignore', 'monasca/templates/grafana-deployment.yaml', 'monasca/templates/thresh-deployment.yaml', 'zookeeper/values.yaml', 'storm/README.md', 'monasca-agent/templates/configmap.yaml', 'monasca/templates/mysql-init-job.yaml', 'keystone-init/templates/cleanup-serviceaccount.yaml', 'monasca-agent/templates/daemonset.yaml', 'monasca-agent/templates/deployment.yaml', 'keystone-init/templates/keystone-role.yaml', 'monasca/templates/alarms-configmap.yaml', 'monasca/templates/cleanup-rolebinding.yaml', 'zookeeper/README.md', 'monasca/templates/forwarder-deployment.yaml', 'mysql-users-init/templates/mysql-users-init-role.yaml', 'mysql-users-init/templates/mysql-users-init-job.yaml', 'mysql-users-init/templates/NOTES.txt', 'monasca/requirements.yaml', 'monasca/templates/grafana-svc.yaml', 'keystone-init/templates/NOTES.txt', 'monasca/README.md', 'keystone-init/templates/_keystone_env.tpl', 'keystone-init/values.yaml', 'monasca/templates/mysql-notification-secret.yaml', 'storm/.helmignore', 'monasca/templates/alarm-definition-resource.yaml', 'keystone-init/templates/keystone-rolebinding.yaml', 'monasca/templates/grafana-init-job.yaml', 'storm/templates/nimbus-pvc.yaml', 'zookeeper/Chart.yaml', 'monasca/templates/grafana-configmap.yaml', 'mysql-users-init/templates/cleanup-serviceaccount.yaml', 'storm/templates/supervisor-deployment.yaml', 'mysql-users-init/templates/cleanup-hook.yaml', 'monasca/templates/alarm-definition-controller-deployment.yaml', 'monasca/templates/memcached-deployment.yaml', 'mysql-users-init/.helmignore', 'monasca/templates/aggregator-configmap.yaml', 'monasca/templates/keystone-deployment.yaml', 'keystone-init/templates/keystone-init-job.yaml', 'monasca/templates/tempest-tests-pod.yaml', 'keystone-init/templates/keystone-preload-configmap.yaml', 'monasca/templates/cleanup-serviceaccount.yaml', 'monasca/templates/api-svc.yaml', 'monasca/templates/notification-deployment.yaml', 'storm/templates/NOTES.txt', 'monasca/templates/alarms-init-job.yaml', 'keystone-init/templates/cleanup-role.yaml', 'monasca/templates/agent-clusterrole.yaml', 'storm/templates/nimbus-svc.yaml', 'monasca-alarms/values.yaml', 'monasca/templates/thresh-init-job.yaml', 'mysql-users-init/templates/cleanup-role.yaml', 'zookeeper/templates/zookeeper-pvc.yaml', 'monasca-alarms/templates/alarms.yaml', 'mysql-users-init/templates/mysql-users-init-serviceaccount.yaml', 'monasca/templates/forwarder-configmap.yaml', 'keystone-init/templates/keystone-serviceaccount.yaml', 'monasca/templates/client-deployment.yaml', 'monasca/templates/aggregator-deployment.yaml', 'monasca/templates/static-api-svc.yaml', 'monasca-alarms/Chart.yaml', 'monasca/templates/api-deployment.yaml', 'keystone-init/templates/cleanup-rolebinding.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-addons/commit/760a12f5755c9f1612faeaf74f8a366fe0f96c33', 'message': 'Remove monasca from OpenStack-Helm-Addons repo\n\nBased on weekly IRC meeting, monasca and its related charts are\nremoved from the OpenStack-Helm-AddOns repository.\n\n[0] http://eavesdrop.openstack.org/meetings/openstack_helm/2020/openstack_helm.2020-03-10-15.00.html\n\nChange-Id: I5350fd44714da5a14c0058dc96635126a7d2ba4f\nSigned-off-by: Tin Lam <tin@irrational.io>\n'}]",0,712110,760a12f5755c9f1612faeaf74f8a366fe0f96c33,10,3,3,20466,,,0,"Remove monasca from OpenStack-Helm-Addons repo

Based on weekly IRC meeting, monasca and its related charts are
removed from the OpenStack-Helm-AddOns repository.

[0] http://eavesdrop.openstack.org/meetings/openstack_helm/2020/openstack_helm.2020-03-10-15.00.html

Change-Id: I5350fd44714da5a14c0058dc96635126a7d2ba4f
Signed-off-by: Tin Lam <tin@irrational.io>
",git fetch https://review.opendev.org/openstack/openstack-helm-addons refs/changes/10/712110/2 && git format-patch -1 --stdout FETCH_HEAD,"['storm/templates/_helpers.tpl', 'monasca/values.yaml', 'monasca-agent/values.yaml', 'monasca-agent/Chart.yaml', 'storm/templates/nimbus-deployment.yaml', 'monasca-alarms/README.md', 'monasca/templates/mysql-api-secret.yaml', 'keystone-init/Chart.yaml', 'zookeeper/.helmignore', 'zookeeper/templates/zookeeper-deployment.yaml', 'monasca/templates/influx-init-job.yaml', 'monasca-agent/templates/_helpers.tpl', 'monasca-agent/templates/role.yaml', 'monasca/templates/mysql-keystone-secret.yaml', 'monasca/templates/agent-deployment.yaml', 'monasca/Chart.yaml', 'monasca/templates/static-keystone-svc.yaml', 'monasca/templates/cleanup-role.yaml', 'storm/Chart.yaml', 'keystone-init/templates/cleanup-hook.yaml', 'keystone-init/templates/_helpers.tpl', 'monasca/templates/_secret_env.tpl', 'mysql-users-init/templates/mysql-users-init-rolebinding.yaml', 'monasca/templates/persister-deployment.yaml', 'zookeeper/templates/_helpers.tpl', 'monasca/templates/agent-daemonset.yaml', 'monasca/templates/agent-configmap.yaml', 'monasca/templates/agent-serviceaccount.yaml', 'keystone-init/.helmignore', 'storm/values.yaml', 'zookeeper/templates/zookeeper-configmap.yaml', 'monasca/templates/keystone-configmap.yaml', 'monasca-alarms/templates/_helpers.tpl', 'monasca/templates/memcached-svc.yaml', 'mysql-users-init/templates/_helpers.tpl', 'mysql-users-init/templates/cleanup-rolebinding.yaml', 'zookeeper/templates/zookeeper-svc.yaml', 'monasca/templates/mysql-thresh-secret.yaml', 'mysql-users-init/templates/mysql-users-preload-configmap.yaml', 'zookeeper/templates/zookeeper-static-svc.yaml', 'monasca/templates/cleanup-hook.yaml', 'monasca/templates/mysql-grafana-secret.yaml', 'mysql-users-init/values.yaml', 'monasca/templates/smoke-test-pod.yaml', 'monasca-agent/.helmignore', 'monasca/templates/_helpers.tpl', 'monasca/templates/notification-hipchat-configmap.yaml', 'monasca/templates/agent-clusterrolebinding.yaml', 'mysql-users-init/Chart.yaml', 'monasca/templates/keystone-svc.yaml', 'monasca/.helmignore', 'monasca/templates/grafana-deployment.yaml', 'monasca/templates/thresh-deployment.yaml', 'zookeeper/values.yaml', 'storm/README.md', 'monasca-agent/templates/configmap.yaml', 'monasca/templates/mysql-init-job.yaml', 'keystone-init/templates/cleanup-serviceaccount.yaml', 'monasca-agent/templates/daemonset.yaml', 'monasca-agent/templates/deployment.yaml', 'keystone-init/templates/keystone-role.yaml', 'monasca/templates/alarms-configmap.yaml', 'monasca/templates/cleanup-rolebinding.yaml', 'zookeeper/README.md', 'monasca/templates/forwarder-deployment.yaml', 'mysql-users-init/templates/mysql-users-init-role.yaml', 'mysql-users-init/templates/mysql-users-init-job.yaml', 'mysql-users-init/templates/NOTES.txt', 'monasca/requirements.yaml', 'monasca/templates/grafana-svc.yaml', 'keystone-init/templates/NOTES.txt', 'monasca/README.md', 'keystone-init/templates/_keystone_env.tpl', 'keystone-init/values.yaml', 'monasca/templates/mysql-notification-secret.yaml', 'storm/.helmignore', 'monasca/templates/alarm-definition-resource.yaml', 'keystone-init/templates/keystone-rolebinding.yaml', 'monasca/templates/grafana-init-job.yaml', 'storm/templates/nimbus-pvc.yaml', 'zookeeper/Chart.yaml', 'monasca/templates/grafana-configmap.yaml', 'mysql-users-init/templates/cleanup-serviceaccount.yaml', 'storm/templates/supervisor-deployment.yaml', 'mysql-users-init/templates/cleanup-hook.yaml', 'monasca/templates/alarm-definition-controller-deployment.yaml', 'monasca/templates/memcached-deployment.yaml', 'mysql-users-init/.helmignore', 'monasca/templates/aggregator-configmap.yaml', 'monasca/templates/keystone-deployment.yaml', 'keystone-init/templates/keystone-init-job.yaml', 'monasca/templates/tempest-tests-pod.yaml', 'keystone-init/templates/keystone-preload-configmap.yaml', 'monasca/templates/cleanup-serviceaccount.yaml', 'monasca/templates/api-svc.yaml', 'monasca/templates/notification-deployment.yaml', 'storm/templates/NOTES.txt', 'monasca/templates/alarms-init-job.yaml', 'keystone-init/templates/cleanup-role.yaml', 'monasca/templates/agent-clusterrole.yaml', 'storm/templates/nimbus-svc.yaml', 'monasca-alarms/values.yaml', 'monasca/templates/thresh-init-job.yaml', 'mysql-users-init/templates/cleanup-role.yaml', 'zookeeper/templates/zookeeper-pvc.yaml', 'monasca-alarms/templates/alarms.yaml', 'mysql-users-init/templates/mysql-users-init-serviceaccount.yaml', 'monasca/templates/forwarder-configmap.yaml', 'keystone-init/templates/keystone-serviceaccount.yaml', 'monasca/templates/client-deployment.yaml', 'monasca/templates/aggregator-deployment.yaml', 'monasca/templates/static-api-svc.yaml', 'monasca-alarms/Chart.yaml', 'monasca/templates/api-deployment.yaml', 'keystone-init/templates/cleanup-rolebinding.yaml']",115,3a8eafae2ff53a9015360ce6c4da99ff2febdb5a,rm-monasca,,"{{- if and (.Values.rbac.create) (not .Values.cleanup.serviceAccount) }} {{- if .Capabilities.APIVersions.Has ""rbac.authorization.k8s.io/v1"" }} apiVersion: rbac.authorization.k8s.io/v1 {{- else if .Capabilities.APIVersions.Has ""rbac.authorization.k8s.io/v1beta1"" }} apiVersion: rbac.authorization.k8s.io/v1beta1 {{- else if .Capabilities.APIVersions.Has ""rbac.authorization.k8s.io/v1alpha1"" }} apiVersion: rbac.authorization.k8s.io/v1alpha1 {{- end }} kind: RoleBinding metadata: name: {{ template ""cleanup.fullname"" . }} labels: app: {{ template ""fullname"" . }} chart: {{ .Chart.Name }}-{{ .Chart.Version }} component: ""{{ .Values.cleanup.name }}"" heritage: {{ .Release.Service }} release: {{ .Release.Name }} subjects: - kind: ServiceAccount name: {{ template ""cleanup.fullname"" . }} namespace: ""{{ .Release.Namespace }}"" roleRef: kind: Role name: {{ template ""cleanup.fullname"" . }} apiGroup: rbac.authorization.k8s.io {{- end }} ",0,7028
openstack%2Fopenstack-helm-addons~master~I631ae4345f18fee70b380867ba8b33af5e3b3254,openstack/openstack-helm-addons,master,I631ae4345f18fee70b380867ba8b33af5e3b3254,Remove OSH Authors copyright,MERGED,2020-03-02 20:02:16.000000000,2020-03-20 04:44:49.000000000,2020-03-20 04:44:49.000000000,"[{'_account_id': 8898}, {'_account_id': 20466}, {'_account_id': 22348}, {'_account_id': 28701}, {'_account_id': 29144}]","[{'number': 1, 'created': '2020-03-02 20:02:16.000000000', 'files': ['ranger/templates/service-ranger.yaml', 'tools/deployment/common/wait-for-pods.sh', 'artifactory/templates/secret-db.yaml', 'tools/deployment/common/deploy-k8s.sh', 'ranger-agent/templates/bin/_health-probe.py.tpl', 'ranger-agent/templates/service-ranger-agent-api.yaml', 'artifactory/values.yaml', 'mini-mirror/templates/service-ingress-mini-mirror.yaml', 'ranger-agent/templates/bin/_db-sync.sh.tpl', 'ranger-agent/templates/secret-ssh-key.yaml', 'ranger/templates/configmap-etc.yaml', 'ranger/templates/secret-ssh-key.yaml', 'sonobuoy/templates/serviceaccount-readonly.yaml', 'artifactory/templates/configmap-etc.yaml', 'ranger-agent/templates/secret-keystone.yaml', 'tools/gate/playbooks/zuul-linter.yaml', 'ranger/values.yaml', 'ranger-agent/templates/bin/_ranger-agent-engine.sh.tpl', 'shaker/templates/secret-keystone.yaml', 'ranger/templates/pdb-api.yaml', 'ranger-agent/templates/bin/_health-check.sh.tpl', 'ranger-agent/templates/job-db-init.yaml', 'ranger-agent/templates/pdb-api.yaml', 'ranger/templates/service-ingress-uuid.yaml', 'artifactory/templates/job-db-drop.yaml', 'sonobuoy/templates/secret-plugin-values.yaml', 'tools/gate/playbooks/osh-infra-upgrade-host.yaml', 'tools/gate/playbooks/vars.yaml', 'shaker/templates/pod-shaker-test.yaml', 'shaker/values.yaml', 'ranger/templates/secret-ingress-tls.yaml', 'ranger-agent/templates/bin/_ranger-agent-api.sh.tpl', 'sonobuoy/templates/secret-etc.yaml', 'mini-mirror/templates/secret-ingress-tls.yaml', 'ranger/templates/job-image-repo-sync.yaml', 'prometheus-bot/templates/secret-telegram-token.yaml', 'ranger-agent/templates/deployment-ranger-agent-api.yaml', 'shaker/templates/job-image-repo-sync.yaml', 'sonobuoy/Chart.yaml', 'shaker/Chart.yaml', 'ranger/requirements.yaml', 'artifactory/templates/statefulset.yaml', 'artifactory/templates/job-image-repo-sync.yaml', 'ranger-agent/templates/job-ks-user.yaml', 'ranger/templates/configmap-bin.yaml', 'prometheus-bot/requirements.yaml', 'ranger/templates/bin/_db-sync.sh.tpl', 'ranger-agent/templates/job-image-repo-sync.yaml', 'ranger-agent/values.yaml', 'ranger-agent/templates/deployment-ranger-agent-engine.yaml', 'ranger-agent/templates/job-db-sync.yaml', 'ranger-agent/templates/job-db-drop.yaml', 'artifactory/templates/job-db-init.yaml', 'tools/gate/playbooks/osh-infra-build.yaml', 'ranger/templates/service-ingress-rms.yaml', 'ranger-agent/templates/secret-ingress-tls.yaml', 'shaker/templates/job-ks-user.yaml', 'ranger/templates/service-ingress-fms.yaml', 'tools/gate/playbooks/osh-infra-deploy-k8s.yaml', 'mini-mirror/templates/ingress-mini-mirror.yaml', 'artifactory/requirements.yaml', 'ranger-agent/templates/job-ks-endpoints.yaml', 'ranger/templates/ingress-ranger.yaml', 'ranger/templates/bin/_health-probe.py.tpl', 'sonobuoy/templates/job-ks-user.yaml', 'shaker/templates/configmap-etc.yaml', 'ranger/templates/bin/_ranger-services.sh.tpl', 'ranger-agent/templates/job-ks-user-ranger-admin.yaml', 'sonobuoy/templates/configmap-bin.yaml', 'tools/gate/playbooks/osh-infra-deploy-docker.yaml', 'ranger-agent/templates/configmap-bin.yaml', 'tools/deployment/component/sonobuoy/sonobuoy.sh', 'prometheus-bot/templates/service.yaml', 'ranger-agent/requirements.yaml', 'ranger/templates/job-db-drop.yaml', 'tools/deployment/component/mini-mirror/mini-mirror.sh', 'shaker/templates/pvc-shaker.yaml', 'ranger-agent/templates/service-ingress-ranger-agent-api.yaml', 'artifactory/Chart.yaml', 'ranger/templates/bin/_ranger-test.py.tpl', 'ranger/templates/pod-test.yaml', 'sonobuoy/requirements.yaml', 'ranger/templates/service-ingress-ims.yaml', 'ranger/templates/service-ingress-cms.yaml', 'shaker/templates/service-shaker.yaml', 'prometheus-bot/values.yaml', 'sonobuoy/templates/bin/_publish_results.sh.tpl', 'ranger-agent/templates/pod-test.yaml', 'artifactory/templates/configmap-bin.yaml', 'Makefile', 'ranger-agent/templates/secret-db.yaml', 'ranger/templates/service-ingress-audit.yaml', 'sonobuoy/templates/bin/_run_master.sh.tpl', 'artifactory/templates/secret-db-creds.yaml', 'tools/deployment/component/shaker/shaker.sh', 'ranger-agent/templates/configmap-etc.yaml', 'ranger/templates/job-db-sync.yaml', 'ranger/templates/service-ingress-rds.yaml', 'ranger-agent/templates/job-ks-user-ranger.yaml', 'shaker/templates/configmap-bin.yaml', 'ranger/templates/deployment-ranger-services.yaml', 'tools/gate/playbooks/osh-addons-check.yaml', 'sonobuoy/values.yaml', 'ranger-agent/templates/ingress-ranger-agent-api.yaml', 'tools/gate/playbooks/osh-infra-collect-logs.yaml', 'sonobuoy/templates/configmap-plugins.yaml', 'shaker/requirements.yaml', 'sonobuoy/templates/pod-api.yaml', 'ranger/templates/secret-db.yaml', 'ranger/templates/job-db-init.yaml', 'shaker/templates/bin/_run-tests.sh.tpl', 'sonobuoy/templates/secret-keystone.yaml', 'ranger-agent/templates/secret-rabbitmq.yaml', 'artifactory/templates/service.yaml', 'ranger-agent/templates/job-ks-service.yaml', 'prometheus-bot/templates/deployment.yaml', 'ranger-agent/templates/job-rabbit-init.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-addons/commit/09b6e52c32c8f4fc9c3ac0da9f01ea3cd0974d30', 'message': 'Remove OSH Authors copyright\n\nThe current copyright refers to a non-existent group\n""openstack helm authors"" with often out-of-date references that\nare confusing when adding a new file to the repo.\n\nThis change removes all references to this copyright by the\nnon-existent group and any blank lines underneath.\n\nChange-Id: I631ae4345f18fee70b380867ba8b33af5e3b3254\n'}]",0,710863,09b6e52c32c8f4fc9c3ac0da9f01ea3cd0974d30,9,5,1,21420,,,0,"Remove OSH Authors copyright

The current copyright refers to a non-existent group
""openstack helm authors"" with often out-of-date references that
are confusing when adding a new file to the repo.

This change removes all references to this copyright by the
non-existent group and any blank lines underneath.

Change-Id: I631ae4345f18fee70b380867ba8b33af5e3b3254
",git fetch https://review.opendev.org/openstack/openstack-helm-addons refs/changes/63/710863/1 && git format-patch -1 --stdout FETCH_HEAD,"['ranger/templates/service-ranger.yaml', 'tools/deployment/common/wait-for-pods.sh', 'artifactory/templates/secret-db.yaml', 'tools/deployment/common/deploy-k8s.sh', 'ranger-agent/templates/bin/_health-probe.py.tpl', 'ranger-agent/templates/service-ranger-agent-api.yaml', 'artifactory/values.yaml', 'mini-mirror/templates/service-ingress-mini-mirror.yaml', 'ranger-agent/templates/bin/_db-sync.sh.tpl', 'ranger-agent/templates/secret-ssh-key.yaml', 'ranger/templates/configmap-etc.yaml', 'ranger/templates/secret-ssh-key.yaml', 'sonobuoy/templates/serviceaccount-readonly.yaml', 'artifactory/templates/configmap-etc.yaml', 'ranger-agent/templates/secret-keystone.yaml', 'tools/gate/playbooks/zuul-linter.yaml', 'ranger/values.yaml', 'ranger-agent/templates/bin/_ranger-agent-engine.sh.tpl', 'shaker/templates/secret-keystone.yaml', 'ranger/templates/pdb-api.yaml', 'ranger-agent/templates/bin/_health-check.sh.tpl', 'ranger-agent/templates/job-db-init.yaml', 'ranger-agent/templates/pdb-api.yaml', 'ranger/templates/service-ingress-uuid.yaml', 'artifactory/templates/job-db-drop.yaml', 'sonobuoy/templates/secret-plugin-values.yaml', 'tools/gate/playbooks/osh-infra-upgrade-host.yaml', 'tools/gate/playbooks/vars.yaml', 'shaker/templates/pod-shaker-test.yaml', 'shaker/values.yaml', 'ranger/templates/secret-ingress-tls.yaml', 'ranger-agent/templates/bin/_ranger-agent-api.sh.tpl', 'sonobuoy/templates/secret-etc.yaml', 'mini-mirror/templates/secret-ingress-tls.yaml', 'ranger/templates/job-image-repo-sync.yaml', 'prometheus-bot/templates/secret-telegram-token.yaml', 'ranger-agent/templates/deployment-ranger-agent-api.yaml', 'shaker/templates/job-image-repo-sync.yaml', 'sonobuoy/Chart.yaml', 'shaker/Chart.yaml', 'ranger/requirements.yaml', 'artifactory/templates/statefulset.yaml', 'artifactory/templates/job-image-repo-sync.yaml', 'ranger-agent/templates/job-ks-user.yaml', 'ranger/templates/configmap-bin.yaml', 'prometheus-bot/requirements.yaml', 'ranger/templates/bin/_db-sync.sh.tpl', 'ranger-agent/templates/job-image-repo-sync.yaml', 'ranger-agent/values.yaml', 'ranger-agent/templates/deployment-ranger-agent-engine.yaml', 'ranger-agent/templates/job-db-sync.yaml', 'ranger-agent/templates/job-db-drop.yaml', 'artifactory/templates/job-db-init.yaml', 'tools/gate/playbooks/osh-infra-build.yaml', 'ranger/templates/service-ingress-rms.yaml', 'ranger-agent/templates/secret-ingress-tls.yaml', 'shaker/templates/job-ks-user.yaml', 'ranger/templates/service-ingress-fms.yaml', 'tools/gate/playbooks/osh-infra-deploy-k8s.yaml', 'mini-mirror/templates/ingress-mini-mirror.yaml', 'artifactory/requirements.yaml', 'ranger-agent/templates/job-ks-endpoints.yaml', 'ranger/templates/ingress-ranger.yaml', 'ranger/templates/bin/_health-probe.py.tpl', 'sonobuoy/templates/job-ks-user.yaml', 'shaker/templates/configmap-etc.yaml', 'ranger/templates/bin/_ranger-services.sh.tpl', 'ranger-agent/templates/job-ks-user-ranger-admin.yaml', 'sonobuoy/templates/configmap-bin.yaml', 'tools/gate/playbooks/osh-infra-deploy-docker.yaml', 'ranger-agent/templates/configmap-bin.yaml', 'tools/deployment/component/sonobuoy/sonobuoy.sh', 'prometheus-bot/templates/service.yaml', 'ranger-agent/requirements.yaml', 'ranger/templates/job-db-drop.yaml', 'tools/deployment/component/mini-mirror/mini-mirror.sh', 'shaker/templates/pvc-shaker.yaml', 'ranger-agent/templates/service-ingress-ranger-agent-api.yaml', 'artifactory/Chart.yaml', 'ranger/templates/bin/_ranger-test.py.tpl', 'ranger/templates/pod-test.yaml', 'sonobuoy/requirements.yaml', 'ranger/templates/service-ingress-ims.yaml', 'ranger/templates/service-ingress-cms.yaml', 'shaker/templates/service-shaker.yaml', 'prometheus-bot/values.yaml', 'sonobuoy/templates/bin/_publish_results.sh.tpl', 'ranger-agent/templates/pod-test.yaml', 'artifactory/templates/configmap-bin.yaml', 'Makefile', 'ranger-agent/templates/secret-db.yaml', 'ranger/templates/service-ingress-audit.yaml', 'sonobuoy/templates/bin/_run_master.sh.tpl', 'artifactory/templates/secret-db-creds.yaml', 'tools/deployment/component/shaker/shaker.sh', 'ranger-agent/templates/configmap-etc.yaml', 'ranger/templates/job-db-sync.yaml', 'ranger/templates/service-ingress-rds.yaml', 'ranger-agent/templates/job-ks-user-ranger.yaml', 'shaker/templates/configmap-bin.yaml', 'ranger/templates/deployment-ranger-services.yaml', 'tools/gate/playbooks/osh-addons-check.yaml', 'sonobuoy/values.yaml', 'ranger-agent/templates/ingress-ranger-agent-api.yaml', 'tools/gate/playbooks/osh-infra-collect-logs.yaml', 'sonobuoy/templates/configmap-plugins.yaml', 'shaker/requirements.yaml', 'sonobuoy/templates/pod-api.yaml', 'ranger/templates/secret-db.yaml', 'ranger/templates/job-db-init.yaml', 'shaker/templates/bin/_run-tests.sh.tpl', 'sonobuoy/templates/secret-keystone.yaml', 'ranger-agent/templates/secret-rabbitmq.yaml', 'artifactory/templates/service.yaml', 'ranger-agent/templates/job-ks-service.yaml', 'prometheus-bot/templates/deployment.yaml', 'ranger-agent/templates/job-rabbit-init.yaml']",117,09b6e52c32c8f4fc9c3ac0da9f01ea3cd0974d30,remove-osh-authors,,Copyright 2017 The Openstack-Helm Authors. ,0,231
openstack%2Fopenstack-helm-infra~master~I937fdade5f36863d14c5ca8a322af72a6e15f621,openstack/openstack-helm-infra,master,I937fdade5f36863d14c5ca8a322af72a6e15f621,Add SUSE distro and bring your own k8s support,ABANDONED,2018-12-05 04:30:04.000000000,2020-03-20 04:31:35.000000000,,"[{'_account_id': 17068}, {'_account_id': 22348}, {'_account_id': 22477}, {'_account_id': 23928}, {'_account_id': 28618}]","[{'number': 1, 'created': '2018-12-05 04:30:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/e757a2eae6dc57679da0b4d7c79a2ad02be7700e', 'message': '(WIP) Add SUSE distro and existing k8s support for 010-deploy-k8s.sh\n\nAdd the SUSE distro support in setting up host and osh infra build playbooks.\nAlso added new playbook to deploy kubectl only in the bring your own k8s use\ncase, instead of bootstraping a new kubeadm. It is a part of effort to\nenable Airskiff on SUSE distro and also a part of the series of efforts to add\nSUSE distro support in OSH.\n\nChange-Id: I937fdade5f36863d14c5ca8a322af72a6e15f621\n'}, {'number': 2, 'created': '2018-12-06 06:35:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/268da56c919447ed47d1b8ea3b30d61d6932457f', 'message': '(WIP) Add SUSE distro and bring your own k8s support\n\nAdd the SUSE distro support in setting up host and osh infra build playbooks.\nAlso add new playbook to deploy kubectl only in the bring your own k8s use\ncase, instead of bootstraping a new kubeadm. It is a part of effort to\nenable Airskiff on SUSE distro and also a part of the series of efforts to add\nSUSE distro support in OSH.\n\nChange-Id: I937fdade5f36863d14c5ca8a322af72a6e15f621\n'}, {'number': 3, 'created': '2018-12-07 18:24:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/c937c5afc8aa3ff853ca48e482257a1e706ca04e', 'message': 'Add SUSE distro and bring your own k8s support\n\nAdd the SUSE distro support in setting up host and osh infra build playbooks.\nAlso add new playbook to deploy kubectl only in the bring your own k8s use\ncase, instead of bootstraping a new kubeadm. It is a part of effort to\nenable Airskiff on SUSE distro and also a part of the series of efforts to add\nSUSE distro support in OSH.\n\nChange-Id: I937fdade5f36863d14c5ca8a322af72a6e15f621\n'}, {'number': 4, 'created': '2018-12-07 18:55:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/d3172efd4e99987306be6b146a49314c87e290b4', 'message': 'Add SUSE distro and bring your own k8s support\n\nAdd the SUSE distro support in setting up host and osh infra build playbooks.\nAlso add new playbook to deploy kubectl only in the bring your own k8s use\ncase, instead of bootstraping a new kubeadm. It is a part of effort to\nenable Airskiff on SUSE distro and also a part of the series of efforts to add\nSUSE support in OSH.\n\nChange-Id: I937fdade5f36863d14c5ca8a322af72a6e15f621\n'}, {'number': 5, 'created': '2018-12-07 22:41:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/efc916bbdb096aba4c1e9a785a5caf7b2ece88ca', 'message': 'Add SUSE distro and bring your own k8s support\n\nAdd the SUSE distro support in setting up host and osh infra build playbooks.\nAlso add new playbook to deploy kubectl only in the bring your own k8s use\ncase, instead of bootstraping a new kubeadm. It is a part of effort to\nenable Airskiff on SUSE distro and also a part of the series of efforts to add\nSUSE support in OSH.\n\nChange-Id: I937fdade5f36863d14c5ca8a322af72a6e15f621\n'}, {'number': 6, 'created': '2019-01-09 21:37:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/cd9094b496404137ca15ed27e457a130283e6584', 'message': 'Add SUSE distro and bring your own k8s support\n\nAdd the SUSE distro support in setting up host and osh infra build playbooks.\nAlso add new playbook to deploy kubectl only in the bring your own k8s use\ncase, instead of bootstraping a new kubeadm. It is a part of effort to\nenable Airskiff on SUSE distro and also a part of the series of efforts to add\nSUSE support in OSH.\n\nChange-Id: I937fdade5f36863d14c5ca8a322af72a6e15f621\n'}, {'number': 7, 'created': '2019-01-12 08:06:59.000000000', 'files': ['roles/deploy-docker/tasks/main.yaml', 'roles/deploy-docker/templates/suse-docker.service.j2', 'roles/deploy-kubectl/tasks/main.yaml', 'roles/deploy-kubectl/tasks/setup-helm-serve.yaml', 'roles/deploy-kubectl/defaults/main.yml', 'roles/deploy-python/tasks/main.yaml', 'tools/gate/devel/start.sh', 'roles/deploy-jq/tasks/main.yaml', 'roles/deploy-package/tasks/dist.yaml', 'playbooks/osh-infra-deploy-kubectl.yaml', 'roles/deploy-python-pip/tasks/main.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/66d6fbf173de8603ca4fb7383878292ed48c9bcb', 'message': 'Add SUSE distro and bring your own k8s support\n\nAdd the SUSE distro support in setting up host and osh infra build playbooks.\nAlso add new playbook to deploy kubectl only in the bring your own k8s use\ncase, instead of bootstraping a new kubeadm. It is a part of effort to\nenable Airskiff on SUSE distro and also a part of the series of efforts to add\nSUSE support in OSH.\n\nChange-Id: I937fdade5f36863d14c5ca8a322af72a6e15f621\n'}]",0,622823,66d6fbf173de8603ca4fb7383878292ed48c9bcb,16,5,7,24580,,,0,"Add SUSE distro and bring your own k8s support

Add the SUSE distro support in setting up host and osh infra build playbooks.
Also add new playbook to deploy kubectl only in the bring your own k8s use
case, instead of bootstraping a new kubeadm. It is a part of effort to
enable Airskiff on SUSE distro and also a part of the series of efforts to add
SUSE support in OSH.

Change-Id: I937fdade5f36863d14c5ca8a322af72a6e15f621
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/23/622823/4 && git format-patch -1 --stdout FETCH_HEAD,"['roles/deploy-docker/tasks/main.yaml', 'roles/deploy-docker/templates/suse-docker.service.j2', 'roles/deploy-kubectl/tasks/main.yaml', 'roles/deploy-kubectl/tasks/setup-helm-serve.yaml', 'roles/deploy-kubectl/defaults/main.yml', 'roles/deploy-python/tasks/main.yaml', 'tools/gate/devel/start.sh', 'roles/deploy-jq/tasks/main.yaml', 'roles/deploy-package/tasks/dist.yaml', 'playbooks/osh-infra-deploy-kubectl.yaml', 'roles/deploy-python-pip/tasks/main.yaml']",11,e757a2eae6dc57679da0b4d7c79a2ad02be7700e,osh_suse,- name: ensuring python pip package is present for SUSE when: ( pip_version_output is failed ) and ( ansible_distribution == 'openSUSE Leap' or ansible_distribution == 'SLES' ) block: - name: ensuring python pip package is present for SUSE zypper: name: python-pip state: present ,,275,2
openstack%2Fopenstack-helm-infra~master~I70baf516901169ca54f2a032099911d47678f696,openstack/openstack-helm-infra,master,I70baf516901169ca54f2a032099911d47678f696,Test DNM,ABANDONED,2020-01-04 01:39:30.000000000,2020-03-20 04:29:33.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2020-01-04 01:39:30.000000000', 'files': ['tools/images/kubeadm-aio/Dockerfile'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/c5dec545bc65ec95f77c2aac0d23380d168e6817', 'message': 'Test DNM\n\nChange-Id: I70baf516901169ca54f2a032099911d47678f696\n'}]",0,701070,c5dec545bc65ec95f77c2aac0d23380d168e6817,3,1,1,28719,,,0,"Test DNM

Change-Id: I70baf516901169ca54f2a032099911d47678f696
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/70/701070/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/images/kubeadm-aio/Dockerfile'],1,c5dec545bc65ec95f77c2aac0d23380d168e6817,," ""ansible==2.5.5"" \ docker-py ;\"," ""ansible==2.5.5"" ;\",2,1
openstack%2Fopenstack-helm-images~master~Ifcfda0bbc8372758b5d1c62089e9e27b39ac90db,openstack/openstack-helm-images,master,Ifcfda0bbc8372758b5d1c62089e9e27b39ac90db,test,ABANDONED,2020-01-27 21:49:39.000000000,2020-03-20 04:25:05.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2020-01-27 21:49:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/afd54bb00211a39cfaadc679ba12eea2c76ffed1', 'message': 'test https://review.opendev.org/#/c/704433\n\nChange-Id: Ifcfda0bbc8372758b5d1c62089e9e27b39ac90db\n'}, {'number': 2, 'created': '2020-01-27 21:54:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/0382089b157dc0f068950df05c162353f07fd730', 'message': 'test https://review.opendev.org/#/c/704433\n\nChange-Id: Ifcfda0bbc8372758b5d1c62089e9e27b39ac90db\n'}, {'number': 3, 'created': '2020-01-27 22:44:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/21ee7d2abc71f7563841b565bad1a980f8a47f02', 'message': 'test\n\nDepends-On: https://review.opendev.org/#/c/704433\n\nChange-Id: Ifcfda0bbc8372758b5d1c62089e9e27b39ac90db\n'}, {'number': 4, 'created': '2020-01-27 22:44:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/43f46b16b3ee5d826ea25b2c90faaa1965807acc', 'message': 'test\n\nDepends-On: https://review.opendev.org/#/c/704433\n\nChange-Id: Ifcfda0bbc8372758b5d1c62089e9e27b39ac90db\n'}, {'number': 5, 'created': '2020-02-11 06:00:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/0e0c8995c484a455f9be27c77164bfdc89f8bfe7', 'message': 'test\n\nDepends-On: https://review.opendev.org/#/c/707058/\n\nChange-Id: Ifcfda0bbc8372758b5d1c62089e9e27b39ac90db\n'}, {'number': 6, 'created': '2020-02-11 06:00:14.000000000', 'files': ['openstack/loci/build-ocata.sh', 'openstack/loci/build-rocky.sh', 'openstack/loci/build-queens.sh', 'openstack/loci/build-stein.sh', 'openstack/loci/build-newton.sh', 'openstack/loci/build-pike.sh', 'openstack/loci/build.sh'], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/1760f05de1246fd4bcd2cafa0a54dbb5c4c8186b', 'message': 'test\n\nDepends-On: https://review.opendev.org/#/c/707058/\n\nChange-Id: Ifcfda0bbc8372758b5d1c62089e9e27b39ac90db\n'}]",0,704438,1760f05de1246fd4bcd2cafa0a54dbb5c4c8186b,9,1,6,8863,,,0,"test

Depends-On: https://review.opendev.org/#/c/707058/

Change-Id: Ifcfda0bbc8372758b5d1c62089e9e27b39ac90db
",git fetch https://review.opendev.org/openstack/openstack-helm-images refs/changes/38/704438/5 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/playbooks/build-loci.yml'],1,afd54bb00211a39cfaadc679ba12eea2c76ffed1,," - name: Checkout to loci open change command: ""cd {{ zuul.project.src_dir }}; git fetch https://review.opendev.org/openstack/loci refs/changes/33/704433/1 && git checkout FETCH_HEAD;"" ",,3,0
openstack%2Fopenstack-helm-images~master~Ia781c99c53cd54b8e7f39765e48efe45ec65d9ca,openstack/openstack-helm-images,master,Ia781c99c53cd54b8e7f39765e48efe45ec65d9ca,test,ABANDONED,2020-01-27 22:07:37.000000000,2020-03-20 04:24:54.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2020-01-27 22:07:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/af2b7aa2339ddb7d5b8e3db06657c29da2370986', 'message': 'test\n\nDepends-On: https://review.opendev.org/#/c/703647/\nChange-Id: Ia781c99c53cd54b8e7f39765e48efe45ec65d9ca\n'}]",0,704444,af2b7aa2339ddb7d5b8e3db06657c29da2370986,3,1,1,8863,,,0,"test

Depends-On: https://review.opendev.org/#/c/703647/
Change-Id: Ia781c99c53cd54b8e7f39765e48efe45ec65d9ca
",git fetch https://review.opendev.org/openstack/openstack-helm-images refs/changes/44/704444/1 && git format-patch -1 --stdout FETCH_HEAD,[],0,af2b7aa2339ddb7d5b8e3db06657c29da2370986,,,,0,0
openstack%2Fswift~feature%2Flosf~Ie0ec8dbb7e0d875d03ff9ae2eba3e95a21ae5db3,openstack/swift,feature/losf,Ie0ec8dbb7e0d875d03ff9ae2eba3e95a21ae5db3,"Raise an exception if get_vfile() fails, rather than ignoring the error",MERGED,2020-03-19 15:47:50.000000000,2020-03-20 04:18:18.000000000,2020-03-20 04:16:50.000000000,"[{'_account_id': 13852}, {'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 15:47:50.000000000', 'files': ['swift/obj/kvfile.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/df7f895ff8467344967795529056270a5ea93546', 'message': 'Raise an exception if get_vfile() fails, rather than ignoring the error\n\nWithout the patch we see ugly exceptions on machines with failing disks, or with\ncorrupted metadata\n\nChange-Id: Ie0ec8dbb7e0d875d03ff9ae2eba3e95a21ae5db3\n'}]",2,713911,df7f895ff8467344967795529056270a5ea93546,11,3,1,25251,,,0,"Raise an exception if get_vfile() fails, rather than ignoring the error

Without the patch we see ugly exceptions on machines with failing disks, or with
corrupted metadata

Change-Id: Ie0ec8dbb7e0d875d03ff9ae2eba3e95a21ae5db3
",git fetch https://review.opendev.org/openstack/swift refs/changes/11/713911/1 && git format-patch -1 --stdout FETCH_HEAD,['swift/obj/kvfile.py'],1,df7f895ff8467344967795529056270a5ea93546,dev/alecuyer/feature-branch-catchup," try: ctype_vfr = vfile.VFileReader.get_vfile(ctype_file, self._logger) except IOError as e: if e.errno == errno.ENOENT: raise DiskFileNotExist() raise except IOError as e: if e.errno == errno.ENOENT: raise DiskFileNotExist() raise"," ctype_vfr = vfile.VFileReader.get_vfile(ctype_file, self._logger) except IOError: pass",10,3
openstack%2Fopenstack-helm-infra~master~Ia230a9eafd97e35d131c365036e9d9bd8d7008ec,openstack/openstack-helm-infra,master,Ia230a9eafd97e35d131c365036e9d9bd8d7008ec,Test commit,ABANDONED,2020-03-06 16:48:21.000000000,2020-03-20 04:08:40.000000000,,"[{'_account_id': 22348}, {'_account_id': 31479}]","[{'number': 1, 'created': '2020-03-06 16:48:21.000000000', 'files': ['memcached/values.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/c3d1c52fea4c1c883a0cce0cc32b1b11dd0b10ac', 'message': 'Test commit\n\nChange-Id: Ia230a9eafd97e35d131c365036e9d9bd8d7008ec\n'}]",0,711655,c3d1c52fea4c1c883a0cce0cc32b1b11dd0b10ac,5,2,1,31479,,,0,"Test commit

Change-Id: Ia230a9eafd97e35d131c365036e9d9bd8d7008ec
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/55/711655/1 && git format-patch -1 --stdout FETCH_HEAD,['memcached/values.yaml'],1,c3d1c52fea4c1c883a0cce0cc32b1b11dd0b10ac,,,,1,0
openstack%2Fsenlin~master~Ic252cf7d5cc7277fa86bf12df8776dac27151f83,openstack/senlin,master,Ic252cf7d5cc7277fa86bf12df8776dac27151f83,Allow LB creation with VIP_NETWORK *or* VIP_SUBNET,MERGED,2020-03-16 17:25:32.000000000,2020-03-20 03:54:47.000000000,2020-03-20 03:53:22.000000000,"[{'_account_id': 10273}, {'_account_id': 22348}, {'_account_id': 22623}, {'_account_id': 22998}, {'_account_id': 27224}, {'_account_id': 28691}]","[{'number': 1, 'created': '2020-03-16 17:25:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/senlin/commit/2ff2f80cc81a0efddbf459fb80d3a1c5a9276d87', 'message': 'Allow LB creation with VIP_NETWORK *or* VIP_SUBNET\n\nOctavia allows LB creation with only a vip_network_id specified. This is\nnecessary for things like routed-network environments where subnets are\noften not exposed to users, or else not well understood by them, and\nOctavia should retain responsibility for choosing the subnet.\n\nChange-Id: Ic252cf7d5cc7277fa86bf12df8776dac27151f83\n'}, {'number': 2, 'created': '2020-03-16 19:46:22.000000000', 'files': ['senlin/tests/unit/policies/test_lb_policy.py', 'senlin/drivers/os/lbaas.py', 'senlin/policies/lb_policy.py', 'senlin/drivers/os/octavia_v2.py', 'doc/source/user/policy_types/load_balancing.rst', 'releasenotes/notes/octavia-network_id-and-subnet_id-changes-9ba43e19ae29ac7d.yaml', 'senlin/tests/unit/drivers/test_lbaas.py', 'setup.cfg', 'senlin/tests/drivers/os_test/octavia_v2.py'], 'web_link': 'https://opendev.org/openstack/senlin/commit/d3bd9ef3f36f6271eb48e57edec41f312863803e', 'message': 'Allow LB creation with VIP_NETWORK *or* VIP_SUBNET\n\nOctavia allows LB creation with only a vip_network_id specified. This is\nnecessary for things like routed-network environments where subnets are\noften not exposed to users, or else not well understood by them, and\nOctavia should retain responsibility for choosing the subnet.\n\nChange-Id: Ic252cf7d5cc7277fa86bf12df8776dac27151f83\n'}]",1,713296,d3bd9ef3f36f6271eb48e57edec41f312863803e,13,6,2,10273,,,0,"Allow LB creation with VIP_NETWORK *or* VIP_SUBNET

Octavia allows LB creation with only a vip_network_id specified. This is
necessary for things like routed-network environments where subnets are
often not exposed to users, or else not well understood by them, and
Octavia should retain responsibility for choosing the subnet.

Change-Id: Ic252cf7d5cc7277fa86bf12df8776dac27151f83
",git fetch https://review.opendev.org/openstack/senlin refs/changes/96/713296/1 && git format-patch -1 --stdout FETCH_HEAD,"['senlin/tests/unit/policies/test_lb_policy.py', 'senlin/policies/lb_policy.py', 'senlin/tests/drivers/os_test/octavia_v2.py']",3,2ff2f80cc81a0efddbf459fb80d3a1c5a9276d87,," ""availability_zone"": ""test_az"", ""vip_subnet_id"": ""08dce793-daef-411d-a896-d389cd45b1ea"", ""vip_network_id"": ""e2de51e5-f10a-40f3-8f5c-7bab784b1380"", def loadbalancer_create(self, vip_subnet_id=None, vip_network_id=None, vip_address=None, admin_state_up=True, name=None, description=None): self.fake_lb[""vip_network_id""] = vip_network_id"," ""vip_subnet_id"": ""08dce793-daef-411d-a896-d389cd45b1ea"" def loadbalancer_create(self, vip_subnet_id, vip_address=None, admin_state_up=True, name=None, description=None):",192,25
openstack%2Ftripleo-ci~master~I5d29a9a502eb41feaa570bb4fdab7c3d6efb4e9f,openstack/tripleo-ci,master,I5d29a9a502eb41feaa570bb4fdab7c3d6efb4e9f,Make centos-8 scenario007 voting,MERGED,2020-03-12 12:12:38.000000000,2020-03-20 03:25:26.000000000,2020-03-20 03:25:26.000000000,"[{'_account_id': 9592}, {'_account_id': 9976}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-12 12:12:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/98f7695538b31edc8f09d7e0327450c0e0971990', 'message': 'Make centos-8 scenario007 and scenario010 voting\n\nJobs seems to be stable enough to switch to voting\n\nChange-Id: I5d29a9a502eb41feaa570bb4fdab7c3d6efb4e9f\n'}, {'number': 2, 'created': '2020-03-12 12:20:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/c51b89e91e3d950b8740d4e9307a7a72bed06f0d', 'message': 'Make centos-8 scenario007 and scenario010 voting\n\nJobs seems to be stable enough to switch to voting\n\nChange-Id: I5d29a9a502eb41feaa570bb4fdab7c3d6efb4e9f\n'}, {'number': 3, 'created': '2020-03-12 12:50:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/6c2f56bb075be4aea4f97f5e7fbef0bd0e3bd169', 'message': 'Make centos-8 scenario007 voting\n\nJob seems to be stable enough to switch to voting\n\nChange-Id: I5d29a9a502eb41feaa570bb4fdab7c3d6efb4e9f\n'}, {'number': 4, 'created': '2020-03-19 18:14:38.000000000', 'files': ['zuul.d/standalone-jobs.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/03271f88cea164ea89a530a2e0fafcd874d0302e', 'message': 'Make centos-8 scenario007 voting\n\nJob seems to be stable enough to switch to voting\n\nChange-Id: I5d29a9a502eb41feaa570bb4fdab7c3d6efb4e9f\n'}]",0,712665,03271f88cea164ea89a530a2e0fafcd874d0302e,17,4,4,8367,,,0,"Make centos-8 scenario007 voting

Job seems to be stable enough to switch to voting

Change-Id: I5d29a9a502eb41feaa570bb4fdab7c3d6efb4e9f
",git fetch https://review.opendev.org/openstack/tripleo-ci refs/changes/65/712665/4 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/standalone-jobs.yaml'],1,98f7695538b31edc8f09d7e0327450c0e0971990,centos-8-jobs, voting: true voting: true, voting: false voting: false,2,2
openstack%2Fnova~master~If9e173c8df2a7ab47dc5fca90e1d333c0f34197b,openstack/nova,master,If9e173c8df2a7ab47dc5fca90e1d333c0f34197b,libvirt: Fix unit test error block info on non x86 architecture,MERGED,2020-03-19 15:36:41.000000000,2020-03-20 02:56:16.000000000,2020-03-19 22:10:56.000000000,"[{'_account_id': 9008}, {'_account_id': 9708}, {'_account_id': 10135}, {'_account_id': 15334}, {'_account_id': 15941}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-03-19 15:36:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/787b5e57f29a5783510fa03ad994eea74393f5a1', 'message': 'fix unit test error block info on non x86 architecture\n\nCloses-Bugs: #1868121\nChange-Id: If9e173c8df2a7ab47dc5fca90e1d333c0f34197b\nSigned-off-by: Kevin Zhao <kevin.zhao@linaro.org>\n'}, {'number': 2, 'created': '2020-03-19 16:24:09.000000000', 'files': ['nova/tests/unit/virt/libvirt/test_blockinfo.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/d10ed8e75b8861f13c4a3a3e823b7e679b08d0ef', 'message': ""libvirt: Fix unit test error block info on non x86 architecture\n\nThe Q35 machine type does not provide an IDE bus. As a result, when\ntrying to provide a default bus for 'cdrom' type devices, we check the\nmachine type and select either 'ide' or 'sata' depending on whether\nwe're using the Q35 machine type. However, this only applies for x86 as\nother architectures use different machine types.\n\nThe 'test_get_disk_bus_for_device_type_cdrom_with_q35_image_meta' test\nis intended to ensure that the retrieved bus type for a 'cdrom' type\ndevice is actually 'sata' when the machine type is 'q35'. However,\nthis test can still fail on non-x86 platforms. Correct this by using\nthe 'hw_architecture' image metadata property to ensure consistent\nbehavior on all platforms.\n\nChange-Id: If9e173c8df2a7ab47dc5fca90e1d333c0f34197b\nSigned-off-by: Kevin Zhao <kevin.zhao@linaro.org>\nCloses-Bug: #1868121\n""}]",0,713905,d10ed8e75b8861f13c4a3a3e823b7e679b08d0ef,18,9,2,22076,,,0,"libvirt: Fix unit test error block info on non x86 architecture

The Q35 machine type does not provide an IDE bus. As a result, when
trying to provide a default bus for 'cdrom' type devices, we check the
machine type and select either 'ide' or 'sata' depending on whether
we're using the Q35 machine type. However, this only applies for x86 as
other architectures use different machine types.

The 'test_get_disk_bus_for_device_type_cdrom_with_q35_image_meta' test
is intended to ensure that the retrieved bus type for a 'cdrom' type
device is actually 'sata' when the machine type is 'q35'. However,
this test can still fail on non-x86 platforms. Correct this by using
the 'hw_architecture' image metadata property to ensure consistent
behavior on all platforms.

Change-Id: If9e173c8df2a7ab47dc5fca90e1d333c0f34197b
Signed-off-by: Kevin Zhao <kevin.zhao@linaro.org>
Closes-Bug: #1868121
",git fetch https://review.opendev.org/openstack/nova refs/changes/05/713905/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/unit/virt/libvirt/test_blockinfo.py'],1,787b5e57f29a5783510fa03ad994eea74393f5a1,," image_meta = {'properties': { 'hw_machine_type': 'pc-q35-rhel8.0.0', 'hw_architecture': obj_fields.Architecture.X86_64}}", image_meta = {'properties': {'hw_machine_type': 'pc-q35-rhel8.0.0'}},3,1
openstack%2Fswift~master~Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3,openstack/swift,master,Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3,Add gate job for CORS func tests,ABANDONED,2020-03-11 23:37:15.000000000,2020-03-20 02:40:48.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2020-03-11 23:37:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/18a703337c531094164b3ec24ad855a36ce92d2c', 'message': 'WIP: Add gate job for CORS func tests\n\nChange-Id: Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3\n'}, {'number': 2, 'created': '2020-03-11 23:46:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/14d267f476e9a51c99ed357195aafd435d369d7e', 'message': 'WIP: Add gate job for CORS func tests\n\nChange-Id: Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3\n'}, {'number': 3, 'created': '2020-03-11 23:56:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ab6c53db8bdd18f821a734c768b5cefbf822dce1', 'message': 'WIP: Add gate job for CORS func tests\n\nChange-Id: Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3\n'}, {'number': 4, 'created': '2020-03-12 00:10:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0dbc185badfc49efdaf8ca72bbaa0a930c176d34', 'message': 'WIP: Add gate job for CORS func tests\n\nChange-Id: Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3\n'}, {'number': 5, 'created': '2020-03-12 00:23:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/827bca268810aec1a116564d75f0c4248c30ca7e', 'message': 'WIP: Add gate job for CORS func tests\n\nChange-Id: Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3\n'}, {'number': 6, 'created': '2020-03-12 00:39:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1378d669cd567d1f71fc566f0722bce324c98f3d', 'message': 'WIP: Add gate job for CORS func tests\n\nChange-Id: Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3\n'}, {'number': 7, 'created': '2020-03-12 00:54:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7902745ff9b06f02d77919c3de2c160d471d3dcb', 'message': 'WIP: Add gate job for CORS func tests\n\nChange-Id: Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3\n'}, {'number': 8, 'created': '2020-03-12 01:04:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/31d4c16a7f6c2c2d5bbf6d8cf7364928b4a35e4d', 'message': 'WIP: Add gate job for CORS func tests\n\nChange-Id: Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3\n'}, {'number': 9, 'created': '2020-03-12 01:36:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/be115944d7494eb95b25247485d8ddb46e7c3e9f', 'message': ""WIP: Add gate job for CORS func tests\n\nAreas for future work:\n\n- Install chromium and chromdriver in the gate; tests should\n  automatically pick up on the fact that it's available\n- Capture the web browser's console logs, too, so we can get\n  more info when things go wrong\n\nDepends-On: https://review.opendev.org/#/c/712238/\nChange-Id: Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3\n""}, {'number': 10, 'created': '2020-03-16 20:16:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8e16d51ced604efcac91325af703026a2a00c2e2', 'message': ""Add gate job for CORS func tests\n\nAreas for future work:\n\n- Install chromium and chromdriver in the gate; tests should\n  automatically pick up on the fact that it's available\n- Capture the web browser's console logs, too, so we can get\n  more info when things go wrong\n\nChange-Id: Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3\n""}, {'number': 11, 'created': '2020-03-16 20:54:05.000000000', 'files': ['tools/playbooks/cors/post.yaml', '.zuul.yaml', 'tools/playbooks/cors/run.yaml', 'tools/playbooks/cors/install_selenium.yaml'], 'web_link': 'https://opendev.org/openstack/swift/commit/f3c73c93b2456ca445188eb15490958a9ad113a1', 'message': ""Add gate job for CORS func tests\n\nAreas for future work:\n\n- Install chromium and chromdriver in the gate; tests should\n  automatically pick up on the fact that it's available\n- Capture the web browser's console logs, too, so we can get\n  more info when things go wrong\n\nChange-Id: Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3\n""}]",0,712585,f3c73c93b2456ca445188eb15490958a9ad113a1,18,1,11,15343,,,0,"Add gate job for CORS func tests

Areas for future work:

- Install chromium and chromdriver in the gate; tests should
  automatically pick up on the fact that it's available
- Capture the web browser's console logs, too, so we can get
  more info when things go wrong

Change-Id: Id00f184c0d85cf7f645c5c8ac9a24cbdd39571c3
",git fetch https://review.opendev.org/openstack/swift refs/changes/85/712585/11 && git format-patch -1 --stdout FETCH_HEAD,"['tools/playbooks/cors/post.yaml', '.zuul.yaml', 'tools/playbooks/cors/run.yaml', 'tools/playbooks/cors/install_selenium.yaml']",4,18a703337c531094164b3ec24ad855a36ce92d2c,712238,- hosts: all tasks: - name: install virtual frame buffer yum: name: xorg-x11-server-Xvfb state: present - name: install selenium pip: name: selenium state: present - name: install firefox yum: name: firefox state: present - name: install chromium yum: name: chromium-headless state: present ,,56,147
openstack%2Fswift~master~Ia20ae0b7a9c53fb3f6cc196395e0f0ec64a4d3ce,openstack/swift,master,Ia20ae0b7a9c53fb3f6cc196395e0f0ec64a4d3ce,Move cors out of functests,ABANDONED,2020-03-18 23:04:06.000000000,2020-03-20 02:40:12.000000000,,"[{'_account_id': 1179}, {'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-18 23:04:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/2923853bb02cd4271999dd8174900e50ba29f307', 'message': ""Move cors out of functests\n\nI also bind my run server to 0.0.0.0 so I can hit it from my host and\nuse 'saio' instead of 'localhost' in the output.\n\nChange-Id: Ia20ae0b7a9c53fb3f6cc196395e0f0ec64a4d3ce\n""}, {'number': 2, 'created': '2020-03-19 15:18:29.000000000', 'files': ['test/cors/test-container.js', 'test/cors/test-account.js', 'test/cors/index.html', 'test/cors/test-large-objects.js', 'test/cors/harness.js', 'test/cors/test-object.js', 'test/cors/main.py', 'test/cors/test-info.js', 'test/cors/test-symlink.js'], 'web_link': 'https://opendev.org/openstack/swift/commit/2cb5f31c71a3f67efa0ba4f49fc5120fba532f8f', 'message': ""Move cors out of functests\n\nI also bind my run server to 0.0.0.0 so I can hit it from my host and\nuse a new --hostname option for 'saio' instead of 'localhost'\n\nChange-Id: Ia20ae0b7a9c53fb3f6cc196395e0f0ec64a4d3ce\n""}]",6,713754,2cb5f31c71a3f67efa0ba4f49fc5120fba532f8f,8,3,2,1179,,,0,"Move cors out of functests

I also bind my run server to 0.0.0.0 so I can hit it from my host and
use a new --hostname option for 'saio' instead of 'localhost'

Change-Id: Ia20ae0b7a9c53fb3f6cc196395e0f0ec64a4d3ce
",git fetch https://review.opendev.org/openstack/swift refs/changes/54/713754/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/cors/test-account.js', 'test/cors/test-container.js', 'test/cors/index.html', 'test/cors/test-large-objects.js', 'test/cors/harness.js', 'test/cors/test-object.js', 'test/cors/main.py', 'test/cors/test-info.js', 'test/cors/test-symlink.js']",9,2923853bb02cd4271999dd8174900e50ba29f307,,,,5,2
openstack%2Fqa-specs~master~Ic63f35b238d4410a438c99d228a932b3fa3ec98c,openstack/qa-specs,master,Ic63f35b238d4410a438c99d228a932b3fa3ec98c,Whitebox Tempest Plugin,MERGED,2019-05-02 22:35:02.000000000,2020-03-20 01:58:13.000000000,2020-03-20 01:56:57.000000000,"[{'_account_id': 5689}, {'_account_id': 8556}, {'_account_id': 8864}, {'_account_id': 22348}, {'_account_id': 22873}, {'_account_id': 30742}]","[{'number': 1, 'created': '2019-05-02 22:35:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/qa-specs/commit/86a2b1a6eb27fdbab8c6c2aa890cf2dc4e9cce5a', 'message': '[Very WIP] Whitebox Templest Plugin\n\nChange-Id: Ic63f35b238d4410a438c99d228a932b3fa3ec98c\n'}, {'number': 2, 'created': '2019-09-17 17:34:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/qa-specs/commit/1dd109ca0fa7ecbb8cb3e54bf02ffebd5241a091', 'message': 'Whitebox Templest Plugin\n\nTempest defines its scope as only what is accessible through the\nvarious REST APIs. Some cloud features cannot be properly tested when\nusing only the REST API. The whitebox-tempest-plugin is a Tempest\nplugin whose scope explicitly requires peeking behind the curtain. In\nother words, if a feature or behavior can be fully tested using only a\nREST API, such a test does not belong in whitebox-tempest-plugin. On\nthe other hand, if fully testing a feature or behavior requires\naccessing the control plane like a human operator or admin would, such\na test belongs in whitebox-tempest plugin.\n\nChange-Id: Ic63f35b238d4410a438c99d228a932b3fa3ec98c\n'}, {'number': 3, 'created': '2020-03-03 18:23:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/qa-specs/commit/2ac200e678b2015b15ef0d1b714cbe69291da27b', 'message': 'Whitebox Templest Plugin\n\nTempest defines its scope as only what is accessible through the\nvarious REST APIs. Some cloud features cannot be properly tested when\nusing only the REST API. The whitebox-tempest-plugin is a Tempest\nplugin whose scope explicitly requires peeking behind the curtain. In\nother words, if a feature or behavior can be fully tested using only a\nREST API, such a test does not belong in whitebox-tempest-plugin. On\nthe other hand, if fully testing a feature or behavior requires\naccessing the control plane like a human operator or admin would, such\na test belongs in whitebox-tempest plugin.\n\nChange-Id: Ic63f35b238d4410a438c99d228a932b3fa3ec98c\n'}, {'number': 4, 'created': '2020-03-03 19:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/qa-specs/commit/4621dbf49f84286eef50353cdeae6dfe41ba2d15', 'message': 'Whitebox Templest Plugin\n\nTempest defines its scope as only what is accessible through the\nvarious REST APIs. Some cloud features cannot be properly tested when\nusing only the REST API. The whitebox-tempest-plugin is a Tempest\nplugin whose scope explicitly requires peeking behind the curtain. In\nother words, if a feature or behavior can be fully tested using only a\nREST API, such a test does not belong in whitebox-tempest-plugin. On\nthe other hand, if fully testing a feature or behavior requires\naccessing the control plane like a human operator or admin would, such\na test belongs in whitebox-tempest plugin.\n\nChange-Id: Ic63f35b238d4410a438c99d228a932b3fa3ec98c\n'}, {'number': 5, 'created': '2020-03-04 13:33:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/qa-specs/commit/69a6bce70f61563fcf315fabd1db52965558c5e8', 'message': 'Whitebox Tempest Plugin\n\nTempest defines its scope as only what is accessible through the\nvarious REST APIs. Some cloud features cannot be properly tested when\nusing only the REST API. The whitebox-tempest-plugin is a Tempest\nplugin whose scope explicitly requires peeking behind the curtain. In\nother words, if a feature or behavior can be fully tested using only a\nREST API, such a test does not belong in whitebox-tempest-plugin. On\nthe other hand, if fully testing a feature or behavior requires\naccessing the control plane like a human operator or admin would, such\na test belongs in whitebox-tempest plugin.\n\nChange-Id: Ic63f35b238d4410a438c99d228a932b3fa3ec98c\n'}, {'number': 6, 'created': '2020-03-05 01:38:29.000000000', 'files': ['specs/other/whitebox-tempest-plugin.rst'], 'web_link': 'https://opendev.org/openstack/qa-specs/commit/6c500e1933b35edfe6092f3339ec3ed364bfc7dd', 'message': 'Whitebox Tempest Plugin\n\nTempest defines its scope as only what is accessible through the\nvarious REST APIs. Some cloud features cannot be properly tested when\nusing only the REST API. The whitebox-tempest-plugin is a Tempest\nplugin whose scope explicitly requires peeking behind the curtain. In\nother words, if a feature or behavior can be fully tested using only a\nREST API, such a test does not belong in whitebox-tempest-plugin. On\nthe other hand, if fully testing a feature or behavior requires\naccessing the control plane like a human operator or admin would, such\na test belongs in whitebox-tempest plugin.\n\nChange-Id: Ic63f35b238d4410a438c99d228a932b3fa3ec98c\n'}]",30,656890,6c500e1933b35edfe6092f3339ec3ed364bfc7dd,30,6,6,8864,,,0,"Whitebox Tempest Plugin

Tempest defines its scope as only what is accessible through the
various REST APIs. Some cloud features cannot be properly tested when
using only the REST API. The whitebox-tempest-plugin is a Tempest
plugin whose scope explicitly requires peeking behind the curtain. In
other words, if a feature or behavior can be fully tested using only a
REST API, such a test does not belong in whitebox-tempest-plugin. On
the other hand, if fully testing a feature or behavior requires
accessing the control plane like a human operator or admin would, such
a test belongs in whitebox-tempest plugin.

Change-Id: Ic63f35b238d4410a438c99d228a932b3fa3ec98c
",git fetch https://review.opendev.org/openstack/qa-specs refs/changes/90/656890/6 && git format-patch -1 --stdout FETCH_HEAD,['specs/other/whitebox-tempest-plugin.rst'],1,86a2b1a6eb27fdbab8c6c2aa890cf2dc4e9cce5a,,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode .. ======================= Whitebox Tempest Plugin ======================= https://blueprints.launchpad.net/tempest/+spec/whitebox-tempest-plugin Problem description =================== Tempest defines its scope as only what is accessible through the various REST APIs. Some cloud features cannot be properly tested when using only the REST API. For example, to assert correctness of a live migration of an instance with dedicated CPUs, it is necessary to examine the instance's libvirt XML. Proposed change =============== Here is where you cover the change you propose to make in detail. How do you propose to solve this problem? If this is one part of a larger effort make it clear where this piece ends. In other words, what's the scope of this effort? Include where in the tempest tree hierarchy this will reside. If the change is designed to test other OpenStack projects then list which ones it is targeted at. Alternatives ------------ This is an optional section which applies if the blueprint covers infrastructure changes to tempest. For example, blueprints which are just adding more API tests can leave this section empty. Where it does apply we'd just like a demonstration that some thought has been put into why the proposed approach is the best one. Projects ======== List the qa projects that this spec effects. For example: * openstack/tempest * openstack/tempest-lib * openstack-dev/devstack Implementation ============== Assignee(s) ----------- Who is leading the writing of the code? Or is this a blueprint where you're throwing it out there to see who picks it up? If more than one person is working on the implementation, please designate the primary author and contact. Primary assignee: <launchpad-id or None> Can optionally can list additional ids if they intend on doing substantial implementation work on this blueprint. Milestones ---------- Target Milestone for completion: Juno-1 Work Items ---------- Work items or tasks -- break the feature up into the things that need to be done to implement it. Those parts might end up being done by different people, but we're mostly trying to understand the timeline for implementation. Dependencies ============ - Include specific references to specs and/or blueprints in tempest, or in other projects, that this one either depends on or is related to. - Does this feature require any new library dependencies or code otherwise not included in OpenStack? Or does it depend on a specific version of library? ",,95,0
openstack%2Ftripleo-common~stable%2Ftrain~I54097b2bbf19cd07bceb6ce608aa7f80435ad894,openstack/tripleo-common,stable/train,I54097b2bbf19cd07bceb6ce608aa7f80435ad894,Use unversioned RHEL element w/ RHEL8,MERGED,2020-03-17 20:13:07.000000000,2020-03-20 01:22:49.000000000,2020-03-20 01:21:15.000000000,"[{'_account_id': 3153}, {'_account_id': 8161}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-17 20:13:07.000000000', 'files': ['image-yaml/overcloud-images-ceph-rhel8.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/e76a1c19ed542afbdd55b72a69dc1af65109b909', 'message': ""Use unversioned RHEL element w/ RHEL8\n\nUsing 'rhel8' instead of 'rhel' causes errors.\n\nRelated to commits:\n  18788b2b1748ffaa088a23caeec165391d9d7b4e\n  5877ef54d3c3a2392297123a70cbf4a03f93e977\n\nChange-Id: I54097b2bbf19cd07bceb6ce608aa7f80435ad894\nSigned-off-by: Lon Hohberger <lhh@redhat.com>\n(cherry picked from commit eda7ffe39b9caccc637597624a3c255c5d7443b6)\n""}]",0,713527,e76a1c19ed542afbdd55b72a69dc1af65109b909,8,4,1,14985,,,0,"Use unversioned RHEL element w/ RHEL8

Using 'rhel8' instead of 'rhel' causes errors.

Related to commits:
  18788b2b1748ffaa088a23caeec165391d9d7b4e
  5877ef54d3c3a2392297123a70cbf4a03f93e977

Change-Id: I54097b2bbf19cd07bceb6ce608aa7f80435ad894
Signed-off-by: Lon Hohberger <lhh@redhat.com>
(cherry picked from commit eda7ffe39b9caccc637597624a3c255c5d7443b6)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/27/713527/1 && git format-patch -1 --stdout FETCH_HEAD,['image-yaml/overcloud-images-ceph-rhel8.yaml'],1,e76a1c19ed542afbdd55b72a69dc1af65109b909,c8imagebuild-stable/train, distro: rhel, distro: rhel8,1,1
openstack%2Ftripleo-common~master~I5b1fa484010f88212ddb312f963a8f530e5a78d9,openstack/tripleo-common,master,I5b1fa484010f88212ddb312f963a8f530e5a78d9,Move DeployStackAction functionality to utils,MERGED,2020-03-18 06:59:05.000000000,2020-03-20 01:22:32.000000000,2020-03-20 01:21:15.000000000,"[{'_account_id': 3153}, {'_account_id': 7353}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25877}]","[{'number': 1, 'created': '2020-03-18 06:59:05.000000000', 'files': ['tripleo_common/tests/actions/test_deployment.py', 'tripleo_common/tests/utils/test_stack.py', 'tripleo_common/actions/deployment.py', 'tripleo_common/utils/stack.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/f6d55d57eb3deec7b06dddd13b7535bd7cf5e726', 'message': 'Move DeployStackAction functionality to utils\n\nChange-Id: I5b1fa484010f88212ddb312f963a8f530e5a78d9\n'}]",0,713580,f6d55d57eb3deec7b06dddd13b7535bd7cf5e726,10,5,1,8833,,,0,"Move DeployStackAction functionality to utils

Change-Id: I5b1fa484010f88212ddb312f963a8f530e5a78d9
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/80/713580/1 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_common/tests/actions/test_deployment.py', 'tripleo_common/tests/utils/test_stack.py', 'tripleo_common/actions/deployment.py', 'tripleo_common/utils/stack.py']",4,f6d55d57eb3deec7b06dddd13b7535bd7cf5e726,mistral_to_ansible,"from tripleo_common import update def deploy_stack(swift, heat, container, skip_deploy_identifier=False, timeout_mins=240): try: stack = heat.stacks.get(container, resolve_outputs=False) except heat_exc.HTTPNotFound: stack = None stack_is_new = stack is None # update StackAction, DeployIdentifier and UpdateIdentifier parameters = dict() if not skip_deploy_identifier: parameters['DeployIdentifier'] = int(time.time()) else: parameters['DeployIdentifier'] = '' parameters['StackAction'] = 'CREATE' if stack_is_new else 'UPDATE' try: env = plan_utils.get_env(swift, container) except swiftexceptions.ClientException as err: err_msg = (""Error retrieving environment for plan %s: %s"" % ( container, err)) LOG.exception(err_msg) raise RuntimeError(err_msg) set_tls_parameters(parameters, env) try: plan_utils.update_in_env(swift, env, 'parameter_defaults', parameters) except swiftexceptions.ClientException as err: err_msg = (""Error updating environment for plan %s: %s"" % ( container, err)) LOG.exception(err_msg) raise RuntimeError(err_msg) if not stack_is_new: try: LOG.debug('Checking for compatible neutron mechanism drivers') msg = update.check_neutron_mechanism_drivers(env, stack, swift, container) if msg: raise RuntimeError(msg) except swiftexceptions.ClientException as err: err_msg = (""Error getting template %s: %s"" % ( container, err)) LOG.exception(err_msg) raise RuntimeError(err_msg) # process all plan files and create or update a stack processed_data = templates.process_templates( swift, heat, container=container, prune_services=True ) stack_args = processed_data.copy() stack_args['timeout_mins'] = timeout_mins if stack_is_new: try: swift.copy_object( ""%s-swift-rings"" % container, ""swift-rings.tar.gz"", ""%s-swift-rings/%s-%d"" % ( container, ""swift-rings.tar.gz"", time.time())) swift.delete_object( ""%s-swift-rings"" % container, ""swift-rings.tar.gz"") except swiftexceptions.ClientException: pass LOG.info(""Perfoming Heat stack create"") try: return heat.stacks.create(**stack_args) except heat_exc.HTTPException as err: err_msg = ""Error during stack creation: %s"" % (err,) LOG.exception(err_msg) raise RuntimeError(err_msg) LOG.info(""Performing Heat stack update"") stack_args['existing'] = 'true' try: return heat.stacks.update(stack.id, **stack_args) except heat_exc.HTTPException as err: err_msg = ""Error during stack update: %s"" % (err,) LOG.exception(err_msg) raise RuntimeError(err_msg) def set_tls_parameters(parameters, env, local_ca_path=constants.LOCAL_CACERT_PATH): def get_camap(): return env['parameter_defaults'].get('CAMap', {}) def get_updated_camap_entry(entry_name, cacert, orig_camap): ca_map_entry = { entry_name: { 'content': cacert } } orig_camap.update(ca_map_entry) return orig_camap cacert_string = get_local_cacert(local_ca_path) if cacert_string: parameters['CAMap'] = get_updated_camap_entry( 'undercloud-ca', cacert_string, get_camap()) def get_local_cacert(local_ca_path): # Since the undercloud has TLS by default, we'll add the undercloud's # CA to be trusted by the overcloud. try: with open(local_ca_path, 'rb') as ca_file: return ca_file.read().decode('utf-8') except IOError: # If the file wasn't found it means that the undercloud's TLS # was explicitly disabled or another CA is being used. So we'll # let the user handle this. return None except Exception: raise",,407,427
openstack%2Fpython-openstackclient~master~Ifcb5b4fe2486087a5ca1ff9609f7bf09ef026974,openstack/python-openstackclient,master,Ifcb5b4fe2486087a5ca1ff9609f7bf09ef026974,Update image building jobs,MERGED,2020-03-19 21:18:10.000000000,2020-03-20 01:17:55.000000000,2020-03-20 01:15:26.000000000,"[{'_account_id': 2}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 21:18:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/9baa8f6f829a025db77719f06d4cb00605cac198', 'message': ""Update image building jobs\n\nWe're failing on promote but not upload. That's weird. Make sure\nthe secret is appropriately encoded, and copy what zuul is doing.\nAlso make promote a zero-node job.\n\nChange-Id: Ifcb5b4fe2486087a5ca1ff9609f7bf09ef026974\n""}, {'number': 2, 'created': '2020-03-19 21:18:29.000000000', 'files': ['.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/332457bc875401ba990a50c2d7ed14f49ea4e462', 'message': ""Update image building jobs\n\nWe're failing on promote but not upload. That's weird. Make sure\nthe secret is appropriately encoded, and copy what zuul is doing.\nAlso make promote a zero-node job.\n\nChange-Id: Ifcb5b4fe2486087a5ca1ff9609f7bf09ef026974\n""}]",0,713983,332457bc875401ba990a50c2d7ed14f49ea4e462,8,2,2,2,,,0,"Update image building jobs

We're failing on promote but not upload. That's weird. Make sure
the secret is appropriately encoded, and copy what zuul is doing.
Also make promote a zero-node job.

Change-Id: Ifcb5b4fe2486087a5ca1ff9609f7bf09ef026974
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/83/713983/1 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.yaml'],1,9baa8f6f829a025db77719f06d4cb00605cac198,command_exception, - n764ECFMGlEna6S5ezNyvW5nmq8IZCBts/7QRzdo2tWLQMp/mNFoaQensd797Ra3NLaS7 NzCFJwGQvgWF6hJJIUfnf3h2+RecCwHahLN4r95RtjhAltARzHSZVDCeNXhiRJqSDS4Qc kmXR2NTNfz8kkWUhnWNVjaBhYdMk0LnqZjQCxiCaR+eNdmeecWlSuJXg0Uz6vObLNfGHO KxV3RiGc4J0AATYpYFEpC/SyPbBk0pJv6JWJb7nNIe0CEVW/7hkCfA6o3hQ5PNAswn5ZP sp/L8NdoRQe/fEWOm/9K2lZqQehEj6SKsk6jkx3Wiy5stqFcGfafrxWcfQoQWpKHY5TeP R9U0jJy+ipnhfnm0flBIBt9XHYykrTuFwp5QVdRhRRQDwg5RZBX+VmaBeSQlS2Z0oJmCX PXFQmUDfnoU5go0BALlXDdy1sYE+SrQH4Eydw+hgf2oDFh+EkdhXMFburxnU8B7t4ey14 EM1W4BdOBUgeI4fa/92BP6ipgUFvcJu19FYTdg4v7NZ/ApnwZnZ5KC4eYlDaKNQiPQUmW pFJrnxxYXeDgmiXij8mCCgo8KEGvPCKHAghZ14iBCaWqvniLXuOSkFI1gYU+llg4i2jAp ts3GfQqBe8jfROGPMexVuonqZxZBxvWmIgDsAaqAeJCtykS1xeIiAMtA8rNl40= secrets: vars: *osc_image_vars secrets: - name: docker_credentials secret: osc-dockerhub pass-to-parent: true nodeset: nodes: [], - qQ0O7bXUWBhkygjSKcPHogWvR2ax67EgHZcYd27zgg6KvpdK9GsNTRTIeD5yeBb9Dzr/K RcAf+0pQT3fRIsKyEx2odHNevGpePjmUZENd5vHTIvTuZWq+X5ehpXgkEYvw3jwYJg78F ids1igEaHsE86OMHjWauyc1QUzYfwkf+ziK7TIOZ6RpVRHgq5Bf9S+Hz/QnVdxOLaIlO0 VC/bchKX/36vOQKd20KkNhBQAnUlDBQWMnZocvZKZYtkDs2w2vqlnUPRlzEppBWm5Yae6 5acyIHEEAIbECd/wC/OT8YndoeOUiqOZY0uSWtv4JgEKl6AexP+54VxPrsz7LayRMDJ4B jVCZK6y1sss9mF6mNXvZipPEVgklGcGM76GfGdqTeuQ3i8CqaKmCTBo1IKlEmcslXR/5T vjibWzvNHPpFcpYEEM6GLGg2K6nja1MCE1s/L76pN3FtxCZHdl8rZXU+mJH37uQk9zvdR Y6qtWJ+3o5sbgYfjgdp/nPs1xXMUvuG83qykuzYgtOYvlEw51eqwd2SPXd3op/KApAhKR Zlu8fBUkm/FyXToOpCl0s/eR4w1d+Spv0A+UhrS5pmV18+NlpNs0Krj5wS9KWMUIec0ae opgPkQrFfj/zD45rrIUJRzT+alZlZeK+WQfeNOXt2i6MLtOPesHMukTc6ksXtA= vars: *osc_image_vars secrets: &osc_image_secrets secrets: *osc_image_secrets,18,13
openstack%2Ftripleo-common~stable%2Ftrain~Ice40fec0eacefd9778614996fb3417b78cdd17d3,openstack/tripleo-common,stable/train,Ice40fec0eacefd9778614996fb3417b78cdd17d3,Add interface-names to centos8 images,MERGED,2020-03-13 13:38:31.000000000,2020-03-20 00:59:48.000000000,2020-03-20 00:58:32.000000000,"[{'_account_id': 3153}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-13 13:38:31.000000000', 'files': ['image-yaml/overcloud-images-ceph-centos8.yaml', 'image-yaml/overcloud-images-centos8.yaml', 'image-yaml/overcloud-hardened-images-centos8.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/e64cccf5e6cb6bd923efbc6b82eca95cf0b9a119', 'message': 'Add interface-names to centos8 images\n\nBack in Bug #1841441, we disabled the net.ifnames because of the\nRHEL7->RHEL8 changes to interface names. Now that we have centos8, we\nneed to ensure this action is also run on those images.\n\nDepends-On: https://review.opendev.org/#/c/712947/\nChange-Id: Ice40fec0eacefd9778614996fb3417b78cdd17d3\nRelated-Bug: #1866202\nRelated-Bug: #1841441\n(cherry picked from commit 2589b8f22ad2d263bcecbb8278e73f0822e7eba3)\n'}]",0,712949,e64cccf5e6cb6bd923efbc6b82eca95cf0b9a119,9,3,1,14985,,,0,"Add interface-names to centos8 images

Back in Bug #1841441, we disabled the net.ifnames because of the
RHEL7->RHEL8 changes to interface names. Now that we have centos8, we
need to ensure this action is also run on those images.

Depends-On: https://review.opendev.org/#/c/712947/
Change-Id: Ice40fec0eacefd9778614996fb3417b78cdd17d3
Related-Bug: #1866202
Related-Bug: #1841441
(cherry picked from commit 2589b8f22ad2d263bcecbb8278e73f0822e7eba3)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/49/712949/1 && git format-patch -1 --stdout FETCH_HEAD,"['image-yaml/overcloud-images-ceph-centos8.yaml', 'image-yaml/overcloud-images-centos8.yaml', 'image-yaml/overcloud-hardened-images-centos8.yaml']",3,e64cccf5e6cb6bd923efbc6b82eca95cf0b9a119,c8imagebuild-stable/train, - interface-names,,4,0
openstack%2Fneutron~master~Iffef8ceb9ca458820a7b3e604a53281f7e93b810,openstack/neutron,master,Iffef8ceb9ca458820a7b3e604a53281f7e93b810,Mark TestVirtualPorts tests as unstable,MERGED,2020-03-19 12:16:10.000000000,2020-03-20 00:58:22.000000000,2020-03-20 00:56:29.000000000,"[{'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 22348}, {'_account_id': 24791}, {'_account_id': 26622}]","[{'number': 1, 'created': '2020-03-19 12:16:10.000000000', 'files': ['neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/ceb27f2c7d9d91526e9062bfaad71d2a270318bf', 'message': ""Mark TestVirtualPorts tests as unstable\n\nBased on debug performed in [1] the TestVirtualPorts\nare unstable. At lease one of tests per 20\nretries of functional job fail in various\nplaces.\nWe don't think it is worth to spend more\ntime on debugging those tests, because there is\na possible easier solution: set virtual port\ntype based on DEVICE_TYPE, and do not rely\non allowed address pairs.\n\n[1] https://review.opendev.org/#/c/712888/\n\nRelated-Bug: #1865453\n\nChange-Id: Iffef8ceb9ca458820a7b3e604a53281f7e93b810\n""}]",0,713860,ceb27f2c7d9d91526e9062bfaad71d2a270318bf,15,6,1,24791,,,0,"Mark TestVirtualPorts tests as unstable

Based on debug performed in [1] the TestVirtualPorts
are unstable. At lease one of tests per 20
retries of functional job fail in various
places.
We don't think it is worth to spend more
time on debugging those tests, because there is
a possible easier solution: set virtual port
type based on DEVICE_TYPE, and do not rely
on allowed address pairs.

[1] https://review.opendev.org/#/c/712888/

Related-Bug: #1865453

Change-Id: Iffef8ceb9ca458820a7b3e604a53281f7e93b810
",git fetch https://review.opendev.org/openstack/neutron refs/changes/60/713860/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py'],1,ceb27f2c7d9d91526e9062bfaad71d2a270318bf,bug/1865453,"from neutron.tests import base as tests_base @tests_base.unstable_test(""bug 1865453"") @tests_base.unstable_test(""bug 1865453"") @tests_base.unstable_test(""bug 1865453"") @tests_base.unstable_test(""bug 1865453"")",,5,0
openstack%2Frequirements~master~Ice8895202f2f0a80ff33e5185010b4ad5147c743,openstack/requirements,master,Ice8895202f2f0a80ff33e5185010b4ad5147c743,update constraint for tripleo-common to new release 12.2.0,MERGED,2020-03-19 22:01:51.000000000,2020-03-20 00:58:19.000000000,2020-03-20 00:56:32.000000000,"[{'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 22:01:51.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/f621337f13dd285974a6ece56462d5702a4f0c9d', 'message': 'update constraint for tripleo-common to new release 12.2.0\n\nChange-Id: Ice8895202f2f0a80ff33e5185010b4ad5147c743\nmeta:version: 12.2.0\nmeta:diff-start: -\nmeta:series: ussuri\nmeta:release-type: release\nmeta:pypi: no\nmeta:first: no\nmeta:release:Author: Rabi Mishra <ramishra@redhat.com>\nmeta:release:Commit: Rabi Mishra <ramishra@redhat.com>\nmeta:release:Change-Id: I947922e88aab0b050fd5ef3368caf239fe9b7808\nmeta:release:Code-Review+1: Alex Schultz <aschultz@redhat.com>\nmeta:release:Code-Review+1: Emilien Macchi <emilien@redhat.com>\nmeta:release:Code-Review+2: Sean McGinnis <sean.mcginnis@gmail.com>\nmeta:release:Workflow+1: Sean McGinnis <sean.mcginnis@gmail.com>\n'}]",0,713997,f621337f13dd285974a6ece56462d5702a4f0c9d,8,3,1,11131,,,0,"update constraint for tripleo-common to new release 12.2.0

Change-Id: Ice8895202f2f0a80ff33e5185010b4ad5147c743
meta:version: 12.2.0
meta:diff-start: -
meta:series: ussuri
meta:release-type: release
meta:pypi: no
meta:first: no
meta:release:Author: Rabi Mishra <ramishra@redhat.com>
meta:release:Commit: Rabi Mishra <ramishra@redhat.com>
meta:release:Change-Id: I947922e88aab0b050fd5ef3368caf239fe9b7808
meta:release:Code-Review+1: Alex Schultz <aschultz@redhat.com>
meta:release:Code-Review+1: Emilien Macchi <emilien@redhat.com>
meta:release:Code-Review+2: Sean McGinnis <sean.mcginnis@gmail.com>
meta:release:Workflow+1: Sean McGinnis <sean.mcginnis@gmail.com>
",git fetch https://review.opendev.org/openstack/requirements refs/changes/97/713997/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,f621337f13dd285974a6ece56462d5702a4f0c9d,new-release,tripleo-common===12.2.0,tripleo-common===12.1.0,1,1
openstack%2Fopenstack-ansible-repo_build~master~I4ae6737f1ded4cbb680a380c8c309a0f5bf64723,openstack/openstack-ansible-repo_build,master,I4ae6737f1ded4cbb680a380c8c309a0f5bf64723,repo_build: take distro version into account,MERGED,2020-03-16 16:26:21.000000000,2020-03-20 00:49:07.000000000,2020-03-19 12:16:13.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 25600}, {'_account_id': 28619}]","[{'number': 1, 'created': '2020-03-16 16:26:21.000000000', 'files': ['defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-repo_build/commit/d5e33c8afa6401392f9286f9cf3ffe7bc1a5bc8e', 'message': 'repo_build: take distro version into account\n\nWhen introducing a new repo_container with a different distro, using\nthe same directory for symlinks can result in weird builds. We should\nprevent the links directory to be copied all willy-nilly, as suggested\nhere;\n\nhttps://review.opendev.org/#/c/712784/\nhttps://review.opendev.org/#/c/712787/\n\nBut we can further prevent bad builds by separating under distro-arch\nunder links with this patch.\n\nDepends-On: Ieca6ec9bf898432c76c019a1d2d97c280cd85dce\nChange-Id: I4ae6737f1ded4cbb680a380c8c309a0f5bf64723\n'}]",0,713284,d5e33c8afa6401392f9286f9cf3ffe7bc1a5bc8e,10,4,1,25600,,,0,"repo_build: take distro version into account

When introducing a new repo_container with a different distro, using
the same directory for symlinks can result in weird builds. We should
prevent the links directory to be copied all willy-nilly, as suggested
here;

https://review.opendev.org/#/c/712784/
https://review.opendev.org/#/c/712787/

But we can further prevent bad builds by separating under distro-arch
under links with this patch.

Depends-On: Ieca6ec9bf898432c76c019a1d2d97c280cd85dce
Change-Id: I4ae6737f1ded4cbb680a380c8c309a0f5bf64723
",git fetch https://review.opendev.org/openstack/openstack-ansible-repo_build refs/changes/84/713284/1 && git format-patch -1 --stdout FETCH_HEAD,['defaults/main.yml'],1,d5e33c8afa6401392f9286f9cf3ffe7bc1a5bc8e,,"repo_build_global_links_path: ""{{ repo_build_base_path }}/links/{{ repo_build_os_distro_version }}""","repo_build_global_links_path: ""{{ repo_build_base_path }}/links""",1,1
openstack%2Ftripleo-common~stable%2Fstein~I6de47600f430d3af5e3052ff456cd718a0df0d76,openstack/tripleo-common,stable/stein,I6de47600f430d3af5e3052ff456cd718a0df0d76,Re-raise same exception type in export_stream,MERGED,2020-03-19 13:11:02.000000000,2020-03-20 00:48:50.000000000,2020-03-20 00:46:26.000000000,"[{'_account_id': 3153}, {'_account_id': 7144}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 13:11:02.000000000', 'files': ['tripleo_common/image/image_export.py', 'tripleo_common/tests/image/test_image_export.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/68867107c892c6aedef1eae099680fc331ec2513', 'message': 'Re-raise same exception type in export_stream\n\ntripleo-container-image-prepare can consume a lot of memory during\nthe export. Previously, when a MemoryError was raised by python, the\nexception was hidden because there was no message in the exception and\nthe code re-raised an IOError.\n\nThis patch updates the code so that a MemoryError is specifically caught\nif one was thrown, and an accurate message is logged.\n\nChange-Id: I6de47600f430d3af5e3052ff456cd718a0df0d76\n(cherry picked from commit 9d7a9b83ed8d20c8baa9a6b49812f4f30faf7782)\n'}]",0,713872,68867107c892c6aedef1eae099680fc331ec2513,8,4,1,14985,,,0,"Re-raise same exception type in export_stream

tripleo-container-image-prepare can consume a lot of memory during
the export. Previously, when a MemoryError was raised by python, the
exception was hidden because there was no message in the exception and
the code re-raised an IOError.

This patch updates the code so that a MemoryError is specifically caught
if one was thrown, and an accurate message is logged.

Change-Id: I6de47600f430d3af5e3052ff456cd718a0df0d76
(cherry picked from commit 9d7a9b83ed8d20c8baa9a6b49812f4f30faf7782)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/72/713872/1 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_common/image/image_export.py', 'tripleo_common/tests/image/test_image_export.py']",2,68867107c892c6aedef1eae099680fc331ec2513,,"import mock @mock.patch('tripleo_common.image.image_export.open', side_effect=MemoryError()) def test_export_stream_memory_error(self, mock_open): blob_data = six.b('The Blob') blob_compressed = zlib.compress(blob_data) calc_digest = hashlib.sha256() calc_digest.update(blob_compressed) target_url = urlparse('docker://localhost:8787/t/nova-api:latest') layer = { 'digest': 'sha256:somethingelse' } calc_digest = hashlib.sha256() layer_stream = io.BytesIO(blob_compressed) self.assertRaises(MemoryError, image_export.export_stream, target_url, layer, layer_stream, verify_digest=False) ",,30,4
openstack%2Fopenstack-ansible-repo_build~master~I3bd6d3d987e32ee11c5f1fcb5c1b4b0fc797e7f9,openstack/openstack-ansible-repo_build,master,I3bd6d3d987e32ee11c5f1fcb5c1b4b0fc797e7f9,repo_build: Do not sync links directory between repo_containers,MERGED,2020-03-12 20:04:12.000000000,2020-03-20 00:47:05.000000000,2020-03-19 12:16:14.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 25600}, {'_account_id': 28619}]","[{'number': 1, 'created': '2020-03-12 20:04:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-repo_build/commit/e5cf413582b57e70c77bfea016f193b8c5455f6c', 'message': 'repo_build: Do not sync links directory between repo_containers\n\nThe links directory is only used by repo_build when creating requirement\nwheels. The repo_build process builds wheels for it\'s own distro/arch combo\nin serial. So in a mixed environment, where you might have ubuntu-16.04-x86_64\nand ubuntu-18.04-x86_64 repo_containers, heck even centos-7-x86_64 for that\nmatter, copying the links pointing to pools/distro-arch over to another\nrepo_container creating wheels for a different distro/arch leads to bad builds\ngetting deployed later.\n\nWe also need to prevent the later lsync config from spreading these out\nhttps://review.opendev.org/#/c/705390/\n\nWe can prevent this mix-up from happening even further by setting\nrepo_build_global_links_path: ""{{ repo_build_base_path }}/links/{{ repo_build_os_distro_version }}""\nas suggested in https://review.opendev.org/#/c/708115/\n\nThis is probably only relevant for stable/rocky.\n\nChange-Id: I3bd6d3d987e32ee11c5f1fcb5c1b4b0fc797e7f9\n'}, {'number': 2, 'created': '2020-03-13 07:32:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-repo_build/commit/6014af3f4aae9554ab1c73966ed562dcd484f69a', 'message': 'repo_build: Do not sync links directory between repo_containers\n\nThe links directory is only used by repo_build when creating requirement\nwheels. The repo_build process builds wheels for it\'s own distro/arch combo\nin serial. So in a mixed environment, where you might have ubuntu-16.04-x86_64\nand ubuntu-18.04-x86_64 repo_containers, heck even centos-7-x86_64 for that\nmatter, copying the links pointing to pools/distro-arch over to another\nrepo_container creating wheels for a different distro/arch leads to bad builds\ngetting deployed later.\n\nWe also need to prevent the later lsync config from spreading these out\nhttps://review.opendev.org/#/c/705390/\n\nWe can prevent this mix-up from happening even further by setting\nrepo_build_global_links_path: ""{{ repo_build_base_path }}/links/{{ repo_build_os_distro_version }}""\nas suggested in https://review.opendev.org/#/c/708115/\n\nThis is probably only relevant for stable/rocky.\n\nChange-Id: I3bd6d3d987e32ee11c5f1fcb5c1b4b0fc797e7f9\n'}, {'number': 3, 'created': '2020-03-13 09:00:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-repo_build/commit/d5103bca2e474b91a9a598252265ad6721ce48b3', 'message': 'DNM: Probe what\'s wrong with the git cache under tests\n\nrepo_build: Do not sync links directory between repo_containers\n\nThe links directory is only used by repo_build when creating requirement\nwheels. The repo_build process builds wheels for it\'s own distro/arch combo\nin serial. So in a mixed environment, where you might have ubuntu-16.04-x86_64\nand ubuntu-18.04-x86_64 repo_containers, heck even centos-7-x86_64 for that\nmatter, copying the links pointing to pools/distro-arch over to another\nrepo_container creating wheels for a different distro/arch leads to bad builds\ngetting deployed later.\n\nWe also need to prevent the later lsync config from spreading these out\nhttps://review.opendev.org/#/c/705390/\n\nWe can prevent this mix-up from happening even further by setting\nrepo_build_global_links_path: ""{{ repo_build_base_path }}/links/{{ repo_build_os_distro_version }}""\nas suggested in https://review.opendev.org/#/c/708115/\n\nThis is probably only relevant for stable/rocky.\n\nChange-Id: I3bd6d3d987e32ee11c5f1fcb5c1b4b0fc797e7f9\n'}, {'number': 4, 'created': '2020-03-13 09:56:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-repo_build/commit/21b4e47b25256cbf6a9044c4e30bc8ab7907117c', 'message': 'DNM: Probe what\'s wrong with the git cache under tests\n\nrepo_build: Do not sync links directory between repo_containers\n\nThe links directory is only used by repo_build when creating requirement\nwheels. The repo_build process builds wheels for it\'s own distro/arch combo\nin serial. So in a mixed environment, where you might have ubuntu-16.04-x86_64\nand ubuntu-18.04-x86_64 repo_containers, heck even centos-7-x86_64 for that\nmatter, copying the links pointing to pools/distro-arch over to another\nrepo_container creating wheels for a different distro/arch leads to bad builds\ngetting deployed later.\n\nWe also need to prevent the later lsync config from spreading these out\nhttps://review.opendev.org/#/c/705390/\n\nWe can prevent this mix-up from happening even further by setting\nrepo_build_global_links_path: ""{{ repo_build_base_path }}/links/{{ repo_build_os_distro_version }}""\nas suggested in https://review.opendev.org/#/c/708115/\n\nThis is probably only relevant for stable/rocky.\n\nChange-Id: I3bd6d3d987e32ee11c5f1fcb5c1b4b0fc797e7f9\n'}, {'number': 5, 'created': '2020-03-13 11:09:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-repo_build/commit/48b21e19e220e5bf35c44bc605f02615b09a8994', 'message': 'DNM: Probe what\'s wrong with the git cache under tests\n\nrepo_build: Do not sync links directory between repo_containers\n\nThe links directory is only used by repo_build when creating requirement\nwheels. The repo_build process builds wheels for it\'s own distro/arch combo\nin serial. So in a mixed environment, where you might have ubuntu-16.04-x86_64\nand ubuntu-18.04-x86_64 repo_containers, heck even centos-7-x86_64 for that\nmatter, copying the links pointing to pools/distro-arch over to another\nrepo_container creating wheels for a different distro/arch leads to bad builds\ngetting deployed later.\n\nWe also need to prevent the later lsync config from spreading these out\nhttps://review.opendev.org/#/c/705390/\n\nWe can prevent this mix-up from happening even further by setting\nrepo_build_global_links_path: ""{{ repo_build_base_path }}/links/{{ repo_build_os_distro_version }}""\nas suggested in https://review.opendev.org/#/c/708115/\n\nThis is probably only relevant for stable/rocky.\n\nChange-Id: I3bd6d3d987e32ee11c5f1fcb5c1b4b0fc797e7f9\n'}, {'number': 6, 'created': '2020-03-13 11:53:57.000000000', 'files': ['tasks/repo_package_sync.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-repo_build/commit/4bd52916e724046ca3b138e55b443d730547c0b6', 'message': 'repo_build: Do not sync links directory between repo_containers\n\nThe links directory is only used by repo_build when creating requirement\nwheels. The repo_build process builds wheels for it\'s own distro/arch combo\nin serial. So in a mixed environment, where you might have ubuntu-16.04-x86_64\nand ubuntu-18.04-x86_64 repo_containers, heck even centos-7-x86_64 for that\nmatter, copying the links pointing to pools/distro-arch over to another\nrepo_container creating wheels for a different distro/arch leads to bad builds\ngetting deployed later.\n\nWe also need to prevent the later lsync config from spreading these out\nhttps://review.opendev.org/#/c/705390/\n\nWe can prevent this mix-up from happening even further by setting\nrepo_build_global_links_path: ""{{ repo_build_base_path }}/links/{{ repo_build_os_distro_version }}""\nas suggested in https://review.opendev.org/#/c/708115/\n\nThis is probably only relevant for stable/rocky.\nDepends-On: Ieca6ec9bf898432c76c019a1d2d97c280cd85dce\nChange-Id: I3bd6d3d987e32ee11c5f1fcb5c1b4b0fc797e7f9\n'}]",0,712784,4bd52916e724046ca3b138e55b443d730547c0b6,21,4,6,25600,,,0,"repo_build: Do not sync links directory between repo_containers

The links directory is only used by repo_build when creating requirement
wheels. The repo_build process builds wheels for it's own distro/arch combo
in serial. So in a mixed environment, where you might have ubuntu-16.04-x86_64
and ubuntu-18.04-x86_64 repo_containers, heck even centos-7-x86_64 for that
matter, copying the links pointing to pools/distro-arch over to another
repo_container creating wheels for a different distro/arch leads to bad builds
getting deployed later.

We also need to prevent the later lsync config from spreading these out
https://review.opendev.org/#/c/705390/

We can prevent this mix-up from happening even further by setting
repo_build_global_links_path: ""{{ repo_build_base_path }}/links/{{ repo_build_os_distro_version }}""
as suggested in https://review.opendev.org/#/c/708115/

This is probably only relevant for stable/rocky.
Depends-On: Ieca6ec9bf898432c76c019a1d2d97c280cd85dce
Change-Id: I3bd6d3d987e32ee11c5f1fcb5c1b4b0fc797e7f9
",git fetch https://review.opendev.org/openstack/openstack-ansible-repo_build refs/changes/84/712784/5 && git format-patch -1 --stdout FETCH_HEAD,['tasks/repo_package_sync.yml'],1,e5cf413582b57e70c77bfea016f193b8c5455f6c,,," - src: ""{{ repo_build_global_links_path }}"" dest: ""{{ repo_build_base_path }}""",0,2
openstack%2Ftripleo-heat-templates~stable%2Ftrain~I7a1ce4ba4ac358733747e9301038d0245ae08016,openstack/tripleo-heat-templates,stable/train,I7a1ce4ba4ac358733747e9301038d0245ae08016,Move ceph-ansible required variables in the main group,MERGED,2020-03-19 18:33:56.000000000,2020-03-20 00:46:25.000000000,2020-03-20 00:46:25.000000000,"[{'_account_id': 3153}, {'_account_id': 6796}, {'_account_id': 14985}, {'_account_id': 18002}, {'_account_id': 20182}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 25402}]","[{'number': 1, 'created': '2020-03-19 18:33:56.000000000', 'files': ['deployment/ceph-ansible/ceph-base.yaml', 'deployment/ceph-ansible/ceph-grafana.yaml', 'deployment/ceph-ansible/ceph-mgr.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/52a3f896de217275721adb8940d4fbb68685fe1f', 'message': ""Move ceph-ansible required variables in the main group\n\nceph-ansible v4.0.16 introduced some changes to double check\nthe ceph dashboard and grafana passwords exist, are properly\ngenerated and are not defaults values.\nHowever, the constraint introduced forced tripleo to put\nthese variables in all.yaml instead of distributing them on\nthe related roles (mgr and grafana-server).\nThis change moves these variables in ceph-base, since here\nwe're able to generate the ceph_ansible_group_vars_all.\n\nCloses-Bug: #1867973\nChange-Id: I7a1ce4ba4ac358733747e9301038d0245ae08016\n(cherry picked from commit ebe1a40fb94f35f90972a675d4998bb50fb6f752)\n""}]",0,713959,52a3f896de217275721adb8940d4fbb68685fe1f,8,8,1,25402,,,0,"Move ceph-ansible required variables in the main group

ceph-ansible v4.0.16 introduced some changes to double check
the ceph dashboard and grafana passwords exist, are properly
generated and are not defaults values.
However, the constraint introduced forced tripleo to put
these variables in all.yaml instead of distributing them on
the related roles (mgr and grafana-server).
This change moves these variables in ceph-base, since here
we're able to generate the ceph_ansible_group_vars_all.

Closes-Bug: #1867973
Change-Id: I7a1ce4ba4ac358733747e9301038d0245ae08016
(cherry picked from commit ebe1a40fb94f35f90972a675d4998bb50fb6f752)
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/59/713959/1 && git format-patch -1 --stdout FETCH_HEAD,"['deployment/ceph-ansible/ceph-base.yaml', 'deployment/ceph-ansible/ceph-grafana.yaml', 'deployment/ceph-ansible/ceph-mgr.yaml']",3,52a3f896de217275721adb8940d4fbb68685fe1f,lp1867973-stable/train,, CephDashboardAdminPassword: description: Admin password for the dashboard component type: string hidden: true dashboard_admin_password: {get_param: CephDashboardAdminPassword},17,11
openstack%2Ftripleo-quickstart~master~I89f969808877c389af3d1fb977530735984b9bbf,openstack/tripleo-quickstart,master,I89f969808877c389af3d1fb977530735984b9bbf,Adjust molecule requirements,MERGED,2020-03-17 20:46:41.000000000,2020-03-20 00:32:10.000000000,2020-03-20 00:30:23.000000000,"[{'_account_id': 3153}, {'_account_id': 9592}, {'_account_id': 18846}, {'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-17 20:46:41.000000000', 'files': ['tox.ini', 'molecule-requirements.txt'], 'web_link': 'https://opendev.org/openstack/tripleo-quickstart/commit/076c67e8796a06b18e05f3da9e962fd9518d1602', 'message': 'Adjust molecule requirements\n\nopenstack-tox-molecule is currently broken with dependency issues\nbetween pytest-html and molecule.\n\nThis change uses the same pytest/molecule requirements spec as\ntripleo-ansible[1] and breaks out the requirements into their own\nfile.\n\n[1] https://opendev.org/openstack/tripleo-ansible/src/branch/master/molecule-requirements.txt\nCloses-Bug: #1867035\n\nChange-Id: I89f969808877c389af3d1fb977530735984b9bbf\n'}]",0,713532,076c67e8796a06b18e05f3da9e962fd9518d1602,16,5,1,4571,,,0,"Adjust molecule requirements

openstack-tox-molecule is currently broken with dependency issues
between pytest-html and molecule.

This change uses the same pytest/molecule requirements spec as
tripleo-ansible[1] and breaks out the requirements into their own
file.

[1] https://opendev.org/openstack/tripleo-ansible/src/branch/master/molecule-requirements.txt
Closes-Bug: #1867035

Change-Id: I89f969808877c389af3d1fb977530735984b9bbf
",git fetch https://review.opendev.org/openstack/tripleo-quickstart refs/changes/32/713532/1 && git format-patch -1 --stdout FETCH_HEAD,"['tox.ini', 'molecule-requirements.txt']",2,076c67e8796a06b18e05f3da9e962fd9518d1602,bug/1867035,"ansi2html docker>=4.0.1 paramiko>=2.5.0 pytest pytest-cov pytest-html pytest-molecule pytest-xdist mock molecule>=3.0,<3.1 selinux>=0.2.1 ",,12,12
openstack%2Fpaunch~master~I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da,openstack/paunch,master,I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da,Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all',MERGED,2020-03-19 16:49:21.000000000,2020-03-20 00:32:06.000000000,2020-03-20 00:30:24.000000000,"[{'_account_id': 3153}, {'_account_id': 9592}, {'_account_id': 14985}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 23618}]","[{'number': 1, 'created': '2020-03-19 16:49:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/paunch/commit/38a7b516c2f7cfc04e89eae678e5f4b3be4d7849', 'message': ""Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all'\n\nIn the case of nova_libvirt container, we want to use all CPUs that are\nreported online.\nRather than computing the list with Python (which has proven to be\nproblematic on PPC), let the container engine figuring it out by itself\nlike it was the case before.\n\nChange-Id: I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da\nCloses-Bug: #1868135\n""}, {'number': 2, 'created': '2020-03-19 17:00:45.000000000', 'files': ['paunch/builder/podman.py', 'paunch/tests/test_builder_compose1.py', 'paunch/utils/common.py', 'paunch/builder/compose1.py', 'paunch/tests/test_builder_base.py', 'paunch/tests/test_utils_common.py'], 'web_link': 'https://opendev.org/openstack/paunch/commit/9b6276512141693bddd064c915b08c1d027e5985', 'message': ""Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all'\n\nIn the case of nova_libvirt container, we want to use all CPUs that are\nreported online.\nRather than computing the list with Python (which has proven to be\nproblematic on PPC), let the container engine figuring it out by itself\nlike it was the case before.\n\nChange-Id: I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da\nCloses-Bug: #1868135\n""}]",0,713932,9b6276512141693bddd064c915b08c1d027e5985,13,6,2,3153,,,0,"Do not set cpuset-cpus if cconfig['cpuset_cpus'] == 'all'

In the case of nova_libvirt container, we want to use all CPUs that are
reported online.
Rather than computing the list with Python (which has proven to be
problematic on PPC), let the container engine figuring it out by itself
like it was the case before.

Change-Id: I5d1b88c90dbc4114d996008a407cd1dd9a6eb9da
Closes-Bug: #1868135
",git fetch https://review.opendev.org/openstack/paunch refs/changes/32/713932/2 && git format-patch -1 --stdout FETCH_HEAD,"['paunch/builder/podman.py', 'paunch/utils/common.py', 'paunch/builder/compose1.py', 'paunch/tests/test_utils_common.py']",4,38a7b516c2f7cfc04e89eae678e5f4b3be4d7849,bug/1868135,," @mock.patch(""psutil.cpu_count"", return_value=4) def test_get_all_cpus(self, mock_cpu): expected_list = '0-3' actual_list = common.get_all_cpus() self.assertEqual(actual_list, expected_list) ",10,22
openstack%2Fdevstack-plugin-ceph~master~I9210e5d97872da529e353c965a3102591b158d17,openstack/devstack-plugin-ceph,master,I9210e5d97872da529e353c965a3102591b158d17,Use Luminous as default CEPH_RELEASE,ABANDONED,2019-03-13 20:21:01.000000000,2020-03-20 00:19:55.000000000,,"[{'_account_id': 4523}, {'_account_id': 6873}, {'_account_id': 13252}, {'_account_id': 22348}, {'_account_id': 26233}]","[{'number': 1, 'created': '2019-03-13 20:21:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-plugin-ceph/commit/ae7fc31a8990be0265640653dfe80e34bbfdde28', 'message': ""Use Luminous as default CEPH_RELEASE\n\nThe Hammer release is no longer active [1] and recently, we ran into\na nova-grenade-live-migration job failure in preparation for the\nchange from ubuntu-xenial to ubuntu-bionic default CI node type\nbecause the Hammer release is not available on ubuntu-bionic.\n\nThis updates the default release to Luminous, as it is an active\nrelease and is what's available in the UCA for Queens [2].\n\nCloses-Bug: #1819944\n\n[1] http://docs.ceph.com/docs/master/releases\n[2] http://ubuntu-cloud.archive.canonical.com/ubuntu/dists/xenial-proposed/queens/main/binary-amd64/Packages\n\nChange-Id: I9210e5d97872da529e353c965a3102591b158d17\n""}, {'number': 2, 'created': '2019-03-13 20:21:43.000000000', 'files': ['devstack/lib/ceph'], 'web_link': 'https://opendev.org/openstack/devstack-plugin-ceph/commit/cc6963db613d3b1b062cf8847fe8bee845d03255', 'message': ""Use Luminous as default CEPH_RELEASE\n\nThe Hammer release is no longer active [1] and recently, we ran into\na nova-grenade-live-migration job failure in preparation for the\nchange from ubuntu-xenial to ubuntu-bionic default CI node type\nbecause the Hammer release is not available on ubuntu-bionic.\n\nThis updates the default release to Luminous, as it is an active\nrelease and is what's available in the UCA for Queens [2].\n\nCloses-Bug: #1819944\n\nDepends-On: https://review.openstack.org/639017\n\n[1] http://docs.ceph.com/docs/master/releases\n[2] http://ubuntu-cloud.archive.canonical.com/ubuntu/dists/xenial-proposed/queens/main/binary-amd64/Packages\n\nChange-Id: I9210e5d97872da529e353c965a3102591b158d17\n""}]",4,643154,cc6963db613d3b1b062cf8847fe8bee845d03255,8,5,2,4690,,,0,"Use Luminous as default CEPH_RELEASE

The Hammer release is no longer active [1] and recently, we ran into
a nova-grenade-live-migration job failure in preparation for the
change from ubuntu-xenial to ubuntu-bionic default CI node type
because the Hammer release is not available on ubuntu-bionic.

This updates the default release to Luminous, as it is an active
release and is what's available in the UCA for Queens [2].

Closes-Bug: #1819944

Depends-On: https://review.openstack.org/639017

[1] http://docs.ceph.com/docs/master/releases
[2] http://ubuntu-cloud.archive.canonical.com/ubuntu/dists/xenial-proposed/queens/main/binary-amd64/Packages

Change-Id: I9210e5d97872da529e353c965a3102591b158d17
",git fetch https://review.opendev.org/openstack/devstack-plugin-ceph refs/changes/54/643154/2 && git format-patch -1 --stdout FETCH_HEAD,['devstack/lib/ceph'],1,ae7fc31a8990be0265640653dfe80e34bbfdde28,bug/1819944,CEPH_RELEASE=${CEPH_RELEASE:-luminous},CEPH_RELEASE=${CEPH_RELEASE:-hammer},1,1
openstack%2Ftempest~master~Ia001541e34193a2f30cd95be222e944d2ba62e9d,openstack/tempest,master,Ia001541e34193a2f30cd95be222e944d2ba62e9d,DNM: Use config settings for SSL in test_novnc,ABANDONED,2019-07-31 03:51:44.000000000,2020-03-20 00:17:05.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2019-07-31 03:51:44.000000000', 'files': ['tempest/api/compute/servers/test_novnc.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/f2f1da48d9a90ad91881cbea83af0859690458af', 'message': 'DNM: Use config settings for SSL in test_novnc\n\nChange-Id: Ia001541e34193a2f30cd95be222e944d2ba62e9d\n'}]",0,673723,f2f1da48d9a90ad91881cbea83af0859690458af,3,1,1,4690,,,0,"DNM: Use config settings for SSL in test_novnc

Change-Id: Ia001541e34193a2f30cd95be222e944d2ba62e9d
",git fetch https://review.opendev.org/openstack/tempest refs/changes/23/673723/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/compute/servers/test_novnc.py'],1,f2f1da48d9a90ad91881cbea83af0859690458af,test-novnc-proxy-ssl," # Use the configuration settings for SSL cert validation for our # request to the vnc proxy kwargs = {} if CONF.identity.disable_ssl_certificate_validation: urllib3.disable_warnings() kwargs['cert_reqs'] = 'CERT_NONE' elif CONF.identity.ca_certificates_file: kwargs['cert_reqs'] = 'CERT_REQUIRED' kwargs['ca_certs'] = CONF.identity.ca_certificates_file resp = urllib3.PoolManager(**kwargs).request('GET', vnc_url)"," resp = urllib3.PoolManager().request('GET', vnc_url)",10,1
openstack%2Fnova~master~Ic236dbf75df68aae47f23f203a1bf958834fcc95,openstack/nova,master,Ic236dbf75df68aae47f23f203a1bf958834fcc95,WIP Remove project_only=True from database queries,ABANDONED,2019-02-14 17:15:01.000000000,2020-03-20 00:16:14.000000000,,"[{'_account_id': 4690}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 14595}, {'_account_id': 16128}, {'_account_id': 16376}, {'_account_id': 22348}, {'_account_id': 23498}, {'_account_id': 26515}]","[{'number': 1, 'created': '2019-02-14 17:15:01.000000000', 'files': ['nova/db/sqlalchemy/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/b4fc2dab945c8130ee38e59d7a1f081302ea78ea', 'message': 'WIP Remove project_only=True from database queries\n\nChange-Id: Ic236dbf75df68aae47f23f203a1bf958834fcc95\n'}]",0,637010,b4fc2dab945c8130ee38e59d7a1f081302ea78ea,15,9,1,4690,,,0,"WIP Remove project_only=True from database queries

Change-Id: Ic236dbf75df68aae47f23f203a1bf958834fcc95
",git fetch https://review.opendev.org/openstack/nova refs/changes/10/637010/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/db/sqlalchemy/api.py'],1,b4fc2dab945c8130ee38e59d7a1f081302ea78ea,project-only," result = model_query(context, models.FloatingIp).\ # We need project_only here because it will cause the context.project # to be used to validate whether the instance exists in the project (by # way of filtering the results to that project only). query = model_query(context, models.Instance).\def _instance_get_all_query(context, joins=None): query = model_query(context, models.Instance) instance_ref = model_query(context, models.Instance).\ query = _security_group_get_query(context, join_rules=join_rules).\"," result = model_query(context, models.FloatingIp, project_only=True).\ # FIXME(sirp): shouldn't we just use project_only here to restrict the # results? # NOTE(sirp): shouldn't we just use project_only here to restrict the # results? query = model_query(context, models.Instance, project_only=True).\def _instance_get_all_query(context, project_only=False, joins=None): query = model_query(context, models.Instance, project_only=project_only) instance_ref = model_query(context, models.Instance, project_only=True).\ query = _security_group_get_query(context, project_only=True, join_rules=join_rules).\",9,14
openstack%2Frequirements~master~I8857361d699c0f79dd12307dc263868fdfd219a1,openstack/requirements,master,I8857361d699c0f79dd12307dc263868fdfd219a1,update constraint for oslo.vmware to new release 3.2.1,MERGED,2020-03-19 10:23:13.000000000,2020-03-20 00:11:22.000000000,2020-03-20 00:08:21.000000000,"[{'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 10:23:13.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/1bed59facb4c4510a2453e100718db79de9995d9', 'message': 'update constraint for oslo.vmware to new release 3.2.1\n\nChange-Id: I8857361d699c0f79dd12307dc263868fdfd219a1\nmeta:version: 3.2.1\nmeta:diff-start: -\nmeta:series: ussuri\nmeta:release-type: release\nmeta:pypi: yes\nmeta:first: no\nmeta:release:Author: Ben Nemec <bnemec@redhat.com>\nmeta:release:Commit: Ben Nemec <bnemec@redhat.com>\nmeta:release:Change-Id: Id2792199eb5e1843b2f2fb02b5574992e3ba17fa\nmeta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nmeta:release:Workflow+1: Thierry Carrez <thierry@openstack.org>\n'}]",0,713821,1bed59facb4c4510a2453e100718db79de9995d9,13,3,1,11131,,,0,"update constraint for oslo.vmware to new release 3.2.1

Change-Id: I8857361d699c0f79dd12307dc263868fdfd219a1
meta:version: 3.2.1
meta:diff-start: -
meta:series: ussuri
meta:release-type: release
meta:pypi: yes
meta:first: no
meta:release:Author: Ben Nemec <bnemec@redhat.com>
meta:release:Commit: Ben Nemec <bnemec@redhat.com>
meta:release:Change-Id: Id2792199eb5e1843b2f2fb02b5574992e3ba17fa
meta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
meta:release:Workflow+1: Thierry Carrez <thierry@openstack.org>
",git fetch https://review.opendev.org/openstack/requirements refs/changes/21/713821/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,1bed59facb4c4510a2453e100718db79de9995d9,new-release,oslo.vmware===3.2.1;python_version=='3.6' oslo.vmware===3.2.1;python_version=='3.7',oslo.vmware===3.2.0;python_version=='3.6' oslo.vmware===3.2.0;python_version=='3.7',2,2
openstack%2Fnova~stable%2Ftrain~Icd7ab2ca4ddbed92c7e883a63a23245920d961e7,openstack/nova,stable/train,Icd7ab2ca4ddbed92c7e883a63a23245920d961e7,nova-live-migration: Wait for n-cpu services to come up after configuring Ceph,MERGED,2020-03-19 11:03:54.000000000,2020-03-20 00:10:49.000000000,2020-03-20 00:08:08.000000000,"[{'_account_id': 4690}, {'_account_id': 10118}, {'_account_id': 10135}, {'_account_id': 14595}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 26515}, {'_account_id': 29963}]","[{'number': 1, 'created': '2020-03-19 11:03:54.000000000', 'files': ['gate/live_migration/hooks/ceph.sh'], 'web_link': 'https://opendev.org/openstack/nova/commit/70447bca2f4f33c6872eaf94a2e4351bb257c22a', 'message': 'nova-live-migration: Wait for n-cpu services to come up after configuring Ceph\n\nPreviously the ceph.sh script used during the nova-live-migration job\nwould only grep for a `compute` process when checking if the services\nhad been restarted. This check was bogus and would always return 0 as it\nwould always match itself. For example:\n\n2020-03-13 21:06:47.682073 | primary | 2020-03-13 21:06:47.681 | root\n29529  0.0  0.0   4500   736 pts/0    S+   21:06   0:00 /bin/sh -c ps\n       aux | grep compute\n2020-03-13 21:06:47.683964 | primary | 2020-03-13 21:06:47.683 | root\n29531  0.0  0.0  14616   944 pts/0    S+   21:06   0:00 grep compute\n\nFailures of this job were seen on the stable/pike branch where slower CI\nnodes appeared to struggle to allow Libvirt to report to n-cpu in time\nbefore Tempest was started. This in-turn caused instance build failures\nand the overall failure of the job.\n\nThis change resolves this issue by switching to pgrep and ensuring\nn-cpu services are reported as fully up after a cold restart before\nstarting the Tempest test run.\n\nCloses-Bug: 1867380\nChange-Id: Icd7ab2ca4ddbed92c7e883a63a23245920d961e7\n(cherry picked from commit e23c3c2c8df3843c5853c87ef684bd21c4af95d8)\n'}]",0,713836,70447bca2f4f33c6872eaf94a2e4351bb257c22a,17,8,1,10135,,,0,"nova-live-migration: Wait for n-cpu services to come up after configuring Ceph

Previously the ceph.sh script used during the nova-live-migration job
would only grep for a `compute` process when checking if the services
had been restarted. This check was bogus and would always return 0 as it
would always match itself. For example:

2020-03-13 21:06:47.682073 | primary | 2020-03-13 21:06:47.681 | root
29529  0.0  0.0   4500   736 pts/0    S+   21:06   0:00 /bin/sh -c ps
       aux | grep compute
2020-03-13 21:06:47.683964 | primary | 2020-03-13 21:06:47.683 | root
29531  0.0  0.0  14616   944 pts/0    S+   21:06   0:00 grep compute

Failures of this job were seen on the stable/pike branch where slower CI
nodes appeared to struggle to allow Libvirt to report to n-cpu in time
before Tempest was started. This in-turn caused instance build failures
and the overall failure of the job.

This change resolves this issue by switching to pgrep and ensuring
n-cpu services are reported as fully up after a cold restart before
starting the Tempest test run.

Closes-Bug: 1867380
Change-Id: Icd7ab2ca4ddbed92c7e883a63a23245920d961e7
(cherry picked from commit e23c3c2c8df3843c5853c87ef684bd21c4af95d8)
",git fetch https://review.opendev.org/openstack/nova refs/changes/36/713836/1 && git format-patch -1 --stdout FETCH_HEAD,['gate/live_migration/hooks/ceph.sh'],1,70447bca2f4f33c6872eaf94a2e4351bb257c22a,bug/1867380,"function _wait_for_nova_compute_service_state { source $BASE/new/devstack/openrc admin admin local status=$1 local attempt=1 local max_attempts=24 local attempt_sleep=5 local computes_count=$(openstack compute service list | grep -c nova-compute) local computes_ready=$(openstack compute service list | grep nova-compute | grep $status | wc -l) echo ""Waiting for $computes_count computes to report as $status"" while [ ""$computes_ready"" -ne ""$computes_count"" ]; do if [ ""$attempt"" -eq ""$max_attempts"" ]; then echo ""Failed waiting for computes to report as ${status}, ${computes_ready}/${computes_count} ${status} after ${max_attempts} attempts"" exit 4 fi echo ""Waiting ${attempt_sleep} seconds for ${computes_count} computes to report as ${status}, ${computes_ready}/${computes_count} ${status} after ${attempt}/${max_attempts} attempts"" sleep $attempt_sleep attempt=$((attempt+1)) computes_ready=$(openstack compute service list | grep nova-compute | grep $status | wc -l) done echo ""All computes are now reporting as ${status} after ${attempt} attempts"" } function configure_and_start_nova { echo ""Checking all n-cpu services"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""pgrep -u stack -a nova-compute"" # stop nova-compute echo ""Stopping all n-cpu services"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""systemctl stop devstack@n-cpu"" # Wait for the service to be marked as down _wait_for_nova_compute_service_state ""down"" # start nova-compute echo ""Starting all n-cpu services"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""systemctl start devstack@n-cpu"" echo ""Checking all n-cpu services"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""pgrep -u stack -a nova-compute"" # Wait for the service to be marked as up _wait_for_nova_compute_service_state ""up""","function configure_and_start_nova { echo 'check compute processes before restart' $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""ps aux | grep compute"" # restart nova-compute $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""systemctl restart devstack@n-cpu"" $ANSIBLE all --become -f 5 -i ""$WORKSPACE/inventory"" -m shell -a ""ps aux | grep compute"" ",42,5
openstack%2Fswift~feature%2Flosf~I0e4ab89bbe3d0aee305272fdd920122e80530b8b,openstack/swift,feature/losf,I0e4ab89bbe3d0aee305272fdd920122e80530b8b,Fix get_stats,MERGED,2020-03-19 15:47:50.000000000,2020-03-20 00:10:13.000000000,2020-03-20 00:08:24.000000000,"[{'_account_id': 13852}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 15:47:50.000000000', 'files': ['swift/obj/rpc_http.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/b6e5f407b46f6cde08c9945fcef96535c5d42483', 'message': 'Fix get_stats\n\nThe response is now the stats themselves (remove "".stats"")\n\nChange-Id: I0e4ab89bbe3d0aee305272fdd920122e80530b8b\n'}]",0,713910,b6e5f407b46f6cde08c9945fcef96535c5d42483,7,2,1,25251,,,0,"Fix get_stats

The response is now the stats themselves (remove "".stats"")

Change-Id: I0e4ab89bbe3d0aee305272fdd920122e80530b8b
",git fetch https://review.opendev.org/openstack/swift refs/changes/10/713910/1 && git format-patch -1 --stdout FETCH_HEAD,['swift/obj/rpc_http.py'],1,b6e5f407b46f6cde08c9945fcef96535c5d42483,dev/alecuyer/feature-branch-catchup, return response, return response.stats,1,1
openstack%2Frequirements~master~I25d0fcbd6f8cdafd645b9a0aeacbbe35a229714c,openstack/requirements,master,I25d0fcbd6f8cdafd645b9a0aeacbbe35a229714c,update constraint for oslo.policy to new release 3.0.1,MERGED,2020-03-19 10:26:03.000000000,2020-03-20 00:10:08.000000000,2020-03-20 00:08:17.000000000,"[{'_account_id': 6928}, {'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 10:26:03.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/f44f726db9437eb40865b447d78fc92843a13859', 'message': 'update constraint for oslo.policy to new release 3.0.1\n\nChange-Id: I25d0fcbd6f8cdafd645b9a0aeacbbe35a229714c\nmeta:version: 3.0.1\nmeta:diff-start: -\nmeta:series: ussuri\nmeta:release-type: release\nmeta:pypi: yes\nmeta:first: no\nmeta:release:Author: Ben Nemec <bnemec@redhat.com>\nmeta:release:Commit: Ben Nemec <bnemec@redhat.com>\nmeta:release:Change-Id: Id2792199eb5e1843b2f2fb02b5574992e3ba17fa\nmeta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nmeta:release:Workflow+1: Thierry Carrez <thierry@openstack.org>\n'}]",0,713823,f44f726db9437eb40865b447d78fc92843a13859,13,4,1,11131,,,0,"update constraint for oslo.policy to new release 3.0.1

Change-Id: I25d0fcbd6f8cdafd645b9a0aeacbbe35a229714c
meta:version: 3.0.1
meta:diff-start: -
meta:series: ussuri
meta:release-type: release
meta:pypi: yes
meta:first: no
meta:release:Author: Ben Nemec <bnemec@redhat.com>
meta:release:Commit: Ben Nemec <bnemec@redhat.com>
meta:release:Change-Id: Id2792199eb5e1843b2f2fb02b5574992e3ba17fa
meta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
meta:release:Workflow+1: Thierry Carrez <thierry@openstack.org>
",git fetch https://review.opendev.org/openstack/requirements refs/changes/23/713823/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,f44f726db9437eb40865b447d78fc92843a13859,new-release,oslo.policy===3.0.1,oslo.policy===2.4.1,1,1
openstack%2Frequirements~master~I5f76614f6fed6a5fcff5539ddc97898fd1cbad18,openstack/requirements,master,I5f76614f6fed6a5fcff5539ddc97898fd1cbad18,update constraint for oslo.service to new release 2.1.0,MERGED,2020-03-19 10:21:38.000000000,2020-03-20 00:08:19.000000000,2020-03-20 00:08:19.000000000,"[{'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 10:21:38.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/c99ce20e1d1e38faca2a69368f037a31e88d1b84', 'message': 'update constraint for oslo.service to new release 2.1.0\n\nChange-Id: I5f76614f6fed6a5fcff5539ddc97898fd1cbad18\nmeta:version: 2.1.0\nmeta:diff-start: -\nmeta:series: ussuri\nmeta:release-type: release\nmeta:pypi: yes\nmeta:first: no\nmeta:release:Author: Ben Nemec <bnemec@redhat.com>\nmeta:release:Commit: Ben Nemec <bnemec@redhat.com>\nmeta:release:Change-Id: Id2792199eb5e1843b2f2fb02b5574992e3ba17fa\nmeta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nmeta:release:Workflow+1: Thierry Carrez <thierry@openstack.org>\n'}]",0,713819,c99ce20e1d1e38faca2a69368f037a31e88d1b84,11,3,1,11131,,,0,"update constraint for oslo.service to new release 2.1.0

Change-Id: I5f76614f6fed6a5fcff5539ddc97898fd1cbad18
meta:version: 2.1.0
meta:diff-start: -
meta:series: ussuri
meta:release-type: release
meta:pypi: yes
meta:first: no
meta:release:Author: Ben Nemec <bnemec@redhat.com>
meta:release:Commit: Ben Nemec <bnemec@redhat.com>
meta:release:Change-Id: Id2792199eb5e1843b2f2fb02b5574992e3ba17fa
meta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
meta:release:Workflow+1: Thierry Carrez <thierry@openstack.org>
",git fetch https://review.opendev.org/openstack/requirements refs/changes/19/713819/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,c99ce20e1d1e38faca2a69368f037a31e88d1b84,new-release,oslo.service===2.1.0;python_version=='3.6' oslo.service===2.1.0;python_version=='3.7',oslo.service===2.0.0;python_version=='3.6' oslo.service===2.0.0;python_version=='3.7',2,2
openstack%2Frequirements~stable%2Frocky~Iff22a24f6c453c515b652e6a4319c2b438791875,openstack/requirements,stable/rocky,Iff22a24f6c453c515b652e6a4319c2b438791875,update constraint for glance_store to new release 0.26.2,MERGED,2020-03-19 12:45:09.000000000,2020-03-20 00:08:16.000000000,2020-03-20 00:08:16.000000000,"[{'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 12:45:09.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/6ec8f2efd1b4f84b02f6eff1fd025bcfb2f2c5f1', 'message': 'update constraint for glance_store to new release 0.26.2\n\nChange-Id: Iff22a24f6c453c515b652e6a4319c2b438791875\nmeta:version: 0.26.2\nmeta:diff-start: -\nmeta:series: rocky\nmeta:release-type: release\nmeta:pypi: yes\nmeta:first: no\nmeta:release:Author: Abhishek Kekane <akekane@redhat.com>\nmeta:release:Commit: Abhishek Kekane <akekane@redhat.com>\nmeta:release:Change-Id: Ie518c1d82ab3c7fdd5447666e89dacfbd3bda993\nmeta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nmeta:release:Code-Review+2: Sean McGinnis <sean.mcginnis@gmail.com>\nmeta:release:Workflow+1: Sean McGinnis <sean.mcginnis@gmail.com>\n'}]",0,713868,6ec8f2efd1b4f84b02f6eff1fd025bcfb2f2c5f1,11,3,1,11131,,,0,"update constraint for glance_store to new release 0.26.2

Change-Id: Iff22a24f6c453c515b652e6a4319c2b438791875
meta:version: 0.26.2
meta:diff-start: -
meta:series: rocky
meta:release-type: release
meta:pypi: yes
meta:first: no
meta:release:Author: Abhishek Kekane <akekane@redhat.com>
meta:release:Commit: Abhishek Kekane <akekane@redhat.com>
meta:release:Change-Id: Ie518c1d82ab3c7fdd5447666e89dacfbd3bda993
meta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
meta:release:Code-Review+2: Sean McGinnis <sean.mcginnis@gmail.com>
meta:release:Workflow+1: Sean McGinnis <sean.mcginnis@gmail.com>
",git fetch https://review.opendev.org/openstack/requirements refs/changes/68/713868/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,6ec8f2efd1b4f84b02f6eff1fd025bcfb2f2c5f1,new-release,glance-store===0.26.2,glance-store===0.26.1,1,1
openstack%2Fopenstack-helm-infra~master~Id11079b5ce3a8d1010e604300f457e4060aee582,openstack/openstack-helm-infra,master,Id11079b5ce3a8d1010e604300f457e4060aee582,[libvirt] update values.yaml to use train libvirt image,MERGED,2020-03-09 23:51:29.000000000,2020-03-20 00:01:37.000000000,2020-03-20 00:00:04.000000000,"[{'_account_id': 8898}, {'_account_id': 20466}, {'_account_id': 21420}, {'_account_id': 22348}, {'_account_id': 23928}]","[{'number': 1, 'created': '2020-03-09 23:51:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/df97dda583c4fc91eb628edf45156051f8c09277', 'message': ""[libvirt] update values.yaml to use train libvirt image\n\nthis means the chart works 'as-is' (with the rest of the currently\nreleased components) in most cases without the explicit need for an\nimage override.\n\nChange-Id: Id11079b5ce3a8d1010e604300f457e4060aee582\n""}, {'number': 2, 'created': '2020-03-13 18:41:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/23140972dc932d1fe8fa216ce097b48fc2c9e59b', 'message': ""[libvirt] update values.yaml to use train libvirt image\n\nthis means the chart works 'as-is' (with the rest of the currently\nreleased components) in most cases without the explicit need for an\nimage override.\n\nChange-Id: Id11079b5ce3a8d1010e604300f457e4060aee582\n""}, {'number': 3, 'created': '2020-03-13 20:57:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/9844d699b855c990d81096097422a7e4ea503af4', 'message': ""[libvirt] update values.yaml to use train libvirt image\n\nthis means the chart works 'as-is' (with the rest of the currently\nreleased components) in most cases without the explicit need for an\nimage override.\n\nChange-Id: Id11079b5ce3a8d1010e604300f457e4060aee582\n""}, {'number': 4, 'created': '2020-03-14 12:49:02.000000000', 'files': ['libvirt/values_overrides/stein-ubuntu_bionic.yaml', 'libvirt/values_overrides/queens-ubuntu_xenial.yaml', 'libvirt/values_overrides/rocky-ubuntu_xenial.yaml', 'libvirt/values_overrides/ocata-ubuntu_xenial.yaml', 'libvirt/values_overrides/pike-ubuntu_xenial.yaml', 'libvirt/values_overrides/train-ubuntu_bionic.yaml', 'libvirt/values.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/14d8118e2ec5a2f2043106695c1b84ae6975f4a8', 'message': ""[libvirt] update values.yaml to use train libvirt image\n\nthis means the chart works 'as-is' (with the rest of the currently\nreleased components) in most cases without the explicit need for an\nimage override.\n\nChange-Id: Id11079b5ce3a8d1010e604300f457e4060aee582\n""}]",1,712004,14d8118e2ec5a2f2043106695c1b84ae6975f4a8,17,5,4,8898,,,0,"[libvirt] update values.yaml to use train libvirt image

this means the chart works 'as-is' (with the rest of the currently
released components) in most cases without the explicit need for an
image override.

Change-Id: Id11079b5ce3a8d1010e604300f457e4060aee582
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/04/712004/3 && git format-patch -1 --stdout FETCH_HEAD,['libvirt/values.yaml'],1,df97dda583c4fc91eb628edf45156051f8c09277,libvirt-default, libvirt: docker.io/openstackhelm/libvirt:latest-ubuntu_bionic, libvirt: docker.io/openstackhelm/libvirt:ubuntu_xenial-20190903,1,1
openstack%2Fironic~master~If2496e9c143400dcce017a79cf179768951eb137,openstack/ironic,master,If2496e9c143400dcce017a79cf179768951eb137,Support centos 7 rootwrap data directory,MERGED,2020-03-19 00:11:26.000000000,2020-03-19 23:49:33.000000000,2020-03-19 23:47:50.000000000,"[{'_account_id': 6994}, {'_account_id': 10239}, {'_account_id': 11655}, {'_account_id': 14629}, {'_account_id': 19339}, {'_account_id': 22348}, {'_account_id': 23851}]","[{'number': 1, 'created': '2020-03-19 00:11:26.000000000', 'files': ['devstack/lib/ironic'], 'web_link': 'https://opendev.org/openstack/ironic/commit/bee537a4a362c23958d6e9e7b05f82e563b6b54a', 'message': 'Support centos 7 rootwrap data directory\n\nCentos 7 pip seems to install the rootwrap filters to /usr/local\ndespite its data path resolving to /usr.\n\nChange-Id: If2496e9c143400dcce017a79cf179768951eb137\n'}]",0,713765,bee537a4a362c23958d6e9e7b05f82e563b6b54a,14,7,1,6994,,,0,"Support centos 7 rootwrap data directory

Centos 7 pip seems to install the rootwrap filters to /usr/local
despite its data path resolving to /usr.

Change-Id: If2496e9c143400dcce017a79cf179768951eb137
",git fetch https://review.opendev.org/openstack/ironic refs/changes/65/713765/1 && git format-patch -1 --stdout FETCH_HEAD,['devstack/lib/ironic'],1,bee537a4a362c23958d6e9e7b05f82e563b6b54a,devstack_c7_rootwrap, # on Centos7 the data is installed to /usr/local if [ ! -d $ironic_lib_prefix/etc/ironic/rootwrap.d ]; then ironic_lib_prefix=/usr/local fi,,5,0
openstack%2Fneutron~master~I9fb0f05ede42afa1a349635b1936028edf540a1f,openstack/neutron,master,I9fb0f05ede42afa1a349635b1936028edf540a1f,Deny delete last slaac subnet with allocation on segment,MERGED,2020-02-29 04:27:56.000000000,2020-03-19 23:25:57.000000000,2020-03-19 23:20:00.000000000,"[{'_account_id': 1131}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 24245}, {'_account_id': 26622}]","[{'number': 1, 'created': '2020-02-29 04:27:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/4bf737888652e2104a9df97108c37d4535acc3c3', 'message': ""Don't allow delete only auto-alloc subnet of port\n\nWhen a port has only one IP allocation on auto-allocation\nsubnet, do not allow the delete. Raise SubnetInUse exception.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1864333\nCloses-Bug: #1865138\nChange-Id: I9fb0f05ede42afa1a349635b1936028edf540a1f\n""}, {'number': 2, 'created': '2020-02-29 18:06:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8be393cfceff5390c5c65cdee0421fb59360c53d', 'message': ""Don't allow delete only auto-alloc subnet of port\n\nWhen a port has only one IP allocation on auto-allocation\nsubnet, do not allow the delete. Raise SubnetInUse exception.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1864333\nCloses-Bug: #1865138\nChange-Id: I9fb0f05ede42afa1a349635b1936028edf540a1f\n""}, {'number': 3, 'created': '2020-03-01 00:54:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ae1e5cbcc043a6e7eafb0ee1e5561b86e1038f44', 'message': 'Deny delete last slaac subnet with allocation on segment\n\nWhen a port has only one IP allocation on auto-allocation\nsubnet which is associated with a segment, do not allow\nthe delete of the subnet. Raise SubnetInUse exception instead.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1864333\nCloses-Bug: #1865138\nChange-Id: I9fb0f05ede42afa1a349635b1936028edf540a1f\n'}, {'number': 4, 'created': '2020-03-01 12:10:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/694eb26e4e42d81861d4c633ca5ef832fd98b199', 'message': 'Deny delete last slaac subnet with allocation on segment\n\nWhen a port has only one IP allocation on auto-allocation\nsubnet which is associated with a segment, do not allow\nthe delete of the subnet. Raise SubnetInUse exception instead.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1864333\nCloses-Bug: #1865138\nChange-Id: I9fb0f05ede42afa1a349635b1936028edf540a1f\n'}, {'number': 5, 'created': '2020-03-04 10:04:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/da6777d882f4fd34b04c8f3d34573d7259202615', 'message': 'Deny delete last slaac subnet with allocation on segment\n\nWhen a port has only one IP allocation on auto-allocation\nsubnet which is associated with a segment, do not allow\nthe delete of the subnet. Raise SubnetInUse exception instead.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1864333\nCloses-Bug: #1865138\nChange-Id: I9fb0f05ede42afa1a349635b1936028edf540a1f\n'}, {'number': 6, 'created': '2020-03-05 11:45:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8f96de06d49c91fb623c75053d3c1862ee53b5ba', 'message': 'Deny delete last slaac subnet with allocation on segment\n\nWhen a port has only one IP allocation on auto-allocation\nsubnet which is associated with a segment, do not allow\nthe delete of the subnet. Raise SubnetInUse exception instead.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1864333\nCloses-Bug: #1865138\nChange-Id: I9fb0f05ede42afa1a349635b1936028edf540a1f\n'}, {'number': 7, 'created': '2020-03-12 10:09:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ccf5e351fb1ac199db7256b04912d4266521d53b', 'message': 'Deny delete last slaac subnet with allocation on segment\n\nWhen a port has only one IP allocation on auto-allocation\nsubnet which is associated with a segment, do not allow\nthe delete of the subnet. Raise SubnetInUse exception instead.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1864333\nCloses-Bug: #1865138\nChange-Id: I9fb0f05ede42afa1a349635b1936028edf540a1f\n'}, {'number': 8, 'created': '2020-03-12 10:22:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ecd7e1eca1b0571b0729a4d4e908211bc58f97e0', 'message': 'Deny delete last slaac subnet with allocation on segment\n\nWhen a port has only one IP allocation on auto-allocation\nsubnet which is associated with a segment, do not allow\nthe delete of the subnet. Raise SubnetInUse exception instead.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1864333\nCloses-Bug: #1865138\nChange-Id: I9fb0f05ede42afa1a349635b1936028edf540a1f\n'}, {'number': 9, 'created': '2020-03-17 22:03:57.000000000', 'files': ['neutron/services/segments/plugin.py', 'neutron/db/db_base_plugin_v2.py', 'neutron/tests/unit/extensions/test_segment.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/f987486febb9fbe88232bb9139508981b92147f1', 'message': 'Deny delete last slaac subnet with allocation on segment\n\nWhen a port has only one IP allocation on auto-allocation\nsubnet which is associated with a segment, do not allow\nthe delete of the subnet. Raise SubnetInUse exception instead.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1864333\nCloses-Bug: #1865138\nChange-Id: I9fb0f05ede42afa1a349635b1936028edf540a1f\n'}]",17,710547,f987486febb9fbe88232bb9139508981b92147f1,62,7,9,24245,,,0,"Deny delete last slaac subnet with allocation on segment

When a port has only one IP allocation on auto-allocation
subnet which is associated with a segment, do not allow
the delete of the subnet. Raise SubnetInUse exception instead.

Related: rhbz#1803989
Related-Bug: #1864225
Related-Bug: #1864333
Closes-Bug: #1865138
Change-Id: I9fb0f05ede42afa1a349635b1936028edf540a1f
",git fetch https://review.opendev.org/openstack/neutron refs/changes/47/710547/8 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/plugins/ml2/test_plugin.py', 'neutron/tests/unit/db/test_db_base_plugin_v2.py', 'neutron/db/db_base_plugin_v2.py', 'neutron/tests/unit/extensions/test_segment.py']",4,4bf737888652e2104a9df97108c37d4535acc3c3,bug/1864225," def test_slaac_segment_aware_delete_last_subnet_on_segment_fails(self): (network, segment_a, segment_b, subnet_a0, subnet_a1, subnet_b0, subnet_b1) = self._create_net_two_segments_four_slaac_subnets() # Create a two ports, port_a with subnet_a0 in fixed_ips and port_b # with subnet_b0 in fixed_ips port_a = self._create_port_and_show( network, fixed_ips=[{'subnet_id': subnet_a0['subnet']['id']}]) port_b = self._create_port_and_show( network, fixed_ips=[{'subnet_id': subnet_b0['subnet']['id']}]) self._validate_immediate_ip_allocation(port_a['port']['id']) self._validate_immediate_ip_allocation(port_b['port']['id']) self.assertEqual(2, len(port_a['port']['fixed_ips'])) self.assertEqual(2, len(port_b['port']['fixed_ips'])) # Delete subnet_b1 on segment_b req = self.new_delete_request('subnets', subnet_b1['subnet']['id']) res = req.get_response(self.api) self.assertEqual(webob.exc.HTTPNoContent.code, res.status_int) # Delete subnet_b0 on segment_b fails because port_b has no other # allocation, SubnetInUse req = self.new_delete_request('subnets', subnet_b0['subnet']['id']) res = req.get_response(self.api) self.assertEqual(webob.exc.HTTPConflict.code, res.status_int) # Delete port_b req = self.new_delete_request('ports', port_b['port']['id']) res = req.get_response(self.api) self.assertEqual(webob.exc.HTTPNoContent.code, res.status_int) # Try to delete subnet_b0 again, should not fail with no ports req = self.new_delete_request('subnets', subnet_b0['subnet']['id']) res = req.get_response(self.api) self.assertEqual(webob.exc.HTTPNoContent.code, res.status_int) ",,86,17
openstack%2Fneutron~master~I75ae14c64db076434ca9897ba9a6d97702e233ad,openstack/neutron,master,I75ae14c64db076434ca9897ba9a6d97702e233ad,subnet create - segment aware auto-addr allocation,MERGED,2020-02-29 04:27:56.000000000,2020-03-19 23:23:47.000000000,2020-03-19 23:19:57.000000000,"[{'_account_id': 1131}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 24245}, {'_account_id': 26622}]","[{'number': 1, 'created': '2020-02-29 04:27:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8065551601defe5e4dfe4c31de4f4df1b249b722', 'message': ""subnet create - segment aware auto-addr allocation\n\nWhen creating additional subnets with ipv6 auto-addressing\nip allocation was added to existing ports without filtering\non current allocation's segment.\n\nThis adds fitering to only add auto-address allocation when\nthe new subnet is on the same segment as the ports current\nipam allocations.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1865138\nCloses-Bug: #1864333\nChange-Id: I75ae14c64db076434ca9897ba9a6d97702e233ad\n""}, {'number': 2, 'created': '2020-02-29 18:06:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/25443b26dfd617cfc1de31a4b52fa75192fe71f6', 'message': ""subnet create - segment aware auto-addr allocation\n\nWhen creating additional subnets with ipv6 auto-addressing\nip allocation was added to existing ports without filtering\non current allocation's segment.\n\nThis adds fitering to only add auto-address allocation when\nthe new subnet is on the same segment as the ports current\nipam allocations.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1865138\nCloses-Bug: #1864333\nChange-Id: I75ae14c64db076434ca9897ba9a6d97702e233ad\n""}, {'number': 3, 'created': '2020-03-01 12:10:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/54fd425f2fa67081fa4e74f57e8120b6daa1a157', 'message': ""subnet create - segment aware auto-addr allocation\n\nWhen creating additional subnets with ipv6 auto-addressing\nip allocation was added to existing ports without filtering\non current allocation's segment.\n\nThis adds fitering to only add auto-address allocation when\nthe new subnet is on the same segment as the ports current\nipam allocations.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1865138\nCloses-Bug: #1864333\nChange-Id: I75ae14c64db076434ca9897ba9a6d97702e233ad\n""}, {'number': 4, 'created': '2020-03-04 10:04:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c494642ff5da0c13ad207f48a8a038de82ef659f', 'message': ""subnet create - segment aware auto-addr allocation\n\nWhen creating additional subnets with ipv6 auto-addressing\nip allocation was added to existing ports without filtering\non current allocation's segment.\n\nThis adds fitering to only add auto-address allocation when\nthe new subnet is on the same segment as the ports current\nipam allocations.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1865138\nCloses-Bug: #1864333\nChange-Id: I75ae14c64db076434ca9897ba9a6d97702e233ad\n""}, {'number': 5, 'created': '2020-03-05 11:45:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/88ac43a97b616a4ac6253656c767b69885a063fb', 'message': ""subnet create - segment aware auto-addr allocation\n\nWhen creating additional subnets with ipv6 auto-addressing\nip allocation was added to existing ports without filtering\non current allocation's segment.\n\nThis adds fitering to only add auto-address allocation when\nthe new subnet is on the same segment as the ports current\nipam allocations.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1865138\nCloses-Bug: #1864333\nChange-Id: I75ae14c64db076434ca9897ba9a6d97702e233ad\n""}, {'number': 6, 'created': '2020-03-11 14:24:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/135df60e65a28008a827b422905530d81dc31018', 'message': ""subnet create - segment aware auto-addr allocation\n\nWhen creating additional subnets with ipv6 auto-addressing\nip allocation was added to existing ports without filtering\non current allocation's segment.\n\nThis adds fitering to only add auto-address allocation when\nthe new subnet is on the same segment as the ports current\nipam allocations.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1865138\nCloses-Bug: #1864333\nChange-Id: I75ae14c64db076434ca9897ba9a6d97702e233ad\n""}, {'number': 7, 'created': '2020-03-11 14:25:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/b8348c5d92e1982108848c2c3b9646eff263b74f', 'message': ""subnet create - segment aware auto-addr allocation\n\nWhen creating additional subnets with ipv6 auto-addressing\nip allocation was added to existing ports without filtering\non current allocation's segment.\n\nThis adds fitering to only add auto-address allocation when\nthe new subnet is on the same segment as the ports current\nipam allocations.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1865138\nCloses-Bug: #1864333\nChange-Id: I75ae14c64db076434ca9897ba9a6d97702e233ad\n""}, {'number': 8, 'created': '2020-03-12 10:22:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/fb710c4669af83269907a60a9216ee0f70c3cf8d', 'message': ""subnet create - segment aware auto-addr allocation\n\nWhen creating additional subnets with ipv6 auto-addressing\nip allocation was added to existing ports without filtering\non current allocation's segment.\n\nThis adds fitering to only add auto-address allocation when\nthe new subnet is on the same segment as the ports current\nipam allocations.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1865138\nCloses-Bug: #1864333\nChange-Id: I75ae14c64db076434ca9897ba9a6d97702e233ad\n""}, {'number': 9, 'created': '2020-03-17 20:51:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/f1a12f57e8aa9f7b2f4253c9daa4d9269a9f6064', 'message': ""subnet create - segment aware auto-addr allocation\n\nWhen creating additional subnets with ipv6 auto-addressing\nip allocation was added to existing ports without filtering\non current allocation's segment.\n\nThis adds fitering to only add auto-address allocation when\nthe new subnet is on the same segment as the ports current\nipam allocations.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1865138\nCloses-Bug: #1864333\nChange-Id: I75ae14c64db076434ca9897ba9a6d97702e233ad\n""}, {'number': 10, 'created': '2020-03-17 22:03:57.000000000', 'files': ['neutron/db/ipam_pluggable_backend.py', 'neutron/tests/unit/extensions/test_segment.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/3d3dc60408148cf16bc19cccb76b8652f980fa1c', 'message': ""subnet create - segment aware auto-addr allocation\n\nWhen creating additional subnets with ipv6 auto-addressing\nip allocation was added to existing ports without filtering\non current allocation's segment.\n\nThis adds fitering to only add auto-address allocation when\nthe new subnet is on the same segment as the ports current\nipam allocations.\n\nRelated: rhbz#1803989\nRelated-Bug: #1864225\nRelated-Bug: #1865138\nCloses-Bug: #1864333\nChange-Id: I75ae14c64db076434ca9897ba9a6d97702e233ad\n""}]",4,710546,3d3dc60408148cf16bc19cccb76b8652f980fa1c,75,7,10,24245,,,0,"subnet create - segment aware auto-addr allocation

When creating additional subnets with ipv6 auto-addressing
ip allocation was added to existing ports without filtering
on current allocation's segment.

This adds fitering to only add auto-address allocation when
the new subnet is on the same segment as the ports current
ipam allocations.

Related: rhbz#1803989
Related-Bug: #1864225
Related-Bug: #1865138
Closes-Bug: #1864333
Change-Id: I75ae14c64db076434ca9897ba9a6d97702e233ad
",git fetch https://review.opendev.org/openstack/neutron refs/changes/46/710546/10 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/db/ipam_pluggable_backend.py', 'neutron/tests/unit/extensions/test_segment.py']",2,8065551601defe5e4dfe4c31de4f4df1b249b722,bug/1864225," def test_slaac_segment_aware_add_subnet(self): (network, segment_a, segment_b, subnet_a0, subnet_a1, subnet_b0, subnet_b1) = self._create_net_two_segments_four_slaac_subnets() # Create a port with no IP address (since there is no subnet) port_deferred = self._create_deferred_ip_port(network) self._validate_deferred_ip_allocation(port_deferred['port']['id']) # Create a two ports, port_a with subnet_a0 in fixed_ips and port_b # with subnet_b0 in fixed_ips port_a = self._create_port_and_show( network, fixed_ips=[{'subnet_id': subnet_a0['subnet']['id']}]) port_b = self._create_port_and_show( network, fixed_ips=[{'subnet_id': subnet_b0['subnet']['id']}]) self._validate_immediate_ip_allocation(port_a['port']['id']) self._validate_immediate_ip_allocation(port_b['port']['id']) self.assertEqual(2, len(port_a['port']['fixed_ips'])) self.assertEqual(2, len(port_b['port']['fixed_ips'])) # Add another subnet on segment_a subnet_a2 = self._create_test_slaac_subnet_with_segment( network, segment_a, '2001:db8:a:2::/64') # The port with deferred allocation should not have an allocation req = self.new_show_request('ports', port_deferred['port']['id']) res = req.get_response(self.api) port_deferred = self.deserialize(self.fmt, res) self._validate_deferred_ip_allocation(port_deferred['port']['id']) self.assertEqual(0, len(port_deferred['port']['fixed_ips'])) # port_a should get an allocation on the new subnet. # port_b does not get an allocation. req = self.new_show_request('ports', port_a['port']['id']) res = req.get_response(self.api) port_a = self.deserialize(self.fmt, res) req = self.new_show_request('ports', port_b['port']['id']) res = req.get_response(self.api) port_b = self.deserialize(self.fmt, res) self.assertEqual(3, len(port_a['port']['fixed_ips'])) self.assertEqual(2, len(port_b['port']['fixed_ips'])) port_a_snet_ids = [f['subnet_id'] for f in port_a['port']['fixed_ips']] self.assertIn(subnet_a2['subnet']['id'], port_a_snet_ids) def test_slaac_segment_aware_delete_subnet(self): (network, segment_a, segment_b, subnet_a0, subnet_a1, subnet_b0, subnet_b1) = self._create_net_two_segments_four_slaac_subnets() # Create a two ports, port_a with subnet_a0 in fixed_ips and port_b # with subnet_b0 in fixed_ips port_a = self._create_port_and_show( network, fixed_ips=[{'subnet_id': subnet_a0['subnet']['id']}]) port_b = self._create_port_and_show( network, fixed_ips=[{'subnet_id': subnet_b0['subnet']['id']}]) self._validate_immediate_ip_allocation(port_a['port']['id']) self._validate_immediate_ip_allocation(port_b['port']['id']) self.assertEqual(2, len(port_a['port']['fixed_ips'])) self.assertEqual(2, len(port_b['port']['fixed_ips'])) # Delete subnet_b1 on segment_b, port_a should keep it's allocations # on the new subnet. Allocation for deleted subnet removed on port_b. req = self.new_delete_request('subnets', subnet_b1['subnet']['id']) res = req.get_response(self.api) self.assertEqual(webob.exc.HTTPNoContent.code, res.status_int) req = self.new_show_request('ports', port_a['port']['id']) res = req.get_response(self.api) port_a = self.deserialize(self.fmt, res) req = self.new_show_request('ports', port_b['port']['id']) res = req.get_response(self.api) port_b = self.deserialize(self.fmt, res) self.assertEqual(2, len(port_a['port']['fixed_ips'])) self.assertEqual(1, len(port_b['port']['fixed_ips'])) port_b_snet_ids = [f['subnet_id'] for f in port_b['port']['fixed_ips']] self.assertNotIn(subnet_b1['subnet']['id'], port_b_snet_ids) ",,88,0
openstack%2Fopenstack-helm~master~I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07,openstack/openstack-helm,master,I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07,Heat: Add ingress network policy overrides,MERGED,2019-09-30 14:25:56.000000000,2020-03-19 23:07:06.000000000,2020-03-19 23:05:11.000000000,"[{'_account_id': 8898}, {'_account_id': 11934}, {'_account_id': 17591}, {'_account_id': 18236}, {'_account_id': 18250}, {'_account_id': 20466}, {'_account_id': 21420}, {'_account_id': 22348}, {'_account_id': 22636}, {'_account_id': 23140}, {'_account_id': 24780}, {'_account_id': 29397}, {'_account_id': 30746}, {'_account_id': 31479}]","[{'number': 1, 'created': '2019-09-30 14:25:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/db49f72e4c579719ba8f8f5cafc36803bc44925f', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 2, 'created': '2019-09-30 15:06:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/5a4cf39a160c80a5608422942571d25596f1a8c8', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 3, 'created': '2019-09-30 15:20:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/fae08e531cd89ce00a4bb1126052d5afc1bf369f', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 4, 'created': '2019-09-30 15:37:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/1c0b622b3f7c94fc9cb6735d2a03eac092833805', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 5, 'created': '2019-09-30 17:53:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/109b611c27f612956c2acfb0f2c6ce5484fc7eae', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 6, 'created': '2019-09-30 19:08:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/3ed19c4cb80e2110599afb10307b0b643773ae04', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 7, 'created': '2019-10-01 12:45:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/3c0b3ae3ecde0cca23b894a2073889d54c0523ea', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 8, 'created': '2019-10-01 15:55:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/0704d1218015d5408f4b3f7b1404b3b10dc18918', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 9, 'created': '2019-10-01 18:31:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/b9bd8ed27cfb2d4f3781f99ee4215cd3e62cc9e3', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 10, 'created': '2019-10-01 19:46:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/2780895563d4aade58a05c7118b1d559dd9aaf52', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 11, 'created': '2019-10-01 20:29:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/5779b1af985320bfc29e8fa981d7a24458b0f5d6', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 12, 'created': '2019-10-01 20:29:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/4b17fb8b3b072a1c4036af7161992787bedb6eed', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 13, 'created': '2019-10-25 20:14:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/090d90667376cd37c469e506081c9c209ea6bab9', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 14, 'created': '2019-10-28 12:14:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/8f1ec1c70ed33e58477f4557c5b92cb4a63a2403', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 15, 'created': '2020-02-26 15:44:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/15ee3aeaa587d35743e85533691ab119d88550e1', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 16, 'created': '2020-03-17 16:31:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/67157ec86f41bc9c41a709b75bf4bcae75bb139e', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 17, 'created': '2020-03-18 15:41:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/c4f1fb4ec266b7a62792cfe1b0c55f950e5acf4b', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 18, 'created': '2020-03-18 15:42:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/0585e6c6af9da6f6767e702a67f4e368df84d86e', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}, {'number': 19, 'created': '2020-03-19 09:08:14.000000000', 'files': ['heat/values_overrides/netpol.yaml', 'tools/deployment/common/test-networkpolicy.sh'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/f76a3f5e0a6f62ad409cca7ef703b7bdeec94da6', 'message': 'Heat: Add ingress network policy overrides\n\nThis patch set adds in default heat ingress overrides.\n\nChange-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07\n'}]",11,685723,f76a3f5e0a6f62ad409cca7ef703b7bdeec94da6,79,14,19,24780,,,0,"Heat: Add ingress network policy overrides

This patch set adds in default heat ingress overrides.

Change-Id: I4e1f2b6687a05f2bf3ca91c941c2cf11abe68f07
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/23/685723/19 && git format-patch -1 --stdout FETCH_HEAD,"['heat/values.yaml', 'tools/deployment/common/test-networkpolicy.sh']",2,db49f72e4c579719ba8f8f5cafc36803bc44925f,netpol, # Heat Negative Tests test_netpol openstack cinder api heat.openstack.svc.cluster.local:8004 fail test_netpol openstack keystone api heat.openstack.svc.cluster.local:8004 fail test_netpol openstack nova os-api heat.openstack.svc.cluster.local:8004 fail test_netpol openstack neutron server heat.openstack.svc.cluster.local:8004 fail test_netpol openstack glance api heat.openstack.svc.cluster.local:8004 fail test_netpol openstack horizon server heat.openstack.svc.cluster.local:8004 success test_netpol openstack heat api heat.openstack.svc.cluster.local:8004 success,,27,1
openstack%2Ftrove~master~I41bf34cd482dcf880cd66c8bd123898f77c10056,openstack/trove,master,I41bf34cd482dcf880cd66c8bd123898f77c10056,Improve the doc,MERGED,2020-03-19 10:01:13.000000000,2020-03-19 23:00:17.000000000,2020-03-19 22:58:49.000000000,"[{'_account_id': 6732}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 10:01:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/682c2d1040f2efd9ddb66b783a60a522cd2dd495', 'message': 'Improve the doc\n\nChange-Id: I41bf34cd482dcf880cd66c8bd123898f77c10056\n'}, {'number': 2, 'created': '2020-03-19 20:50:13.000000000', 'files': ['doc/source/install/install-ubuntu.rst', 'doc/source/install/index.rst', 'doc/source/install/install-suse.rst', 'doc/source/install/verify.rst', 'doc/source/install/install-redhat.rst', 'doc/source/install/get_started.rst', 'doc/source/install/install-manual.rst', 'doc/source/install/common_configure.txt'], 'web_link': 'https://opendev.org/openstack/trove/commit/dc0bfe1d39e0b05c94c3bad394b9dc415a75ad22', 'message': 'Improve the doc\n\nChange-Id: I41bf34cd482dcf880cd66c8bd123898f77c10056\n'}]",0,713814,dc0bfe1d39e0b05c94c3bad394b9dc415a75ad22,9,2,2,6732,,,0,"Improve the doc

Change-Id: I41bf34cd482dcf880cd66c8bd123898f77c10056
",git fetch https://review.opendev.org/openstack/trove refs/changes/14/713814/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/install/install-ubuntu.rst', 'doc/source/install/index.rst', 'doc/source/install/verify.rst', 'doc/source/install/get_started.rst', 'doc/source/install/common_configure.txt', 'doc/source/install/install-manual.rst']",6,682c2d1040f2efd9ddb66b783a60a522cd2dd495,doc,"- api-paste.ini — For trove-api service - trove.conf - For trove-api, trove-taskmanagerr, trove-conductor services.- ``<datastore_manager>.cloudinit`` — Userdata for trove instance during provisioning#. Service tenant credentials, change the values according to your own [service_credentials] auth_url = <Keystone service URL> username = admin password = password user_domain_name = default project_name = admin project_domain_name = default region_name = RegionOne`Build guest agent image <https://docs.openstack.org/trove/latest/admin/building_guest_images.html>`_","- api-paste.ini and trove.conf — For trove-api service- ``<datastore_manager>.cloudinit`` — Userdata for VMs during provisioning#. Config service tenant model, change the values according to your own nova_proxy_admin_user = admin nova_proxy_admin_pass = password nova_proxy_admin_tenant_name = admin nova_proxy_admin_tenant_id = f472127c03f6410899225e26a3c1d22c nova_proxy_admin_user_domain_name = default nova_proxy_admin_project_domain_name = default remote_nova_client = trove.common.clients_admin.nova_client_trove_admin remote_cinder_client = trove.common.clients_admin.cinder_client_trove_admin remote_neutron_client = trove.common.clients_admin.neutron_client_trove_admin os_region_name = RegionOne`Build guest agent image <https://docs.openstack.org/trove/latest/admin/trovestack.html#build-guest-agent-image>`_",116,206
openstack%2Ftripleo-heat-templates~stable%2Ftrain~Ib957da14fd47953d7419438236888efc41034e1a,openstack/tripleo-heat-templates,stable/train,Ib957da14fd47953d7419438236888efc41034e1a,roles: Update description of ComputeRealTime role,MERGED,2020-01-22 13:39:13.000000000,2020-03-19 22:46:17.000000000,2020-03-19 22:46:17.000000000,"[{'_account_id': 3153}, {'_account_id': 14985}, {'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 23181}, {'_account_id': 24162}]","[{'number': 1, 'created': '2020-01-22 13:39:13.000000000', 'files': ['roles/ComputeRealTime.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/fb6cbd62583fea25643075e0fec83c15a10857eb', 'message': ""roles: Update description of ComputeRealTime role\n\nChange I52c52b62f1c21214b98c98773c8647609cb81d52 removed use of the\n'NovaVcpuPinSet' from this role but did not remove references from the\ndescription of same. Fix this now.\n\nChange-Id: Ib957da14fd47953d7419438236888efc41034e1a\nSigned-off-by: Stephen Finucane <stephenfin@redhat.com>\nRelated-bug: #1860009\n""}]",0,703793,fb6cbd62583fea25643075e0fec83c15a10857eb,23,6,1,15334,,,0,"roles: Update description of ComputeRealTime role

Change I52c52b62f1c21214b98c98773c8647609cb81d52 removed use of the
'NovaVcpuPinSet' from this role but did not remove references from the
description of same. Fix this now.

Change-Id: Ib957da14fd47953d7419438236888efc41034e1a
Signed-off-by: Stephen Finucane <stephenfin@redhat.com>
Related-bug: #1860009
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/93/703793/1 && git format-patch -1 --stdout FETCH_HEAD,['roles/ComputeRealTime.yaml'],1,fb6cbd62583fea25643075e0fec83c15a10857eb,bug/1860009,"# Role: ComputeRealTime # Compute role that is optimized for real-time behaviour. When using this role it is mandatory that an overcloud-realtime-compute image is available and the role specific parameters IsolCpusList, NovaComputeCpuDedicatedSet, and NovaComputeCpuSharedSet are set according to the hardware of the real-time compute nodes.",# Role: ComputeRealTime # Compute role that is optimized for real-time behaviour. When using this role it is mandatory that an overcloud-realtime-compute image is available and the role specific parameters IsolCpusList and NovaVcpuPinSet are set accordingly to the hardware of the real-time compute nodes.,6,5
openstack%2Ftripleo-heat-templates~stable%2Ftrain~I0240e74f1da0219c8c029db191b3490062ca5722,openstack/tripleo-heat-templates,stable/train,I0240e74f1da0219c8c029db191b3490062ca5722,[TRAIN and before] ComputePPC64LE: disable ContainerCpusetCpus by default,ABANDONED,2020-03-19 15:43:41.000000000,2020-03-19 22:38:56.000000000,,"[{'_account_id': 22348}, {'_account_id': 23181}]","[{'number': 1, 'created': '2020-03-19 15:43:41.000000000', 'files': ['roles/ComputePPC64LE.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/14373de65f35d388b56a3046c1403ff14fed1a66', 'message': ""[TRAIN and before] ComputePPC64LE: disable ContainerCpusetCpus by default\n\nWhen deploying the ComputePPC64LE role, make sure ContainerCpusetCpus is\ndisabled by default in order to let Docker figuring out what Cpus the\ncontainer can use.\n\nIn Paunch, we'll rework the get_all_cpus() on PPC to return correct\ndata; which seems to be broken on that platform right now.\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1813091\n\nNote: this patch doesn't apply on master; only Train and will be\nbackported to Stein and Queens.\n\nChange-Id: I0240e74f1da0219c8c029db191b3490062ca5722\n""}]",0,713908,14373de65f35d388b56a3046c1403ff14fed1a66,4,2,1,3153,,,0,"[TRAIN and before] ComputePPC64LE: disable ContainerCpusetCpus by default

When deploying the ComputePPC64LE role, make sure ContainerCpusetCpus is
disabled by default in order to let Docker figuring out what Cpus the
container can use.

In Paunch, we'll rework the get_all_cpus() on PPC to return correct
data; which seems to be broken on that platform right now.

https://bugzilla.redhat.com/show_bug.cgi?id=1813091

Note: this patch doesn't apply on master; only Train and will be
backported to Stein and Queens.

Change-Id: I0240e74f1da0219c8c029db191b3490062ca5722
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/08/713908/1 && git format-patch -1 --stdout FETCH_HEAD,['roles/ComputePPC64LE.yaml'],1,14373de65f35d388b56a3046c1403ff14fed1a66,train/ppc," # https://bugzilla.redhat.com/show_bug.cgi?id=1813091 # Until we find out a proper way to get a single list of all CPUs # on PPC hardware, let's not use the ""all"" default to avoid issues from BZ. ContainerCpusetCpus: """"",,4,0
openstack%2Fopenstack-helm~master~I7392ae36f116c52eb4e1929721ffa19e61cf8d94,openstack/openstack-helm,master,I7392ae36f116c52eb4e1929721ffa19e61cf8d94,Move common nova Train overrides from distro overrides,MERGED,2020-03-19 00:11:25.000000000,2020-03-19 22:26:33.000000000,2020-03-19 22:24:48.000000000,"[{'_account_id': 8863}, {'_account_id': 8898}, {'_account_id': 20466}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 00:11:25.000000000', 'files': ['nova/values_overrides/train.yaml', 'nova/values_overrides/train-ubuntu_bionic.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/5964ca18a50ff1dca099332a6e5dca5ef78230f3', 'message': 'Move common nova Train overrides from distro overrides\n\nChange-Id: I7392ae36f116c52eb4e1929721ffa19e61cf8d94\n'}]",0,713764,5964ca18a50ff1dca099332a6e5dca5ef78230f3,16,4,1,8863,,,0,"Move common nova Train overrides from distro overrides

Change-Id: I7392ae36f116c52eb4e1929721ffa19e61cf8d94
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/64/713764/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/values_overrides/train.yaml', 'nova/values_overrides/train-ubuntu_bionic.yaml']",2,5964ca18a50ff1dca099332a6e5dca5ef78230f3,,,manifests: # NOTE(lamt): The nova-consoleauth service has been removed. It was # deprecated since the 18.0.0 (Rocky) release. # See: https://docs.openstack.org/releasenotes/nova/train.html deployment_consoleauth: false # NOTE(lamt): Placement code was extracted from nova post Stein. # A placement chart will need to be deployed to replace. # See: https://docs.openstack.org/releasenotes/nova/train.html deployment_placement: false,9,9
openstack%2Freleases~master~I486774fd2b4d1bf41a45ded69829ba4fde233582,openstack/releases,master,I486774fd2b4d1bf41a45ded69829ba4fde233582,[tripleo] Transition Rocky to EM,MERGED,2020-02-26 01:01:59.000000000,2020-03-19 22:20:18.000000000,2020-03-19 22:20:18.000000000,"[{'_account_id': 308}, {'_account_id': 3153}, {'_account_id': 9592}, {'_account_id': 11904}, {'_account_id': 17068}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2020-02-26 01:01:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/8415eece227279348a8ad52ea484c13db030c3d8', 'message': '[tripleo] Transition Rocky to EM\n\nThis transition the rocky branch to extended maintenance.\nChanges for bugfixes and things the team deems important are\nstill encouraged, but there will no longer be official releases\noff of the branch.\n\nPlease +1 if the team is ready for us to proceed with this\ntransition, or -1 if there are any final backports currently in\nflight that we should wait for. For the latter case, please\nupdate the patch with the new commit hash after doing a final\nrelease to get those changes out so we know to proceed with the\ntransition.\n\nChange-Id: I486774fd2b4d1bf41a45ded69829ba4fde233582\n'}, {'number': 2, 'created': '2020-02-26 01:44:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/f2b6f628ddf965b7c2596b888c676b8f05dfd0ae', 'message': '[tripleo] Transition Rocky to EM\n\nThis transition the rocky branch to extended maintenance.\nChanges for bugfixes and things the team deems important are\nstill encouraged, but there will no longer be official releases\noff of the branch.\n\nPlease +1 if the team is ready for us to proceed with this\ntransition, or -1 if there are any final backports currently in\nflight that we should wait for. For the latter case, please\nupdate the patch with the new commit hash after doing a final\nrelease to get those changes out so we know to proceed with the\ntransition.\n\nChange-Id: I486774fd2b4d1bf41a45ded69829ba4fde233582\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}, {'number': 3, 'created': '2020-03-19 17:24:56.000000000', 'files': ['deliverables/rocky/os-net-config.yaml', 'deliverables/rocky/instack-undercloud.yaml', 'deliverables/rocky/instack.yaml', 'deliverables/rocky/python-tripleoclient.yaml', 'deliverables/rocky/tripleo-heat-templates.yaml', 'deliverables/rocky/tripleo-ipsec.yaml', 'deliverables/rocky/tripleo-ui.yaml', 'deliverables/rocky/tripleo-common.yaml', 'deliverables/rocky/tempest-tripleo-ui.yaml', 'deliverables/rocky/tripleo-validations.yaml', 'deliverables/rocky/tripleo-puppet-elements.yaml', 'deliverables/rocky/os-apply-config.yaml', 'deliverables/rocky/puppet-tripleo.yaml', 'deliverables/rocky/paunch.yaml', 'deliverables/rocky/os-collect-config.yaml', 'deliverables/rocky/os-refresh-config.yaml', 'deliverables/rocky/tripleo-image-elements.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/b0b5b672a5be86d59aef63b3d6c9e24932c9c9ad', 'message': '[tripleo] Transition Rocky to EM\n\nThis transition the rocky branch to extended maintenance.\nChanges for bugfixes and things the team deems important are\nstill encouraged, but there will no longer be official releases\noff of the branch.\n\nPlease +1 if the team is ready for us to proceed with this\ntransition, or -1 if there are any final backports currently in\nflight that we should wait for. For the latter case, please\nupdate the patch with the new commit hash after doing a final\nrelease to get those changes out so we know to proceed with the\ntransition.\n\nChange-Id: I486774fd2b4d1bf41a45ded69829ba4fde233582\nSigned-off-by: Sean McGinnis <sean.mcginnis@gmail.com>\n'}]",0,709912,b0b5b672a5be86d59aef63b3d6c9e24932c9c9ad,16,7,3,11904,,,0,"[tripleo] Transition Rocky to EM

This transition the rocky branch to extended maintenance.
Changes for bugfixes and things the team deems important are
still encouraged, but there will no longer be official releases
off of the branch.

Please +1 if the team is ready for us to proceed with this
transition, or -1 if there are any final backports currently in
flight that we should wait for. For the latter case, please
update the patch with the new commit hash after doing a final
release to get those changes out so we know to proceed with the
transition.

Change-Id: I486774fd2b4d1bf41a45ded69829ba4fde233582
Signed-off-by: Sean McGinnis <sean.mcginnis@gmail.com>
",git fetch https://review.opendev.org/openstack/releases refs/changes/12/709912/1 && git format-patch -1 --stdout FETCH_HEAD,"['deliverables/rocky/instack-undercloud.yaml', 'deliverables/rocky/instack.yaml']",2,8415eece227279348a8ad52ea484c13db030c3d8,rocky-em, - version: rocky-em projects: - repo: openstack/instack hash: 1dca54c838b759eab1ae7c49871e2bba85dae06c,,8,0
openstack%2Frequirements~master~I32003146da642825d2cd59c1ad51ea24cda73e61,openstack/requirements,master,I32003146da642825d2cd59c1ad51ea24cda73e61,update constraint for taskflow to new release 4.1.0,MERGED,2020-03-19 10:22:48.000000000,2020-03-19 22:12:48.000000000,2020-03-19 22:10:53.000000000,"[{'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2020-03-19 10:22:48.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/1497b32e080ed248378ece9148ce4998db6d2d10', 'message': 'update constraint for taskflow to new release 4.1.0\n\nChange-Id: I32003146da642825d2cd59c1ad51ea24cda73e61\nmeta:version: 4.1.0\nmeta:diff-start: -\nmeta:series: ussuri\nmeta:release-type: release\nmeta:pypi: yes\nmeta:first: no\nmeta:release:Author: Ben Nemec <bnemec@redhat.com>\nmeta:release:Commit: Ben Nemec <bnemec@redhat.com>\nmeta:release:Change-Id: Id2792199eb5e1843b2f2fb02b5574992e3ba17fa\nmeta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nmeta:release:Workflow+1: Thierry Carrez <thierry@openstack.org>\n'}]",0,713820,1497b32e080ed248378ece9148ce4998db6d2d10,10,3,1,11131,,,0,"update constraint for taskflow to new release 4.1.0

Change-Id: I32003146da642825d2cd59c1ad51ea24cda73e61
meta:version: 4.1.0
meta:diff-start: -
meta:series: ussuri
meta:release-type: release
meta:pypi: yes
meta:first: no
meta:release:Author: Ben Nemec <bnemec@redhat.com>
meta:release:Commit: Ben Nemec <bnemec@redhat.com>
meta:release:Change-Id: Id2792199eb5e1843b2f2fb02b5574992e3ba17fa
meta:release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
meta:release:Workflow+1: Thierry Carrez <thierry@openstack.org>
",git fetch https://review.opendev.org/openstack/requirements refs/changes/20/713820/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,1497b32e080ed248378ece9148ce4998db6d2d10,new-release,taskflow===4.1.0;python_version=='3.6' taskflow===4.1.0;python_version=='3.7',taskflow===4.0.0;python_version=='3.6' taskflow===4.0.0;python_version=='3.7',2,2
