id,project,branch,change_id,subject,status,created,updated,submitted,reviewers,revisions,total_comment_count,number,current_revision,discussion_messages_count,reviewers_count,revisions_count,owner_account_id,owner_name,owner_username,is_owner_bot,commit_message,git_command,changed_files,files_count,commit_id,topic,added_lines,deleted_lines,insertions,deletions
openstack%2Ffuel-web~master~I67f1794b806e3281a34aff7deda233c986937942,openstack/fuel-web,master,I67f1794b806e3281a34aff7deda233c986937942,Fix NodeCollectionNICsDefaultHandler and add tests,MERGED,2014-09-26 14:42:02.000000000,2014-09-29 13:31:27.000000000,2014-09-26 15:54:45.000000000,"[{'_account_id': 3}, {'_account_id': 8392}, {'_account_id': 8907}, {'_account_id': 8931}, {'_account_id': 8971}, {'_account_id': 10959}]","[{'number': 1, 'created': '2014-09-26 14:42:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/b0b14c5a0af7857e80a48833880d21386d3b169d', 'message': ""Fix NodeCollectionNICsDefaultHandler and add tests\n\nSince last refactoring we have broken NodeCollectionNICsDefaultHandler,\nso it's not working at all.\n\nChange-Id: I67f1794b806e3281a34aff7deda233c986937942\nCloses-Bug: #1374356\n""}, {'number': 2, 'created': '2014-09-26 14:50:04.000000000', 'files': ['nailgun/nailgun/api/v1/handlers/node.py', 'nailgun/nailgun/test/integration/test_node_nic_collection_handler.py'], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/af6247aef7540156a093582fd091c95a8501a7aa', 'message': ""Fix NodeCollectionNICsDefaultHandler and add tests\n\nSince last refactoring we have broken NodeCollectionNICsDefaultHandler,\nso it's not working at all.\n\nChange-Id: I67f1794b806e3281a34aff7deda233c986937942\nCloses-Bug: #1374356\n""}]",0,124421,af6247aef7540156a093582fd091c95a8501a7aa,16,6,2,10391,,,0,"Fix NodeCollectionNICsDefaultHandler and add tests

Since last refactoring we have broken NodeCollectionNICsDefaultHandler,
so it's not working at all.

Change-Id: I67f1794b806e3281a34aff7deda233c986937942
Closes-Bug: #1374356
",git fetch https://review.opendev.org/openstack/fuel-web refs/changes/21/124421/1 && git format-patch -1 --stdout FETCH_HEAD,"['nailgun/nailgun/api/v1/handlers/node.py', 'nailgun/nailgun/test/integration/test_node_nic_collection_handler.py']",2,b0b14c5a0af7857e80a48833880d21386d3b169d,bug/1374356,"class TestNodeCollectionNICsHandler(BaseIntegrationTest): class TestNodeCollectionNICsDefaultHandler(BaseIntegrationTest): def setUp(self): super(TestNodeCollectionNICsDefaultHandler, self).setUp() # two nodes in one cluster self.cluster = self.env.create( nodes_kwargs=[ {'roles': ['controller'], 'mac': '01:01:01:01:01:01'}, {'roles': ['compute'], 'mac': '02:02:02:02:02:02'}]) # one node in another cluster self.env.create( nodes_kwargs=[ {'roles': ['controller'], 'mac': '03:03:03:03:03:03'}]) def test_get_w_cluster_id(self): # get nics of cluster and check that response is ok resp = self.app.get( '{url}?cluster_id={cluster_id}'.format( url=reverse('NodeCollectionNICsDefaultHandler'), cluster_id=self.cluster['id']), headers=self.default_headers) self.assertEqual(resp.status_code, 200) # check response resp = jsonutils.loads(resp.body) self.assertEqual(len(resp), 2) macs = [iface['mac'] for node in resp for iface in node] self.assertTrue('01:01:01:01:01:01' in macs) self.assertTrue('02:02:02:02:02:02' in macs) self.assertFalse('03:03:03:03:03:03' in macs) def test_get_wo_cluster_id(self): # get nics of cluster and check that response is ok resp = self.app.get( reverse('NodeCollectionNICsDefaultHandler'), headers=self.default_headers) self.assertEqual(resp.status_code, 200) # check response resp = jsonutils.loads(resp.body) self.assertEqual(len(resp), 3) macs = [iface['mac'] for node in resp for iface in node] self.assertTrue('01:01:01:01:01:01' in macs) self.assertTrue('02:02:02:02:02:02' in macs) self.assertTrue('03:03:03:03:03:03' in macs)",class TestHandlers(BaseIntegrationTest):,60,16
openstack%2Fmurano-dashboard~master~I8c69e1863f229244cbe3b9dd5d0017f73c93638d,openstack/murano-dashboard,master,I8c69e1863f229244cbe3b9dd5d0017f73c93638d,Fix wrong usage of the coverage command,MERGED,2014-09-26 15:37:17.000000000,2014-09-29 13:26:56.000000000,2014-09-29 13:26:55.000000000,"[{'_account_id': 3}, {'_account_id': 7225}, {'_account_id': 7600}, {'_account_id': 7821}]","[{'number': 1, 'created': '2014-09-26 15:37:17.000000000', 'files': ['run_tests.sh'], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/48b524fc1b03438a9ebb5505f7193d823532a9c4', 'message': 'Fix wrong usage of the coverage command\n\nThere\'s no /usr/bin/coverage in Debian and Ubuntu. The package\nmaintainer of python-coverage decided that it was too generic, and\nthat the python-coverage shouldn\'t use just /usr/bin/coverage.\nSo in Debian and Ubuntu, we have to use python-coverage, or\n""python -m coverage"". However, ""python -m coverage"" doesn\'t work\nwith Python 2.6.\n\nThis patch takes all this into account, and offers fallbacks. If\n/usr/bin/coverage exists, we use that. Otherwise, we check for\n/usr/bin/python-coverage. Last, if none of these are available,\nthen we use ""python -m coverage"" as a fallback.\n\nNote that this is the solution which has been adopted by tuskar-ui,\nwhile Horizon uses now just ""python -m coverage"", which I think is\nless good than this patch (though, I correct Horizon before we did\nthe tuskar-ui). So, there\'s alread ya concensus that this problem\n(eg: the non-availability of /usr/bin/coverage) must be addressed,\nat least to allow distribution package maintainers (Debian in my\ncase) to run unit tests.\n\nChange-Id: I8c69e1863f229244cbe3b9dd5d0017f73c93638d\n'}]",0,124444,48b524fc1b03438a9ebb5505f7193d823532a9c4,9,4,1,6476,,,0,"Fix wrong usage of the coverage command

There's no /usr/bin/coverage in Debian and Ubuntu. The package
maintainer of python-coverage decided that it was too generic, and
that the python-coverage shouldn't use just /usr/bin/coverage.
So in Debian and Ubuntu, we have to use python-coverage, or
""python -m coverage"". However, ""python -m coverage"" doesn't work
with Python 2.6.

This patch takes all this into account, and offers fallbacks. If
/usr/bin/coverage exists, we use that. Otherwise, we check for
/usr/bin/python-coverage. Last, if none of these are available,
then we use ""python -m coverage"" as a fallback.

Note that this is the solution which has been adopted by tuskar-ui,
while Horizon uses now just ""python -m coverage"", which I think is
less good than this patch (though, I correct Horizon before we did
the tuskar-ui). So, there's alread ya concensus that this problem
(eg: the non-availability of /usr/bin/coverage) must be addressed,
at least to allow distribution package maintainers (Debian in my
case) to run unit tests.

Change-Id: I8c69e1863f229244cbe3b9dd5d0017f73c93638d
",git fetch https://review.opendev.org/openstack/murano-dashboard refs/changes/44/124444/1 && git format-patch -1 --stdout FETCH_HEAD,['run_tests.sh'],1,48b524fc1b03438a9ebb5505f7193d823532a9c4,,"# There's no /usr/bin/coverage in Debian and Ubuntu, # though using ""python -m coverage"" will fail in a # python 2.6 venv. Which is why we do what's below, # to make sure it works everywhere. COVERAGE_CMD=`which coverage || true` if [ -z ""${COVERAGE_CMD}"" ] ; then COVERAGE_CMD=`which python-coverage || true` fi if [ -z ""${COVERAGE_CMD}"" ] ; then COVERAGE_CMD=""python -m coverage"" fi ${command_wrapper} ${COVERAGE_CMD} erase ${command_wrapper} ${COVERAGE_CMD} run -p $root/manage.py test horizon --settings=horizon.test.settings $testopts ${command_wrapper} ${COVERAGE_CMD} run -p $root/manage.py test openstack_dashboard --settings=openstack_dashboard.test.settings $testopts ${command_wrapper} ${COVERAGE_CMD} combine ${command_wrapper} ${COVERAGE_CMD} xml -i --omit='/usr*,setup.py,*egg*,.venv/*' ${command_wrapper} ${COVERAGE_CMD} html -i --omit='/usr*,setup.py,*egg*,.venv/*' -d reports"," ${command_wrapper} coverage erase ${command_wrapper} coverage run -p $root/manage.py test horizon --settings=horizon.test.settings $testopts ${command_wrapper} coverage run -p $root/manage.py test openstack_dashboard --settings=openstack_dashboard.test.settings $testopts ${command_wrapper} coverage combine ${command_wrapper} coverage xml -i --omit='/usr*,setup.py,*egg*,.venv/*' ${command_wrapper} coverage html -i --omit='/usr*,setup.py,*egg*,.venv/*' -d reports",18,6
openstack%2Fmurano~master~Icde635015a426664c1f78f5602ae73a5cb71f76e,openstack/murano,master,Icde635015a426664c1f78f5602ae73a5cb71f76e,Runtime properties may no longer have default value,MERGED,2014-09-18 14:18:58.000000000,2014-09-29 13:26:50.000000000,2014-09-29 13:26:49.000000000,"[{'_account_id': 3}, {'_account_id': 7225}, {'_account_id': 7562}, {'_account_id': 7600}, {'_account_id': 7821}, {'_account_id': 10063}]","[{'number': 1, 'created': '2014-09-18 14:18:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano/commit/a8efdfbc1603c0ee645743abafa5c3a5f9ecfd43', 'message': 'Runtime properties may no longer have default value\n\nDefault value was ignored for Runtime properties.\nNow Runtime properties get initialized automatically if they have Default key\n(it may be of null value). If such property was not initialized neither by Default\nvalue nor in initializer an exception is thrown upon access to such property.\n\nChange-Id: Icde635015a426664c1f78f5602ae73a5cb71f76e\nCloses-Bug: #1364484\n'}, {'number': 2, 'created': '2014-09-23 12:35:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano/commit/e0abd12d1557811931b083851eb624dda63e41e2', 'message': 'Runtime properties may no longer have default value\n\nDefault value was ignored for Runtime properties.\nNow Runtime properties get initialized automatically if they have Default key\n(it may be of null value). If such property was not initialized neither by Default\nvalue nor in initializer an exception is thrown upon access to such property.\n\nChange-Id: Icde635015a426664c1f78f5602ae73a5cb71f76e\nCloses-Bug: #1364484\n'}, {'number': 3, 'created': '2014-09-24 16:36:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano/commit/85746fb94b8448ba60b8bb99bb522336efca70af', 'message': 'Runtime properties may no longer have default value\n\nDefault value was ignored for Runtime properties.\nNow Runtime properties get initialized automatically if they have Default key\n(it may be of null value). If such property was not initialized neither by Default\nvalue nor in initializer an exception is thrown upon access to such property.\n\nChange-Id: Icde635015a426664c1f78f5602ae73a5cb71f76e\nCloses-Bug: #1364484\n'}, {'number': 4, 'created': '2014-09-25 12:48:04.000000000', 'files': ['murano/dsl/exceptions.py', 'murano/tests/unit/dsl/test_property_access.py', 'murano/tests/unit/dsl/meta/PropertyInit.yaml', 'murano/dsl/murano_object.py', 'murano/tests/unit/dsl/test_property_inititialization.py'], 'web_link': 'https://opendev.org/openstack/murano/commit/382ea845b12c449a00939c1b207268f2e5494c38', 'message': 'Runtime properties may no longer have default value\n\nDefault value was ignored for Runtime properties.\nNow Runtime properties get initialized automatically if they have Default key\n(it may be of null value). If such property was not initialized neither by Default\nvalue nor in initializer an exception is thrown upon access to such property.\n\nChange-Id: Icde635015a426664c1f78f5602ae73a5cb71f76e\nCloses-Bug: #1364484\n'}]",1,122431,382ea845b12c449a00939c1b207268f2e5494c38,26,6,4,7226,,,0,"Runtime properties may no longer have default value

Default value was ignored for Runtime properties.
Now Runtime properties get initialized automatically if they have Default key
(it may be of null value). If such property was not initialized neither by Default
value nor in initializer an exception is thrown upon access to such property.

Change-Id: Icde635015a426664c1f78f5602ae73a5cb71f76e
Closes-Bug: #1364484
",git fetch https://review.opendev.org/openstack/murano refs/changes/31/122431/4 && git format-patch -1 --stdout FETCH_HEAD,"['murano/dsl/exceptions.py', 'murano/tests/unit/dsl/meta/PropertyInit.yaml', 'murano/tests/unit/dsl/test_property_access.py', 'murano/dsl/murano_object.py', 'murano/tests/unit/dsl/test_property_inititialization.py']",5,a8efdfbc1603c0ee645743abafa5c3a5f9ecfd43,bug/1364484,"# Copyright (c) 2014 Mirantis, Inc. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from murano.dsl import exceptions from murano.tests.unit.dsl.foundation import object_model as om from murano.tests.unit.dsl.foundation import test_case from murano.engine.system import agent_listener class TestPropertyInitialization(test_case.DslTestCase): def setUp(self): super(TestPropertyInitialization, self).setUp() model = om.Object( 'PropertyInit' ) self._runner = self.new_runner(model) def test_runtime_property_default(self): self.assertEquals( 'DEFAULT', self._runner.testRuntimePropertyDefault()) def test_runtime_property_without_default(self): self.assertRaises( exceptions.UninitializedPropertyAccessError, self._runner.testRuntimePropertyWithoutDefault) def test_runtime_property_with_strict_contract_without_default(self): self.assertEquals( 'VALUE', self._runner.testRuntimePropertyWithStrictContractWithoutDefault()) def test_uninitialized_runtime_property_with_strict_contract(self): self.assertRaises( exceptions.UninitializedPropertyAccessError, self._runner.testUninitializedRuntimeProperty) ",,107,19
openstack%2Fmurano~master~Ia4366cba486e9ce4c5ae35ecb3e042bbf5f7ef3d,openstack/murano,master,Ia4366cba486e9ce4c5ae35ecb3e042bbf5f7ef3d,No error's reported if heat stack creation/update fails,MERGED,2014-09-26 14:48:20.000000000,2014-09-29 13:26:43.000000000,2014-09-29 13:26:43.000000000,"[{'_account_id': 3}, {'_account_id': 7225}, {'_account_id': 7227}, {'_account_id': 7600}, {'_account_id': 7821}]","[{'number': 1, 'created': '2014-09-26 14:48:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano/commit/578884044aec00f167147402f954488689aeef4d', 'message': ""No error's reported if heat stack creation/update fails\n\nAdds error status reason text to exception message that is raised\nwhen Heat stack create/update fails\n\nAlso disables stack rollback for consistency (otherwise if error happens during\nstack creation it silently disappears and no reason provided)\n\nChange-Id: Ia4366cba486e9ce4c5ae35ecb3e042bbf5f7ef3d\nCloses-Bug: #1369722\n""}, {'number': 2, 'created': '2014-09-29 07:53:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano/commit/cbbe8d8c0868d7940be5fd0279b0966ab3a4ad49', 'message': ""No error's reported if heat stack creation/update fails\n\nAdds error status reason text to exception message that is raised\nwhen Heat stack create/update fails\n\nAlso disables stack rollback for consistency (otherwise if error happens during\nstack creation it silently disappears and no reason provided)\n\nChange-Id: Ia4366cba486e9ce4c5ae35ecb3e042bbf5f7ef3d\nCloses-Bug: #1369722\n""}, {'number': 3, 'created': '2014-09-29 10:21:42.000000000', 'files': ['murano/engine/system/heat_stack.py', 'murano/tests/unit/test_heat_stack.py'], 'web_link': 'https://opendev.org/openstack/murano/commit/f901e9acfa4d571283134a863ca68ff34819138d', 'message': ""No error's reported if heat stack creation/update fails\n\nAdds error status reason text to exception message that is raised\nwhen Heat stack create/update fails\n\nAlso disables stack rollback for consistency (otherwise if error happens during\nstack creation it silently disappears and no reason provided)\n\nChange-Id: Ia4366cba486e9ce4c5ae35ecb3e042bbf5f7ef3d\nCloses-Bug: #1369722\n""}]",0,124426,f901e9acfa4d571283134a863ca68ff34819138d,20,5,3,7226,,,0,"No error's reported if heat stack creation/update fails

Adds error status reason text to exception message that is raised
when Heat stack create/update fails

Also disables stack rollback for consistency (otherwise if error happens during
stack creation it silently disappears and no reason provided)

Change-Id: Ia4366cba486e9ce4c5ae35ecb3e042bbf5f7ef3d
Closes-Bug: #1369722
",git fetch https://review.opendev.org/openstack/murano refs/changes/26/124426/1 && git format-patch -1 --stdout FETCH_HEAD,['murano/engine/system/heat_stack.py'],1,578884044aec00f167147402f954488689aeef4d,bug/1369722," reason = ': {0}'.format( stack_info.stack_status_reason) if stack_info else '' ""Unexpected stack state {0}{1}"".format(status, reason)) disable_rollback=True)"," ""Unexpected stack state {0}"".format(status)) disable_rollback=False)",4,2
openstack%2Fceilometer~master~I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f,openstack/ceilometer,master,I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f,[HBase] Improves speed of unit tests on real HBase backend,MERGED,2014-08-19 13:52:31.000000000,2014-09-29 13:16:05.000000000,2014-09-29 13:16:04.000000000,"[{'_account_id': 3}, {'_account_id': 2813}, {'_account_id': 3012}, {'_account_id': 6537}, {'_account_id': 7478}, {'_account_id': 7729}, {'_account_id': 8871}, {'_account_id': 9562}, {'_account_id': 10987}]","[{'number': 1, 'created': '2014-08-19 13:52:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/9f7b18fd5164ae8863f70ff8d1a9705f964a3406', 'message': '[HBase] Improve speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually used.\nJenkins jobs and a predominant amount of developer are uses\nHBase mock. One of the reasons for this, unittests at real\nHBase are too slow. It\'s due to processing commands ""disable_table""\nand ""delete_table"". In HBase these command running can spend time\nto 1-2 seconds and for separation data of different tests for every test\ninstance are created and deleted new tables set with unique prefix.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. It implements using row prefix for\nseparation tests data and allows to reduce table sets count to 1.\nSo unittests run several minutes instead several hours.\n\nCreating and deleting required table happens at setup-test-env.sh.\nSeparating data is implemented with mock.patchs which are transform\nrow value in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f\n'}, {'number': 2, 'created': '2014-08-20 13:33:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/d6f02bebb3824153b5b081df3750bf02887528ef', 'message': '[HBase] Improve speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually used.\nJenkins jobs and a predominant amount of developer are uses\nHBase mock. One of the reasons for this, unittests at real\nHBase are too slow. It\'s due to processing commands ""disable_table""\nand ""delete_table"". In HBase these command running can spend time\nto 1-2 seconds and for separation data of different tests for every test\ninstance are created and deleted new tables set with unique prefix.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. It implements using row prefix for\nseparation tests data and allows to reduce table sets count to 1.\nSo unittests run several minutes instead several hours.\n\nCreating and deleting required table happens at setup-test-env.sh.\nSeparating data is implemented with mock.patchs which are transform\nrow value in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f\n'}, {'number': 3, 'created': '2014-09-01 10:38:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/9d67f70658ef685367ec499ad06bc3ff8d1cb107', 'message': '[HBase] Improves speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually used.\nJenkins jobs and a predominant amount of developers are use\nHBase mock. One of the reasons for this is that unittests at real\nHBase are too slow. It\'s due to processing of ""disable_table""\nand ""delete_table"" commands. In HBase these command spend time\nto 1-2 seconds. To prevent the intersection of data \nfor the different tests was implemented via creation and deletion of \nnew tables with name containing the unique prefix.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. It uses row prefix for \nseparation tests data and allows to reduce table sets count to 1.\nSo unittests run several minutes instead several hours.\n\nCreating and deleting required table are implements at \nsetup-test-env.sh.\nSeparating data is implemented with mock.patchs which transforms row value in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f'}, {'number': 4, 'created': '2014-09-01 10:39:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/f9fa854f1184f85a0ea112fab9ecd2422e79e28e', 'message': '[HBase] Improves speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually used.\nJenkins jobs and a predominant amount of developers are use\nHBase mock. One of the reasons for this is that unittests at real\nHBase are too slow. It\'s due to processing of ""disable_table""\nand ""delete_table"" commands. In HBase these command spend time\nto 1-2 seconds. To prevent the intersection of data \nfor the different tests was implemented via creation and deletion of \nnew tables with name containing the unique prefix.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. It uses row prefix for \nseparation tests data and allows to reduce table sets count to 1.\nSo unittests run several minutes instead several hours.\n\nCreating and deleting required table are implements at \nsetup-test-env.sh.\nSeparating data is implemented with mock.patchs which transforms row \nvalue in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f'}, {'number': 5, 'created': '2014-09-01 13:28:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/58a56f48432d4fc8b9eeed42187c6a4905ca4b12', 'message': '[HBase] Improves speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually used.\nJenkins jobs and a predominant amount of developers are use\nHBase mock. One of the reasons for this is that unittests at real\nHBase are too slow. It\'s due to processing of ""disable_table""\nand ""delete_table"" commands. In HBase these command spend time\nto 1-2 seconds. To prevent the intersection of data \nfor the different tests was implemented via creation and deletion of \nnew tables with name containing the unique prefix.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. It uses row prefix for \nseparation tests data and allows to reduce table sets count to 1.\nSo unittests run several minutes instead several hours.\n\nCreating and deleting required table are implements at \nsetup-test-env.sh.\nSeparating data is implemented with mock.patchs which transforms row \nvalue in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f'}, {'number': 6, 'created': '2014-09-05 12:03:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/b6604e626ff140989bd58ee61e2b1b8076e0faea', 'message': '[HBase] Improves speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually use.\nJenkins jobs and a predominant amount of developers are use\nHBase mock. One of the reasons for this is that unittests at real\nHBase are too slow. It\'s due to processing of ""disable_table""\nand ""delete_table"" commands. In HBase these command take up\nto 1-2 seconds. Now we create all table-set for each test that\'s \nwhy at real HBase unit tests may be executed several hours.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. To solve this problem it was decided \nto keep all test data in one table. To provide a distinguishability \nof data from different tests unique row-prefix is used for each one.\n\nCreating and deleting required table are implements at \nsetup-test-env.sh.\nSeparating data is implemented with mock.patchs which transforms row \nvalue in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f'}, {'number': 7, 'created': '2014-09-05 12:36:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/8b65519635f2ea7ccf7c816d462b712ecebfe433', 'message': '[HBase] Improves speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually use.\nJenkins jobs and a predominant amount of developers are use\nHBase mock. One of the reasons for this is that unittests at real\nHBase are too slow. It\'s due to processing of ""disable_table""\nand ""delete_table"" commands. In HBase these command take up\nto 1-2 seconds. Now we create all table-set for each test that\'s\nwhy at real HBase unit tests may be executed several hours.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. To solve this problem it was decided\nto keep all test data in one table. To provide a distinguishability\nof data from different tests unique row-prefix is used for each one.\n\nCreating and deleting required table are implements at\nsetup-test-env.sh.\nSeparating data is implemented with mock.patchs which transforms row\nvalue in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f\n'}, {'number': 8, 'created': '2014-09-05 13:33:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/341ae81242bada723d828b2833529d0248af73c6', 'message': '[HBase] Improves speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually use.\nJenkins jobs and a predominant amount of developers are use\nHBase mock. One of the reasons for this is that unittests at real\nHBase are too slow. It\'s due to processing of ""disable_table""\nand ""delete_table"" commands. In HBase these command take up\nto 1-2 seconds. Now we create all table-set for each test that\'s\nwhy at real HBase unit tests may be executed several hours.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. To solve this problem it was decided\nto keep all test data in one table. To provide a distinguishability\nof data from different tests unique row-prefix is used for each one.\n\nCreating and deleting required table are implements at\nsetup-test-env.sh.\nSeparating data is implemented with mock.patchs which transforms row\nvalue in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f\n'}, {'number': 9, 'created': '2014-09-08 10:37:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/073f5429e65fafa1818bb878c28ebe7383428434', 'message': '[HBase] Improves speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually used.\nJenkins jobs and a predominant amount of developers use\nHBase mock. One of the reasons for this is that unittests at real\nHBase are too slow. It\'s due to processing of ""disable_table""\nand ""delete_table"" commands. In HBase these command take up\nto 1-2 seconds. Now we create all table-set for each test that\'s\nwhy at real HBase unit tests may be executed several hours.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. To solve this problem it was decided\nto keep all test data in one table. To provide a distinguishability\nof data from different tests unique row-prefix is used for each one.\n\nCreating and deleting required table are implements at\nsetup-test-env.sh.\nSeparating data is implemented with mock.patchs which transforms row\nvalue in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f'}, {'number': 10, 'created': '2014-09-08 11:52:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/8efedd0a215a188b9b5d067b78b328790e35ecfd', 'message': '[HBase] Improves speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually used.\nJenkins jobs and a predominant amount of developers use\nHBase mock. One of the reasons for this is that unittests at real\nHBase are too slow. It\'s due to processing of ""disable_table""\nand ""delete_table"" commands. In HBase these command take up\nto 1-2 seconds. Now we create all table-set for each test that\'s\nwhy at real HBase unit tests may be executed several hours.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. To solve this problem it was decided\nto keep all test data in one table. To provide a distinguishability\nof data from different tests unique row-prefix is used for each one.\n\nCreating and deleting required table are implements at\nsetup-test-env.sh.\nSeparating data is implemented with mock.patchs which transforms row\nvalue in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f\n'}, {'number': 11, 'created': '2014-09-19 13:51:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/ecc234b114f026cb989be59d9b16bfede2cdb3f9', 'message': '[HBase] Improves speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually used.\nJenkins jobs and a predominant amount of developers use\nHBase mock. One of the reasons for this is that unittests at real\nHBase are too slow. It\'s due to processing of ""disable_table""\nand ""delete_table"" commands. In HBase these command take up\nto 1-2 seconds. Now we create all table-set for each test that\'s\nwhy at real HBase unit tests may be executed several hours.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. To solve this problem it was decided\nto keep all test data in one table. To provide a distinguishability\nof data from different tests unique row-prefix is used for each one.\n\nCreating and deleting required table are implements at\nsetup-test-env.sh.\nSeparating data is implemented with mock.patchs which transforms row\nvalue in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f\n'}, {'number': 12, 'created': '2014-09-23 12:40:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/6b812119713da892f6918e60b6c13aaff6e19ab3', 'message': '[HBase] Improves speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually used.\nJenkins jobs and a predominant amount of developers use\nHBase mock. One of the reasons for this is that unittests at real\nHBase are too slow. It\'s due to processing of ""disable_table""\nand ""delete_table"" commands. In HBase these command take up\nto 1-2 seconds. Now we create all table-set for each test that\'s\nwhy at real HBase unit tests may be executed several hours.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. To solve this problem it was decided\nto keep all test data in one table. To provide a distinguishability\nof data from different tests unique row-prefix is used for each one.\n\nCreating and deleting required table are implements at\nsetup-test-env.sh.\nSeparating data is implemented with mock.patchs which transforms row\nvalue in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f\nCloses-bug: #1372912'}, {'number': 13, 'created': '2014-09-24 11:09:01.000000000', 'files': ['setup-test-env.sh', 'ceilometer/tests/mocks.py', 'tools/test_hbase_table_utils.py', 'ceilometer/tests/db.py', 'ceilometer/tests/storage/test_impl_hbase.py'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/125d950dd4a413801a28bfed7ac831227c0653f2', 'message': '[HBase] Improves speed of unit tests on real HBase backend\n\nCurrently, unittests at real HBase are not usually used.\nJenkins jobs and a predominant amount of developers use\nHBase mock. One of the reasons for this is that unittests at real\nHBase are too slow. It\'s due to processing of ""disable_table""\nand ""delete_table"" commands. In HBase these command take up\nto 1-2 seconds. Now we create all table-set for each test that\'s\nwhy at real HBase unit tests may be executed several hours.\nSo, at real HBase backend unit tests may be executed several hours.\n\nMy CR speeds up this case. To solve this problem it was decided\nto keep all test data in one table. To provide a distinguishability\nof data from different tests unique row-prefix is used for each one.\n\nCreating and deleting required table are implements at\nsetup-test-env.sh.\nSeparating data is implemented with mock.patchs which transforms row\nvalue in happybase.Table methods.\n\nChange-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f\nCloses-bug: #1372912\n'}]",71,115290,125d950dd4a413801a28bfed7ac831227c0653f2,78,9,13,7729,,,0,"[HBase] Improves speed of unit tests on real HBase backend

Currently, unittests at real HBase are not usually used.
Jenkins jobs and a predominant amount of developers use
HBase mock. One of the reasons for this is that unittests at real
HBase are too slow. It's due to processing of ""disable_table""
and ""delete_table"" commands. In HBase these command take up
to 1-2 seconds. Now we create all table-set for each test that's
why at real HBase unit tests may be executed several hours.
So, at real HBase backend unit tests may be executed several hours.

My CR speeds up this case. To solve this problem it was decided
to keep all test data in one table. To provide a distinguishability
of data from different tests unique row-prefix is used for each one.

Creating and deleting required table are implements at
setup-test-env.sh.
Separating data is implemented with mock.patchs which transforms row
value in happybase.Table methods.

Change-Id: I1883d6e0619b0b2f223a4e58bdc0fc0656636e1f
Closes-bug: #1372912
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/90/115290/9 && git format-patch -1 --stdout FETCH_HEAD,"['tools/create_test_hbase_tables.py', 'setup-test-env.sh', 'ceilometer/tests/db.py', 'ceilometer/tests/storage/test_impl_hbase.py', 'tools/clear_test_hbase_tables.py']",5,9f7b18fd5164ae8863f70ff8d1a9705f964a3406,fast_hbase_tests,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import os from oslo.config import cfg from ceilometer import storage def main(): cfg.CONF([], project='ceilometer') url = (""%s?table_prefix=%s"" % (os.getenv(""CEILOMETER_TEST_HBASE_URL"", ""hbase://localhost:9090""), os.getenv(""CEILOMETER_TEST_HBASE_TABLE_PREFIX"", ""test""))) conn = storage.get_connection(url, 'ceilometer.metering.storage') conn.clear() alarm_conn = storage.get_connection(url, 'ceilometer.alarm.storage') alarm_conn.clear() if __name__ == '__main__': main()",,148,6
openstack%2Ftempest~master~I21354541bace349fbc84bc5f5c0cf6e131ebbd01,openstack/tempest,master,I21354541bace349fbc84bc5f5c0cf6e131ebbd01,Drop OfficialClientTest and NetworkTest,MERGED,2014-09-25 11:06:49.000000000,2014-09-29 13:15:55.000000000,2014-09-29 13:15:54.000000000,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 5196}, {'_account_id': 5689}, {'_account_id': 5803}, {'_account_id': 8556}, {'_account_id': 8576}]","[{'number': 1, 'created': '2014-09-25 11:06:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/64a0cb6675bbf830c71792e390b74033847b18fc', 'message': 'Drop OfficialClientTest and NetworkTest\n\nMigration of scenario tests to tempest clients is complete.\nDrop the two base classes based on official clients, which\nare now unused.\n\nChange-Id: I21354541bace349fbc84bc5f5c0cf6e131ebbd01\nPartially-implements: bp/tempest-client-scenarios\n'}, {'number': 2, 'created': '2014-09-26 03:26:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/fe93e70bb109abc9f08ac1bfef2f1b2a1067af21', 'message': 'Drop OfficialClientTest and NetworkTest\n\nMigration of scenario tests to tempest clients is complete.\nDrop the two base classes based on official clients, which\nare now unused.\n\nChange-Id: I21354541bace349fbc84bc5f5c0cf6e131ebbd01\nPartially-implements: bp/tempest-client-scenarios\n'}, {'number': 3, 'created': '2014-09-26 11:35:36.000000000', 'files': ['tempest/scenario/manager.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/966331cebfa5f431af5a8027f8c7a298d8449eec', 'message': 'Drop OfficialClientTest and NetworkTest\n\nMigration of scenario tests to tempest clients is complete.\nDrop the two base classes based on official clients, which\nare now unused.\n\nChange-Id: I21354541bace349fbc84bc5f5c0cf6e131ebbd01\nPartially-implements: bp/tempest-client-scenarios\n'}]",2,124013,966331cebfa5f431af5a8027f8c7a298d8449eec,25,7,3,1921,,,0,"Drop OfficialClientTest and NetworkTest

Migration of scenario tests to tempest clients is complete.
Drop the two base classes based on official clients, which
are now unused.

Change-Id: I21354541bace349fbc84bc5f5c0cf6e131ebbd01
Partially-implements: bp/tempest-client-scenarios
",git fetch https://review.opendev.org/openstack/tempest refs/changes/13/124013/2 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/manager.py'],1,64a0cb6675bbf830c71792e390b74033847b18fc,bp/tempest-client-scenarios,,"from cinderclient import exceptions as cinder_exceptions import glanceclientfrom novaclient import exceptions as nova_exceptionsclass OfficialClientTest(tempest.test.BaseTestCase): """""" Official Client test base class for scenario testing. Official Client tests are tests that have the following characteristics: * Test basic operations of an API, typically in an order that a regular user would perform those operations * Test only the correct inputs and action paths -- no fuzz or random input data is sent, only valid inputs. * Use only the default client tool for calling an API """""" @classmethod def setUpClass(cls): super(OfficialClientTest, cls).setUpClass() cls.isolated_creds = isolated_creds.IsolatedCreds( cls.__name__, tempest_client=False, network_resources=cls.network_resources) cls.manager = clients.OfficialClientManager( credentials=cls.credentials()) cls.compute_client = cls.manager.compute_client cls.image_client = cls.manager.image_client cls.baremetal_client = cls.manager.baremetal_client cls.identity_client = cls.manager.identity_client cls.network_client = cls.manager.network_client cls.volume_client = cls.manager.volume_client cls.object_storage_client = cls.manager.object_storage_client cls.orchestration_client = cls.manager.orchestration_client cls.data_processing_client = cls.manager.data_processing_client cls.ceilometer_client = cls.manager.ceilometer_client @classmethod def _get_credentials(cls, get_creds, ctype): if CONF.compute.allow_tenant_isolation: creds = get_creds() else: creds = auth.get_default_credentials(ctype) return creds @classmethod def credentials(cls): return cls._get_credentials(cls.isolated_creds.get_primary_creds, 'user') @classmethod def alt_credentials(cls): return cls._get_credentials(cls.isolated_creds.get_alt_creds, 'alt_user') @classmethod def admin_credentials(cls): return cls._get_credentials(cls.isolated_creds.get_admin_creds, 'identity_admin') def setUp(self): super(OfficialClientTest, self).setUp() self.cleanup_waits = [] # NOTE(mtreinish) This is safe to do in setUp instead of setUp class # because scenario tests in the same test class should not share # resources. If resources were shared between test cases then it # should be a single scenario test instead of multiples. # NOTE(yfried): this list is cleaned at the end of test_methods and # not at the end of the class self.addCleanup(self._wait_for_cleanups) @staticmethod def not_found_exception(exception): """""" @return: True if exception is of NotFound type """""" NOT_FOUND_LIST = ['NotFound', 'HTTPNotFound'] return (exception.__class__.__name__ in NOT_FOUND_LIST or hasattr(exception, 'status_code') and exception.status_code == 404) def delete_wrapper(self, thing): """"""Ignores NotFound exceptions for delete operations. @param thing: object with delete() method. OpenStack resources are assumed to have a delete() method which destroys the resource """""" try: thing.delete() except Exception as e: # If the resource is already missing, mission accomplished. if not self.not_found_exception(e): raise def _wait_for_cleanups(self): """"""To handle async delete actions, a list of waits is added which will be iterated over as the last step of clearing the cleanup queue. That way all the delete calls are made up front and the tests won't succeed unless the deletes are eventually successful. This is the same basic approach used in the api tests to limit cleanup execution time except here it is multi-resource, because of the nature of the scenario tests. """""" for wait in self.cleanup_waits: self.delete_timeout(**wait) def addCleanup_with_wait(self, things, thing_id, error_status='ERROR', exc_type=nova_exceptions.NotFound, cleanup_callable=None, cleanup_args=None, cleanup_kwargs=None): """"""Adds wait for ansyc resource deletion at the end of cleanups @param things: type of the resource to delete @param thing_id: @param error_status: see manager.delete_timeout() @param exc_type: see manager.delete_timeout() @param cleanup_callable: method to load pass to self.addCleanup with the following *cleanup_args, **cleanup_kwargs. usually a delete method. if not used, will try to use: things.delete(thing_id) """""" if cleanup_args is None: cleanup_args = [] if cleanup_kwargs is None: cleanup_kwargs = {} if cleanup_callable is None: LOG.debug(""no delete method passed. using {rclass}.delete({id}) as"" "" default"".format(rclass=things, id=thing_id)) self.addCleanup(things.delete, thing_id) else: self.addCleanup(cleanup_callable, *cleanup_args, **cleanup_kwargs) wait_dict = { 'things': things, 'thing_id': thing_id, 'error_status': error_status, 'not_found_exception': exc_type, } self.cleanup_waits.append(wait_dict) def status_timeout(self, things, thing_id, expected_status, error_status='ERROR', not_found_exception=nova_exceptions.NotFound): """""" Given a thing and an expected status, do a loop, sleeping for a configurable amount of time, checking for the expected status to show. At any time, if the returned status of the thing is ERROR, fail out. """""" self._status_timeout(things, thing_id, expected_status=expected_status, error_status=error_status, not_found_exception=not_found_exception) def delete_timeout(self, things, thing_id, error_status='ERROR', not_found_exception=nova_exceptions.NotFound): """""" Given a thing, do a loop, sleeping for a configurable amount of time, checking for the deleted status to show. At any time, if the returned status of the thing is ERROR, fail out. """""" self._status_timeout(things, thing_id, allow_notfound=True, error_status=error_status, not_found_exception=not_found_exception) def _status_timeout(self, things, thing_id, expected_status=None, allow_notfound=False, error_status='ERROR', not_found_exception=nova_exceptions.NotFound): log_status = expected_status if expected_status else '' if allow_notfound: log_status += ' or NotFound' if log_status != '' else 'NotFound' def check_status(): # python-novaclient has resources available to its client # that all implement a get() method taking an identifier # for the singular resource to retrieve. try: thing = things.get(thing_id) except not_found_exception: if allow_notfound: return True raise except Exception as e: if allow_notfound and self.not_found_exception(e): return True raise new_status = thing.status # Some components are reporting error status in lower case # so case sensitive comparisons can really mess things # up. if new_status.lower() == error_status.lower(): message = (""%s failed to get to expected status (%s). "" ""In %s state."") % (thing, expected_status, new_status) raise exceptions.BuildErrorException(message, server_id=thing_id) elif new_status == expected_status and expected_status is not None: return True # All good. LOG.debug(""Waiting for %s to get to %s status. "" ""Currently in %s status"", thing, log_status, new_status) if not tempest.test.call_until_true( check_status, CONF.compute.build_timeout, CONF.compute.build_interval): message = (""Timed out waiting for thing %s "" ""to become %s"") % (thing_id, log_status) raise exceptions.TimeoutException(message) def _create_loginable_secgroup_rule_nova(self, client=None, secgroup_id=None): if client is None: client = self.compute_client if secgroup_id is None: sgs = client.security_groups.list() for sg in sgs: if sg.name == 'default': secgroup_id = sg.id # These rules are intended to permit inbound ssh and icmp # traffic from all sources, so no group_id is provided. # Setting a group_id would only permit traffic from ports # belonging to the same security group. rulesets = [ { # ssh 'ip_protocol': 'tcp', 'from_port': 22, 'to_port': 22, 'cidr': '0.0.0.0/0', }, { # ssh -6 'ip_protocol': 'tcp', 'from_port': 22, 'to_port': 22, 'cidr': '::/0', }, { # ping 'ip_protocol': 'icmp', 'from_port': -1, 'to_port': -1, 'cidr': '0.0.0.0/0', }, { # ping6 'ip_protocol': 'icmp', 'from_port': -1, 'to_port': -1, 'cidr': '::/0', } ] rules = list() for ruleset in rulesets: sg_rule = client.security_group_rules.create(secgroup_id, **ruleset) self.addCleanup(self.delete_wrapper, sg_rule) rules.append(sg_rule) return rules def _create_security_group_nova(self, client=None, namestart='secgroup-smoke-'): if client is None: client = self.compute_client # Create security group sg_name = data_utils.rand_name(namestart) sg_desc = sg_name + "" description"" secgroup = client.security_groups.create(sg_name, sg_desc) self.assertEqual(secgroup.name, sg_name) self.assertEqual(secgroup.description, sg_desc) self.addCleanup(self.delete_wrapper, secgroup) # Add rules to the security group self._create_loginable_secgroup_rule_nova(client, secgroup.id) return secgroup def rebuild_server(self, server, client=None, image=None, preserve_ephemeral=False, wait=True, rebuild_kwargs=None): if client is None: client = self.compute_client if image is None: image = CONF.compute.image_ref rebuild_kwargs = rebuild_kwargs or {} LOG.debug(""Rebuilding server (name: %s, image: %s, preserve eph: %s)"", server.name, image, preserve_ephemeral) server.rebuild(image, preserve_ephemeral=preserve_ephemeral, **rebuild_kwargs) if wait: self.status_timeout(client.servers, server.id, 'ACTIVE') def create_server(self, client=None, name=None, image=None, flavor=None, wait_on_boot=True, wait_on_delete=True, create_kwargs=None): """"""Creates VM instance. @param client: compute client to create the instance @param image: image from which to create the instance @param wait_on_boot: wait for status ACTIVE before continue @param wait_on_delete: force synchronous delete on cleanup @param create_kwargs: additional details for instance creation @return: client.server object """""" if client is None: client = self.compute_client if name is None: name = data_utils.rand_name('scenario-server-') if image is None: image = CONF.compute.image_ref if flavor is None: flavor = CONF.compute.flavor_ref if create_kwargs is None: create_kwargs = {} fixed_network_name = CONF.compute.fixed_network_name if 'nics' not in create_kwargs and fixed_network_name: networks = client.networks.list() # If several networks found, set the NetID on which to connect the # server to avoid the following error ""Multiple possible networks # found, use a Network ID to be more specific."" # See Tempest #1250866 if len(networks) > 1: for network in networks: if network.label == fixed_network_name: create_kwargs['nics'] = [{'net-id': network.id}] break # If we didn't find the network we were looking for : else: msg = (""The network on which the NIC of the server must "" ""be connected can not be found : "" ""fixed_network_name=%s. Starting instance without "" ""specifying a network."") % fixed_network_name LOG.info(msg) LOG.debug(""Creating a server (name: %s, image: %s, flavor: %s)"", name, image, flavor) server = client.servers.create(name, image, flavor, **create_kwargs) self.assertEqual(server.name, name) if wait_on_delete: self.addCleanup(self.delete_timeout, self.compute_client.servers, server.id) self.addCleanup_with_wait(self.compute_client.servers, server.id, cleanup_callable=self.delete_wrapper, cleanup_args=[server]) if wait_on_boot: self.status_timeout(client.servers, server.id, 'ACTIVE') # The instance retrieved on creation is missing network # details, necessitating retrieval after it becomes active to # ensure correct details. server = client.servers.get(server.id) LOG.debug(""Created server: %s"", server) return server def create_volume(self, client=None, size=1, name=None, snapshot_id=None, imageRef=None, volume_type=None, wait_on_delete=True): if client is None: client = self.volume_client if name is None: name = data_utils.rand_name('scenario-volume-') LOG.debug(""Creating a volume (size: %s, name: %s)"", size, name) volume = client.volumes.create(size=size, display_name=name, snapshot_id=snapshot_id, imageRef=imageRef, volume_type=volume_type) if wait_on_delete: self.addCleanup(self.delete_timeout, self.volume_client.volumes, volume.id) self.addCleanup_with_wait(self.volume_client.volumes, volume.id, exc_type=cinder_exceptions.NotFound) self.assertEqual(name, volume.display_name) self.status_timeout(client.volumes, volume.id, 'available') LOG.debug(""Created volume: %s"", volume) return volume def create_server_snapshot(self, server, compute_client=None, image_client=None, name=None): if compute_client is None: compute_client = self.compute_client if image_client is None: image_client = self.image_client if name is None: name = data_utils.rand_name('scenario-snapshot-') LOG.debug(""Creating a snapshot image for server: %s"", server.name) image_id = compute_client.servers.create_image(server, name) self.addCleanup_with_wait(self.image_client.images, image_id, exc_type=glanceclient.exc.HTTPNotFound) self.status_timeout(image_client.images, image_id, 'active') snapshot_image = image_client.images.get(image_id) self.assertEqual(name, snapshot_image.name) LOG.debug(""Created snapshot image %s for server %s"", snapshot_image.name, server.name) return snapshot_image def create_keypair(self, client=None, name=None): if client is None: client = self.compute_client if name is None: name = data_utils.rand_name('scenario-keypair-') keypair = client.keypairs.create(name) self.assertEqual(keypair.name, name) self.addCleanup(self.delete_wrapper, keypair) return keypair def get_remote_client(self, server_or_ip, username=None, private_key=None): if isinstance(server_or_ip, six.string_types): ip = server_or_ip else: network_name_for_ssh = CONF.compute.network_for_ssh ip = server_or_ip.networks[network_name_for_ssh][0] if username is None: username = CONF.scenario.ssh_user if private_key is None: private_key = self.keypair.private_key linux_client = remote_client.RemoteClient(ip, username, pkey=private_key) try: linux_client.validate_authentication() except exceptions.SSHTimeout: LOG.exception('ssh connection to %s failed' % ip) debug.log_net_debug() raise return linux_client def _log_console_output(self, servers=None): if not CONF.compute_feature_enabled.console_output: LOG.debug('Console output not supported, cannot log') return if not servers: servers = self.compute_client.servers.list() for server in servers: LOG.debug('Console output for %s', server.id) LOG.debug(server.get_console_output()) def wait_for_volume_status(self, status): volume_id = self.volume.id self.status_timeout( self.volume_client.volumes, volume_id, status) def _image_create(self, name, fmt, path, properties=None): if properties is None: properties = {} name = data_utils.rand_name('%s-' % name) image_file = open(path, 'rb') self.addCleanup(image_file.close) params = { 'name': name, 'container_format': fmt, 'disk_format': fmt, 'is_public': 'False', } params.update(properties) image = self.image_client.images.create(**params) self.addCleanup(self.image_client.images.delete, image) self.assertEqual(""queued"", image.status) image.update(data=image_file) return image.id def glance_image_create(self): img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.img_file aki_img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.aki_img_file ari_img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.ari_img_file ami_img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.ami_img_file img_container_format = CONF.scenario.img_container_format img_disk_format = CONF.scenario.img_disk_format LOG.debug(""paths: img: %s, container_fomat: %s, disk_format: %s, "" ""ami: %s, ari: %s, aki: %s"" % (img_path, img_container_format, img_disk_format, ami_img_path, ari_img_path, aki_img_path)) try: self.image = self._image_create('scenario-img', img_container_format, img_path, properties={'disk_format': img_disk_format}) except IOError: LOG.debug(""A qcow2 image was not found. Try to get a uec image."") kernel = self._image_create('scenario-aki', 'aki', aki_img_path) ramdisk = self._image_create('scenario-ari', 'ari', ari_img_path) properties = { 'properties': {'kernel_id': kernel, 'ramdisk_id': ramdisk} } self.image = self._image_create('scenario-ami', 'ami', path=ami_img_path, properties=properties) LOG.debug(""image:%s"" % self.image) class NetworkScenarioTest(OfficialClientTest): """""" Base class for network scenario tests """""" @classmethod def check_preconditions(cls): if (CONF.service_available.neutron): cls.enabled = True # verify that neutron_available is telling the truth try: cls.network_client.list_networks() except exc.EndpointNotFound: cls.enabled = False raise else: cls.enabled = False msg = 'Neutron not available' raise cls.skipException(msg) @classmethod def setUpClass(cls): super(NetworkScenarioTest, cls).setUpClass() cls.tenant_id = cls.manager.identity_client.tenant_id def _create_network(self, tenant_id, namestart='network-smoke-'): name = data_utils.rand_name(namestart) body = dict( network=dict( name=name, tenant_id=tenant_id, ), ) result = self.network_client.create_network(body=body) network = net_common.DeletableNetwork(client=self.network_client, **result['network']) self.assertEqual(network.name, name) self.addCleanup(self.delete_wrapper, network) return network def _list_networks(self, **kwargs): nets = self.network_client.list_networks(**kwargs) return nets['networks'] def _list_subnets(self, **kwargs): subnets = self.network_client.list_subnets(**kwargs) return subnets['subnets'] def _list_routers(self, **kwargs): routers = self.network_client.list_routers(**kwargs) return routers['routers'] def _list_ports(self, **kwargs): ports = self.network_client.list_ports(**kwargs) return ports['ports'] def _get_tenant_own_network_num(self, tenant_id): nets = self._list_networks(tenant_id=tenant_id) return len(nets) def _get_tenant_own_subnet_num(self, tenant_id): subnets = self._list_subnets(tenant_id=tenant_id) return len(subnets) def _get_tenant_own_port_num(self, tenant_id): ports = self._list_ports(tenant_id=tenant_id) return len(ports) def _create_subnet(self, network, namestart='subnet-smoke-', **kwargs): """""" Create a subnet for the given network within the cidr block configured for tenant networks. """""" def cidr_in_use(cidr, tenant_id): """""" :return True if subnet with cidr already exist in tenant False else """""" cidr_in_use = self._list_subnets(tenant_id=tenant_id, cidr=cidr) return len(cidr_in_use) != 0 tenant_cidr = netaddr.IPNetwork(CONF.network.tenant_network_cidr) result = None # Repeatedly attempt subnet creation with sequential cidr # blocks until an unallocated block is found. for subnet_cidr in tenant_cidr.subnet( CONF.network.tenant_network_mask_bits): str_cidr = str(subnet_cidr) if cidr_in_use(str_cidr, tenant_id=network.tenant_id): continue body = dict( subnet=dict( name=data_utils.rand_name(namestart), ip_version=4, network_id=network.id, tenant_id=network.tenant_id, cidr=str_cidr, ), ) body['subnet'].update(kwargs) try: result = self.network_client.create_subnet(body=body) break except exc.NeutronClientException as e: is_overlapping_cidr = 'overlaps with another subnet' in str(e) if not is_overlapping_cidr: raise self.assertIsNotNone(result, 'Unable to allocate tenant network') subnet = net_common.DeletableSubnet(client=self.network_client, **result['subnet']) self.assertEqual(subnet.cidr, str_cidr) self.addCleanup(self.delete_wrapper, subnet) return subnet def _create_port(self, network, namestart='port-quotatest-'): name = data_utils.rand_name(namestart) body = dict( port=dict(name=name, network_id=network.id, tenant_id=network.tenant_id)) result = self.network_client.create_port(body=body) self.assertIsNotNone(result, 'Unable to allocate port') port = net_common.DeletablePort(client=self.network_client, **result['port']) self.addCleanup(self.delete_wrapper, port) return port def _get_server_port_id(self, server, ip_addr=None): ports = self._list_ports(device_id=server.id, fixed_ip=ip_addr) self.assertEqual(len(ports), 1, ""Unable to determine which port to target."") return ports[0]['id'] def _get_network_by_name(self, network_name): net = self._list_networks(name=network_name) return net_common.AttributeDict(net[0]) def _create_floating_ip(self, thing, external_network_id, port_id=None): if not port_id: port_id = self._get_server_port_id(thing) body = dict( floatingip=dict( floating_network_id=external_network_id, port_id=port_id, tenant_id=thing.tenant_id, ) ) result = self.network_client.create_floatingip(body=body) floating_ip = net_common.DeletableFloatingIp( client=self.network_client, **result['floatingip']) self.addCleanup(self.delete_wrapper, floating_ip) return floating_ip def _associate_floating_ip(self, floating_ip, server): port_id = self._get_server_port_id(server) floating_ip.update(port_id=port_id) self.assertEqual(port_id, floating_ip.port_id) return floating_ip def _disassociate_floating_ip(self, floating_ip): """""" :param floating_ip: type DeletableFloatingIp """""" floating_ip.update(port_id=None) self.assertIsNone(floating_ip.port_id) return floating_ip def _ping_ip_address(self, ip_address, should_succeed=True): cmd = ['ping', '-c1', '-w1', ip_address] def ping(): proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE) proc.communicate() return (proc.returncode == 0) == should_succeed return tempest.test.call_until_true( ping, CONF.compute.ping_timeout, 1) def _create_pool(self, lb_method, protocol, subnet_id): """"""Wrapper utility that returns a test pool."""""" name = data_utils.rand_name('pool-') body = { ""pool"": { ""protocol"": protocol, ""name"": name, ""subnet_id"": subnet_id, ""lb_method"": lb_method } } resp = self.network_client.create_pool(body=body) pool = net_common.DeletablePool(client=self.network_client, **resp['pool']) self.assertEqual(pool['name'], name) self.addCleanup(self.delete_wrapper, pool) return pool def _create_member(self, address, protocol_port, pool_id): """"""Wrapper utility that returns a test member."""""" body = { ""member"": { ""protocol_port"": protocol_port, ""pool_id"": pool_id, ""address"": address } } resp = self.network_client.create_member(body) member = net_common.DeletableMember(client=self.network_client, **resp['member']) self.addCleanup(self.delete_wrapper, member) return member def _create_vip(self, protocol, protocol_port, subnet_id, pool_id): """"""Wrapper utility that returns a test vip."""""" name = data_utils.rand_name('vip-') body = { ""vip"": { ""protocol"": protocol, ""name"": name, ""subnet_id"": subnet_id, ""pool_id"": pool_id, ""protocol_port"": protocol_port } } resp = self.network_client.create_vip(body) vip = net_common.DeletableVip(client=self.network_client, **resp['vip']) self.assertEqual(vip['name'], name) self.addCleanup(self.delete_wrapper, vip) return vip def _check_vm_connectivity(self, ip_address, username=None, private_key=None, should_connect=True): """""" :param ip_address: server to test against :param username: server's ssh username :param private_key: server's ssh private key to be used :param should_connect: True/False indicates positive/negative test positive - attempt ping and ssh negative - attempt ping and fail if succeed :raises: AssertError if the result of the connectivity check does not match the value of the should_connect param """""" if should_connect: msg = ""Timed out waiting for %s to become reachable"" % ip_address else: msg = ""ip address %s is reachable"" % ip_address self.assertTrue(self._ping_ip_address(ip_address, should_succeed=should_connect), msg=msg) if should_connect: # no need to check ssh for negative connectivity self.get_remote_client(ip_address, username, private_key) def _check_public_network_connectivity(self, ip_address, username, private_key, should_connect=True, msg=None, servers=None): # The target login is assumed to have been configured for # key-based authentication by cloud-init. LOG.debug('checking network connections to IP %s with user: %s' % (ip_address, username)) try: self._check_vm_connectivity(ip_address, username, private_key, should_connect=should_connect) except Exception as e: ex_msg = 'Public network connectivity check failed' if msg: ex_msg += "": "" + msg LOG.exception(ex_msg) self._log_console_output(servers) # network debug is called as part of ssh init if not isinstance(e, exceptions.SSHTimeout): debug.log_net_debug() raise def _check_tenant_network_connectivity(self, server, username, private_key, should_connect=True, servers_for_debug=None): if not CONF.network.tenant_networks_reachable: msg = 'Tenant networks not configured to be reachable.' LOG.info(msg) return # The target login is assumed to have been configured for # key-based authentication by cloud-init. try: for net_name, ip_addresses in server.networks.iteritems(): for ip_address in ip_addresses: self._check_vm_connectivity(ip_address, username, private_key, should_connect=should_connect) except Exception as e: LOG.exception('Tenant network connectivity check failed') self._log_console_output(servers_for_debug) # network debug is called as part of ssh init if not isinstance(e, exceptions.SSHTimeout): debug.log_net_debug() raise def _check_remote_connectivity(self, source, dest, should_succeed=True): """""" check ping server via source ssh connection :param source: RemoteClient: an ssh connection from which to ping :param dest: and IP to ping against :param should_succeed: boolean should ping succeed or not :returns: boolean -- should_succeed == ping :returns: ping is false if ping failed """""" def ping_remote(): try: source.ping_host(dest) except exceptions.SSHExecCommandFailed: LOG.warn('Failed to ping IP: %s via a ssh connection from: %s.' % (dest, source.ssh_client.host)) return not should_succeed return should_succeed return tempest.test.call_until_true(ping_remote, CONF.compute.ping_timeout, 1) def _create_security_group_neutron(self, tenant_id, client=None, namestart='secgroup-smoke-'): if client is None: client = self.network_client secgroup = self._create_empty_security_group(namestart=namestart, client=client, tenant_id=tenant_id) # Add rules to the security group rules = self._create_loginable_secgroup_rule_neutron(secgroup=secgroup) for rule in rules: self.assertEqual(tenant_id, rule.tenant_id) self.assertEqual(secgroup.id, rule.security_group_id) return secgroup def _create_empty_security_group(self, tenant_id, client=None, namestart='secgroup-smoke-'): """"""Create a security group without rules. Default rules will be created: - IPv4 egress to any - IPv6 egress to any :param tenant_id: secgroup will be created in this tenant :returns: DeletableSecurityGroup -- containing the secgroup created """""" if client is None: client = self.network_client sg_name = data_utils.rand_name(namestart) sg_desc = sg_name + "" description"" sg_dict = dict(name=sg_name, description=sg_desc) sg_dict['tenant_id'] = tenant_id body = dict(security_group=sg_dict) result = client.create_security_group(body=body) secgroup = net_common.DeletableSecurityGroup( client=client, **result['security_group'] ) self.assertEqual(secgroup.name, sg_name) self.assertEqual(tenant_id, secgroup.tenant_id) self.assertEqual(secgroup.description, sg_desc) self.addCleanup(self.delete_wrapper, secgroup) return secgroup def _default_security_group(self, tenant_id, client=None): """"""Get default secgroup for given tenant_id. :returns: DeletableSecurityGroup -- default secgroup for given tenant """""" if client is None: client = self.network_client sgs = [ sg for sg in client.list_security_groups().values()[0] if sg['tenant_id'] == tenant_id and sg['name'] == 'default' ] msg = ""No default security group for tenant %s."" % (tenant_id) self.assertTrue(len(sgs) > 0, msg) if len(sgs) > 1: msg = ""Found %d default security groups"" % len(sgs) raise exc.NeutronClientNoUniqueMatch(msg=msg) return net_common.DeletableSecurityGroup(client=client, **sgs[0]) def _create_security_group_rule(self, client=None, secgroup=None, tenant_id=None, **kwargs): """"""Create a rule from a dictionary of rule parameters. Create a rule in a secgroup. if secgroup not defined will search for default secgroup in tenant_id. :param secgroup: type DeletableSecurityGroup. :param secgroup_id: search for secgroup by id default -- choose default secgroup for given tenant_id :param tenant_id: if secgroup not passed -- the tenant in which to search for default secgroup :param kwargs: a dictionary containing rule parameters: for example, to allow incoming ssh: rule = { direction: 'ingress' protocol:'tcp', port_range_min: 22, port_range_max: 22 } """""" if client is None: client = self.network_client if secgroup is None: secgroup = self._default_security_group(tenant_id) ruleset = dict(security_group_id=secgroup.id, tenant_id=secgroup.tenant_id, ) ruleset.update(kwargs) body = dict(security_group_rule=dict(ruleset)) sg_rule = client.create_security_group_rule(body=body) sg_rule = net_common.DeletableSecurityGroupRule( client=client, **sg_rule['security_group_rule'] ) self.addCleanup(self.delete_wrapper, sg_rule) self.assertEqual(secgroup.tenant_id, sg_rule.tenant_id) self.assertEqual(secgroup.id, sg_rule.security_group_id) return sg_rule def _create_loginable_secgroup_rule_neutron(self, client=None, secgroup=None): """"""These rules are intended to permit inbound ssh and icmp traffic from all sources, so no group_id is provided. Setting a group_id would only permit traffic from ports belonging to the same security group. """""" if client is None: client = self.network_client rules = [] rulesets = [ dict( # ssh protocol='tcp', port_range_min=22, port_range_max=22, ), dict( # ping protocol='icmp', ) ] for ruleset in rulesets: for r_direction in ['ingress', 'egress']: ruleset['direction'] = r_direction try: sg_rule = self._create_security_group_rule( client=client, secgroup=secgroup, **ruleset) except exc.NeutronClientException as ex: # if rule already exist - skip rule and continue if not (ex.status_code is 409 and 'Security group rule' ' already exists' in ex.message): raise ex else: self.assertEqual(r_direction, sg_rule.direction) rules.append(sg_rule) return rules def _ssh_to_server(self, server, private_key): ssh_login = CONF.compute.image_ssh_user return self.get_remote_client(server, username=ssh_login, private_key=private_key) def _show_quota_network(self, tenant_id): quota = self.network_client.show_quota(tenant_id) return quota['quota']['network'] def _show_quota_subnet(self, tenant_id): quota = self.network_client.show_quota(tenant_id) return quota['quota']['subnet'] def _show_quota_port(self, tenant_id): quota = self.network_client.show_quota(tenant_id) return quota['quota']['port'] def _get_router(self, tenant_id): """"""Retrieve a router for the given tenant id. If a public router has been configured, it will be returned. If a public router has not been configured, but a public network has, a tenant router will be created and returned that routes traffic to the public network. """""" router_id = CONF.network.public_router_id network_id = CONF.network.public_network_id if router_id: result = self.network_client.show_router(router_id) return net_common.AttributeDict(**result['router']) elif network_id: router = self._create_router(tenant_id) router.add_gateway(network_id) return router else: raise Exception(""Neither of 'public_router_id' or "" ""'public_network_id' has been defined."") def _create_router(self, tenant_id, namestart='router-smoke-'): name = data_utils.rand_name(namestart) body = dict( router=dict( name=name, admin_state_up=True, tenant_id=tenant_id, ), ) result = self.network_client.create_router(body=body) router = net_common.DeletableRouter(client=self.network_client, **result['router']) self.assertEqual(router.name, name) self.addCleanup(self.delete_wrapper, router) return router def create_networks(self, tenant_id=None): """"""Create a network with a subnet connected to a router. The baremetal driver is a special case since all nodes are on the same shared network. :returns: network, subnet, router """""" if CONF.baremetal.driver_enabled: # NOTE(Shrews): This exception is for environments where tenant # credential isolation is available, but network separation is # not (the current baremetal case). Likely can be removed when # test account mgmt is reworked: # https://blueprints.launchpad.net/tempest/+spec/test-accounts network = self._get_network_by_name( CONF.compute.fixed_network_name) router = None subnet = None else: if tenant_id is None: tenant_id = self.tenant_id network = self._create_network(tenant_id) router = self._get_router(tenant_id) subnet = self._create_subnet(network) subnet.add_to_router(router.id) return network, subnet, router ",0,1072
openstack%2Fzaqar~master~Ie6314d2e36b04ae1e82a54165b550f8f84ccef1f,openstack/zaqar,master,Ie6314d2e36b04ae1e82a54165b550f8f84ccef1f,Fixes docstring typos,MERGED,2014-09-25 02:19:36.000000000,2014-09-29 13:10:50.000000000,2014-09-29 13:10:48.000000000,"[{'_account_id': 3}, {'_account_id': 6413}, {'_account_id': 7488}]","[{'number': 1, 'created': '2014-09-25 02:19:36.000000000', 'files': ['zaqar/tests/functional/http.py', 'zaqar/tests/helpers.py'], 'web_link': 'https://opendev.org/openstack/zaqar/commit/f7adfaddb90e800d43908df160049143da267f5c', 'message': 'Fixes docstring typos\n\nChange-Id: Ie6314d2e36b04ae1e82a54165b550f8f84ccef1f\n'}]",0,123915,f7adfaddb90e800d43908df160049143da267f5c,9,3,1,13300,,,0,"Fixes docstring typos

Change-Id: Ie6314d2e36b04ae1e82a54165b550f8f84ccef1f
",git fetch https://review.opendev.org/openstack/zaqar refs/changes/15/123915/1 && git format-patch -1 --stdout FETCH_HEAD,"['zaqar/tests/functional/http.py', 'zaqar/tests/helpers.py']",2,f7adfaddb90e800d43908df160049143da267f5c,," """"""A context manager to validate raised exceptions."," """"""A context manager to validate raised expections.",2,2
openstack%2Fhorizon~master~Ieb224ea4db2adc46073b86e7181981ea598c9a8f,openstack/horizon,master,Ieb224ea4db2adc46073b86e7181981ea598c9a8f,[Sahara] Added ability to specify security groups for node group,MERGED,2014-09-02 23:35:16.000000000,2014-09-29 13:02:08.000000000,2014-09-29 13:02:06.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 6786}, {'_account_id': 6914}, {'_account_id': 7012}, {'_account_id': 7125}, {'_account_id': 7132}, {'_account_id': 8090}, {'_account_id': 8411}, {'_account_id': 9981}, {'_account_id': 10670}]","[{'number': 1, 'created': '2014-09-02 23:35:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/5c7cb937c6f3f8f6c23e539f79879bfa5190afe5', 'message': '[Sahara] Added ability to specify security groups for node group\n\nKnown issues:\n1. Security groups added to the same page as general parameters. Page\n   is a little overloaded now. Will improve this later.\n2. New python-saharaclient is required. Waiting for\n   https://review.openstack.org/#/c/110501/\n3. Additional fix will be needed to address bug #1364659\n\nChange-Id: Ieb224ea4db2adc46073b86e7181981ea598c9a8f\nBlueprint: cluster-secgroups\n'}, {'number': 2, 'created': '2014-09-22 22:01:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/2d04f35fe90a80a4e4e3c7fe73c52f5d5545c694', 'message': '[Sahara] Added ability to specify security groups for node group\n\nKnown issues:\n1. Security groups added to the same page as general parameters. Page\n   is a little overloaded now. Will improve this later.\n2. New python-saharaclient is required. Waiting for\n   https://review.openstack.org/#/c/110501/\n3. Additional fix will be needed to address bug #1364659\n\nChange-Id: Ieb224ea4db2adc46073b86e7181981ea598c9a8f\nBlueprint: cluster-secgroups\n'}, {'number': 3, 'created': '2014-09-23 22:36:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/0464bfd67129b27a577775e3baca03c9fa9e8ffd', 'message': '[sahara] Added ability to specify security groups for node group\n\nKnown issues:\n1. Security groups added to the same page as general parameters. Page\n   is a little overloaded now. Will improve this later.\n2. New python-saharaclient is required. Waiting for\n   https://review.openstack.org/#/c/110501/\n3. Additional fix will be needed to address bug #1364659\n\nBlueprint: cluster-secgroups\nCloses-Bug: #1373157\n\nChange-Id: Ieb224ea4db2adc46073b86e7181981ea598c9a8f'}, {'number': 4, 'created': '2014-09-24 17:10:57.000000000', 'files': ['openstack_dashboard/api/sahara.py', 'openstack_dashboard/dashboards/project/data_processing/cluster_templates/templates/data_processing.cluster_templates/_nodegroups_details.html', 'openstack_dashboard/dashboards/project/data_processing/nodegroup_templates/templates/data_processing.nodegroup_templates/_details.html', 'openstack_dashboard/dashboards/project/data_processing/nodegroup_templates/workflows/create.py', 'openstack_dashboard/dashboards/project/data_processing/clusters/templates/data_processing.clusters/_nodegroups_details.html'], 'web_link': 'https://opendev.org/openstack/horizon/commit/8001eb74a4febe4e5e1676a2236eee9b5b4612ac', 'message': '[Sahara] Added ability to specify security groups for node group\n\nKnown issues:\n1. Security groups added to the same page as general parameters. Page\n   is a little overloaded now. Will improve this later.\n\nChange-Id: Ieb224ea4db2adc46073b86e7181981ea598c9a8f\nBlueprint: cluster-secgroups\nCloses-Bug: #1364659\n'}]",18,118493,8001eb74a4febe4e5e1676a2236eee9b5b4612ac,30,11,4,8411,,,0,"[Sahara] Added ability to specify security groups for node group

Known issues:
1. Security groups added to the same page as general parameters. Page
   is a little overloaded now. Will improve this later.

Change-Id: Ieb224ea4db2adc46073b86e7181981ea598c9a8f
Blueprint: cluster-secgroups
Closes-Bug: #1364659
",git fetch https://review.opendev.org/openstack/horizon refs/changes/93/118493/4 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/api/sahara.py', 'openstack_dashboard/dashboards/project/data_processing/nodegroup_templates/templates/data_processing.nodegroup_templates/_details.html', 'openstack_dashboard/dashboards/project/data_processing/nodegroup_templates/workflows/create.py', 'openstack_dashboard/dashboards/project/data_processing/clusters/templates/data_processing.clusters/_nodegroups_details.html']",4,5c7cb937c6f3f8f6c23e539f79879bfa5190afe5,bug/1364659," <dt>{% trans ""Auto Security Group"" %}</dt> <dd>{{ node_group.auto_security_group }}</dd> <dt>{% trans ""Security Groups"" %}</dt> <dd> <ul class=""list-bullet""> {% for group in node_group.security_groups %} <li>{{ group }}</li> {% endfor %} </ul> </dd> ",,53,3
openstack%2Fzaqar~master~Id3b59532fbfd2d344728ee9be4a228811c1ffb82,openstack/zaqar,master,Id3b59532fbfd2d344728ee9be4a228811c1ffb82,Use OpenStack Incubated branding for docs,MERGED,2014-09-29 09:01:07.000000000,2014-09-29 12:59:28.000000000,2014-09-29 12:59:28.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 6159}, {'_account_id': 6413}, {'_account_id': 6484}, {'_account_id': 6547}, {'_account_id': 7488}]","[{'number': 1, 'created': '2014-09-29 09:01:07.000000000', 'files': ['doc/source/conf.py'], 'web_link': 'https://opendev.org/openstack/zaqar/commit/60cf3bc797fe9ba1a289ab745439e5342df46d2f', 'message': 'Use OpenStack Incubated branding for docs\n\nZaqar is not integrated but incubated. The branding of the web pages at\nhttp://docs.openstack.org/developer/zaqar/ should thus be the incubated\nbranding.\n\nChange-Id: Id3b59532fbfd2d344728ee9be4a228811c1ffb82\n'}]",0,124688,60cf3bc797fe9ba1a289ab745439e5342df46d2f,10,7,1,6547,,,0,"Use OpenStack Incubated branding for docs

Zaqar is not integrated but incubated. The branding of the web pages at
http://docs.openstack.org/developer/zaqar/ should thus be the incubated
branding.

Change-Id: Id3b59532fbfd2d344728ee9be4a228811c1ffb82
",git fetch https://review.opendev.org/openstack/zaqar refs/changes/88/124688/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/conf.py'],1,60cf3bc797fe9ba1a289ab745439e5342df46d2f,incubated-branding,html_theme_options = {'incubating': True},#html_theme_options = {},1,1
openstack%2Foslo.vmware~master~Ie0635607847061c68c879e87db8f2541436f03d6,openstack/oslo.vmware,master,Ie0635607847061c68c879e87db8f2541436f03d6,Use custom transport adapter for file URLs,MERGED,2014-09-19 12:17:04.000000000,2014-09-29 12:58:38.000000000,2014-09-28 12:11:21.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 5367}, {'_account_id': 5638}, {'_account_id': 9171}]","[{'number': 1, 'created': '2014-09-19 12:17:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/1dc296239d2f9a0723e9f670b8a8be5592c732b7', 'message': ""Use custom transport adapter for file URLs\n\nThe requests library doesn't support local file URLs. The PBM WSDL is\nstored locally and this affects all PBM API invocations. This patch\nfixes the problem by registering a custom transport adapter for local\nfiles.\n\nRequests Version 1.2.1 doesn't support 'file:' URLs and raises\nInvalidURL. This patch updates the minimum version to 2.1.0 to fix\nthis.\n\nCloses-Bug: #1371575\nChange-Id: Ie0635607847061c68c879e87db8f2541436f03d6\n""}, {'number': 2, 'created': '2014-09-24 12:31:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/ef99591fdbf33c34eff6678be7950089a08bd3fe', 'message': ""Use custom transport adapter for file URLs\n\nThe requests library doesn't support local file URLs. The PBM WSDL is\nstored locally and this affects all PBM API invocations. This patch\nfixes the problem by registering a custom transport adapter for local\nfiles.\n\nRequests Version 1.2.1 doesn't support 'file:' URLs and raises\nInvalidURL. This patch updates the minimum version to 2.1.0 to fix\nthis.\n\nCloses-Bug: #1371575\nChange-Id: Ie0635607847061c68c879e87db8f2541436f03d6\n""}, {'number': 3, 'created': '2014-09-25 11:12:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/bb42187a77e125933eb95e221e5cffed23ce989c', 'message': ""Use custom transport adapter for file URLs\n\nThe requests library doesn't support local file URLs. The PBM WSDL is\nstored locally and this affects all PBM API invocations. This patch\nfixes the problem by registering a custom transport adapter for local\nfiles.\n\nRequests Version 1.2.1 doesn't support 'file:' URLs and raises\nInvalidURL. This patch updates the minimum version to 2.1.0 to fix\nthis.\n\nCloses-Bug: #1371575\nChange-Id: Ie0635607847061c68c879e87db8f2541436f03d6\n""}, {'number': 4, 'created': '2014-09-25 11:14:34.000000000', 'files': ['tests/test_service.py', 'oslo/vmware/service.py'], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/e55e677120d18e3204bc33570c9e4db916d27300', 'message': ""Use custom transport adapter for file URLs\n\nThe requests library doesn't support local file URLs. The PBM WSDL is\nstored locally and this affects all PBM API invocations. This patch\nfixes the problem by registering a custom transport adapter for local\nfiles.\n\nCloses-Bug: #1371575\nChange-Id: Ie0635607847061c68c879e87db8f2541436f03d6\n""}]",2,122716,e55e677120d18e3204bc33570c9e4db916d27300,23,5,4,9171,,,0,"Use custom transport adapter for file URLs

The requests library doesn't support local file URLs. The PBM WSDL is
stored locally and this affects all PBM API invocations. This patch
fixes the problem by registering a custom transport adapter for local
files.

Closes-Bug: #1371575
Change-Id: Ie0635607847061c68c879e87db8f2541436f03d6
",git fetch https://review.opendev.org/openstack/oslo.vmware refs/changes/16/122716/3 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'oslo/vmware/service.py']",2,1dc296239d2f9a0723e9f670b8a8be5592c732b7,bug/1371575,"import osclass Response(six.BytesIO): """"""Response with an input stream as source."""""" def __init__(self, stream, status=200, headers=None): self.status = status self.headers = headers or {} self.reason = requests.status_codes._codes.get( status, [''])[0].upper().replace('_', ' ') six.BytesIO.__init__(self, stream) @property def _original_response(self): return self @property def msg(self): return self def read(self, chunk_size, **kwargs): return six.BytesIO.read(self, chunk_size) def info(self): return self def get_all(self, name, default): result = self.headers.get(name) if not result: return default return [result] def getheaders(self, name): return self.get_all(name, []) def release_conn(self): self.close() class LocalFileAdapter(requests.adapters.HTTPAdapter): """"""Transport adapter for local files. See http://stackoverflow.com/a/22989322 """""" def _build_response_from_file(self, request): file_path = request.url[7:] with open(file_path, 'r') as file: buff = bytearray(os.path.getsize(file_path)) file.readinto(buff) resp = Response(buff) return self.build_response(request, resp) def send(self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None): return self._build_response_from_file(request) self.session.mount('file:///', LocalFileAdapter())",,59,1
openstack%2Fzaqar~master~If872da7399e81d7d90a42773143012fd82362c4c,openstack/zaqar,master,If872da7399e81d7d90a42773143012fd82362c4c,Fix typo in README,MERGED,2014-09-29 10:11:49.000000000,2014-09-29 12:51:14.000000000,2014-09-29 12:51:13.000000000,"[{'_account_id': 3}, {'_account_id': 6413}, {'_account_id': 7488}]","[{'number': 1, 'created': '2014-09-29 10:11:49.000000000', 'files': ['README.rst'], 'web_link': 'https://opendev.org/openstack/zaqar/commit/b293b114a342872094d21444a0394514e6e112ac', 'message': 'Fix typo in README\n\nChange-Id: If872da7399e81d7d90a42773143012fd82362c4c\nSigned-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>'}]",0,124702,b293b114a342872094d21444a0394514e6e112ac,7,3,1,6549,,,0,"Fix typo in README

Change-Id: If872da7399e81d7d90a42773143012fd82362c4c
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>",git fetch https://review.opendev.org/openstack/zaqar refs/changes/02/124702/1 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,b293b114a342872094d21444a0394514e6e112ac,, $ zaqar-bench -pp 4 -pw 10 -cp 4 -cw 20 -ow 0 -t 30, $ zaqar-bench -pp 4 -pw 10 -cw 4 -cw 20 -ow 0 -t 30,1,1
openstack%2Fpuppet-ceilometer~stable%2Ficehouse~Ic9457cb8ad96a0d39f7a29d3e593e788bbeb6d28,openstack/puppet-ceilometer,stable/icehouse,Ic9457cb8ad96a0d39f7a29d3e593e788bbeb6d28,Fix the mysql_grant call to not fail with puppetlabs-mysql 2.3,MERGED,2014-09-29 12:16:18.000000000,2014-09-29 12:36:04.000000000,2014-09-29 12:36:04.000000000,"[{'_account_id': 3}, {'_account_id': 3153}, {'_account_id': 6994}]","[{'number': 1, 'created': '2014-09-29 12:16:18.000000000', 'files': ['manifests/db/mysql/host_access.pp'], 'web_link': 'https://opendev.org/openstack/puppet-ceilometer/commit/0d724878bdb9d2dbc1262d40da23f18d3c2c0b8a', 'message': 'Fix the mysql_grant call to not fail with puppetlabs-mysql 2.3\n\nDue to a recent change in puppetlabs-mysql, the ""host_access"" call in\nmanifests/db/mysql/host_access.pp of the current puppet-ceilometer\nmodule doesn\'t work anymore. The problem is obvious -- this is the\nactual code:\n\n    mysql_grant { ""${user}@${name}/${database}"":\n      privileges => \'all\',\n      provider => \'mysql\',\n      table => ""${database}.*"",\n      user => ""${user}@${name}"",\n      require => Mysql_user[""${user}@${name}""]\n    }\n\nAs you will notice, mysql_grant now performs this check (as in\nhttps://github.com/puppetlabs/puppetlabs-mysql/commit/07b661dcea926981cf5cd1c703a1c982d6eb6ef1):\n\nfail(\'name must match user and table parameters\') if self[:name] != ""#{self[:user]}/#{self[:table]}\n\nThe problem here is that ""${database}"" is not equal to ""${database}.*"",\nwhich will make the mysql_grant call fail. The fix simply is to change\nthe function\'s name:\n\n    mysql_grant { ""${user}@${name}/${database}.*"":\n      privileges => \'all\',\n      provider => \'mysql\',\n      table => ""${database}.*"",\n      user => ""${user}@${name}"",\n      require => Mysql_user[""${user}@${name}""]\n    }\n\nThis commit conducts the proposed change.\n\nChange-Id: Ic9457cb8ad96a0d39f7a29d3e593e788bbeb6d28\nCloses-Bug: 1375233\n'}]",0,124723,0d724878bdb9d2dbc1262d40da23f18d3c2c0b8a,7,3,1,2792,,,0,"Fix the mysql_grant call to not fail with puppetlabs-mysql 2.3

Due to a recent change in puppetlabs-mysql, the ""host_access"" call in
manifests/db/mysql/host_access.pp of the current puppet-ceilometer
module doesn't work anymore. The problem is obvious -- this is the
actual code:

    mysql_grant { ""${user}@${name}/${database}"":
      privileges => 'all',
      provider => 'mysql',
      table => ""${database}.*"",
      user => ""${user}@${name}"",
      require => Mysql_user[""${user}@${name}""]
    }

As you will notice, mysql_grant now performs this check (as in
https://github.com/puppetlabs/puppetlabs-mysql/commit/07b661dcea926981cf5cd1c703a1c982d6eb6ef1):

fail('name must match user and table parameters') if self[:name] != ""#{self[:user]}/#{self[:table]}

The problem here is that ""${database}"" is not equal to ""${database}.*"",
which will make the mysql_grant call fail. The fix simply is to change
the function's name:

    mysql_grant { ""${user}@${name}/${database}.*"":
      privileges => 'all',
      provider => 'mysql',
      table => ""${database}.*"",
      user => ""${user}@${name}"",
      require => Mysql_user[""${user}@${name}""]
    }

This commit conducts the proposed change.

Change-Id: Ic9457cb8ad96a0d39f7a29d3e593e788bbeb6d28
Closes-Bug: 1375233
",git fetch https://review.opendev.org/openstack/puppet-ceilometer refs/changes/23/124723/1 && git format-patch -1 --stdout FETCH_HEAD,['manifests/db/mysql/host_access.pp'],1,0d724878bdb9d2dbc1262d40da23f18d3c2c0b8a,bug/1375233," mysql_grant { ""${user}@${name}/${database}.*"":"," mysql_grant { ""${user}@${name}/${database}"":",1,1
openstack%2Foslo.concurrency~master~I171df45b62beb9776e8dbb52e33a408a832bf86f,openstack/oslo.concurrency,master,I171df45b62beb9776e8dbb52e33a408a832bf86f,Add hacking import exception for i18n,MERGED,2014-09-26 16:04:16.000000000,2014-09-29 12:35:46.000000000,2014-09-29 12:35:45.000000000,"[{'_account_id': 3}, {'_account_id': 708}, {'_account_id': 1669}]","[{'number': 1, 'created': '2014-09-26 16:04:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.concurrency/commit/586ad64cdc8072a6283d301baa76ac59d5219516', 'message': ""Add hacking import exception for i18n\n\nThis is preferred over adding noqa's everywhere we import a\ntranslation function.\n\nChange-Id: I171df45b62beb9776e8dbb52e33a408a832bf86f\n""}, {'number': 2, 'created': '2014-09-26 17:33:49.000000000', 'files': ['oslo/concurrency/lockutils.py', 'oslo/concurrency/processutils.py', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/oslo.concurrency/commit/19dbcbbb896b3b3655daf4b88a4ec1ff76ae342c', 'message': ""Add hacking import exception for i18n\n\nThis is preferred over adding noqa's everywhere we import a\ntranslation function.\n\nChange-Id: I171df45b62beb9776e8dbb52e33a408a832bf86f\n""}]",0,124450,19dbcbbb896b3b3655daf4b88a4ec1ff76ae342c,8,3,2,6928,,,0,"Add hacking import exception for i18n

This is preferred over adding noqa's everywhere we import a
translation function.

Change-Id: I171df45b62beb9776e8dbb52e33a408a832bf86f
",git fetch https://review.opendev.org/openstack/oslo.concurrency refs/changes/50/124450/2 && git format-patch -1 --stdout FETCH_HEAD,"['oslo/concurrency/lockutils.py', 'oslo/concurrency/processutils.py', 'tox.ini']",3,586ad64cdc8072a6283d301baa76ac59d5219516,concurrency-cleanup, [hacking] import_exceptions = oslo.concurrency._i18n,,6,2
openstack%2Ffuel-ostf~master~If69a4292fcc66565ba969396d591e38eddba92ce,openstack/fuel-ostf,master,If69a4292fcc66565ba969396d591e38eddba92ce,Add logging to file for OSTF tests,MERGED,2014-07-18 09:17:23.000000000,2014-09-29 12:32:08.000000000,2014-09-29 12:32:08.000000000,"[{'_account_id': 3}, {'_account_id': 7195}, {'_account_id': 8392}, {'_account_id': 8749}, {'_account_id': 8907}, {'_account_id': 8931}, {'_account_id': 8954}, {'_account_id': 8971}, {'_account_id': 11082}, {'_account_id': 12200}]","[{'number': 1, 'created': '2014-07-18 09:17:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-ostf/commit/295662b4a7445bcb05cbbd80975e38d563d2f26d', 'message': 'Add logging to file for OSTF tests\n\nAfter that change  when performing a OSTF test its result will be\nlogged to a file under /var/log/ostf directory.\nEach file has name in format cluseter_CLUSTER_ID_TESTSET.log,\nfor example: cluster_1_smoke.log\n\nChange-Id: If69a4292fcc66565ba969396d591e38eddba92ce\nCloses-Bug: #1297958\n'}, {'number': 2, 'created': '2014-07-18 09:35:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-ostf/commit/0fa72c31841dc243b4efe883920763710bd65a26', 'message': 'Add logging to file for OSTF tests\n\nAfter that change  when performing a OSTF test its result will be\nlogged to a file under /var/log/ostf directory.\nEach file has name in format cluseter_CLUSTER_ID_TESTSET.log,\nfor example: cluster_1_smoke.log\n\n* added ResultsLogger for logging tests restuts\n* removed deprecation because of wrong import in models\n\nChange-Id: If69a4292fcc66565ba969396d591e38eddba92ce\nCloses-Bug: #1297958\n'}, {'number': 3, 'created': '2014-07-25 09:40:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-ostf/commit/f800a01311ec5dc72af7466ae162a9d60b9c5f30', 'message': 'Add logging to file for OSTF tests\n\nAfter that change  when performing a OSTF test its result will be\nlogged to a file under /var/log/ostf directory.\nEach file has name in format cluseter_CLUSTER_ID_TESTSET.log,\nfor example: cluster_1_smoke.log\n\n* added ResultsLogger for logging tests restuts\n* removed deprecation because of wrong import in models\n\nChange-Id: If69a4292fcc66565ba969396d591e38eddba92ce\nCloses-Bug: #1297958\n'}, {'number': 4, 'created': '2014-09-12 10:23:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-ostf/commit/f09fdbf6b122295747e4aaf927e09613a7365805', 'message': 'Add logging to file for OSTF tests\n\nAfter that change  when performing a OSTF test its result will be\nlogged to a file under /var/log/ostf directory.\nEach file has name in format cluseter_CLUSTER_ID_TESTSET.log,\nfor example: cluster_1_smoke.log\n\n* added ResultsLogger for logging tests restuts\n* removed deprecation because of wrong import in models\n\nChange-Id: If69a4292fcc66565ba969396d591e38eddba92ce\nCloses-Bug: #1297958\n'}, {'number': 5, 'created': '2014-09-29 08:25:18.000000000', 'files': ['fuel_plugin/ostf_adapter/storage/models.py', 'fuel_plugin/ostf_adapter/nose_plugin/nose_storage_plugin.py', 'fuel_plugin/ostf_adapter/nose_plugin/nose_adapter.py', 'fuel_plugin/ostf_adapter/nose_plugin/nose_utils.py', 'fuel_plugin/ostf_adapter/logger.py', 'fuel_plugin/testing/tests/unit/test_results_logger.py'], 'web_link': 'https://opendev.org/openstack/fuel-ostf/commit/727cd3cc93e255635fef13c80baee649ced075be', 'message': 'Add logging to file for OSTF tests\n\nAfter that change  when performing a OSTF test its result will be\nlogged to a file under /var/log/ostf directory.\nEach file has name in format cluseter_CLUSTER_ID_TESTSET.log,\nfor example: cluster_1_smoke.log\n\n* added ResultsLogger for logging tests restuts\n* removed deprecation because of wrong import in models\n\nChange-Id: If69a4292fcc66565ba969396d591e38eddba92ce\nCloses-Bug: #1297958\n'}]",2,107938,727cd3cc93e255635fef13c80baee649ced075be,41,10,5,12200,,,0,"Add logging to file for OSTF tests

After that change  when performing a OSTF test its result will be
logged to a file under /var/log/ostf directory.
Each file has name in format cluseter_CLUSTER_ID_TESTSET.log,
for example: cluster_1_smoke.log

* added ResultsLogger for logging tests restuts
* removed deprecation because of wrong import in models

Change-Id: If69a4292fcc66565ba969396d591e38eddba92ce
Closes-Bug: #1297958
",git fetch https://review.opendev.org/openstack/fuel-ostf refs/changes/38/107938/4 && git format-patch -1 --stdout FETCH_HEAD,"['fuel_plugin/ostf_adapter/nose_plugin/nose_storage_plugin.py', 'fuel_plugin/ostf_adapter/nose_plugin/nose_adapter.py', 'fuel_plugin/ostf_adapter/nose_plugin/nose_utils.py', 'fuel_plugin/ostf_adapter/logger.py', 'fuel_plugin/testing/tests/unit/test_results_logger.py']",5,295662b4a7445bcb05cbbd80975e38d563d2f26d,bug/1297958,"# -*- coding: utf-8 -*- # Copyright 2014 Mirantis, Inc. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import mock from unittest2 import TestCase from fuel_plugin.ostf_adapter.logger import ResultsLogger class TestResultsLogger(TestCase): def get_logger(self, **kwargs): options = { 'testset': 'testset', 'cluster_id': 1, } options.update(kwargs) return ResultsLogger(**options) def test_filename(self): logger = self.get_logger(testset='testset_name', cluster_id=99) expected = ""cluster_99_testset_name.log"" self.assertEqual(logger.filename, expected) def test_log_format_on_success(self): logger = self.get_logger() logger._logger = mock.Mock() logger.log_results( test_id='tests.successful.test', test_name='Successful test', status='SUCCESS', message='') expected = 'SUCCESS Successful test (tests.successful.test) ' logger._logger.info.assert_called_once_with(expected) def test_log_format_on_fail(self): logger = self.get_logger() logger._logger = mock.Mock() logger.log_results( test_id='tests.failing.test', test_name='Failing test', status='FAIL', message='Message after fail') expected = ('FAIL Failing test (tests.failing.test) ' 'Message after fail') logger._logger.info.assert_called_once_with(expected) def test_log_format_on_error(self): logger = self.get_logger() logger._logger = mock.Mock() logger.log_results( test_id='tests.error.test', test_name='Error test', status='ERROR', message='Message after error') expected = ('ERROR Error test (tests.error.test) ' 'Message after error') logger._logger.info.assert_called_once_with(expected) ",,159,20
openstack%2Fopenstack-manuals~master~I47f60f9c94aad3e3df2fe8c213b10b5312a19d3c,openstack/openstack-manuals,master,I47f60f9c94aad3e3df2fe8c213b10b5312a19d3c,Update new and deprecated options (except swift),MERGED,2014-09-28 19:20:22.000000000,2014-09-29 12:23:18.000000000,2014-09-29 12:23:17.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-09-28 19:20:22.000000000', 'files': ['doc/common/tables/glance-conf-changes.xml', 'doc/common/tables/nova-conf-changes.xml', 'doc/common/tables/trove-conf-changes.xml', 'doc/common/tables/keystone-conf-changes.xml', 'doc/common/tables/ceilometer-conf-changes.xml', 'doc/common/tables/heat-conf-changes.xml', 'doc/common/tables/cinder-conf-changes.xml', 'doc/common/tables/neutron-conf-changes.xml'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/c07345427886d98e38bf9d99f210a9409fa7a4bb', 'message': 'Update new and deprecated options (except swift)\n\nChange-Id: I47f60f9c94aad3e3df2fe8c213b10b5312a19d3c\n'}]",0,124647,c07345427886d98e38bf9d99f210a9409fa7a4bb,7,3,1,7923,,,0,"Update new and deprecated options (except swift)

Change-Id: I47f60f9c94aad3e3df2fe8c213b10b5312a19d3c
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/47/124647/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/common/tables/glance-conf-changes.xml', 'doc/common/tables/nova-conf-changes.xml', 'doc/common/tables/trove-conf-changes.xml', 'doc/common/tables/keystone-conf-changes.xml', 'doc/common/tables/ceilometer-conf-changes.xml', 'doc/common/tables/heat-conf-changes.xml', 'doc/common/tables/cinder-conf-changes.xml', 'doc/common/tables/neutron-conf-changes.xml']",8,c07345427886d98e38bf9d99f210a9409fa7a4bb,diff_branches," <td>[DEFAULT] agent_down_time = 75</td> <td>(IntOpt) Seconds to regard the agent is down; should be at least twice report_interval, to be sure the agent is down for good.</td> </tr> <tr> <td>[DEFAULT] dhcp_agents_per_network = 1</td> <td>(IntOpt) Number of DHCP agents scheduled to host a network.</td> <td>[DEFAULT] enable_metadata_proxy = True</td> <td>(BoolOpt) Allow running metadata proxy.</td> </tr> <tr> <td>[DEFAULT] gateway_external_network_id = </td> <td>(StrOpt) UUID of external network for routers implemented by the agents.</td> </tr> <tr> <td>[DEFAULT] ha_confs_path = $state_path/ha_confs</td> <td>(StrOpt) Location to store keepalived/conntrackd config files</td> </tr> <tr> <td>[DEFAULT] ha_vrrp_advert_int = 2</td> <td>(IntOpt) The advertisement interval in seconds</td> </tr> <tr> <td>[DEFAULT] ha_vrrp_auth_password = None</td> <td>(StrOpt) VRRP authentication password</td> </tr> <tr> <td>[DEFAULT] ha_vrrp_auth_type = PASS</td> <td>(StrOpt) VRRP authentication type AH/PASS</td> </tr> <tr> <td>[DEFAULT] handle_internal_only_routers = True</td> <td>(BoolOpt) Agent should implement routers with no gateway</td> <td>[DEFAULT] l3_ha = False</td> <td>(BoolOpt) Enable HA mode for virtual routers.</td> </tr> <tr> <td>[DEFAULT] l3_ha_net_cidr = 169.254.192.0/18</td> <td>(StrOpt) Subnet used for the l3 HA admin network.</td> </tr> <tr> <td>[DEFAULT] loadbalancer_pool_scheduler_driver = neutron.services.loadbalancer.agent_scheduler.ChanceScheduler</td> <td>(StrOpt) Driver to use for scheduling pool to a default loadbalancer agent</td> </tr> <tr> <td>[DEFAULT] max_l3_agents_per_router = 3</td> <td>(IntOpt) Maximum number of agents on which a router will be scheduled.</td> </tr> <tr> <td>[DEFAULT] max_routes = 30</td> <td>(IntOpt) Maximum number of routes</td> </tr> <tr> <td>[DEFAULT] metadata_port = 9697</td> <td>(IntOpt) TCP Port used by Neutron metadata namespace proxy.</td> </tr> <tr> <td>[DEFAULT] min_l3_agents_per_router = 2</td> <td>(IntOpt) Minimum number of agents on which a router will be scheduled.</td> </tr> <tr> <td>[DEFAULT] network_auto_schedule = True</td> <td>(BoolOpt) Allow auto scheduling networks to DHCP agent.</td> </tr> <tr> <td>[DEFAULT] network_scheduler_driver = neutron.scheduler.dhcp_agent_scheduler.ChanceScheduler</td> <td>(StrOpt) Driver to use for scheduling network to DHCP agent</td> </tr> <tr> <td>[DEFAULT] qpid_receiver_capacity = 1</td> <td>(IntOpt) The number of prefetched messages held by receiver.</td> </tr> <tr> <td>[DEFAULT] router_auto_schedule = True</td> <td>(BoolOpt) Allow auto scheduling of routers to L3 agent.</td> </tr> <tr> <td>[DEFAULT] router_delete_namespaces = False</td> <td>(BoolOpt) Delete namespace after removing a router.</td> </tr> <tr> <td>[DEFAULT] router_id = </td> <td>(StrOpt) If namespaces is disabled, the l3 agent can only configure a router that has the matching router ID.</td> <td>[DEFAULT] router_scheduler_driver = neutron.scheduler.l3_agent_scheduler.ChanceScheduler</td> <td>(StrOpt) Driver to use for scheduling router to a default L3 agent</td> </tr> <tr> <td>[DEFAULT] send_arp_for_ha = 3</td> <td>(IntOpt) Send this many gratuitous ARPs for HA setup, if less than or equal to 0, the feature is disabled</td> <td>[HYPERV] network_vlan_ranges = []</td> <td>(ListOpt) List of &lt;physical_network&gt;:&lt;vlan_min&gt;:&lt;vlan_max&gt; or &lt;physical_network&gt;</td> </tr> <tr> <td>[HYPERV] tenant_network_type = local</td> <td>(StrOpt) Network type for tenant networks (local, flat, vlan or none)</td> </tr> <tr> <td>[NSX_DHCP] default_lease_time = 43200</td> <td>(IntOpt) Default DHCP lease time</td> </tr> <tr> <td>[NSX_DHCP] domain_name = openstacklocal</td> <td>(StrOpt) Domain to use for building the hostnames</td> </tr> <tr> <td>[NSX_DHCP] extra_domain_name_servers = []</td> <td>(ListOpt) Comma separated list of additional domain name servers</td> </tr> <tr> <td>[NSX_LSN] sync_on_missing_data = False</td> <td>(BoolOpt) Pull LSN information from NSX in case it is missing from the local data store. This is useful to rebuild the local store in case of server recovery.</td> </tr> <tr> <td>[NSX_METADATA] metadata_server_address = 127.0.0.1</td> <td>(StrOpt) IP address used by Metadata server.</td> </tr> <tr> <td>[NSX_METADATA] metadata_server_port = 8775</td> <td>(IntOpt) TCP Port used by Metadata server.</td> </tr> <tr> <td>[NSX_METADATA] metadata_shared_secret = </td> <td>(StrOpt) Shared secret to sign instance-id request</td> </tr> <tr> <td>[PHYSICAL_INTERFACE] physical_interface = eth0</td> <td>(StrOpt) The network interface to use when creatinga port</td> </tr> <tr> <td>[QUOTAS] quota_firewall = 1</td> <td>(IntOpt) Number of firewalls allowed per tenant. A negative value means unlimited.</td> </tr> <tr> <td>[QUOTAS] quota_firewall_policy = 1</td> <td>(IntOpt) Number of firewall policies allowed per tenant. A negative value means unlimited.</td> </tr> <tr> <td>[QUOTAS] quota_firewall_rule = 100</td> <td>(IntOpt) Number of firewall rules allowed per tenant. A negative value means unlimited.</td> </tr> <tr> <td>[QUOTAS] quota_floatingip = 50</td> <td>(IntOpt) Number of floating IPs allowed per tenant. A negative value means unlimited.</td> </tr> <tr> <td>[QUOTAS] quota_health_monitor = -1</td> <td>(IntOpt) Number of health monitors allowed per tenant. A negative value means unlimited.</td> </tr> <tr> <td>[QUOTAS] quota_member = -1</td> <td>(IntOpt) Number of pool members allowed per tenant. A negative value means unlimited.</td> </tr> <tr> <td>[QUOTAS] quota_network_gateway = 5</td> <td>(IntOpt) Number of network gateways allowed per tenant, -1 for unlimited</td> </tr> <tr> <td>[QUOTAS] quota_packet_filter = 100</td> <td>(IntOpt) Number of packet_filters allowed per tenant, -1 for unlimited</td> </tr> <tr> <td>[QUOTAS] quota_pool = 10</td> <td>(IntOpt) Number of pools allowed per tenant. A negative value means unlimited.</td> </tr> <tr> <td>[QUOTAS] quota_router = 10</td> <td>(IntOpt) Number of routers allowed per tenant. A negative value means unlimited.</td> </tr> <tr> <td>[QUOTAS] quota_security_group = 10</td> <td>(IntOpt) Number of security groups allowed per tenant. A negative value means unlimited.</td> </tr> <tr> <td>[QUOTAS] quota_security_group_rule = 100</td> <td>(IntOpt) Number of security rules allowed per tenant. A negative value means unlimited.</td> </tr> <tr> <td>[QUOTAS] quota_vip = 10</td> <td>(IntOpt) Number of vips allowed per tenant. A negative value means unlimited.</td> </tr> <tr> <td>[SECURITYGROUP] enable_ipset = True</td> <td>(BoolOpt) Use ipset to speed-up the iptables based security groups.</td> </tr> <tr> <td>[SWITCH] address = </td> <td>(StrOpt) The address of the host to SSH to</td> </tr> <tr> <td>[SWITCH] ostype = NOS</td> <td>(StrOpt) Currently unused</td> </tr> <tr> <td>[SWITCH] password = </td> <td>(StrOpt) The SSH password to use</td> </tr> <tr> <td>[SWITCH] username = </td> <td>(StrOpt) The SSH username to use</td> </tr> <tr> <td>[cfg_agent] device_connection_timeout = 30</td> <td>(IntOpt) Time in seconds for connecting to a hosting device</td> </tr> <tr> <td>[cfg_agent] hosting_device_dead_timeout = 300</td> <td>(IntOpt) The time in seconds until a backlogged hosting device is presumed dead. This value should be set up high enough to recover from a period of connectivity loss or high load when the device may not be responding.</td> </tr> <tr> <td>[cfg_agent] routing_svc_helper_class = neutron.plugins.cisco.cfg_agent.service_helpers.routing_svc_helper.RoutingServiceHelper</td> <td>(StrOpt) Path of the routing service helper class.</td> </tr> <tr> <td>[cfg_agent] rpc_loop_interval = 10</td> <td>(IntOpt) Interval when the process_services() loop executes in seconds. This is when the config agent lets each service helper to process its neutron resources.</td> </tr> <tr> <td>(BoolOpt) If set, ignore any SSL validation issues.</td> <td>(StrOpt) Auth strategy for connecting to neutron in admin context.</td> <td>(StrOpt) CRD Auth URL.</td> <td>(StrOpt) CRD Service Password.</td> <td>(StrOpt) Region name for connecting to CRD Service in admin context.</td> <td>(StrOpt) CRD Tenant Name.</td> <td>(StrOpt) URL for connecting to CRD service.</td> <td>(IntOpt) Timeout value for connecting to CRD service in seconds.</td> <td>(StrOpt) CRD service Username.</td> <td>[netscaler_driver] netscaler_ncc_password = None</td> <td>(StrOpt) Password to login to the NetScaler Control Center Server.</td> </tr> <tr> <td>[netscaler_driver] netscaler_ncc_uri = None</td> <td>(StrOpt) The URL to reach the NetScaler Control Center Server.</td> </tr> <tr> <td>[netscaler_driver] netscaler_ncc_username = None</td> <td>(StrOpt) Username to login to the NetScaler Control Center Server.</td> </tr> <tr> <td>[plumgriddirector] director_server = localhost</td> <td>(StrOpt) PLUMgrid Director server to connect to</td> </tr> <tr> <td>[plumgriddirector] director_server_port = 8080</td> <td>(StrOpt) PLUMgrid Director server port to connect to</td> </tr> <tr> <td>[plumgriddirector] password = password</td> <td>(StrOpt) PLUMgrid Director admin password</td> </tr> <tr> <td>[plumgriddirector] servertimeout = 5</td> <td>(IntOpt) PLUMgrid Director server timeout</td> </tr> <tr> <td>[plumgriddirector] username = username</td> <td>(StrOpt) PLUMgrid Director admin username</td> </tr> <tr> <td>[radware] actions_to_skip = ['setup_l2_l3']</td> <td>(ListOpt) List of actions that are not pushed to the completion queue.</td> </tr> <tr> <tr> <td>[radware] l2_l3_ctor_params = {'ha_network_name': 'HA-Network', 'service': '_REPLACE_', 'ha_ip_pool_name': 'default', 'twoleg_enabled': '_REPLACE_', 'allocate_ha_ips': True, 'allocate_ha_vrrp': True}</td> <td>(DictOpt) Parameter for l2_l3 workflow constructor.</td> </tr> <tr> <td>[radware] l2_l3_setup_params = {'data_ip_address': '192.168.200.99', 'data_port': 1, 'gateway': '192.168.200.1', 'ha_port': 2, 'data_ip_mask': '255.255.255.0'}</td> <td>(DictOpt) Parameter for l2_l3 workflow setup.</td> </tr> <tr> <td>[radware] l2_l3_workflow_name = openstack_l2_l3</td> <td>(StrOpt) Name of l2_l3 workflow. Default: openstack_l2_l3.</td> </tr> <tr> <td>[radware] l4_action_name = BaseCreate</td> <td>(StrOpt) Name of the l4 workflow action. Default: BaseCreate.</td> </tr> <tr> <td>[radware] l4_workflow_name = openstack_l4</td> <td>(StrOpt) Name of l4 workflow. Default: openstack_l4.</td> </tr> <tr> <td>[radware] service_adc_type = VA</td> <td>(StrOpt) Service ADC type. Default: VA.</td> </tr> <tr> <td>[radware] service_adc_version = </td> <td>(StrOpt) Service ADC version.</td> </tr> <tr> <td>[radware] service_cache = 20</td> <td>(IntOpt) Size of service cache. Default: 20.</td> </tr> <tr> <td>[radware] service_compression_throughput = 100</td> <td>(IntOpt) Service compression throughput. Default: 100.</td> </tr> <tr> <td>[radware] service_ha_pair = False</td> <td>(BoolOpt) Enables or disables the Service HA pair. Default: False.</td> </tr> <tr> <td>[radware] service_isl_vlan = -1</td> <td>(IntOpt) A required VLAN for the interswitch link to use.</td> </tr> <tr> <td>[radware] service_resource_pool_ids = []</td> <td>(ListOpt) Resource pool IDs.</td> </tr> <tr> <td>[radware] service_session_mirroring_enabled = False</td> <td>(BoolOpt) Enable or disable Alteon interswitch link for stateful session failover. Default: False.</td> </tr> <tr> <td>[radware] service_ssl_throughput = 100</td> <td>(IntOpt) Service SSL throughput. Default: 100.</td> </tr> <tr> <td>[radware] service_throughput = 1000</td> <td>(IntOpt) Service throughput. Default: 1000.</td> </tr> <tr> <td>[radware] vdirect_address = None</td> <td>(StrOpt) IP address of vDirect server.</td> </tr> <tr> <td>[radware] vdirect_password = radware</td> <td>(StrOpt) vDirect user password.</td> </tr> <tr> <td>[radware] vdirect_user = vDirect</td> <td>(StrOpt) vDirect user name.</td> </tr> <tr> <td>[vpnagent] vpn_device_driver = ['neutron.services.vpn.device_drivers.ipsec.OpenSwanDriver']</td> <td>(MultiStrOpt) The vpn device drivers Neutron will use</td> </tr> <td>[DEFAULT] endpoint_type</td> <td>adminURL</td> <td>publicURL</td> </tr> <tr>"," <td>[DEFAULT] device_connection_timeout = 30</td> <td>(IntOpt) Time in seconds for connecting to a hosting device</td> <td>[DEFAULT] hosting_device_dead_timeout = 300</td> <td>(IntOpt) The time in seconds until a backlogged hosting device is presumed dead. This value should be set up high enough to recover from a period of connectivity loss or high load when the device may not be responding.</td> <td>[DEFAULT] routing_svc_helper_class = neutron.plugins.cisco.cfg_agent.service_helpers.routing_svc_helper.RoutingServiceHelper</td> <td>(StrOpt) Path of the routing service helper class.</td> <td>[DEFAULT] rpc_loop_interval = 10</td> <td>(IntOpt) Interval when the process_services() loop executes in seconds. This is when the config agent lets each service helper to process its neutron resources.</td> <td>(BoolOpt) If set, ignore any SSL validation issues</td> <td>(StrOpt) Auth strategy for connecting to neutron in admin context</td> <td>(StrOpt) CRD Auth URL</td> <td>(StrOpt) CRD Service Password</td> <td>(StrOpt) Region name for connecting to CRD Service in admin context</td> <td>(StrOpt) CRD Tenant Name</td> <td>(StrOpt) URL for connecting to CRD service</td> <td>(IntOpt) Timeout value for connecting to CRD service in seconds</td> <td>(StrOpt) CRD service Username</td> <td>[DEFAULT] send_arp_for_ha</td> <td>0</td> <td>3</td> </tr> <tr> <td>[QUOTAS] quota_firewall_rule</td> <td>-1</td> <td>100</td> </tr> <tr> <tr> <td>[radware] l2_l3_ctor_params</td> <td>{'ha_ip_pool_name': 'default', 'allocate_ha_vrrp': True, 'ha_network_name': 'HA-Network', 'service': '_REPLACE_', 'allocate_ha_ips': True}</td> <td>{'ha_network_name': 'HA-Network', 'service': '_REPLACE_', 'ha_ip_pool_name': 'default', 'twoleg_enabled': '_REPLACE_', 'allocate_ha_ips': True, 'allocate_ha_vrrp': True}</td> </tr>",783,128
openstack%2Fneutron~master~I5e6794dc33c64c8fe309d8e72a8af3385c7d4442,openstack/neutron,master,I5e6794dc33c64c8fe309d8e72a8af3385c7d4442,Mock out all RPC calls with a fixture,MERGED,2014-09-21 11:26:06.000000000,2014-09-29 12:20:32.000000000,2014-09-23 09:51:36.000000000,"[{'_account_id': 3}, {'_account_id': 2035}, {'_account_id': 4395}, {'_account_id': 5170}, {'_account_id': 5217}, {'_account_id': 5948}, {'_account_id': 6524}, {'_account_id': 6854}, {'_account_id': 7293}, {'_account_id': 7787}, {'_account_id': 8645}, {'_account_id': 8873}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-21 11:26:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/5fdffaa86a869f9a5f3a112d3103f0e508b286e7', 'message': 'Mock out RPC state report in OVS agent test\n\nMock out the _report_state call in the OVS agent unit test\nthat otherwise blocks for 15 seconds and eats up valuable\nunit test time.\n\nThis change results in a reduction from 330.8 seconds to 2.7 seconds\nfor the neutron.tests.unit.openvswitch.test_ovs_neutron_agent\ntest module.\n\nCloses-Bug: #1372076\nChange-Id: I5e6794dc33c64c8fe309d8e72a8af3385c7d4442\n'}, {'number': 2, 'created': '2014-09-21 11:30:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/bf96505726a21bfae6ac1fd3eedff38e1eea235b', 'message': 'Mock out RPC state report in OVS agent test\n\nMock out the _report_state call in the OVS agent unit test\nthat otherwise blocks for 15 seconds and eats up valuable\nunit test time. This issue was introduced in Change-ID\nIdd770a85a9eabff112d9613e75d8bb524020234a.\n\nThis change results in a reduction from 330.8 seconds to 2.7 seconds\nfor the neutron.tests.unit.openvswitch.test_ovs_neutron_agent\ntest module.\n\nCloses-Bug: #1372076\nChange-Id: I5e6794dc33c64c8fe309d8e72a8af3385c7d4442\n'}, {'number': 3, 'created': '2014-09-22 02:59:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/f69d4278704a7b15a68fd95cf5f6647a2590b3d7', 'message': 'Mock out RPC state report in OVS agent test\n\nMock out the rpc calls in the OVS agent unit tests\nthat otherwise block for 15 seconds and eat up valuable\nunit test time. This issue was introduced in Change-ID\nIdd770a85a9eabff112d9613e75d8bb524020234a.\n\nThis change results in a reduction from 330.8 seconds to 2.7 seconds\nfor the neutron.tests.unit.openvswitch.test_ovs_neutron_agent\ntest module.\n\nCloses-Bug: #1372076\nChange-Id: I5e6794dc33c64c8fe309d8e72a8af3385c7d4442\n'}, {'number': 4, 'created': '2014-09-22 10:50:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/4379915879a5d3b13c6c30340d9c1dfb3f54ad97', 'message': 'Mock out all RPC calls with a fixture\n\nMock out the rpc proxy calls used by various agents to\nprevent unit tests from blocking for 10+ seconds while waiting\nfor a timeout. This happened with the OVS agent unit tests\nrecently in Change-ID Idd770a85a9eabff112d9613e75d8bb524020234a.\n\nThis change results in a reduction from 330.8 seconds to 2.7 seconds\nfor the neutron.tests.unit.openvswitch.test_ovs_neutron_agent\ntest module.\n\nCloses-Bug: #1372076\nChange-Id: I5e6794dc33c64c8fe309d8e72a8af3385c7d4442\n'}, {'number': 5, 'created': '2014-09-22 11:10:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/47f7e5d82bec706eaa7a309602c2921a652f5aed', 'message': 'Mock out all RPC calls with a fixture\n\nMock out the rpc proxy calls used by various agents to\nprevent unit tests from blocking for 10+ seconds while waiting\nfor a timeout. This happened with the OVS agent unit tests\nrecently in Change-ID Idd770a85a9eabff112d9613e75d8bb524020234a.\n\nThis change results in a reduction from 330.8 seconds to 2.7 seconds\nfor the neutron.tests.unit.openvswitch.test_ovs_neutron_agent\ntest module.\n\nCloses-Bug: #1372076\nChange-Id: I5e6794dc33c64c8fe309d8e72a8af3385c7d4442\n'}, {'number': 6, 'created': '2014-09-22 20:55:06.000000000', 'files': ['neutron/tests/unit/openvswitch/test_ovs_neutron_agent.py', 'neutron/tests/base.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/086496bfc45e01cd2905a074d526a7d513bf4ec2', 'message': 'Mock out all RPC calls with a fixture\n\nMock out the rpc proxy calls used by various agents to\nprevent unit tests from blocking for 10+ seconds while waiting\nfor a timeout. This happened with the OVS agent unit tests\nrecently in Change-ID Idd770a85a9eabff112d9613e75d8bb524020234a.\n\nThis change results in a reduction from 330.8 seconds to 2.7 seconds\nfor the neutron.tests.unit.openvswitch.test_ovs_neutron_agent\ntest module.\n\nCloses-Bug: #1372076\nChange-Id: I5e6794dc33c64c8fe309d8e72a8af3385c7d4442\n'}]",5,122999,086496bfc45e01cd2905a074d526a7d513bf4ec2,129,27,6,7787,,,0,"Mock out all RPC calls with a fixture

Mock out the rpc proxy calls used by various agents to
prevent unit tests from blocking for 10+ seconds while waiting
for a timeout. This happened with the OVS agent unit tests
recently in Change-ID Idd770a85a9eabff112d9613e75d8bb524020234a.

This change results in a reduction from 330.8 seconds to 2.7 seconds
for the neutron.tests.unit.openvswitch.test_ovs_neutron_agent
test module.

Closes-Bug: #1372076
Change-Id: I5e6794dc33c64c8fe309d8e72a8af3385c7d4442
",git fetch https://review.opendev.org/openstack/neutron refs/changes/99/122999/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/unit/openvswitch/test_ovs_neutron_agent.py'],1,5fdffaa86a869f9a5f3a112d3103f0e508b286e7,bug/1372076," 'OVSNeutronAgent._report_state'), mock.patch('neutron.plugins.openvswitch.agent.ovs_neutron_agent.'",,2,0
openstack%2Ftripleo-ci~master~I45c69055ede1ac7e5d664e2feca912527d86568a,openstack/tripleo-ci,master,I45c69055ede1ac7e5d664e2feca912527d86568a,Perform heat resource-list on test fail,MERGED,2014-09-28 13:21:46.000000000,2014-09-29 12:05:52.000000000,2014-09-29 12:05:52.000000000,"[{'_account_id': 3}, {'_account_id': 1726}, {'_account_id': 9369}, {'_account_id': 10035}]","[{'number': 1, 'created': '2014-09-28 13:21:46.000000000', 'files': ['toci_devtest.sh'], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/2d7e86efa9d506bd58896d8c5da71caf81541f67', 'message': 'Perform heat resource-list on test fail\n\nOften CI runs fail with heat never reaching COMPLETED state. It is\nespecially hard to debug these failed runs when there are many nodes\n(such as the HA tests) without knowing what resources are still in\nprogress.\n\nChange-Id: I45c69055ede1ac7e5d664e2feca912527d86568a\n'}]",0,124630,2d7e86efa9d506bd58896d8c5da71caf81541f67,9,4,1,10035,,,0,"Perform heat resource-list on test fail

Often CI runs fail with heat never reaching COMPLETED state. It is
especially hard to debug these failed runs when there are many nodes
(such as the HA tests) without knowing what resources are still in
progress.

Change-Id: I45c69055ede1ac7e5d664e2feca912527d86568a
",git fetch https://review.opendev.org/openstack/tripleo-ci refs/changes/30/124630/1 && git format-patch -1 --stdout FETCH_HEAD,['toci_devtest.sh'],1,2d7e86efa9d506bd58896d8c5da71caf81541f67,feature/heat-resource-list-on-fail, heat resource-list $TRIPLEO_TEST,,1,0
openstack%2Fgnocchi~master~I7e068d31eabf94e9352bfe3604059e31a8567d13,openstack/gnocchi,master,I7e068d31eabf94e9352bfe3604059e31a8567d13,Switch user_id and project_id to be UUID,MERGED,2014-09-24 14:37:58.000000000,2014-09-29 12:05:41.000000000,2014-09-29 12:05:40.000000000,"[{'_account_id': 3}, {'_account_id': 2284}]","[{'number': 1, 'created': '2014-09-24 14:37:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/17c99354c36bcb39652c9e1ac386d68d3876bd7f', 'message': 'Switch user_id and project_id to be UUID\n\nChange-Id: I7e068d31eabf94e9352bfe3604059e31a8567d13\n'}, {'number': 2, 'created': '2014-09-25 14:06:23.000000000', 'files': ['gnocchi/tests/test_indexer.py', 'gnocchi/rest/__init__.py', 'gnocchi/indexer/sqlalchemy.py', 'gnocchi/tests/test_rest.py'], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/5f8c639f3b2e5caf5be4c1aea88c832d211165bb', 'message': 'Switch user_id and project_id to be UUID\n\nChange-Id: I7e068d31eabf94e9352bfe3604059e31a8567d13\n'}]",1,123746,5f8c639f3b2e5caf5be4c1aea88c832d211165bb,9,2,2,1669,,,0,"Switch user_id and project_id to be UUID

Change-Id: I7e068d31eabf94e9352bfe3604059e31a8567d13
",git fetch https://review.opendev.org/openstack/gnocchi refs/changes/46/123746/1 && git format-patch -1 --stdout FETCH_HEAD,"['gnocchi/tests/test_indexer.py', 'gnocchi/rest/__init__.py', 'gnocchi/indexer/sqlalchemy.py', 'gnocchi/tests/test_rest.py']",4,17c99354c36bcb39652c9e1ac386d68d3876bd7f,jd/uuid-as-user-project," ""user_id"": str(uuid.uuid4()), ""project_id"": str(uuid.uuid4()), ""user_id"": str(uuid.uuid4()), ""project_id"": str(uuid.uuid4()), ""user_id"": str(uuid.uuid4()), ""project_id"": str(uuid.uuid4()), ""user_id"": str(uuid.uuid4()), ""project_id"": str(uuid.uuid4()), ""user_id"": str(uuid.uuid4()), ""project_id"": str(uuid.uuid4()),"," ""user_id"": ""foo"", ""project_id"": ""bar"", ""user_id"": ""foo"", ""project_id"": ""bar"", ""user_id"": ""foo"", ""project_id"": ""bar"", ""user_id"": ""foo"", ""project_id"": ""bar"", ""user_id"": ""foo"", ""project_id"": ""bar"",",115,74
openstack%2Fgnocchi~master~I2d67be195a5fff6589637c02eab0d15f7a745c0a,openstack/gnocchi,master,I2d67be195a5fff6589637c02eab0d15f7a745c0a,Update to latest oslo-incubator,MERGED,2014-09-24 16:03:42.000000000,2014-09-29 12:01:32.000000000,2014-09-29 12:01:31.000000000,"[{'_account_id': 3}, {'_account_id': 2284}]","[{'number': 1, 'created': '2014-09-24 16:03:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/d438f27770438ad698424d63ef8abb15018e525b', 'message': 'Update to latest oslo-incubator\n\nChange-Id: I2d67be195a5fff6589637c02eab0d15f7a745c0a\n'}, {'number': 2, 'created': '2014-09-25 13:39:44.000000000', 'files': ['gnocchi/openstack/common/jsonutils.py', 'gnocchi/openstack/common/__init__.py', 'gnocchi/openstack/common/config/__init__.py', 'tools/config/check_uptodate.sh', 'gnocchi/openstack/common/strutils.py', 'tools/config/generate_sample.sh', 'gnocchi/openstack/common/config/generator.py', 'openstack-common.conf', 'gnocchi/openstack/common/importutils.py', 'gnocchi/openstack/common/log.py', 'gnocchi/openstack/common/timeutils.py'], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/a80f94eabf0cd62a66b404a4ec1e07798392950a', 'message': 'Update to latest oslo-incubator\n\nChange-Id: I2d67be195a5fff6589637c02eab0d15f7a745c0a\n'}]",0,123773,a80f94eabf0cd62a66b404a4ec1e07798392950a,8,2,2,1669,,,0,"Update to latest oslo-incubator

Change-Id: I2d67be195a5fff6589637c02eab0d15f7a745c0a
",git fetch https://review.opendev.org/openstack/gnocchi refs/changes/73/123773/1 && git format-patch -1 --stdout FETCH_HEAD,"['gnocchi/openstack/common/__init__.py', 'gnocchi/openstack/common/jsonutils.py', 'gnocchi/openstack/common/strutils.py', 'gnocchi/openstack/common/config/generator.py', 'gnocchi/openstack/common/importutils.py', 'gnocchi/openstack/common/log.py', 'gnocchi/openstack/common/timeutils.py']",7,d438f27770438ad698424d63ef8abb15018e525b,jd/update-oslo,,"# Copyright 2011 OpenStack Foundation. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """""" Time related utilities and helper functions. """""" import calendar import datetime import time import iso8601 import six # ISO 8601 extended time format with microseconds _ISO8601_TIME_FORMAT_SUBSECOND = '%Y-%m-%dT%H:%M:%S.%f' _ISO8601_TIME_FORMAT = '%Y-%m-%dT%H:%M:%S' PERFECT_TIME_FORMAT = _ISO8601_TIME_FORMAT_SUBSECOND def isotime(at=None, subsecond=False): """"""Stringify time in ISO 8601 format."""""" if not at: at = utcnow() st = at.strftime(_ISO8601_TIME_FORMAT if not subsecond else _ISO8601_TIME_FORMAT_SUBSECOND) tz = at.tzinfo.tzname(None) if at.tzinfo else 'UTC' st += ('Z' if tz == 'UTC' else tz) return st def parse_isotime(timestr): """"""Parse time from ISO 8601 format."""""" try: return iso8601.parse_date(timestr) except iso8601.ParseError as e: raise ValueError(six.text_type(e)) except TypeError as e: raise ValueError(six.text_type(e)) def strtime(at=None, fmt=PERFECT_TIME_FORMAT): """"""Returns formatted utcnow."""""" if not at: at = utcnow() return at.strftime(fmt) def parse_strtime(timestr, fmt=PERFECT_TIME_FORMAT): """"""Turn a formatted time back into a datetime."""""" return datetime.datetime.strptime(timestr, fmt) def normalize_time(timestamp): """"""Normalize time in arbitrary timezone to UTC naive object."""""" offset = timestamp.utcoffset() if offset is None: return timestamp return timestamp.replace(tzinfo=None) - offset def is_older_than(before, seconds): """"""Return True if before is older than seconds."""""" if isinstance(before, six.string_types): before = parse_strtime(before).replace(tzinfo=None) else: before = before.replace(tzinfo=None) return utcnow() - before > datetime.timedelta(seconds=seconds) def is_newer_than(after, seconds): """"""Return True if after is newer than seconds."""""" if isinstance(after, six.string_types): after = parse_strtime(after).replace(tzinfo=None) else: after = after.replace(tzinfo=None) return after - utcnow() > datetime.timedelta(seconds=seconds) def utcnow_ts(): """"""Timestamp version of our utcnow function."""""" if utcnow.override_time is None: # NOTE(kgriffs): This is several times faster # than going through calendar.timegm(...) return int(time.time()) return calendar.timegm(utcnow().timetuple()) def utcnow(): """"""Overridable version of utils.utcnow."""""" if utcnow.override_time: try: return utcnow.override_time.pop(0) except AttributeError: return utcnow.override_time return datetime.datetime.utcnow() def iso8601_from_timestamp(timestamp): """"""Returns an iso8601 formatted date from timestamp."""""" return isotime(datetime.datetime.utcfromtimestamp(timestamp)) utcnow.override_time = None def set_time_override(override_time=None): """"""Overrides utils.utcnow. Make it return a constant time or a list thereof, one at a time. :param override_time: datetime instance or list thereof. If not given, defaults to the current UTC time. """""" utcnow.override_time = override_time or datetime.datetime.utcnow() def advance_time_delta(timedelta): """"""Advance overridden time using a datetime.timedelta."""""" assert utcnow.override_time is not None try: for dt in utcnow.override_time: dt += timedelta except TypeError: utcnow.override_time += timedelta def advance_time_seconds(seconds): """"""Advance overridden time by seconds."""""" advance_time_delta(datetime.timedelta(0, seconds)) def clear_time_override(): """"""Remove the overridden time."""""" utcnow.override_time = None def marshall_now(now=None): """"""Make an rpc-safe datetime with microseconds. Note: tzinfo is stripped, but not required for relative times. """""" if not now: now = utcnow() return dict(day=now.day, month=now.month, year=now.year, hour=now.hour, minute=now.minute, second=now.second, microsecond=now.microsecond) def unmarshall_time(tyme): """"""Unmarshall a datetime dict."""""" return datetime.datetime(day=tyme['day'], month=tyme['month'], year=tyme['year'], hour=tyme['hour'], minute=tyme['minute'], second=tyme['second'], microsecond=tyme['microsecond']) def delta_seconds(before, after): """"""Return the difference between two timing objects. Compute the difference in seconds between two date, time, or datetime objects (as a float, to microsecond resolution). """""" delta = after - before return total_seconds(delta) def total_seconds(delta): """"""Return the total seconds of datetime.timedelta object. Compute total seconds of datetime.timedelta, datetime.timedelta doesn't have method total_seconds in Python2.6, calculate it manually. """""" try: return delta.total_seconds() except AttributeError: return ((delta.days * 24 * 3600) + delta.seconds + float(delta.microseconds) / (10 ** 6)) def is_soon(dt, window): """"""Determines if time is going to happen in the next window seconds. :param dt: the time :param window: minimum seconds to remain to consider the time not soon :return: True if expiration is within the given duration """""" soon = (utcnow() + datetime.timedelta(seconds=window)) return normalize_time(dt) <= soon ",64,627
openstack%2Fgnocchi~master~Iebef91ab771f445af7757d690dc6d232a5498cf3,openstack/gnocchi,master,Iebef91ab771f445af7757d690dc6d232a5498cf3,Fix pep8 errors,MERGED,2014-09-26 14:15:07.000000000,2014-09-29 11:59:12.000000000,2014-09-29 11:59:12.000000000,"[{'_account_id': 3}, {'_account_id': 2284}]","[{'number': 1, 'created': '2014-09-26 14:15:07.000000000', 'files': ['gnocchi/rest/__init__.py', 'gnocchi/indexer/sqlalchemy.py', 'gnocchi/carbonara.py', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/gnocchi/commit/1cfcbf088b7e0a1d93046857ec8be8778a9c4ea3', 'message': 'Fix pep8 errors\n\nChange-Id: Iebef91ab771f445af7757d690dc6d232a5498cf3\nSigned-off-by: Julien Danjou <julien@danjou.info>\n'}]",0,124414,1cfcbf088b7e0a1d93046857ec8be8778a9c4ea3,6,2,1,1669,,,0,"Fix pep8 errors

Change-Id: Iebef91ab771f445af7757d690dc6d232a5498cf3
Signed-off-by: Julien Danjou <julien@danjou.info>
",git fetch https://review.opendev.org/openstack/gnocchi refs/changes/14/124414/1 && git format-patch -1 --stdout FETCH_HEAD,"['gnocchi/rest/__init__.py', 'gnocchi/indexer/sqlalchemy.py', 'gnocchi/carbonara.py', 'tox.ini']",4,1cfcbf088b7e0a1d93046857ec8be8778a9c4ea3,jd/fix-pep8,"exclude = .tox,doc,gnocchi/openstack/common","ignore = E126,H405,H904 exclude = .tox,doc",16,17
openstack%2Fkeystone~master~I1a79c252916c4459fdc75a6e022d72c0c14582ef,openstack/keystone,master,I1a79c252916c4459fdc75a6e022d72c0c14582ef,test_v3_token_id correctly hash token,MERGED,2014-03-28 00:14:11.000000000,2014-09-29 11:48:40.000000000,2014-03-28 19:40:16.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 2903}]","[{'number': 1, 'created': '2014-03-28 00:14:11.000000000', 'files': ['keystone/tests/test_v3_auth.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/5257f65e57b4387fe93a2fb30983f2e1d856e609', 'message': ""test_v3_token_id correctly hash token\n\nThe test_v3_token_id test was calculating the token hash by\nparsing the response JSON and generating a new JSON document.\nThis isn't guaranteed to work because a Python dict can have its\nfields in any order[1], so the new JSON document may not match the\nresponse JSON and you'd get a different hash. The correct way to\ngenerate a token hash is to use the response as is.\n\n[1] http://docs.python.org/2/library/stdtypes.html#dict.items\n\nChange-Id: I1a79c252916c4459fdc75a6e022d72c0c14582ef\n""}]",0,83628,5257f65e57b4387fe93a2fb30983f2e1d856e609,8,3,1,6486,,,0,"test_v3_token_id correctly hash token

The test_v3_token_id test was calculating the token hash by
parsing the response JSON and generating a new JSON document.
This isn't guaranteed to work because a Python dict can have its
fields in any order[1], so the new JSON document may not match the
response JSON and you'd get a different hash. The correct way to
generate a token hash is to use the response as is.

[1] http://docs.python.org/2/library/stdtypes.html#dict.items

Change-Id: I1a79c252916c4459fdc75a6e022d72c0c14582ef
",git fetch https://review.opendev.org/openstack/keystone refs/changes/28/83628/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/tests/test_v3_auth.py'],1,5257f65e57b4387fe93a2fb30983f2e1d856e609,bug/1174499," expected_token_id = cms.cms_sign_token(resp.body,","import json expected_token_id = cms.cms_sign_token(json.dumps(token_data),",1,2
openstack%2Frally~master~Id7d0fce04e978a684bc87fb3297b2010d8361972,openstack/rally,master,Id7d0fce04e978a684bc87fb3297b2010d8361972,Fixes InvalidBenchmarkConfig error while running tempest scenario,MERGED,2014-09-25 11:57:28.000000000,2014-09-29 11:47:01.000000000,2014-09-29 11:47:00.000000000,"[{'_account_id': 3}, {'_account_id': 6172}, {'_account_id': 7369}, {'_account_id': 9545}, {'_account_id': 10475}]","[{'number': 1, 'created': '2014-09-25 11:57:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/556b6417b264ae0310b3ca2c5715e196ccc426d6', 'message': 'Fixes InvalidBenchmarkConfig error while running tempest scenario\n\nChange-Id: Id7d0fce04e978a684bc87fb3297b2010d8361972\nCloses-Bug: #1372784\n'}, {'number': 2, 'created': '2014-09-25 16:15:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/13537f8df17b5d61812b2749c642682f69ee66de', 'message': 'Fixes InvalidBenchmarkConfig error while running tempest scenario\n\nThis patch also adds new classes into tests.fakes:\n     * FakeObject - simplifies creating of simple objects\n     * FakeDbTask - fake task model from db backend\n     * FakeDb - initial implementation of rally.db module\n\nChange-Id: Id7d0fce04e978a684bc87fb3297b2010d8361972\nCloses-Bug: #1372784\n'}, {'number': 3, 'created': '2014-09-25 16:24:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/96473e09683d52e58cc4f39a504c71fceba68cf1', 'message': 'Fixes InvalidBenchmarkConfig error while running tempest scenario\n\nThis patch also adds new classes into tests.fakes:\n     * FakeObject - simplifies creating of simple objects\n     * FakeDbTask - fake task model from db backend\n     * FakeDb - initial implementation of rally.db module\n\nChange-Id: Id7d0fce04e978a684bc87fb3297b2010d8361972\nCloses-Bug: #1372784\n'}, {'number': 4, 'created': '2014-09-26 11:50:28.000000000', 'files': ['rally/benchmark/validation.py', 'tests/benchmark/test_validation.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/61dfb24c5f4bfbe15f62ce0ccfe0906a0616a73b', 'message': 'Fixes InvalidBenchmarkConfig error while running tempest scenario\n\nChange-Id: Id7d0fce04e978a684bc87fb3297b2010d8361972\nCloses-Bug: #1372784\n'}]",4,124028,61dfb24c5f4bfbe15f62ce0ccfe0906a0616a73b,32,5,4,10475,,,0,"Fixes InvalidBenchmarkConfig error while running tempest scenario

Change-Id: Id7d0fce04e978a684bc87fb3297b2010d8361972
Closes-Bug: #1372784
",git fetch https://review.opendev.org/openstack/rally refs/changes/28/124028/4 && git format-patch -1 --stdout FETCH_HEAD,['rally/benchmark/validation.py'],1,556b6417b264ae0310b3ca2c5715e196ccc426d6,bug/1372784, verifier = tempest.Tempest(task.task.deployment_uuid), verifier = tempest.Tempest(task.deployment_uuid),1,2
openstack%2Fceilometer~master~I481eea51799b6b23f7f80ff151c61a89a1cf6565,openstack/ceilometer,master,I481eea51799b6b23f7f80ff151c61a89a1cf6565,Remove explicit zeroizing of PYTHONHASHSEED,ABANDONED,2014-09-29 10:31:02.000000000,2014-09-29 11:32:29.000000000,,[],"[{'number': 1, 'created': '2014-09-29 10:31:02.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/d96e432cee3ce00fb8df33e72f9dfc8ff66b3430', 'message': 'Remove explicit zeroizing of PYTHONHASHSEED\n\nJust smoking out whether there are remaining failures in the gate.\n\nChange-Id: I481eea51799b6b23f7f80ff151c61a89a1cf6565\nRelated-Bug: #1348818\n'}]",0,124703,d96e432cee3ce00fb8df33e72f9dfc8ff66b3430,2,0,1,2284,,,0,"Remove explicit zeroizing of PYTHONHASHSEED

Just smoking out whether there are remaining failures in the gate.

Change-Id: I481eea51799b6b23f7f80ff151c61a89a1cf6565
Related-Bug: #1348818
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/03/124703/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,d96e432cee3ce00fb8df33e72f9dfc8ff66b3430,,, PYTHONHASHSEED=0,0,1
openstack%2Foslo.utils~master~Ia723490a02b1816fe96c3a02e069d57f4d97d940,openstack/oslo.utils,master,Ia723490a02b1816fe96c3a02e069d57f4d97d940,Add ip address validation,MERGED,2014-09-23 16:07:05.000000000,2014-09-29 11:25:53.000000000,2014-09-29 11:25:52.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 1669}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 6159}, {'_account_id': 6348}, {'_account_id': 6928}, {'_account_id': 7491}, {'_account_id': 12363}]","[{'number': 1, 'created': '2014-09-23 16:07:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.utils/commit/8d1cbe51d1870be99a426d9d8c0f7341e3650b78', 'message': 'Add ip address validation\n\nChange-Id: Ia723490a02b1816fe96c3a02e069d57f4d97d940\n'}, {'number': 2, 'created': '2014-09-24 12:12:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.utils/commit/866d8006a6ae48c0d3309495d070021d1307909a', 'message': 'Add ip address validation\n\nThere are many projects on OpenStack that use\nip validation. It would be nice to have function\nfor validation in common module.\n\nChange-Id: Ia723490a02b1816fe96c3a02e069d57f4d97d940\n'}, {'number': 3, 'created': '2014-09-25 13:59:30.000000000', 'files': ['requirements.txt', 'oslo/utils/netutils.py', 'tests/test_netutils.py'], 'web_link': 'https://opendev.org/openstack/oslo.utils/commit/baacebcd46e1d480c48bf6eae87256661e8ac441', 'message': 'Add ip address validation\n\nThere are many projects on OpenStack that use\nip validation. It would be nice to have function\nfor validation in common module.\n\nChange-Id: Ia723490a02b1816fe96c3a02e069d57f4d97d940\n'}]",2,123497,baacebcd46e1d480c48bf6eae87256661e8ac441,22,10,3,12363,,,0,"Add ip address validation

There are many projects on OpenStack that use
ip validation. It would be nice to have function
for validation in common module.

Change-Id: Ia723490a02b1816fe96c3a02e069d57f4d97d940
",git fetch https://review.opendev.org/openstack/oslo.utils refs/changes/97/123497/3 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'oslo/utils/netutils.py', 'tests/test_netutils.py']",3,8d1cbe51d1870be99a426d9d8c0f7341e3650b78,add_ip_validation, def test_is_valid_ipv4(self): self.assertTrue(netutils.is_valid_ipv4('42.42.42.42')) self.assertFalse(netutils.is_valid_ipv4('-1.11.11.11')) def test_is_valid_ipv6(self): self.assertTrue(netutils.is_valid_ipv6('::1')) self.assertFalse(netutils.is_valid_ipv6( '1fff::a88:85a3::172.31.128.1')),,29,0
openstack%2Fzaqar~master~I202b3ead24a0ba018fcb70b59318751dc0732515,openstack/zaqar,master,I202b3ead24a0ba018fcb70b59318751dc0732515,Enable ZAQAR_TEST_SLOW for py27,MERGED,2014-09-19 13:21:05.000000000,2014-09-29 11:02:34.000000000,2014-09-29 11:02:34.000000000,"[{'_account_id': 3}, {'_account_id': 6159}, {'_account_id': 6413}, {'_account_id': 6484}]","[{'number': 1, 'created': '2014-09-19 13:21:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zaqar/commit/18d4dcf22efacfb4c18681cdc9b9793d667c48fc', 'message': ""Enable ZAQAR_TEST_SLOW for py27\n\nWe're testing Mongodb for py27 jobs, use this same gate to run slow\ntests.\n\nChange-Id: I202b3ead24a0ba018fcb70b59318751dc0732515\nCloses-bug: #1350069\n""}, {'number': 2, 'created': '2014-09-22 20:03:01.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/zaqar/commit/9748a489cb8a1fa72814236b4f7bc485b7727fea', 'message': ""Enable ZAQAR_TEST_SLOW for py27\n\nWe're testing Mongodb for py27 jobs, use this same gate to run slow\ntests.\n\nChange-Id: I202b3ead24a0ba018fcb70b59318751dc0732515\nCloses-bug: #1350069\n""}]",0,122732,9748a489cb8a1fa72814236b4f7bc485b7727fea,11,4,2,6159,,,0,"Enable ZAQAR_TEST_SLOW for py27

We're testing Mongodb for py27 jobs, use this same gate to run slow
tests.

Change-Id: I202b3ead24a0ba018fcb70b59318751dc0732515
Closes-bug: #1350069
",git fetch https://review.opendev.org/openstack/zaqar refs/changes/32/122732/2 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,18d4dcf22efacfb4c18681cdc9b9793d667c48fc,, ZAQAR_TEST_SLOW=1,,1,0
openstack%2Fmistral~master~I1d8dc81298495da3ab18a2dcacf1d70de5b7c703,openstack/mistral,master,I1d8dc81298495da3ab18a2dcacf1d70de5b7c703,Style changes in Scheduler and its tests,MERGED,2014-09-25 19:00:14.000000000,2014-09-29 10:58:11.000000000,2014-09-29 10:58:10.000000000,"[{'_account_id': 3}, {'_account_id': 7700}, {'_account_id': 8731}, {'_account_id': 9432}]","[{'number': 1, 'created': '2014-09-25 19:00:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/57a49741665365deacde2c6a0d3a38590a1cbda7', 'message': 'Style changes in Scheduler and its tests\n\nChange-Id: I1d8dc81298495da3ab18a2dcacf1d70de5b7c703\n'}, {'number': 2, 'created': '2014-09-26 07:17:21.000000000', 'files': ['mistral/services/scheduler.py', 'mistral/tests/unit/services/test_scheduler.py', 'AUTHORS'], 'web_link': 'https://opendev.org/openstack/mistral/commit/55ff358703d524b95cf51be2fd306d8390be3b11', 'message': 'Style changes in Scheduler and its tests\n\n* Style changes in Scheduler and its tests\n* Modified AUTHORS\n\nChange-Id: I1d8dc81298495da3ab18a2dcacf1d70de5b7c703\n'}]",9,124133,55ff358703d524b95cf51be2fd306d8390be3b11,13,4,2,8731,,,0,"Style changes in Scheduler and its tests

* Style changes in Scheduler and its tests
* Modified AUTHORS

Change-Id: I1d8dc81298495da3ab18a2dcacf1d70de5b7c703
",git fetch https://review.opendev.org/openstack/mistral refs/changes/33/124133/2 && git format-patch -1 --stdout FETCH_HEAD,"['mistral/services/scheduler.py', 'mistral/tests/unit/services/test_scheduler.py', 'AUTHORS']",3,57a49741665365deacde2c6a0d3a38590a1cbda7,syle_changes_in_scheduler,Anastasia Kuznetsova <akuznetsova@mirantis.com>Christian Berendt <berendt@b1-systems.de>Jeremy Stanley <fungi@yuggoth.org>Ray Chen <chenrano2002@gmail.com>Bryan Havenstein <bryan.havenstein@ericsson.com>, ,80,59
openstack%2Fneutron~master~I423b68aff58486c113d0e5c5f4726f9eabf6920e,openstack/neutron,master,I423b68aff58486c113d0e5c5f4726f9eabf6920e,Do not assume order of body and tags elements,MERGED,2014-08-06 09:30:03.000000000,2014-09-29 10:57:57.000000000,2014-09-27 22:42:56.000000000,"[{'_account_id': 3}, {'_account_id': 490}, {'_account_id': 704}, {'_account_id': 748}, {'_account_id': 1923}, {'_account_id': 4395}, {'_account_id': 5170}, {'_account_id': 6524}, {'_account_id': 6635}, {'_account_id': 6637}, {'_account_id': 6638}, {'_account_id': 6659}, {'_account_id': 7249}, {'_account_id': 7293}, {'_account_id': 7787}, {'_account_id': 8124}, {'_account_id': 8213}, {'_account_id': 8645}, {'_account_id': 9008}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10068}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12040}, {'_account_id': 12444}, {'_account_id': 12614}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-08-06 09:30:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/bcb3418bfbd30ac01d48dca2285d698e8cad16bd', 'message': 'Do not assume order of tags list elements\n\nThis fixes the l2gateway unit test that breaks with a randomized PYTHONHASHSEED\n(see the bug report).\n\nThe test assumed that the tags list from self._create_expected_req_body had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\nThe fix refactors the test case to handle an unsorted tags list.\n\nPartial-bug: #1348818\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: I423b68aff58486c113d0e5c5f4726f9eabf6920e\n'}, {'number': 2, 'created': '2014-08-06 09:47:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/0719db0ad3f34499155c17b35210c59cf1a70a93', 'message': 'Do not assume order of tags list elements\n\nThis fixes the l2gateway unit test that breaks with a randomized PYTHONHASHSEED\n(see the bug report).\n\nThe test assumed that the tags list from self._create_expected_req_body had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\nThe fix refactors the test case to handle an unsorted tags list.\n\nPartial-bug: #1348818\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: I423b68aff58486c113d0e5c5f4726f9eabf6920e\n'}, {'number': 3, 'created': '2014-08-06 10:28:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/69787a053898708314e7bc302f65d1047f898dcd', 'message': 'Do not assume order of tags list elements\n\nThis fixes the l2gateway unit test that breaks with a randomized PYTHONHASHSEED\n(see the bug report).\n\nThe test assumed that the tags list from self._create_expected_req_body had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\nThe fix refactors the test case to handle an unsorted tags list.\n\nPartial-bug: #1348818\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: I423b68aff58486c113d0e5c5f4726f9eabf6920e\n'}, {'number': 4, 'created': '2014-08-06 15:01:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e00631ab58c560caf4909a65267521acc6297c40', 'message': 'Do not assume order of tags list elements\n\nThis fixes the l2gateway unit test that breaks with a randomized PYTHONHASHSEED\n(see the bug report).\n\nThe test assumed that the tags list from self._create_expected_req_body had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\nThe fix refactors the test case to handle an unsorted tags list, and also\nbody dict enclosing it.\n\nPartial-bug: #1348818\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: I423b68aff58486c113d0e5c5f4726f9eabf6920e\n'}, {'number': 5, 'created': '2014-08-18 08:27:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/f513a2b971a38681bd6c721a8de87be93dde76c7', 'message': 'Do not assume order of tags list elements\n\nThis fixes the l2gateway unit test that breaks with a randomized PYTHONHASHSEED\n(see the bug report).\n\nThe test assumed that the tags list from self._create_expected_req_body had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\nThe fix refactors the test case to handle an unsorted tags list, and also\nbody dict enclosing it.\n\nPartial-bug: #1348818\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: I423b68aff58486c113d0e5c5f4726f9eabf6920e\n'}, {'number': 6, 'created': '2014-09-22 08:52:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/560f56557c04deb10903254f2cbf3f0fdacef594', 'message': 'Do not assume order of tags list elements\n\nThis fixes the l2gateway unit test that breaks with a randomized PYTHONHASHSEED\n(see the bug report).\n\nThe test assumed that the tags list from self._create_expected_req_body had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\nThe fix refactors the test case to handle an unsorted tags list, and also\nbody dict enclosing it.\n\nPartial-bug: #1348818\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: I423b68aff58486c113d0e5c5f4726f9eabf6920e\n'}, {'number': 7, 'created': '2014-09-23 16:42:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/0deab650f7f8bb4517e6f6ee0db99b23e84cc7a5', 'message': 'Do not assume order of tags list elements\n\nThis fixes the l2gateway unit test that breaks with a randomized PYTHONHASHSEED\n(see the bug report).\n\nThe test assumed that the tags list from self._create_expected_req_body had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\nThe fix refactors the test case to handle an unsorted tags list, and also\nbody dict enclosing it.\n\nPartial-bug: #1348818\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: I423b68aff58486c113d0e5c5f4726f9eabf6920e\n'}, {'number': 8, 'created': '2014-09-23 17:46:20.000000000', 'files': ['neutron/tests/unit/vmware/nsxlib/test_l2gateway.py', 'neutron/plugins/vmware/nsxlib/l2gateway.py', 'neutron/plugins/vmware/common/utils.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/d5b90315d2ff647179fd91512377a7f21a57ef0a', 'message': 'Do not assume order of body and tags elements\n\nThis fixes the l2gateway unit test that breaks with a randomized PYTHONHASHSEED\n(see the bug report).\n\nThe test assumed that the body dict from self._create_expected_req_body\nhad elements (including contents of tags list) in a particular order.\nFound with PYTHONHASHSEED=2455351445.\nThe fix ensures that body is in predictable order.\n\nPartial-bug: #1348818\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: I423b68aff58486c113d0e5c5f4726f9eabf6920e\n'}]",7,112248,d5b90315d2ff647179fd91512377a7f21a57ef0a,171,40,8,12737,,,0,"Do not assume order of body and tags elements

This fixes the l2gateway unit test that breaks with a randomized PYTHONHASHSEED
(see the bug report).

The test assumed that the body dict from self._create_expected_req_body
had elements (including contents of tags list) in a particular order.
Found with PYTHONHASHSEED=2455351445.
The fix ensures that body is in predictable order.

Partial-bug: #1348818
Note: There are several other unrelated unit tests that also break with a
randomized PYTHONHASHSEED, but they are not addressed here. They will be
addressed in separate patches.

Change-Id: I423b68aff58486c113d0e5c5f4726f9eabf6920e
",git fetch https://review.opendev.org/openstack/neutron refs/changes/48/112248/8 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/unit/vmware/nsxlib/test_l2gateway.py'],1,bcb3418bfbd30ac01d48dca2285d698e8cad16bd,bug/1348818," body.get(""tags"").sort();",,1,0
openstack%2Fheat~master~Ie7cf788a00660efae578b6444ee231876025ebba,openstack/heat,master,Ie7cf788a00660efae578b6444ee231876025ebba,Handle clients that don't accept auth_url=None,MERGED,2014-09-26 18:07:17.000000000,2014-09-29 10:57:10.000000000,2014-09-29 10:57:09.000000000,"[{'_account_id': 3}, {'_account_id': 3098}, {'_account_id': 4715}, {'_account_id': 6577}, {'_account_id': 7193}, {'_account_id': 7253}, {'_account_id': 7256}, {'_account_id': 8289}, {'_account_id': 8328}, {'_account_id': 8435}]","[{'number': 1, 'created': '2014-09-26 18:07:17.000000000', 'files': ['heat/engine/clients/os/cinder.py', 'heat/engine/clients/os/trove.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/8d4b3ef9ac7ff5d52ae9a81d22ba1d1c07b6e749', 'message': ""Handle clients that don't accept auth_url=None\n\nSome clients fail when passed None as the auth_url. Pass them an empty\nstring instead when the auth_url is None.\n\nChange-Id: Ie7cf788a00660efae578b6444ee231876025ebba\nCloses-Bug: #1332991\nCloses-Bug: #1334492\n""}]",0,124481,8d4b3ef9ac7ff5d52ae9a81d22ba1d1c07b6e749,13,10,1,4257,,,0,"Handle clients that don't accept auth_url=None

Some clients fail when passed None as the auth_url. Pass them an empty
string instead when the auth_url is None.

Change-Id: Ie7cf788a00660efae578b6444ee231876025ebba
Closes-Bug: #1332991
Closes-Bug: #1334492
",git fetch https://review.opendev.org/openstack/heat refs/changes/81/124481/1 && git format-patch -1 --stdout FETCH_HEAD,"['heat/engine/clients/os/cinder.py', 'heat/engine/clients/os/trove.py']",2,8d4b3ef9ac7ff5d52ae9a81d22ba1d1c07b6e749,," 'auth_url': con.auth_url or '',"," 'auth_url': con.auth_url,",2,2
openstack%2Fmistral~master~I197f7dc1d97188c3bc9f6f3089bd29877bf6fa3c,openstack/mistral,master,I197f7dc1d97188c3bc9f6f3089bd29877bf6fa3c,Adding 'tags' to action rest resource,MERGED,2014-09-25 22:34:29.000000000,2014-09-29 10:55:05.000000000,2014-09-29 10:55:05.000000000,"[{'_account_id': 3}, {'_account_id': 7700}, {'_account_id': 8731}, {'_account_id': 9432}]","[{'number': 1, 'created': '2014-09-25 22:34:29.000000000', 'files': ['mistral/api/controllers/v2/action.py', 'mistral/tests/unit/api/v2/test_actions.py'], 'web_link': 'https://opendev.org/openstack/mistral/commit/7c406e4cbc750ef52a2444fec189b499f59d9486', 'message': ""Adding 'tags' to action rest resource\n\nChange-Id: I197f7dc1d97188c3bc9f6f3089bd29877bf6fa3c\n""}]",0,124208,7c406e4cbc750ef52a2444fec189b499f59d9486,7,4,1,8731,,,0,"Adding 'tags' to action rest resource

Change-Id: I197f7dc1d97188c3bc9f6f3089bd29877bf6fa3c
",git fetch https://review.opendev.org/openstack/mistral refs/changes/08/124208/1 && git format-patch -1 --stdout FETCH_HEAD,"['mistral/api/controllers/v2/action.py', 'mistral/tests/unit/api/v2/test_actions.py']",2,7c406e4cbc750ef52a2444fec189b499f59d9486,add_tags_to_workflows_and_actions," tags: ['test', 'v2'] 'tags': ['test', 'v2'],",,3,0
openstack%2Fmistral~master~I6812675d47cfc1d9f5108d2fe4466abaa7e2e40f,openstack/mistral,master,I6812675d47cfc1d9f5108d2fe4466abaa7e2e40f,Modifying workflow and action services to save 'tags',MERGED,2014-09-25 21:57:31.000000000,2014-09-29 10:54:27.000000000,2014-09-29 10:54:27.000000000,"[{'_account_id': 3}, {'_account_id': 7700}, {'_account_id': 8731}, {'_account_id': 9432}]","[{'number': 1, 'created': '2014-09-25 21:57:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/03aed345f7f6ed5b73d6ffb3924f73ae069472f9', 'message': ""Modifying workflow and action services to save 'tags'\n\nChange-Id: I6812675d47cfc1d9f5108d2fe4466abaa7e2e40f\n""}, {'number': 2, 'created': '2014-09-25 22:02:47.000000000', 'files': ['mistral/services/actions.py', 'mistral/tests/unit/services/test_action_service.py', 'mistral/tests/unit/services/test_workflow_service.py', 'mistral/services/workflows.py', 'mistral/db/v2/sqlalchemy/models.py'], 'web_link': 'https://opendev.org/openstack/mistral/commit/914bbcd8c726acb54013f44d11b745be90eebb87', 'message': ""Modifying workflow and action services to save 'tags'\n\nCloses-Bug: #1373295\n\nChange-Id: I6812675d47cfc1d9f5108d2fe4466abaa7e2e40f\n""}]",0,124199,914bbcd8c726acb54013f44d11b745be90eebb87,8,4,2,8731,,,0,"Modifying workflow and action services to save 'tags'

Closes-Bug: #1373295

Change-Id: I6812675d47cfc1d9f5108d2fe4466abaa7e2e40f
",git fetch https://review.opendev.org/openstack/mistral refs/changes/99/124199/1 && git format-patch -1 --stdout FETCH_HEAD,"['mistral/services/actions.py', 'mistral/tests/unit/services/test_action_service.py', 'mistral/tests/unit/services/test_workflow_service.py', 'mistral/services/workflows.py', 'mistral/db/v2/sqlalchemy/models.py']",5,03aed345f7f6ed5b73d6ffb3924f73ae069472f9,bug/1373295, tags = sa.Column(st.JsonListType()),,11,0
openstack%2Fmistral~master~Iba70efc4b0a7b44a4d39b2ea648045f86dd92889,openstack/mistral,master,Iba70efc4b0a7b44a4d39b2ea648045f86dd92889,Adding 'tags' to workflow and action specs,MERGED,2014-09-25 21:43:55.000000000,2014-09-29 10:54:21.000000000,2014-09-29 10:54:20.000000000,"[{'_account_id': 3}, {'_account_id': 7700}, {'_account_id': 8731}, {'_account_id': 9432}]","[{'number': 1, 'created': '2014-09-25 21:43:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/50e2b8a9638c621e48703112ae65e26d8cc5eace', 'message': ""Adding 'tags' to workflow and action specs\n\nChange-Id: Iba70efc4b0a7b44a4d39b2ea648045f86dd92889\n""}, {'number': 2, 'created': '2014-09-25 22:02:47.000000000', 'files': ['mistral/workbook/v2/actions.py', 'mistral/tests/unit/workbook/v2/test_dsl_specs_v2.py', 'mistral/workbook/v2/workflows.py'], 'web_link': 'https://opendev.org/openstack/mistral/commit/840765ec960b70d0540092544ef819454c090555', 'message': ""Adding 'tags' to workflow and action specs\n\nCloses-Bug: #1373295\n\nChange-Id: Iba70efc4b0a7b44a4d39b2ea648045f86dd92889\n""}]",0,124195,840765ec960b70d0540092544ef819454c090555,8,4,2,8731,,,0,"Adding 'tags' to workflow and action specs

Closes-Bug: #1373295

Change-Id: Iba70efc4b0a7b44a4d39b2ea648045f86dd92889
",git fetch https://review.opendev.org/openstack/mistral refs/changes/95/124195/2 && git format-patch -1 --stdout FETCH_HEAD,"['mistral/workbook/v2/actions.py', 'mistral/tests/unit/workbook/v2/test_dsl_specs_v2.py', 'mistral/workbook/v2/workflows.py']",3,50e2b8a9638c621e48703112ae65e26d8cc5eace,bug/1373295," ""tags"": {""type"": ""array""}, self._tags = data.get('tags', []) def get_tags(self): return self._tags ",,17,0
openstack%2Fmistral~master~Ic5d7dbbec7d96b50df094394aabaa4c30de5f2c5,openstack/mistral,master,Ic5d7dbbec7d96b50df094394aabaa4c30de5f2c5,Cleaning up obsolete TODOs and minor style changes,MERGED,2014-09-25 21:34:12.000000000,2014-09-29 10:53:26.000000000,2014-09-29 10:53:25.000000000,"[{'_account_id': 3}, {'_account_id': 7700}, {'_account_id': 8731}, {'_account_id': 9432}]","[{'number': 1, 'created': '2014-09-25 21:34:12.000000000', 'files': ['mistral/engine/executor.py', 'mistral/api/app.py', 'mistral/engine/states.py', 'mistral/engine/retry.py', 'mistral/api/controllers/v1/listener.py', 'mistral/api/controllers/v2/task.py', 'mistral/engine/__init__.py', 'mistral/db/v1/sqlalchemy/api.py', 'mistral/engine/drivers/default/executor.py', 'mistral/workbook/v2/actions.py', 'mistral/workflow/data_flow.py', 'mistral/workbook/v2/tasks.py', 'mistral/context.py', 'mistral/engine/data_flow.py', 'mistral/workbook/v2/task_policies.py', 'mistral/engine/workflow.py', 'mistral/services/periodic.py', 'mistral/api/controllers/resource.py', 'mistral/services/trusts.py', 'mistral/db/v2/sqlalchemy/api.py', 'mistral/engine/drivers/default/engine.py'], 'web_link': 'https://opendev.org/openstack/mistral/commit/f3b4b6cf615bf9df0cf8e7150f318ed1f05eb917', 'message': 'Cleaning up obsolete TODOs and minor style changes\n\nChange-Id: Ic5d7dbbec7d96b50df094394aabaa4c30de5f2c5\n'}]",0,124191,f3b4b6cf615bf9df0cf8e7150f318ed1f05eb917,7,4,1,8731,,,0,"Cleaning up obsolete TODOs and minor style changes

Change-Id: Ic5d7dbbec7d96b50df094394aabaa4c30de5f2c5
",git fetch https://review.opendev.org/openstack/mistral refs/changes/91/124191/1 && git format-patch -1 --stdout FETCH_HEAD,"['mistral/engine/executor.py', 'mistral/api/app.py', 'mistral/engine/states.py', 'mistral/engine/retry.py', 'mistral/api/controllers/v1/listener.py', 'mistral/api/controllers/v2/task.py', 'mistral/engine/__init__.py', 'mistral/db/v1/sqlalchemy/api.py', 'mistral/engine/drivers/default/executor.py', 'mistral/workbook/v2/actions.py', 'mistral/workflow/data_flow.py', 'mistral/workbook/v2/tasks.py', 'mistral/context.py', 'mistral/engine/data_flow.py', 'mistral/workbook/v2/task_policies.py', 'mistral/engine/workflow.py', 'mistral/services/periodic.py', 'mistral/api/controllers/resource.py', 'mistral/services/trusts.py', 'mistral/db/v2/sqlalchemy/api.py', 'mistral/engine/drivers/default/engine.py']",21,f3b4b6cf615bf9df0cf8e7150f318ed1f05eb917,cleanup_obsolete_todos,# TODO(rakhmerov): Deprecated in favor of package 'mistral.engine1'. ,,49,44
openstack%2Fhorizon~master~I66316d4302306a0d9dc76c8d86fcee9c3b6eb1fa,openstack/horizon,master,I66316d4302306a0d9dc76c8d86fcee9c3b6eb1fa,Remove lesscpy dependency from list of precompilers,ABANDONED,2014-09-12 21:02:49.000000000,2014-09-29 10:47:09.000000000,,"[{'_account_id': 3}, {'_account_id': 2455}, {'_account_id': 6610}, {'_account_id': 9981}]","[{'number': 1, 'created': '2014-09-12 21:02:49.000000000', 'files': ['openstack_dashboard/settings.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/7fa4a2b29cab03f508405d45c946432257b9d3e4', 'message': 'Remove lesscpy dependency from list of precompilers\n\nAll .less has been converted to sass, and lesscpy has been removed\nfrom requirements.txt already.\n\nChange-Id: I66316d4302306a0d9dc76c8d86fcee9c3b6eb1fa\n'}]",0,121237,7fa4a2b29cab03f508405d45c946432257b9d3e4,8,4,1,6593,,,0,"Remove lesscpy dependency from list of precompilers

All .less has been converted to sass, and lesscpy has been removed
from requirements.txt already.

Change-Id: I66316d4302306a0d9dc76c8d86fcee9c3b6eb1fa
",git fetch https://review.opendev.org/openstack/horizon refs/changes/37/121237/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack_dashboard/settings.py'],1,7fa4a2b29cab03f508405d45c946432257b9d3e4,,," ('text/less', 'lesscpy {infile}'),",0,1
openstack%2Fsahara~master~I3b82079102f1ef8179f4d31a7a23e76343637cb3,openstack/sahara,master,I3b82079102f1ef8179f4d31a7a23e76343637cb3,Updating Hadoop-Swift documentation,MERGED,2014-09-26 20:05:05.000000000,2014-09-29 10:42:26.000000000,2014-09-29 10:11:18.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 8090}, {'_account_id': 8411}]","[{'number': 1, 'created': '2014-09-26 20:05:05.000000000', 'files': ['doc/source/userdoc/hadoop-swift.rst'], 'web_link': 'https://opendev.org/openstack/sahara/commit/22b4aee9584265d4418149bb5e0afcb641091606', 'message': 'Updating Hadoop-Swift documentation\n\nChanges\n* removed sections about patching Swift and references to Folsom and\n  Grizzly\n* adding domain.name and trust.id to list of configuration options\n* refactoring example section\n\nChange-Id: I3b82079102f1ef8179f4d31a7a23e76343637cb3\nCloses-Bug: #1373066\n'}]",0,124506,22b4aee9584265d4418149bb5e0afcb641091606,15,5,1,10670,,,0,"Updating Hadoop-Swift documentation

Changes
* removed sections about patching Swift and references to Folsom and
  Grizzly
* adding domain.name and trust.id to list of configuration options
* refactoring example section

Change-Id: I3b82079102f1ef8179f4d31a7a23e76343637cb3
Closes-Bug: #1373066
",git fetch https://review.opendev.org/openstack/sahara refs/changes/06/124506/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/userdoc/hadoop-swift.rst'],1,22b4aee9584265d4418149bb5e0afcb641091606,bug/1373066,"Hadoop and Swift integration are the essential continuation of the Hadoop/OpenStack marriage. The key component to making this marriage work is the Hadoop Swift filesystem implementation. Although this implementation has been merged into the upstream Hadoop project, Sahara maintains a version with the most current features enabled. * The original Hadoop patch can be found at https://issues.apache.org/jira/browse/HADOOP-8545 * The most current Sahara maintained version of this patch can be found in the Sahara Extra repository https://github.com/openstack/sahara-extra * The latest compiled version of the jar for this component can be downloaded from http://sahara-files.mirantis.com/hadoop-swift/hadoop-swift-latest.jarYou may build the jar file yourself by choosing the latest patch from the Sahara Extra repository and using Maven to build with the pom.xml file provided. Or you may get the latest jar pre-built from the CDN at http://sahara-files.mirantis.com/hadoop-swift/hadoop-swift-latest.jar You will need to put this file into the hadoop libraries (e.g. /usr/lib/share/hadoop/lib) on each job-tracker and task-tracker node for Hadoop 1.x, or each ResourceManager and NodeManager node for Hadoop 2.x in the cluster.In general, when Sahara runs a job on a cluster it will handle configuring the Hadoop installation. In cases where a user might require more in-depth configuration all the data is set in the ``core-site.xml`` file on the cluster instances using this template:2. Provider-specific. The patch for Hadoop supports different cloud providers. * ``.domain.name`` - Domains can be used to specify users who are not in the tenant specified. * ``.trust.id`` - Trusts are optionally used to scope the authentication tokens of the supplied user.For this example it is assumed that you have setup a Hadoop instance with a valid configuration and the Swift filesystem component. Furthermore there is assumed to be a Swift container named ``integration`` holding an object named ``temp``, as well as a Keystone user named ``admin`` with a password of ``swordfish``. The following example illustrates how to copy an object to a new location in the same container. We will use Hadoop's ``distcp`` command (http://hadoop.apache.org/docs/r0.19.0/distcp.html) to accomplish the copy. Note that the service provider for our Swift access is ``sahara``, and that we will not need to specify the project of our Swift container as it will be provided in the Hadoop configuration. Swift paths are expressed in Hadoop according to the following template: ``swift://${container}.${provider}/${object}``. For our example source this will appear as ``swift://integration.sahara/temp``.After that just confirm that ``temp1`` has been created in our ``integration`` container.**Note:** Please note that container names should be a valid URI.","Hadoop and Swift integration is the essential continuation of Hadoop&OpenStack marriage. There were two steps to achieve this: * Hadoop side: https://issues.apache.org/jira/browse/HADOOP-8545 This patch is not merged yet and is still being developed, so that's why there is an ability to get the latest-version jar file from CDN: http://sahara-files.mirantis.com/hadoop-swift/hadoop-swift-latest.jar * Swift side: https://review.openstack.org/#/c/21015 This patch is merged into Grizzly. If you want to make it work in Folsom see the instructions in the section below. Swift patching -------------- If you are still using Folsom you need to follow these steps: * Go to proxy server and find proxy-server.conf file. Go to ``[pipeline-main]`` section and insert a new filter BEFORE 'authtoken' filter. The name of your new filter is not very important, you will use it only for configuration. E.g. let it be ``${list_endpoints}``: .. sourcecode:: cfg [pipeline:main] pipeline = catch_errors healthcheck cache ratelimit swift3 s3token list_endpoints authtoken keystone proxy-server .. The next thing you need to do here is to add the description of new filter: .. sourcecode:: cfg [filter:list_endpoints] use = egg:swift#${list_endpoints} # list_endpoints_path = /endpoints/ .. ``list_endpoints_path`` is not mandatory and is ""endpoints"" by default. This param is used for http-request construction. See details below. * Go to ``entry_points.txt`` in egg-info. For swift-1.7.4 it may be found in ``/usr/lib/python2.7/dist-packages/swift-1.7.4.egg-info/entry_points.txt``. Add the following description to ``[paste.filter_factory]`` section: .. sourcecode:: cfg ${list_endpoints} = swift.common.middleware.list_endpoints:filter_factory * And the last step: put `list_endpoints.py <https://review.openstack.org/#/c/21015/7/swift/common/middleware/list_endpoints.py>`_ to ``/python2.7/dist-packages/swift/common/middleware/``. Is Swift was patched successfully? ---------------------------------- You may check if patching is successful just sending the following http requests: .. sourcecode:: bash http://${proxy}:8080/endpoints/${account}/${container}/${object} http://${proxy}:8080/endpoints/${account}/${container} http://${proxy}:8080/endpoints/${account} You don't need any additional headers here and authorization (see previous section: filter ${list_endpoints} is before 'authtoken' filter). The response will contain ip's of all swift nodes which contains the corresponding object. You may build jar file by yourself choosing the latest patch from https://issues.apache.org/jira/browse/HADOOP-8545. Or you may get the latest one from CDN http://sahara-files.mirantis.com/hadoop-swift/hadoop-swift-latest.jar You need to put this file to hadoop libraries (e.g. /usr/lib/share/hadoop/lib) into each job-tracker and task-tracker node in cluster. The main step in this section is to configure core-site.xml file on each of this node.All of configs may be rewritten by Hadoop-job or set in ``core-site.xml`` using this template:2. Provider-specific. Patch for Hadoop supports different cloud providers.By this point Swift and Hadoop is ready for use. All configs in hadoop is ok. In example below provider's name is ``sahara``. So let's copy one object to another in one swift container and account. E.g. /dev/integration/temp to /dev/integration/temp1. Will use distcp for this purpose: http://hadoop.apache.org/docs/r0.19.0/distcp.html How to write swift path? In our case it will look as follows: ``swift://integration.sahara/temp``. So the template is: ``swift://${container}.${provider}/${object}``. We don't need to point out the account because it will be automatically determined from tenant name from configs. Actually, account=tenant.After that just check if temp1 is created.**Note:** Please note that container name should be a valid URI.",46,81
openstack%2Fos-refresh-config~master~If1096d70831e055d9d1f77a2fd08d28025c18597,openstack/os-refresh-config,master,If1096d70831e055d9d1f77a2fd08d28025c18597,Coverage report was't created,MERGED,2014-09-26 04:31:43.000000000,2014-09-29 10:40:45.000000000,2014-09-29 10:40:43.000000000,"[{'_account_id': 3}, {'_account_id': 6449}, {'_account_id': 9369}, {'_account_id': 12385}]","[{'number': 1, 'created': '2014-09-26 04:31:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-refresh-config/commit/a4ace3e4bfc824342a11eaac88b2c500af795d89', 'message': 'Coverage report was\'t created\n\nUsually, coverage report is created by ""tox -ecover"" command.\nBut it wasn\'t created. This fixes it.\n\nChange-Id: If1096d70831e055d9d1f77a2fd08d28025c18597\nCloses-Bug: #1374259\n'}, {'number': 2, 'created': '2014-09-26 04:51:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-refresh-config/commit/a91f923994a8ff5c81d9481bf7f316964bf2abce', 'message': 'Coverage report was\'t created\n\nUsually, coverage report is created by ""tox -ecover"" command.\nBut it wasn\'t created. This fixes it.\n\nChange-Id: If1096d70831e055d9d1f77a2fd08d28025c18597\nCloses-Bug: #1374259\n'}, {'number': 3, 'created': '2014-09-26 07:14:12.000000000', 'files': ['.gitignore', '.testr.conf', '.coveragerc', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/os-refresh-config/commit/6df4f30e338e4de63eedd82c306851d3f9f4039c', 'message': 'Coverage report was\'t created\n\nUsually, coverage report is created by ""tox -ecover"" command.\nBut it wasn\'t created. This fixes it.\n\nChange-Id: If1096d70831e055d9d1f77a2fd08d28025c18597\nCloses-Bug: #1374259\n'}]",0,124264,6df4f30e338e4de63eedd82c306851d3f9f4039c,15,4,3,12385,,,0,"Coverage report was't created

Usually, coverage report is created by ""tox -ecover"" command.
But it wasn't created. This fixes it.

Change-Id: If1096d70831e055d9d1f77a2fd08d28025c18597
Closes-Bug: #1374259
",git fetch https://review.opendev.org/openstack/os-refresh-config refs/changes/64/124264/3 && git format-patch -1 --stdout FETCH_HEAD,"['.gitignore', '.testr.conf', 'tox.ini']",3,a4ace3e4bfc824342a11eaac88b2c500af795d89,bug/1374259, python setup.py test --coverage --coverage-package-name=os_refresh_config, python setup.py testr --coverage,3,3
openstack%2Fos-apply-config~master~I3fb3dce4ed2c8d6e8020888caa0adb22a316bae2,openstack/os-apply-config,master,I3fb3dce4ed2c8d6e8020888caa0adb22a316bae2,Updated from global requirements,MERGED,2014-09-11 00:30:17.000000000,2014-09-29 10:39:34.000000000,2014-09-29 10:39:33.000000000,"[{'_account_id': 3}, {'_account_id': 215}, {'_account_id': 1726}, {'_account_id': 1926}, {'_account_id': 6449}, {'_account_id': 9453}, {'_account_id': 9712}]","[{'number': 1, 'created': '2014-09-11 00:30:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-apply-config/commit/738d258f55bc6df5dc6a0f633d7257bc1dffcf9f', 'message': 'Updated from global requirements\n\nChange-Id: I3fb3dce4ed2c8d6e8020888caa0adb22a316bae2\n'}, {'number': 2, 'created': '2014-09-19 08:50:39.000000000', 'files': ['requirements.txt', 'test-requirements.txt'], 'web_link': 'https://opendev.org/openstack/os-apply-config/commit/c88b7c8628cccd704eae39122b44b42afc1a61da', 'message': 'Updated from global requirements\n\nChange-Id: I3fb3dce4ed2c8d6e8020888caa0adb22a316bae2\n'}]",0,120643,c88b7c8628cccd704eae39122b44b42afc1a61da,23,7,2,11131,,,0,"Updated from global requirements

Change-Id: I3fb3dce4ed2c8d6e8020888caa0adb22a316bae2
",git fetch https://review.opendev.org/openstack/os-apply-config refs/changes/43/120643/2 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'test-requirements.txt']",2,738d258f55bc6df5dc6a0f633d7257bc1dffcf9f,openstack/requirements,"# The order of packages is significant, because pip processes them in the order # of appearance. Changing the order has an impact on the overall integration # process, which may cause wedges in the gate later.",,6,0
openstack%2Fos-apply-config~master~I3abfee89d418474b3766a97cfc178c02252e2d02,openstack/os-apply-config,master,I3abfee89d418474b3766a97cfc178c02252e2d02,Add mode to metadata header,MERGED,2014-04-22 14:46:19.000000000,2014-09-29 10:39:27.000000000,2014-09-29 10:39:27.000000000,"[{'_account_id': 3}, {'_account_id': 6449}, {'_account_id': 6488}, {'_account_id': 7582}, {'_account_id': 8399}, {'_account_id': 8688}, {'_account_id': 9453}, {'_account_id': 9712}]","[{'number': 1, 'created': '2014-04-22 14:46:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-apply-config/commit/5aa33135ed1f78e8ed2f5fcb95f9295e46285af1', 'message': 'Add mode to metadata header\n\n  -- os-apply-config --\n  mode: 0644\n  -- end --\n\nMode must be four octal digits. Default is 0644.\n\nChange-Id: I3abfee89d418474b3766a97cfc178c02252e2d02\n'}, {'number': 2, 'created': '2014-04-22 14:50:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-apply-config/commit/75d10ffc5cce09fe0c63834d6b7b2dbb709de7b3', 'message': 'Add mode to metadata header\n\n  -- os-apply-config --\n  mode: 0644\n  -- end --\n\nMode must be four octal digits. Default is 0644.\n\nAdvances blueprint oac-header\n\nChange-Id: I3abfee89d418474b3766a97cfc178c02252e2d02\n'}, {'number': 3, 'created': '2014-04-24 14:29:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-apply-config/commit/a25f02b41bf93d7bfe1c6dec7e44d6ce8667bac6', 'message': 'Add mode to metadata header\n\n  -- os-apply-config --\n  mode: 0644\n  -- end --\n\nMode must be four octal digits. Default is 0644.\n\nAdvances blueprint oac-header\n\nChange-Id: I3abfee89d418474b3766a97cfc178c02252e2d02\n'}, {'number': 4, 'created': '2014-04-24 15:06:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-apply-config/commit/c099101164f814d3062aba5ce5b46940708422e0', 'message': 'Add mode to metadata header\n\n  -- os-apply-config --\n  mode: 0644\n  -- end --\n\nMode must be four octal digits. Default is 0644.\n\nAdvances blueprint oac-header\n\nChange-Id: I3abfee89d418474b3766a97cfc178c02252e2d02\n'}, {'number': 5, 'created': '2014-04-24 15:11:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-apply-config/commit/09eea5e1f6fd675030e2025c03e75046d7860fca', 'message': 'Add mode to metadata header\n\n  -- os-apply-config --\n  mode: 0644\n  -- end --\n\nMode must be four octal digits. Default is 0644.\n\nAdvances blueprint oac-header\n\nChange-Id: I3abfee89d418474b3766a97cfc178c02252e2d02\n'}, {'number': 6, 'created': '2014-04-25 12:37:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-apply-config/commit/51c5ff2c0bb577bca0888ae1b68bd411fa2f436f', 'message': 'Add mode to metadata header\n\n  -- os-apply-config --\n  mode: 0644\n  -- end --\n\nMode must be four octal digits. Default is 0644.\n\nAdvances blueprint oac-header\n\nChange-Id: I3abfee89d418474b3766a97cfc178c02252e2d02\n'}, {'number': 7, 'created': '2014-04-28 16:15:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-apply-config/commit/47bdfa712a0211ecb30299701178d254f7425f19', 'message': 'Add mode to metadata header\n\n  -- os-apply-config --\n  mode: 0644\n  -- end --\n\nMode must be an integer in [0, 0777]. Preferably this will be written as\nthree octal digits. The default is to use the mode of the target file if\nit already exists, otherwise 0644.\n\nAdvances blueprint oac-header\n\nChange-Id: I3abfee89d418474b3766a97cfc178c02252e2d02\n'}, {'number': 8, 'created': '2014-09-15 11:45:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-apply-config/commit/b42f749ab1852ad8a15409963e39b08f0376d47c', 'message': 'Add mode to metadata header\n\n  -- os-apply-config --\n  mode: 0644\n  -- end --\n\nMode must be an integer in [0, 0777]. Preferably this will be written as\nthree octal digits. The default is to use the mode of the target file if\nit already exists, otherwise 0644.\n\nAdvances blueprint oac-header\n\nChange-Id: I3abfee89d418474b3766a97cfc178c02252e2d02\n'}, {'number': 9, 'created': '2014-09-15 11:46:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-apply-config/commit/e135210c194caa974f52c30fcb80afab36b5b43b', 'message': 'Add mode to control file\n\nMode must be an integer in [0, 0777]. Preferably this will be written as\nthree octal digits. The default is to use the mode of the target file if\nit already exists, otherwise 0644.\n\nAdvances blueprint oac-header\n\nChange-Id: I3abfee89d418474b3766a97cfc178c02252e2d02\n'}, {'number': 10, 'created': '2014-09-15 12:02:14.000000000', 'files': ['os_apply_config/tests/test_apply_config.py', 'os_apply_config/tests/templates/etc/control/mode', 'os_apply_config/tests/templates/etc/control/mode.oac', 'os_apply_config/apply_config.py'], 'web_link': 'https://opendev.org/openstack/os-apply-config/commit/638d62518e581f2bf7601e4cf0a96601de375ea3', 'message': 'Add mode to metadata header\n\n  -- os-apply-config --\n  mode: 0644\n  -- end --\n\nMode must be an integer in [0, 0777]. Preferably this will be written as\nthree octal digits. The default is to use the mode of the target file if\nit already exists, otherwise 0644.\n\nAdvances blueprint oac-header\n\nChange-Id: I3abfee89d418474b3766a97cfc178c02252e2d02\n'}]",12,89598,638d62518e581f2bf7601e4cf0a96601de375ea3,56,8,10,8688,,,0,"Add mode to metadata header

  -- os-apply-config --
  mode: 0644
  -- end --

Mode must be an integer in [0, 0777]. Preferably this will be written as
three octal digits. The default is to use the mode of the target file if
it already exists, otherwise 0644.

Advances blueprint oac-header

Change-Id: I3abfee89d418474b3766a97cfc178c02252e2d02
",git fetch https://review.opendev.org/openstack/os-apply-config refs/changes/98/89598/3 && git format-patch -1 --stdout FETCH_HEAD,"['os_apply_config/tests/test_apply_config.py', 'os_apply_config/tests/templates/etc/header/mode', 'os_apply_config/apply_config.py']",3,5aa33135ed1f78e8ed2f5fcb95f9295e46285af1,bp/oac-header,"HDR_MODE = 'mode' mode = obj.get(HDR_MODE, None) or mode mode: the permissions to set on the file, EG 0755 if key == HDR_MODE: if type(value) is not int: raise exc.ConfigException( ""invalid mode '%s': %s"" % (value, filename)) if value < 0 or value > 0777: raise exc.ConfigException( ""mode '0%o' out of range: %s"" % (value, filename)) return value ",,30,0
openstack%2Fopenstack-manuals~master~I47883b3e524aba67f82304112a066a281c341be3,openstack/openstack-manuals,master,I47883b3e524aba67f82304112a066a281c341be3,Update the config reference tables (exept swift),MERGED,2014-09-28 17:22:29.000000000,2014-09-29 10:36:51.000000000,2014-09-29 10:36:50.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 6547}, {'_account_id': 7923}]","[{'number': 1, 'created': '2014-09-28 17:22:29.000000000', 'files': ['doc/common/tables/trove-quota.xml', 'doc/common/tables/trove-db_percona.xml', 'doc/common/tables/trove-dns.xml', 'doc/common/tables/glance-database.xml', 'doc/common/tables/neutron-ml2_bigswitch.xml', 'doc/common/tables/neutron-cfg_agent.xml', 'doc/common/tables/trove-cluster.xml', 'doc/common/tables/keystone-qpid.xml', 'doc/config-reference/block-storage/section_misc.xml', 'doc/common/tables/trove-db_redis.xml', 'doc/common/tables/ceilometer-ipmi.xml', 'doc/common/tables/trove-nova.xml', 'doc/common/tables/keystone-saml.xml', 'doc/common/tables/trove-db_mongodb.xml', 'doc/common/tables/trove-network.xml', 'doc/common/tables/neutron-l3_agent.xml', 'doc/common/tables/cinder-rpc.xml', 'tools/autogenerate-config-flagmappings/ceilometer.headers', 'doc/common/tables/trove-compute.xml', 'doc/config-reference/ch_telemetryconfigure.xml', 'doc/common/tables/ceilometer-rootwrap.xml', 'doc/common/tables/nova-xen.xml', 'doc/common/tables/ceilometer-common.xml', 'tools/autogenerate-config-flagmappings/nova.flagmappings', 'tools/autogenerate-config-flagmappings/heat.flagmappings', 'doc/common/tables/neutron-ml2_fslsdn.xml', 'doc/common/tables/neutron-bigswitch.xml', 'doc/common/tables/sahara-qpid.xml', 'tools/autogenerate-config-flagmappings/sahara.flagmappings', 'doc/common/tables/keystone-memcache.xml', 'doc/common/tables/trove-guestagent.xml', 'doc/common/tables/neutron-ml2_mlnx.xml', 'doc/common/tables/keystone-token.xml', 'doc/common/tables/sahara-domain.xml', 'doc/common/tables/nova-libvirt.xml', 'doc/common/tables/heat-qpid.xml', 'doc/common/tables/neutron-common.xml', 'doc/common/tables/nova-logging.xml', 'doc/common/tables/trove-db_couchbase.xml', 'doc/common/tables/cinder-common.xml', 'doc/common/tables/cinder-smbfs.xml', 'doc/common/tables/ceilometer-qpid.xml', 'doc/common/tables/trove-db_mysql.xml', 'doc/common/tables/cinder-logging.xml', 'doc/common/tables/keystone-api.xml', 'doc/config-reference/ch_dataprocessingserviceconfigure.xml', 'doc/common/tables/trove-rpc.xml', 'doc/common/tables/cinder-prophetstor_dpl.xml', 'doc/common/tables/ceilometer-database.xml', 'doc/common/tables/cinder-api.xml', 'doc/common/tables/trove-logging.xml', 'doc/common/tables/neutron-qpid.xml', 'doc/common/tables/trove-db_cassandra.xml', 'doc/common/tables/keystone-ca.xml', 'doc/common/tables/nova-quota.xml', 'doc/common/tables/trove-volume.xml', 'tools/autogenerate-config-flagmappings/neutron.flagmappings', 'doc/common/tables/cinder-images.xml', 'doc/common/tables/neutron-nuage.xml', 'doc/common/tables/ceilometer-nova_cells.xml', 'doc/common/tables/glance-qpid.xml', 'doc/common/tables/keystone-catalog.xml', 'doc/common/tables/ceilometer-service_types.xml', 'doc/common/tables/neutron-metadata.xml', 'doc/common/tables/neutron-ml2_brocade.xml', 'doc/config-reference/ch_identityconfigure.xml', 'tools/autogenerate-config-flagmappings/glance.flagmappings', 'doc/common/tables/ceilometer-logging.xml', 'doc/common/tables/ceilometer-nova.xml', 'tools/autogenerate-config-flagmappings/keystone.flagmappings', 'tools/autogenerate-config-flagmappings/ceilometer.flagmappings', 'tools/autogenerate-config-flagmappings/cinder.flagmappings', 'doc/common/tables/keystone-cache.xml', 'doc/common/tables/sahara-common.xml', 'doc/common/tables/keystone-ldap.xml', 'doc/common/tables/trove-api.xml', 'doc/common/tables/trove-heat.xml', 'doc/common/tables/trove-db_postgresql.xml', 'doc/common/tables/trove-backup.xml', 'doc/common/tables/trove-taskmanager.xml', 'doc/common/tables/neutron-securitygroups.xml', 'doc/common/tables/neutron-cisco.xml', 'doc/common/tables/cinder-storage_gpfs.xml', 'doc/common/tables/trove-clients.xml', 'tools/autogenerate-config-flagmappings/trove.flagmappings', 'doc/common/tables/keystone-revoke.xml', 'tools/autogenerate-config-flagmappings/cinder.headers', 'doc/common/tables/glance-registry.xml', 'doc/common/tables/trove-database.xml', 'doc/common/tables/cinder-zfssa.xml', 'tools/autogenerate-config-flagmappings/keystone.headers', 'doc/common/tables/ceilometer-api.xml', 'doc/common/tables/trove-amqp.xml', 'doc/common/tables/heat-cloudwatch_api.xml', 'doc/common/tables/trove-common.xml', 'tools/autogenerate-config-flagmappings/sahara.headers'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/ad9faae7c31d33f21c3c7ef49cffa28d36857c89', 'message': 'Update the config reference tables (exept swift)\n\nInclude new tables in the config reference.\n\nCloses-Bug: #1366489\nCloses-Bug: #1374844\n\nChange-Id: I47883b3e524aba67f82304112a066a281c341be3\n'}]",0,124639,ad9faae7c31d33f21c3c7ef49cffa28d36857c89,8,4,1,7923,,,0,"Update the config reference tables (exept swift)

Include new tables in the config reference.

Closes-Bug: #1366489
Closes-Bug: #1374844

Change-Id: I47883b3e524aba67f82304112a066a281c341be3
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/39/124639/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/common/tables/trove-quota.xml', 'doc/common/tables/trove-db_percona.xml', 'doc/common/tables/trove-dns.xml', 'doc/common/tables/glance-database.xml', 'doc/common/tables/neutron-ml2_bigswitch.xml', 'doc/common/tables/neutron-cfg_agent.xml', 'doc/common/tables/trove-cluster.xml', 'doc/common/tables/keystone-qpid.xml', 'doc/config-reference/block-storage/section_misc.xml', 'doc/common/tables/trove-db_redis.xml', 'doc/common/tables/ceilometer-ipmi.xml', 'doc/common/tables/trove-nova.xml', 'doc/common/tables/keystone-saml.xml', 'doc/common/tables/trove-db_mongodb.xml', 'doc/common/tables/trove-network.xml', 'doc/common/tables/neutron-l3_agent.xml', 'doc/common/tables/cinder-rpc.xml', 'tools/autogenerate-config-flagmappings/ceilometer.headers', 'doc/common/tables/trove-compute.xml', 'doc/config-reference/ch_telemetryconfigure.xml', 'doc/common/tables/ceilometer-rootwrap.xml', 'doc/common/tables/nova-xen.xml', 'doc/common/tables/ceilometer-common.xml', 'tools/autogenerate-config-flagmappings/nova.flagmappings', 'tools/autogenerate-config-flagmappings/heat.flagmappings', 'doc/common/tables/neutron-ml2_fslsdn.xml', 'doc/common/tables/neutron-bigswitch.xml', 'doc/common/tables/sahara-qpid.xml', 'tools/autogenerate-config-flagmappings/sahara.flagmappings', 'doc/common/tables/keystone-memcache.xml', 'doc/common/tables/trove-guestagent.xml', 'doc/common/tables/neutron-ml2_mlnx.xml', 'doc/common/tables/keystone-token.xml', 'doc/common/tables/sahara-domain.xml', 'doc/common/tables/nova-libvirt.xml', 'doc/common/tables/heat-qpid.xml', 'doc/common/tables/neutron-common.xml', 'doc/common/tables/nova-logging.xml', 'doc/common/tables/trove-db_couchbase.xml', 'doc/common/tables/cinder-common.xml', 'doc/common/tables/cinder-smbfs.xml', 'doc/common/tables/ceilometer-qpid.xml', 'doc/common/tables/trove-db_mysql.xml', 'doc/common/tables/cinder-logging.xml', 'doc/common/tables/keystone-api.xml', 'doc/config-reference/ch_dataprocessingserviceconfigure.xml', 'doc/common/tables/trove-rpc.xml', 'doc/common/tables/cinder-prophetstor_dpl.xml', 'doc/common/tables/ceilometer-database.xml', 'doc/common/tables/cinder-api.xml', 'doc/common/tables/trove-logging.xml', 'doc/common/tables/neutron-qpid.xml', 'doc/common/tables/trove-db_cassandra.xml', 'doc/common/tables/keystone-ca.xml', 'doc/common/tables/nova-quota.xml', 'doc/common/tables/trove-volume.xml', 'tools/autogenerate-config-flagmappings/neutron.flagmappings', 'doc/common/tables/cinder-images.xml', 'doc/common/tables/neutron-nuage.xml', 'doc/common/tables/ceilometer-nova_cells.xml', 'doc/common/tables/glance-qpid.xml', 'doc/common/tables/keystone-catalog.xml', 'doc/common/tables/ceilometer-service_types.xml', 'doc/common/tables/neutron-metadata.xml', 'doc/common/tables/neutron-ml2_brocade.xml', 'doc/config-reference/ch_identityconfigure.xml', 'tools/autogenerate-config-flagmappings/glance.flagmappings', 'doc/common/tables/ceilometer-logging.xml', 'doc/common/tables/ceilometer-nova.xml', 'tools/autogenerate-config-flagmappings/keystone.flagmappings', 'tools/autogenerate-config-flagmappings/ceilometer.flagmappings', 'tools/autogenerate-config-flagmappings/cinder.flagmappings', 'doc/common/tables/keystone-cache.xml', 'doc/common/tables/sahara-common.xml', 'doc/common/tables/keystone-ldap.xml', 'doc/common/tables/trove-api.xml', 'doc/common/tables/trove-heat.xml', 'doc/common/tables/trove-db_postgresql.xml', 'doc/common/tables/trove-backup.xml', 'doc/common/tables/trove-taskmanager.xml', 'doc/common/tables/neutron-securitygroups.xml', 'doc/common/tables/neutron-cisco.xml', 'doc/common/tables/cinder-storage_gpfs.xml', 'doc/common/tables/trove-clients.xml', 'tools/autogenerate-config-flagmappings/trove.flagmappings', 'doc/common/tables/keystone-revoke.xml', 'tools/autogenerate-config-flagmappings/cinder.headers', 'doc/common/tables/glance-registry.xml', 'doc/common/tables/trove-database.xml', 'doc/common/tables/cinder-zfssa.xml', 'tools/autogenerate-config-flagmappings/keystone.headers', 'doc/common/tables/ceilometer-api.xml', 'doc/common/tables/trove-amqp.xml', 'doc/common/tables/heat-cloudwatch_api.xml', 'doc/common/tables/trove-common.xml', 'tools/autogenerate-config-flagmappings/sahara.headers']",96,ad9faae7c31d33f21c3c7ef49cffa28d36857c89,config,domain domain ,,951,229
openstack%2Fdevstack~master~I50e5b5a161207d46e8ce0e304d816e8e5b68dbe8,openstack/devstack,master,I50e5b5a161207d46e8ce0e304d816e8e5b68dbe8,XenAPI: Fix race condition waiting for VM to start,MERGED,2014-09-17 11:42:55.000000000,2014-09-29 10:28:58.000000000,2014-09-29 10:28:57.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 1653}, {'_account_id': 6735}, {'_account_id': 6854}, {'_account_id': 7118}, {'_account_id': 8871}, {'_account_id': 9009}]","[{'number': 1, 'created': '2014-09-17 11:42:55.000000000', 'files': ['tools/xen/install_os_domU.sh'], 'web_link': 'https://opendev.org/openstack/devstack/commit/6a95b605bcf94ba8385660a6681ffbbe46f2e39d', 'message': 'XenAPI: Fix race condition waiting for VM to start\n\nBuildroot on Ubuntu Trusty is slow to populate the vnc-port.\nWait for 20 seconds until the port is populated.\n\nChange-Id: I50e5b5a161207d46e8ce0e304d816e8e5b68dbe8\n'}]",0,122125,6a95b605bcf94ba8385660a6681ffbbe46f2e39d,28,8,1,6735,,,0,"XenAPI: Fix race condition waiting for VM to start

Buildroot on Ubuntu Trusty is slow to populate the vnc-port.
Wait for 20 seconds until the port is populated.

Change-Id: I50e5b5a161207d46e8ce0e304d816e8e5b68dbe8
",git fetch https://review.opendev.org/openstack/devstack refs/changes/25/122125/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/xen/install_os_domU.sh'],1,6a95b605bcf94ba8385660a6681ffbbe46f2e39d,, sleep 20 # Wait for the vnc-port to be written,,1,0
openstack%2Fpython-mistralclient~master~I567112b659c5c8de02d56c33633007185bcdee46,openstack/python-mistralclient,master,I567112b659c5c8de02d56c33633007185bcdee46,Update requirements according to global requirements (master),MERGED,2014-09-25 09:33:54.000000000,2014-09-29 10:26:13.000000000,2014-09-29 10:26:12.000000000,"[{'_account_id': 3}, {'_account_id': 7700}, {'_account_id': 8731}, {'_account_id': 9432}]","[{'number': 1, 'created': '2014-09-25 09:33:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-mistralclient/commit/a658146634046020b0465f52ea931bae637e5528', 'message': 'Update requirements according to global requirements (master)\n\nChange-Id: I567112b659c5c8de02d56c33633007185bcdee46\n'}, {'number': 2, 'created': '2014-09-25 13:51:44.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/python-mistralclient/commit/f183f37a0632f615235edfe79ef13a2a1e48807a', 'message': 'Update requirements according to global requirements (master)\n\nChange-Id: I567112b659c5c8de02d56c33633007185bcdee46\n'}]",0,123986,f183f37a0632f615235edfe79ef13a2a1e48807a,9,4,2,8592,,,0,"Update requirements according to global requirements (master)

Change-Id: I567112b659c5c8de02d56c33633007185bcdee46
",git fetch https://review.opendev.org/openstack/python-mistralclient refs/changes/86/123986/2 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'test-requirements.txt']",2,a658146634046020b0465f52ea931bae637e5528,,flake8==2.2.3,flake8==2.1.0,4,4
openstack%2Fdib-utils~master~I171499c30e1a992d7225be8a9dc00e0f03ec46b0,openstack/dib-utils,master,I171499c30e1a992d7225be8a9dc00e0f03ec46b0,Sort the list of scripts in the profile view,MERGED,2014-07-25 09:34:06.000000000,2014-09-29 10:23:38.000000000,2014-09-29 10:23:38.000000000,"[{'_account_id': 3}, {'_account_id': 1726}, {'_account_id': 6449}, {'_account_id': 7144}, {'_account_id': 9453}, {'_account_id': 12385}]","[{'number': 1, 'created': '2014-07-25 09:34:06.000000000', 'files': ['bin/dib-run-parts'], 'web_link': 'https://opendev.org/openstack/dib-utils/commit/494c06793013b7eca38c1afbec552cac00867c50', 'message': 'Sort the list of scripts in the profile view\n\nWhen showing the timings of the executed scripts, sort the scripts in\nthe same way they were executed. This eases the comparison between runs\nof dib-run-parts.\n\nChange-Id: I171499c30e1a992d7225be8a9dc00e0f03ec46b0\n'}]",0,109530,494c06793013b7eca38c1afbec552cac00867c50,14,6,1,12320,,,0,"Sort the list of scripts in the profile view

When showing the timings of the executed scripts, sort the scripts in
the same way they were executed. This eases the comparison between runs
of dib-run-parts.

Change-Id: I171499c30e1a992d7225be8a9dc00e0f03ec46b0
",git fetch https://review.opendev.org/openstack/dib-utils refs/changes/30/109530/1 && git format-patch -1 --stdout FETCH_HEAD,['bin/dib-run-parts'],1,494c06793013b7eca38c1afbec552cac00867c50,,for target in $(find . -name 'start_*' -printf '%f\n' | env LC_ALL=C sort -n) ; do,for target in $(find . -name 'start_*' -printf '%f\n') ; do,1,1
openstack%2Ffuel-main~master~I367ba62273e879364979388aa9241b5010993fc1,openstack/fuel-main,master,I367ba62273e879364979388aa9241b5010993fc1,Update  nailgun spec file,MERGED,2014-09-26 07:52:21.000000000,2014-09-29 10:16:30.000000000,2014-09-29 10:16:30.000000000,"[{'_account_id': 3}, {'_account_id': 7195}, {'_account_id': 8789}, {'_account_id': 8971}]","[{'number': 1, 'created': '2014-09-26 07:52:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-main/commit/2cbb82604c98df5b7161a543b62dd8ba626688ce', 'message': 'Update  nailgun spec file\n\nChange alembic and sqlalchemy verion\nCloses-bug: #1325537\n\nChange-Id: I367ba62273e879364979388aa9241b5010993fc1\n'}, {'number': 2, 'created': '2014-09-26 08:09:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-main/commit/46a4302c3b795d765645d27436b0bd5f42d6b457', 'message': 'Update  nailgun spec file\n\nChange alembic and sqlalchemy verion\nCloses-bug: #1325537\n\nChange-Id: I367ba62273e879364979388aa9241b5010993fc1\n'}, {'number': 3, 'created': '2014-09-26 15:40:05.000000000', 'files': ['packages/rpm/specs/nailgun.spec'], 'web_link': 'https://opendev.org/openstack/fuel-main/commit/0ca7d67bc6e04edd2865508734974f3e9472e882', 'message': 'Update  nailgun spec file\n\nChange alembic and sqlalchemy verion\nCloses-bug: #1325537\n\nChange-Id: I367ba62273e879364979388aa9241b5010993fc1\n'}]",0,124319,0ca7d67bc6e04edd2865508734974f3e9472e882,18,4,3,8777,,,0,"Update  nailgun spec file

Change alembic and sqlalchemy verion
Closes-bug: #1325537

Change-Id: I367ba62273e879364979388aa9241b5010993fc1
",git fetch https://review.opendev.org/openstack/fuel-main refs/changes/19/124319/2 && git format-patch -1 --stdout FETCH_HEAD,['packages/rpm/specs/nailgun.spec'],1,2cbb82604c98df5b7161a543b62dd8ba626688ce,bug/1325537,Requires: python-alembic == 0.6.7Requires: python-sqlalchemy == 0.9.7,Requires: python-alembic == 0.6.2Requires: python-sqlalchemy == 0.7.9 ,2,4
openstack%2Fsahara~master~If83cdef109a4356fdb2db4487cde4e76d7d280a8,openstack/sahara,master,If83cdef109a4356fdb2db4487cde4e76d7d280a8,Adding job execution examples to UI user guide,MERGED,2014-09-26 15:18:33.000000000,2014-09-29 10:11:25.000000000,2014-09-29 10:11:25.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7125}, {'_account_id': 7213}, {'_account_id': 8090}, {'_account_id': 8411}, {'_account_id': 10670}]","[{'number': 1, 'created': '2014-09-26 15:18:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/9cf49f7c6ab4bad446fc68f11ec39ca247e82185', 'message': 'Adding job execution examples to UI user guide\n\nAdding examples for a Pig job and a Spark job to\nthe Sahara UI user guide.  The examples referenced\nare the ones from the Sahara repository /etc/edp-examples\ndirectory.\n\nChange-Id: If83cdef109a4356fdb2db4487cde4e76d7d280a8\nCloses-Bug: #1288914\n'}, {'number': 2, 'created': '2014-09-26 15:47:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/8451b91f3d06165f740440b5bec95095e0296f73', 'message': 'Adding job execution examples to UI user guide\n\nAdding examples for a Pig job and a Spark job to\nthe Sahara UI user guide.  The examples referenced\nare the ones from the Sahara repository /etc/edp-examples\ndirectory.  Also, removed language that referred to the\n""Sahara"" dashboard that formerly existed as a separate\nentity.  The updated version references the ""Data Processing""\npanel where it now lives (under Project).\n\nChange-Id: If83cdef109a4356fdb2db4487cde4e76d7d280a8\nCloses-Bug: #1288914\n'}, {'number': 3, 'created': '2014-09-26 20:20:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/35a49f2e77bb09661a39edc55ad51748a1ed3e2e', 'message': 'Adding job execution examples to UI user guide\n\nAdding examples for a Pig job and a Spark job to\nthe Sahara UI user guide.  The examples referenced\nare the ones from the Sahara repository /etc/edp-examples\ndirectory.  Also, removed language that referred to the\n""Sahara"" dashboard that formerly existed as a separate\nentity.  The updated version references the ""Data Processing""\npanel where it now lives (under Project).\n\nChange-Id: If83cdef109a4356fdb2db4487cde4e76d7d280a8\nCloses-Bug: #1288914\n'}, {'number': 4, 'created': '2014-09-26 20:38:16.000000000', 'files': ['doc/source/horizon/dashboard.user.guide.rst'], 'web_link': 'https://opendev.org/openstack/sahara/commit/b3ef8c3dfb17bb39d8fe0ffa60eaa28eb2c9cd71', 'message': 'Adding job execution examples to UI user guide\n\nAdding examples for a Pig job and a Spark job to\nthe Sahara UI user guide.  The examples referenced\nare the ones from the Sahara repository /etc/edp-examples\ndirectory.  Also, removed language that referred to the\n""Sahara"" dashboard that formerly existed as a separate\nentity.  The updated version references the ""Data Processing""\npanel where it now lives (under Project).\n\nChange-Id: If83cdef109a4356fdb2db4487cde4e76d7d280a8\nCloses-Bug: #1288914\n'}]",9,124440,b3ef8c3dfb17bb39d8fe0ffa60eaa28eb2c9cd71,27,7,4,8090,,,0,"Adding job execution examples to UI user guide

Adding examples for a Pig job and a Spark job to
the Sahara UI user guide.  The examples referenced
are the ones from the Sahara repository /etc/edp-examples
directory.  Also, removed language that referred to the
""Sahara"" dashboard that formerly existed as a separate
entity.  The updated version references the ""Data Processing""
panel where it now lives (under Project).

Change-Id: If83cdef109a4356fdb2db4487cde4e76d7d280a8
Closes-Bug: #1288914
",git fetch https://review.opendev.org/openstack/sahara refs/changes/40/124440/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/horizon/dashboard.user.guide.rst'],1,9cf49f7c6ab4bad446fc68f11ec39ca247e82185,bug/1288914,"Sahara (Data Processing) UI User Guide ====================================== This guide assumes that you already have Sahara service and the Horizon dashboard up and running.1) Navigate to the ""Project"" dashboard, then the ""Data Processing"" tab, then click on the ""Image Registry"" panel.1) Navigate to the ""Project"" dashboard, then the ""Data Processing"" tab, then click on the ""Node Group Templates"" panel.1) Navigate to the ""Project"" dashboard, then the ""Data Processing"" tab, then click on the ""Cluster Templates"" panel.1) Navigate to the ""Project"" dashboard, then the ""Data Processing"" tab, then click on the ""Clusters"" panel.1) From the Data Processing/Clusters page, click on the ""Scale Cluster"" button of the row that contains the cluster that you want to scale.1) From the Data Processing/Data Sources page, click on the ""Create Data Source"" button at the top right.1) From the Data Processing/Job Binaries page, click on the ""Create Job Binary"" button at the top right.1) From the Data Processing/Jobs page, click on the ""Create Job"" button at the top right.1) From the Data Processing/Jobs page, find the row that contains the job you want to launch and click on the ""Launch Job"" button at the right side of that row.Example Jobs ------------ There are sample jobs located in the sahara repository. The instructions there guide you through running the jobs via the command line. In this section, we will give a walkthrough on how to run those jobs via the Horizon UI. These steps assume that you already have a cluster up and running (in the ""Active"" state). 1) Sample Pig job - https://github.com/openstack/sahara/tree/master/etc/edp-examples/pig-job - Load the input data file from https://github.com/openstack/sahara/tree/master/etc/edp-examples/pig-job/data/input into swift - Click on Projet/Object Store/Containers and create a container with any name (""samplecontainer"" for our purposes here). - Click on Upload Object and give the object a name (""piginput"" in this case) - Navigate to Data Processing/Data Sources, Click on Create Data Source. - Name your Data Source (""pig-input-ds"" in this sample) - Type = Swift, URL samplecontainer.sahara/piginput, fill-in the Source username/password fields with your username/password and click ""Create"" - Create another Data Source to use as output for the job - Create another Data Source to use as output for our job. Name = pig-output-ds, Type = Swift, URL = samplecontainer.sahara/pigoutput, Source username/password, ""Create"" - Store your Job Binaries in the Sahara database - Navigate to Data Processing/Job Binaries, Click on Create Job Binary - Name = example.pig, Storage type = Internal database, click Browse and find example.pig wherever you checked out the sahara project <sahara root>/etc/edp-examples/pig-job - Create another Job Binary: Name = udf.jar, Storage type = Internal database, click Browse and find udf.jar wherever you checked out the sahara project <sahara root>/etc/edp-examples/pig-job - Create a Job - Navigate to Data Processing/Jobs, Click on Create Job - Name = pigsample, Job Type = Pig, Choose ""example.pig"" as the main binary - Click on the ""Libs"" tab and choose ""udf.jar"", then hit the ""Choose"" button beneath the dropdown, then click on ""Create"" - Launch your job - To launch your job from the Jobs page, click on the down arrow at the far right of the screen and choose ""Launch on Existing Cluster"" - For the input, choose ""pig-input-ds"", for output choose ""pig-output-ds"". Also choose whichever cluster you'd like to run the job on. - For this job, no additional configuration is necessary, so you can just click on ""Launch"" - You will be taken to the ""Job Executions"" page where you can see your job progress through ""PENDING, RUNNING, SUCCEEDED"" phases - When your job finishes with ""SUCCEEDED"", you can navigate back to Object Store/Containers and browse to the samplecontainer to see your output. It should be in the ""pigoutput"" folder. 2) Sample Spark job - https://github.com/openstack/sahara/tree/master/etc/edp-examples/edp-spark - Store the Job Binary in the Sahara database - Navigate to Data Processing/Job Binaries, Click on Create Job Binary - Name = sparkexample.jar, Storage type = Internal database, Browse to the location <sahara root>/etc/edp-examples/edp-spark and choose spark-example.jar, Click ""Create"" - Create a Job - Name = sparkexamplejob, Job Type = Spark, Main binary = Choose sparkexample.jar, Click ""Create"" - Launch your job - To launch your job from the Jobs page, click on the down arrow at the far right of the screen and choose ""Launch on Existing Cluster"" - Choose whichever cluster you'd like to run the job on. - Click on the ""Configure"" tab - Set the main class to be: org.apache.spark.examples.SparkPi - Under Arguments, click Add and fill in the number of ""Slices"" you want to use for the job. For this example, let's use 100 as the value - Click on Launch - You will be taken to the ""Job Executions"" page where you can see your job progress through ""PENDING, RUNNING, SUCCEEDED"" phases - When your job finishes with ""SUCCEEDED"", you can see your results by sshing to the Spark ""master"" node. - The output is located at /tmp/spark-edp/<name of job>/<job execution id>. You can do ``cat stdout`` which should display something like ""Pi is roughly 3.14156132"" - It should be noted that for more complex jobs, the input/output may be elsewhere. This particular job just writes to stdout, which is logged in the folder under /tmp. ","Sahara UI User Guide ==================== This guide assumes that you already have Sahara and the Sahara Dashboard configured and running.1) Navigate to the ""Sahara"" tab in the dashboard, then click on the ""Image Registry"" panel.1) Navigate to the ""Sahara"" tab in the dashboard, then click on the ""Node Group Templates"" panel.1) Navigate to the ""Sahara"" tab in the dashboard, then click on the ""Cluster Templates"" panel.1) Navigate to the ""Sahara"" tab in the dashboard, then click on the ""Clusters"" panel.1) From the Sahara/Clusters page, click on the ""Scale Cluster"" button of the row that contains the cluster that you want to scale.1) From the Sahara/Data Sources page, click on the ""Create Data Source"" button at the top right.1) From the Sahara/Job Binaries page, click on the ""Create Job Binary"" button at the top right.1) From the Sahara/Jobs page, click on the ""Create Job"" button at the top right.1) From the Sahara/Jobs page, find the row that contains the job you want to launch and click on the ""Launch Job"" button at the right side of that row.",98,12
openstack%2Fceilometer~master~Iff3b33db58302fb2e89b1b3722937a031a70be5f,openstack/ceilometer,master,Iff3b33db58302fb2e89b1b3722937a031a70be5f,Partition static resources defined in pipeline.yaml,MERGED,2014-09-15 14:28:14.000000000,2014-09-29 10:11:02.000000000,2014-09-29 10:11:01.000000000,"[{'_account_id': 3}, {'_account_id': 2284}, {'_account_id': 3012}, {'_account_id': 4491}, {'_account_id': 6537}, {'_account_id': 7336}, {'_account_id': 8052}, {'_account_id': 9562}, {'_account_id': 11564}]","[{'number': 1, 'created': '2014-09-15 14:28:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/315b6ddebc75baaa80e6efa0bee230c1319ac291', 'message': ""Partition static resources defined in pipeline.yaml\n\nResources statically defined in pipeline.yaml are currently not\nsubject to workload partitioning, so we can't do HA. If we have\nmultiple agents running with the same pipeline.yaml, the samples\nwill be duplicated.\n\nThis patch partitions the static resources as well.\n\nCloses-bug: #1369538\nChange-Id: Iff3b33db58302fb2e89b1b3722937a031a70be5f\n""}, {'number': 2, 'created': '2014-09-26 13:12:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/b7c165e55da9911a17b598027056d397f7c73d6b', 'message': ""Partition static resources defined in pipeline.yaml\n\nResources statically defined in pipeline.yaml are currently not\nsubject to workload partitioning, so we can't do HA. If we have\nmultiple agents running with the same pipeline.yaml, the samples\nwill be duplicated.\n\nThis patch partitions the static resources as well.\n\nCloses-bug: #1369538\nChange-Id: Iff3b33db58302fb2e89b1b3722937a031a70be5f\n""}, {'number': 3, 'created': '2014-09-26 13:14:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/78bd9746a3f4ca3d23fd69c062849fbd61860da4', 'message': ""Partition static resources defined in pipeline.yaml\n\nResources statically defined in pipeline.yaml are currently not\nsubject to workload partitioning, so we can't do HA. If we have\nmultiple agents running with the same pipeline.yaml, the samples\nwill be duplicated.\n\nThis patch partitions the static resources as well.\n\nCloses-bug: #1369538\nChange-Id: Iff3b33db58302fb2e89b1b3722937a031a70be5f\n""}, {'number': 4, 'created': '2014-09-29 06:17:02.000000000', 'files': ['ceilometer/tests/test_utils.py', 'ceilometer/tests/agentbase.py', 'ceilometer/agent.py', 'ceilometer/utils.py'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/fa3f4d6c5dd724a1fb2b811cd798fb80d075192e', 'message': ""Partition static resources defined in pipeline.yaml\n\nResources statically defined in pipeline.yaml are currently not\nsubject to workload partitioning, so we can't do HA. If we have\nmultiple agents running with the same pipeline.yaml, the samples\nwill be duplicated.\n\nThis patch partitions the static resources as well.\n\nCloses-bug: #1369538\nChange-Id: Iff3b33db58302fb2e89b1b3722937a031a70be5f\n""}]",6,121586,fa3f4d6c5dd724a1fb2b811cd798fb80d075192e,23,9,4,8052,,,0,"Partition static resources defined in pipeline.yaml

Resources statically defined in pipeline.yaml are currently not
subject to workload partitioning, so we can't do HA. If we have
multiple agents running with the same pipeline.yaml, the samples
will be duplicated.

This patch partitions the static resources as well.

Closes-bug: #1369538
Change-Id: Iff3b33db58302fb2e89b1b3722937a031a70be5f
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/86/121586/2 && git format-patch -1 --stdout FETCH_HEAD,"['ceilometer/tests/agentbase.py', 'ceilometer/tests/test_utils.py', 'ceilometer/agent.py', 'ceilometer/utils.py']",4,315b6ddebc75baaa80e6efa0bee230c1319ac291,static-resources-bug1369538,def hash_of_set(s): return str(hash(frozenset(s))) ,,54,7
openstack%2Fglance_store~master~I9efe3d57e0ce64296c7a75838bd2ee19249f0fa8,openstack/glance_store,master,I9efe3d57e0ce64296c7a75838bd2ee19249f0fa8,Switch to using oslo.utils,MERGED,2014-09-28 04:11:05.000000000,2014-09-29 10:10:59.000000000,2014-09-29 10:10:59.000000000,"[{'_account_id': 3}, {'_account_id': 6549}]","[{'number': 1, 'created': '2014-09-28 04:11:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/3cc8f4f8b246e1fc9cf391db5339f70e473876fc', 'message': 'Swith to using oslo.utils\n\nThis change swithed glance_store to using oslo.utils. It is not good to\nadd dependency on oslo-incubator if we can avoid it.\n\nChange-Id: I9efe3d57e0ce64296c7a75838bd2ee19249f0fa8\nSigned-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>'}, {'number': 2, 'created': '2014-09-29 05:12:08.000000000', 'files': ['tests/unit/test_filesystem_store.py', 'glance_store/openstack/common/excutils.py', 'glance_store/openstack/common/gettextutils.py', 'glance_store/openstack/common/processutils.py', 'glance_store/openstack/common/strutils.py', 'glance_store/openstack/common/units.py', 'tests/unit/test_vmware_store.py', 'glance_store/_drivers/s3.py', 'tests/unit/test_s3_store.py', 'glance_store/_drivers/vmware_datastore.py', 'glance_store/_drivers/swift/store.py', 'glance_store/openstack/common/importutils.py', 'glance_store/_drivers/gridfs.py', 'glance_store/openstack/common/local.py', 'glance_store/_drivers/sheepdog.py', 'glance_store/driver.py', 'openstack-common.conf', 'glance_store/common/utils.py', 'glance_store/openstack/common/timeutils.py', 'tests/unit/test_swift_store.py', 'glance_store/_drivers/filesystem.py', 'glance_store/openstack/common/_i18n.py'], 'web_link': 'https://opendev.org/openstack/glance_store/commit/05842041754a0481562a73827ecb89c7b8b66326', 'message': 'Switch to using oslo.utils\n\nThis change switched glance_store to using oslo.utils. It is not good to\nadd dependency on oslo-incubator if we can avoid it.\n\nChange-Id: I9efe3d57e0ce64296c7a75838bd2ee19249f0fa8\nSigned-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>\n'}]",1,124613,05842041754a0481562a73827ecb89c7b8b66326,12,2,2,6549,,,0,"Switch to using oslo.utils

This change switched glance_store to using oslo.utils. It is not good to
add dependency on oslo-incubator if we can avoid it.

Change-Id: I9efe3d57e0ce64296c7a75838bd2ee19249f0fa8
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>
",git fetch https://review.opendev.org/openstack/glance_store refs/changes/13/124613/2 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/test_filesystem_store.py', 'glance_store/openstack/common/excutils.py', 'glance_store/openstack/common/gettextutils.py', 'glance_store/openstack/common/processutils.py', 'glance_store/openstack/common/strutils.py', 'glance_store/openstack/common/units.py', 'tests/unit/test_vmware_store.py', 'glance_store/_drivers/s3.py', 'tests/unit/test_s3_store.py', 'glance_store/_drivers/vmware_datastore.py', 'glance_store/_drivers/swift/store.py', 'glance_store/openstack/common/importutils.py', 'glance_store/_drivers/gridfs.py', 'glance_store/openstack/common/local.py', 'glance_store/_drivers/sheepdog.py', 'glance_store/driver.py', 'openstack-common.conf', 'glance_store/common/utils.py', 'glance_store/openstack/common/timeutils.py', 'tests/unit/test_swift_store.py', 'glance_store/_drivers/filesystem.py', 'glance_store/openstack/common/_i18n.py']",22,3cc8f4f8b246e1fc9cf391db5339f70e473876fc,,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """"""oslo.i18n integration module. See http://docs.openstack.org/developer/oslo.i18n/usage.html """""" import oslo.i18n # NOTE(dhellmann): This reference to o-s-l-o will be replaced by the # application name when this module is synced into the separate # repository. It is OK to have more than one translation function # using the same domain, since there will still only be one message # catalog. _translators = oslo.i18n.TranslatorFactory(domain='glance_store') # The primary translation function using the well-known name ""_"" _ = _translators.primary # Translators for log levels. # # The abbreviated names are meant to reflect the usual use of a short # name like '_'. The ""L"" is for ""log"" and the other letter comes from # the level. _LI = _translators.log_info _LW = _translators.log_warning _LE = _translators.log_error _LC = _translators.log_critical ",,79,1290
openstack%2Ftrove~master~I4a445dedb3a67c0f156f90bb677219d245a78111,openstack/trove,master,I4a445dedb3a67c0f156f90bb677219d245a78111,Updated from global requirements,MERGED,2014-09-24 11:40:53.000000000,2014-09-29 10:10:35.000000000,2014-09-29 10:10:35.000000000,"[{'_account_id': 3}, {'_account_id': 5293}, {'_account_id': 7092}, {'_account_id': 8415}]","[{'number': 1, 'created': '2014-09-24 11:40:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/787aa42a14828afb9ebf9ad95f255d60b7f40653', 'message': 'Updated from global requirements\n\nChange-Id: I4a445dedb3a67c0f156f90bb677219d245a78111\n'}, {'number': 2, 'created': '2014-09-25 04:12:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/c1d8f10e0f1e4baeffdf7c8b197fe208d5bbc33e', 'message': 'Updated from global requirements\n\nChange-Id: I4a445dedb3a67c0f156f90bb677219d245a78111\n'}, {'number': 3, 'created': '2014-09-26 04:00:23.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/trove/commit/d7b3a19b313f74fbf16cdb3c05e30485a9f09f85', 'message': 'Updated from global requirements\n\nChange-Id: I4a445dedb3a67c0f156f90bb677219d245a78111\n'}]",0,123703,d7b3a19b313f74fbf16cdb3c05e30485a9f09f85,21,4,3,11131,,,0,"Updated from global requirements

Change-Id: I4a445dedb3a67c0f156f90bb677219d245a78111
",git fetch https://review.opendev.org/openstack/trove refs/changes/03/123703/2 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,787aa42a14828afb9ebf9ad95f255d60b7f40653,openstack/requirements,python-cinderclient>=1.1.0,python-cinderclient>=1.0.7,1,1
openstack%2Fnova~master~I10034a2fed62d4e93bc384aef3ff594b66e285f2,openstack/nova,master,I10034a2fed62d4e93bc384aef3ff594b66e285f2,Fixes missing ec2 api address disassociate error on failure,MERGED,2014-09-26 07:56:25.000000000,2014-09-29 10:10:21.000000000,2014-09-29 10:10:19.000000000,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 5292}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-26 07:56:25.000000000', 'files': ['nova/exception.py', 'nova/tests/api/ec2/test_cloud.py', 'nova/api/ec2/cloud.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/00d8c334bbfaef3ffec4b04b494e609740b96253', 'message': 'Fixes missing ec2 api address disassociate error on failure\n\nFixes ec2 api address disassociate so that the ec2 error\nInvalidAssociationID.NotFound is returned when it is passed an\ninvalid association rather than returning success. Note this is\nslightly different than the original bug report but the type\nof failure has changed since the bug was originally filed.\n\nChange-Id: I10034a2fed62d4e93bc384aef3ff594b66e285f2\nCloses-Bug: 1179816\n'}]",0,124320,00d8c334bbfaef3ffec4b04b494e609740b96253,17,6,1,5292,,,0,"Fixes missing ec2 api address disassociate error on failure

Fixes ec2 api address disassociate so that the ec2 error
InvalidAssociationID.NotFound is returned when it is passed an
invalid association rather than returning success. Note this is
slightly different than the original bug report but the type
of failure has changed since the bug was originally filed.

Change-Id: I10034a2fed62d4e93bc384aef3ff594b66e285f2
Closes-Bug: 1179816
",git fetch https://review.opendev.org/openstack/nova refs/changes/20/124320/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/exception.py', 'nova/tests/api/ec2/test_cloud.py', 'nova/api/ec2/cloud.py']",3,00d8c334bbfaef3ffec4b04b494e609740b96253,bug/1179816, else: msg = _('Floating ip is not associated.') raise exception.InvalidAssociation(message=msg),,11,3
openstack%2Fneutron~master~I3e3b3354f2821dea6acfd81170cf5d474b44e1ad,openstack/neutron,master,I3e3b3354f2821dea6acfd81170cf5d474b44e1ad,Fix OrderedDict usage for py26 compatibility,ABANDONED,2014-08-20 07:51:15.000000000,2014-09-29 10:01:33.000000000,,"[{'_account_id': 3}, {'_account_id': 5170}, {'_account_id': 6524}, {'_account_id': 6659}, {'_account_id': 7249}, {'_account_id': 7787}, {'_account_id': 7924}, {'_account_id': 8124}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9751}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 12548}]","[{'number': 1, 'created': '2014-08-20 07:51:15.000000000', 'files': ['neutron/tests/unit/test_api_v2.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/39fd4849f42aebb0aeb973ea0f4e1ba086bc4eff', 'message': 'Fix OrderedDict usage for py26 compatibility\n\nOrderedDict was added to `collections` module in python 2.7 [1] so\nif we use python 2.6 we should use OrderedDict from `ordereddict` library.\n\n[1] https://docs.python.org/2/library/collections.html#collections.OrderedDict\n\nChange-Id: I3e3b3354f2821dea6acfd81170cf5d474b44e1ad\n'}]",0,115530,39fd4849f42aebb0aeb973ea0f4e1ba086bc4eff,30,24,1,7491,,,0,"Fix OrderedDict usage for py26 compatibility

OrderedDict was added to `collections` module in python 2.7 [1] so
if we use python 2.6 we should use OrderedDict from `ordereddict` library.

[1] https://docs.python.org/2/library/collections.html#collections.OrderedDict

Change-Id: I3e3b3354f2821dea6acfd81170cf5d474b44e1ad
",git fetch https://review.opendev.org/openstack/neutron refs/changes/30/115530/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/unit/test_api_v2.py'],1,39fd4849f42aebb0aeb973ea0f4e1ba086bc4eff,,try: OrderedDict = collections.OrderedDict except AttributeError: # python 2.6 import ordereddict OrderedDict = ordereddict.OrderedDict args_dict = OrderedDict(, args_dict = collections.OrderedDict(,8,1
openstack%2Fmurano~master~Ida0fb1ccf31988606b4d1af4e5162157f5c94d27,openstack/murano,master,Ida0fb1ccf31988606b4d1af4e5162157f5c94d27,Fixes typo in JsonPatch UT,MERGED,2014-09-29 08:45:19.000000000,2014-09-29 10:00:18.000000000,2014-09-29 10:00:18.000000000,"[{'_account_id': 3}, {'_account_id': 7225}, {'_account_id': 7600}, {'_account_id': 7821}]","[{'number': 1, 'created': '2014-09-29 08:45:19.000000000', 'files': ['murano/tests/unit/dsl/meta/TestEngineFunctions.yaml'], 'web_link': 'https://opendev.org/openstack/murano/commit/387d309b4a5e034bba3e7c27de34888380b0272b', 'message': 'Fixes typo in JsonPatch UT\n\nUnit test became broken because of a type once jsonpatch library\nupdate to version 1.8\n\nChange-Id: Ida0fb1ccf31988606b4d1af4e5162157f5c94d27\n'}]",0,124681,387d309b4a5e034bba3e7c27de34888380b0272b,9,4,1,7226,,,0,"Fixes typo in JsonPatch UT

Unit test became broken because of a type once jsonpatch library
update to version 1.8

Change-Id: Ida0fb1ccf31988606b4d1af4e5162157f5c94d27
",git fetch https://review.opendev.org/openstack/murano refs/changes/81/124681/1 && git format-patch -1 --stdout FETCH_HEAD,['murano/tests/unit/dsl/meta/TestEngineFunctions.yaml'],1,387d309b4a5e034bba3e7c27de34888380b0272b,," value: [1, 3]"," value': [1, 3]",1,1
openstack%2Fnova~stable%2Ficehouse~If5cd9a6f5948c08ef5f1cba2eda31f1175709137,openstack/nova,stable/icehouse,If5cd9a6f5948c08ef5f1cba2eda31f1175709137,Adds get_instance_disk_info to compute drivers,MERGED,2014-08-22 13:32:29.000000000,2014-09-29 10:00:04.000000000,2014-09-29 10:00:01.000000000,"[{'_account_id': 3}, {'_account_id': 1313}, {'_account_id': 1420}, {'_account_id': 3185}, {'_account_id': 5170}, {'_account_id': 8213}, {'_account_id': 8543}, {'_account_id': 9656}, {'_account_id': 10635}]","[{'number': 1, 'created': '2014-08-22 13:32:29.000000000', 'files': ['nova/virt/hyperv/driver.py', 'nova/tests/virt/hyperv/test_hypervapi.py', 'nova/virt/vmwareapi/driver.py', 'nova/tests/virt/vmwareapi/test_driver_api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/21551885f56d05b16785129b6c2995b1ab22cc76', 'message': 'Adds get_instance_disk_info to compute drivers\n\nSolves a live migration regression issue by adding the\nget_instance_disk_info to the compute drivers that do not\nhave it already implemented.\n\nThe method is called by the compute manager.\n\nChange-Id: If5cd9a6f5948c08ef5f1cba2eda31f1175709137\nCloses-Bug: #1358719\nCo-Authored-By: Ionut Balutoiu <ibalutoiu@cloudbasesolutions.com>\n(cherry picked from commit 9c9e44e111abafde1e05a5e38f7e1645ec8ba6df)\n'}]",0,116272,21551885f56d05b16785129b6c2995b1ab22cc76,57,9,1,3185,,,0,"Adds get_instance_disk_info to compute drivers

Solves a live migration regression issue by adding the
get_instance_disk_info to the compute drivers that do not
have it already implemented.

The method is called by the compute manager.

Change-Id: If5cd9a6f5948c08ef5f1cba2eda31f1175709137
Closes-Bug: #1358719
Co-Authored-By: Ionut Balutoiu <ibalutoiu@cloudbasesolutions.com>
(cherry picked from commit 9c9e44e111abafde1e05a5e38f7e1645ec8ba6df)
",git fetch https://review.opendev.org/openstack/nova refs/changes/72/116272/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/hyperv/driver.py', 'nova/tests/virt/hyperv/test_hypervapi.py', 'nova/virt/vmwareapi/driver.py', 'nova/tests/virt/vmwareapi/test_driver_api.py']",4,21551885f56d05b16785129b6c2995b1ab22cc76,," def test_get_instance_disk_info_is_implemented(self): # Ensure that the method has been implemented in the driver try: disk_info = self.conn.get_instance_disk_info('fake_instance_name') self.assertIsNone(disk_info) except NotImplementedError: self.fail(""test_get_instance_disk_info() should not raise "" ""NotImplementedError"") ",,24,0
openstack%2Fsahara~master~I91d0190417cd9683badf630fe1e025e3b4b9329e,openstack/sahara,master,I91d0190417cd9683badf630fe1e025e3b4b9329e,[DOC] Changed feature matrix for Spark,MERGED,2014-09-26 23:45:15.000000000,2014-09-29 09:59:53.000000000,2014-09-29 09:59:53.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7125}, {'_account_id': 7213}, {'_account_id': 8411}]","[{'number': 1, 'created': '2014-09-26 23:45:15.000000000', 'files': ['doc/source/userdoc/features.rst'], 'web_link': 'https://opendev.org/openstack/sahara/commit/70c22daff1ad9c19162507374e9311db8c53a439', 'message': '[DOC] Changed feature matrix for Spark\n\nChange-Id: I91d0190417cd9683badf630fe1e025e3b4b9329e\nCloses-Bug: #1374550\n'}]",0,124544,70c22daff1ad9c19162507374e9311db8c53a439,14,5,1,8411,,,0,"[DOC] Changed feature matrix for Spark

Change-Id: I91d0190417cd9683badf630fe1e025e3b4b9329e
Closes-Bug: #1374550
",git fetch https://review.opendev.org/openstack/sahara refs/changes/44/124544/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/userdoc/features.rst'],1,70c22daff1ad9c19162507374e9311db8c53a439,bug/1374550,| Cluster Scaling | x | Scale Up | x | x || Swift Integration | x | x | x | N/A || EDP | x | x | x | x |,| Cluster Scaling | x | Scale Up | x | TBD || Swift Integration | x | x | x | TBD || EDP | x | x | x | TBD |,3,3
openstack%2Fsahara~master~Ia131e17c87cf40ce5f52eb4cfdf156308cf28e56,openstack/sahara,master,Ia131e17c87cf40ce5f52eb4cfdf156308cf28e56,Add CDH plugin in plugin availability matrix (userdoc),MERGED,2014-09-26 16:17:09.000000000,2014-09-29 09:59:42.000000000,2014-09-29 09:59:41.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 8411}]","[{'number': 1, 'created': '2014-09-26 16:17:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/c6ac704cc1331f3425e14d99749d792eec6450d9', 'message': 'Add CDH plugin in plugin availability matrix (userdoc)\n\nChange-Id: Ia131e17c87cf40ce5f52eb4cfdf156308cf28e56\nCloses-Bug: 1371426\n'}, {'number': 2, 'created': '2014-09-26 16:18:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/74cb10df1c0603680b7e35342688e4d58bc975dd', 'message': 'Add CDH plugin in plugin availability matrix (userdoc)\n\nChange-Id: Ia131e17c87cf40ce5f52eb4cfdf156308cf28e56\nCloses-Bug: 1371426\n'}, {'number': 3, 'created': '2014-09-26 16:50:03.000000000', 'files': ['doc/source/userdoc/features.rst'], 'web_link': 'https://opendev.org/openstack/sahara/commit/82dd380b872f4974d3a6124055ce84ebe6457efd', 'message': 'Add CDH plugin in plugin availability matrix (userdoc)\n\nChange-Id: Ia131e17c87cf40ce5f52eb4cfdf156308cf28e56\nCloses-Bug: 1371426\n'}]",1,124457,82dd380b872f4974d3a6124055ce84ebe6457efd,20,4,3,9548,,,0,"Add CDH plugin in plugin availability matrix (userdoc)

Change-Id: Ia131e17c87cf40ce5f52eb4cfdf156308cf28e56
Closes-Bug: 1371426
",git fetch https://review.opendev.org/openstack/sahara refs/changes/57/124457/3 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/userdoc/features.rst'],1,c6ac704cc1331f3425e14d99749d792eec6450d9,bug/1371426,+--------------------------+---------+----------+----------+-------+ | | Plugin | | +---------+----------+----------+-------+ | Feature | Vanilla | HDP | Cloudera | Spark | +==========================+=========+==========+==========+=======+ | Nova and Neutron network | x | x | x | x | +--------------------------+---------+----------+----------+-------+ | Cluster Scaling | x | Scale Up | x | TBD | +--------------------------+---------+----------+----------+-------+ | Swift Integration | x | x | x | TBD | +--------------------------+---------+----------+----------+-------+ | Cinder Support | x | x | x | x | +--------------------------+---------+----------+----------+-------+ | Data Locality | x | x | TBD | x | +--------------------------+---------+----------+----------+-------+ | EDP | x | x | x | TBD | +--------------------------+---------+----------+----------+-------+,+--------------------------+---------+--------------+----------+ | | Plugin | | +---------+--------------+----------+ | Feature | Vanilla | HDP | Spark | +==========================+=========+==============+==========+ | Nova and Neutron network | x | x | x | +--------------------------+---------+--------------+----------+ | Cluster Scaling | x | Scale Up | TBD | +--------------------------+---------+--------------+----------+ | Swift Integration | x | x | TBD | +--------------------------+---------+--------------+----------+ | Cinder Support | x | x | x | +--------------------------+---------+--------------+----------+ | Data Locality | x | x | x | +--------------------------+---------+--------------+----------+ | EDP | x | x | TBD | +--------------------------+---------+--------------+----------+,17,17
openstack%2Ftempest~master~I263bd4ef45ea02b18c6d00b6e27f0b12442be9ee,openstack/tempest,master,I263bd4ef45ea02b18c6d00b6e27f0b12442be9ee,Unskip test_update_port_with_second_ip(),MERGED,2014-09-15 14:15:16.000000000,2014-09-29 09:59:32.000000000,2014-09-29 09:59:31.000000000,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 5196}, {'_account_id': 5803}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-15 14:15:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5d5be11b071019125b71617b8ad3ece6e3e247af', 'message': 'Unskip test_update_port_with_second_ip()\n\ntest_update_port_with_second_ip() was skipped because of bug 1364166\nwhich is now in a state which should allow us to unskip it.\n\nChange-Id: I263bd4ef45ea02b18c6d00b6e27f0b12442be9ee\nRelated-Bug: #1364166\n'}, {'number': 2, 'created': '2014-09-15 19:08:02.000000000', 'files': ['tempest/api/network/test_ports.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/c987536940ec0894550df8e1431af5a9d0eb2d38', 'message': 'Unskip test_update_port_with_second_ip()\n\ntest_update_port_with_second_ip() was skipped because of bug 1364166\nwhich is now in a state which should allow us to unskip it.\n\nChange-Id: I263bd4ef45ea02b18c6d00b6e27f0b12442be9ee\nRelated-Bug: #1364166\n'}]",0,121576,c987536940ec0894550df8e1431af5a9d0eb2d38,13,5,2,5196,,,0,"Unskip test_update_port_with_second_ip()

test_update_port_with_second_ip() was skipped because of bug 1364166
which is now in a state which should allow us to unskip it.

Change-Id: I263bd4ef45ea02b18c6d00b6e27f0b12442be9ee
Related-Bug: #1364166
",git fetch https://review.opendev.org/openstack/tempest refs/changes/76/121576/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/network/test_ports.py'],1,5d5be11b071019125b71617b8ad3ece6e3e247af,bug/1364166,," @test.skip_because(bug=""1364166"")",0,1
openstack%2Ftempest~master~I3b63bc730f993cdb626c5a77c18ae917d98e911b,openstack/tempest,master,I3b63bc730f993cdb626c5a77c18ae917d98e911b,Add a test for deleting multiple objects by POST method,MERGED,2014-08-25 04:28:21.000000000,2014-09-29 09:59:22.000000000,2014-09-29 09:59:21.000000000,"[{'_account_id': 3}, {'_account_id': 5196}, {'_account_id': 5803}, {'_account_id': 7350}, {'_account_id': 8859}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-25 04:28:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8ae30fe3431c7e863778a50c53566accbc0f39ff', 'message': ""Add a test for deleting multiple objects by POST method\n\nFrom swift Icehouse version, bulk deletion supports POST method as well as\nDELETE.\n\nExtra fixes:\n - Bulk is an optional function with WSGI middleware, therefore each test\n   is annotated with @test.requires_ext\n - Simplify delete_account method of the Swift's account service client\n\nPartially implements blueprint add-icehouse-swift-tests\n\nChange-Id: I3b63bc730f993cdb626c5a77c18ae917d98e911b\n""}, {'number': 2, 'created': '2014-09-11 07:06:20.000000000', 'files': ['tempest/services/object_storage/account_client.py', 'tempest/api/object_storage/test_account_bulk.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/e3f6ed35a9b00107bb13153523ca3899ff28e1d0', 'message': ""Add a test for deleting multiple objects by POST method\n\nFrom swift Icehouse version, bulk deletion supports POST method as well as\nDELETE.\n\nExtra fixes:\n - Bulk is an optional function with WSGI middleware, therefore each test\n   is annotated with @test.requires_ext\n - Refactor test_account_bulk.py by separating common codes to one method\n - Simplify delete_account method of the Swift's account service client\n\nPartially implements blueprint add-icehouse-swift-tests\n\nChange-Id: I3b63bc730f993cdb626c5a77c18ae917d98e911b\n""}]",6,116547,e3f6ed35a9b00107bb13153523ca3899ff28e1d0,15,6,2,8859,,,0,"Add a test for deleting multiple objects by POST method

From swift Icehouse version, bulk deletion supports POST method as well as
DELETE.

Extra fixes:
 - Bulk is an optional function with WSGI middleware, therefore each test
   is annotated with @test.requires_ext
 - Refactor test_account_bulk.py by separating common codes to one method
 - Simplify delete_account method of the Swift's account service client

Partially implements blueprint add-icehouse-swift-tests

Change-Id: I3b63bc730f993cdb626c5a77c18ae917d98e911b
",git fetch https://review.opendev.org/openstack/tempest refs/changes/47/116547/2 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/services/object_storage/account_client.py', 'tempest/api/object_storage/test_account_bulk.py']",2,8ae30fe3431c7e863778a50c53566accbc0f39ff,bp/add-icehouse-swift-tests," @test.requires_ext(extension='bulk', service='object') @test.requires_ext(extension='bulk', service='object') self.assertEqual(int(resp['status']), 204) self.assertHeaders(resp, 'Account', 'GET') self.assertNotIn(container_name, body) @test.attr(type='gate') @test.requires_ext(extension='bulk', service='object') def test_bulk_delete_by_POST(self): # Test bulk operation of deleting multiple files filepath, container_name, object_name = self._create_archive() params = {'extract-archive': 'tar'} with open(filepath) as fh: mydata = fh.read() resp, body = self.account_client.create_account(data=mydata, params=params) data = '%s/%s\n%s' % (container_name, object_name, container_name) params = {'bulk-delete': ''} resp, body = self.account_client.create_account_metadata( {}, data=data, params=params) # When deleting multiple files using the bulk operation, the response # does not contain 'content-length' header. This is the special case, # therefore the existence of response headers is checked without # custom matcher. self.assertIn('transfer-encoding', resp) self.assertIn('content-type', resp) self.assertIn('x-trans-id', resp) self.assertIn('date', resp) # Check only the format of common headers with custom matcher self.assertThat(resp, custom_matchers.AreAllWellFormatted()) # Check if a container is deleted param = {'format': 'txt'} resp, body = self.account_client.list_account_containers(param) self.assertEqual(int(resp['status']), 204)",,53,6
openstack%2Fhorizon~master~I4daa1763b261260bfdea115981814b6e91ac9c19,openstack/horizon,master,I4daa1763b261260bfdea115981814b6e91ac9c19,Imported Translations from Transifex,MERGED,2014-09-29 06:03:40.000000000,2014-09-29 09:59:17.000000000,2014-09-29 09:59:16.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 6914}]","[{'number': 1, 'created': '2014-09-29 06:03:40.000000000', 'files': ['openstack_dashboard/locale/zh_CN/LC_MESSAGES/django.po', 'openstack_dashboard/locale/pt_BR/LC_MESSAGES/django.po'], 'web_link': 'https://opendev.org/openstack/horizon/commit/0296a4ee30dc91d3d0ab2268c4b7d571622659b4', 'message': 'Imported Translations from Transifex\n\nChange-Id: I4daa1763b261260bfdea115981814b6e91ac9c19\n'}]",0,124665,0296a4ee30dc91d3d0ab2268c4b7d571622659b4,7,3,1,11131,,,0,"Imported Translations from Transifex

Change-Id: I4daa1763b261260bfdea115981814b6e91ac9c19
",git fetch https://review.opendev.org/openstack/horizon refs/changes/65/124665/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/locale/zh_CN/LC_MESSAGES/django.po', 'openstack_dashboard/locale/pt_BR/LC_MESSAGES/django.po']",2,0296a4ee30dc91d3d0ab2268c4b7d571622659b4,transifex/translations,"# Felipe Costa Batista <felipecosta.fcb@gmail.com>, 2014""PO-Revision-Date: 2014-09-29 03:40+0000\n"" ""Last-Translator: Felipe Costa Batista <felipecosta.fcb@gmail.com>\n""msgstr ""\nAdicionar, alterar ou remover o QOS Spec associado a este tipo de volume. <br> \""Nenhum\"" indica que nenhuma QOS Spec est atualmente associado. Por outro lado, a definio da QOS Spec para \""None\"" para remover o atual associao. <br> Este  equivalente ao <tt> cinder qos-associate </ tt> e <tt> cincer qos-disassociate </ tt> comandos.""msgstr ""\nQOS Specs pode ser associado com os tipos de volume.\nEle  usado para mapear um conjunto de qualidade de capacidades de servio solicitado\npelo proprietrio do volume. Isto  equivalente ao\n<tt> qos-criar cinza comando </ tt>. Uma vez que o QOS Spec  criado,\nclique no boto \""Gerenciar Specs\"" para gerenciar as especificaes de valores-chave para o QOS Spec.\n<br>\n<br>\nCada entidade QOS Specs ter um valor de \""consumidor\"", que indica onde o\nadministrador gostaria que a diretiva de QoS para ser executada. Este valor pode ser \""front-end\""\n(Nova Compute), \""back-end\"" (Cinder back-end), ou ambos.""msgstr ""\nTipo de Volume  um tipo ou o rtulo que pode ser selecionado na criao de volume\ntempo em OpenStack. Geralmente, os mapas para um conjunto de capacidades de armazenamento\ncontrolador de back-end para ser usado para este volume. Exemplos: \""Performance\"",\n\""SSD\"", \""Backup\"", etc Isto  equivalente ao\n<tt> cinder type-create</ tt>. Uma vez que o tipo de volume  criado,\nclique no boto \""Ver Specs extra\"" para configurar especificaes extras de valor-chave\npar (es) para aquele tipo de volume""msgstr ""\nCada entidade QOS Specs ter um valor de \""consumidor\"", que indica onde o\nadministrador gostaria que a diretiva de QoS para ser executada. Este valor pode ser \""front-end\"" \n(Nova Compute), \""back-end\"" (Cinder back-end), ou ambos.""msgstr ""\nO status de um volume normalmente  gerenciado automaticamente. Em algumas circunstncias onde p administrador pode precisar atualizar explicitamente o valor de status. Isto  equivalente ao\n<tt> ccinder reset-state </ tt> comando.""msgstr ""No  possvel recuperar as atribuies de funo de domnio do usurio.""msgstr ""Voc no pode revogar seus privilgios administrativos a partir do domnio que voc est conectado no momento. Por favor, mude para outro domnio com privilgios administrativos ou remover a funo administrativa manualmente atravs do CLI.""msgstr ""Os grupos so usados para gerenciar o acesso e atribuir funes para vrios usurios ao mesmo tempo. Depois de criar o grupo, editar o grupo para adicionar usurios.""msgstr ""Os grupos so usados para gerenciar o acesso e atribuir funes para vrios usurios ao mesmo tempo. Edite o grupo para adicionar usurios.""msgstr ""Nvel de privilgio insuficiente para exibir informaes de funo""msgstr ""No  possvel recuperar as informaes do usurio""msgstr ""Os grupos de segurana so conjuntos de regras de filtragem de IP que so aplicados s configuraes de rede da VM. Depois que o grupo de segurana  criado, voc pode adicionar regras ao grupo de segurana.""msgstr ""Os grupos de segurana so conjuntos de regras de filtragem de IP que so aplicados s configuraes de rede da VM. Edite o grupo de segurana para adicionar e alterar as regras.""msgstr ""Faa uma nova cpia de um objeto existente para armazenar neste ou em outro recipiente. Alm disso, voc pode especificar o caminho dentro do recipiente escolhido que a nova cpia deve ser armazenado.""msgstr ""No foi possvel obter detalhes do grupo de node.""msgstr ""No foi possvel obeter modelo de cluster.""msgstr ""Este modelo de Cluseter ser criado para:""msgstr ""Verso Hadoop""msgstr ""O mode de cluter deve especificar modelos de node group que sero usados para construir um Hadoop Cluster.\nVoc pode adicionar node group usando modelos de node group em Grupos & quotNode & tab quot.""msgstr ""Voc pode configurar <b> cluster </ b> com configuraes do escopo Hadoop em guias correspondentes.""msgstr ""O modelo de cluster pode especificar uma lista de processos em anti-affinity group.\nIsso significa que esses processos no podem ser lanados mais de uma vez em um nico host.""msgstr ""Selecione um plugin e verso Hadoop para um novo modelo de cluster.""msgstr ""Viso geral Template""msgstr ""Anti-affinity ativada Para""msgstr ""Configuraes de Cluster no esto especificadas.""msgstr ""Flavor no  especificado""msgstr ""Modelo no especificado""msgstr ""Escolha um modelo de Node Group para adicionar""msgstr ""Criar Modelo de Cluster""msgstr ""Detalhes do modelo de cluster""msgstr ""Cpia de modelo de Cluster %s criada""msgstr ""No foi possvel obter modelo para copiar.""msgstr ""No foi possvel obter lista de plugin""msgstr ""Escolha um plugin e verso do Hadoop para o modelo de cluster""msgstr ""No foi possvel criar""msgstr ""falha na criao de modelo %s""msgstr ""Falha na criao de modelo de cluster""msgstr ""No foi possvel atualizar a linha""msgstr ""Contagem de instncias ""msgstr ""No foi possvel obter detalhes do node group""msgstr ""No foi possvel obter detalhes da instncia""msgstr ""No foi possvel obter a lista de cluster""msgstr ""Este Cluster ser iniciado com:""msgstr ""Cluster pode ser lanado usando modelos cluster existente.""msgstr ""O objeto Cluster deve especificar Imagem OpenStack para iniciar instncias para Hadoop Cluster.""msgstr ""O usurio tem que escolher um keypair para ter acesso a instncias de clusters.""msgstr ""Selecione um plugin e verso Hadoop para um novo cluster.""msgstr ""Viso geral Cluster""msgstr ""Neutron Management Network""msgstr ""Escolha um plugin e verso do Hadoop para clusters""msgstr ""No foi possvel obter opes de imagem.""msgstr ""No foi possvel obter opces de keypair .""msgstr ""Cluster escalado iniciado com sucesso.""msgstr ""No foi possvel obter cluster para escalar.""msgstr ""No foi possvel obter cluster para escalar.""msgstr ""Operao de escalar cluster falhou""msgstr ""Imagem atualizada com sucesso.""msgstr ""Falha ao atualizar imagem.""msgstr ""No  possvel recuperar as imagens com filtro %s ""msgstr ""No foi possvel obter imagens disponveis.""msgstr[0] ""Imagens no registradas"" msgstr[1] ""Imagens no registradas""msgstr[0] ""imagem no regitrada"" msgstr[1] ""Imagens no registradas""msgstr ""No  possvel recuperar lista de imagens""msgstr ""No  possvel processar as tags do plugin""msgstr ""No foi possvel obter os detalhes da imagem""msgstr ""Ferramenta de registro de imagem:""msgstr ""Registro de imagem  utilizado para fornecer informaes adicionais sobre as imagens para processamento de dados.""msgstr ""Nome de usurio especificado ser usado pelo processamento de dados para aplicar configuraes e gerenciar os processos em instncias.""msgstr ""Os cdigos so utilizados para filtrar imagens adequadas para cada plugin e cada Verso de Processamento de Dados. Para adicionar marcas necessrias, selecionar um plug-in e uma verso de Processamento de Dados e clique em &quot;Add plugin tags&quot; button.""msgstr ""Voc tambm pode adicionar qualquer tag personalizado.""msgstr ""Etiquetas desnecessrias podem ser removidos clicando no nome de uma tag.""msgstr ""Registre marcas necessrias para o Plugin com verso de Processamento de Dados especificado""msgstr ""No  possvel recuperar plugin.""msgstr ""No  possvel recuperar plugins processamento de dados.""msgstr ""Plugins de Processamento de Dados""msgstr ""Data Sources""msgstr ""Criar Data Source""msgstr ""No  possvel recuperar detalhes do Data Source""msgstr ""No foi possvel obter Data Sources.""msgstr ""Criar uma Data Source com um nome especificado.""msgstr ""Selecionar o tipo do seu Data Source""msgstr ""Voc pode precisar digitar o nome de usurio e senha para seu Data Source.""msgstr ""Voc tambm pode inserir uma descrio opcional para o seu Data Source.""msgstr ""Viso Geral do Data Source""msgstr ""Detalhes do Data Source""msgstr ""tipo do Data Source""msgstr ""Data Source criado""msgstr ""No foi possvel criar Data Source""msgstr ""binrio interno""msgstr ""Falha ao obter a lista de binrios internos.""msgstr ""No  possvel criar job binrio""msgstr ""Criar Job Binrio""msgstr ""No  possvel fazer upload job binrio""msgstr ""Falha ao buscar lista de binrio interno.""msgstr ""Job binrio""msgstr[0] ""deletar job binario"" msgstr[1] ""Deletar job binrios""msgstr[0] ""Job binario excluido"" msgstr[1] ""Job binrios excluidos""msgstr ""Baixar Job Binrio""msgstr ""No foi possvel obter job binrios.""msgstr ""No foi possvel obter lista de Job binrios.""msgstr ""No foi possvel obter job binrio: %(exc)s""msgstr ""<b> Importante </ b>: O nome que voc d o seu job binrio ser o nome usado em seu job execution.\nSe o seu binrio requer um determinado nome ou extenso (ie: \""jar\""), certifique-se de inclu-lo aqui.""msgstr ""Seleciona o tipo de armazenamento para o seu Job binrio.""msgstr ""Processamento de dados interno database""msgstr ""Para o processamento de dados job binrios interno, voc pode escolher uma das seguintes opes:""msgstr ""Criar um script para ser carregado dinamicamente""msgstr ""Para objeto de armazenamento de job binrios, voc deve:""msgstr ""Digite a URL para o arquivo""msgstr ""Digite o nome de usurio e a senha necessrios para acessar esse arquivo""msgstr ""Voc tambm pode inserir uma descrio opcional para o seu job binrio.""msgstr ""Viso geral Job Binrio""msgstr ""Download Job Binrio""msgstr ""Detalhes Job binrio""msgstr ""Execuo Job""msgstr[0] ""Deletar execuo job"" msgstr[1] ""Deletar execuo do Jobs""msgstr[0] ""Jog execuo deletada."" msgstr[1] ""Job execuo deletada""msgstr[0] ""Iniciar Job"" msgstr[1] ""Iniciar Jobs""msgstr[0] ""Job iniciado"" msgstr[1] ""Jobs Iniciado""msgstr ""Reiniciar em cluster existente.""msgstr ""Reiniciar em um novo cluster""msgstr ""No foi possvel obter as execues de job.""msgstr ""Overview execuo job""msgstr ""Entrada Data Source""msgstr ""Sada Data source""msgstr ""Finalizar""msgstr ""Cdigo de retorno""msgstr ""Oozie Job ID""msgstr ""Configurao do Job""msgstr ""Detalhes da execuo do Job""msgstr ""Criar Job""msgstr[0] ""Deletar Job"" msgstr[1] ""Deletar jobs""msgstr[0] ""Job deletado"" msgstr[1] ""Jobs deletados""msgstr ""Iniciar em cluster existente""msgstr ""Iniciar em Novo cluster""msgstr ""No foi possvel obter jobs""msgstr ""Criar um Job com um nome especificado.""msgstr ""Seleciona o tipo do seu job:""msgstr ""Pig""msgstr ""hive""msgstr ""Spark""msgstr ""MapReduce""msgstr ""Java Action""msgstr ""Escolha ou crie o seu binrio principal. Bibliotecas adicionais podem ser adicionados a partir da aba \""libs\"".""msgstr ""Para os Spark Jobs, apenas uma principal  necessria, \""libs\"" so opcionais.""msgstr ""Para MapReduce ou Java Action Jobs, \""mains\"" no so aplicveis. Voc  obrigado a adicionar uma ou mais \""libs\"" para estes trabalhos.""msgstr ""Voc tambm pode inserir uma descrio opcional para o seu job.""msgstr ""Adicione bibliotecas para o seu Job""msgstr ""Escolha na lista de jobs e clique em \""escolher\"" para adicionar a biblioteca para o seu jobs. Esta informao poder ser repetido para bibliotecas adicionais.""msgstr ""Mains""msgstr ""Libs""msgstr ""Digite qualquer configurao personalizada necessria para a execuo do seu job.""msgstr ""Inicie o job dado em um cluster.""msgstr ""Escolha o cluster para usar para a execuo do job.""msgstr ""Escolha a entrada Data Source (n / a para java jobs).""msgstr ""Escolha a sada data source (n / a para java jobs).""msgstr ""parmetros""msgstr ""argumentos""msgstr ""Detalhes do job""msgstr ""Escolha""msgstr ""Bibliotecas escolhidas""msgstr ""Escolha bibliotecas""msgstr ""-- No Selecionado --""msgstr ""Tipo Job""msgstr ""Escolha um binrio principal""msgstr ""Escolha o binrio que deve ser utilizada neste job.""msgstr ""Streaming MapReduce""msgstr ""Job criado""msgstr ""No foi possvel criar job""msgstr ""Input""msgstr ""Output""msgstr ""No foi possvel obter clusters.""msgstr ""Classe principal""msgstr ""Java Opts""msgstr ""Mapeador""msgstr ""Redutor""msgstr ""Configurar""msgstr ""Persistir cluster aps sair do job""msgstr ""Job iniciado""msgstr ""No foi possvel iniciar o trabalho""msgstr ""Job configuraes""msgstr ""Job Argumentos""msgstr ""Job parmetros""msgstr ""ID de execuo do JOB""msgstr ""No  possvel criar novo cluster para o JOB.""msgstr ""No  possvel iniciar o job.""msgstr ""Modelo de Node Group""msgstr ""Configurar Modelo""msgstr ""No foi possvel obter modelo node group.""msgstr ""No foi possvel obter flavor para o modelo.""msgstr ""No foi possvel obter pools de ip flutuante.""msgstr ""Configuraes de Servios""msgstr ""No foi possvel obter lista de modelos do node group""msgstr ""No foi possvel obter modelo de objeto.""msgstr ""Este modelo de node group ser criado para:""msgstr ""O modelo de objeto node group deve especificar os processos que sero lanados em cada instncia. Tambm um flavor OpenStack  necessrio para inicializar VMs.""msgstr ""Processamento de Dados oferece diferentes opes de localizao de armazenamento. Voc pode escolher Ephemereal Drive ou um Volume Cinder para ser anexado s instncias.""msgstr ""Quando os processos so selecionados, voc pode definir <b> n </ b> com escopo configuraes Hadoop nas guias correspondentes.""msgstr ""Selecione um plugin e verso Hadoop para um novo modelo de node group.""msgstr ""colocao HDFS""msgstr ""Volumes Cinder""msgstr ""Volumes por node""msgstr ""Tamanho do Volume""msgstr ""Ephemeral drive""msgstr ""Mostrar configurao Completa""msgstr ""Esconder configurao completa""msgstr ""Criar Mode de Node Group""msgstr ""Detalhes do modelo de Nodegroup""msgstr ""Detalhes de modelo do Node group""msgstr ""Copia do Modelo de Node Group %s Criado""msgstr ""No foi possvel obter detalhes de plugin.""msgstr ""OpenStack Flavor""msgstr ""Local de Armazenamento""msgstr ""Escolha o local de armazenamento""msgstr ""Tamanho dos Volumes (GB)""msgstr ""No foi possvel gerar opes de processo.""msgstr ""Pool de Ip Flutuante""msgstr ""Processos""msgstr ""Processos a serem lanados no node group""msgstr ""Configurar Template Node Group""msgstr ""Template Criado Node group %s""msgstr ""Escolha um plugin e verso do Hadoop""msgstr ""Use grupos anti-affinity para:""msgstr ""Use grupos anti-affinity para processos""msgstr ""Incapaz de preencher os processos anti-affinity.""msgstr ""Node group cluster""msgstr ""Contagem""msgstr ""Nome Plugin""msgstr ""Datastore""msgstr ""Verso Datastore""msgstr ""Incremental""msgstr ""No  possvel recuperar detalhes do Backup: %s""msgstr ""Especifique os detalhes para o backup do banco de dados.""msgstr ""Voc pode realizar um backup incremental, especificando um backup pai. <strong> No entanto, </ strong> nem todos os bancos de dados oferece suporte a backups incrementais, caso em que esta operao ir resultar em um erro.""msgstr ""Detalhes do Backup""msgstr ""Detalhes do Backup:""msgstr ""Incremental Backup""msgstr ""Backup pai""msgstr ""Backup pai Opcional""msgstr ""No foi possvel listar as instncias de banco de dados para backup.""msgstr ""No foi possvel listar os backups de banco de dados para o pai.""msgstr ""Selecione o backup pai""msgstr ""No h backups disponveis""msgstr ""Nova tamanho para o volume tem de ser maior do que o tamanho actual.""msgstr ""Redimensionamento de volume \""%s\""""msgstr ""No  possvel redimensionar volume. %s""msgstr ""Redimensionar volume""msgstr ""No foi possvel obter os dados do usurio.""msgstr ""No  possvel obter dados de bancos de dados.""msgstr ""No  possvel obter dados de backup de banco de dados.""msgstr ""Os bancos de dados iniciais""msgstr ""Permitido Anfitrio (opcional)""msgstr ""\nMova redes de \""Redes disponveis\"" para \""redes selecionadas 'by clicar no boto, ou arrastando e soltando. Voc pode mudar o fim NIC, arrastando e soltando .""msgstr ""Redimensionar Volume de banco de dados""msgstr ""Especifique o novo tamanho do volume para a instncia de banco de dados.""msgstr ""<strong> Ateno: </ strong> O novo valor deve ser maior que o tamanho do volume existente.""msgstr ""Tipo e verso do armazenamento de dados.""msgstr ""Voc deve selecionar um tipo de armazenamento de dados e verso.""msgstr ""No  possvel obter flavors.""msgstr ""Selecione o tipo de armazenamento de dados e verso""msgstr[0] ""Eliminao programada da Regra"" msgstr[1] ""Eliminao programada das Regras""msgstr[0] ""Eliminao programada da Policie"" msgstr[1] ""Eliminao programada das Policies""msgstr[0] ""Eliminao programada do Firewall"" msgstr[1] ""Eliminao programada dos Firewalls""msgstr ""Memriam RAM minima(MB)""msgstr ""As imagens podem ser fornecidas atravs de um URL HTTP ou ser carregado a partir de seu sistema de arquivos local. Imagem Binarias compactada so suportadas (zip e tar.gz).""msgstr ""Se voc selecionar uma imagem atravs de um URL HTTP, o campo Localizao da imagem deve ser uma URL vlida e direta com a imagem binria; ele tambm deve ser acessvel para o Servio de Imagem. URLs que redirecionam ou servir pginas de erro resultar em imagens inutilizveis.""msgstr ""Request ID""msgstr ""Lista de Aes Instncia""msgstr ""Tipo Console \""%s\"" no  suportado.""msgstr[0] ""Terminar instncia"" msgstr[1] ""Terminar Instncias""msgstr[0] ""Agendar terminao da Instncia"" msgstr[1] ""Agendar terminao das instncias""msgstr[0] ""Hard Reboot instncia"" msgstr[1] ""Hard Reboot Instncias""msgstr[0] ""Hard Rebooted instncias"" msgstr[1] ""Hard Rebooted instncias""msgstr[0] ""Soft Reboot instncias"" msgstr[1] ""Soft Reboot instncias""msgstr[0] ""Soft Rebooted instncia"" msgstr[1] ""Soft Rebooted instncias""msgstr[0] ""Pause instncia"" msgstr[1] ""Pause Instncias""msgstr[0] ""Resume Instncia"" msgstr[1] ""Resume Instncias""msgstr[0] ""Instncia Pausada"" msgstr[1] ""Instncias Pausadas""msgstr[0] ""Resumed instncia"" msgstr[1] ""Resumed Instncias""msgstr[0] ""Suspender Instncia"" msgstr[1] ""Suspender Instncias""msgstr[0] ""Instncia Suspendida"" msgstr[1] ""Instncias Suspendida""msgstr[0] ""Iniciar Instncia"" msgstr[1] ""Iniciar Instncias""msgstr[0] ""Desligar Instncia"" msgstr[1] ""Desligar Instncias""msgstr[0] ""Desligar Instncia"" msgstr[1] ""Desligar instncias""msgstr ""Desligar""msgstr ""Resgate""msgstr ""Soft Deleted""msgstr ""arquivado""msgstr ""Arquivoado em Offloaded""msgstr ""Bloco de mapeamento de dispositivos""msgstr ""Spawning""msgstr ""Snapshot Image Pendente""msgstr ""Upload de Imagem pendente""msgstr ""Hard Rebooting""msgstr ""Reconstruir Spawning""msgstr ""Soft Deleting""msgstr ""Arquivamento""msgstr ""Upload de arquivamento de imagem pendente""msgstr ""Upload de arquivamento de imagem""msgstr ""Arquivamento Offloading""msgstr ""Desarquivar""msgstr ""Crashed""msgstr ""No  possvel recuperar lista de aes da instncia.""msgstr ""Especifique opes avanadas para usar quando iniciar uma instncia.""msgstr ""Voc pode personalizar sua instncia depois de ter lanado usando as opes disponveis aqui.""msgstr ""\""Customization Script\""  anlogo ao \""User Data\"" em outros sistemas.""msgstr ""Escolha rede de redes disponveis para as redes selecionadas por boto ou arrastar e soltar, voc pode alterar a ordem de NIC por arrastar e soltar tambm.""msgstr[0] ""O pedido instanciado no pode ser lanado como voc s tem %(avail)i de sua quota disponvel."" msgstr[1] ""O pedido%(req)i instanciado no pode ser lanado como voc s tem %(avail)i de sua quota disponvel.""msgstr ""A instncia requerida no pode ser lanado. A quota seguinte recurso solicitado(s) exceder(s):%s.""msgstr ""Personalizao Source Script""msgstr ""Arquivo de Script""msgstr ""Script Data""msgstr ""Arquivo excede o tamanho mximo (16kb)""msgstr ""Automtico: O disco inteiro  uma nica partio e automaticamente redimensiona. Manual: Resultados no mais rpido tempo de construo, mas requer particionamento manual.""msgstr ""Configurao de Drive""msgstr ""Configurar OpenStack para escrever metadados para uma unidade de configurao especial que atribui  instncia quando ele inicia.""msgstr ""Por favor, escolha um novo flavor que no  o mesmo que o antigo.""msgstr ""Adicionar e remover grupos de segurana para esse projeto a partir da lista de grupos de segurana disponveis.""msgstr ""%(type)s: url:%(url_path)s method:%(http_method)s codes:%(expected_codes)s delay:%(delay)d retries:%(max_retries)d timeout:%(timeout)d""msgstr ""Criar um VIP para este conjunto. Atribua um nome, descrio, endereo IP, porta e conexes mximas permitidas para a VIP. Escolha o mtodo de persistncia e protocolo de sesso para a VIP. Estado Admin  UP (marcada) por padro.""msgstr ""Parte relativa de pedidos este membro do pool serve, em comparao com outros.O mesmo peso ir ser aplicada a todos os membros seleccionados e podem ser modificados posteriormente. O peso deve estar na faixa de 1 a 256.""msgstr ""Introduzir um valor inteiro entre 1 e 65535 A mesma porta ir ser utilizado para todos os membros seleccionados e podem ser modificados posteriormente.""msgstr ""Adicionar membro (s) ao pool. selecionado Escolha um ou mais cotadas casos a serem adicionados  piscina como membro (s). Atribua um peso numrico eo nmero da porta para o membro (s) selecionado para operar (s) on; por exemplo, 80Apenas uma porta pode ser associado com cada exemplo.""msgstr ""No  possvel adicionar membro (s)""msgstr ""No  possvel recuperar o pool especificado.""msgstr ""Voc pode atualizar os atributos piscina aqui: edio de nome, descrio e mtodo de balanceamento de carga ou estado de administrao.""msgstr ""O estado para iniciar a rede em.""msgstr ""Criar uma nova rede. Alm disso, uma sub-rede associado com a rede pode ser criada no painel seguinte.""msgstr ""Endereo de rede em formato CIDR (por exemplo, 192.168.0.0/24, 2001: DB8 :: / 48)""msgstr ""Endereo IP do Gateway (por exemplo, 192.168.0.254) O valor padro  o primeiro IP do endereo de rede (por exemplo, 192.168.0.1 para 192.168.0.0/24, 2001: DB8 :: 1 para 2001: DB8 :: / 48). Se voc usar o padro, deixe em branco. Se voc no quiser usar um gateway, consulte 'Disable Gateway' abaixo.""msgstr ""Criar uma sub-rede associada com a nova rede, em que \""endereo de rede\"" caso deve ser especificado. Se voc deseja criar uma rede sem uma sub-rede, desmarque a caixa de seleo \""Criar sub-rede\"".""msgstr ""Modo de configurao de endereos IPv6""msgstr ""Ele especifica como endereo IPv6 e informaes adicionais so configurados. Podemos especificar SLAAC / DHCPv6 stateful / DHCPv6 stateless fornecido pelo OpenStack, ou especificar nenhuma opo. \""Nenhuma opo selecionada\"" significa endereos so configurados manualmente ou configurado pelo sistema no-OpenStack.""msgstr ""Endereo IP piscinas de alocao. Cada entrada : start_ip_address, end_ip_address (por exemplo, 192.168.1.100,192.168.1.120) e uma entrada por linha.""msgstr ""Rotas adicionais anunciados para os hosts. Cada entrada : destination_cidr, nexthop (por exemplo, 192.168.200.0/24,10.56.1.254) e uma entrada por linha.""msgstr ""Especifique atributos adicionais para a sub-rede.""msgstr ""SLAAC: Endereo descoberto a partir de OpenStack Router""msgstr ""DHCPv6 stateful: Endereo descoberto a partir de OpenStack DHCP""msgstr ""DHCPv6 stateless: Endereo descoberto a partir de OpenStack Router e informaes de OpenStack DHCP""msgstr ""Criar uma sub-rede associado com a rede. A configurao avanada est disponvel clicando na guia \""Detalhes de sub-rede\"".""msgstr ""Endereo IP do Gateway (por exemplo, 192.168.0.254). Especifique um endereo explcito para definir o gateway. Se voc no quiser usar um gateway, consulte \""Disable Gateway\"" abaixo.""msgstr ""Atualizar uma sub-rede associada com a rede. A configurao avanada esto disponveis na guia \""Detalhes de sub-rede\"".""msgstr ""Nenhum dispositivo conectado""msgstr ""\nOutros Modeos IPv6 : ipv6_ra_mode =%(ra_mode)s, ipv6_address_mode =%(addr_mode)s""msgstr ""Quota excedida para router de recursos.""msgstr ""Router%s foi atualizado com sucesso.""msgstr ""external network \""%(ext_net_id)s\"" o esperado, mas no foi encontrado para router \""%(router_id)s\"".""msgstr ""Especifique um endereo IP para a interface criada (por exemplo, 192.168.0.254).""msgstr ""Selecione um modelo para lanar uma pilha.""msgstr ""Selecione um novo modelo para relanar uma pilha.""msgstr ""Atualize uma pilha com os valores fornecidos. Por favor, note que todos os parmetros criptografados, como senhas, ser redefinir para o padro se voc no alter-las aqui.""msgstr ""No  possvel recuperar backups de volume.""msgstr ""No  possvel criar backup de volume.""msgstr ""No foi possvel pesquisar informaes de backup de volume ou.""msgstr ""Restaurados com xito backup %(backup_name)s volume com id:%(volume_id)s""msgstr ""No  possvel recuperar detalhes de backup.""msgstr ""Atualizao de volume snapshot \""%s\""""msgstr ""No  possvel atualizar o volume snapshot.""msgstr[0] ""Eliminao programada de Desconto Snapshot"" msgstr[1] ""Eliminao programada de Desconto Snapshots""msgstr ""Volume backups so armazenados usando o servio de armazenamento de objetos. Voc deve ter este servio ativado, a fim de criar um backup.""msgstr ""Se nenhum nome de recipiente  fornecido, um continer padro chamado volumebackups ser provisionado para voc. Cpias de segurana ser o mesmo tamanho que o volume que se originam a partir de.""msgstr ""Viso geral do Volume de backup ""msgstr ""Detalhes do Volume de backup""msgstr ""Restaurar o backup de um volume""msgstr ""Modificar o nome e a descrio de um snapshot.""msgstr ""Estender o tamanho de um volume.""msgstr ""\nAlterar o tipo de volume de um volume aps a sua criao. Este  equivalente ao <tt> cinder retype </ tt> command.""msgstr ""\nO \""Tipo de Volume\"" selecionada deve ser diferente do type. volume atual""msgstr ""\nA \""Migration Policy\""  usado apenas se o volume no pode ser concludo. Se o \""Migration Policy\""  \""On Demand\"", o back-end will realizar a migrao volume. Note-se que a migrao pode levar uma quantidade significante de tempo para concluir, em alguns casos, horas.""msgstr ""A partir daqui voc pode criar um snapshot de um volume.""msgstr ""\nCarregar o volume para o Servio de imagem como um image.This  equivalente ao comando <tt> cinder upload-a-image </ tt>.""msgstr ""\nEscolha \""Disk Format\"" para a imagem. As imagens de volume so criados a imagem de disco QEMU ultilidade.""msgstr ""\nQuando o status do volume  \""em uso\"", voc pode usar \""force\"" para carregar o\nvolume para uma imagem.""msgstr ""Fonte da imagem deve ser especificado""msgstr ""Fonte Snapshot deve ser especificado""msgstr ""Fonte volume deve ser especificado""msgstr ""Nome do dispositivo real pode ser diferente, devido s opes de hypervisor. Se no for especificado, ento hypervisor vai selecionar um nome de dispositivo.""msgstr ""Enviada com sucesso o pedido de upload de volume a imagem para o volume: \""%s\""""msgstr ""No  possvel fazer upload de volume a imagem para o volume: \""%s\""""msgstr ""Nova tamanho deve ser maior do que o tamanho actual.""msgstr ""O volume no pode ser estendida a %(req)iGB como voc s tem%(avail)iGB da sua quota disponvel.""msgstr ""Estendendo o volume: \""%s\""""msgstr ""No  possvel recuperar a lista de tipos de volume.""msgstr ""Novo tipo de volume deve ser diferente do tipo volume original \""%s\"".""msgstr ""Enviada com sucesso o pedido para alterar o tipo de volume para \""%(vtype)s\"" para o volume:\""%(name)s\""""msgstr ""No  possvel alterar o tipo de volume para volume: \""%s\""""msgstr[0] ""Excluso programada do Volume"" msgstr[1] ""Excluso programada dos Volumes""msgstr ""No  possvel recuperar os limites do tenant.""msgstr ""Volume%(volume_name)s na instncia%(instance_name)s""msgstr ""No  possvel recuperar as informaes de volume para volume: \""%s\""""msgstr ""Igual ou superior a 68 se a sub-rede local  IPv4. Igual ou maior que 1280 se a sub-rede local  IPv6.""msgstr ""O estado a iniciar em.""msgstr ""Criar servio VPN para project. atual Especifique um nome, descrio, roteador e sub-rede para o servio de VPN. Estado Admin  Up (marcada) por padro.""msgstr ""1-4093 para VLAN; 5000 e acima para Overlay""msgstr ""Range Multicast IPv4 (por exemplo. 224.0.1.0-224.0.1.100)""msgstr ""No foi possvel obter perfil de rede de ligao""msgstr ""Edite o perfil de rede para atualizar o nome, faixa de segmento ou faixa de IP multicast.""","""PO-Revision-Date: 2014-09-27 06:20+0000\n"" ""Last-Translator: openstackjenkins <jenkins@openstack.org>\n""msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr[0] """" msgstr[1] """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr[0] """" msgstr[1] """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr[0] """" msgstr[1] """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr[0] """" msgstr[1] """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"",657,656
openstack%2Ftempest~master~I6994a0109c9ec26b290fd4dce9c524dc4a060a54,openstack/tempest,master,I6994a0109c9ec26b290fd4dce9c524dc4a060a54,Cleanup leftover dependencies to official clients,MERGED,2014-09-25 11:06:49.000000000,2014-09-29 09:59:06.000000000,2014-09-29 09:59:05.000000000,"[{'_account_id': 3}, {'_account_id': 5196}, {'_account_id': 5689}, {'_account_id': 5803}, {'_account_id': 8556}, {'_account_id': 8576}]","[{'number': 1, 'created': '2014-09-25 11:06:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/4dce0a228ac31c3ef05af47f8dd13a64f0c29a95', 'message': ""Cleanup leftover dependencies to official clients\n\nSome scenario test classes still references to official clients\nafter the migration. Cleaning them up.\n\nNeutron check_preconditions validates configuration via discovery.\nThe general approach in tempest is to rely on configuration rather\nthan discovery of available services and features.\nThis code uses a neutron client exception, so rather then migrating\nit I'm removing it as it's wrong anyways.\n\nChange-Id: I6994a0109c9ec26b290fd4dce9c524dc4a060a54\nPartially-implements: bp/tempest-client-scenarios\n""}, {'number': 2, 'created': '2014-09-26 03:26:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8bef43601847ed18edcc1f5e770e9789c0803539', 'message': ""Cleanup leftover dependencies to official clients\n\nSome scenario test classes still references to official clients\nafter the migration. Cleaning them up.\n\nNeutron check_preconditions validates configuration via discovery.\nThe general approach in tempest is to rely on configuration rather\nthan discovery of available services and features.\nThis code uses a neutron client exception, so rather then migrating\nit I'm removing it as it's wrong anyways.\n\nChange-Id: I6994a0109c9ec26b290fd4dce9c524dc4a060a54\nPartially-implements: bp/tempest-client-scenarios\n""}, {'number': 3, 'created': '2014-09-26 11:35:36.000000000', 'files': ['tempest/scenario/manager.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/2ddc2636bbc5e42cf0d3780e88b0e7f366bf6b71', 'message': ""Cleanup leftover dependencies to official clients\n\nSome scenario test classes still references to official clients\nafter the migration. Cleaning them up.\n\nNeutron check_preconditions validates configuration via discovery.\nThe general approach in tempest is to rely on configuration rather\nthan discovery of available services and features.\nThis code uses a neutron client exception, so rather then migrating\nit I'm removing it as it's wrong anyways.\n\nChange-Id: I6994a0109c9ec26b290fd4dce9c524dc4a060a54\nPartially-implements: bp/tempest-client-scenarios\n""}]",2,124016,2ddc2636bbc5e42cf0d3780e88b0e7f366bf6b71,15,6,3,1921,,,0,"Cleanup leftover dependencies to official clients

Some scenario test classes still references to official clients
after the migration. Cleaning them up.

Neutron check_preconditions validates configuration via discovery.
The general approach in tempest is to rely on configuration rather
than discovery of available services and features.
This code uses a neutron client exception, so rather then migrating
it I'm removing it as it's wrong anyways.

Change-Id: I6994a0109c9ec26b290fd4dce9c524dc4a060a54
Partially-implements: bp/tempest-client-scenarios
",git fetch https://review.opendev.org/openstack/tempest refs/changes/16/124016/3 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/manager.py'],1,4dce0a228ac31c3ef05af47f8dd13a64f0c29a95,bp/tempest-client-scenarios, if not CONF.service_available.neutron: raise cls.skipException('Neutron not available') except exceptions.NotFound:,"from neutronclient.common import exceptions as exc if CONF.service_available.neutron: cls.enabled = True # verify that neutron_available is telling the truth try: cls.network_client.list_networks() except exc.EndpointNotFound: cls.enabled = False raise else: cls.enabled = False msg = 'Neutron not available' raise cls.skipException(msg) if len(sgs) > 1: msg = ""Found %d default security groups"" % len(sgs) raise exc.NeutronClientNoUniqueMatch(msg=msg) from ironicclient import exc as ironic_exceptions except ironic_exceptions.HTTPNotFound:",3,18
openstack%2Fhorizon~master~Id768349b132c91851839bfd208e955ddc6954b41,openstack/horizon,master,Id768349b132c91851839bfd208e955ddc6954b41,Job Execuiton form improvement,MERGED,2014-09-01 13:02:14.000000000,2014-09-29 09:46:24.000000000,2014-09-29 09:46:23.000000000,"[{'_account_id': 3}, {'_account_id': 5623}, {'_account_id': 6786}, {'_account_id': 6914}, {'_account_id': 7132}, {'_account_id': 8090}]","[{'number': 1, 'created': '2014-09-01 13:02:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/316826e5c9ab3cb59bd98176d7cc3277a5d0ee8b', 'message': 'Job Execuiton form improvement\n\nThe form controls are now aligned using bootstrap 3 classes. JS parts\nupdated to work with new layout.\n\nChange-Id: Id768349b132c91851839bfd208e955ddc6954b41\n'}, {'number': 2, 'created': '2014-09-02 08:05:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/be491e9cc76cc2f279858be17de97aeaf40f4471', 'message': 'Job Execuiton form improvement\n\nThe form controls are now aligned using bootstrap 3 classes. JS parts\nupdated to work with new layout.\n\nChange-Id: Id768349b132c91851839bfd208e955ddc6954b41\n'}, {'number': 3, 'created': '2014-09-23 22:40:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/f3972735d2c2cb25ede2e43d8f04826c188702f1', 'message': 'Job Execuiton form improvement\n\nThe form controls are now aligned using bootstrap 3 classes. JS parts\nupdated to work with new layout.\n\nCloses-Bug: #1373160\n\nChange-Id: Id768349b132c91851839bfd208e955ddc6954b41'}, {'number': 4, 'created': '2014-09-24 13:55:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/eef60342e05d956d741f046dac34ce07c4e6437a', 'message': 'Job Execuiton form improvement\n\nThe form controls are now aligned using bootstrap 3 classes. JS parts\nupdated to work with new layout.\n\nCloses-Bug: #1373160\n\nChange-Id: Id768349b132c91851839bfd208e955ddc6954b41'}, {'number': 5, 'created': '2014-09-25 10:27:34.000000000', 'files': ['openstack_dashboard/dashboards/project/data_processing/jobs/templates/data_processing.jobs/config_template.html'], 'web_link': 'https://opendev.org/openstack/horizon/commit/75712a62e010580d7b69b5f1fcf0b3ff757c7ede', 'message': 'Job Execuiton form improvement\n\nThe form controls are now aligned using bootstrap 3 classes. JS parts\nupdated to work with new layout.\n\nCloses-Bug: #1373160\n\nChange-Id: Id768349b132c91851839bfd208e955ddc6954b41'}]",0,118159,75712a62e010580d7b69b5f1fcf0b3ff757c7ede,22,6,5,7132,,,0,"Job Execuiton form improvement

The form controls are now aligned using bootstrap 3 classes. JS parts
updated to work with new layout.

Closes-Bug: #1373160

Change-Id: Id768349b132c91851839bfd208e955ddc6954b41",git fetch https://review.opendev.org/openstack/horizon refs/changes/59/118159/5 && git format-patch -1 --stdout FETCH_HEAD,['openstack_dashboard/dashboards/project/data_processing/jobs/templates/data_processing.jobs/config_template.html'],1,316826e5c9ab3cb59bd98176d7cc3277a5d0ee8b,," <tr id_attr=""$id"" class=""row data-row""> <td class=""col-sm-5 small-padding""> <input type=""text"" class=""form-control"" field=""key"" list=""properties"" placeholder=""{% trans ""Select property name"" %}"" onkeyup=""trySetValue(this)"" onchange=""trySetValue(this)"" onclick=""trySetValue(this)"" value=""$name"" /> </td> <td class=""col-sm-5 small-padding""> <input type=""text"" class=""form-control"" field=""value"" onkeyup=""set_props()"" onchange=""set_props()"" onclick=""set_props()"" value=""$value"" /> </td> <td class=""col-sm-2 small-padding""> <input type=""button"" class=""btn btn-danger"" onclick=""delete_prop(this)"" value=""{% trans ""Remove"" %}"" /> </td> <tr id_attr=""$id"" class=""row data-row""> <td class=""col-sm-5 small-padding""> <input type=""text"" class=""form-control"" field=""key"" onkeyup=""set_props()"" onchange=""set_props()"" onclick=""set_props()"" value=""$name"" /> </td> <td class=""col-sm-5 small-padding""> <input type=""text"" class=""form-control"" field=""value"" onkeyup=""set_props()"" onchange=""set_props()"" onclick=""set_props()"" value=""$value"" /> </td> <td class=""col-sm-2 small-padding""> <input type=""button"" class=""btn btn-danger"" onclick=""delete_prop(this)"" value=""{% trans ""Remove"" %}"" /> </td> <tr id_attr=""$id"" class=""row data-row""> <td class=""col-sm-10 small-padding""> <input type=""text"" class=""form-control"" field=""value"" onkeyup=""set_props()"" onchange=""set_props()"" onclick=""set_props()"" value=""$value"" /> </td> <td class=""col-sm-2 small-padding""> <input type=""button"" class=""btn btn-danger"" onclick=""delete_prop(this)"" value=""{% trans ""Remove"" %}"" /> </td> <tr class=""row""> <td class=""col-sm-5 small-padding""><label>{% trans ""Name"" %}</label></td> <td class=""col-sm-5 small-padding""><label>{% trans ""Value"" %}</label></td> <a class=""btn btn-default btn-inline"" onclick=""add_prop('$target');"">{% trans ""Add"" %}</a> <tr class=""row""> <th class=""col-sm-10 small-padding""><label>{% trans ""Value"" %}</label></th> <a class=""btn btn-default btn-inline"" onclick=""add_prop('$target');"">{% trans ""Add"" %}</a> $(""#"" + target + "" tr.data-row"").each(function () { $(""#"" + target + "" tr.data-row"").each(function (index) { $(""#"" + target + "" tr.data-row"").each(function () { $(""#"" + target + "" tr"").last().after(template); var tr = $(el).closest(""tr""); var target = $(tr).closest(""div"").attr(""id""); tr.remove(); var prop_id = $(el).parent().attr(""id_attr""); if (properties[$(el).val()] != ""undefined"") { $(""#configs"").find(""tr[id_attr='"" + prop_id + ""']"")"," <tr id_attr=""$id""> <div class=""input form-group""> <td><input style=""display: inline"" field=""key"" list=""properties"" placeholder=""Select property name"" onkeyup=""trySetValue(this)"" onchange=""trySetValue(this)"" onclick=""trySetValue(this)"" value=""$name""/></td> <td><input style=""display: inline; margin-left:10px; margin-right:10px"" field=""value"" class=""input-medium"" onkeyup=""set_props()"" onchange=""set_props()"" onclick=""set_props()"" value=""$value""/></td> <td><input style=""display: inline; margin-top:-10px"" type=""button"" class=""btn btn-danger"" onclick=""delete_prop(this)"" value=""Remove""/></td> </div> <tr id_attr=""$id""> <div class=""input form-group""> <td><input style=""display: inline"" field=""key"" onkeyup=""set_props()"" onchange=""set_props()"" onclick=""set_props()"" value=""$name""/></td> <td><input style=""display: inline; margin-left:10px; margin-right:10px"" field=""value"" class=""input-medium"" onkeyup=""set_props()"" onchange=""set_props()"" onclick=""set_props()"" value=""$value""/></td> <td><input style=""display: inline; margin-top:-10px"" type=""button"" class=""btn btn-danger"" onclick=""delete_prop(this)"" value=""Remove""/></td> </div> <tr id_attr=""$id""> <div class=""input form-group""> <td><input style=""display: inline; margin-left:10px; margin-right:10px"" field=""value"" class=""input-medium"" onkeyup=""set_props()"" onchange=""set_props()"" onclick=""set_props()"" value=""$value""/></td> <td><input style=""display: inline; margin-top:-10px"" type=""button"" class=""btn btn-danger"" onclick=""delete_prop(this)"" value=""Remove""/></td> </div> <thead> <tr> <td><label>Name</label></td> <td><label style=""margin-left: 10px"">Value</label></td> </thead> <tbody> </tbody> <a class=""btn btn-default btn-inline"" onclick=""add_prop('$target');"">Add</a> <thead> <tr> <td><label style=""margin-left: 10px"">Value</label></td> </thead> <tbody> </tbody> <a class=""btn btn-default btn-inline"" onclick=""add_prop('$target');"">Add</a> $(""#"" + target + "" tbody tr"").each(function () { $(""#"" + target + "" tbody tr"").each(function (index) { $(""#"" + target + "" tbody tr"").each(function () { $(""#"" + target + "" tbody"").append(template); var tr = $(el).parents(""tr"")[0]; var target = $(tr).parent().parent().parent().attr(""id""); tr.parentNode.removeChild(tr); var prop_id = $(el).parent().parent().attr(""id_attr""); if (properties[$(el).val()] != ""undefined"") { $(""#configs"").find(""tbody tr[id_attr='"" + prop_id + ""']"")",56,54
openstack%2Fhorizon~master~I4f9166f112487a500d23f9cc98b98660be2ebe55,openstack/horizon,master,I4f9166f112487a500d23f9cc98b98660be2ebe55,Fixes downloading a keypair,MERGED,2014-08-07 08:37:00.000000000,2014-09-29 09:46:14.000000000,2014-09-29 09:46:13.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 4264}, {'_account_id': 4978}, {'_account_id': 6637}, {'_account_id': 8090}, {'_account_id': 8648}, {'_account_id': 9647}, {'_account_id': 9981}, {'_account_id': 10295}, {'_account_id': 10369}, {'_account_id': 11599}, {'_account_id': 12231}]","[{'number': 1, 'created': '2014-08-07 08:37:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/4ae6916dd14476ce60369d0f11372b3262b7d057', 'message': ""Fixes downloading a keypair\n\nThis patch fixes the invalid error which user gets when downloading\na keypair that wasn't saved from the keypair download link from the\nkeypair download page.\n\nWhen user clicks the download link, i just sending that to a different\nurl and then deletling the flavor and then again creating it with\nsame name.\n\nChange-Id: I4f9166f112487a500d23f9cc98b98660be2ebe55\nCloses-bug: #1182797\n""}, {'number': 2, 'created': '2014-08-07 12:43:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/a87a49c6e1d9f687161c864e548196fa63bcf3e3', 'message': ""Fixes downloading a keypair\n\nThis patch fixes the invalid error which user gets when downloading\na keypair that wasn't saved from the keypair download link from the\nkeypair download page.\n\nWhen user clicks the download link, i just sending that to a different\nurl and then deleting the keypair and then again creating it with\nsame name.\n\nChange-Id: I4f9166f112487a500d23f9cc98b98660be2ebe55\nCloses-bug: #1182797\n""}, {'number': 3, 'created': '2014-08-19 06:20:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/f39fb7f8388226beb26b4ee5b7540fe8db976865', 'message': ""Fixes downloading a keypair\n\nThis patch fixes the invalid error which user gets when downloading\na keypair that wasn't saved from the keypair download link from the\nkeypair download page.\n\nWhen user clicks the download link, i just sending that to a different\nurl and then deleting the keypair and then again creating it with\nsame name.\n\nAdds test case for this change.\n\nChange-Id: I4f9166f112487a500d23f9cc98b98660be2ebe55\nCloses-bug: #1182797\n""}, {'number': 4, 'created': '2014-09-01 07:30:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/0c4975938f07ea122e458dabf5a57951a799db85', 'message': ""Fixes downloading a keypair\n\nThis patch fixes the invalid error which user gets when downloading\na keypair that wasn't saved from the keypair download link from the\nkeypair download page.\n\nWhen user clicks the download link, i just sending that to a different\nurl and then deleting the keypair and then again creating it with\nsame name.\n\nChange-Id: I4f9166f112487a500d23f9cc98b98660be2ebe55\nCloses-bug: #1182797\n""}, {'number': 5, 'created': '2014-09-19 07:23:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/56c35a1451556faffa49b29eccde9c29c7591289', 'message': ""Fixes downloading a keypair\n\nThis patch fixes the invalid error which user gets when downloading\na keypair that wasn't saved from the keypair download link from the\nkeypair download page.\n\nWhen user clicks the download link, i just sending that to a different\nurl and then deleting the keypair and then again creating it with\nsame name.\n\nChange-Id: I4f9166f112487a500d23f9cc98b98660be2ebe55\nCloses-bug: #1182797\n""}, {'number': 6, 'created': '2014-09-19 09:26:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/a189bcb9aba519645cc469b6260b34d2435d820f', 'message': ""Fixes downloading a keypair\n\nThis patch fixes the invalid error which user gets when downloading\na keypair that wasn't saved from the keypair download link from the\nkeypair download page.\n\nWhen user clicks the download link, i just sending that to a different\nurl and then deleting the keypair and then again creating it with\nsame name.\n\nChange-Id: I4f9166f112487a500d23f9cc98b98660be2ebe55\nCloses-bug: #1182797\n""}, {'number': 7, 'created': '2014-09-22 11:12:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/1ad2f621dc276741a75484e9a659b1627825549c', 'message': ""Fixes downloading a keypair\n\nThis patch fixes the invalid error which user gets when downloading\na keypair that wasn't saved from the keypair download link from the\nkeypair download page.\n\nWhen user clicks the download link, i just sending that to a different\nurl and then deleting the keypair and then again creating it with\nsame name.\n\nChange-Id: I4f9166f112487a500d23f9cc98b98660be2ebe55\nCloses-bug: #1182797\n""}, {'number': 8, 'created': '2014-09-23 07:16:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/5a3cc18cd7c752ec73d025a9b391976f2168cff2', 'message': ""Fixes downloading a keypair\n\nThis patch fixes the invalid error which user gets when downloading\na keypair that wasn't saved from the keypair download link from the\nkeypair download page.\n\nWhen user clicks the download link, i just sending that to a different\nurl and then deleting the keypair and then again creating it with\nsame name.\n\nChange-Id: I4f9166f112487a500d23f9cc98b98660be2ebe55\nCloses-bug: #1182797\n""}, {'number': 9, 'created': '2014-09-23 09:21:21.000000000', 'files': ['openstack_dashboard/dashboards/project/access_and_security/templates/access_and_security/keypairs/download.html', 'openstack_dashboard/dashboards/project/access_and_security/keypairs/tests.py', 'openstack_dashboard/dashboards/project/access_and_security/keypairs/urls.py', 'openstack_dashboard/dashboards/project/access_and_security/keypairs/views.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/5b9ea17e515ec8a20d23f13787563d3e3a9f4d7c', 'message': ""Fixes downloading a keypair\n\nThis patch fixes the invalid error which user gets when downloading\na keypair that wasn't saved from the keypair download link from the\nkeypair download page.\n\nWhen user clicks the download link, i just sending that to a different\nurl and then deleting the keypair and then again creating it with\nsame name.\n\nChange-Id: I4f9166f112487a500d23f9cc98b98660be2ebe55\nCloses-bug: #1182797\n""}]",6,112513,5b9ea17e515ec8a20d23f13787563d3e3a9f4d7c,55,13,9,11599,,,0,"Fixes downloading a keypair

This patch fixes the invalid error which user gets when downloading
a keypair that wasn't saved from the keypair download link from the
keypair download page.

When user clicks the download link, i just sending that to a different
url and then deleting the keypair and then again creating it with
same name.

Change-Id: I4f9166f112487a500d23f9cc98b98660be2ebe55
Closes-bug: #1182797
",git fetch https://review.opendev.org/openstack/horizon refs/changes/13/112513/2 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/access_and_security/templates/access_and_security/keypairs/download.html', 'openstack_dashboard/dashboards/project/access_and_security/keypairs/urls.py', 'openstack_dashboard/dashboards/project/access_and_security/keypairs/views.py']",3,4ae6916dd14476ce60369d0f11372b3262b7d057,bug/#1182797," class OnClickGenerateView(View): def get(self, request, keypair_name=None): try: api.nova.keypair_delete(request, keypair_name) keypair = api.nova.keypair_create(request, keypair_name) except Exception: redirect = reverse('horizon:project:access_and_security:index') exceptions.handle(self.request, _('Unable to create key pair: %(exc)s'), redirect=redirect) response = http.HttpResponse(content_type='application/binary') response['Content-Disposition'] = \ 'attachment; filename=%s.pem' % slugify(keypair.name) response.write(keypair.private_key) response['Content-Length'] = str(len(response.content)) return response",,22,1
openstack%2Fceilometer~master~I77c4d02b0821b559d7de7329acf11c6d490f51b1,openstack/ceilometer,master,I77c4d02b0821b559d7de7329acf11c6d490f51b1,Fix bug with wrong bool opt value interpolation,MERGED,2014-09-24 11:41:02.000000000,2014-09-29 09:38:17.000000000,2014-09-25 04:02:36.000000000,"[{'_account_id': 3}, {'_account_id': 2284}, {'_account_id': 3012}, {'_account_id': 6537}, {'_account_id': 6676}, {'_account_id': 7126}, {'_account_id': 8052}, {'_account_id': 9562}, {'_account_id': 10987}, {'_account_id': 11564}]","[{'number': 1, 'created': '2014-09-24 11:41:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/8c12391aa24b66aecb48f888d64d1913baa5fcb0', 'message': ""Fix the default value of the pecan_debug option\n\nCurrently if we'll run tools/config/generate_sample.sh it will\nfail due to the fact that default pecan_debug option value is\nset currently to the '$debug' that's obviously not boolean.\n\nActually option type is checked on two layers of config\nsample generation:\n * in the oslo.config.cfg module while the option initialization\n   as there is passed oslo.cfg type there, that asserts the default\n   option value with something looking like bool. This assertion\n   might be tricked with new local SomeNewOpt and SomeNewType\n   classes defined in ceilometer.api.app module\n * in the openstack.common.config.generator module, were we expect\n   bool opt to have bool value, actually. This assertion has\n   no opportunity to be tricked using the '$debug' behaviour, as\n   it's working only while the console script is running from the\n   cmd, actually. Moreover, we can't use kind of 'eval' here for the\n   '$debug' to avoid security issues.\n\nThat's why default value of the pecan_debug option is simply set\nto the False.\n\nCloses-Bug: 1373360\nChange-Id: I77c4d02b0821b559d7de7329acf11c6d490f51b1\n""}, {'number': 2, 'created': '2014-09-24 11:43:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/06af9b4d987985eb6172e4b1770db95d25b0d932', 'message': ""Fix the default value of the pecan_debug option\n\nCurrently if we'll run tools/config/generate_sample.sh it will\nfail due to the fact that default pecan_debug option value is\nset currently to the '$debug' that's obviously not boolean.\n\nActually option type is checked on two layers of config\nsample generation:\n * in the oslo.config.cfg module while the option initialization\n   as there is passed oslo.cfg type there, that asserts the default\n   option value with something looking like bool. This assertion\n   might be tricked with new local SomeNewOpt and SomeNewType\n   classes defined in ceilometer.api.app module\n * in the openstack.common.config.generator module, were we expect\n   bool opt to have bool value, actually. This assertion has\n   no opportunity to be tricked using the '$debug' behaviour, as\n   it's working only while the console script is running from the\n   cmd, actually. Moreover, we can't use kind of 'eval' here for the\n   '$debug' to avoid security issues.\n\nThat's why default value of the pecan_debug option is simply set\nto the False.\n\nCloses-Bug: 1373360\nChange-Id: I77c4d02b0821b559d7de7329acf11c6d490f51b1\n""}, {'number': 3, 'created': '2014-09-24 11:54:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/c3f3852b2b4ed99c79a0882dccaf995dd1ef3582', 'message': ""Fix the default value of the pecan_debug option\n\nCurrently if we'll run tools/config/generate_sample.sh it will\nfail due to the fact that default pecan_debug option value is\nset currently to the '$debug' that's obviously not boolean.\n\nCloses-Bug: 1373360\nChange-Id: I77c4d02b0821b559d7de7329acf11c6d490f51b1\n""}, {'number': 4, 'created': '2014-09-24 12:39:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/870b06e915c4ad8679262b10e48ab715f95e60d2', 'message': ""Work around a bug with wrong opt value interpolation\n\nConfig generator cannot work with the option value\ninterpolation, that's why this config generation\nis failing for the pecan_debug option\n\nRelated-Bug: 1373360\nChange-Id: I77c4d02b0821b559d7de7329acf11c6d490f51b1\n""}, {'number': 5, 'created': '2014-09-24 12:55:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/a1c7852d1690413bd20de522a6e8cb4cbe611ebf', 'message': 'Fix bug with wrong bool opt value interpolation\n\nOther way of global debug opt value interpolation is needed\nfor the pecan_debug option, as bool opt interpolation\nseems to be not working in the oslo.config\n\nCloses-Bug: 1373360\nChange-Id: I77c4d02b0821b559d7de7329acf11c6d490f51b1\n'}, {'number': 6, 'created': '2014-09-24 13:17:04.000000000', 'files': ['ceilometer/api/app.py'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/f92ddd636d5d764949202d376597cb85c399d467', 'message': 'Fix bug with wrong bool opt value interpolation\n\nOther way of global debug opt value interpolation is needed\nfor the pecan_debug option, as bool opt interpolation\nseems to be not working in the oslo.config\n\nCloses-Bug: 1373360\nChange-Id: I77c4d02b0821b559d7de7329acf11c6d490f51b1\n'}]",4,123705,f92ddd636d5d764949202d376597cb85c399d467,28,10,6,3012,,,0,"Fix bug with wrong bool opt value interpolation

Other way of global debug opt value interpolation is needed
for the pecan_debug option, as bool opt interpolation
seems to be not working in the oslo.config

Closes-Bug: 1373360
Change-Id: I77c4d02b0821b559d7de7329acf11c6d490f51b1
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/05/123705/1 && git format-patch -1 --stdout FETCH_HEAD,['ceilometer/api/app.py'],1,8c12391aa24b66aecb48f888d64d1913baa5fcb0,bug/1357343," default=False,"," default='$debug',",1,1
openstack%2Ffuel-docs~master~If8711cd7d7e3aa477e4a15feb0787f53ab92a7c2,openstack/fuel-docs,master,If8711cd7d7e3aa477e4a15feb0787f53ab92a7c2,Describes Fuel buildsystem,ABANDONED,2014-09-29 09:12:06.000000000,2014-09-29 09:20:32.000000000,,"[{'_account_id': 3}, {'_account_id': 8971}]","[{'number': 1, 'created': '2014-09-29 09:12:06.000000000', 'files': ['contents/contents-buildsystem.rst'], 'web_link': 'https://opendev.org/openstack/fuel-docs/commit/c4af6017154a81dd8c04ca7f71c9133b00bd748b', 'message': 'Describes Fuel buildsystem\n\nChange-Id: If8711cd7d7e3aa477e4a15feb0787f53ab92a7c2\nCloses-Bug: 1365441\n'}]",0,124691,c4af6017154a81dd8c04ca7f71c9133b00bd748b,5,2,1,13082,,,0,"Describes Fuel buildsystem

Change-Id: If8711cd7d7e3aa477e4a15feb0787f53ab92a7c2
Closes-Bug: 1365441
",git fetch https://review.opendev.org/openstack/fuel-docs refs/changes/91/124691/1 && git format-patch -1 --stdout FETCH_HEAD,['contents/contents-buildsystem.rst'],1,c4af6017154a81dd8c04ca7f71c9133b00bd748b,bug/1365441,,,0,0
openstack%2Fos-cloud-config~master~I137f2f78fbb9790459a2f92aaab046649460e51d,openstack/os-cloud-config,master,I137f2f78fbb9790459a2f92aaab046649460e51d,Fix help for register-nodes,MERGED,2014-09-11 20:25:08.000000000,2014-09-29 08:06:06.000000000,2014-09-29 08:06:05.000000000,"[{'_account_id': 3}, {'_account_id': 215}, {'_account_id': 6348}, {'_account_id': 7144}, {'_account_id': 7585}, {'_account_id': 8449}, {'_account_id': 9369}, {'_account_id': 9453}]","[{'number': 1, 'created': '2014-09-11 20:25:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-cloud-config/commit/e65fa79629f8d60554eabf5fd8366fa806f25f1e', 'message': 'Fix help for register-nodes\n\nThe help for the --service-host arg for register-nodes seemed too\nspecific to Nova baremetal.\n\nChange-Id: I137f2f78fbb9790459a2f92aaab046649460e51d\n'}, {'number': 2, 'created': '2014-09-12 20:37:37.000000000', 'files': ['os_cloud_config/cmd/register_nodes.py'], 'web_link': 'https://opendev.org/openstack/os-cloud-config/commit/e33abbc147d6984a566bd390b733ff778428355d', 'message': 'Fix help for register-nodes\n\nThe help for the --service-host arg for register-nodes seemed too\nspecific to Nova baremetal.\n\nChange-Id: I137f2f78fbb9790459a2f92aaab046649460e51d\n'}]",4,120903,e33abbc147d6984a566bd390b733ff778428355d,17,8,2,7144,,,0,"Fix help for register-nodes

The help for the --service-host arg for register-nodes seemed too
specific to Nova baremetal.

Change-Id: I137f2f78fbb9790459a2f92aaab046649460e51d
",git fetch https://review.opendev.org/openstack/os-cloud-config refs/changes/03/120903/2 && git format-patch -1 --stdout FETCH_HEAD,['os_cloud_config/cmd/register_nodes.py'],1,e65fa79629f8d60554eabf5fd8366fa806f25f1e,help, help='Nova compute service host to register nodes with'), help='Nova-bm service host to register nodes with'),1,1
openstack%2Ftraining-guides~master~I23336bcb5227e884eaf6dd6e9eb5cb0fc24977cc,openstack/training-guides,master,I23336bcb5227e884eaf6dd6e9eb5cb0fc24977cc,labs: set errexit and no nounset for client scripts,MERGED,2014-09-29 06:17:23.000000000,2014-09-29 07:59:55.000000000,2014-09-29 07:59:55.000000000,"[{'_account_id': 3}, {'_account_id': 7007}]","[{'number': 1, 'created': '2014-09-29 06:17:23.000000000', 'files': ['labs/scripts/apt_install_mysql.sh', 'labs/scripts/setup_horizon.sh', 'labs/scripts/setup_cinder_volumes.sh', 'labs/scripts/setup_cinder_controller.sh', 'labs/scripts/yum_init.sh', 'labs/scripts/setup_neutron_controller.sh', 'labs/scripts/osbash/base_fixups.sh', 'labs/scripts/install_rabbitmq.sh', 'labs/scripts/config_external_network.sh', 'labs/scripts/setup_nova_controller.sh', 'labs/scripts/etc_hosts.sh', 'labs/scripts/setup_nova_compute.sh', 'labs/scripts/osbash/activate_autostart.sh', 'labs/scripts/osbash/init_xxx_node.sh', 'labs/scripts/apt_pre-download.sh', 'labs/lib/functions.guest', 'labs/scripts/config_tenant_network.sh', 'labs/scripts/setup_glance.sh', 'labs/scripts/setup_neutron_compute.sh', 'labs/scripts/setup_neutron_network.sh', 'labs/scripts/zero_empty.sh', 'labs/scripts/shutdown.sh', 'labs/scripts/yum_update.sh', 'labs/scripts/apt_init.sh', 'labs/scripts/setup_keystone.sh', 'labs/scripts/config_demo_user.sh', 'labs/scripts/osbash/wait_debug.sh', 'labs/scripts/osbash/enable_vagrant_ssh_keys.sh', 'labs/scripts/apt_upgrade.sh'], 'web_link': 'https://opendev.org/openstack/training-guides/commit/b1cdd3679a419484ea0b85adce3325d42e39bec3', 'message': 'labs: set errexit and no nounset for client scripts\n\nUse bash options to have client scripts exit with an error if any\ncommand returns an error (errexit) or if an unset variable is used\n(nounset).\n\nFix those scripts that would otherwise fail due to this change.\n\nChange-Id: I23336bcb5227e884eaf6dd6e9eb5cb0fc24977cc\n'}]",0,124668,b1cdd3679a419484ea0b85adce3325d42e39bec3,6,2,1,11109,,,0,"labs: set errexit and no nounset for client scripts

Use bash options to have client scripts exit with an error if any
command returns an error (errexit) or if an unset variable is used
(nounset).

Fix those scripts that would otherwise fail due to this change.

Change-Id: I23336bcb5227e884eaf6dd6e9eb5cb0fc24977cc
",git fetch https://review.opendev.org/openstack/training-guides refs/changes/68/124668/1 && git format-patch -1 --stdout FETCH_HEAD,"['labs/scripts/apt_install_mysql.sh', 'labs/scripts/setup_horizon.sh', 'labs/scripts/setup_cinder_volumes.sh', 'labs/scripts/setup_cinder_controller.sh', 'labs/scripts/yum_init.sh', 'labs/scripts/setup_neutron_controller.sh', 'labs/scripts/osbash/base_fixups.sh', 'labs/scripts/install_rabbitmq.sh', 'labs/scripts/config_external_network.sh', 'labs/scripts/setup_nova_controller.sh', 'labs/scripts/etc_hosts.sh', 'labs/scripts/setup_nova_compute.sh', 'labs/scripts/osbash/activate_autostart.sh', 'labs/scripts/osbash/init_xxx_node.sh', 'labs/scripts/apt_pre-download.sh', 'labs/lib/functions.guest', 'labs/scripts/config_tenant_network.sh', 'labs/scripts/setup_glance.sh', 'labs/scripts/setup_neutron_compute.sh', 'labs/scripts/setup_neutron_network.sh', 'labs/scripts/zero_empty.sh', 'labs/scripts/shutdown.sh', 'labs/scripts/yum_update.sh', 'labs/scripts/apt_init.sh', 'labs/scripts/setup_keystone.sh', 'labs/scripts/config_demo_user.sh', 'labs/scripts/osbash/wait_debug.sh', 'labs/scripts/osbash/enable_vagrant_ssh_keys.sh', 'labs/scripts/apt_upgrade.sh']",29,b1cdd3679a419484ea0b85adce3325d42e39bec3,errexit_nounset,set -o errexit -o nounset init_os_ident,,45,1
openstack%2Fzaqar~master~I9642ed3f10a3cee640592110690a743ecc77b434,openstack/zaqar,master,I9642ed3f10a3cee640592110690a743ecc77b434,Updated from global requirements,MERGED,2014-09-19 08:52:38.000000000,2014-09-29 07:48:06.000000000,2014-09-29 07:48:05.000000000,"[{'_account_id': 3}, {'_account_id': 6159}, {'_account_id': 6413}]","[{'number': 1, 'created': '2014-09-19 08:52:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zaqar/commit/d6f6cd71667c346f5f2d6a2dfe33542e0fd3f63e', 'message': 'Updated from global requirements\n\nChange-Id: I9642ed3f10a3cee640592110690a743ecc77b434\n'}, {'number': 2, 'created': '2014-09-23 02:01:42.000000000', 'files': ['requirements.txt', 'test-requirements.txt', 'requirements-py3.txt', 'test-requirements-py3.txt'], 'web_link': 'https://opendev.org/openstack/zaqar/commit/5d4c9352a1f53d5c2187cdb2559d359ec033bf9c', 'message': 'Updated from global requirements\n\nChange-Id: I9642ed3f10a3cee640592110690a743ecc77b434\n'}]",0,122676,5d4c9352a1f53d5c2187cdb2559d359ec033bf9c,12,3,2,11131,,,0,"Updated from global requirements

Change-Id: I9642ed3f10a3cee640592110690a743ecc77b434
",git fetch https://review.opendev.org/openstack/zaqar refs/changes/76/122676/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'test-requirements.txt', 'requirements-py3.txt', 'test-requirements-py3.txt']",4,d6f6cd71667c346f5f2d6a2dfe33542e0fd3f63e,openstack/requirements,oslosphinx>=2.2.0 # Apache-2.0,oslosphinx>=2.2.0.0a2,10,10
openstack%2Fzaqar~master~If10752c7c7dd106d655e032d2b2abea685768520,openstack/zaqar,master,If10752c7c7dd106d655e032d2b2abea685768520,WIP: basics of msgpack encoding test,ABANDONED,2014-07-09 18:18:15.000000000,2014-09-29 07:20:35.000000000,,"[{'_account_id': 3}, {'_account_id': 6159}, {'_account_id': 6427}, {'_account_id': 6484}, {'_account_id': 10181}, {'_account_id': 10634}]","[{'number': 1, 'created': '2014-07-09 18:18:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zaqar/commit/53ef4b9ed3524ffb6b8cef9172adc47f2e65d3fb', 'message': 'WIP: Sending this test in for review, please help clarify the scope of the blueprint.  There are many references to json, please help to isolate just those that need an update per this requirement.\n\nChange-Id: If10752c7c7dd106d655e032d2b2abea685768520\n'}, {'number': 2, 'created': '2014-07-14 23:16:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zaqar/commit/c11cf2984d97a966dff8048a49e9f94bfe4509bb', 'message': 'WIP: Sending this test in for review, please help clarify the scope of the blueprint.  There are many references to json, please help to isolate just those that need an update per this requirement.\n\nChange-Id: If10752c7c7dd106d655e032d2b2abea685768520\n'}, {'number': 3, 'created': '2014-07-14 23:52:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zaqar/commit/3a04e3062a2b1e6ef7aef34007f87e33c1e54520', 'message': ""WIP: basics of msgpack encoding test\n\nThere are still some issues around py2/3 string handling.  The organization is improved but I'm not sure what the intent is with the other test subclasses; specifically would we want to run a test run with TestMessagesMongoDB+JSON and TestMessagesMongoDB+Msgpack?  Or is one subclass like the way I have it now sufficient?\n\nChange-Id: If10752c7c7dd106d655e032d2b2abea685768520\n""}, {'number': 4, 'created': '2014-07-15 00:17:08.000000000', 'files': ['marconi/tests/functional/helpers.py', 'marconi/queues/transport/wsgi/v1_1/messages.py', 'marconi/tests/queues/transport/wsgi/v1_1/test_messages.py'], 'web_link': 'https://opendev.org/openstack/zaqar/commit/045839820f5e94fb870f5c52b79967924a6a0dcd', 'message': ""WIP: basics of msgpack encoding test\n\nThere are still some issues around py2/3 string handling.  The organization is improved but I'm not sure what the intent is with the other test subclasses; specifically would we want to run a test run with TestMessagesMongoDB+JSON and TestMessagesMongoDB+Msgpack?  Or is one subclass like the way I have it now sufficient?\n\nChange-Id: If10752c7c7dd106d655e032d2b2abea685768520\n""}]",13,105830,045839820f5e94fb870f5c52b79967924a6a0dcd,24,6,4,10181,,,0,"WIP: basics of msgpack encoding test

There are still some issues around py2/3 string handling.  The organization is improved but I'm not sure what the intent is with the other test subclasses; specifically would we want to run a test run with TestMessagesMongoDB+JSON and TestMessagesMongoDB+Msgpack?  Or is one subclass like the way I have it now sufficient?

Change-Id: If10752c7c7dd106d655e032d2b2abea685768520
",git fetch https://review.opendev.org/openstack/zaqar refs/changes/30/105830/1 && git format-patch -1 --stdout FETCH_HEAD,"['marconi/tests/functional/helpers.py', 'marconi/queues/transport/wsgi/v1_1/messages.py', 'marconi/tests/queues/transport/wsgi/v1_1/test_messages.py']",3,53ef4b9ed3524ffb6b8cef9172adc47f2e65d3fb,v1.1msgpack,"#import msgpack def _test_post(self, sample_messages, use_msgpack=False): if use_msgpack: sample_doc = msgpack.Packer().pack(sample_messages) self.headers['Content-Type'] = 'application/x-msgpack' else: sample_doc = jsonutils.dumps(sample_messages) if use_msgpack: unpacker = msgpack.Unpacker() unpacker.feed(result[0]) result_doc = unpacker.unpack() else: result_doc = jsonutils.loads(result[0]) # TODO msgpack this case if use_msgpack: unpacker = msgpack.Unpacker() unpacker.feed(result[0]) result_doc = unpacker.unpack() else: result_doc = jsonutils.loads(result[0]) def test_post_single_msgpack(self): sample_messages = [ {'body': {'key': 'value'}, 'ttl': 200}, ] self._test_post(sample_messages, use_msgpack=True) "," def _test_post(self, sample_messages): sample_doc = jsonutils.dumps(sample_messages) result_doc = jsonutils.loads(result[0]) result_doc = jsonutils.loads(result[0])",30,4
openstack%2Fhorizon~master~I0898b446abd79672b43b55be62dba6250a4fdd58,openstack/horizon,master,I0898b446abd79672b43b55be62dba6250a4fdd58,Fix concatenation in Network and Router actions,MERGED,2014-09-25 18:11:04.000000000,2014-09-29 07:10:19.000000000,2014-09-29 07:10:18.000000000,"[{'_account_id': 3}, {'_account_id': 2455}, {'_account_id': 6914}, {'_account_id': 9576}, {'_account_id': 9622}, {'_account_id': 11592}]","[{'number': 1, 'created': '2014-09-25 18:11:04.000000000', 'files': ['openstack_dashboard/dashboards/project/routers/tables.py', 'openstack_dashboard/dashboards/project/networks/subnets/tables.py', 'openstack_dashboard/dashboards/project/routers/extensions/routerrules/tables.py', 'openstack_dashboard/dashboards/project/networks/tables.py', 'openstack_dashboard/dashboards/router/nexus1000v/tables.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/827e5a36fabee65bba739da66dacb7388d604f5d', 'message': 'Fix concatenation in Network and Router actions\n\nRemove concatenation and pluralization issues from Delete\nNetwork, Delete Subnet, Delete Router Rule, Clear Gateway,\nand Delete Network Profile\n\nChange-Id: I0898b446abd79672b43b55be62dba6250a4fdd58\npartial-bug: 1307476\n'}]",0,124124,827e5a36fabee65bba739da66dacb7388d604f5d,10,6,1,9981,,,0,"Fix concatenation in Network and Router actions

Remove concatenation and pluralization issues from Delete
Network, Delete Subnet, Delete Router Rule, Clear Gateway,
and Delete Network Profile

Change-Id: I0898b446abd79672b43b55be62dba6250a4fdd58
partial-bug: 1307476
",git fetch https://review.opendev.org/openstack/horizon refs/changes/24/124124/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/routers/tables.py', 'openstack_dashboard/dashboards/project/networks/subnets/tables.py', 'openstack_dashboard/dashboards/project/routers/extensions/routerrules/tables.py', 'openstack_dashboard/dashboards/project/networks/tables.py', 'openstack_dashboard/dashboards/router/nexus1000v/tables.py']",5,827e5a36fabee65bba739da66dacb7388d604f5d,bug/1307476,"from django.utils.translation import ungettext_lazy @staticmethod def action_present(count): return ungettext_lazy( u""Delete Network Profile"", u""Delete Network Profiles"", count ) @staticmethod def action_past(count): return ungettext_lazy( u""Deleted Network Profile"", u""Deleted Network Profiles"", count )"," data_type_singular = _(""Network Profile"") data_type_plural = _(""Network Profiles"")",83,12
openstack%2Fhorizon~master~Ia6eb39cb2d736345afbdb3bfce8cb61bb2586871,openstack/horizon,master,Ia6eb39cb2d736345afbdb3bfce8cb61bb2586871,Fix concatenation in more misc actions,MERGED,2014-09-25 18:46:35.000000000,2014-09-29 07:09:00.000000000,2014-09-29 07:08:59.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 2455}, {'_account_id': 6914}, {'_account_id': 9981}, {'_account_id': 10295}, {'_account_id': 11592}]","[{'number': 1, 'created': '2014-09-25 18:46:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/148b4bf9fb88b62063465fce675ac99d9c771406', 'message': 'Fix concatenation in more misc actions\n\nRemove concatenation and pluralization issues from Evacuate\nHost, Sell Puppies (renamed from Delete Puppies.  I cannot\ncondone the deleting of virtual puppies.), Delete Continer,\nDelete Object, Delete Stack\n\nChange-Id: Ia6eb39cb2d736345afbdb3bfce8cb61bb2586871\npartial-bug: 1307476\n'}, {'number': 2, 'created': '2014-09-25 19:04:21.000000000', 'files': ['horizon/test/test_dashboards/dogs/puppies/tables.py', 'openstack_dashboard/dashboards/project/containers/tables.py', 'openstack_dashboard/dashboards/project/stacks/tables.py', 'openstack_dashboard/dashboards/admin/hypervisors/compute/tables.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/41d16ce4caba47b6436847bc3578855924d6ed41', 'message': 'Fix concatenation in more misc actions\n\nRemove concatenation and pluralization issues from Evacuate\nHost, Sell Puppies (renamed from Delete Puppies.  I cannot\ncondone the deleting of virtual puppies.), Delete Continer,\nDelete Object, Delete Stack\n\nChange-Id: Ia6eb39cb2d736345afbdb3bfce8cb61bb2586871\npartial-bug: 1307476\n'}]",2,124129,41d16ce4caba47b6436847bc3578855924d6ed41,12,7,2,9981,,,0,"Fix concatenation in more misc actions

Remove concatenation and pluralization issues from Evacuate
Host, Sell Puppies (renamed from Delete Puppies.  I cannot
condone the deleting of virtual puppies.), Delete Continer,
Delete Object, Delete Stack

Change-Id: Ia6eb39cb2d736345afbdb3bfce8cb61bb2586871
partial-bug: 1307476
",git fetch https://review.opendev.org/openstack/horizon refs/changes/29/124129/1 && git format-patch -1 --stdout FETCH_HEAD,"['horizon/test/test_dashboards/dogs/puppies/tables.py', 'openstack_dashboard/dashboards/project/containers/tables.py', 'openstack_dashboard/dashboards/project/stacks/tables.py', 'openstack_dashboard/dashboards/admin/hypervisors/compute/tables.py']",4,148b4bf9fb88b62063465fce675ac99d9c771406,bug/1307476,"from django.utils.translation import ungettext_lazy @staticmethod def action_present(count): return ungettext_lazy( u""Evacuate Host"", u""Evacuate Hosts"", count ) @staticmethod def action_past(count): return ungettext_lazy( u""Evacuated Host"", u""Evacuated Hosts"", count ) "," data_type_singular = _(""Host"") data_type_plural = _(""Hosts"") self.action_present = kwargs.get('action_present', _(""Evacuate"")) self.action_past = kwargs.get('action_past', _(""Evacuated""))",86,12
openstack%2Fdevstack~master~If363dbf439dbea9aafd265c2c665ff4c2b733738,openstack/devstack,master,If363dbf439dbea9aafd265c2c665ff4c2b733738,Install haproxy on SUSE distros,MERGED,2014-09-24 12:53:59.000000000,2014-09-29 07:05:29.000000000,2014-09-29 07:05:29.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 1653}, {'_account_id': 6593}, {'_account_id': 7007}, {'_account_id': 7102}, {'_account_id': 7369}, {'_account_id': 9009}, {'_account_id': 11236}]","[{'number': 1, 'created': '2014-09-24 12:53:59.000000000', 'files': ['lib/neutron_plugins/services/loadbalancer'], 'web_link': 'https://opendev.org/openstack/devstack/commit/cbd892b4363716be3d900ba753c81c06639fa97d', 'message': 'Install haproxy on SUSE distros\n\nhaproxy package is available for SUSE distros, so install it.\n\nChange-Id: If363dbf439dbea9aafd265c2c665ff4c2b733738\n'}]",0,123719,cbd892b4363716be3d900ba753c81c06639fa97d,14,9,1,7102,,,0,"Install haproxy on SUSE distros

haproxy package is available for SUSE distros, so install it.

Change-Id: If363dbf439dbea9aafd265c2c665ff4c2b733738
",git fetch https://review.opendev.org/openstack/devstack refs/changes/19/123719/1 && git format-patch -1 --stdout FETCH_HEAD,['lib/neutron_plugins/services/loadbalancer'],1,cbd892b4363716be3d900ba753c81c06639fa97d,haproxy-on-suse, if is_ubuntu || is_fedora || is_suse; then," if is_ubuntu || is_fedora; then elif is_suse; then ### FIXME: Find out if package can be pushed to Factory echo ""HAProxy packages can be installed from server:http project in OBS""",1,4
openstack%2Ftricircle~master~I4fb5acc1e184f7c4085b42ad873355492dffca8d,openstack/tricircle,master,I4fb5acc1e184f7c4085b42ad873355492dffca8d,Add code lines developed description,MERGED,2014-09-29 06:40:41.000000000,2014-09-29 06:58:25.000000000,2014-09-29 06:58:25.000000000,"[{'_account_id': 3}, {'_account_id': 9684}]","[{'number': 1, 'created': '2014-09-29 06:40:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tricircle/commit/87ca49511ecf4a177aabe958b13db89b647ad198', 'message': 'Add code lines developed description\n\nAdd how much code lines developed for OpenStack cascading,\nincluding cascading for Nova,Cinder,Neutron and Glance Sync Manager.\n\nChange-Id: I4fb5acc1e184f7c4085b42ad873355492dffca8d\n'}, {'number': 2, 'created': '2014-09-29 06:44:47.000000000', 'files': ['README.md'], 'web_link': 'https://opendev.org/openstack/tricircle/commit/06182d11c014248da57dfd46dae5ca3e08590aff', 'message': 'Add code lines developed description\n\nAdd how much code lines developed for OpenStack cascading,\nincluding cascading for Nova,Cinder,Neutron and Glance Sync Manager.\n\nChange-Id: I4fb5acc1e184f7c4085b42ad873355492dffca8d\n'}]",0,124670,06182d11c014248da57dfd46dae5ca3e08590aff,8,2,2,11819,,,0,"Add code lines developed description

Add how much code lines developed for OpenStack cascading,
including cascading for Nova,Cinder,Neutron and Glance Sync Manager.

Change-Id: I4fb5acc1e184f7c4085b42ad873355492dffca8d
",git fetch https://review.opendev.org/openstack/tricircle refs/changes/70/124670/1 && git format-patch -1 --stdout FETCH_HEAD,['README.md'],1,87ca49511ecf4a177aabe958b13db89b647ad198,modify_readme,"* Only about 15k code lines developed for OpenStack cascading. * The initial source code is for PoC only. Refactory will be done constantly to reach OpenStack acceptance standard. * DVR-Patch for IceHouse: the PoC source code is based on IceHouse version, while Neutron is a master branch snapshot on July 1, 2014 which include DVR feature, not IceHouse version. The Neutron code is download from github when it was still in the developement and review status. The source code of DVR part is not stable, and not all DVR features are included, for example, N-S functions not ready. * DVR-Patch is the majority source code in the repository, about 180k. The patch will be remove if OpenStack cascading is developed base on Juno. ","* the initial source code is for PoC only. Refactory will be done constantly to reach OpenStack acceptance standard. * the PoC source code is based on IceHouse version, while Neutron is a master branch snapshot on July 1, 2014 which include DVR feature, not IceHouse version. The Neutron code is download from github when it was still in the developement and review status. The source code of DVR part is not stable, and not all DVR features are included, for example, N-S functions not ready.",4,2
openstack%2Fproject-config~master~I0111bedcd3bb4affde8b9bd7cd2a21e6849c1513,openstack/project-config,master,I0111bedcd3bb4affde8b9bd7cd2a21e6849c1513,Fix wrong check for empty files,MERGED,2014-09-26 08:46:23.000000000,2014-09-29 06:55:14.000000000,2014-09-29 06:55:13.000000000,"[{'_account_id': 3}, {'_account_id': 5263}, {'_account_id': 6316}, {'_account_id': 6609}, {'_account_id': 7069}]","[{'number': 1, 'created': '2014-09-26 08:46:23.000000000', 'files': ['jenkins/scripts/common_translation_update.sh'], 'web_link': 'https://opendev.org/openstack/project-config/commit/103970ddf9ec69aeba81b860ee9627b639405f74', 'message': 'Fix wrong check for empty files\n\nThe condition needs to be negated, we want to remove files\nwhere the grep for non-empty lines does not succeed.\n\nChange-Id: I0111bedcd3bb4affde8b9bd7cd2a21e6849c1513\n'}]",0,124329,103970ddf9ec69aeba81b860ee9627b639405f74,9,5,1,6547,,,0,"Fix wrong check for empty files

The condition needs to be negated, we want to remove files
where the grep for non-empty lines does not succeed.

Change-Id: I0111bedcd3bb4affde8b9bd7cd2a21e6849c1513
",git fetch https://review.opendev.org/openstack/project-config refs/changes/29/124329/1 && git format-patch -1 --stdout FETCH_HEAD,['jenkins/scripts/common_translation_update.sh'],1,103970ddf9ec69aeba81b860ee9627b639405f74,translations," if ! grep -q 'msgid ""[^""]' ""$f"" ; then"," if grep -q 'msgid ""[^""]' ""$f"" ; then",1,1
openstack%2Fcinder~master~Ifac2432776de1322a6bee0d8d7de97acb7fa3b63,openstack/cinder,master,Ifac2432776de1322a6bee0d8d7de97acb7fa3b63,Remove unused variable LOG,ABANDONED,2014-08-20 14:43:00.000000000,2014-09-29 06:46:18.000000000,,"[{'_account_id': 3}, {'_account_id': 1207}, {'_account_id': 9751}, {'_account_id': 11811}]","[{'number': 1, 'created': '2014-08-20 14:43:00.000000000', 'files': ['cinder/tests/backup/fake_service.py', 'cinder/tests/backup/fake_service_with_verify.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/9c4d3b36cea41dbea7b5294c6ca42ead90db8845', 'message': ""Remove unused variable LOG\n\nThe fake backup services won't log anything since there are only fake\nclasses without any programming logic.\n\nChange-Id: Ifac2432776de1322a6bee0d8d7de97acb7fa3b63\n""}]",0,115666,9c4d3b36cea41dbea7b5294c6ca42ead90db8845,6,4,1,7872,,,0,"Remove unused variable LOG

The fake backup services won't log anything since there are only fake
classes without any programming logic.

Change-Id: Ifac2432776de1322a6bee0d8d7de97acb7fa3b63
",git fetch https://review.opendev.org/openstack/cinder refs/changes/66/115666/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/backup/fake_service.py', 'cinder/tests/backup/fake_service_with_verify.py']",2,9c4d3b36cea41dbea7b5294c6ca42ead90db8845,unused_log,,from cinder.openstack.common import log as loggingLOG = logging.getLogger(__name__) ,0,6
openstack%2Fopenstack-manuals~master~I11bd68bcb1ccc2dd51c15a02b6e5ad17ab4ebab4,openstack/openstack-manuals,master,I11bd68bcb1ccc2dd51c15a02b6e5ad17ab4ebab4,Imported Translations from Transifex,MERGED,2014-09-29 06:09:40.000000000,2014-09-29 06:35:16.000000000,2014-09-29 06:35:15.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-09-29 06:09:40.000000000', 'files': ['doc/user-guide-admin/locale/ja.po'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/38e8bd8654c0e1c729cfc55a2d9f89b313e8efca', 'message': 'Imported Translations from Transifex\n\nChange-Id: I11bd68bcb1ccc2dd51c15a02b6e5ad17ab4ebab4\n'}]",0,124666,38e8bd8654c0e1c729cfc55a2d9f89b313e8efca,7,3,1,11131,,,0,"Imported Translations from Transifex

Change-Id: I11bd68bcb1ccc2dd51c15a02b6e5ad17ab4ebab4
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/66/124666/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/user-guide-admin/locale/ja.po'],1,38e8bd8654c0e1c729cfc55a2d9f89b313e8efca,transifex/translations,"""POT-Creation-Date: 2014-09-29 04:19+0000\n"" ""PO-Revision-Date: 2014-09-28 09:50+0000\n"" ""Last-Translator: Tomoyuki KATO <tomo@dream.daynight.jp>\n""msgstr """"","""POT-Creation-Date: 2014-09-27 10:51+0000\n"" ""PO-Revision-Date: 2014-09-27 10:50+0000\n"" ""Last-Translator: openstackjenkins <jenkins@openstack.org>\n""msgstr """"",4,4
openstack%2Fcinder~master~Ia0ab72d930c1b9099dd2796032b7b2b0b857bad5,openstack/cinder,master,Ia0ab72d930c1b9099dd2796032b7b2b0b857bad5,Add required spaces in log messages,MERGED,2014-09-23 07:37:02.000000000,2014-09-29 06:26:29.000000000,2014-09-29 06:26:28.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 5538}, {'_account_id': 7198}, {'_account_id': 9008}, {'_account_id': 9236}, {'_account_id': 9303}, {'_account_id': 9366}, {'_account_id': 10503}, {'_account_id': 11047}, {'_account_id': 11903}, {'_account_id': 12017}, {'_account_id': 12369}]","[{'number': 1, 'created': '2014-09-23 07:37:02.000000000', 'files': ['cinder/transfer/api.py', 'cinder/brick/initiator/linuxscsi.py', 'cinder/volume/drivers/huawei/huawei_t.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/44c635574b752dd89871fa1b671c2ecd12912899', 'message': 'Add required spaces in log messages\n\nAdded required spaces in between two words for log messages.\n\ncinder/volume/drivers/huawei/huawei_t.py validate_connector method.\ncinder/transfer/api.py accept method.\ncinder/brick/initiator/linuxscsi.py flush_device_io method.\n\nCloses-Bug: #1372788\nChange-Id: Ia0ab72d930c1b9099dd2796032b7b2b0b857bad5\n'}]",0,123379,44c635574b752dd89871fa1b671c2ecd12912899,33,13,1,9303,,,0,"Add required spaces in log messages

Added required spaces in between two words for log messages.

cinder/volume/drivers/huawei/huawei_t.py validate_connector method.
cinder/transfer/api.py accept method.
cinder/brick/initiator/linuxscsi.py flush_device_io method.

Closes-Bug: #1372788
Change-Id: Ia0ab72d930c1b9099dd2796032b7b2b0b857bad5
",git fetch https://review.opendev.org/openstack/cinder refs/changes/79/123379/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/transfer/api.py', 'cinder/brick/initiator/linuxscsi.py', 'cinder/volume/drivers/huawei/huawei_t.py']",3,44c635574b752dd89871fa1b671c2ecd12912899,bug/1372788, ' wwpns in the connector.')), 'wwpns in the connector.')),3,3
openstack%2Ftraining-guides~master~I18087777f8a3254eb204f6591ae1892a757abe10,openstack/training-guides,master,I18087777f8a3254eb204f6591ae1892a757abe10,Updated from openstack-manuals,MERGED,2014-09-26 14:31:10.000000000,2014-09-29 06:04:17.000000000,2014-09-29 06:04:16.000000000,"[{'_account_id': 3}, {'_account_id': 6547}, {'_account_id': 6923}, {'_account_id': 11109}, {'_account_id': 11889}]","[{'number': 1, 'created': '2014-09-26 14:31:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/training-guides/commit/fefd7aa09fac57720cb7da5246a2fd9f75d9a423', 'message': 'Updated from openstack-manuals\n\nChange-Id: I18087777f8a3254eb204f6591ae1892a757abe10\n'}, {'number': 2, 'created': '2014-09-26 16:33:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/training-guides/commit/1b81b0203b9547c72bbb091a8c0c7e7e8341f8ff', 'message': 'Updated from openstack-manuals\n\nChange-Id: I18087777f8a3254eb204f6591ae1892a757abe10\n'}, {'number': 3, 'created': '2014-09-26 19:13:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/training-guides/commit/ecdd21aae25f32cdf575732f6c91ee4f6cf27216', 'message': 'Updated from openstack-manuals\n\nChange-Id: I18087777f8a3254eb204f6591ae1892a757abe10\n'}, {'number': 4, 'created': '2014-09-28 16:07:32.000000000', 'files': ['doc/training-guides/openstack.ent'], 'web_link': 'https://opendev.org/openstack/training-guides/commit/c91af6a6ef9f6937aba912da3d02d2b6b88c8a92', 'message': 'Updated from openstack-manuals\n\nChange-Id: I18087777f8a3254eb204f6591ae1892a757abe10\n'}]",0,124416,c91af6a6ef9f6937aba912da3d02d2b6b88c8a92,18,5,4,11131,,,0,"Updated from openstack-manuals

Change-Id: I18087777f8a3254eb204f6591ae1892a757abe10
",git fetch https://review.opendev.org/openstack/training-guides refs/changes/16/124416/3 && git format-patch -1 --stdout FETCH_HEAD,['doc/training-guides/openstack.ent'],1,fefd7aa09fac57720cb7da5246a2fd9f75d9a423,openstack/openstack-manuals, Any changes to the master file will override changes in other, Any changes to the master file will override changs in other,1,1
openstack%2Fpuppet-glance~master~I08336b91b0cd294a86326605fb6c28653e8eedb8,openstack/puppet-glance,master,I08336b91b0cd294a86326605fb6c28653e8eedb8,New parameter for swift backend,MERGED,2014-09-26 00:31:42.000000000,2014-09-29 05:13:40.000000000,2014-09-29 05:13:38.000000000,"[{'_account_id': 3}, {'_account_id': 3153}, {'_account_id': 6926}, {'_account_id': 6994}, {'_account_id': 11090}]","[{'number': 1, 'created': '2014-09-26 00:31:42.000000000', 'files': ['manifests/backend/swift.pp', 'spec/classes/glance_backend_swift_spec.rb'], 'web_link': 'https://opendev.org/openstack/puppet-glance/commit/baa47b4bbad03ef2316c1b80a2f512749a6f022a', 'message': ""New parameter for swift backend\n\nNew parameter is 'swift_store_large_object_size'.\n\nSometimes default value (5G) doesn't work when user attempts to\nmake a snapshots with size > 300 Mb. So it may be usefull to have\na possibity to adjust this parameter to your needs.\n\nChange-Id: I08336b91b0cd294a86326605fb6c28653e8eedb8\n""}]",0,124234,baa47b4bbad03ef2316c1b80a2f512749a6f022a,7,5,1,9387,,,0,"New parameter for swift backend

New parameter is 'swift_store_large_object_size'.

Sometimes default value (5G) doesn't work when user attempts to
make a snapshots with size > 300 Mb. So it may be usefull to have
a possibity to adjust this parameter to your needs.

Change-Id: I08336b91b0cd294a86326605fb6c28653e8eedb8
",git fetch https://review.opendev.org/openstack/puppet-glance refs/changes/34/124234/1 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/backend/swift.pp', 'spec/classes/glance_backend_swift_spec.rb']",2,baa47b4bbad03ef2316c1b80a2f512749a6f022a,add_new_param_for_swift_backend," should contain_glance_api_config('DEFAULT/swift_store_large_object_size').with_value('5120') should contain_glance_cache_config('DEFAULT/swift_store_large_object_size').with_value('5120') :swift_store_large_object_size => '100', should contain_glance_api_config('DEFAULT/swift_store_large_object_size').with_value('100') should contain_glance_cache_config('DEFAULT/swift_store_large_object_size').with_value('100')",,12,0
openstack%2Fglance_store~master~Iacc7c36ff83ee9698ee669a5ecc56dd452cc7575,openstack/glance_store,master,Iacc7c36ff83ee9698ee669a5ecc56dd452cc7575,Remove network_utils,MERGED,2014-09-26 11:04:43.000000000,2014-09-29 04:51:53.000000000,2014-09-29 04:51:53.000000000,"[{'_account_id': 3}, {'_account_id': 2537}, {'_account_id': 6159}, {'_account_id': 6549}]","[{'number': 1, 'created': '2014-09-26 11:04:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/02f1e70f4d7fd62f5b6eb95651f4236a7400c011', 'message': 'Remove network_utils\n\nIt is not good to add dependency on oslo-incubator if we can\navoid it. Moreover, there is no `network_utils.py` in\noslo-incubator. Use oslo.utils instead.\n\nChange-Id: Iacc7c36ff83ee9698ee669a5ecc56dd452cc7575\n'}, {'number': 2, 'created': '2014-09-28 04:11:05.000000000', 'files': ['requirements.txt', 'glance_store/_drivers/s3.py', 'glance_store/openstack/common/network_utils.py'], 'web_link': 'https://opendev.org/openstack/glance_store/commit/f181b61e3633bd75048649ec38f49566093d7bd1', 'message': 'Remove network_utils\n\nIt is not good to add dependency on oslo-incubator if we can\navoid it. Moreover, there is no `network_utils.py` in\noslo-incubator. Use oslo.utils instead.\n\nChange-Id: Iacc7c36ff83ee9698ee669a5ecc56dd452cc7575\n'}]",0,124361,f181b61e3633bd75048649ec38f49566093d7bd1,10,4,2,12363,,,0,"Remove network_utils

It is not good to add dependency on oslo-incubator if we can
avoid it. Moreover, there is no `network_utils.py` in
oslo-incubator. Use oslo.utils instead.

Change-Id: Iacc7c36ff83ee9698ee669a5ecc56dd452cc7575
",git fetch https://review.opendev.org/openstack/glance_store refs/changes/61/124361/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'glance_store/_drivers/s3.py', 'glance_store/openstack/common/network_utils.py']",3,02f1e70f4d7fd62f5b6eb95651f4236a7400c011,remove_network_utils,,"# Copyright 2012 OpenStack Foundation. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """""" Network-related utilities and helper functions. """""" import logging import socket from six.moves.urllib import parse from glance_store.openstack.common.gettextutils import _LW LOG = logging.getLogger(__name__) def parse_host_port(address, default_port=None): """"""Interpret a string as a host:port pair. An IPv6 address MUST be escaped if accompanied by a port, because otherwise ambiguity ensues: 2001:db8:85a3::8a2e:370:7334 means both [2001:db8:85a3::8a2e:370:7334] and [2001:db8:85a3::8a2e:370]:7334. >>> parse_host_port('server01:80') ('server01', 80) >>> parse_host_port('server01') ('server01', None) >>> parse_host_port('server01', default_port=1234) ('server01', 1234) >>> parse_host_port('[::1]:80') ('::1', 80) >>> parse_host_port('[::1]') ('::1', None) >>> parse_host_port('[::1]', default_port=1234) ('::1', 1234) >>> parse_host_port('2001:db8:85a3::8a2e:370:7334', default_port=1234) ('2001:db8:85a3::8a2e:370:7334', 1234) >>> parse_host_port(None) (None, None) """""" if not address: return (None, None) if address[0] == '[': # Escaped ipv6 _host, _port = address[1:].split(']') host = _host if ':' in _port: port = _port.split(':')[1] else: port = default_port else: if address.count(':') == 1: host, port = address.split(':') else: # 0 means ipv4, >1 means ipv6. # We prohibit unescaped ipv6 addresses with port. host = address port = default_port return (host, None if port is None else int(port)) class ModifiedSplitResult(parse.SplitResult): """"""Split results class for urlsplit."""""" # NOTE(dims): The functions below are needed for Python 2.6.x. # We can remove these when we drop support for 2.6.x. @property def hostname(self): netloc = self.netloc.split('@', 1)[-1] host, port = parse_host_port(netloc) return host @property def port(self): netloc = self.netloc.split('@', 1)[-1] host, port = parse_host_port(netloc) return port def urlsplit(url, scheme='', allow_fragments=True): """"""Parse a URL using urlparse.urlsplit(), splitting query and fragments. This function papers over Python issue9374 when needed. The parameters are the same as urlparse.urlsplit. """""" scheme, netloc, path, query, fragment = parse.urlsplit( url, scheme, allow_fragments) if allow_fragments and '#' in path: path, fragment = path.split('#', 1) if '?' in path: path, query = path.split('?', 1) return ModifiedSplitResult(scheme, netloc, path, query, fragment) def set_tcp_keepalive(sock, tcp_keepalive=True, tcp_keepidle=None, tcp_keepalive_interval=None, tcp_keepalive_count=None): """"""Set values for tcp keepalive parameters This function configures tcp keepalive parameters if users wish to do so. :param tcp_keepalive: Boolean, turn on or off tcp_keepalive. If users are not sure, this should be True, and default values will be used. :param tcp_keepidle: time to wait before starting to send keepalive probes :param tcp_keepalive_interval: time between successive probes, once the initial wait time is over :param tcp_keepalive_count: number of probes to send before the connection is killed """""" # NOTE(praneshp): Despite keepalive being a tcp concept, the level is # still SOL_SOCKET. This is a quirk. if isinstance(tcp_keepalive, bool): sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, tcp_keepalive) else: raise TypeError(""tcp_keepalive must be a boolean"") if not tcp_keepalive: return # These options aren't available in the OS X version of eventlet, # Idle + Count * Interval effectively gives you the total timeout. if tcp_keepidle is not None: if hasattr(socket, 'TCP_KEEPIDLE'): sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPIDLE, tcp_keepidle) else: LOG.warning(_LW('tcp_keepidle not available on your system')) if tcp_keepalive_interval is not None: if hasattr(socket, 'TCP_KEEPINTVL'): sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPINTVL, tcp_keepalive_interval) else: LOG.warning(_LW('tcp_keepintvl not available on your system')) if tcp_keepalive_count is not None: if hasattr(socket, 'TCP_KEEPCNT'): sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPCNT, tcp_keepalive_count) else: LOG.warning(_LW('tcp_keepknt not available on your system')) ",5,167
openstack%2Fpuppet-keystone~master~I8f6d6f3903b9140bf22c676b3661c2dda5766db6,openstack/puppet-keystone,master,I8f6d6f3903b9140bf22c676b3661c2dda5766db6,Make user creation optional when creating service.,MERGED,2014-09-10 18:43:46.000000000,2014-09-29 04:30:53.000000000,2014-09-29 04:30:53.000000000,"[{'_account_id': 3}, {'_account_id': 3153}, {'_account_id': 7155}, {'_account_id': 8482}, {'_account_id': 9060}]","[{'number': 1, 'created': '2014-09-10 18:43:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-keystone/commit/1e6afe491e3c10746aad0e695fc099f5c690d8cf', 'message': 'Make user creation optional when creating service.\n\nIn some cases it is useful to be able to just configure\nthe service in Keystone and not the service user. This\nis the case when e.g. a read only LDAP backend is used.\nAdded a parameter configure_user (defaults to true).\nCloses-Bug: 1360232\n\nChange-Id: I8f6d6f3903b9140bf22c676b3661c2dda5766db6\n'}, {'number': 2, 'created': '2014-09-10 20:42:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-keystone/commit/7d56aa11a6fe098813a8053a016c93592c364a46', 'message': 'Make user creation optional when creating service.\n\nIn some cases it is useful to be able to just configure\nthe service in Keystone and not the service user. This\nis the case when e.g. a read only LDAP backend is used.\nAdded a parameter configure_user (defaults to true).\nCloses-Bug: 1360232\n\nChange-Id: I8f6d6f3903b9140bf22c676b3661c2dda5766db6\n'}, {'number': 3, 'created': '2014-09-11 22:39:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-keystone/commit/dac50c45b04c83c2bc496ff10d3e73b75f0809b2', 'message': 'Make user creation optional when creating service.\n\nIn some cases it is useful to be able to just configure\nthe service in Keystone and not the service user. This\nis the case when e.g. a read only LDAP backend is used.\nAdded a parameter configure_user (defaults to true).\nCloses-Bug: 1360232\n\nChange-Id: I8f6d6f3903b9140bf22c676b3661c2dda5766db6\n'}, {'number': 4, 'created': '2014-09-11 22:51:39.000000000', 'files': ['spec/classes/keystone_roles_admin_spec.rb', 'manifests/roles/admin.pp'], 'web_link': 'https://opendev.org/openstack/puppet-keystone/commit/55c122caa2b57b20575f68bfbad95bd8ae1035ce', 'message': 'Make user creation optional when creating service.\n\nIn some cases it is useful to be able to just configure\nthe service in Keystone and not the service user. This\nis the case when e.g. a read only LDAP backend is used.\nAdded a parameter configure_user (defaults to true).\nCloses-Bug: 1360232\n\nChange-Id: I8f6d6f3903b9140bf22c676b3661c2dda5766db6\n'}]",0,120525,55c122caa2b57b20575f68bfbad95bd8ae1035ce,14,5,4,9060,,,0,"Make user creation optional when creating service.

In some cases it is useful to be able to just configure
the service in Keystone and not the service user. This
is the case when e.g. a read only LDAP backend is used.
Added a parameter configure_user (defaults to true).
Closes-Bug: 1360232

Change-Id: I8f6d6f3903b9140bf22c676b3661c2dda5766db6
",git fetch https://review.opendev.org/openstack/puppet-keystone refs/changes/25/120525/1 && git format-patch -1 --stdout FETCH_HEAD,"['spec/classes/keystone_roles_admin_spec.rb', 'manifests/roles/admin.pp']",2,1e6afe491e3c10746aad0e695fc099f5c690d8cf,bug/1360232,"# [configure_user] Optional. Should the admin user be created? Defaults to 'true'. $configure_user = 'true', if $configure_user { keystone_user { $admin: ensure => present, enabled => true, tenant => $admin_tenant, email => $email, password => $password, ignore_default_tenant => $ignore_default_tenant, } keystone_user_role { ""${admin}@${admin_tenant}"": ensure => present, roles => 'admin', }"," keystone_user { $admin: ensure => present, enabled => true, tenant => $admin_tenant, email => $email, password => $password, ignore_default_tenant => $ignore_default_tenant, } keystone_user_role { ""${admin}@${admin_tenant}"": ensure => present, roles => 'admin',",29,11
openstack%2Fpuppet-heat~stable%2Ficehouse~Ib06a0f967dd5d5f8cc1c4dc7257c0e196786e8ae,openstack/puppet-heat,stable/icehouse,Ib06a0f967dd5d5f8cc1c4dc7257c0e196786e8ae,Hide secrets from puppet logs,MERGED,2014-09-25 22:59:38.000000000,2014-09-29 04:28:58.000000000,2014-09-29 04:28:58.000000000,"[{'_account_id': 3}, {'_account_id': 3153}, {'_account_id': 7155}]","[{'number': 1, 'created': '2014-09-25 22:59:38.000000000', 'files': ['manifests/init.pp', 'spec/classes/heat_init_spec.rb', 'lib/puppet/type/heat_config.rb'], 'web_link': 'https://opendev.org/openstack/puppet-heat/commit/02e0cdaabbcd86d009f35968f5bbcb908873b96e', 'message': 'Hide secrets from puppet logs\n\nCurrently secrets like rabbit_password or admin_password are laked\n\npuppet logs when changed. This commit changes heat_*_config and\nheat_*_ini types adding a new parameter that triggers obfuscation\nthe values in puppet logs.\n\nChange-Id: Ib06a0f967dd5d5f8cc1c4dc7257c0e196786e8ae\nCloses-Bug: #1328448\n(cherry picked from commit 6a89a44f9d2c6114b3bb1a4a74eff62ec13f545e)\n'}]",0,124218,02e0cdaabbcd86d009f35968f5bbcb908873b96e,6,3,1,10540,,,0,"Hide secrets from puppet logs

Currently secrets like rabbit_password or admin_password are laked

puppet logs when changed. This commit changes heat_*_config and
heat_*_ini types adding a new parameter that triggers obfuscation
the values in puppet logs.

Change-Id: Ib06a0f967dd5d5f8cc1c4dc7257c0e196786e8ae
Closes-Bug: #1328448
(cherry picked from commit 6a89a44f9d2c6114b3bb1a4a74eff62ec13f545e)
",git fetch https://review.opendev.org/openstack/puppet-heat refs/changes/18/124218/1 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/init.pp', 'spec/classes/heat_init_spec.rb', 'lib/puppet/type/heat_config.rb']",3,02e0cdaabbcd86d009f35968f5bbcb908873b96e,," def is_to_s( currentvalue ) if resource.secret? return '[old secret redacted]' else return currentvalue end end def should_to_s( newvalue ) if resource.secret? return '[new secret redacted]' else return newvalue end end end newparam(:secret, :boolean => true) do desc 'Whether to hide the value from Puppet logs. Defaults to `false`.' newvalues(:true, :false) defaultto false",,31,3
openstack%2Fpuppet-glance~master~I222b1a3318f5163f6ad1e39cbb8be10c440ab99f,openstack/puppet-glance,master,I222b1a3318f5163f6ad1e39cbb8be10c440ab99f,Add manage_service feature,MERGED,2014-08-25 13:16:43.000000000,2014-09-29 04:25:10.000000000,2014-09-29 04:25:09.000000000,"[{'_account_id': 3}, {'_account_id': 1607}, {'_account_id': 3153}, {'_account_id': 5241}, {'_account_id': 6994}, {'_account_id': 7155}, {'_account_id': 7888}, {'_account_id': 8482}]","[{'number': 1, 'created': '2014-08-25 13:16:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-glance/commit/b67098a0704ec5bc5a91412ab975a87db81b8b61', 'message': 'Add manage_service feature\n\npuppet-glance lacks of disabling service managing. This patch adds\n$manage_service parameter to relevant classes.\n\nChange-Id: I222b1a3318f5163f6ad1e39cbb8be10c440ab99f\nCloses-bug: #1359823\n'}, {'number': 2, 'created': '2014-09-17 11:58:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-glance/commit/146e6a181948bdd2f5d36cb42b2d7eef2420d4cf', 'message': 'Add manage_service feature\n\npuppet-glance lacks of disabling service managing. This patch adds\n$manage_service parameter to relevant classes.\n\nChange-Id: I222b1a3318f5163f6ad1e39cbb8be10c440ab99f\nCloses-bug: #1359823\n'}, {'number': 3, 'created': '2014-09-17 12:32:54.000000000', 'files': ['manifests/registry.pp', 'manifests/api.pp', 'spec/classes/glance_registry_spec.rb', 'spec/classes/glance_api_spec.rb'], 'web_link': 'https://opendev.org/openstack/puppet-glance/commit/da217a13f5f3b1123a16566d7a3c2e69360b432b', 'message': 'Add manage_service feature\n\npuppet-glance lacks of disabling service managing. This patch adds\n$manage_service parameter to relevant classes.\n\nChange-Id: I222b1a3318f5163f6ad1e39cbb8be10c440ab99f\nCloses-bug: #1359823\n'}]",0,116610,da217a13f5f3b1123a16566d7a3c2e69360b432b,28,8,3,5241,,,0,"Add manage_service feature

puppet-glance lacks of disabling service managing. This patch adds
$manage_service parameter to relevant classes.

Change-Id: I222b1a3318f5163f6ad1e39cbb8be10c440ab99f
Closes-bug: #1359823
",git fetch https://review.opendev.org/openstack/puppet-glance refs/changes/10/116610/2 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/registry.pp', 'manifests/api.pp']",2,b67098a0704ec5bc5a91412ab975a87db81b8b61,manage_service,"# [*manage_service*] # (optional) If Puppet should manage service startup / shutdown. # Defaults to true. # $manage_service = true, if $manage_service { if $enabled { $service_ensure = 'running' } else { $service_ensure = 'stopped' }", if $enabled { $service_ensure = 'running' } else { $service_ensure = 'stopped',32,17
openstack%2Fpuppet-glance~master~I12a76e742db7279eafa3f3ec3c92947ff551b782,openstack/puppet-glance,master,I12a76e742db7279eafa3f3ec3c92947ff551b782,add command_options to glance::cache::[cleaner|pruner],MERGED,2014-09-22 08:40:42.000000000,2014-09-29 04:19:07.000000000,2014-09-29 04:19:06.000000000,"[{'_account_id': 3}, {'_account_id': 3153}, {'_account_id': 7155}, {'_account_id': 7888}]","[{'number': 1, 'created': '2014-09-22 08:40:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-glance/commit/e1c109f40d2c3b5ab67558c864ee025d12a5b3c5', 'message': 'add command_options to glance::cache::[cleaner|pruner]\n\nThe parameter command_options allows to add options\n(like config file to use) to the cron job command.\nThis makes it also possible to redirect the output.\n\nChange-Id: I12a76e742db7279eafa3f3ec3c92947ff551b782\n'}, {'number': 2, 'created': '2014-09-22 13:17:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-glance/commit/3de028365f9d61f4653024df900cf9f52b95a786', 'message': 'add command_options to glance::cache::[cleaner|pruner]\n\nThe parameter command_options allows to add options\n(like config file to use) to the cron job command.\nThis makes it also possible to redirect the output.\n\nExtend tests, to support different operating systems\nand test parameters.\n\nChange-Id: I12a76e742db7279eafa3f3ec3c92947ff551b782\n'}, {'number': 3, 'created': '2014-09-23 06:17:48.000000000', 'files': ['spec/classes/glance_cache_cleaner_spec.rb', 'manifests/cache/cleaner.pp', 'manifests/cache/pruner.pp', 'spec/classes/glance_cache_pruner_spec.rb'], 'web_link': 'https://opendev.org/openstack/puppet-glance/commit/b8a59c770654ad30868239773725c803c61e3a7b', 'message': 'add command_options to glance::cache::[cleaner|pruner]\n\nThe parameter command_options allows to add options\n(like config file to use) to the cron job command.\nThis makes it also possible to redirect the output.\n\nExtend tests, to support different operating systems\nand test parameters.\n\nChange-Id: I12a76e742db7279eafa3f3ec3c92947ff551b782\n'}]",4,123076,b8a59c770654ad30868239773725c803c61e3a7b,13,4,3,7888,,,0,"add command_options to glance::cache::[cleaner|pruner]

The parameter command_options allows to add options
(like config file to use) to the cron job command.
This makes it also possible to redirect the output.

Extend tests, to support different operating systems
and test parameters.

Change-Id: I12a76e742db7279eafa3f3ec3c92947ff551b782
",git fetch https://review.opendev.org/openstack/puppet-glance refs/changes/76/123076/1 && git format-patch -1 --stdout FETCH_HEAD,"['spec/classes/glance_cache_cleaner_spec.rb', 'manifests/cache/cleaner.pp', 'manifests/cache/pruner.pp', 'spec/classes/glance_cache_pruner_spec.rb']",4,e1c109f40d2c3b5ab67558c864ee025d12a5b3c5,," :command => 'glance-cache-pruner ',"," :command => 'glance-cache-pruner',",26,14
openstack%2Fopenstack-manuals~master~Id987496a112e7f154750ae465a8d6b17410dd401,openstack/openstack-manuals,master,Id987496a112e7f154750ae465a8d6b17410dd401,Fix wrong openstack-config command,MERGED,2014-09-28 17:24:36.000000000,2014-09-29 04:18:08.000000000,2014-09-29 04:18:07.000000000,"[{'_account_id': 3}, {'_account_id': 3153}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-09-28 17:24:36.000000000', 'files': ['doc/common/section_keystone_config_ldap-hardening.xml'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/88264ed719c54b41e4fec7a5cc0c85b21d063496', 'message': 'Fix wrong openstack-config command\n\nChange-Id: Id987496a112e7f154750ae465a8d6b17410dd401\nCloses-Bug: #1374127\n'}]",0,124640,88264ed719c54b41e4fec7a5cc0c85b21d063496,7,3,1,7923,,,0,"Fix wrong openstack-config command

Change-Id: Id987496a112e7f154750ae465a8d6b17410dd401
Closes-Bug: #1374127
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/40/124640/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/common/section_keystone_config_ldap-hardening.xml'],1,88264ed719c54b41e4fec7a5cc0c85b21d063496,bug/1374127," <screen os=""rhel;centos;fedora;opensuse;sles""><prompt>#</prompt> <userinput>openstack-config --set /etc/keystone/keystone.conf \"," <screen os=""rhel;centos;fedora;opensuse;sles""><prompt>#</prompt> <userinput>openstack --config --set /etc/keystone/keystone.conf \",1,1
openstack%2Fpuppet-ceilometer~master~Iee545b46ef5d7cf92859356ae84b8103564bf047,openstack/puppet-ceilometer,master,Iee545b46ef5d7cf92859356ae84b8103564bf047,Add manage_service feature,MERGED,2014-08-20 13:59:54.000000000,2014-09-29 04:13:53.000000000,2014-09-29 04:13:53.000000000,"[{'_account_id': 3}, {'_account_id': 3153}, {'_account_id': 5241}, {'_account_id': 7155}, {'_account_id': 7156}]","[{'number': 1, 'created': '2014-08-20 13:59:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-ceilometer/commit/5f1cf6fd7133e9830cb77258efae65f23d20634a', 'message': 'Add manage_service feature\n\npuppet-ceilometer lacks of disabling service managing. This patch adds\n$manage_service parameter to all relevant classes.\n\nChange-Id: Iee545b46ef5d7cf92859356ae84b8103564bf047\n'}, {'number': 2, 'created': '2014-08-21 15:56:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-ceilometer/commit/646abeae781c4b3a3217512cc03716cadee40738', 'message': 'Add manage_service feature\n\npuppet-ceilometer lacks of disabling service managing. This patch adds\n$manage_service parameter to all relevant classes.\n\nCloses-bug: #1359823\nChange-Id: Iee545b46ef5d7cf92859356ae84b8103564bf047\n'}, {'number': 3, 'created': '2014-09-17 14:26:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-ceilometer/commit/7b65d1e32f53c07762d2d03e3ac83e5d588ae86a', 'message': 'Add manage_service feature\n\npuppet-ceilometer lacks of disabling service managing. This patch adds\n$manage_service parameter to all relevant classes.\n\nCloses-bug: #1359823\nChange-Id: Iee545b46ef5d7cf92859356ae84b8103564bf047\n'}, {'number': 4, 'created': '2014-09-19 10:30:06.000000000', 'files': ['spec/classes/ceilometer_agent_central_spec.rb', 'spec/classes/ceilometer_agent_compute_spec.rb', 'manifests/api.pp', 'spec/classes/ceilometer_api_spec.rb', 'manifests/agent/notification.pp', 'spec/classes/ceilometer_agent_notification_spec.rb', 'spec/classes/ceilometer_alarm_notifier_spec.rb', 'manifests/alarm/evaluator.pp', 'spec/classes/ceilometer_alarm_evaluator_spec.rb', 'manifests/agent/central.pp', 'manifests/agent/compute.pp', 'spec/classes/ceilometer_collector_spec.rb', 'manifests/collector.pp', 'manifests/alarm/notifier.pp'], 'web_link': 'https://opendev.org/openstack/puppet-ceilometer/commit/10d38bd8191fc82f4087731ea6932583dd66db8d', 'message': 'Add manage_service feature\n\npuppet-ceilometer lacks of disabling service managing. This patch adds\n$manage_service parameter to all relevant classes.\n\nCloses-bug: #1359823\nChange-Id: Iee545b46ef5d7cf92859356ae84b8103564bf047\n'}]",0,115652,10d38bd8191fc82f4087731ea6932583dd66db8d,20,5,4,5241,,,0,"Add manage_service feature

puppet-ceilometer lacks of disabling service managing. This patch adds
$manage_service parameter to all relevant classes.

Closes-bug: #1359823
Change-Id: Iee545b46ef5d7cf92859356ae84b8103564bf047
",git fetch https://review.opendev.org/openstack/puppet-ceilometer refs/changes/52/115652/4 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/agent/notification.pp', 'manifests/alarm/evaluator.pp', 'manifests/agent/central.pp', 'manifests/api.pp', 'manifests/agent/compute.pp', 'manifests/collector.pp', 'manifests/alarm/notifier.pp']",7,5f1cf6fd7133e9830cb77258efae65f23d20634a,manage_service," $manage_service = true, if $manage_service { if $enabled { $service_ensure = 'running' } else { $service_ensure = 'stopped' }", if $enabled { $service_ensure = 'running' } else { $service_ensure = 'stopped',55,34
openstack%2Fneutron~stable%2Ficehouse~I03ca04c83ac2ef9c879fbd87e74bae495daea16d,openstack/neutron,stable/icehouse,I03ca04c83ac2ef9c879fbd87e74bae495daea16d,Add delete operations for the ODL MechanismDriver,MERGED,2014-09-20 07:42:46.000000000,2014-09-29 04:02:29.000000000,2014-09-26 22:35:45.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 1420}, {'_account_id': 1955}, {'_account_id': 2888}, {'_account_id': 5170}, {'_account_id': 9656}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9695}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9846}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10192}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 11692}, {'_account_id': 12040}, {'_account_id': 13308}]","[{'number': 1, 'created': '2014-09-20 07:42:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/abc3f2392066f0714de48bcac41c49968f08de9b', 'message': 'Add delete operations for the ODL MechanismDriver\n\nThis commit adds delete operations (networks, subnets and ports) for the ODL MechanismDriver.\nIt also modifies sync_single_resource to reduce db operations.\n\nChange-Id: I03ca04c83ac2ef9c879fbd87e74bae495daea16d\nCloses-Bug: #1324450\nPartial-Bug: #1325184\n(cherry picked from commit c1ed203ccb816ac0a3a0e015d2790ed3aee04564)\n'}, {'number': 2, 'created': '2014-09-25 13:46:43.000000000', 'files': ['neutron/plugins/ml2/drivers/mechanism_odl.py', 'neutron/tests/unit/ml2/test_mechanism_odl.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/dc2c893e3f07e35278c3bc411612eb20297dddab', 'message': 'Add delete operations for the ODL MechanismDriver\n\nThis commit adds delete operations (networks, subnets and ports) for the ODL MechanismDriver.\nIt also modifies sync_single_resource to reduce db operations.\n\nChange-Id: I03ca04c83ac2ef9c879fbd87e74bae495daea16d\nCloses-Bug: #1324450\nPartial-Bug: #1325184\n(cherry picked from commit c1ed203ccb816ac0a3a0e015d2790ed3aee04564)\n'}]",0,122931,dc2c893e3f07e35278c3bc411612eb20297dddab,78,21,2,11692,,,0,"Add delete operations for the ODL MechanismDriver

This commit adds delete operations (networks, subnets and ports) for the ODL MechanismDriver.
It also modifies sync_single_resource to reduce db operations.

Change-Id: I03ca04c83ac2ef9c879fbd87e74bae495daea16d
Closes-Bug: #1324450
Partial-Bug: #1325184
(cherry picked from commit c1ed203ccb816ac0a3a0e015d2790ed3aee04564)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/31/122931/2 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ml2/drivers/mechanism_odl.py', 'neutron/tests/unit/ml2/test_mechanism_odl.py']",2,abc3f2392066f0714de48bcac41c49968f08de9b,I03ca04c83ac2ef9c879fbd87e74bae495daea16d,"import mock import requests class AuthMatcher(object): def __eq__(self, obj): return (obj.username == config.cfg.CONF.ml2_odl.username and obj.password == config.cfg.CONF.ml2_odl.password) class OpenDaylightMechanismDriverTestCase(base.BaseTestCase): def setUp(self): super(OpenDaylightMechanismDriverTestCase, self).setUp() config.cfg.CONF.set_override('mechanism_drivers', ['logger', 'opendaylight'], 'ml2') config.cfg.CONF.set_override('url', 'http://127.0.0.1:9999', 'ml2_odl') config.cfg.CONF.set_override('username', 'someuser', 'ml2_odl') config.cfg.CONF.set_override('password', 'somepass', 'ml2_odl') self.mech = mechanism_odl.OpenDaylightMechanismDriver() self.mech.initialize() @staticmethod def _get_mock_delete_resource_context(): current = {'id': '00000000-1111-2222-3333-444444444444'} context = mock.Mock(current=current) return context _status_code_msgs = { 204: '', 401: '401 Client Error: Unauthorized', 403: '403 Client Error: Forbidden', 404: '404 Client Error: Not Found', 409: '409 Client Error: Conflict', 501: '501 Server Error: Not Implemented' } @classmethod def _get_mock_request_response(cls, status_code): response = mock.Mock(status_code=status_code) response.raise_for_status = mock.Mock() if status_code < 400 else ( mock.Mock(side_effect=requests.exceptions.HTTPError( cls._status_code_msgs[status_code]))) return response def _test_delete_resource_postcommit(self, object_type, status_code, exc_class=None): self.mech.out_of_sync = False method = getattr(self.mech, 'delete_%s_postcommit' % object_type) context = self._get_mock_delete_resource_context() request_response = self._get_mock_request_response(status_code) with mock.patch('requests.request', return_value=request_response) as mock_method: if exc_class is not None: self.assertRaises(exc_class, method, context) else: method(context) url = '%s/%ss/%s' % (config.cfg.CONF.ml2_odl.url, object_type, context.current['id']) mock_method.assert_called_once_with( 'delete', url=url, headers={'Content-Type': 'application/json'}, data=None, auth=AuthMatcher(), timeout=config.cfg.CONF.ml2_odl.timeout) def test_delete_network_postcommit(self): self._test_delete_resource_postcommit('network', requests.codes.no_content) for status_code in (requests.codes.unauthorized, requests.codes.not_found, requests.codes.conflict): self._test_delete_resource_postcommit( 'network', status_code, requests.exceptions.HTTPError) def test_delete_subnet_postcommit(self): self._test_delete_resource_postcommit('subnet', requests.codes.no_content) for status_code in (requests.codes.unauthorized, requests.codes.not_found, requests.codes.conflict, requests.codes.not_implemented): self._test_delete_resource_postcommit( 'subnet', status_code, requests.exceptions.HTTPError) def test_delete_port_postcommit(self): self._test_delete_resource_postcommit('port', requests.codes.no_content) for status_code in (requests.codes.unauthorized, requests.codes.forbidden, requests.codes.not_found, requests.codes.not_implemented): self._test_delete_resource_postcommit( 'port', status_code, requests.exceptions.HTTPError)",,117,42
openstack%2Fneutron~master~I0e62027ae4d98944ef91a5d457d43d4224441b2f,openstack/neutron,master,I0e62027ae4d98944ef91a5d457d43d4224441b2f,Fix broken port query in Extraroute test case,MERGED,2014-09-20 07:22:24.000000000,2014-09-29 03:56:36.000000000,2014-09-29 03:56:34.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 5170}, {'_account_id': 5948}, {'_account_id': 6659}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-20 07:22:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c42ced6de90ad9a1939074f272c6ffcacbdeb80f', 'message': ""Fix broken port query in Extraroute test case\n\nOne of the queries in an extra route test case tries\nto filter based on the port owner, but the _list_ports\nmethod it calls doesn't take a device_owner parameter.\nThis can cause failures if a DHCP port is created on\nthe same subnet.\n\nChange-Id: I0e62027ae4d98944ef91a5d457d43d4224441b2f\n""}, {'number': 2, 'created': '2014-09-27 19:30:19.000000000', 'files': ['neutron/tests/unit/test_db_plugin.py', 'neutron/tests/unit/test_extension_extraroute.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/78a2ecb9923417f63c17c1e7055200ac97e6e947', 'message': ""Fix broken port query in Extraroute test case\n\nOne of the queries in an extra route test case tries\nto filter based on the port owner, but the _list_ports\nmethod it calls doesn't take a device_owner parameter.\nThis can cause failures if a DHCP port is created on\nthe same subnet.\n\nChange-Id: I0e62027ae4d98944ef91a5d457d43d4224441b2f\n""}]",1,122929,78a2ecb9923417f63c17c1e7055200ac97e6e947,45,21,2,7787,,,0,"Fix broken port query in Extraroute test case

One of the queries in an extra route test case tries
to filter based on the port owner, but the _list_ports
method it calls doesn't take a device_owner parameter.
This can cause failures if a DHCP port is created on
the same subnet.

Change-Id: I0e62027ae4d98944ef91a5d457d43d4224441b2f
",git fetch https://review.opendev.org/openstack/neutron refs/changes/29/122929/2 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/test_db_plugin.py', 'neutron/tests/unit/test_extension_extraroute.py']",2,c42ced6de90ad9a1939074f272c6ffcacbdeb80f,bug/1374836, device_owner=constants.DEVICE_OWNER_ROUTER_GW), device_own=constants.DEVICE_OWNER_ROUTER_GW),6,4
openstack%2Fpython-keystoneclient~master~Iff063149e1f12df69bbf9015222d09d798980872,openstack/python-keystoneclient,master,Iff063149e1f12df69bbf9015222d09d798980872,Change cms_sign_data to use sha256 message digest,MERGED,2014-08-27 23:07:10.000000000,2014-09-29 03:52:50.000000000,2014-09-29 03:52:49.000000000,"[{'_account_id': 3}, {'_account_id': 2903}, {'_account_id': 6486}, {'_account_id': 6802}, {'_account_id': 7191}, {'_account_id': 8978}]","[{'number': 1, 'created': '2014-08-27 23:07:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/495eec908e919e8809c8b3ea601c2708792ee7a5', 'message': 'Change cms_sign_data to use sha256 message digest\n\ncms_sign_data was not passing the md parameter to openssl, so it was\nusing the default digest of sha1. Some security standards require a\nSHA2 algorithm for the digest.\n\nThis if for security hardening.\n\nSecurityImpact\n\nChange-Id: Iff063149e1f12df69bbf9015222d09d798980872\nCloses-Bug: #1362343\n'}, {'number': 2, 'created': '2014-09-24 16:01:17.000000000', 'files': ['keystoneclient/common/cms.py'], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/84c9ccaed34d83b7e97a4890561b1b218d99b1ba', 'message': 'Change cms_sign_data to use sha256 message digest\n\ncms_sign_data was not passing the md parameter to openssl, so it was\nusing the default digest of sha1. Some security standards require a\nSHA2 algorithm for the digest.\n\nThis if for security hardening.\n\nSecurityImpact\n\nChange-Id: Iff063149e1f12df69bbf9015222d09d798980872\nCloses-Bug: #1362343\n'}]",0,117371,84c9ccaed34d83b7e97a4890561b1b218d99b1ba,16,6,2,6486,,,0,"Change cms_sign_data to use sha256 message digest

cms_sign_data was not passing the md parameter to openssl, so it was
using the default digest of sha1. Some security standards require a
SHA2 algorithm for the digest.

This if for security hardening.

SecurityImpact

Change-Id: Iff063149e1f12df69bbf9015222d09d798980872
Closes-Bug: #1362343
",git fetch https://review.opendev.org/openstack/python-keystoneclient refs/changes/71/117371/1 && git format-patch -1 --stdout FETCH_HEAD,['keystoneclient/common/cms.py'],1,495eec908e919e8809c8b3ea601c2708792ee7a5,bug/1362343," '-nocerts', '-noattr', '-md', 'sha256', ],"," '-nocerts', '-noattr'],",2,1
openstack%2Fkeystonemiddleware~master~I5bf5dae3143f15a840a580a2e01ba67c50911276,openstack/keystonemiddleware,master,I5bf5dae3143f15a840a580a2e01ba67c50911276,Refactor auth_token cache,MERGED,2014-07-08 00:56:06.000000000,2014-09-29 03:34:55.000000000,2014-09-29 03:34:55.000000000,"[{'_account_id': 3}, {'_account_id': 1916}, {'_account_id': 2218}, {'_account_id': 2903}, {'_account_id': 6486}, {'_account_id': 7191}, {'_account_id': 8871}, {'_account_id': 8978}]","[{'number': 1, 'created': '2014-07-08 00:56:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/d72ff32ccb38a763fb4bed29ed37278ad5b02f6e', 'message': ""Refactor auth_token cache\n\nThere are very different paths for when we are using encrypting tokens\nbefore saving them to memcache. Extract the encrypted strategy out to\nit's own class to see the similarities.\n\nChange-Id: I5bf5dae3143f15a840a580a2e01ba67c50911276\n""}, {'number': 2, 'created': '2014-07-08 01:02:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/407baff911cb3955de05dcfe584d3ba45b795527', 'message': ""Refactor auth_token cache\n\nThere are very different paths for when we are using encrypting tokens\nbefore saving them to memcache. Extract the encrypted strategy out to\nit's own class to see the similarities.\n\nChange-Id: I5bf5dae3143f15a840a580a2e01ba67c50911276\n""}, {'number': 3, 'created': '2014-07-08 01:28:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/f3b4ad3af9e05d1423ac74ba74d32ef5f43aafc0', 'message': ""Refactor auth_token cache\n\nThere are very different paths for when we are using encrypting tokens\nbefore saving them to memcache. Extract the encrypted strategy out to\nit's own class to see the similarities.\n\nChange-Id: I5bf5dae3143f15a840a580a2e01ba67c50911276\n""}, {'number': 4, 'created': '2014-07-08 21:56:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/336f0cd243aca223fe70c3622fa6fb1d75e3ca36', 'message': ""Refactor auth_token cache\n\nThere are very different paths for when we are using encrypting tokens\nbefore saving them to memcache. Extract the encrypted strategy out to\nit's own class to see the similarities.\n\nChange-Id: I5bf5dae3143f15a840a580a2e01ba67c50911276\n""}, {'number': 5, 'created': '2014-07-21 01:55:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/1f37a9fea8662df2c1565c5756785fdff7e5e3dc', 'message': ""Refactor auth_token cache\n\nThere are very different paths for when we are using encrypting tokens\nbefore saving them to memcache. Extract the encrypted strategy out to\nit's own class to see the similarities.\n\nChange-Id: I5bf5dae3143f15a840a580a2e01ba67c50911276\n""}, {'number': 6, 'created': '2014-07-21 01:57:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/d967ea7f524b66f2669d6178ed8ff89f6e36e103', 'message': 'Refactor auth_token cache\n\nThere are very different paths for when we are encrypting tokens before\nsaving them to memcache. Extract the encrypted strategy out to its own\nclass to see the similarities.\n\nChange-Id: I5bf5dae3143f15a840a580a2e01ba67c50911276\n'}, {'number': 7, 'created': '2014-08-13 01:52:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/bc9d8f809841bae2a33e2ceac0fc4796f06cbed6', 'message': 'Refactor auth_token cache\n\nThere are very different paths for when we are encrypting tokens before\nsaving them to memcache. Extract the encrypted strategy out to its own\nclass to see the similarities.\n\nChange-Id: I5bf5dae3143f15a840a580a2e01ba67c50911276\n'}, {'number': 8, 'created': '2014-08-15 05:25:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/e4e88d0cf573e427b6be2d400569814f63da0d8f', 'message': 'Refactor auth_token cache\n\nThere are very different paths for when we are encrypting tokens before\nsaving them to memcache. Extract the encrypted strategy out to its own\nclass to see the similarities.\n\nChange-Id: I5bf5dae3143f15a840a580a2e01ba67c50911276\n'}, {'number': 9, 'created': '2014-09-27 14:05:40.000000000', 'files': ['keystonemiddleware/auth_token.py'], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/b99291cbdca002b38d4acfc842278bfe35965364', 'message': 'Refactor auth_token cache\n\nThere are very different paths for when we are encrypting tokens before\nsaving them to memcache. Extract the encrypted strategy out to its own\nclass to see the similarities.\n\nChange-Id: I5bf5dae3143f15a840a580a2e01ba67c50911276\n'}]",22,105314,b99291cbdca002b38d4acfc842278bfe35965364,52,8,9,7191,,,0,"Refactor auth_token cache

There are very different paths for when we are encrypting tokens before
saving them to memcache. Extract the encrypted strategy out to its own
class to see the similarities.

Change-Id: I5bf5dae3143f15a840a580a2e01ba67c50911276
",git fetch https://review.opendev.org/openstack/keystonemiddleware refs/changes/14/105314/8 && git format-patch -1 --stdout FETCH_HEAD,['keystonemiddleware/auth_token.py'],1,d72ff32ccb38a763fb4bed29ed37278ad5b02f6e,secure-cache," security_strategy = self._conf_get('memcache_security_strategy') secret_key = self._conf_get('memcache_secret_key') cache_kwargs = dict( ) if security_strategy or secret_key: self._token_cache = _SecureTokenCache(self._LOG, security_strategy, secret_key, **cache_kwargs) else: self._token_cache = _TokenCache(self._LOG, **cache_kwargs) env_cache_name=None, memcached_servers=None): def _get_cache_key(self, token_id): return self._CACHE_KEY_TEMPLATE % token_id, None def _deserialize(self, data, context): return data def _serialize(self, data, context): return data key, context = self._get_cache_key(token_id) with self._cache_pool.reserve() as cache: data = cache.get(key) if data is None: data = self._deserialize(data, context) if not isinstance(data, six.string_types): data = data.decode('utf-8') cached = jsonutils.loads(data) data = jsonutils.dumps(data) if isinstance(data, six.text_type): data = data.encode('utf-8') key, context = self._get_cache_key(token_id) data_to_store = self._serialize(data, context) cache.set(key, data_to_store, time=self._cache_time) class _SecureTokenCache(_TokenCache): def __init__(self, log, security_strategy, secret_key, **kwargs): super(_SecureTokenCache, self).__init__(log, **kwargs) security_strategy = security_strategy.upper() if security_strategy not in ('MAC', 'ENCRYPT'): raise ConfigurationError('memcache_security_strategy must be ' 'ENCRYPT or MAC') if not secret_key: raise ConfigurationError('memcache_secret_key must be defined ' 'when a memcache_security_strategy ' 'is defined') if isinstance(security_strategy, six.string_types): security_strategy = security_strategy.encode('utf-8') if isinstance(secret_key, six.string_types): secret_key = secret_key.encode('utf-8') self._security_strategy = security_strategy self._secret_key = secret_key def _get_cache_key(self, token_id): context = memcache_crypt.derive_keys(token_id, self._secret_key, self._security_strategy) key = self._CACHE_KEY_TEMPLATE % memcache_crypt.get_cache_key(keys) return key, context def _deserialize(self, data, context): try: # unprotect_data will return None if raw_cached is None return memcache_crypt.unprotect_data(context, data) except Exception: msg = 'Failed to decrypt/verify cache data' self._LOG.exception(msg) # this should have the same effect as data not # found in cache return None def _serialize(self, data, context): return memcache_crypt.protect_data(context, data)"," memcache_security_strategy = ( self._conf_get('memcache_security_strategy')) self._token_cache = _TokenCache( self._LOG, memcache_security_strategy=memcache_security_strategy, memcache_secret_key=self._conf_get('memcache_secret_key')) env_cache_name=None, memcached_servers=None, memcache_security_strategy=None, memcache_secret_key=None): # memcache value treatment, ENCRYPT or MAC self._memcache_security_strategy = memcache_security_strategy if self._memcache_security_strategy is not None: self._memcache_security_strategy = ( self._memcache_security_strategy.upper()) self._memcache_secret_key = memcache_secret_key self._assert_valid_memcache_protection_config() def _assert_valid_memcache_protection_config(self): if self._memcache_security_strategy: if self._memcache_security_strategy not in ('MAC', 'ENCRYPT'): raise ConfigurationError('memcache_security_strategy must be ' 'ENCRYPT or MAC') if not self._memcache_secret_key: raise ConfigurationError('memcache_secret_key must be defined ' 'when a memcache_security_strategy ' 'is defined') if self._memcache_security_strategy is None: key = self._CACHE_KEY_TEMPLATE % token_id with self._cache_pool.reserve() as cache: serialized = cache.get(key) else: secret_key = self._memcache_secret_key if isinstance(secret_key, six.string_types): secret_key = secret_key.encode('utf-8') security_strategy = self._memcache_security_strategy if isinstance(security_strategy, six.string_types): security_strategy = security_strategy.encode('utf-8') keys = memcache_crypt.derive_keys( token_id, secret_key, security_strategy) cache_key = self._CACHE_KEY_TEMPLATE % ( memcache_crypt.get_cache_key(keys)) with self._cache_pool.reserve() as cache: raw_cached = cache.get(cache_key) try: # unprotect_data will return None if raw_cached is None serialized = memcache_crypt.unprotect_data(keys, raw_cached) except Exception: msg = 'Failed to decrypt/verify cache data' self._LOG.exception(msg) # this should have the same effect as data not # found in cache serialized = None if serialized is None: if not isinstance(serialized, six.string_types): serialized = serialized.decode('utf-8') cached = jsonutils.loads(serialized) serialized_data = jsonutils.dumps(data) if isinstance(serialized_data, six.text_type): serialized_data = serialized_data.encode('utf-8') if self._memcache_security_strategy is None: cache_key = self._CACHE_KEY_TEMPLATE % token_id data_to_store = serialized_data else: secret_key = self._memcache_secret_key if isinstance(secret_key, six.string_types): secret_key = secret_key.encode('utf-8') security_strategy = self._memcache_security_strategy if isinstance(security_strategy, six.string_types): security_strategy = security_strategy.encode('utf-8') keys = memcache_crypt.derive_keys( token_id, secret_key, security_strategy) cache_key = memcache_crypt.get_cache_key(keys) cache_key = self._CACHE_KEY_TEMPLATE % cache_key data_to_store = memcache_crypt.protect_data(keys, serialized_data) cache.set(cache_key, data_to_store, time=self._cache_time)",84,78
openstack%2Fkeystone~master~I96a231d136a829e049a4e026ed07ff3730eeb388,openstack/keystone,master,I96a231d136a829e049a4e026ed07ff3730eeb388,Remove trailing space from string,MERGED,2014-09-08 20:42:46.000000000,2014-09-29 03:34:48.000000000,2014-09-29 03:34:47.000000000,"[{'_account_id': 3}, {'_account_id': 2903}, {'_account_id': 7725}, {'_account_id': 8866}, {'_account_id': 8978}, {'_account_id': 9101}]","[{'number': 1, 'created': '2014-09-08 20:42:46.000000000', 'files': ['keystone/common/ldap/core.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/60b4b15e51078da5b1409f39d6e5a13f75fe63ec', 'message': 'Remove trailing space from string\n\nThere was an extra space at the end of the debug log string.\n\nChange-Id: I96a231d136a829e049a4e026ed07ff3730eeb388\n'}]",0,119905,60b4b15e51078da5b1409f39d6e5a13f75fe63ec,10,6,1,6486,,,0,"Remove trailing space from string

There was an extra space at the end of the debug log string.

Change-Id: I96a231d136a829e049a4e026ed07ff3730eeb388
",git fetch https://review.opendev.org/openstack/keystone refs/changes/05/119905/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/common/ldap/core.py'],1,60b4b15e51078da5b1409f39d6e5a13f75fe63ec,cleanup," LOG.debug('Unable to decode value for attribute %s', kind)"," LOG.debug('Unable to decode value for attribute %s ', kind)",1,1
openstack%2Fheat~master~I36ac2fc4a55f9d5c2d1f4392fce93468fef42453,openstack/heat,master,I36ac2fc4a55f9d5c2d1f4392fce93468fef42453,Fix usage of client in Barbican resources,MERGED,2014-09-26 11:44:17.000000000,2014-09-29 03:20:43.000000000,2014-09-29 03:20:42.000000000,"[{'_account_id': 3}, {'_account_id': 4715}, {'_account_id': 6577}]","[{'number': 1, 'created': '2014-09-26 11:44:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/1461b0901f4968c12eb29df2b383b3c7645ad4ed', 'message': 'Fix usage of client in Barbican resources\n\nRemove access to the Resource clients attribute and the unavailable\nclient import.\n\nChange-Id: I36ac2fc4a55f9d5c2d1f4392fce93468fef42453\nCloses-Bug: 1374101\nCloses-Bug: 1374125\n'}, {'number': 2, 'created': '2014-09-26 11:49:44.000000000', 'files': ['contrib/heat_barbican/heat_barbican/resources/secret.py', 'contrib/heat_barbican/heat_barbican/tests/test_secret.py', 'contrib/heat_barbican/heat_barbican/tests/test_order.py', 'contrib/heat_barbican/heat_barbican/client.py', 'contrib/heat_barbican/heat_barbican/resources/order.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/45f0308ed55303abd8ccc35a6cb345560a73197c', 'message': 'Fix usage of client in Barbican resources\n\nRemove access to the Resource clients attribute and the unavailable\nclient import.\n\nChange-Id: I36ac2fc4a55f9d5c2d1f4392fce93468fef42453\nCloses-Bug: 1374101\nCloses-Bug: 1374125\n'}]",0,124377,45f0308ed55303abd8ccc35a6cb345560a73197c,10,3,2,7385,,,0,"Fix usage of client in Barbican resources

Remove access to the Resource clients attribute and the unavailable
client import.

Change-Id: I36ac2fc4a55f9d5c2d1f4392fce93468fef42453
Closes-Bug: 1374101
Closes-Bug: 1374125
",git fetch https://review.opendev.org/openstack/heat refs/changes/77/124377/2 && git format-patch -1 --stdout FETCH_HEAD,"['contrib/heat_barbican/heat_barbican/resources/secret.py', 'contrib/heat_barbican/heat_barbican/client.py', 'contrib/heat_barbican/heat_barbican/resources/order.py']",3,1461b0901f4968c12eb29df2b383b3c7645ad4ed,bug/1374101, return self.client('barbican') client = self.barbican() try: client.orders.delete(self.resource_id),from .. import client # noqa return self.clients.client('barbican') try: self.barbican().orders.delete(self.resource_id),7,9
openstack%2Fneutron~master~I88679173117739f628ae289636b5cdea10417e6c,openstack/neutron,master,I88679173117739f628ae289636b5cdea10417e6c,Remove unnecessary _make_port function in BSN UTs,MERGED,2014-09-20 00:19:46.000000000,2014-09-29 03:03:04.000000000,2014-09-22 14:46:33.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 5170}, {'_account_id': 5948}, {'_account_id': 6854}, {'_account_id': 7787}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9695}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10386}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-20 00:19:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e38a7e98b0525672165bc46d1cee8d478e9f6386', 'message': ""Remove unnecessary _make_port function in BSN UTs\n\nThe Big Switch unit tests had an unnecessary copy of the\n_make_port function to allow the binding:host_id field to\nbe set. This was already possible with the existing _make_port\ncall through the use of kwargs so the extra function wasn't\nnecessary.\n\nChange-Id: I88679173117739f628ae289636b5cdea10417e6c\n""}, {'number': 2, 'created': '2014-09-20 07:39:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/2014371573f7270b200dce8009b51a6b980b7d8c', 'message': ""Remove unnecessary _make_port function in BSN UTs\n\nThe Big Switch unit tests had unnecessary copies of the\n_make_port function to allow the binding:host_id field to\nbe set. This was already possible with the existing _make_port\ncall through the use of kwargs so the extra function wasn't\nnecessary.\n\nChange-Id: I88679173117739f628ae289636b5cdea10417e6c\n""}, {'number': 3, 'created': '2014-09-22 03:06:03.000000000', 'files': ['neutron/tests/unit/ml2/drivers/test_bigswitch_mech.py', 'neutron/tests/unit/bigswitch/test_restproxy_plugin.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/8e98d8827587f1b58c3b22f25bc055d5b964422e', 'message': ""Remove unnecessary _make_port function in BSN UTs\n\nThe Big Switch unit tests had unnecessary copies of the\n_make_port function to allow the binding:host_id field to\nbe set. This was already possible with the existing _make_port\ncall through the use of kwargs so the extra function wasn't\nnecessary.\n\nChange-Id: I88679173117739f628ae289636b5cdea10417e6c\n""}]",0,122899,8e98d8827587f1b58c3b22f25bc055d5b964422e,77,24,3,7787,,,0,"Remove unnecessary _make_port function in BSN UTs

The Big Switch unit tests had unnecessary copies of the
_make_port function to allow the binding:host_id field to
be set. This was already possible with the existing _make_port
call through the use of kwargs so the extra function wasn't
necessary.

Change-Id: I88679173117739f628ae289636b5cdea10417e6c
",git fetch https://review.opendev.org/openstack/neutron refs/changes/99/122899/2 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/unit/bigswitch/test_restproxy_plugin.py'],1,e38a7e98b0525672165bc46d1cee8d478e9f6386,," 'device_id': 'override_dev', 'arg_list': ('binding:host_id',)}"," 'device_id': 'override_dev'} def _make_port(self, fmt, net_id, expected_res_status=None, arg_list=None, **kwargs): arg_list = arg_list or () arg_list += ('binding:host_id', ) res = self._create_port(fmt, net_id, expected_res_status, arg_list, **kwargs) # Things can go wrong - raise HTTP exc with res code only # so it can be caught by unit tests if res.status_int >= 400: raise webob.exc.HTTPClientError(code=res.status_int) return self.deserialize(fmt, res) ",2,13
openstack%2Fnova~master~I90ff4e9ca0eeb149aa0db7be2d171561586724e9,openstack/nova,master,I90ff4e9ca0eeb149aa0db7be2d171561586724e9,Fix test_create_instance_invalid_key_name,MERGED,2014-09-12 08:10:11.000000000,2014-09-29 02:24:00.000000000,2014-09-29 02:23:57.000000000,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 2271}, {'_account_id': 5170}, {'_account_id': 5754}, {'_account_id': 6167}, {'_account_id': 6773}, {'_account_id': 9008}, {'_account_id': 9545}, {'_account_id': 9578}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-12 08:10:11.000000000', 'files': ['nova/tests/api/openstack/compute/test_servers.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/e1627f8cc4fa68ad14393ac5ab3a807e074101d8', 'message': 'Fix test_create_instance_invalid_key_name\n\nIn test_create_instance_invalid_key_name, there were the duplicated\nnegative factors. One is nonexistent image-id, the other one is non-\nexistent keypair.\nThis patch removes nonexistent image-id for the test purpose.\n\nChange-Id: I90ff4e9ca0eeb149aa0db7be2d171561586724e9\n'}]",0,121021,e1627f8cc4fa68ad14393ac5ab3a807e074101d8,19,11,1,6167,,,0,"Fix test_create_instance_invalid_key_name

In test_create_instance_invalid_key_name, there were the duplicated
negative factors. One is nonexistent image-id, the other one is non-
existent keypair.
This patch removes nonexistent image-id for the test purpose.

Change-Id: I90ff4e9ca0eeb149aa0db7be2d171561586724e9
",git fetch https://review.opendev.org/openstack/nova refs/changes/21/121021/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/api/openstack/compute/test_servers.py'],1,e1627f8cc4fa68ad14393ac5ab3a807e074101d8,fix_unittest," @mock.patch('nova.compute.api.API.create', side_effect=exception.KeypairNotFound(name='nonexistentkey', user_id=1)) def test_create_instance_invalid_key_name(self, mock_create):", def test_create_instance_invalid_key_name(self): image_href = 'http://localhost/v2/images/2' self.body['server']['imageRef'] = image_href,4,3
openstack%2Fnova~master~I7075a130c01be22f3ee5315d4a65b8e524b5a1ad,openstack/nova,master,I7075a130c01be22f3ee5315d4a65b8e524b5a1ad,Add plan for kilo blueprints: project priorities,MERGED,2014-08-07 23:59:56.000000000,2014-09-29 02:23:42.000000000,2014-09-29 02:23:40.000000000,"[{'_account_id': 3}, {'_account_id': 67}, {'_account_id': 287}, {'_account_id': 308}, {'_account_id': 360}, {'_account_id': 642}, {'_account_id': 679}, {'_account_id': 782}, {'_account_id': 1030}, {'_account_id': 1247}, {'_account_id': 1297}, {'_account_id': 1561}, {'_account_id': 1779}, {'_account_id': 1812}, {'_account_id': 1849}, {'_account_id': 2271}, {'_account_id': 2284}, {'_account_id': 2750}, {'_account_id': 2835}, {'_account_id': 4393}, {'_account_id': 5170}, {'_account_id': 5292}, {'_account_id': 5441}, {'_account_id': 5511}, {'_account_id': 6167}, {'_account_id': 6873}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-07 23:59:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/70b9502008eb42b97d5ef6089ac346a2b08b8894', 'message': 'WIP: Add blueprint slots document\n\nNOTE: This is a very rough draft, and is being posted to get very\npreliminary feedback.\n\nAs discussed at the nova mid-cycle, we would like to try a different\nprocess for managing blueprints in Kilo.\n\nChange-Id: I7075a130c01be22f3ee5315d4a65b8e524b5a1ad\nTODO: better commit message\n'}, {'number': 2, 'created': '2014-08-08 02:22:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/dd2b473b10138c8035b1523ca2a27830ed34a090', 'message': 'WIP: Add blueprint runway document\n\nNOTE: This is a very rough draft, and is being posted to get very\npreliminary feedback.\n\nAs discussed at the nova mid-cycle, we would like to try a different\nprocess for managing blueprints in Kilo.\n\nTODO: better commit message\n\nChange-Id: I7075a130c01be22f3ee5315d4a65b8e524b5a1ad\n'}, {'number': 3, 'created': '2014-08-08 22:24:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/0552d3c241c02b5f1b7c735a88206ccd6eeb3ce3', 'message': 'WIP: Add blueprint runway document\n\nNOTE: This is a rough draft, and is being posted to get very\nfeedback.\n\nAs discussed at the nova mid-cycle, we would like to try a different\nprocess for managing blueprints in Kilo.\n\nChange-Id: I7075a130c01be22f3ee5315d4a65b8e524b5a1ad\n'}, {'number': 4, 'created': '2014-08-12 00:43:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c28dd53cfc23f7330cac4125e8270a28a7495e11', 'message': ""Add plan for blueprint runways in Kilo\n\nNOTE: This is a rough draft, and is being posted to get feedback.\n\nAt the nova mid-cycle we agreed that while specs have mode are blueprint\nprocess significantly better, we think the current approach still leaves\nmuch to be lacking. This document is the outcome of the discussion at\nthe mid cycle on how we want to manage blueprints in Kilo.\n\nThis proposal is being placed in devref, as we want to use gerrit's\nworkflow (two core reviews, inline comments, revisions etc) and this is\na document for developers.\n\nChange-Id: I7075a130c01be22f3ee5315d4a65b8e524b5a1ad\n""}, {'number': 5, 'created': '2014-08-19 21:38:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c67c677df3ddaa1e3a760bd73e67975f269fa120', 'message': ""Add plan for kilo blueprints: project priorities\n\nAt the nova mid-cycle we agreed that while specs have mode are blueprint\nprocess significantly better, we think the current approach still leaves\nmuch to be lacking. This document is the outcome of the discussion at\nthe mid cycle on how we want to manage blueprints in Kilo.\n\nThis proposal is being placed in devref, as we want to use gerrit's\nworkflow (two core reviews, inline comments, revisions etc) and this is\na document for developers.\n\nChange-Id: I7075a130c01be22f3ee5315d4a65b8e524b5a1ad\n""}, {'number': 6, 'created': '2014-08-25 18:37:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1d7e54bb71201dfd7c624543a13f26a6d1f1c7c9', 'message': ""Add plan for kilo blueprints: project priorities\n\nAt the nova mid-cycle we agreed that while specs have made our blueprint\nprocess significantly better, we think the current approach still leaves\nmuch to be desired. This document is the outcome of the discussion at\nthe mid-cycle on how we want to manage blueprints in Kilo.\n\nThis proposal is being placed in devref, as we want to use gerrit's\nworkflow (two core reviews, inline comments, revisions etc) and this is\na document for developers.\n\nIntroduce the idea of project priorities.\n\nChange-Id: I7075a130c01be22f3ee5315d4a65b8e524b5a1ad\n""}, {'number': 7, 'created': '2014-08-29 20:44:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/22c64b87d578e494b07e02983a5ab50259347fff', 'message': ""Add plan for kilo blueprints: project priorities\n\nAt the nova mid-cycle we agreed that while specs have made our blueprint\nprocess significantly better, we think the current approach still leaves\nmuch to be desired. This document is the outcome of the discussion at\nthe mid-cycle on how we want to manage blueprints in Kilo.\n\nThis proposal is being placed in devref, as we want to use gerrit's\nworkflow (two core reviews, inline comments, revisions etc) and this is\na document for developers.\n\nIntroduce the idea of project priorities.\n\nChange-Id: I7075a130c01be22f3ee5315d4a65b8e524b5a1ad\n""}, {'number': 8, 'created': '2014-09-06 01:34:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3fceb18a016d8b40e6996d63bcb295e56d0a05a0', 'message': ""Add plan for kilo blueprints: project priorities\n\nAt the nova mid-cycle we agreed that while specs have made our blueprint\nprocess significantly better, we think the current approach still leaves\nmuch to be desired. This document is the outcome of the discussion at\nthe mid-cycle on how we want to manage blueprints in Kilo.\n\nThis proposal is being placed in devref, as we want to use gerrit's\nworkflow (two core reviews, inline comments, revisions etc) and this is\na document for developers.\n\nIntroduce the idea of project priorities.\n\nChange-Id: I7075a130c01be22f3ee5315d4a65b8e524b5a1ad\n""}, {'number': 9, 'created': '2014-09-12 22:34:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/6f7137812d7b1164e0f71aa1ddb815a766542f7f', 'message': ""Add plan for kilo blueprints: project priorities\n\nAt the nova mid-cycle we agreed that while specs have made our blueprint\nprocess significantly better, we think the current approach still leaves\nmuch to be desired. This document is the outcome of the discussion at\nthe mid-cycle on how we want to manage blueprints in Kilo.\n\nThis proposal is being placed in devref, as we want to use gerrit's\nworkflow (two core reviews, inline comments, revisions etc) and this is\na document for developers.\n\nIntroduce the idea of project priorities.\n\nChange-Id: I7075a130c01be22f3ee5315d4a65b8e524b5a1ad\n""}, {'number': 10, 'created': '2014-09-17 21:15:00.000000000', 'files': ['doc/source/devref/kilo.blueprints.rst'], 'web_link': 'https://opendev.org/openstack/nova/commit/f569f0a002d64965d406c06fba1ec5154f08f0a0', 'message': ""Add plan for kilo blueprints: project priorities\n\nAt the nova mid-cycle we agreed that while specs have made our blueprint\nprocess significantly better, we think the current approach still leaves\nmuch to be desired. This document is the outcome of the discussion at\nthe mid-cycle on how we want to manage blueprints in Kilo.\n\nThis proposal is being placed in devref, as we want to use gerrit's\nworkflow (two core reviews, inline comments, revisions etc) and this is\na document for developers.\n\nIntroduce the idea of project priorities.\n\nChange-Id: I7075a130c01be22f3ee5315d4a65b8e524b5a1ad\n""}]",128,112733,f569f0a002d64965d406c06fba1ec5154f08f0a0,135,31,10,1849,,,0,"Add plan for kilo blueprints: project priorities

At the nova mid-cycle we agreed that while specs have made our blueprint
process significantly better, we think the current approach still leaves
much to be desired. This document is the outcome of the discussion at
the mid-cycle on how we want to manage blueprints in Kilo.

This proposal is being placed in devref, as we want to use gerrit's
workflow (two core reviews, inline comments, revisions etc) and this is
a document for developers.

Introduce the idea of project priorities.

Change-Id: I7075a130c01be22f3ee5315d4a65b8e524b5a1ad
",git fetch https://review.opendev.org/openstack/nova refs/changes/33/112733/9 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/devref/index.rst', 'doc/source/devref/slots.rst']",2,70b9502008eb42b97d5ef6089ac346a2b08b8894,bp/s,"=============================================== Blueprint in Kilo: Slots and project Priorities =============================================== Note: This as an experiment, and we should reevaluate it at the end of Kilo Today ------ * Over 85 approved blueprints for Juno, many of which won't land * 33 blueprints currently need code review * Over 500 open patches * With so many patches/blueprints in code review the nova review team has to do a lot of multitasking and context switching between patches * Not enough time spent working on 'strategic' features * Review overload Problem -------- * Humans are bad at multitasking and context switching. By trying to review so many blueprints at once, the review team ends up wasting a significant amount of time in context switching. * Poor job at prioritizing 'strategic' features over 'tactical' ones. As a team we have not spent enough time on 'strategic' features such as: finishing efforts we previously started, upgradeabilty, DB, fault tolerance, functional testing, usability, stability and scalability related features. Idea ---- * To address the reviewer context switching issue, restrict the number of blueprints up for review to 10 'slots' at any given time. Once a blueprint has been merged, fill the slot with another blueprint. * By limiting the number of blueprints up for review at any given time hopefully we can actually merge more blueprints in a given cycle. * Blueprints that do not have a slot are blocked * Slots are for review bandwidth not development bandwidth * Only change how we deal with blueprints, not bugs * 10 slots as an arbitrary starting point (roughly half the number of core reviewers). * We can reevaluate the number of slots periodically. * Split the slots into two groups: tactical and strategic. * For Kilo, we want to focus on strategic work, so 8 strategic slots and 2 tactical slots. * The breakdown should be periodically reevaluated. * Fill slots from prioritized queues (one for strategic and one for tactical) of features that are ready to be reviewed * Pick several themes for the 'strategic' queue to help us prioritize the queue. Sample 'strategic' themes: * Objects * Next generation API * Performance * Fault tolerance * Live upgrades * Functional testing Sample 'strategic' blueprints: * Convert X to Objects * Glance V2 support * Cinder v2 support * Tasks * Remove stacktraces from nova-api * Improve usage of sqlalchy: https://wiki.openstack.org/wiki/Openstack_and_SQLAlchemy#ORM_Quick_Wins_Proof_of_Concept When is a blueprint needed? --------------------------- A blueprint is needed for any feature that requires a design discussion. In the past we have used blueprints and specs for all new features no matter how big or small they are. We have added the specs workflow to help improve the design discussion part of blueprints, but are using it as a documentation tool as well. If a new feature is straight forward enough that it doesn't need any design discussion we shouldn't require a blueprint purely for documentation purposes. Instead we should require the ``DocImpact`` flag and a thorough commit message. Guidelines for when a feature doesn't need a blueprint (and can use a wishlist bug instead). * Is the feature a single self contained change? * If the feature touches code all over the place, it probably should have a design discussion. * If the feature is big enough that it needs more then one commit, it probably should have a design discussion. * Not an API change. * API changes always require a design discussion. Examples where no blueprint is needed: * New scheduler filters * Allow DB migration backports Slots workflow --------------- TODO ",,91,0
openstack%2Fkeystone~master~I4aaa57b05e9aab2ff223f62bd1f5fe0f37e7f247,openstack/keystone,master,I4aaa57b05e9aab2ff223f62bd1f5fe0f37e7f247,Fixes code comment to be more accurate,MERGED,2014-09-08 13:46:15.000000000,2014-09-29 02:23:31.000000000,2014-09-29 02:23:31.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 2903}, {'_account_id': 6486}, {'_account_id': 8978}, {'_account_id': 10873}, {'_account_id': 11428}, {'_account_id': 13055}]","[{'number': 1, 'created': '2014-09-08 13:46:15.000000000', 'files': ['keystone/assignment/core.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/ab1333af306e191dd4ca5983fc936b568d719d7f', 'message': 'Fixes code comment to be more accurate\n\nChange-Id: I4aaa57b05e9aab2ff223f62bd1f5fe0f37e7f247\n'}]",0,119760,ab1333af306e191dd4ca5983fc936b568d719d7f,14,8,1,7725,,,0,"Fixes code comment to be more accurate

Change-Id: I4aaa57b05e9aab2ff223f62bd1f5fe0f37e7f247
",git fetch https://review.opendev.org/openstack/keystone refs/changes/60/119760/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/assignment/core.py'],1,ab1333af306e191dd4ca5983fc936b568d719d7f,," # Domain management functions for backends that only allow a single # domain. Currently, this is only LDAP, but might be used by other # backends in the future."," # domain management functions for backends that only allow a single # domain. currently, this is only LDAP, but might be used by PAM or other # backends as well. This is used by both identity and assignment drivers.",3,3
openstack%2Fkeystone~master~Ic3f4a3eb9da303a4da7d532f02f6c6e82a725924,openstack/keystone,master,Ic3f4a3eb9da303a4da7d532f02f6c6e82a725924,Correct typos in keystone/common/base64utils.py docstrings,MERGED,2014-09-08 14:34:25.000000000,2014-09-29 02:22:24.000000000,2014-09-29 02:22:23.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 2903}, {'_account_id': 6460}, {'_account_id': 6486}, {'_account_id': 6890}, {'_account_id': 8978}, {'_account_id': 13009}]","[{'number': 1, 'created': '2014-09-08 14:34:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/9a5a7161b9954a5139c538ebfcf376801d8fd73d', 'message': 'Correct typos in keystone/common/base64utils.py docstrings\n\nChange-Id: Ic3f4a3eb9da303a4da7d532f02f6c6e82a725924\n'}, {'number': 2, 'created': '2014-09-08 14:35:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/00228655a7eb80878d64ad74c541ed59db7e5562', 'message': 'Correct typos in keystone/common/base64utils.py docstrings\n\nCloses-bug: #1360446\nChange-Id: Ic3f4a3eb9da303a4da7d532f02f6c6e82a725924\n'}, {'number': 3, 'created': '2014-09-08 14:35:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/2b75a7fcb5171f1ac17fb2f56a80a1fb292cf384', 'message': 'Correct typos in keystone/common/base64utils.py docstrings\n\nChange-Id: Ic3f4a3eb9da303a4da7d532f02f6c6e82a725924\n'}, {'number': 4, 'created': '2014-09-08 14:35:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/0b476048c9af788ee09a7c3a3ce2957082c0a0d9', 'message': 'Correct typos in keystone/common/base64utils.py docstrings\n\nCloses-bug: #1366649\nChange-Id: Ic3f4a3eb9da303a4da7d532f02f6c6e82a725924\n'}, {'number': 5, 'created': '2014-09-11 10:14:50.000000000', 'files': ['keystone/common/base64utils.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/3f35c5c0f99180518739a08d1ade5ee1f4c2e726', 'message': 'Correct typos in keystone/common/base64utils.py docstrings\n\nCloses-bug: #1366649\nChange-Id: Ic3f4a3eb9da303a4da7d532f02f6c6e82a725924\n'}]",0,119775,3f35c5c0f99180518739a08d1ade5ee1f4c2e726,25,8,5,13009,,,0,"Correct typos in keystone/common/base64utils.py docstrings

Closes-bug: #1366649
Change-Id: Ic3f4a3eb9da303a4da7d532f02f6c6e82a725924
",git fetch https://review.opendev.org/openstack/keystone refs/changes/75/119775/5 && git format-patch -1 --stdout FETCH_HEAD,['keystone/common/base64utils.py'],1,9a5a7161b9954a5139c538ebfcf376801d8fd73d,typo-corr,Standardized base64 is defined in base64url text is designed to be safe for use in file names and percent-encoding has the downside of requiring Base64 text is normally expected to be a multiple of 4 always a multiple of 4. Padding is ignored and does not alter the alphabet (i.e no whitespace). Iteration yields a sequence of lines. alphabet (i.e no whitespace). Iteration yields a sequence of lines. alphabet (i.e no whitespace). Fold the text into lines whose,Standarized base64 is defined in base64url text is designed to be safe for use in filenames and percent-enconding has the downside of requiring Base64 text is normally expected to be a multple of 4 always a multple of 4. Padding is ignored and does not alter the alphabet (i.e no whitepace). Iteration yields a sequence of lines. alphabet (i.e no whitepace). Iteration yields a sequence of lines. alphabet (i.e no whitepace). Fold the text into lines whose,8,8
openstack%2Fkeystone~master~Ie1afee4e37cfa149ddb73a001985c98aa90b97a5,openstack/keystone,master,Ie1afee4e37cfa149ddb73a001985c98aa90b97a5,improve dependency injection doc strings,MERGED,2014-09-10 14:34:19.000000000,2014-09-29 02:21:55.000000000,2014-09-29 02:21:55.000000000,"[{'_account_id': 3}, {'_account_id': 2903}, {'_account_id': 6460}, {'_account_id': 6486}, {'_account_id': 8978}]","[{'number': 1, 'created': '2014-09-10 14:34:19.000000000', 'files': ['keystone/common/dependency.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/5e432fc52860ce7789f646dcde52c4e9b1792cf2', 'message': ""improve dependency injection doc strings\n\nThis also renames the provider_name argument to better convey it's\nstatus as a kwarg for internal use only.\n\nChange-Id: Ie1afee4e37cfa149ddb73a001985c98aa90b97a5\n""}]",2,120438,5e432fc52860ce7789f646dcde52c4e9b1792cf2,9,5,1,4,,,0,"improve dependency injection doc strings

This also renames the provider_name argument to better convey it's
status as a kwarg for internal use only.

Change-Id: Ie1afee4e37cfa149ddb73a001985c98aa90b97a5
",git fetch https://review.opendev.org/openstack/keystone refs/changes/38/120438/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/common/dependency.py'],1,5e432fc52860ce7789f646dcde52c4e9b1792cf2,,"Providers are registered via the ``@provider()`` decorator, and dependencies on them are registered with ``@requires()`` or ``@optional()``. Providers are available to their consumers via an attribute. See the documentation for the individual functions for more detail. """"""Raised when a required dependency is not resolvable. See ``resolve_future_dependencies()`` for more details. """"""A class decorator used to register providers. When ``@provider()`` is used to decorate a class, members of that class will register themselves as providers for the named dependency. As an example, In the code fragment:: The object ``foo`` will be registered as a provider for ``foo_api``. No more than one such instance should be created; additional instances will replace the previous ones, possibly resulting in different instances being used by different consumers. resolve_future_dependencies(__provider_name=name) """"""A class decorator used to inject providers into consumers. class via an attribute with the same name as the provider. For example, in the code fragment:: The object ``client`` will have attributes named ``foo_api`` and ``bar_api``, which are instances of the named providers. ``resolve_future_dependencies()`` has been called; they may not exist Dependencies registered via ``@required()`` must have providers; if not, an ``UnresolvableDependencyException`` will be raised when ``resolve_future_dependencies()`` is called. """"""Similar to ``@requires()``, except that the dependencies are optional. If no provider is available, the attributes will be set to ``None``. """"""def resolve_future_dependencies(__provider_name=None): """"""Forces injection of all dependencies. ``UnresolvableDependencyException``. the optional argument, ``__provider_name`` is used internally, and should be treated as an implementation detail. if __provider_name: targets = _future_dependencies.pop(__provider_name, []) targets.extend(_future_optionals.pop(__provider_name, [])) setattr(target, __provider_name, REGISTRY[__provider_name])","Providers are registered via the 'provider' decorator, and dependencies on them are registered with 'requires' or 'optional'. Providers are available to their consumers via an attribute. See the documentation for the individual functions for more detail. """"""An UnresolvableDependencyException is raised when a required dependency is not resolvable; see 'resolve_future_dependencies'. """"""'provider' is a class decorator used to register providers. When 'provider' is used to decorate a class, members of that class will register themselves as providers for the named dependency. As an example, In the code fragment:: The object 'foo' will be registered as a provider for 'foo_api'. No more than one such instance should be created; additional instances will replace the previous ones, possibly resulting in different instances being used by different consumers. resolve_future_dependencies(name) """"""'requires' is a class decorator used to inject providers into consumers. class via an attribute with the same name as the provider. For example, in the code fragment:: The object 'client' will have attributes named 'foo_api' and 'bar_api', which are instances of the named providers. 'resolve_future_dependencies' has been called; they may not exist Dependencies registered via 'required' must have providers - if not, an exception will be raised when 'resolve_future_dependencies' is called. """"""'optional' is the same as 'requires', except that the dependencies are optional - if no provider is available, the attributes will be set to None. """""" def resolve_future_dependencies(provider_name=None): """"""'resolve_future_dependencies' forces injection of all dependencies. UnresolvableDependencyException. the optional argument is used internally, and should be treated as an implementation detail. if provider_name: targets = _future_dependencies.pop(provider_name, []) targets.extend(_future_optionals.pop(provider_name, [])) setattr(target, provider_name, REGISTRY[provider_name])",45,35
openstack%2Fcookbook-openstack-network~stable%2Ficehouse~Ib695668a585c7d909d371da94e8fb244ae349e82,openstack/cookbook-openstack-network,stable/icehouse,Ib695668a585c7d909d371da94e8fb244ae349e82,Added fix for dependacy for mysql-chef_gem,MERGED,2014-09-24 16:01:23.000000000,2014-09-29 01:53:22.000000000,2014-09-29 01:53:21.000000000,"[{'_account_id': 3}, {'_account_id': 1032}, {'_account_id': 7128}]","[{'number': 1, 'created': '2014-09-24 16:01:23.000000000', 'files': ['Berksfile.lock'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-network/commit/97835859f9b10d56098d1a31c66e7bf447399857', 'message': ""Added fix for dependacy for mysql-chef_gem\n\nBumped the version to 0.0.4 that has a fix for ubuntu ruby191-dev.\nThe error this resolves is this:\n```ruby\n        /usr/bin/ruby1.9.1 extconf.rb\n/usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require': cannot load such file -- mkmf (LoadError)\n        from /usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require'\n        from extconf.rb:5:in `<main>'\n```\n\nChange-Id: Ib695668a585c7d909d371da94e8fb244ae349e82\n""}]",0,123772,97835859f9b10d56098d1a31c66e7bf447399857,7,3,1,12323,,,0,"Added fix for dependacy for mysql-chef_gem

Bumped the version to 0.0.4 that has a fix for ubuntu ruby191-dev.
The error this resolves is this:
```ruby
        /usr/bin/ruby1.9.1 extconf.rb
/usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require': cannot load such file -- mkmf (LoadError)
        from /usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require'
        from extconf.rb:5:in `<main>'
```

Change-Id: Ib695668a585c7d909d371da94e8fb244ae349e82
",git fetch https://review.opendev.org/openstack/cookbook-openstack-network refs/changes/72/123772/1 && git format-patch -1 --stdout FETCH_HEAD,['Berksfile.lock'],1,97835859f9b10d56098d1a31c66e7bf447399857,mysql-chef_gem-update," ""locked_version"": ""0.0.4"""," ""locked_version"": ""0.0.2""",1,1
openstack%2Fcookbook-openstack-image~stable%2Ficehouse~I18d684815b71754e76d086d9fd56afab3fabe45a,openstack/cookbook-openstack-image,stable/icehouse,I18d684815b71754e76d086d9fd56afab3fabe45a,Added fix for dependacy for mysql-chef_gem,MERGED,2014-09-24 15:52:48.000000000,2014-09-29 01:52:14.000000000,2014-09-29 01:52:14.000000000,"[{'_account_id': 3}, {'_account_id': 1032}, {'_account_id': 7128}]","[{'number': 1, 'created': '2014-09-24 15:52:48.000000000', 'files': ['Berksfile.lock'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-image/commit/35615e7ce2827e5eba3c962ed4d9e414b6c2a322', 'message': ""Added fix for dependacy for mysql-chef_gem\n\nBumped the version to 0.0.4 that has a fix for ubuntu ruby191-dev.\nThe error this resolves is this:\n```ruby\n        /usr/bin/ruby1.9.1 extconf.rb\n/usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require': cannot load such file -- mkmf (LoadError)\n        from /usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require'\n        from extconf.rb:5:in `<main>'\n```\n\nChange-Id: I18d684815b71754e76d086d9fd56afab3fabe45a\n""}]",0,123770,35615e7ce2827e5eba3c962ed4d9e414b6c2a322,7,3,1,12323,,,0,"Added fix for dependacy for mysql-chef_gem

Bumped the version to 0.0.4 that has a fix for ubuntu ruby191-dev.
The error this resolves is this:
```ruby
        /usr/bin/ruby1.9.1 extconf.rb
/usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require': cannot load such file -- mkmf (LoadError)
        from /usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require'
        from extconf.rb:5:in `<main>'
```

Change-Id: I18d684815b71754e76d086d9fd56afab3fabe45a
",git fetch https://review.opendev.org/openstack/cookbook-openstack-image refs/changes/70/123770/1 && git format-patch -1 --stdout FETCH_HEAD,['Berksfile.lock'],1,35615e7ce2827e5eba3c962ed4d9e414b6c2a322,mysql-chef_gem-update," ""locked_version"": ""0.0.4"""," ""locked_version"": ""0.0.2""",1,1
openstack%2Fcookbook-openstack-block-storage~master~I147fbb75cf75357f578e4f84a0d898149be9b1d0,openstack/cookbook-openstack-block-storage,master,I147fbb75cf75357f578e4f84a0d898149be9b1d0,Add attribute for ibm nas stroage driver ibmnas_platform_type,MERGED,2014-09-17 15:56:33.000000000,2014-09-29 01:51:21.000000000,2014-09-29 01:51:21.000000000,"[{'_account_id': 3}, {'_account_id': 1032}, {'_account_id': 2589}, {'_account_id': 7128}]","[{'number': 1, 'created': '2014-09-17 15:56:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-block-storage/commit/3f7ad8adabaef35e912ba4b551c4f7356c8b196a', 'message': 'Add attribute for ibm nas stroage driver ibmnas_platform_type\n\n* Add new ibmnas_platform_type attribute\n* Fix minor bug with nas_ssh_port\n\nChange-Id: I147fbb75cf75357f578e4f84a0d898149be9b1d0\nCloses-Bug: #1370574\n'}, {'number': 2, 'created': '2014-09-23 15:12:44.000000000', 'files': ['attributes/default.rb', 'spec/cinder_common_spec.rb', 'templates/default/cinder.conf.erb', 'CHANGELOG.md', 'README.md'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-block-storage/commit/094e023ccc709de91f7852ed496ee14730339fbf', 'message': 'Add attribute for ibm nas stroage driver ibmnas_platform_type\n\n* Add new ibmnas_platform_type attribute\n* Fix minor bug with nas_ssh_port\n\nChange-Id: I147fbb75cf75357f578e4f84a0d898149be9b1d0\nCloses-Bug: #1370574\n'}]",0,122189,094e023ccc709de91f7852ed496ee14730339fbf,14,4,2,7128,,,0,"Add attribute for ibm nas stroage driver ibmnas_platform_type

* Add new ibmnas_platform_type attribute
* Fix minor bug with nas_ssh_port

Change-Id: I147fbb75cf75357f578e4f84a0d898149be9b1d0
Closes-Bug: #1370574
",git fetch https://review.opendev.org/openstack/cookbook-openstack-block-storage refs/changes/89/122189/2 && git format-patch -1 --stdout FETCH_HEAD,"['attributes/default.rb', 'spec/cinder_common_spec.rb', 'templates/default/cinder.conf.erb', 'CHANGELOG.md', 'README.md']",5,3f7ad8adabaef35e912ba4b551c4f7356c8b196a,bug/1370574,* `openstack['block-storage']['ibmnas']['nas_ssh_port']` - ssh port of IBMNAS storage* `openstack['block-storage']['ibmnas']['ibmnas_platform_type']` - Platform type to be used as backend storage,,25,3
openstack%2Fcookbook-openstack-telemetry~stable%2Ficehouse~I307053e38aaef141bb46644a87512c33e3d934c0,openstack/cookbook-openstack-telemetry,stable/icehouse,I307053e38aaef141bb46644a87512c33e3d934c0,Added fix for dependacy for mysql-chef_gem,MERGED,2014-09-24 15:55:37.000000000,2014-09-29 01:51:06.000000000,2014-09-29 01:51:05.000000000,"[{'_account_id': 3}, {'_account_id': 1032}, {'_account_id': 7128}]","[{'number': 1, 'created': '2014-09-24 15:55:37.000000000', 'files': ['Berksfile.lock'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-telemetry/commit/877dfdf9e7dc5215e79d5aeaf34594b2a9bcbde1', 'message': ""Added fix for dependacy for mysql-chef_gem\n\nBumped the version to 0.0.4 that has a fix for ubuntu ruby191-dev.\nThe error this resolves is this:\n```ruby\n        /usr/bin/ruby1.9.1 extconf.rb\n/usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require': cannot load such file -- mkmf (LoadError)\n        from /usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require'\n        from extconf.rb:5:in `<main>'\n```\n\nChange-Id: I307053e38aaef141bb46644a87512c33e3d934c0\n""}]",0,123771,877dfdf9e7dc5215e79d5aeaf34594b2a9bcbde1,7,3,1,12323,,,0,"Added fix for dependacy for mysql-chef_gem

Bumped the version to 0.0.4 that has a fix for ubuntu ruby191-dev.
The error this resolves is this:
```ruby
        /usr/bin/ruby1.9.1 extconf.rb
/usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require': cannot load such file -- mkmf (LoadError)
        from /usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require'
        from extconf.rb:5:in `<main>'
```

Change-Id: I307053e38aaef141bb46644a87512c33e3d934c0
",git fetch https://review.opendev.org/openstack/cookbook-openstack-telemetry refs/changes/71/123771/1 && git format-patch -1 --stdout FETCH_HEAD,['Berksfile.lock'],1,877dfdf9e7dc5215e79d5aeaf34594b2a9bcbde1,mysql-chef_gem-update," ""locked_version"": ""0.0.4"""," ""locked_version"": ""0.0.2""",1,1
openstack%2Fnova~master~I5765720d8c3e0a7962b493fa49359514f3136544,openstack/nova,master,I5765720d8c3e0a7962b493fa49359514f3136544,Add test for get_instance_disk_info to test_virt_drivers,MERGED,2014-08-19 21:31:52.000000000,2014-09-29 01:48:52.000000000,2014-09-29 01:48:50.000000000,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 2271}, {'_account_id': 3185}, {'_account_id': 5170}, {'_account_id': 6873}, {'_account_id': 9008}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-08-19 21:31:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/65201711dd7f55ae53a9f8388e63519f88dd0c2c', 'message': ""Add test for get_instance_disk_info to test_virt_drivers\n\nCommit 2bed16c89356554a193a111d268a9587709ed2f7 added\nget_instance_disk_info to the base ComputeDriver class\nbut didn't add a test for it in test_virt_drivers, so\nit's added here.\n\nNote that this wouldn't have found the problem for the\nhyperv or vmware drivers due to bug 1209421.\n\nChange-Id: I5765720d8c3e0a7962b493fa49359514f3136544\nRelated-Bug: #1358719\n""}, {'number': 2, 'created': '2014-08-19 21:32:38.000000000', 'files': ['nova/virt/fake.py', 'nova/tests/virt/test_virt_drivers.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/269e27356c5cc70249d0419cb0e6e5449632a2b9', 'message': ""Add test for get_instance_disk_info to test_virt_drivers\n\nCommit 2bed16c89356554a193a111d268a9587709ed2f7 added\nget_instance_disk_info to the base ComputeDriver class\nbut didn't add a test for it in test_virt_drivers, so\nit's added here.\n\nThis also fixes the wrong method signature in the fake\nvirt driver.\n\nNote that this wouldn't have found the problem for the\nhyperv or vmware drivers due to bug 1209421.\n\nChange-Id: I5765720d8c3e0a7962b493fa49359514f3136544\nRelated-Bug: #1358719\n""}]",0,115406,269e27356c5cc70249d0419cb0e6e5449632a2b9,21,8,2,6873,,,0,"Add test for get_instance_disk_info to test_virt_drivers

Commit 2bed16c89356554a193a111d268a9587709ed2f7 added
get_instance_disk_info to the base ComputeDriver class
but didn't add a test for it in test_virt_drivers, so
it's added here.

This also fixes the wrong method signature in the fake
virt driver.

Note that this wouldn't have found the problem for the
hyperv or vmware drivers due to bug 1209421.

Change-Id: I5765720d8c3e0a7962b493fa49359514f3136544
Related-Bug: #1358719
",git fetch https://review.opendev.org/openstack/nova refs/changes/06/115406/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/fake.py', 'nova/tests/virt/test_virt_drivers.py']",2,65201711dd7f55ae53a9f8388e63519f88dd0c2c,bug/1209421," @catch_notimplementederror def test_get_instance_disk_info(self): # This should be implemented by any driver that supports live migrate. instance_ref, network_info = self._get_running_instance() self.connection.get_instance_disk_info(instance_ref['name'], block_device_info={}) ",,8,1
openstack%2Fopenstack-chef-specs~master~I2f97605d28f8ef7d32a295a3097750e73feea727,openstack/openstack-chef-specs,master,I2f97605d28f8ef7d32a295a3097750e73feea727,Update Chef gem to version 11.16,MERGED,2014-09-19 18:29:12.000000000,2014-09-29 01:36:07.000000000,2014-09-29 01:36:07.000000000,"[{'_account_id': 3}, {'_account_id': 1032}, {'_account_id': 2589}]","[{'number': 1, 'created': '2014-09-19 18:29:12.000000000', 'files': ['specs/juno/common/bump-chef-to-11-16.rst'], 'web_link': 'https://opendev.org/openstack/openstack-chef-specs/commit/14b8fcbb7c3c564b087d191877bd2c4cc73db213', 'message': 'Update Chef gem to version 11.16\n\nBlueprint bump-chef-to-11.16\n\nChange-Id: I2f97605d28f8ef7d32a295a3097750e73feea727\n'}]",0,122817,14b8fcbb7c3c564b087d191877bd2c4cc73db213,7,3,1,7128,,,0,"Update Chef gem to version 11.16

Blueprint bump-chef-to-11.16

Change-Id: I2f97605d28f8ef7d32a295a3097750e73feea727
",git fetch https://review.opendev.org/openstack/openstack-chef-specs refs/changes/17/122817/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/common/bump-chef-to-11-16.rst'],1,14b8fcbb7c3c564b087d191877bd2c4cc73db213,bp/bump-chef-to-11,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Bump Chef gem to 11.16 ========================================== Launchpad blueprint: https://blueprints.launchpad.net/openstack-chef/+spec/bump-chef-to-11-16 Propose to update all the cookbooks at master branch to use Chef 11.16. The current version is at 11.12. Problem description =================== A detailed description of the problem: * During each Master branch cycle, we need to keep it up to date with the latest tooling. Proposed change =============== Change all the cookbook Gemfile's to use Chef 11.16 Alternatives ------------ none Data model impact ----------------- none REST API impact --------------- none Security impact --------------- none Notifications impact -------------------- none Other end user impact --------------------- none Performance Impact ------------------ Should help improve performance as there have been many fixes, for example: Other deployer impact --------------------- none Developer impact ---------------- The current bundle install command will take care of updating Chef to the required 11.16 version level. No additional steps should be needed by the developers. Implementation ============== Assignee(s) ----------- Primary assignee: Mark Vanderwiel (vanderwl) Other contributors: none Work Items ---------- * Each cookbooks Gemfile needs to specific Chef 11.16 Dependencies ============ none Testing ======= Basic testing (unit, lint, style, all-in-one) using Chef 11.16 has been done and no issues were found. Documentation Impact ==================== none References ========== * `Chef versions <https://rubygems.org/gems/chef/versions>`_ * `Chef change log <https://github.com/opscode/chef/blob/master/CHANGELOG.md>`_",,116,0
openstack%2Ffuel-web~master~I3d38cc1cc9e3b50206139ff5881e293c28da54ee,openstack/fuel-web,master,I3d38cc1cc9e3b50206139ff5881e293c28da54ee,Discard changes title not shown,MERGED,2014-09-26 14:15:32.000000000,2014-09-29 01:13:53.000000000,2014-09-29 01:13:52.000000000,"[{'_account_id': 3}, {'_account_id': 8735}, {'_account_id': 8766}, {'_account_id': 8971}, {'_account_id': 9091}, {'_account_id': 9730}]","[{'number': 1, 'created': '2014-09-26 14:15:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/9f193dba5f74c4c911501cb6f23b193ddf9b8dbd', 'message': 'Discard changes title not shown\n\nChange-Id: I3d38cc1cc9e3b50206139ff5881e293c28da54ee\n'}, {'number': 2, 'created': '2014-09-26 14:26:26.000000000', 'files': ['nailgun/static/js/views/cluster_page_subviews.jsx'], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/e569fce5258dda4c59bfc265efb0c61787719a95', 'message': 'Discard changes title not shown\n\nChange-Id: I3d38cc1cc9e3b50206139ff5881e293c28da54ee\n'}]",2,124415,e569fce5258dda4c59bfc265efb0c61787719a95,16,6,2,9091,,,0,"Discard changes title not shown

Change-Id: I3d38cc1cc9e3b50206139ff5881e293c28da54ee
",git fetch https://review.opendev.org/openstack/fuel-web refs/changes/15/124415/2 && git format-patch -1 --stdout FETCH_HEAD,['nailgun/static/js/views/cluster_page_subviews.jsx'],1,9f193dba5f74c4c911501cb6f23b193ddf9b8dbd,fix," title={$.t(""cluster_page.discard_changes"")}"," title='{$.t(""cluster_page.discard_changes"")}'",1,1
openstack%2Ffuel-web~master~Ie9acbdf14471d7ceb034047cae1f41336fb4d43e,openstack/fuel-web,master,Ie9acbdf14471d7ceb034047cae1f41336fb4d43e,Added requirejs-plugins,MERGED,2014-09-25 21:36:30.000000000,2014-09-29 01:13:02.000000000,2014-09-29 01:13:01.000000000,"[{'_account_id': 3}, {'_account_id': 8735}, {'_account_id': 8766}, {'_account_id': 8971}, {'_account_id': 9091}, {'_account_id': 9730}]","[{'number': 1, 'created': '2014-09-25 21:36:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/3e66bb6fc5ddf4674b26cb37bfd982a01ef46ab1', 'message': 'Added requirejs-plugins\n\n- translation.json is now loaded using require-json plugin\n- preparation for UI plugins\n\nChange-Id: Ie9acbdf14471d7ceb034047cae1f41336fb4d43e\n'}, {'number': 2, 'created': '2014-09-25 22:01:27.000000000', 'files': ['nailgun/package.json', 'nailgun/npm-shrinkwrap.json', 'nailgun/bower.json', 'nailgun/static/js/main.js'], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/ff45f3d298bf8b2dffc3108e633d79fd14e8f8d0', 'message': 'Added requirejs-plugins\n\n- translation.json is now loaded using require-json plugin\n- preparation for UI plugins\n\nChange-Id: Ie9acbdf14471d7ceb034047cae1f41336fb4d43e\n'}]",0,124192,ff45f3d298bf8b2dffc3108e633d79fd14e8f8d0,14,6,2,8735,,,0,"Added requirejs-plugins

- translation.json is now loaded using require-json plugin
- preparation for UI plugins

Change-Id: Ie9acbdf14471d7ceb034047cae1f41336fb4d43e
",git fetch https://review.opendev.org/openstack/fuel-web refs/changes/92/124192/1 && git format-patch -1 --stdout FETCH_HEAD,"['nailgun/bower.json', 'nailgun/static/js/main.js']",2,3e66bb6fc5ddf4674b26cb37bfd982a01ef46ab1,(detached," json: 'js/libs/bower/requirejs-plugins/json', deps: ['json!i18n/translation.json', 'jquery'], $.i18n.init({resStore: translation, fallbackLng: 'en-US'});"," deps: ['text!i18n/translation.json', 'jquery'], $.i18n.init({resStore: JSON.parse(translation), fallbackLng: 'en-US'});",4,2
openstack%2Ftempest~master~I30843dcaaa4d71efd151f550cb1e5b43b309e198,openstack/tempest,master,I30843dcaaa4d71efd151f550cb1e5b43b309e198,Skip large_ops_scenario if large_ops_number < 1,MERGED,2014-09-21 10:02:38.000000000,2014-09-29 00:55:29.000000000,2014-09-29 00:55:28.000000000,"[{'_account_id': 3}, {'_account_id': 5292}, {'_account_id': 6167}, {'_account_id': 8556}, {'_account_id': 8576}, {'_account_id': 8871}]","[{'number': 1, 'created': '2014-09-21 10:02:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/16fbfc9052bb1a6ab862bd6db470cd6facc5032a', 'message': 'Skip large_ops_scenario if large_ops_number < 1\n\ntest_large_ops_scenario does nothing if CONF.scenario.large_ops_number < 1\nSet to skip with appropriate message instead\n\nChange-Id: I30843dcaaa4d71efd151f550cb1e5b43b309e198\n'}, {'number': 2, 'created': '2014-09-22 07:02:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/2bd1b21c9f726d21afb9eadb48da25a853d7b0df', 'message': 'Skip large_ops_scenario if large_ops_number < 1\n\ntest_large_ops_scenario does nothing if CONF.scenario.large_ops_number < 1\nSet to skip with appropriate message instead\n\nChange-Id: I30843dcaaa4d71efd151f550cb1e5b43b309e198\n'}, {'number': 3, 'created': '2014-09-28 11:30:31.000000000', 'files': ['tempest/scenario/test_large_ops.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/c1f1e83877c8693fab69e74ed45e1ead3db43a3b', 'message': 'Skip large_ops_scenario if large_ops_number < 1\n\ntest_large_ops_scenario does nothing if CONF.scenario.large_ops_number < 1\nSet to skip with appropriate message instead\n\nChange-Id: I30843dcaaa4d71efd151f550cb1e5b43b309e198\n'}]",0,122998,c1f1e83877c8693fab69e74ed45e1ead3db43a3b,22,6,3,8576,,,0,"Skip large_ops_scenario if large_ops_number < 1

test_large_ops_scenario does nothing if CONF.scenario.large_ops_number < 1
Set to skip with appropriate message instead

Change-Id: I30843dcaaa4d71efd151f550cb1e5b43b309e198
",git fetch https://review.opendev.org/openstack/tempest refs/changes/98/122998/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/test_large_ops.py'],1,16fbfc9052bb1a6ab862bd6db470cd6facc5032a,large_scale_skip," if CONF.scenario.large_ops_number < 1: raise cls.skipException(""large_ops_number not set to multiple "" ""instances"")", if CONF.scenario.large_ops_number < 1: return,3,2
openstack%2Ftaskflow~master~I7cebd882b655958d539be36ce3b4deb75ac4a0b7,openstack/taskflow,master,I7cebd882b655958d539be36ce3b4deb75ac4a0b7,Adjust the WBE log levels,MERGED,2014-09-12 00:15:47.000000000,2014-09-29 00:07:46.000000000,2014-09-29 00:07:45.000000000,"[{'_account_id': 3}, {'_account_id': 5638}, {'_account_id': 6648}, {'_account_id': 9608}]","[{'number': 1, 'created': '2014-09-12 00:15:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/528daed882a639389d9fa63e525260f1ec15406a', 'message': 'Adjust the WBE log levels\n\nTo conform better with the logging level standards\nmove away from using LOG.exception when the level\nis more appropriately a warning/warn.\n\nAlso changes how a message that can not be sent is really\na critical error and should be treated as such (since such an\nerror affects the overall execution model).\n\nChange-Id: I7cebd882b655958d539be36ce3b4deb75ac4a0b7\n'}, {'number': 2, 'created': '2014-09-27 21:58:24.000000000', 'files': ['taskflow/engines/worker_based/server.py', 'taskflow/engines/worker_based/executor.py', 'taskflow/tests/unit/worker_based/test_server.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/28b2f8fb1bd6a618fe4f24ed9eb0d5b6ae76d12e', 'message': 'Adjust the WBE log levels\n\nTo conform better with the logging level standards\nmove away from using LOG.exception when the level\nis more appropriately a warning/warn.\n\nAlso changes how a message that can not be sent is really\na critical error and should be treated as such (since such an\nerror affects the overall execution model).\n\nChange-Id: I7cebd882b655958d539be36ce3b4deb75ac4a0b7\n'}]",0,120946,28b2f8fb1bd6a618fe4f24ed9eb0d5b6ae76d12e,13,4,2,1297,,,0,"Adjust the WBE log levels

To conform better with the logging level standards
move away from using LOG.exception when the level
is more appropriately a warning/warn.

Also changes how a message that can not be sent is really
a critical error and should be treated as such (since such an
error affects the overall execution model).

Change-Id: I7cebd882b655958d539be36ce3b4deb75ac4a0b7
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/46/120946/1 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/engines/worker_based/server.py', 'taskflow/engines/worker_based/executor.py', 'taskflow/tests/unit/worker_based/test_server.py']",3,528daed882a639389d9fa63e525260f1ec15406a,," @mock.patch(""taskflow.engines.worker_based.server.LOG.critical"") @mock.patch(""taskflow.engines.worker_based.server.LOG.warn"")"," @mock.patch(""taskflow.engines.worker_based.server.LOG.exception"") @mock.patch(""taskflow.engines.worker_based.server.LOG.exception"")",28,16
openstack%2Fneutron~master~I72e5afe3d03ae223fcd8e75f1f4f07624c3a7daf,openstack/neutron,master,I72e5afe3d03ae223fcd8e75f1f4f07624c3a7daf,Use dict_extend_functions to populate provider network attributes,MERGED,2014-09-03 22:13:56.000000000,2014-09-28 22:24:28.000000000,2014-09-23 13:18:43.000000000,"[{'_account_id': 3}, {'_account_id': 490}, {'_account_id': 704}, {'_account_id': 5170}, {'_account_id': 6072}, {'_account_id': 6854}, {'_account_id': 7293}, {'_account_id': 7962}, {'_account_id': 8645}, {'_account_id': 9380}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10624}, {'_account_id': 10980}, {'_account_id': 11701}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-03 22:13:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9f9b570b92b188f475c01b1bbf2ff93008b3e303', 'message': 'Use dict_extend_functions to populate provider network attributes\n\nUse dict_extend_functions mechanism to handle populating additional\nprovider network attributes into Network model in Nuage plugin.\n\nChange-Id: I72e5afe3d03ae223fcd8e75f1f4f07624c3a7daf\nCloses-Bug: #1362308\n'}, {'number': 2, 'created': '2014-09-04 20:54:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/6dc82a5ebae75865a2051b633c0365b046a4698f', 'message': 'Use dict_extend_functions to populate provider network attributes\n\nUse dict_extend_functions mechanism to handle populating additional\nprovider network attributes into Network model in Nuage plugin.\n\nChange-Id: I72e5afe3d03ae223fcd8e75f1f4f07624c3a7daf\nCloses-Bug: #1362308\n'}, {'number': 3, 'created': '2014-09-17 19:28:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/03150e4a53cdf21069b80e502fdd932d188e0eec', 'message': 'Use dict_extend_functions to populate provider network attributes\n\nUse dict_extend_functions mechanism to handle populating additional\nprovider network attributes into Network model in Nuage plugin.\n\nChange-Id: I72e5afe3d03ae223fcd8e75f1f4f07624c3a7daf\nCloses-Bug: #1362308\n'}, {'number': 4, 'created': '2014-09-19 00:03:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/461358b3ee7bec473521993f96ffec1296374a2e', 'message': 'Use dict_extend_functions to populate provider network attributes\n\nUse dict_extend_functions mechanism to handle populating additional\nprovider network attributes into Network model in Nuage plugin.\n\nChange-Id: I72e5afe3d03ae223fcd8e75f1f4f07624c3a7daf\nCloses-Bug: #1362308\n'}, {'number': 5, 'created': '2014-09-19 05:50:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/018ce3935eb4f6551cb13b6b6006558ef50ea7a8', 'message': 'Use dict_extend_functions to populate provider network attributes\n\nUse dict_extend_functions mechanism to handle populating additional\nprovider network attributes into Network model in Nuage plugin.\n\nChange-Id: I72e5afe3d03ae223fcd8e75f1f4f07624c3a7daf\nCloses-Bug: #1362308\n'}, {'number': 6, 'created': '2014-09-19 21:24:21.000000000', 'files': ['neutron/plugins/nuage/plugin.py', 'neutron/plugins/nuage/nuage_models.py', 'neutron/plugins/nuage/nuagedb.py', 'neutron/tests/unit/nuage/test_nuage_plugin.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/271a280d0e8f86b96d24970f588184b1cfc34ef8', 'message': 'Use dict_extend_functions to populate provider network attributes\n\nUse dict_extend_functions mechanism to handle populating additional\nprovider network attributes into Network model in Nuage plugin.\n\nChange-Id: I72e5afe3d03ae223fcd8e75f1f4f07624c3a7daf\nCloses-Bug: #1362308\n'}]",10,118911,271a280d0e8f86b96d24970f588184b1cfc34ef8,185,29,6,11701,,,0,"Use dict_extend_functions to populate provider network attributes

Use dict_extend_functions mechanism to handle populating additional
provider network attributes into Network model in Nuage plugin.

Change-Id: I72e5afe3d03ae223fcd8e75f1f4f07624c3a7daf
Closes-Bug: #1362308
",git fetch https://review.opendev.org/openstack/neutron refs/changes/11/118911/6 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/nuage/nuage_models.py', 'neutron/plugins/nuage/plugin.py', 'neutron/plugins/nuage/nuagedb.py']",3,9f9b570b92b188f475c01b1bbf2ff93008b3e303,bug/1362308, return binding,,16,8
openstack%2Fneutron~master~Ibd86611ad3e7eff085d769bdff777a5870f30c58,openstack/neutron,master,Ibd86611ad3e7eff085d769bdff777a5870f30c58,Remove RPC notification from transaction in create/update port,MERGED,2014-09-19 17:52:28.000000000,2014-09-28 21:58:20.000000000,2014-09-24 14:42:05.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 748}, {'_account_id': 1689}, {'_account_id': 5170}, {'_account_id': 6659}, {'_account_id': 6788}, {'_account_id': 7448}, {'_account_id': 7787}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9695}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10386}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-19 17:52:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/591b7774548c56cd4dc71b5e3115d7d6e44bed57', 'message': ""WIP Try to remove RPC notification for DVR from create_port\n\nThis is not my best work (yet).  We need to see if moving this call to\nnotify L3 agents that a port has come up on a host out of the\ncreate_port and update_port transactions eliminates the lock wait\ntimeouts in the dvr check queue job.\n\nCloses-Bug: I haven't filed a bug for this specific problem.  Stay\ntuned.\n\nChange-Id: Ibd86611ad3e7eff085d769bdff777a5870f30c58\n""}, {'number': 2, 'created': '2014-09-22 17:49:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/6e996862d18825fa65f0dd285595e62026728875', 'message': 'Remove RPC notification from transaction in create/update port\n\nRemoving notifications to the L3 agent from within the transaction in\ncreate_port and update_port eliminates many lock wait timeouts in the\ndvr check queue job and in scale testing locally.\n\nCloses-Bug: #1371732\n\nChange-Id: Ibd86611ad3e7eff085d769bdff777a5870f30c58\n'}, {'number': 3, 'created': '2014-09-22 23:06:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/0bc3978b6506f8475ec7040f205c2eaf8aa4517c', 'message': 'Remove RPC notification from transaction in create/update port\n\nRemoving notifications to the L3 agent from within the transaction in\ncreate_port and update_port eliminates many lock wait timeouts in the\ndvr check queue job and in scale testing locally.\n\nCloses-Bug: #1371732\n\nChange-Id: Ibd86611ad3e7eff085d769bdff777a5870f30c58\n'}, {'number': 4, 'created': '2014-09-23 16:53:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/37fbb1b3e7710d8cfc449a308f7d164ceab8564a', 'message': 'Remove RPC notification from transaction in create/update port\n\nRemoving notifications to the L3 agent from within the transaction in\ncreate_port and update_port eliminates many lock wait timeouts in the\ndvr check queue job and in scale testing locally.\n\nCloses-Bug: #1371732\n\nChange-Id: Ibd86611ad3e7eff085d769bdff777a5870f30c58\n'}, {'number': 5, 'created': '2014-09-23 17:28:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/7690c9c4843a369f81124acf276ef8c153cafdfa', 'message': 'Remove RPC notification from transaction in create/update port\n\nRemoving notifications to the L3 agent from within the transaction in\ncreate_port and update_port eliminates many lock wait timeouts in the\ndvr check queue job and in scale testing locally.\n\nSince this patch leaves context unused in _process_port_binding, the\nargument is removed from the method.\n\nCloses-Bug: #1371732\n\nChange-Id: Ibd86611ad3e7eff085d769bdff777a5870f30c58\n'}, {'number': 6, 'created': '2014-09-23 19:03:55.000000000', 'files': ['neutron/tests/unit/ml2/test_ml2_plugin.py', 'neutron/plugins/ml2/plugin.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/411836b5411411a6046043e0264aaa7b6f5760f0', 'message': 'Remove RPC notification from transaction in create/update port\n\nRemoving notifications to the L3 agent from within the transaction in\ncreate_port and update_port eliminates many lock wait timeouts in the\ndvr check queue job and in scale testing locally.\n\nSince this patch leaves context unused in _process_port_binding, the\nargument is removed from the method.\n\nCloses-Bug: #1371732\n\nChange-Id: Ibd86611ad3e7eff085d769bdff777a5870f30c58\n'}]",16,122805,411836b5411411a6046043e0264aaa7b6f5760f0,151,27,6,7448,,,0,"Remove RPC notification from transaction in create/update port

Removing notifications to the L3 agent from within the transaction in
create_port and update_port eliminates many lock wait timeouts in the
dvr check queue job and in scale testing locally.

Since this patch leaves context unused in _process_port_binding, the
argument is removed from the method.

Closes-Bug: #1371732

Change-Id: Ibd86611ad3e7eff085d769bdff777a5870f30c58
",git fetch https://review.opendev.org/openstack/neutron refs/changes/05/122805/2 && git format-patch -1 --stdout FETCH_HEAD,['neutron/plugins/ml2/plugin.py'],1,591b7774548c56cd4dc71b5e3115d7d6e44bed57,remove-rpc-from-transaction," host_changed = None host_changed = port return (changes, host_changed) _, new_host_port = self._process_port_binding(mech_context, context, attrs) if new_host_port: # Whenever a DVR serviceable port comes up on a # node, it has to be communicated to the L3 Plugin # and agent for creating the respective namespaces. if (utils.is_dvr_serviced(new_host_port['device_owner'])): l3plugin = manager.NeutronManager.get_service_plugins().get( service_constants.L3_ROUTER_NAT) if (utils.is_extension_supported( l3plugin, const.L3_DISTRIBUTED_EXT_ALIAS)): l3plugin.dvr_update_router_addvm(context, new_host_port) changes, new_host_port = self._process_port_binding( need_port_update_notify |= changes if new_host_port: # Whenever a DVR serviceable port comes up on a # node, it has to be communicated to the L3 Plugin # and agent for creating the respective namespaces. if (utils.is_dvr_serviced(new_host_port['device_owner'])): l3plugin = manager.NeutronManager.get_service_plugins().get( service_constants.L3_ROUTER_NAT) if (utils.is_extension_supported( l3plugin, const.L3_DISTRIBUTED_EXT_ALIAS)): l3plugin.dvr_update_router_addvm(context, new_host_port) "," # Whenever a DVR serviceable port comes up on a # node, it has to be communicated to the L3 Plugin # and agent for creating the respective namespaces. if (utils.is_dvr_serviced(port['device_owner'])): l3plugin = manager.NeutronManager.get_service_plugins().get( service_constants.L3_ROUTER_NAT) if (utils.is_extension_supported( l3plugin, const.L3_DISTRIBUTED_EXT_ALIAS)): l3plugin.dvr_update_router_addvm(context, port) return changes self._process_port_binding(mech_context, context, attrs) need_port_update_notify |= self._process_port_binding(",29,12
openstack%2Fdevstack~master~I2c7e0833a61926b9fc9b5de4e38fdd626501d78d,openstack/devstack,master,I2c7e0833a61926b9fc9b5de4e38fdd626501d78d,Cleanup nova-cinder nova.conf section,MERGED,2014-09-26 16:52:53.000000000,2014-09-28 21:27:40.000000000,2014-09-27 20:28:22.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 5196}, {'_account_id': 9009}]","[{'number': 1, 'created': '2014-09-26 16:52:53.000000000', 'files': ['lib/nova'], 'web_link': 'https://opendev.org/openstack/devstack/commit/a7bde1fdf7df38490b80871dd652401fb1721232', 'message': ""Cleanup nova-cinder nova.conf section\n\nStop using deprecated conf names, don't override default values with\ndefault values.\n\nChange-Id: I2c7e0833a61926b9fc9b5de4e38fdd626501d78d\n""}]",0,124464,a7bde1fdf7df38490b80871dd652401fb1721232,8,4,1,1849,,,0,"Cleanup nova-cinder nova.conf section

Stop using deprecated conf names, don't override default values with
default values.

Change-Id: I2c7e0833a61926b9fc9b5de4e38fdd626501d78d
",git fetch https://review.opendev.org/openstack/devstack refs/changes/64/124464/1 && git format-patch -1 --stdout FETCH_HEAD,['lib/nova'],1,a7bde1fdf7df38490b80871dd652401fb1721232,cinder-nova," iniset $NOVA_CONF cinder endpoint_template ""https://$CINDER_SERVICE_HOST:$CINDER_SERVICE_PORT/v1/%(project_id)s"" iniset $NOVA_CONF cinder ca_certificates_file $SSL_BUNDLE_FILE"," iniset $NOVA_CONF DEFAULT volume_api_class ""nova.volume.cinder.API"" iniset $NOVA_CONF DEFAULT cinder_endpoint_template ""https://$CINDER_SERVICE_HOST:$CINDER_SERVICE_PORT/v1/%(project_id)s"" iniset $NOVA_CONF DEFAULT cinder_ca_certificates_file $SSL_BUNDLE_FILE",2,3
openstack%2Fdevstack~master~Ib08f3e20e4685b331385431276f890205fa76da6,openstack/devstack,master,Ib08f3e20e4685b331385431276f890205fa76da6,Stop setting nova.conf settings that mean nothing,MERGED,2014-09-26 17:01:23.000000000,2014-09-28 21:22:43.000000000,2014-09-28 01:22:33.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 2750}, {'_account_id': 7715}, {'_account_id': 9009}]","[{'number': 1, 'created': '2014-09-26 17:01:23.000000000', 'files': ['lib/nova'], 'web_link': 'https://opendev.org/openstack/devstack/commit/1f79bad7ecc28d472b1e2c185fdba7a9dd61a524', 'message': ""Stop setting nova.conf settings that mean nothing\n\n* DEFAULT.fixed_range isn't a valid option in nova anymore\n* DEFAULT.osci_compute_workers was never a thing, it should be\n  DEFAULT.osapi_compute_workers\n\nChange-Id: Ib08f3e20e4685b331385431276f890205fa76da6\n""}]",0,124465,1f79bad7ecc28d472b1e2c185fdba7a9dd61a524,9,5,1,1849,,,0,"Stop setting nova.conf settings that mean nothing

* DEFAULT.fixed_range isn't a valid option in nova anymore
* DEFAULT.osci_compute_workers was never a thing, it should be
  DEFAULT.osapi_compute_workers

Change-Id: Ib08f3e20e4685b331385431276f890205fa76da6
",git fetch https://review.opendev.org/openstack/devstack refs/changes/65/124465/1 && git format-patch -1 --stdout FETCH_HEAD,['lib/nova'],1,1f79bad7ecc28d472b1e2c185fdba7a9dd61a524,novaconf," iniset $NOVA_CONF DEFAULT osapi_compute_workers ""$API_WORKERS"""," iniset $NOVA_CONF DEFAULT fixed_range """" iniset $NOVA_CONF DEFAULT osci_compute_workers ""$API_WORKERS""",1,2
openstack%2Ftaskflow~master~Iaf1c101df9c268da48db7432bcbc0467f6486bcd,openstack/taskflow,master,Iaf1c101df9c268da48db7432bcbc0467f6486bcd,Remove db locks and use random db names for tests,MERGED,2014-08-03 04:14:35.000000000,2014-09-28 20:32:24.000000000,2014-09-28 20:32:23.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 7491}, {'_account_id': 8871}, {'_account_id': 9608}, {'_account_id': 11913}]","[{'number': 1, 'created': '2014-08-03 04:14:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/27b544cab518ca653ed2d4313c6df0694d716c39', 'message': ""Remove db locks and use random db names for tests\n\nRemoved database locks against the database openstack_citest and\nimplemented random name generation for new databases created just\nfor persistence testing.\nOnce the locks were removed, using 'template1' as the initial db\nfor postgres connections was problematic, because postgres refuses\nto create databases when there are multiple connections to\n'template1'.  Switched to using 'postgres' as an initial db to use.\nChanged _reset_database to _init_db since we are always working\nwith a brand new database, and removed the 'drop database' before\na 'create database.\nAdded a _remove_db method to remove the database once testing was\nfinished.\n\nChange-Id: Iaf1c101df9c268da48db7432bcbc0467f6486bcd\nCloses-Bug: 1327469\n""}, {'number': 2, 'created': '2014-08-12 18:33:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/c70b6884a85876a00ceb65ad448163cdfe4059a7', 'message': ""Remove db locks and use random db names for tests\n\nRemoved database locks against the database openstack_citest and\nimplemented random name generation for new databases created just\nfor persistence testing.\nOnce the locks were removed, using 'template1' as the initial db\nfor postgres connections was problematic, because postgres refuses\nto create databases when there are multiple connections to\n'template1'.  Switched to using 'postgres' as an initial db to use.\nChanged _reset_database to _init_db since we are always working\nwith a brand new database, and removed the 'drop database' before\na 'create database.\nAdded a _remove_db method to remove the database once testing was\nfinished.\n\nChange-Id: Iaf1c101df9c268da48db7432bcbc0467f6486bcd\nCloses-Bug: 1327469\n""}, {'number': 3, 'created': '2014-08-12 18:40:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/f1f3b4f0467d8ded4d05bba94bf93d6632dfcb09', 'message': ""Remove db locks and use random db names for tests\n\nRemoved database locks against the database openstack_citest and\nimplemented random name generation for new databases created just\nfor persistence testing.\nOnce the locks were removed, using 'template1' as the initial db\nfor postgres connections was problematic, because postgres refuses\nto create databases when there are multiple connections to\n'template1'.  Switched to using 'postgres' as an initial db to use.\nChanged _reset_database to _init_db since we are always working\nwith a brand new database, and removed the 'drop database' before\na 'create database.\nAdded a _remove_db method to remove the database once testing was\nfinished.\n\nChange-Id: Iaf1c101df9c268da48db7432bcbc0467f6486bcd\nCloses-Bug: 1327469\n""}, {'number': 4, 'created': '2014-08-12 20:23:42.000000000', 'files': ['taskflow/tests/unit/persistence/test_sql_persistence.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/98be1df5d1a1a10551e323a0fc24c6367bf97318', 'message': ""Remove db locks and use random db names for tests\n\nRemoved database locks against the database openstack_citest and\nimplemented random name generation for new databases created just\nfor persistence testing.\nOnce the locks were removed, using 'template1' as the initial db\nfor postgres connections was problematic, because postgres refuses\nto create databases when there are multiple connections to\n'template1'.  Switched to using 'postgres' as an initial db to use.\nChanged _reset_database to _init_db since we are always working\nwith a brand new database, and removed the 'drop database' before\na 'create database.\nAdded a _remove_db method to remove the database once testing was\nfinished.\n\nChange-Id: Iaf1c101df9c268da48db7432bcbc0467f6486bcd\nCloses-Bug: 1327469\n""}]",6,111543,98be1df5d1a1a10551e323a0fc24c6367bf97318,27,6,4,11913,,,0,"Remove db locks and use random db names for tests

Removed database locks against the database openstack_citest and
implemented random name generation for new databases created just
for persistence testing.
Once the locks were removed, using 'template1' as the initial db
for postgres connections was problematic, because postgres refuses
to create databases when there are multiple connections to
'template1'.  Switched to using 'postgres' as an initial db to use.
Changed _reset_database to _init_db since we are always working
with a brand new database, and removed the 'drop database' before
a 'create database.
Added a _remove_db method to remove the database once testing was
finished.

Change-Id: Iaf1c101df9c268da48db7432bcbc0467f6486bcd
Closes-Bug: 1327469
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/43/111543/4 && git format-patch -1 --stdout FETCH_HEAD,['taskflow/tests/unit/persistence/test_sql_persistence.py'],1,27b544cab518ca653ed2d4313c6df0694d716c39,bug/1327469,"import random# you need to set up a db user 'openstack_citest' with password # 'openstack_citest' that has the permissions to create databases on # localhost.DATABASE = ""tftest_"" + ''.join(random.choice('0123456789') for _ in range(12)) db_uri = _get_connect_string('postgres', USER, PASSWD, 'postgres') def _init_db(self): """"""Sets up the database, and returns the uri to that database."""""" raise NotImplementedError() def _remove_db(self): """"""Cleans up by removing the database once the tests are done."""""" try: self.db_uri = self._init_db() # Since we are using random database names, we need to make sure # and remove our random database when we are done testing. self.addCleanup(self._remove_db) conf = { 'connection': self.db_uri def _init_db(self): def _remove_db(self): engine = None try: engine = sa.create_engine(self.db_uri) with contextlib.closing(engine.connect()) as conn: conn.execute(""DROP DATABASE IF EXISTS %s"" % DATABASE) except Exception: pass finally: if engine is not None: try: engine.dispose() except Exception: pass def _init_db(self): # why we connect to the database 'postgres' and then create the # desired database. database='postgres') return _get_connect_string('postgres', USER, PASSWD, database=DATABASE) def _remove_db(self): engine = None try: # Postgres can't operate on the database it's connected to, that's # why we connect to the 'postgres' database and then drop the # database. db_uri = _get_connect_string('postgres', USER, PASSWD, database='postgres') engine = sa.create_engine(db_uri) with contextlib.closing(engine.connect()) as conn: conn.connection.set_isolation_level(0) conn.execute(""DROP DATABASE IF EXISTS %s"" % DATABASE) conn.connection.set_isolation_level(1) finally: if engine is not None: try: engine.dispose() except Exception: pass","import threading# you need to set up a db named 'openstack_citest' with user 'openstack_citest' # and password 'openstack_citest' on localhost.DATABASE = ""openstack_citest""from taskflow.utils import lock_utils db_uri = _get_connect_string('postgres', USER, PASSWD, 'template1') LOCK_NAME = None def _reset_database(self): """"""Resets the database, and returns the uri to that database. Called *only* after locking succeeds. """""" self.big_lock.acquire() self.addCleanup(self.big_lock.release) try: conf = { 'connection': self._reset_database(), LOCK_NAME = 'mysql_persistence_test' # We need to make sure that each test goes through a set of locks # to ensure that multiple tests are not modifying the database, # dropping it, creating it at the same time. To accomplish this we use # a lock that ensures multiple parallel processes can't run at the # same time as well as a in-process lock to ensure that multiple # threads can't run at the same time. lock_path = os.path.join(tempfile.gettempdir(), 'taskflow-%s.lock' % (self.LOCK_NAME)) locks = [ lock_utils.InterProcessLock(lock_path), threading.RLock(), ] self.big_lock = lock_utils.MultiLock(locks) def _reset_database(self): conn.execute(""DROP DATABASE IF EXISTS %s"" % DATABASE) LOCK_NAME = 'postgres_persistence_test' # We need to make sure that each test goes through a set of locks # to ensure that multiple tests are not modifying the database, # dropping it, creating it at the same time. To accomplish this we use # a lock that ensures multiple parallel processes can't run at the # same time as well as a in-process lock to ensure that multiple # threads can't run at the same time. lock_path = os.path.join(tempfile.gettempdir(), 'taskflow-%s.lock' % (self.LOCK_NAME)) locks = [ lock_utils.InterProcessLock(lock_path), threading.RLock(), ] self.big_lock = lock_utils.MultiLock(locks) def _reset_database(self): # why we connect to the default template database 'template1' and # then drop and create the desired database. database='template1') conn.execute(""DROP DATABASE IF EXISTS %s"" % DATABASE) conn.connection.set_isolation_level(1) with contextlib.closing(engine.connect()) as conn: conn.connection.set_isolation_level(0) return _get_connect_string('postgres', USER, PASSWD, database=DATABASE)",59,53
openstack%2Fnova~master~Ib184ffa042aa85b52eede765d1eb94ede96f36fa,openstack/nova,master,Ib184ffa042aa85b52eede765d1eb94ede96f36fa,Updated from global requirements,MERGED,2014-09-26 03:57:29.000000000,2014-09-28 20:32:10.000000000,2014-09-28 20:32:08.000000000,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-26 03:57:29.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/nova/commit/9f549ecd562e92fc5525e84bfff9a558d9ffe8da', 'message': 'Updated from global requirements\n\nChange-Id: Ib184ffa042aa85b52eede765d1eb94ede96f36fa\n'}]",0,124260,9f549ecd562e92fc5525e84bfff9a558d9ffe8da,11,5,1,11131,,,0,"Updated from global requirements

Change-Id: Ib184ffa042aa85b52eede765d1eb94ede96f36fa
",git fetch https://review.opendev.org/openstack/nova refs/changes/60/124260/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,9f549ecd562e92fc5525e84bfff9a558d9ffe8da,openstack/requirements,kombu>=2.5.0,kombu>=2.4.8,1,1
openstack%2Fneutron~master~I7e0f80e456ff4d9eb57a1d31c6ffc7cdfca5a163,openstack/neutron,master,I7e0f80e456ff4d9eb57a1d31c6ffc7cdfca5a163,"Revert ""Cleanup floatingips also on router delete""",MERGED,2014-09-16 01:41:34.000000000,2014-09-28 20:31:29.000000000,2014-09-27 17:54:59.000000000,"[{'_account_id': 3}, {'_account_id': 261}, {'_account_id': 748}, {'_account_id': 841}, {'_account_id': 4395}, {'_account_id': 5170}, {'_account_id': 7448}, {'_account_id': 9077}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-16 01:41:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/69ca24e7abc28582b85eee1edeb17b5706e1f307', 'message': 'Revert ""Cleanup floatingips also on router delete""\n\nThis reverts commit c3326996e38cb67f8d4ba3dabd829dc6f327b666.\n\nClutching at straws to see if we can root cause\nsome FIP related failures.\n\nChange-Id: I7e0f80e456ff4d9eb57a1d31c6ffc7cdfca5a163\n'}, {'number': 2, 'created': '2014-09-18 00:14:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/a0180e77cdf47412e052842871be31b8af462249', 'message': 'Revert ""Cleanup floatingips also on router delete""\n\nThis reverts commit c3326996e38cb67f8d4ba3dabd829dc6f327b666.\n\nClutching at straws to see if we can root cause\nsome FIP related failures.\n\nChange-Id: I7e0f80e456ff4d9eb57a1d31c6ffc7cdfca5a163\n'}, {'number': 3, 'created': '2014-09-19 16:31:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/5cb43d8ca8469d2388c8530dea11079759e76a45', 'message': 'Revert ""Cleanup floatingips also on router delete""\n\nThis reverts commit c3326996e38cb67f8d4ba3dabd829dc6f327b666.\n\nClutching at straws to see if we can root cause\nsome FIP related failures.\n\nChange-Id: I7e0f80e456ff4d9eb57a1d31c6ffc7cdfca5a163\n'}, {'number': 4, 'created': '2014-09-23 14:07:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/f13f90a0e3a38e59f15583025f8075779d8c9163', 'message': 'Revert ""Cleanup floatingips also on router delete""\n\nThis reverts commit c3326996e38cb67f8d4ba3dabd829dc6f327b666.\n\nClutching at straws to see if we can root cause\nsome FIP related failures.\n\nChange-Id: I7e0f80e456ff4d9eb57a1d31c6ffc7cdfca5a163\n'}, {'number': 5, 'created': '2014-09-24 14:20:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/1043a35dedec22b09ee000c737ae9f2c018e071f', 'message': 'Revert ""Cleanup floatingips also on router delete""\n\nThis reverts commit c3326996e38cb67f8d4ba3dabd829dc6f327b666.\n\nClutching at straws to see if we can root cause\nsome FIP related failures.\n\nChange-Id: I7e0f80e456ff4d9eb57a1d31c6ffc7cdfca5a163\n'}, {'number': 6, 'created': '2014-09-25 19:17:46.000000000', 'files': ['neutron/agent/l3_agent.py', 'neutron/tests/unit/test_l3_agent.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/45a523681f2136f8fefb6c3da44540decd6a0fda', 'message': 'Revert ""Cleanup floatingips also on router delete""\n\nThis reverts commit c3326996e38cb67f8d4ba3dabd829dc6f327b666.\n\nThe patch being reverted here addresses an issue that can no longer be\nreproduced, in that under no circumstances, I can make the FIP lie around\nbefore deleting a router (which can only be done after all FIP have been\ndisassociated or released).\n\nUnless we have more clarity as to what the initial commit was really meant\nto fix, there is a strong case for reverting this patch at this point.\n\nCloses-bug: #1373100\n\nChange-Id: I7e0f80e456ff4d9eb57a1d31c6ffc7cdfca5a163\n'}]",3,121729,45a523681f2136f8fefb6c3da44540decd6a0fda,196,25,6,748,,,0,"Revert ""Cleanup floatingips also on router delete""

This reverts commit c3326996e38cb67f8d4ba3dabd829dc6f327b666.

The patch being reverted here addresses an issue that can no longer be
reproduced, in that under no circumstances, I can make the FIP lie around
before deleting a router (which can only be done after all FIP have been
disassociated or released).

Unless we have more clarity as to what the initial commit was really meant
to fix, there is a strong case for reverting this patch at this point.

Closes-bug: #1373100

Change-Id: I7e0f80e456ff4d9eb57a1d31c6ffc7cdfca5a163
",git fetch https://review.opendev.org/openstack/neutron refs/changes/29/121729/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/agent/l3_agent.py', 'neutron/tests/unit/test_l3_agent.py']",2,69ca24e7abc28582b85eee1edeb17b5706e1f307,bug/1373100,," ex_gw_port = agent._get_ex_gw_port(ri) agent.process_router_floating_ip_addresses.assert_called_with( ri, ex_gw_port) agent.process_router_floating_ip_addresses.reset_mock() agent.process_router_floating_ip_nat_rules.assert_called_with(ri) agent.process_router_floating_ip_nat_rules.reset_mock() # now no ports no gateway, test state tear down ri.ex_gw_port = None agent.process_router(ri)",2,12
openstack%2Fneutron~master~I261d3e230eced9ea514b35cc3f5f8be04f84c751,openstack/neutron,master,I261d3e230eced9ea514b35cc3f5f8be04f84c751,Add logging for enforced policy rules,MERGED,2014-08-26 15:34:36.000000000,2014-09-28 19:45:15.000000000,2014-09-24 18:26:38.000000000,"[{'_account_id': 3}, {'_account_id': 261}, {'_account_id': 490}, {'_account_id': 841}, {'_account_id': 5170}, {'_account_id': 6072}, {'_account_id': 6659}, {'_account_id': 7293}, {'_account_id': 7787}, {'_account_id': 8213}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10917}, {'_account_id': 10980}, {'_account_id': 12040}, {'_account_id': 12444}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-08-26 15:34:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/2a5e2dfa0e23909e60575578a85f71d413e08071', 'message': 'Add logging for enforced policy rules\n\nThere are a lot of policy rules which should not necessarily\nbe explicitly specified in policy.json to be checked while enforcement.\nThere should be a way for an operator to know which policy rules are\nactually being enforced for each action.\n\nChange-Id: I261d3e230eced9ea514b35cc3f5f8be04f84c751\nRelared-Bug: 1356679\n'}, {'number': 2, 'created': '2014-08-26 15:35:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/23c5c18ddf355dada85047d67c3a0c438d3e2985', 'message': 'Add logging for enforced policy rules\n\nThere are a lot of policy rules which should not necessarily\nbe explicitly specified in policy.json to be checked while enforcement.\nThere should be a way for an operator to know which policy rules are\nactually being enforced for each action.\n\nChange-Id: I261d3e230eced9ea514b35cc3f5f8be04f84c751\nRelated-Bug: 1356679\n'}, {'number': 3, 'created': '2014-09-15 08:15:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/cfd3c5378353f4827b9ca0da9d30ccf0cf891a2c', 'message': 'Add logging for enforced policy rules\n\nThere are a lot of policy rules which should not necessarily\nbe explicitly specified in policy.json to be checked while enforcement.\nThere should be a way for an operator to know which policy rules are\nactually being enforced for each action.\n\nChange-Id: I261d3e230eced9ea514b35cc3f5f8be04f84c751\nCloses-Bug: #1356679\n'}, {'number': 4, 'created': '2014-09-16 05:19:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/73caf5707692308de0e1c79ecda5e34596e5fb63', 'message': 'Add logging for enforced policy rules\n\nThere are a lot of policy rules which should not necessarily\nbe explicitly specified in policy.json to be checked while enforcement.\nThere should be a way for an operator to know which policy rules are\nactually being enforced for each action.\n\nAdded a unit test.\n\nChange-Id: I261d3e230eced9ea514b35cc3f5f8be04f84c751\nCloses-Bug: #1356679\n'}, {'number': 5, 'created': '2014-09-17 06:35:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/7f8e9ab4530ad50f62dd7caaa45aeca74b8c92a6', 'message': 'Add logging for enforced policy rules\n\nThere are a lot of policy rules which should not necessarily\nbe explicitly specified in policy.json to be checked while enforcement.\nThere should be a way for an operator to know which policy rules are\nactually being enforced for each action.\n\nAdded a unit test.\n\nChange-Id: I261d3e230eced9ea514b35cc3f5f8be04f84c751\nCloses-Bug: #1356679\n'}, {'number': 6, 'created': '2014-09-19 16:05:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e81debf953b0130715c5ac0d4c40cb53aa336e7c', 'message': 'Add logging for enforced policy rules\n\nThere are a lot of policy rules which should not necessarily\nbe explicitly specified in policy.json to be checked while enforcement.\nThere should be a way for an operator to know which policy rules are\nactually being enforced for each action.\n\nAdded a unit test.\n\nChange-Id: I261d3e230eced9ea514b35cc3f5f8be04f84c751\nCloses-Bug: #1356679\n'}, {'number': 7, 'created': '2014-09-22 07:46:10.000000000', 'files': ['neutron/tests/unit/test_policy.py', 'neutron/policy.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/0cc7444f7577e53f14068a2cb35431713d6d21a0', 'message': 'Add logging for enforced policy rules\n\nThere are a lot of policy rules which should not necessarily\nbe explicitly specified in policy.json to be checked while enforcement.\nThere should be a way for an operator to know which policy rules are\nactually being enforced for each action.\n\nAdded a unit test.\n\nChange-Id: I261d3e230eced9ea514b35cc3f5f8be04f84c751\nCloses-Bug: #1356679\n'}]",25,116925,0cc7444f7577e53f14068a2cb35431713d6d21a0,177,32,7,7293,,,0,"Add logging for enforced policy rules

There are a lot of policy rules which should not necessarily
be explicitly specified in policy.json to be checked while enforcement.
There should be a way for an operator to know which policy rules are
actually being enforced for each action.

Added a unit test.

Change-Id: I261d3e230eced9ea514b35cc3f5f8be04f84c751
Closes-Bug: #1356679
",git fetch https://review.opendev.org/openstack/neutron refs/changes/25/116925/4 && git format-patch -1 --stdout FETCH_HEAD,['neutron/policy.py'],1,2a5e2dfa0e23909e60575578a85f71d413e08071,log_policy,"def _process_rules_list(rule_list, m_rule): """"""Recursively walk a policy rule to extract a list of match entries."""""" if isinstance(m_rule, policy.RuleCheck): rule_list.append(m_rule.match) elif isinstance(m_rule, policy.AndCheck): for rule in m_rule.rules: _process_rules_list(rule_list, rule) return rule_list rules = [] LOG.debug(""The following rules will now be enforced: %s"", _process_rules_list(rules, match_rule))",,13,0
openstack%2Fneutron~master~I6537bb1da5ef0d6899bc71e4e949f2c760c103c2,openstack/neutron,master,I6537bb1da5ef0d6899bc71e4e949f2c760c103c2,Forbid regular users to reset admin-only attrs to default values,MERGED,2014-08-15 13:35:38.000000000,2014-09-28 18:20:55.000000000,2014-09-25 03:58:57.000000000,"[{'_account_id': 3}, {'_account_id': 261}, {'_account_id': 490}, {'_account_id': 841}, {'_account_id': 5170}, {'_account_id': 5948}, {'_account_id': 6072}, {'_account_id': 6659}, {'_account_id': 6854}, {'_account_id': 7293}, {'_account_id': 7473}, {'_account_id': 8124}, {'_account_id': 8645}, {'_account_id': 8767}, {'_account_id': 9093}, {'_account_id': 9656}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9751}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-08-15 13:35:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8b7b44b7aa8522bde764831ddbf8a23dc43fcf00', 'message': 'WIP: policy checks for update\n\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n'}, {'number': 2, 'created': '2014-08-15 16:16:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/6ab7b298acd170dd29d47b29d2ca5626377fa7cb', 'message': 'WIP: policy checks for update\n\nRelated-Bug: 1357379\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n'}, {'number': 3, 'created': '2014-08-18 10:58:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9fb109f4baa60d61aef07d4f968ec50e4b79a7f3', 'message': 'WIP: policy checks for update\n\nRelated-Bug: 1357379\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n'}, {'number': 4, 'created': '2014-08-18 14:51:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e0fd411642e40e0a4a7864ed564532e9f89fe37c', 'message': 'WIP: policy checks for update\n\nRelated-Bug: 1357379\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n'}, {'number': 5, 'created': '2014-08-19 11:55:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8cf70570710fe5d946849eba579bc592ac5a01e3', 'message': 'WIP: policy checks for update\n\nRelated-Bug: 1357379\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n'}, {'number': 6, 'created': '2014-08-19 12:39:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8f398bdbcdda96680aa6e10bc6420ff2f8a12697', 'message': 'WIP: policy checks for update\n\nRelated-Bug: 1357379\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n'}, {'number': 7, 'created': '2014-08-19 15:49:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/09f6112c273ccebf336652e805b4301946921997', 'message': 'Forbid regular users to reset admin-only attrs to default values\n\nA regular user can reset an admin-only attribute to its default\nvalue due to the fact that a corresponding policy rule is\nenforced only in case when an attribute is present in the\ntarget AND has a non-default value.\n\nAdded a new attribute ""updated"" which contains a list of all\nto-be updated attributes to the body of the target that is\npassed to policy.enforce.\n\nChanged a check for whether an attribute is explicitly set.\nNow in case of update the function should not pay attention\nto a default value of an attribute, but check whether it was\nexplicitly marked as being updated.\n\nAdded unit-tests.\n\nCloses-Bug: 1357379\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n'}, {'number': 8, 'created': '2014-08-20 09:57:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/f1a1047756e5db2646eee4ca16c9e2092cba091d', 'message': 'Forbid regular users to reset admin-only attrs to default values\n\nA regular user can reset an admin-only attribute to its default\nvalue due to the fact that a corresponding policy rule is\nenforced only in case when an attribute is present in the\ntarget AND has a non-default value.\n\nAdded a new attribute ""updated"" which contains a list of all\nto-be updated attributes to the body of the target that is\npassed to policy.enforce.\n\nChanged a check for whether an attribute is explicitly set.\nNow in case of update the function should not pay attention\nto a default value of an attribute, but check whether it was\nexplicitly marked as being updated.\n\nAdded unit-tests.\n\nCloses-Bug: 1357379\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n'}, {'number': 9, 'created': '2014-08-21 11:10:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ebff0efe0c2e05969c3bb0f8ef77585762ff2840', 'message': 'Forbid regular users to reset admin-only attrs to default values\n\nA regular user can reset an admin-only attribute to its default\nvalue due to the fact that a corresponding policy rule is\nenforced only in the case when an attribute is present in the\ntarget AND has a non-default value.\n\nAdded a new attribute ""updated"" which contains a list of all\nto-be updated attributes to the body of the target that is\npassed to policy.enforce.\n\nChanged a check for whether an attribute is explicitly set.\nNow, in the case of update, the function should not pay attention\nto a default value of an attribute, but check whether it was\nexplicitly marked as being updated.\n\nAdded unit-tests.\n\nCloses-Bug: 1357379\nRelated-Bug: 1338880\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n'}, {'number': 10, 'created': '2014-09-15 06:10:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/5e41b227667a3f07e1f5f987d984f56d33f64145', 'message': 'Forbid regular users to reset admin-only attrs to default values\n\nA regular user can reset an admin-only attribute to its default\nvalue due to the fact that a corresponding policy rule is\nenforced only in the case when an attribute is present in the\ntarget AND has a non-default value.\n\nAdded a new attribute ""marked_for_update"" which contains a list\nof all to-be updated attributes to the body of the target that is\npassed to policy.enforce.\n\nChanged a check for whether an attribute is explicitly set.\nNow, in the case of update, the function should not pay attention\nto a default value of an attribute, but check whether it was\nexplicitly marked as being updated.\n\nAdded unit-tests.\n\nCloses-Bug: #1357379\nRelated-Bug: #1338880\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n'}, {'number': 11, 'created': '2014-09-19 15:44:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/018b0e63aba36d83d6d0c649786432aed4712776', 'message': 'Forbid regular users to reset admin-only attrs to default values\n\nA regular user can reset an admin-only attribute to its default\nvalue due to the fact that a corresponding policy rule is\nenforced only in the case when an attribute is present in the\ntarget AND has a non-default value.\n\nAdded a new attribute ""attributes_to_update"" which contains a list\nof all to-be updated attributes to the body of the target that is\npassed to policy.enforce.\n\nChanged a check for whether an attribute is explicitly set.\nNow, in the case of update, the function should not pay attention\nto a default value of an attribute, but check whether it was\nexplicitly marked as being updated.\n\nAdded unit-tests.\n\nCloses-Bug: #1357379\nRelated-Bug: #1338880\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n'}, {'number': 12, 'created': '2014-09-23 11:19:56.000000000', 'files': ['neutron/common/constants.py', 'neutron/api/v2/base.py', 'neutron/tests/unit/test_policy.py', 'neutron/tests/unit/test_api_v2.py', 'neutron/policy.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/74d10939903984d5f06c1749a8707fa3257e44ff', 'message': 'Forbid regular users to reset admin-only attrs to default values\n\nA regular user can reset an admin-only attribute to its default\nvalue due to the fact that a corresponding policy rule is\nenforced only in the case when an attribute is present in the\ntarget AND has a non-default value.\n\nAdded a new attribute ""attributes_to_update"" which contains a list\nof all to-be updated attributes to the body of the target that is\npassed to policy.enforce.\n\nChanged a check for whether an attribute is explicitly set.\nNow, in the case of update, the function should not pay attention\nto a default value of an attribute, but check whether it was\nexplicitly marked as being updated.\n\nAdded unit-tests.\n\nCloses-Bug: #1357379\nRelated-Bug: #1338880\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n'}]",35,114531,74d10939903984d5f06c1749a8707fa3257e44ff,286,37,12,7293,,,0,"Forbid regular users to reset admin-only attrs to default values

A regular user can reset an admin-only attribute to its default
value due to the fact that a corresponding policy rule is
enforced only in the case when an attribute is present in the
target AND has a non-default value.

Added a new attribute ""attributes_to_update"" which contains a list
of all to-be updated attributes to the body of the target that is
passed to policy.enforce.

Changed a check for whether an attribute is explicitly set.
Now, in the case of update, the function should not pay attention
to a default value of an attribute, but check whether it was
explicitly marked as being updated.

Added unit-tests.

Closes-Bug: #1357379
Related-Bug: #1338880
Change-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2
",git fetch https://review.opendev.org/openstack/neutron refs/changes/31/114531/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/api/v2/base.py', 'neutron/policy.py']",2,8b7b44b7aa8522bde764831ddbf8a23dc43fcf00,bug/1357379,"def _is_attribute_explicitly_set(attribute_name, resource, target, action): if 'update' in action: return (attribute_name in target and target[attribute_name] is not attributes.ATTR_NOT_SPECIFIED) target, action):","def _is_attribute_explicitly_set(attribute_name, resource, target): target):",13,3
openstack%2Fpython-barbicanclient~master~Id34ac86cfae3f155e97760f14d8cf256722c7d6f,openstack/python-barbicanclient,master,Id34ac86cfae3f155e97760f14d8cf256722c7d6f,Add Containers to python-barbicanclient,MERGED,2014-08-11 22:35:26.000000000,2014-09-28 17:45:40.000000000,2014-09-28 17:45:40.000000000,"[{'_account_id': 3}, {'_account_id': 7262}, {'_account_id': 7789}, {'_account_id': 7973}, {'_account_id': 8004}, {'_account_id': 9234}, {'_account_id': 10273}, {'_account_id': 10873}]","[{'number': 1, 'created': '2014-08-11 22:35:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/9fb12dc3a8e9396bd633b9c2ff5317ae7c9d16bf', 'message': 'Add Containers to python-barbicanclient\n\nFirst draft of adding Containers -- this operates\nless like the BP spec, and more like the existing\nclient functions for secret/order/verification.\nThe next patchset will be a rework to operate\nthe way the BP specifies. Keeping this version\nas patchset 1 for posterity.\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 2, 'created': '2014-08-11 23:52:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/76b89e2a366e8dd4d1becfd976ea6586735631cb', 'message': 'Add Containers to python-barbicanclient\n\nFirst draft of adding Containers -- this operates\nless like the BP spec, and more like the existing\nclient functions for secret/order/verification.\nThe next patchset will be a rework to operate\nthe way the BP specifies. Keeping this version\nas a patchset for posterity.\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 3, 'created': '2014-08-21 02:09:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/f92708226fc699cbb602b6f06490fd78bf7f4a5d', 'message': 'Add Containers to python-barbicanclient\n\nWork in progress. Still needs tests. Relies on unmerged code\nin Barbican master.\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 4, 'created': '2014-08-21 23:05:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/7462e1f3bf07b40455dd9e304ef4add988a6b4cf', 'message': 'Add Containers to python-barbicanclient\n\nWork in progress. Still needs tests. Relies on unmerged code\nin Barbican master.\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 5, 'created': '2014-08-22 23:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/7a21a4076e6df57e4b3ba6f852de327fae441f8b', 'message': 'Add Containers to python-barbicanclient\n\nWork in progress. Still needs tests. Relies on unmerged code\nin Barbican master.\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 6, 'created': '2014-08-26 17:22:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/27ca0de01e4f1cb637d052d8e8e136587ce04992', 'message': 'Add Containers to python-barbicanclient\n\nWork in progress. Still needs tests. Relies on unmerged code\nin Barbican master.\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 7, 'created': '2014-08-27 00:01:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/8710079ccc1af46e2c8dca3003710c2876adae7b', 'message': 'Add Containers to python-barbicanclient\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 8, 'created': '2014-08-27 21:04:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/27add1ed0d3aeb6ab96904748590ea89d9245b99', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 9, 'created': '2014-08-27 22:42:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/fc7a1d09ebc9985f1790c9c8dbad3dd969590dcc', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 10, 'created': '2014-08-27 22:46:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/b5186e575cca1b5a7d749bf67bed484c6bffec40', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 11, 'created': '2014-08-28 03:16:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/096462c256de00b8ac506a38f00c4dd54c8b2a53', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 12, 'created': '2014-08-28 03:19:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/de2a77feea1d9ea94450b4a657d232b9eea61492', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 13, 'created': '2014-09-04 18:47:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/73f071351860ee9a0cc6c7e9b0431f4f7effb54d', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 14, 'created': '2014-09-10 15:42:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/2da3c3d3828262c875bc45e1ab937c3691bab038', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 15, 'created': '2014-09-11 23:06:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/82f777ecbf0874cb5fc83c0c85f51e21fb10d594', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 16, 'created': '2014-09-16 20:03:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/942773ca22ae09de755dd671d443861b14c46870', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 17, 'created': '2014-09-17 20:49:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/d7c61b2cd98ba8109ef61cba9e7e57996d48f0c7', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 18, 'created': '2014-09-17 20:57:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/04fb5880ceea29109c697ebf35267e7d67211847', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 19, 'created': '2014-09-19 21:35:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/20b8b337b0e7dc14c8db2a13f1dbe38ed07d7845', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 20, 'created': '2014-09-23 00:26:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/cb74741b9f206598b6f68018415b6c898a4d256a', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 21, 'created': '2014-09-23 15:39:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/93994a804b989b2fd7b3e6b68f15a624b74bef17', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 22, 'created': '2014-09-23 21:52:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/d7b4654d82796c2a47fbda06ea25e7af5d2ca6b8', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 23, 'created': '2014-09-24 21:38:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/85ea711c0a464a7c04a51e9e33dca65a1f38e0a9', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}, {'number': 24, 'created': '2014-09-26 22:08:55.000000000', 'files': ['barbicanclient/barbican_cli/containers.py', 'barbicanclient/base.py', 'barbicanclient/client.py', 'barbicanclient/containers.py', 'setup.cfg', 'barbicanclient/test/test_client_containers.py', 'barbicanclient/barbican.py', 'barbicanclient/test/test_client.py'], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/87ca6d750ac9c65b0823571bc06bba4d5b16ae90', 'message': 'Add Containers to python-barbicanclient\n\nAdding Container support to the client and CLI. Also adding Consumer\nRegistration support at the same time (though not in the CLI).\n\nChange-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f\nImplements: blueprint client-add-containers\n'}]",176,113393,87ca6d750ac9c65b0823571bc06bba4d5b16ae90,118,8,24,10273,,,0,"Add Containers to python-barbicanclient

Adding Container support to the client and CLI. Also adding Consumer
Registration support at the same time (though not in the CLI).

Change-Id: Id34ac86cfae3f155e97760f14d8cf256722c7d6f
Implements: blueprint client-add-containers
",git fetch https://review.opendev.org/openstack/python-barbicanclient refs/changes/93/113393/22 && git format-patch -1 --stdout FETCH_HEAD,"['barbicanclient/client.py', 'barbicanclient/containers.py', 'barbicanclient/barbican.py']",3,9fb12dc3a8e9396bd633b9c2ff5317ae7c9d16bf,bp/client-add-containers," choices=['order', 'secret', 'container', 'verification'], help='the algorithm mode to be used with ' 'the requested key (default: %(default)s).') create_parser.add_argument('--type', default='generic', help='type of container to create') create_parser.add_argument('--secret', '-s', action='append', help='one secret to store in a container ' '(can be set multiple times)') get_parser.add_argument('URI', help='The URI reference for the secret, ' 'order, container, or verification.') help='List secrets, orders, ' 'containers, or ' @staticmethod def _parse_secrets(secrets): ret = [] if secrets: for s in secrets: name, ref = s.split(""="") ret.append({""name"": name, ""secret_ref"": ref}) return ret elif args.command == 'container': secrets = self._parse_secrets(args.secret) if not secrets: self.parser.exit(status=1, message='ERROR: must provide at ' 'least one secret to ' 'create a container\n') container = self.client.containers.create(secrets, args.name, args.type) print(container) elif args.command == 'container': self.client.containers.delete(args.URI) elif args.command == 'container': print(self.client.containers.get(args.URI)) elif args.command == 'container': ls = self.client.containers.list(args.limit, args.offset)"," choices=['order', 'secret', 'verification'], help='the algorithmm mode to be used with ' 'the rquested key (default: %(default)s).') get_parser.add_argument('URI', help='The URI reference ' 'for the secret, ' 'order or verification.') help='List secrets, ' 'orders or '",325,8
openstack%2Fneutron~master~Ia3dd49c7b39f9ca07d5bb117972d934813c0cd67,openstack/neutron,master,Ia3dd49c7b39f9ca07d5bb117972d934813c0cd67,Some clean up of code I'm preparing to modify,MERGED,2014-09-18 16:08:37.000000000,2014-09-28 17:16:50.000000000,2014-09-21 07:47:09.000000000,"[{'_account_id': 3}, {'_account_id': 748}, {'_account_id': 5170}, {'_account_id': 5948}, {'_account_id': 6854}, {'_account_id': 7448}, {'_account_id': 7787}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10386}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 10980}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-09-18 16:08:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/146ce3d057d38e9f8ecc8487f7ed87627aa3cc07', 'message': ""Some clean up of code I'm preparing to modify\n\nThis patch has a few benign changes that should be easily reviewed.\nThe purpose of this patch is to allow me to make cleaner edits in\nfollow on patches so that they're more easily reviewed in their\nspecific contexts.\n\nChange-Id: Ia3dd49c7b39f9ca07d5bb117972d934813c0cd67\n""}, {'number': 2, 'created': '2014-09-18 17:38:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/6accaa298be05532497920f30a08f38e161a8cd2', 'message': ""Some clean up of code I'm preparing to modify\n\nThis patch has a few benign changes that should be easily reviewed.\nThe purpose of this patch is to allow me to make cleaner edits in\nfollow on patches so that they're more easily reviewed in their\nspecific contexts.\n\nChange-Id: Ia3dd49c7b39f9ca07d5bb117972d934813c0cd67\n""}, {'number': 3, 'created': '2014-09-18 17:58:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/55939f51a0c11bdca9beb75757ea8bf0a9d22e6e', 'message': ""Some clean up of code I'm preparing to modify\n\nThis patch has a few benign changes that should be easily reviewed.\nThe purpose of this patch is to allow me to make cleaner edits in\nfollow on patches so that they're more easily reviewed in their\nspecific contexts.\n\nChange-Id: Ia3dd49c7b39f9ca07d5bb117972d934813c0cd67\n""}, {'number': 4, 'created': '2014-09-18 20:25:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/1a8dfa2f5725add24f70610d9c10758e9441afaa', 'message': ""Some clean up of code I'm preparing to modify\n\nThis patch has a few benign changes that should be easily reviewed.\nThe purpose of this patch is to allow me to make cleaner edits in\nfollow on patches so that they're more easily reviewed in their\nspecific contexts.\n\nChange-Id: Ia3dd49c7b39f9ca07d5bb117972d934813c0cd67\n""}, {'number': 5, 'created': '2014-09-18 22:07:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8757614ac9c8dbf641b591ff56545b147d077079', 'message': ""Some clean up of code I'm preparing to modify\n\nThis patch has a few benign changes that should be easily reviewed.\nThe purpose of this patch is to allow me to make cleaner edits in\nfollow on patches so that they're more easily reviewed in their\nspecific contexts.\n\nChange-Id: Ia3dd49c7b39f9ca07d5bb117972d934813c0cd67\n""}, {'number': 6, 'created': '2014-09-19 15:25:15.000000000', 'files': ['neutron/db/db_base_plugin_v2.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/2f762d5d4ceff924e6972d68ce0d301844e8f701', 'message': ""Some clean up of code I'm preparing to modify\n\nThis patch has a few benign changes that should be easily reviewed.\nThe purpose of this patch is to allow me to make cleaner edits in\nfollow on patches so that they're more easily reviewed in their\nspecific contexts.\n\nChange-Id: Ia3dd49c7b39f9ca07d5bb117972d934813c0cd67\n""}]",14,122457,2f762d5d4ceff924e6972d68ce0d301844e8f701,145,26,6,6788,,,0,"Some clean up of code I'm preparing to modify

This patch has a few benign changes that should be easily reviewed.
The purpose of this patch is to allow me to make cleaner edits in
follow on patches so that they're more easily reviewed in their
specific contexts.

Change-Id: Ia3dd49c7b39f9ca07d5bb117972d934813c0cd67
",git fetch https://review.opendev.org/openstack/neutron refs/changes/57/122457/4 && git format-patch -1 --stdout FETCH_HEAD,['neutron/db/db_base_plugin_v2.py'],1,146ce3d057d38e9f8ecc8487f7ed87627aa3cc07,opt-locking-PS17-1," LOG.debug(""All IPs from subnet %(subnet_id)s (%(cidr)s) "" ""allocated"", {'subnet_id': subnet['id'], 'cidr': subnet['cidr']}) LOG.debug(""No more free IP's in slice. Deleting "" ""allocation pool."") new_first_ip = str(netaddr.IPAddress(ip_address) + 1) ip_range['first_ip'] = new_first_ip LOG.debug(""Allocated IP - %(ip_address)s from %(first_ip)s "" ""to %(last_ip)s"", {'ip_address': ip_address, 'first_ip': ip_range['first_ip'], 'last_ip': ip_range['last_ip']}) return {'ip_address': ip_address, 'subnet_id': subnet['id']} """"""Rebuild availability ranges. This methos is called only when there's no more IP available or by _update_subnet_allocation_pools. Calling _update_subnet_allocation_pools before calling this function deletes the IPAllocationPools associated with the subnet that is updating, which will result in deleting the IPAvailabilityRange too. """""" new_ip_range = models_v2.IPAvailabilityRange( context.session.add(new_ip_range) db_port = models_v2.Port(tenant_id=tenant_id, name=p['name'], id=port_id, network_id=network_id, mac_address=p['mac_address'], admin_state_up=p['admin_state_up'], status=status, device_id=p['device_id'], device_owner=p['device_owner']) context.session.add(db_port) # Update the IP's for the port ips = self._allocate_ips_for_port(context, port)"," LOG.debug(_(""All IPs from subnet %(subnet_id)s (%(cidr)s) "" ""allocated""), {'subnet_id': subnet['id'], 'cidr': subnet['cidr']}) LOG.debug(_(""Allocated IP - %(ip_address)s from %(first_ip)s "" ""to %(last_ip)s""), {'ip_address': ip_address, 'first_ip': ip_range['first_ip'], 'last_ip': ip_range['last_ip']}) LOG.debug(_(""No more free IP's in slice. Deleting allocation "" ""pool."")) ip_range['first_ip'] = str(netaddr.IPAddress(ip_address) + 1) return {'ip_address': ip_address, 'subnet_id': subnet['id']} ip_range = models_v2.IPAvailabilityRange( context.session.add(ip_range) # Returns the IP's for the port ips = self._allocate_ips_for_port(context, port) port = models_v2.Port(tenant_id=tenant_id, name=p['name'], id=port_id, network_id=network_id, mac_address=p['mac_address'], admin_state_up=p['admin_state_up'], status=status, device_id=p['device_id'], device_owner=p['device_owner']) context.session.add(port) # Update the allocated IP's",37,28
openstack%2Fopenstack-manuals~master~I6a252ae9f5e2151e3c43a05fde23d967c5bceeb6,openstack/openstack-manuals,master,I6a252ae9f5e2151e3c43a05fde23d967c5bceeb6,Adds and changes links to API Reference pages,MERGED,2014-09-25 20:55:28.000000000,2014-09-28 16:05:36.000000000,2014-09-28 16:05:36.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 7923}]","[{'number': 1, 'created': '2014-09-25 20:55:28.000000000', 'files': ['www/index.html'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/c3dc39f9ee2a80cdc774d1950fba2caef03e820b', 'message': 'Adds and changes links to API Reference pages\n\nChange-Id: I6a252ae9f5e2151e3c43a05fde23d967c5bceeb6\nblueprint: multi-version-api-site\n'}]",0,124179,c3dc39f9ee2a80cdc774d1950fba2caef03e820b,7,3,1,964,,,0,"Adds and changes links to API Reference pages

Change-Id: I6a252ae9f5e2151e3c43a05fde23d967c5bceeb6
blueprint: multi-version-api-site
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/79/124179/1 && git format-patch -1 --stdout FETCH_HEAD,['www/index.html'],1,c3dc39f9ee2a80cdc774d1950fba2caef03e820b,bp/multi-version-api-site," <a href=""http://developer.openstack.org/api-ref-blockstorage-v2.html""> <a href=""http://developer.openstack.org/api-ref-blockstorage-v1.html""> <a href=""http://developer.openstack.org/api-ref-compute-v2.html""> <a href=""http://developer.openstack.org/api-ref-compute-v2-ext.html""> Compute API v2 Extensions </a> </dd> <dd> <a href=""http://developer.openstack.org/api-ref-databases-v1.html""> Database Service API v1.0 </a> </dd> <dd> <a href=""http://developer.openstack.org/api-ref-identity-v2.html""> <a href=""http://developer.openstack.org/api-ref-image-v2.html""> <a href=""http://developer.openstack.org/api-ref-image-v1.html""> <a href=""http://developer.openstack.org/api-ref-networking-v2.html""> Networking API v2.0 Reference </a> </dd> <dd> <a href=""http://developer.openstack.org/api-ref-objectstorage-v1.html""> <dd> <a href=""http://developer.openstack.org/api-ref-orchestration-v1.html""> Orchestration API v1 Reference </a> </dd> <dd> <a href=""http://developer.openstack.org/api-ref-telemetry-v2.html""> Telemetry API v2 Reference </a> </dd>"," <a href=""http://docs.openstack.org/api/openstack-block-storage/2.0/content/""> <a href=""http://docs.openstack.org/api/openstack-block-storage/1.0/content/""> <a href=""http://docs.openstack.org/api/openstack-compute/2/content/""> <a href=""http://docs.openstack.org/api/openstack-identity-service/2.0/content/""> <a href=""http://docs.openstack.org/api/openstack-network/2.0/content/""> Networking API v2.0 Reference </a> </dd> <dd> <a href=""http://docs.openstack.org/api/openstack-image-service/2.0/content/""> <a href=""http://docs.openstack.org/api/openstack-image-service/1.1/content/""> <a href=""http://docs.openstack.org/api/openstack-object-storage/1.0/content/"">",32,12
openstack%2Foslotest~master~I4549c6942b1367c7e490e48ed275ff6a3bcddfa5,openstack/oslotest,master,I4549c6942b1367c7e490e48ed275ff6a3bcddfa5,Drop .sh extension from oslo_debug_helper.sh,MERGED,2014-09-18 21:28:03.000000000,2014-09-28 15:29:37.000000000,2014-09-28 15:29:37.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 6476}]","[{'number': 1, 'created': '2014-09-18 21:28:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslotest/commit/c3b8961ed0f3fa307fbd65e6f5013aa9c82228ab', 'message': 'Drop .sh extension from oslo_debug_helper.sh\n\nTo be compliant with Debian policy on naming binary files, the\nshell file should not have a .sh extension.\n\nChange-Id: I4549c6942b1367c7e490e48ed275ff6a3bcddfa5\nCloses-Bug: #1370488\n'}, {'number': 2, 'created': '2014-09-19 01:42:40.000000000', 'files': ['tools/oslo_debug_helper', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/oslotest/commit/b2f8b9dd9b2cd049f667acc7c42289b4f17258db', 'message': 'Drop .sh extension from oslo_debug_helper.sh\n\nTo be compliant with Debian policy on naming binary files, the\nshell file should not have a .sh extension.\n\nChange-Id: I4549c6942b1367c7e490e48ed275ff6a3bcddfa5\nCloses-Bug: #1370488\n'}]",0,122534,b2f8b9dd9b2cd049f667acc7c42289b4f17258db,10,4,2,6482,,,0,"Drop .sh extension from oslo_debug_helper.sh

To be compliant with Debian policy on naming binary files, the
shell file should not have a .sh extension.

Change-Id: I4549c6942b1367c7e490e48ed275ff6a3bcddfa5
Closes-Bug: #1370488
",git fetch https://review.opendev.org/openstack/oslotest refs/changes/34/122534/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/oslo_debug_helper'],1,c3b8961ed0f3fa307fbd65e6f5013aa9c82228ab,bug1370488,,,0,0
openstack%2Fswift~master~I59b68ecb6f07e2b71e4655699d0165007d764718,openstack/swift,master,I59b68ecb6f07e2b71e4655699d0165007d764718,Imported Translations from Transifex,MERGED,2014-09-26 06:09:23.000000000,2014-09-28 15:27:28.000000000,2014-09-28 15:27:27.000000000,"[{'_account_id': 3}, {'_account_id': 330}, {'_account_id': 13052}]","[{'number': 1, 'created': '2014-09-26 06:09:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/c2762f56f14d41a9256a93cbee913b4556159b20', 'message': 'Imported Translations from Transifex\n\nChange-Id: I59b68ecb6f07e2b71e4655699d0165007d764718\n'}, {'number': 2, 'created': '2014-09-27 06:20:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/d4a60b2af1cbdf9489bcb08bac8c4273a33c0b00', 'message': 'Imported Translations from Transifex\n\nChange-Id: I59b68ecb6f07e2b71e4655699d0165007d764718\n'}, {'number': 3, 'created': '2014-09-28 06:08:56.000000000', 'files': ['swift/locale/fr/LC_MESSAGES/swift-log-critical.po', 'swift/locale/fr/LC_MESSAGES/swift-log-warning.po', 'swift/locale/fr/LC_MESSAGES/swift-log-error.po', 'swift/locale/fr/LC_MESSAGES/swift-log-info.po', 'swift/locale/swift.pot'], 'web_link': 'https://opendev.org/openstack/swift/commit/e2285e46db11dd59edbe076540ac0db24ef6f3f2', 'message': 'Imported Translations from Transifex\n\nChange-Id: I59b68ecb6f07e2b71e4655699d0165007d764718\n'}]",0,124290,e2285e46db11dd59edbe076540ac0db24ef6f3f2,13,3,3,11131,,,0,"Imported Translations from Transifex

Change-Id: I59b68ecb6f07e2b71e4655699d0165007d764718
",git fetch https://review.opendev.org/openstack/swift refs/changes/90/124290/2 && git format-patch -1 --stdout FETCH_HEAD,"['swift/locale/fr/LC_MESSAGES/swift-log-critical.po', 'swift/locale/fr/LC_MESSAGES/swift-log-warning.po', 'swift/locale/fr/LC_MESSAGES/swift-log-error.po', 'swift/locale/fr/LC_MESSAGES/swift-log-info.po', 'swift/locale/swift.pot']",5,c2762f56f14d41a9256a93cbee913b4556159b20,transifex/translations,"""Project-Id-Version: swift 2.1.0.93.g5d9190c\n""""POT-Creation-Date: 2014-09-26 06:09+0000\n""#: swift/account/auditor.py:59#: swift/account/auditor.py:82#: swift/account/auditor.py:88 swift/container/auditor.py:86#: swift/account/auditor.py:93#: swift/account/auditor.py:99#: swift/account/auditor.py:104#: swift/account/auditor.py:123 #, python-format msgid """" ""The total %(key)s for the container (%(total)s) does not match the sum of"" "" %(key)s across policies (%(sum)s)"" msgstr """" #: swift/account/auditor.py:149 #, python-format msgid ""Audit Failed for %s: %s"" msgstr """" #: swift/account/auditor.py:153","""Project-Id-Version: swift 2.1.0.77.g0d0c16d\n""""POT-Creation-Date: 2014-09-22 06:07+0000\n""#: swift/account/auditor.py:58#: swift/account/auditor.py:81#: swift/account/auditor.py:87 swift/container/auditor.py:86#: swift/account/auditor.py:92#: swift/account/auditor.py:98#: swift/account/auditor.py:103#: swift/account/auditor.py:124",105,9
openstack%2Ftraining-guides~master~I243002eca1b68bbce1c7d4b2985a28a31da968e1,openstack/training-guides,master,I243002eca1b68bbce1c7d4b2985a28a31da968e1,Show paths relative to this repo in README.rst,MERGED,2014-09-27 09:13:55.000000000,2014-09-28 15:14:19.000000000,2014-09-28 15:14:18.000000000,"[{'_account_id': 3}, {'_account_id': 7007}, {'_account_id': 11109}]","[{'number': 1, 'created': '2014-09-27 09:13:55.000000000', 'files': ['README.rst'], 'web_link': 'https://opendev.org/openstack/training-guides/commit/257cbcfa7a99cd171cba884f2f69b4425e19652f', 'message': ""Show paths relative to this repo in README.rst\n\nThis patch as well fixes the path to the generated PDF documentation\nwhich was wrong because it misses the last 'training-guides' directory.\n\nChange-Id: I243002eca1b68bbce1c7d4b2985a28a31da968e1\n""}]",0,124571,257cbcfa7a99cd171cba884f2f69b4425e19652f,7,3,1,4128,,,0,"Show paths relative to this repo in README.rst

This patch as well fixes the path to the generated PDF documentation
which was wrong because it misses the last 'training-guides' directory.

Change-Id: I243002eca1b68bbce1c7d4b2985a28a31da968e1
",git fetch https://review.opendev.org/openstack/training-guides refs/changes/71/124571/1 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,257cbcfa7a99cd171cba884f2f69b4425e19652f,wip,``doc/`` directory. cd doc/training-guides/ doc/training-guides/target/docbkx/webhelp/training-guides/training-guides.pdf doc/training-guides/target/docbkx/webhelp/training-guides/content/index.html,``openstack-training-guides/doc/`` directory. cd openstack-training-guides/doc/training-guides/ openstack-training-guides/doc/training-guides/target/docbkx/webhelp/training-guides.pdf openstack-training-guides/doc/training-guides/target/docbkx/webhelp/training-guides/content/index.html,4,4
openstack%2Ftraining-guides~master~I161823337b225cb8e39b2bd51d5472f65bbb3a16,openstack/training-guides,master,I161823337b225cb8e39b2bd51d5472f65bbb3a16,"labs: use sudo for ""killall dnsmasq""",MERGED,2014-09-26 16:34:51.000000000,2014-09-28 15:12:08.000000000,2014-09-28 15:12:07.000000000,"[{'_account_id': 3}, {'_account_id': 7007}, {'_account_id': 9178}]","[{'number': 1, 'created': '2014-09-26 16:34:51.000000000', 'files': ['labs/scripts/setup_neutron_network.sh'], 'web_link': 'https://opendev.org/openstack/training-guides/commit/50fe9a731b256f4362865c2d3cf215eab3a4ab01', 'message': 'labs: use sudo for ""killall dnsmasq""\n\nKilling the dnsmasq process requires root privileges.\n\nChange-Id: I161823337b225cb8e39b2bd51d5472f65bbb3a16\n'}]",0,124461,50fe9a731b256f4362865c2d3cf215eab3a4ab01,7,3,1,11109,,,0,"labs: use sudo for ""killall dnsmasq""

Killing the dnsmasq process requires root privileges.

Change-Id: I161823337b225cb8e39b2bd51d5472f65bbb3a16
",git fetch https://review.opendev.org/openstack/training-guides refs/changes/61/124461/1 && git format-patch -1 --stdout FETCH_HEAD,['labs/scripts/setup_neutron_network.sh'],1,50fe9a731b256f4362865c2d3cf215eab3a4ab01,next/killall_dnsmasq,sudo killall dnsmasq,killall dnsmasq,1,1
openstack%2Ftraining-guides~master~Id1bd3be2791acee80df97a564db42885141b245a,openstack/training-guides,master,Id1bd3be2791acee80df97a564db42885141b245a,Add Check_mark_23x20_02.svg,MERGED,2014-09-25 15:39:30.000000000,2014-09-28 15:10:55.000000000,2014-09-28 15:10:54.000000000,"[{'_account_id': 3}, {'_account_id': 7007}, {'_account_id': 11889}]","[{'number': 1, 'created': '2014-09-25 15:39:30.000000000', 'files': ['doc/training-guides/figures/Check_mark_23x20_02.svg'], 'web_link': 'https://opendev.org/openstack/training-guides/commit/61d083565b7d91624b6fc0c6f4ecfff2145df4ba', 'message': ""Add Check_mark_23x20_02.svg\n\nThis is used by the openstack.ent file, let's import it so that the\nentity is usable.\n\nChange-Id: Id1bd3be2791acee80df97a564db42885141b245a\n""}]",0,124084,61d083565b7d91624b6fc0c6f4ecfff2145df4ba,7,3,1,6547,,,0,"Add Check_mark_23x20_02.svg

This is used by the openstack.ent file, let's import it so that the
entity is usable.

Change-Id: Id1bd3be2791acee80df97a564db42885141b245a
",git fetch https://review.opendev.org/openstack/training-guides refs/changes/84/124084/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/training-guides/figures/Check_mark_23x20_02.svg'],1,61d083565b7d91624b6fc0c6f4ecfff2145df4ba,import-check-mark,"<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?> <!-- Created with Inkscape (http://www.inkscape.org/) --> <svg xmlns:dc=""http://purl.org/dc/elements/1.1/"" xmlns:cc=""http://web.resource.org/cc/"" xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#"" xmlns:svg=""http://www.w3.org/2000/svg"" xmlns=""http://www.w3.org/2000/svg"" xmlns:sodipodi=""http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"" xmlns:inkscape=""http://www.inkscape.org/namespaces/inkscape"" width=""19.21315"" height=""18.294994"" id=""svg2"" sodipodi:version=""0.32"" inkscape:version=""0.45"" sodipodi:modified=""true"" version=""1.0""> <defs id=""defs4"" /> <sodipodi:namedview id=""base"" pagecolor=""#ffffff"" bordercolor=""#666666"" borderopacity=""1.0"" gridtolerance=""10000"" guidetolerance=""10"" objecttolerance=""10"" inkscape:pageopacity=""0.0"" inkscape:pageshadow=""2"" inkscape:zoom=""7.9195959"" inkscape:cx=""17.757032"" inkscape:cy=""7.298821"" inkscape:document-units=""px"" inkscape:current-layer=""layer1"" inkscape:window-width=""984"" inkscape:window-height=""852"" inkscape:window-x=""148"" inkscape:window-y=""66"" /> <metadata id=""metadata7""> <rdf:RDF> <cc:Work rdf:about=""""> <dc:format>image/svg+xml</dc:format> <dc:type rdf:resource=""http://purl.org/dc/dcmitype/StillImage"" /> </cc:Work> </rdf:RDF> </metadata> <g inkscape:label=""Layer 1"" inkscape:groupmode=""layer"" id=""layer1"" transform=""translate(-192.905,-516.02064)""> <path style=""fill:#000000"" d=""M 197.67968,534.31563 C 197.40468,534.31208 196.21788,532.53719 195.04234,530.37143 L 192.905,526.43368 L 193.45901,525.87968 C 193.76371,525.57497 194.58269,525.32567 195.27896,525.32567 L 196.5449,525.32567 L 197.18129,527.33076 L 197.81768,529.33584 L 202.88215,523.79451 C 205.66761,520.74678 208.88522,517.75085 210.03239,517.13691 L 212.11815,516.02064 L 207.90871,520.80282 C 205.59351,523.43302 202.45735,527.55085 200.93947,529.95355 C 199.42159,532.35625 197.95468,534.31919 197.67968,534.31563 z "" id=""path2223"" /> </g> </svg> ",,60,0
openstack%2Ftrove~master~I3b49b1d667f6ade9ae3f6765d735440a3e838917,openstack/trove,master,I3b49b1d667f6ade9ae3f6765d735440a3e838917,Sync latest process and str utils from oslo,MERGED,2014-09-14 19:22:05.000000000,2014-09-28 15:07:57.000000000,2014-09-28 15:07:56.000000000,"[{'_account_id': 3}, {'_account_id': 308}, {'_account_id': 1955}, {'_account_id': 5293}, {'_account_id': 6802}, {'_account_id': 7092}, {'_account_id': 8415}, {'_account_id': 9311}, {'_account_id': 9664}]","[{'number': 1, 'created': '2014-09-14 19:22:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/e9beec22cc0209512bbe372e6b38ad27f5f0c1ee', 'message': 'Sync latest process and str utils from oslo\n\nThis sync required changes to fix these issues:\n* Make execute method clean password in exception\n* Make sure mask_password works properly\n\n------------------------------------------------\nThe sync pulls in the following changes (newest to oldest):\n\n6a60f842 - Mask passwords in exceptions and error messages (SSH)\n63c99a0f - Mask passwords in exceptions and error messages\n66142c34 - Make strutils.mask_password more secure\n\n-----------------------------------------------\n\nCloses-Bug: 1343604\nCloses-Bug: 1345233\nSecurityImpact\n\nChange-Id: I3b49b1d667f6ade9ae3f6765d735440a3e838917\n'}, {'number': 2, 'created': '2014-09-22 17:00:09.000000000', 'files': ['trove/openstack/common/processutils.py', 'trove/openstack/common/strutils.py'], 'web_link': 'https://opendev.org/openstack/trove/commit/9672744f090d462cac5eb757ceaacd7122362708', 'message': 'Sync latest process and str utils from oslo\n\nThis sync required changes to fix these issues:\n* Make execute method clean password in exception\n* Make sure mask_password works properly\n\n------------------------------------------------\nThe sync pulls in the following changes (newest to oldest):\n\n6a60f842 - Mask passwords in exceptions and error messages (SSH)\n63c99a0f - Mask passwords in exceptions and error messages\n66142c34 - Make strutils.mask_password more secure\n\n-----------------------------------------------\n\nCloses-Bug: 1343604\nCloses-Bug: 1345233\nSecurityImpact\n\nChange-Id: I3b49b1d667f6ade9ae3f6765d735440a3e838917\n'}]",3,121417,9672744f090d462cac5eb757ceaacd7122362708,19,9,2,9311,,,0,"Sync latest process and str utils from oslo

This sync required changes to fix these issues:
* Make execute method clean password in exception
* Make sure mask_password works properly

------------------------------------------------
The sync pulls in the following changes (newest to oldest):

6a60f842 - Mask passwords in exceptions and error messages (SSH)
63c99a0f - Mask passwords in exceptions and error messages
66142c34 - Make strutils.mask_password more secure

-----------------------------------------------

Closes-Bug: 1343604
Closes-Bug: 1345233
SecurityImpact

Change-Id: I3b49b1d667f6ade9ae3f6765d735440a3e838917
",git fetch https://review.opendev.org/openstack/trove refs/changes/17/121417/2 && git format-patch -1 --stdout FETCH_HEAD,"['trove/openstack/common/processutils.py', 'trove/openstack/common/strutils.py']",2,e9beec22cc0209512bbe372e6b38ad27f5f0c1ee,bug/1343604,"# NOTE(flaper87): The following globals are used by `mask_password`_SANITIZE_PATTERNS_2 = [] _SANITIZE_PATTERNS_1 = [] # NOTE(amrith): Some regular expressions have only one parameter, some # have two parameters. Use different lists of patterns here. _FORMAT_PATTERNS_1 = [r'(%(key)s\s*[=]\s*)[^\s^\'^\""]+'] _FORMAT_PATTERNS_2 = [r'(%(key)s\s*[=]\s*[\""\']).*?([\""\'])', r'(%(key)s\s+[\""\']).*?([\""\'])', r'([-]{2}%(key)s\s+)[^\'^\""^=^\s]+([\s]*)', r'(<%(key)s>).*?(</%(key)s>)', r'([\""\']%(key)s[\""\']\s*:\s*[\""\']).*?([\""\'])', r'([\'""].*?%(key)s[\'""]\s*:\s*u?[\'""]).*?([\'""])', r'([\'""].*?%(key)s[\'""]\s*,\s*\'--?[A-z]+\'\s*,\s*u?' '[\'""]).*?([\'""])', r'(%(key)s\s*--?[A-z]+\s*)\S+(\s*)'] for pattern in _FORMAT_PATTERNS_2: _SANITIZE_PATTERNS_2.append(reg_ex) for pattern in _FORMAT_PATTERNS_1: reg_ex = re.compile(pattern % {'key': key}, re.DOTALL) _SANITIZE_PATTERNS_1.append(reg_ex) substitute = r'\g<1>' + secret + r'\g<2>' for pattern in _SANITIZE_PATTERNS_2: message = re.sub(pattern, substitute, message) substitute = r'\g<1>' + secret for pattern in _SANITIZE_PATTERNS_1: message = re.sub(pattern, substitute, message) ","# NOTE(flaper87): The following 3 globals are used by `mask_password`_SANITIZE_PATTERNS = [] _FORMAT_PATTERNS = [r'(%(key)s\s*[=]\s*[\""\']).*?([\""\'])', r'(<%(key)s>).*?(</%(key)s>)', r'([\""\']%(key)s[\""\']\s*:\s*[\""\']).*?([\""\'])', r'([\'""].*?%(key)s[\'""]\s*:\s*u?[\'""]).*?([\'""])', r'([\'""].*?%(key)s[\'""]\s*,\s*\'--?[A-z]+\'\s*,\s*u?[\'""])' '.*?([\'""])', r'(%(key)s\s*--?[A-z]+\s*)\S+(\s*)'] for pattern in _FORMAT_PATTERNS: _SANITIZE_PATTERNS.append(reg_ex) secret = r'\g<1>' + secret + r'\g<2>' for pattern in _SANITIZE_PATTERNS: message = re.sub(pattern, secret, message)",47,25
openstack%2Fsahara~master~I2bf708d86e183b7810f905c575014f8c76301c4e,openstack/sahara,master,I2bf708d86e183b7810f905c575014f8c76301c4e,Adding links for Juno Fedora images,MERGED,2014-09-24 20:03:10.000000000,2014-09-28 14:11:07.000000000,2014-09-28 14:11:06.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 8090}, {'_account_id': 8091}, {'_account_id': 8411}, {'_account_id': 9548}, {'_account_id': 10670}, {'_account_id': 12038}]","[{'number': 1, 'created': '2014-09-24 20:03:10.000000000', 'files': ['doc/source/userdoc/vanilla_plugin.rst'], 'web_link': 'https://opendev.org/openstack/sahara/commit/d67bbaeb96a4652f96b77a5f8854b8a3d84e59f4', 'message': 'Adding links for Juno Fedora images\n\nChange-Id: I2bf708d86e183b7810f905c575014f8c76301c4e\nCloses-Bug: #1373593\n'}]",0,123836,d67bbaeb96a4652f96b77a5f8854b8a3d84e59f4,30,9,1,10670,,,0,"Adding links for Juno Fedora images

Change-Id: I2bf708d86e183b7810f905c575014f8c76301c4e
Closes-Bug: #1373593
",git fetch https://review.opendev.org/openstack/sahara refs/changes/36/123836/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/userdoc/vanilla_plugin.rst'],1,d67bbaeb96a4652f96b77a5f8854b8a3d84e59f4,bug/1373593,* http://sahara-files.mirantis.com/sahara-juno-vanilla-1.2.1-fedora-20.qcow2* http://sahara-files.mirantis.com/sahara-juno-vanilla-2.4.1-fedora-20.qcow2,,2,0
openstack%2Fhorizon~master~Ife4275e9e439736320c4523f6a933ce315cc20e8,openstack/horizon,master,Ife4275e9e439736320c4523f6a933ce315cc20e8,Imported Translations from Transifex,MERGED,2014-09-27 06:15:29.000000000,2014-09-28 13:57:16.000000000,2014-09-28 13:57:15.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 9576}]","[{'number': 1, 'created': '2014-09-27 06:15:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/9c1e5b223728577f419d2f5f7ec2bfb83a56c81e', 'message': 'Imported Translations from Transifex\n\nChange-Id: Ife4275e9e439736320c4523f6a933ce315cc20e8\n'}, {'number': 2, 'created': '2014-09-28 06:04:10.000000000', 'files': ['openstack_dashboard/locale/fr/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en_GB/LC_MESSAGES/django.po', 'horizon/locale/ru/LC_MESSAGES/django.po', 'openstack_dashboard/locale/de/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ja/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ko_KR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/es/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_CN/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_TW/LC_MESSAGES/django.po', 'openstack_dashboard/locale/pt_BR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en_AU/LC_MESSAGES/django.po'], 'web_link': 'https://opendev.org/openstack/horizon/commit/368f8befa273a2cca08fb092ab2be16b023a1c39', 'message': 'Imported Translations from Transifex\n\nChange-Id: Ife4275e9e439736320c4523f6a933ce315cc20e8\n'}]",0,124564,368f8befa273a2cca08fb092ab2be16b023a1c39,9,3,2,11131,,,0,"Imported Translations from Transifex

Change-Id: Ife4275e9e439736320c4523f6a933ce315cc20e8
",git fetch https://review.opendev.org/openstack/horizon refs/changes/64/124564/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/locale/fr/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en_GB/LC_MESSAGES/django.po', 'horizon/locale/ru/LC_MESSAGES/django.po', 'openstack_dashboard/locale/de/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ja/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ko_KR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/es/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_CN/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_TW/LC_MESSAGES/django.po', 'openstack_dashboard/locale/pt_BR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en_AU/LC_MESSAGES/django.po']",12,9c1e5b223728577f419d2f5f7ec2bfb83a56c81e,transifex/translations,"""POT-Creation-Date: 2014-09-27 01:09-0500\n"" ""PO-Revision-Date: 2014-09-27 06:10+0000\n""#: settings.py:283#: settings.py:289#: settings.py:295#: api/nova.py:748#: api/nova.py:753#: dashboards/admin/networks/subnets/tables.py:91#: dashboards/project/data_processing/cluster_templates/tables.py:112#: dashboards/project/data_processing/clusters/tables.py:108#: dashboards/project/data_processing/data_sources/tables.py:60#: dashboards/project/data_processing/job_binaries/tables.py:81#: dashboards/project/data_processing/jobs/tables.py:149#: dashboards/project/data_processing/nodegroup_templates/tables.py:79#: dashboards/project/networks/subnets/tables.py:101#: dashboards/admin/flavors/tables.py:141#: dashboards/project/data_processing/cluster_templates/tables.py:123#: dashboards/project/data_processing/data_sources/tables.py:65#: dashboards/project/data_processing/data_sources/workflows/create.py:46#: dashboards/project/data_processing/job_binaries/tables.py:86#: dashboards/project/data_processing/jobs/tables.py:152#: dashboards/project/data_processing/job_executions/tables.py:138#: dashboards/router/nexus1000v/forms.py:176#: dashboards/project/data_processing/data_sources/tables.py:63#: dashboards/project/data_processing/clusters/tables.py:111#: dashboards/project/data_processing/job_executions/tables.py:152#: dashboards/router/nexus1000v/forms.py:179#: dashboards/admin/networks/subnets/tables.py:107#: dashboards/project/networks/subnets/tables.py:115#: dashboards/admin/networks/subnets/tables.py:37#: dashboards/admin/networks/subnets/tables.py:45#: dashboards/project/networks/subnets/tables.py:64#: dashboards/project/networks/subnets/tables.py:75#: dashboards/admin/networks/subnets/tables.py:79 #: dashboards/project/networks/subnets/tables.py:89#: dashboards/admin/networks/subnets/tables.py:93#: dashboards/admin/networks/subnets/tables.py:94#: dashboards/project/networks/subnets/tables.py:104#: dashboards/admin/networks/subnets/tables.py:95#: dashboards/project/networks/subnets/tables.py:105#: dashboards/admin/networks/subnets/tables.py:114#: dashboards/project/networks/subnets/tables.py:57 #: dashboards/project/networks/subnets/tables.py:122 usage/quotas.py:78#: dashboards/project/data_processing/data_sources/workflows/create.py:79#: dashboards/admin/volumes/volume_types/extras/tables.py:74 msgid ""Extra Specs"" msgstr ""Extra Specs"" #: dashboards/project/data_processing/cluster_templates/tables.py:127#: dashboards/project/data_processing/cluster_templates/tables.py:31#: dashboards/project/data_processing/cluster_templates/tables.py:39 #: dashboards/project/data_processing/clusters/tables.py:33#: dashboards/project/data_processing/cluster_templates/tables.py:55 #: dashboards/project/data_processing/nodegroup_templates/tables.py:47#: dashboards/project/data_processing/cluster_templates/tables.py:78msgid ""Delete Template"" msgid_plural ""Delete Templates"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/cluster_templates/tables.py:72 #: dashboards/project/data_processing/nodegroup_templates/tables.py:64 msgid ""Deleted Template"" msgid_plural ""Deleted Templates"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/cluster_templates/tables.py:87 #: dashboards/project/data_processing/nodegroup_templates/tables.py:28#: dashboards/project/data_processing/cluster_templates/tables.py:96#: dashboards/project/data_processing/cluster_templates/tables.py:115#: dashboards/project/data_processing/nodegroup_templates/tables.py:82#: dashboards/project/data_processing/cluster_templates/tables.py:117#: dashboards/project/data_processing/nodegroup_templates/tables.py:84#: dashboards/project/data_processing/cluster_templates/tables.py:119#: dashboards/project/data_processing/nodegroup_templates/tables.py:86#: dashboards/project/data_processing/clusters/tables.py:119#: dashboards/project/data_processing/clusters/tables.py:41#: dashboards/project/data_processing/clusters/tables.py:53 msgid ""Delete Cluster"" msgid_plural ""Delete Clusters"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/clusters/tables.py:61 msgid ""Deleted Cluster"" msgid_plural ""Deleted Clusters"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/clusters/tables.py:84#: dashboards/project/data_processing/clusters/tables.py:94#: dashboards/project/data_processing/clusters/tables.py:115#: dashboards/project/data_processing/data_image_registry/tables.py:76#: dashboards/project/data_processing/data_image_registry/tables.py:84#: dashboards/project/data_processing/data_image_registry/tables.py:30#: dashboards/project/data_processing/data_image_registry/tables.py:44#: dashboards/project/data_processing/data_image_registry/tables.py:54 msgid ""Unregister Image"" msgid_plural ""Unregister Images"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/data_image_registry/tables.py:62 msgid ""Unregistered Image"" msgid_plural ""Unregistered Images"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/data_image_registry/tables.py:80#: dashboards/project/data_processing/data_sources/tables.py:69#: dashboards/project/data_processing/data_sources/tables.py:28#: dashboards/project/data_processing/data_sources/workflows/create.py:54 #: dashboards/project/data_processing/data_sources/workflows/create.py:78msgid ""Delete Data source"" msgid_plural ""Delete Data sources"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/data_sources/tables.py:46 msgid ""Deleted Data source"" msgid_plural ""Deleted Data sources"" msgstr[0] """" msgstr[1] """"#: dashboards/project/data_processing/data_sources/workflows/create.py:42#: dashboards/project/data_processing/data_sources/workflows/create.py:80#: dashboards/project/data_processing/data_sources/workflows/create.py:81#: dashboards/project/data_processing/job_binaries/tables.py:31#: dashboards/project/data_processing/job_binaries/tables.py:90msgid ""Delete Job binary"" msgid_plural ""Delete Job binaries"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/job_binaries/tables.py:49 msgid ""Deleted Job binary"" msgid_plural ""Deleted Job binaries"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/job_binaries/tables.py:74#: dashboards/project/data_processing/job_binaries/tables.py:84#: dashboards/project/data_processing/job_executions/tables.py:161msgid ""Delete Job execution"" msgid_plural ""Delete Job executions"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/job_executions/tables.py:42 msgid ""Deleted Job execution"" msgid_plural ""Deleted Job executions"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/job_executions/tables.py:58 #: dashboards/project/data_processing/job_executions/tables.py:87 #: dashboards/project/data_processing/jobs/tables.py:64 #: dashboards/project/data_processing/jobs/tables.py:93 #: dashboards/project/data_processing/jobs/tables.py:122 #: dashboards/project/data_processing/jobs/templates/data_processing.jobs/launch.html:3 #: dashboards/project/data_processing/jobs/templates/data_processing.jobs/launch.html:6 #: dashboards/project/data_processing/jobs/workflows/launch.py:330 #: dashboards/project/data_processing/jobs/workflows/launch.py:405 #: dashboards/project/data_processing/jobs/workflows/launch.py:415 msgid ""Launch Job"" msgid_plural ""Launch Jobs"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/job_executions/tables.py:66 #: dashboards/project/data_processing/job_executions/tables.py:95 #: dashboards/project/data_processing/jobs/tables.py:72 #: dashboards/project/data_processing/jobs/tables.py:101 #: dashboards/project/data_processing/jobs/tables.py:130 msgid ""Launched Job"" msgid_plural ""Launched Jobs"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/job_executions/tables.py:72#: dashboards/project/data_processing/job_executions/tables.py:101 msgid ""Relaunch On New Cluster"" msgstr ""Relaunch On New Cluster"" #: dashboards/project/data_processing/job_executions/tables.py:143#: dashboards/project/data_processing/job_executions/tables.py:147 #: dashboards/project/data_processing/job_executions/templates/data_processing.job_executions/_details.html:16 #: dashboards/project/data_processing/jobs/workflows/launch.py:104 msgid ""Cluster"" msgstr ""Cluster""#: dashboards/project/data_processing/jobs/panel.py:22 #: dashboards/project/data_processing/jobs/tables.py:156 #: dashboards/project/data_processing/jobs/templates/data_processing.jobs/jobs.html:6 msgid ""Jobs"" msgstr ""Jobs"" #: dashboards/project/data_processing/jobs/tables.py:30#: dashboards/project/data_processing/jobs/tables.py:40 msgid ""Delete Job"" msgid_plural ""Delete Jobs"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/jobs/tables.py:48 msgid ""Deleted Job"" msgid_plural ""Deleted Jobs"" msgstr[0] """" msgstr[1] """" #: dashboards/project/data_processing/jobs/tables.py:78#: dashboards/project/data_processing/jobs/tables.py:107 #: dashboards/project/data_processing/jobs/tables.py:136#: dashboards/project/data_processing/jobs/workflows/launch.py:331 #: dashboards/project/data_processing/jobs/workflows/launch.py:416 #: dashboards/project/databases/workflows/create_instance.py:273 #: dashboards/project/images/images/tables.py:35 #: dashboards/project/instances/workflows/create_instance.py:780 #: dashboards/project/stacks/templates/stacks/_create.html:24 msgid ""Launch"" msgstr ""Launch"" #: dashboards/project/data_processing/nodegroup_templates/tables.py:92#: dashboards/project/data_processing/nodegroup_templates/tables.py:37#: dashboards/project/data_processing/nodegroup_templates/tables.py:70 #: dashboards/project/loadbalancers/tables.py:75 #: dashboards/project/loadbalancers/tables.py:89 #: dashboards/project/loadbalancers/tables.py:104 #: dashboards/project/loadbalancers/tables.py:113 #: dashboards/project/vpn/tables.py:67 dashboards/project/vpn/tables.py:81 #: dashboards/project/vpn/tables.py:95 dashboards/project/vpn/tables.py:109 msgid ""Delete"" msgstr ""Delete"" #: dashboards/project/instances/workflows/create_instance.py:782#: dashboards/project/instances/workflows/create_instance.py:799#: dashboards/project/instances/workflows/create_instance.py:779#: dashboards/project/instances/workflows/create_instance.py:781#: dashboards/project/instances/workflows/create_instance.py:744#: dashboards/project/instances/workflows/create_instance.py:745#: dashboards/project/instances/workflows/create_instance.py:754msgid ""Specify advanced options to use when launching an instance."" msgstr """"#: dashboards/project/instances/workflows/create_instance.py:728 msgid """" ""Automatic: The entire disk is a single partition and automatically resizes. "" ""Manual: Results in faster build times but requires manual partitioning."" msgstr """" #: dashboards/project/instances/workflows/create_instance.py:731 msgid ""Configuration Drive"" msgstr """" #: dashboards/project/instances/workflows/create_instance.py:732 msgid """" ""Configure OpenStack to write metadata to a special configuration drive that "" ""attaches to the instance when it boots."" msgstr """" #: dashboards/project/instances/workflows/create_instance.py:758#: dashboards/project/instances/workflows/create_instance.py:796#: dashboards/project/instances/workflows/create_instance.py:854#: dashboards/project/networks/subnets/tables.py:56#: dashboards/project/networks/subnets/tables.py:103msgid ""1-4093 for VLAN; 5000 and above for Overlay"" msgstr """"msgid ""Multicast IPv4 range(e.g. 224.0.1.0-224.0.1.100)"" msgstr """"#: dashboards/router/nexus1000v/forms.py:203#: dashboards/router/nexus1000v/forms.py:209 #, python-format msgid ""Failed to update network profile (%s)."" msgstr ""Failed to update network profile (%s)."" #: dashboards/router/nexus1000v/views.py:142 msgid ""Failed to obtain network profile binding"" msgstr """" msgid """" ""Edit the network profile to update name, segment range or multicast IP "" ""range."" msgstr """"#: dashboards/router/nexus1000v/templates/nexus1000v/update_network_profile.html:3","""POT-Creation-Date: 2014-09-25 13:08-0500\n"" ""PO-Revision-Date: 2014-09-25 17:03+0000\n""#: settings.py:277#: settings.py:283#: settings.py:289#: api/nova.py:747#: api/nova.py:752#: dashboards/admin/networks/subnets/tables.py:97#: dashboards/project/data_processing/cluster_templates/tables.py:100#: dashboards/project/data_processing/clusters/tables.py:95#: dashboards/project/data_processing/data_sources/tables.py:47#: dashboards/project/data_processing/job_binaries/tables.py:68#: dashboards/project/data_processing/jobs/tables.py:100#: dashboards/project/data_processing/nodegroup_templates/tables.py:67#: dashboards/project/networks/subnets/tables.py:98#: dashboards/router/nexus1000v/forms.py:181#: dashboards/project/data_processing/cluster_templates/tables.py:111#: dashboards/project/data_processing/data_sources/tables.py:52#: dashboards/project/data_processing/data_sources/workflows/create.py:44#: dashboards/project/data_processing/job_binaries/tables.py:73#: dashboards/project/data_processing/jobs/tables.py:103#: dashboards/project/data_processing/job_executions/tables.py:101#: dashboards/router/nexus1000v/forms.py:178#: dashboards/admin/flavors/tables.py:141 #: dashboards/admin/volumes/volume_types/extras/tables.py:74 msgid ""Extra Specs"" msgstr ""Extra Specs"" #: dashboards/project/data_processing/data_sources/tables.py:50#: dashboards/project/data_processing/clusters/tables.py:98#: dashboards/project/data_processing/job_executions/tables.py:115#: dashboards/project/data_processing/data_image_registry/tables.py:54#: dashboards/router/nexus1000v/forms.py:192#: dashboards/router/nexus1000v/forms.py:190#: dashboards/admin/networks/subnets/tables.py:113#: dashboards/project/networks/subnets/tables.py:112#: dashboards/admin/networks/subnets/tables.py:36#: dashboards/admin/networks/subnets/tables.py:44#: dashboards/project/networks/subnets/tables.py:55#: dashboards/project/networks/subnets/tables.py:66#: dashboards/admin/networks/subnets/tables.py:84 #: dashboards/project/networks/subnets/tables.py:85#: dashboards/admin/networks/subnets/tables.py:99#: dashboards/admin/networks/subnets/tables.py:100#: dashboards/project/networks/subnets/tables.py:101#: dashboards/admin/networks/subnets/tables.py:101#: dashboards/project/networks/subnets/tables.py:102#: dashboards/admin/networks/subnets/tables.py:120#: dashboards/project/networks/subnets/tables.py:47 #: dashboards/project/networks/subnets/tables.py:119 usage/quotas.py:78#: dashboards/router/nexus1000v/templates/nexus1000v/update_network_profile.html:3#: dashboards/project/data_processing/cluster_templates/tables.py:65 #: dashboards/project/data_processing/clusters/tables.py:51 #: dashboards/project/data_processing/data_sources/tables.py:36 #: dashboards/project/data_processing/job_binaries/tables.py:39 #: dashboards/project/data_processing/job_executions/tables.py:32 #: dashboards/project/data_processing/jobs/tables.py:38 #: dashboards/project/data_processing/nodegroup_templates/tables.py:57#: dashboards/project/data_processing/data_sources/workflows/create.py:89#: dashboards/project/data_processing/cluster_templates/tables.py:66#: dashboards/project/data_processing/nodegroup_templates/tables.py:58#: dashboards/project/data_processing/cluster_templates/tables.py:115#: dashboards/project/data_processing/cluster_templates/tables.py:30#: dashboards/project/data_processing/cluster_templates/tables.py:38 #: dashboards/project/data_processing/clusters/tables.py:32#: dashboards/project/data_processing/cluster_templates/tables.py:54 #: dashboards/project/data_processing/nodegroup_templates/tables.py:46#: dashboards/project/data_processing/cluster_templates/tables.py:61 msgid ""Delete Template"" msgstr ""Delete Template"" #: dashboards/project/data_processing/clusters/tables.py:50 #: dashboards/project/data_processing/data_sources/tables.py:35 #: dashboards/project/data_processing/job_binaries/tables.py:38 #: dashboards/project/data_processing/job_executions/tables.py:31 #: dashboards/project/data_processing/jobs/tables.py:37 #: dashboards/project/data_processing/nodegroup_templates/tables.py:53#: dashboards/project/loadbalancers/tables.py:75 #: dashboards/project/loadbalancers/tables.py:89 #: dashboards/project/loadbalancers/tables.py:104 #: dashboards/project/loadbalancers/tables.py:113 #: dashboards/project/vpn/tables.py:67 dashboards/project/vpn/tables.py:81 #: dashboards/project/vpn/tables.py:95 dashboards/project/vpn/tables.py:109 msgid ""Delete"" msgstr ""Delete"" #: dashboards/project/data_processing/cluster_templates/tables.py:67 #: dashboards/project/data_processing/nodegroup_templates/tables.py:59 msgid ""Templates"" msgstr ""Templates"" #: dashboards/project/data_processing/cluster_templates/tables.py:75 #: dashboards/project/data_processing/nodegroup_templates/tables.py:27#: dashboards/project/data_processing/cluster_templates/tables.py:84#: dashboards/project/data_processing/cluster_templates/tables.py:103#: dashboards/project/data_processing/nodegroup_templates/tables.py:70#: dashboards/project/data_processing/cluster_templates/tables.py:105#: dashboards/project/data_processing/nodegroup_templates/tables.py:72#: dashboards/project/data_processing/cluster_templates/tables.py:107#: dashboards/project/data_processing/nodegroup_templates/tables.py:74#: dashboards/project/data_processing/clusters/tables.py:53 #: dashboards/project/data_processing/clusters/tables.py:106#: dashboards/project/data_processing/clusters/tables.py:40#: dashboards/project/data_processing/clusters/tables.py:52 #: dashboards/project/data_processing/job_executions/tables.py:110 #: dashboards/project/data_processing/job_executions/templates/data_processing.job_executions/_details.html:16 #: dashboards/project/data_processing/jobs/workflows/launch.py:104 msgid ""Cluster"" msgstr ""Cluster"" #: dashboards/project/data_processing/clusters/tables.py:71#: dashboards/project/data_processing/clusters/tables.py:81#: dashboards/project/data_processing/clusters/tables.py:102#: dashboards/project/data_processing/data_image_registry/tables.py:53 #: dashboards/project/data_processing/data_image_registry/tables.py:63#: dashboards/project/data_processing/data_image_registry/tables.py:71#: dashboards/project/data_processing/data_image_registry/tables.py:29#: dashboards/project/data_processing/data_image_registry/tables.py:43#: dashboards/project/data_processing/data_image_registry/tables.py:51 msgid ""Unregister"" msgstr ""Unregister"" #: dashboards/project/data_processing/data_image_registry/tables.py:52 msgid ""Unregistered"" msgstr ""Unregistered"" #: dashboards/project/data_processing/data_image_registry/tables.py:67#: dashboards/project/data_processing/data_sources/tables.py:56#: dashboards/project/data_processing/data_sources/tables.py:27#: dashboards/project/data_processing/data_sources/workflows/create.py:64 #: dashboards/project/data_processing/data_sources/workflows/create.py:88#: dashboards/project/data_processing/data_sources/tables.py:37 msgid ""Data source"" msgstr ""Data source"" msgid ""Data sources"" msgstr ""Data sources""#: dashboards/project/data_processing/data_sources/workflows/create.py:41#: dashboards/project/data_processing/data_sources/workflows/create.py:90#: dashboards/project/data_processing/data_sources/workflows/create.py:91#: dashboards/project/data_processing/job_binaries/tables.py:30#: dashboards/project/data_processing/job_binaries/tables.py:77#: dashboards/project/data_processing/job_binaries/tables.py:40 msgid ""Job binary"" msgstr ""Job binary"" msgid ""Job binaries"" msgstr ""Job binaries"" #: dashboards/project/data_processing/job_binaries/tables.py:61#: dashboards/project/data_processing/job_binaries/tables.py:71#: dashboards/project/data_processing/job_executions/tables.py:124#: dashboards/project/data_processing/job_executions/tables.py:33 msgid ""Job execution"" msgstr ""Job execution"" msgid ""Job executions"" msgstr ""Job executions"" #: dashboards/project/data_processing/job_executions/tables.py:43#: dashboards/project/data_processing/job_executions/tables.py:44 #: dashboards/project/data_processing/job_executions/tables.py:61 #: dashboards/project/data_processing/jobs/tables.py:50 #: dashboards/project/data_processing/jobs/tables.py:67 #: dashboards/project/data_processing/jobs/tables.py:84 #: dashboards/project/data_processing/jobs/workflows/launch.py:331 #: dashboards/project/data_processing/jobs/workflows/launch.py:416 #: dashboards/project/databases/workflows/create_instance.py:273 #: dashboards/project/images/images/tables.py:35 #: dashboards/project/instances/workflows/create_instance.py:758 #: dashboards/project/stacks/templates/stacks/_create.html:24 msgid ""Launch"" msgstr ""Launch"" #: dashboards/project/data_processing/job_executions/tables.py:45 #: dashboards/project/data_processing/job_executions/tables.py:62 #: dashboards/project/data_processing/jobs/tables.py:51 #: dashboards/project/data_processing/jobs/tables.py:68 #: dashboards/project/data_processing/jobs/tables.py:85 msgid ""Launched"" msgstr ""Launched"" #: dashboards/project/data_processing/job_executions/tables.py:46 #: dashboards/project/data_processing/job_executions/tables.py:63 #: dashboards/project/data_processing/job_executions/tables.py:106#: dashboards/project/data_processing/jobs/tables.py:39 #: dashboards/project/data_processing/jobs/tables.py:52 #: dashboards/project/data_processing/jobs/tables.py:69 #: dashboards/project/data_processing/jobs/tables.py:86#: dashboards/project/data_processing/job_executions/tables.py:47 #: dashboards/project/data_processing/job_executions/tables.py:64 #: dashboards/project/data_processing/jobs/panel.py:22 #: dashboards/project/data_processing/jobs/tables.py:40 #: dashboards/project/data_processing/jobs/tables.py:53 #: dashboards/project/data_processing/jobs/tables.py:70 #: dashboards/project/data_processing/jobs/tables.py:87 #: dashboards/project/data_processing/jobs/tables.py:107 #: dashboards/project/data_processing/jobs/templates/data_processing.jobs/jobs.html:6 msgid ""Jobs"" msgstr ""Jobs"" #: dashboards/project/data_processing/job_executions/tables.py:60 msgid ""Relaunch On New Cluster"" msgstr ""Relaunch On New Cluster""#: dashboards/project/data_processing/jobs/tables.py:29#: dashboards/project/data_processing/jobs/tables.py:49#: dashboards/project/data_processing/jobs/tables.py:66 #: dashboards/project/data_processing/jobs/tables.py:83#: dashboards/project/data_processing/jobs/templates/data_processing.jobs/launch.html:3 #: dashboards/project/data_processing/jobs/templates/data_processing.jobs/launch.html:6 #: dashboards/project/data_processing/jobs/workflows/launch.py:330 #: dashboards/project/data_processing/jobs/workflows/launch.py:405 #: dashboards/project/data_processing/jobs/workflows/launch.py:415 msgid ""Launch Job"" msgstr ""Launch Job"" #: dashboards/project/data_processing/nodegroup_templates/tables.py:80#: dashboards/project/data_processing/nodegroup_templates/tables.py:36#: dashboards/project/instances/workflows/create_instance.py:760#: dashboards/project/instances/workflows/create_instance.py:777#: dashboards/project/instances/workflows/create_instance.py:757#: dashboards/project/instances/workflows/create_instance.py:759#: dashboards/project/instances/workflows/create_instance.py:737#: dashboards/project/instances/workflows/create_instance.py:738#: dashboards/project/instances/workflows/create_instance.py:741msgid ""Automatic: Entire disk is single partition and automatically resizes."" msgstr ""Automatic: Entire disk is single partition and automatically resizes."" #: dashboards/project/instances/templates/instances/_launch_advanced_help.html:3 msgid ""Manual: Faster build times but requires manual partitioning."" msgstr ""Manual: Faster build times but requires manual partitioning.""#: dashboards/project/instances/workflows/create_instance.py:745#: dashboards/project/instances/workflows/create_instance.py:774#: dashboards/project/instances/workflows/create_instance.py:832#: dashboards/project/networks/subnets/tables.py:46#: dashboards/project/networks/subnets/tables.py:100#: dashboards/router/nexus1000v/forms.py:182#: dashboards/router/nexus1000v/forms.py:188msgid ""1-4093 for VLAN; 5000-10000 for Overlay"" msgstr ""1-4093 for VLAN; 5000-10000 for Overlay""msgid ""Multicast IPv4 range(e.g. 224.0.0.0-224.0.0.100)"" msgstr ""Multicast IPv4 range(e.g. 224.0.0.0-224.0.0.100)""#: dashboards/router/nexus1000v/forms.py:206msgid ""You may update the editable properties of your network profile here."" msgstr ""You may update the editable properties of your network profile here.""",3063,2720
openstack%2Fmanila~master~Ibe90455675eee8f6df0613a44474643758a65480,openstack/manila,master,Ibe90455675eee8f6df0613a44474643758a65480,Add a section about driver files,ABANDONED,2014-09-26 21:44:29.000000000,2014-09-28 11:12:10.000000000,,"[{'_account_id': 3}, {'_account_id': 6491}, {'_account_id': 8851}, {'_account_id': 11047}, {'_account_id': 11878}]","[{'number': 1, 'created': '2014-09-26 21:44:29.000000000', 'files': ['doc/source/devref/intro.rst'], 'web_link': 'https://opendev.org/openstack/manila/commit/1d23b997d1e44e5b92b35bbfd29df56e11c35a0d', 'message': 'Add a section about driver files\n\nAdd a section about python files included in Manila drivers\nin the intro doc.\n\nChange-Id: Ibe90455675eee8f6df0613a44474643758a65480\n'}]",0,124518,1d23b997d1e44e5b92b35bbfd29df56e11c35a0d,8,5,1,6491,,,0,"Add a section about driver files

Add a section about python files included in Manila drivers
in the intro doc.

Change-Id: Ibe90455675eee8f6df0613a44474643758a65480
",git fetch https://review.opendev.org/openstack/manila refs/changes/18/124518/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/devref/intro.rst'],1,1d23b997d1e44e5b92b35bbfd29df56e11c35a0d,pythonfiles, Python files included in each driver are listed in the following section. Generic ------- The Generic driver includes two files under `manila/share/drivers/` directory: generic.py service_instance.py NetApp Clustered Data ONTAP --------------------------- The NetApp driver consists of the following files under `manila/share/drivers/ netapp/` directory: api.py cluster_mode.py GlusterFS --------- There are two drivers for GlusterFS. Both are under `manila/share/drivers/` directory: glusterfs.py glusterfs_native.py EMC VNX ------- The VNX driver includes two parts: the EMC driver framework and the VNX plugin. The EMC driver framework consists of the following files under `manila/share/drivers/emc/` directory: driver.py plugins/__init__.py plugins/base.py plugins/registry.py The VNX plugin consists of the following files under `manila/share/drivers/emc/plugins/vnx` directory: connection.py helper.py utils.py xml_api_parser.py xml_api_schema.py,,49,0
openstack%2Fneutron~master~I7b5af7e56fd10e073633328740063ea90a5bf226,openstack/neutron,master,I7b5af7e56fd10e073633328740063ea90a5bf226,WIP This is a test,ABANDONED,2014-09-19 08:03:01.000000000,2014-09-28 11:07:27.000000000,,"[{'_account_id': 3}, {'_account_id': 5170}, {'_account_id': 6788}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9695}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10386}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-19 08:03:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/01fadc0a34db3806d59c9f000c1f6c11f1778ea1', 'message': ""WIP This is a test\n\nRemove locking from network and subnet delete op\n\ndelete_subnet in Ml2 plugin instead of using SELECT FOR\nUPDATE deletes the IPAllocations that can be auto-deleted\nstraight away.\nAn exception is raised if there are ports that cannot be\nautodeleted.\n\ndelete_network in ML2 plugin tries to delete all ports\nand subnets before performing the network deletion.\nNo lock is needed here - if some other process modifies\nthe Port or Subnet table, adding new items, the network\ndeletion will fail because of a violation of a foreign\nkey contraint.\nIn that case the operation will be retried.\n\nSome clean up of code I'm preparing to modify\n\nThis patch has a few benign changes that should be easily reviewed.\nThe purpose of this patch is to allow me to make cleaner edits in\nfollow on patches so that they're more easily reviewed in their\nspecific contexts.\n\nRemove SELECT FOR UPDATE in ip allocation code\n\nInstead of using a SELECT FOR UPDATE to ensure consistency\nput in place an optimistic strategy according to the situation:\n1) update the record only if the record contains expected values\nor\n2) deletes the record straight away and perform a check to\nrollback the operation if it wasn't allowed.\n\nIPAvailabilityRange keeps track of which address ranges are still\navailable for allocation. _try_generate_ip and _allocate_specific_ip\nwere locking this table to make sure that only one process could\nmodify the data in the table. With this patch a compare strategy\nis in place. The IPAvailabilityRange is updated only if no other\nprocess modified it. If a modification occurred the operation will\nbe retried after getting a fresh copy of the data from the DB.\nIPAllocation is created right after the IP range update.\n\nSELECT FOR UPDATE is removed from _rebuild_availability_ranges.\nThis function is called if there's no IPAvailabilityRange available\nfor a subnet or by _update_subnet_allocation_pools which removes\nthe IPAvailabilityRange associated with the subnet before calling\n_rebuild_availability_ranges. In any case no IP can be generated\nby another process for this subnet since there's no\nIPAvailabilityRange record in the DB. Two processes could perform\n_rebuild_availability_ranges at the same time, that's why a\nDBDuplicateEntry is catched.\n\nThis change moves DB_MAX_ENTRIES to constants in order to make it\navailable for reference from this new code.\n\nChange-Id: I7b5af7e56fd10e073633328740063ea90a5bf226\n""}, {'number': 2, 'created': '2014-09-19 10:14:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8b02fefc3c3bf4af896d88937468e4a8089e65cd', 'message': ""WIP This is a test\n\nRemove locking from network and subnet delete op\n\ndelete_subnet in Ml2 plugin instead of using SELECT FOR\nUPDATE deletes the IPAllocations that can be auto-deleted\nstraight away.\nAn exception is raised if there are ports that cannot be\nautodeleted.\n\ndelete_network in ML2 plugin tries to delete all ports\nand subnets before performing the network deletion.\nNo lock is needed here - if some other process modifies\nthe Port or Subnet table, adding new items, the network\ndeletion will fail because of a violation of a foreign\nkey contraint.\nIn that case the operation will be retried.\n\nSome clean up of code I'm preparing to modify\n\nThis patch has a few benign changes that should be easily reviewed.\nThe purpose of this patch is to allow me to make cleaner edits in\nfollow on patches so that they're more easily reviewed in their\nspecific contexts.\n\nRemove SELECT FOR UPDATE in ip allocation code\n\nInstead of using a SELECT FOR UPDATE to ensure consistency\nput in place an optimistic strategy according to the situation:\n1) update the record only if the record contains expected values\nor\n2) deletes the record straight away and perform a check to\nrollback the operation if it wasn't allowed.\n\nIPAvailabilityRange keeps track of which address ranges are still\navailable for allocation. _try_generate_ip and _allocate_specific_ip\nwere locking this table to make sure that only one process could\nmodify the data in the table. With this patch a compare strategy\nis in place. The IPAvailabilityRange is updated only if no other\nprocess modified it. If a modification occurred the operation will\nbe retried after getting a fresh copy of the data from the DB.\nIPAllocation is created right after the IP range update.\n\nSELECT FOR UPDATE is removed from _rebuild_availability_ranges.\nThis function is called if there's no IPAvailabilityRange available\nfor a subnet or by _update_subnet_allocation_pools which removes\nthe IPAvailabilityRange associated with the subnet before calling\n_rebuild_availability_ranges. In any case no IP can be generated\nby another process for this subnet since there's no\nIPAvailabilityRange record in the DB. Two processes could perform\n_rebuild_availability_ranges at the same time, that's why a\nDBDuplicateEntry is catched.\n\nThis change moves DB_MAX_ENTRIES to constants in order to make it\navailable for reference from this new code.\n\nChange-Id: I7b5af7e56fd10e073633328740063ea90a5bf226\n""}, {'number': 3, 'created': '2014-09-22 13:01:59.000000000', 'files': ['neutron/common/constants.py', 'neutron/tests/unit/test_db_plugin.py', 'neutron/db/db_base_plugin_v2.py', 'neutron/plugins/ml2/drivers/helpers.py', 'neutron/plugins/ml2/plugin.py', 'neutron/common/exceptions.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/603421df945f0d342ffcab238ff7928242b96b3c', 'message': ""WIP This is a test\n\nRemove SELECT FOR UPDATE in ip allocation code\n\nInstead of using a SELECT FOR UPDATE to ensure consistency\nput in place an optimistic strategy according to the situation:\n1) update the record only if the record contains expected values\nor\n2) deletes the record straight away and perform a check to\nrollback the operation if it wasn't allowed.\n\nIPAvailabilityRange keeps track of which address ranges are still\navailable for allocation. _try_generate_ip and _allocate_specific_ip\nwere locking this table to make sure that only one process could\nmodify the data in the table. With this patch a compare strategy\nis in place. The IPAvailabilityRange is updated only if no other\nprocess modified it. If a modification occurred the operation will\nbe retried after getting a fresh copy of the data from the DB.\nIPAllocation is created right after the IP range update.\n\nSELECT FOR UPDATE is removed from _rebuild_availability_ranges.\nThis function is called if there's no IPAvailabilityRange available\nfor a subnet or by _update_subnet_allocation_pools which removes\nthe IPAvailabilityRange associated with the subnet before calling\n_rebuild_availability_ranges. In any case no IP can be generated\nby another process for this subnet since there's no\nIPAvailabilityRange record in the DB. Two processes could perform\n_rebuild_availability_ranges at the same time, that's why a\nDBDuplicateEntry is catched.\n\nThis change moves DB_MAX_ENTRIES to constants in order to make it\navailable for reference from this new code.\n\nRemove locking from network and subnet delete op\n\ndelete_subnet in Ml2 plugin instead of using SELECT FOR\nUPDATE deletes the IPAllocations that can be auto-deleted\nstraight away.\nAn exception is raised if there are ports that cannot be\nautodeleted.\n\ndelete_network in ML2 plugin tries to delete all ports\nand subnets before performing the network deletion.\nNo lock is needed here - if some other process modifies\nthe Port or Subnet table, adding new items, the network\ndeletion will fail because of a violation of a foreign\nkey contraint.\nIn that case the operation will be retried.\n\nChange-Id: I7b5af7e56fd10e073633328740063ea90a5bf226\n""}]",0,122637,603421df945f0d342ffcab238ff7928242b96b3c,76,21,3,6788,,,0,"WIP This is a test

Remove SELECT FOR UPDATE in ip allocation code

Instead of using a SELECT FOR UPDATE to ensure consistency
put in place an optimistic strategy according to the situation:
1) update the record only if the record contains expected values
or
2) deletes the record straight away and perform a check to
rollback the operation if it wasn't allowed.

IPAvailabilityRange keeps track of which address ranges are still
available for allocation. _try_generate_ip and _allocate_specific_ip
were locking this table to make sure that only one process could
modify the data in the table. With this patch a compare strategy
is in place. The IPAvailabilityRange is updated only if no other
process modified it. If a modification occurred the operation will
be retried after getting a fresh copy of the data from the DB.
IPAllocation is created right after the IP range update.

SELECT FOR UPDATE is removed from _rebuild_availability_ranges.
This function is called if there's no IPAvailabilityRange available
for a subnet or by _update_subnet_allocation_pools which removes
the IPAvailabilityRange associated with the subnet before calling
_rebuild_availability_ranges. In any case no IP can be generated
by another process for this subnet since there's no
IPAvailabilityRange record in the DB. Two processes could perform
_rebuild_availability_ranges at the same time, that's why a
DBDuplicateEntry is catched.

This change moves DB_MAX_ENTRIES to constants in order to make it
available for reference from this new code.

Remove locking from network and subnet delete op

delete_subnet in Ml2 plugin instead of using SELECT FOR
UPDATE deletes the IPAllocations that can be auto-deleted
straight away.
An exception is raised if there are ports that cannot be
autodeleted.

delete_network in ML2 plugin tries to delete all ports
and subnets before performing the network deletion.
No lock is needed here - if some other process modifies
the Port or Subnet table, adding new items, the network
deletion will fail because of a violation of a foreign
key contraint.
In that case the operation will be retried.

Change-Id: I7b5af7e56fd10e073633328740063ea90a5bf226
",git fetch https://review.opendev.org/openstack/neutron refs/changes/37/122637/2 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/common/constants.py', 'neutron/tests/unit/test_db_plugin.py', 'neutron/db/db_base_plugin_v2.py', 'neutron/plugins/ml2/drivers/helpers.py', 'neutron/common/exceptions.py', 'neutron/plugins/ml2/plugin.py']",6,01fadc0a34db3806d59c9f000c1f6c11f1778ea1,," # Locking isn't necessary. Foreign-key constraints prevent # deletion if concurrent creation happens. with session.begin(subtransactions=True): filter_by(network_id=id).all()) filter_by(network_id=id).all()) with session.begin(subtransactions=True): filter( models_v2.Port.device_owner. in_(db_base_plugin_v2.AUTO_DELETE_PORT_OWNERS) ).all()) # Delete all the IPAllocation that can be auto-deleted if allocated: map(session.delete, allocated) LOG.debug(""Ports to auto-deallocate: %s"", allocated) # Check if there are tenant owned ports tenant_port = (session.query(models_v2.IPAllocation). filter_by(subnet_id=id). join(models_v2.Port). first()) if tenant_port: LOG.debug(""Tenant-owned ports exist"") # If allocated is None, then all the IPAllocation were # correctly deleted during the previous pass."," # REVISIT(rkukura): Its not clear that # with_lockmode('update') is really needed in this # transaction, and if not, the semaphore can also be # removed. # # REVISIT: Serialize this operation with a semaphore # to prevent deadlock waiting to acquire a DB lock # held by another thread in the same process, leading # to 'lock wait timeout' errors. with contextlib.nested(lockutils.lock('db-access'), session.begin(subtransactions=True)): filter_by(network_id=id). with_lockmode('update').all()) filter_by(network_id=id). with_lockmode('update').all()) # REVISIT: Serialize this operation with a semaphore to # prevent deadlock waiting to acquire a DB lock held by # another thread in the same process, leading to 'lock # wait timeout' errors. with contextlib.nested(lockutils.lock('db-access'), session.begin(subtransactions=True)): filter_by(network_id=subnet['network_id']). with_lockmode('update').all()) LOG.debug(_(""Ports to auto-deallocate: %s""), allocated) only_auto_del = all(not a.port_id or a.ports.device_owner in db_base_plugin_v2. AUTO_DELETE_PORT_OWNERS for a in allocated) if not only_auto_del: LOG.debug(_(""Tenant-owned ports exist"")) session.delete(a)",301,146
openstack%2Fironic~master~I7b712755ff90fa7b76a8893a247b951e626e4c76,openstack/ironic,master,I7b712755ff90fa7b76a8893a247b951e626e4c76,Fix malformed line in requirements.txt,ABANDONED,2014-09-27 21:53:47.000000000,2014-09-28 08:01:32.000000000,,"[{'_account_id': 3}, {'_account_id': 2889}, {'_account_id': 9608}, {'_account_id': 12081}]","[{'number': 1, 'created': '2014-09-27 21:53:47.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/ironic/commit/2d093c44ab8aa50fe55a6dfe4f7a2e92a7aec3c0', 'message': 'Fix malformed line in requirements.txt\n\nEvery other comment is two spaces away from package name.\n\nChange-Id: I7b712755ff90fa7b76a8893a247b951e626e4c76\n'}]",0,124608,2d093c44ab8aa50fe55a6dfe4f7a2e92a7aec3c0,9,4,1,9608,,,0,"Fix malformed line in requirements.txt

Every other comment is two spaces away from package name.

Change-Id: I7b712755ff90fa7b76a8893a247b951e626e4c76
",git fetch https://review.opendev.org/openstack/ironic refs/changes/08/124608/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,2d093c44ab8aa50fe55a6dfe4f7a2e92a7aec3c0,fix_malform,oslo.utils>=1.0.0 # Apache-2.0,oslo.utils>=1.0.0 # Apache-2.0,1,1
openstack%2Fpython-openstackclient~master~I2a8250d0b01651563cfe74704ce5a9f97dd9fdf4,openstack/python-openstackclient,master,I2a8250d0b01651563cfe74704ce5a9f97dd9fdf4,Updated from global requirements,MERGED,2014-09-24 11:40:08.000000000,2014-09-28 07:42:35.000000000,2014-09-28 07:42:34.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 6482}, {'_account_id': 8736}]","[{'number': 1, 'created': '2014-09-24 11:40:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/e28c0aab5d048110e86ed65710abe1f56a2bfeae', 'message': 'Updated from global requirements\n\nChange-Id: I2a8250d0b01651563cfe74704ce5a9f97dd9fdf4\n'}, {'number': 2, 'created': '2014-09-25 19:08:05.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/3ddd4e2646c4db4172c3d74a0f51eb6d3e91b176', 'message': 'Updated from global requirements\n\nChange-Id: I2a8250d0b01651563cfe74704ce5a9f97dd9fdf4\n'}]",0,123701,3ddd4e2646c4db4172c3d74a0f51eb6d3e91b176,11,4,2,11131,,,0,"Updated from global requirements

Change-Id: I2a8250d0b01651563cfe74704ce5a9f97dd9fdf4
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/01/123701/2 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,e28c0aab5d048110e86ed65710abe1f56a2bfeae,openstack/requirements,python-cinderclient>=1.1.0,python-cinderclient>=1.0.7,1,1
openstack%2Fsecurity-doc~master~Ie113558ee909d3ea551901999f8024653fe6b3f6,openstack/security-doc,master,Ie113558ee909d3ea551901999f8024653fe6b3f6,Imported Translations from Transifex,MERGED,2014-09-28 06:01:25.000000000,2014-09-28 07:42:13.000000000,2014-09-28 07:42:13.000000000,"[{'_account_id': 3}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-09-28 06:01:25.000000000', 'files': ['security-guide/locale/security-guide.pot', 'security-guide/locale/ja.po'], 'web_link': 'https://opendev.org/openstack/security-doc/commit/43793e563a36c76a16a8c628d991154c4f6f045d', 'message': 'Imported Translations from Transifex\n\nChange-Id: Ie113558ee909d3ea551901999f8024653fe6b3f6\n'}]",0,124615,43793e563a36c76a16a8c628d991154c4f6f045d,6,2,1,11131,,,0,"Imported Translations from Transifex

Change-Id: Ie113558ee909d3ea551901999f8024653fe6b3f6
",git fetch https://review.opendev.org/openstack/security-doc refs/changes/15/124615/1 && git format-patch -1 --stdout FETCH_HEAD,"['security-guide/locale/security-guide.pot', 'security-guide/locale/ja.po']",2,43793e563a36c76a16a8c628d991154c4f6f045d,transifex/translations,"""POT-Creation-Date: 2014-09-27 10:54+0000\n"" ""PO-Revision-Date: 2014-09-27 10:54+0000\n""#: ./security-guide/section_hypervisor-selection.xml684(title)#: ./security-guide/section_hypervisor-selection.xml610(title)"" attack. In academic studies attackers were able to identify software "" ""packages and versions running on neighboring virtual machines as well as "" ""software downloads and other sensitive information through analyzing memory "" ""access times on the attacker VM."" msgstr """" #: ./security-guide/section_hypervisor-selection.xml604(para)#: ./security-guide/section_hypervisor-selection.xml611(para)#: ./security-guide/section_hypervisor-selection.xml617(para)#: ./security-guide/section_hypervisor-selection.xml632(para)#: ./security-guide/section_hypervisor-selection.xml633(para)#: ./security-guide/section_hypervisor-selection.xml634(para)#: ./security-guide/section_hypervisor-selection.xml635(para)#: ./security-guide/section_hypervisor-selection.xml636(para)#: ./security-guide/section_hypervisor-selection.xml637(para)#: ./security-guide/section_hypervisor-selection.xml640(para)#: ./security-guide/section_hypervisor-selection.xml649(para)#: ./security-guide/section_hypervisor-selection.xml658(para)#: ./security-guide/section_hypervisor-selection.xml667(para)#: ./security-guide/section_hypervisor-selection.xml677(para)#: ./security-guide/section_hypervisor-selection.xml679(para)#: ./security-guide/section_hypervisor-selection.xml687(para) msgid """" ""Fine grain Cross-VM Attacks on Xen and VMware are possible - Apecechea and "" ""others. <link "" ""href=\""https://eprint.iacr.org/2014/248.pdf\"">https://eprint.iacr.org/2014/248.pdf</link>"" msgstr ""Fine grain Cross-VM Attacks on Xen and VMware are possible - Apecechea and others. <link href=\""https://eprint.iacr.org/2014/248.pdf\"">https://eprint.iacr.org/2014/248.pdf</link>"" #: ./security-guide/section_hypervisor-selection.xml695(para) msgid """" ""Memory Deduplication as a Threat to the Guest OS - Suzaki and others. <link "" ""href=\""https://staff.aist.go.jp/c.artho/papers/EuroSec2011-suzaki.pdf\"">https://staff.aist.go.jp/c.artho/papers/EuroSec2011-suzaki.pdf</link>"" msgstr ""Memory Deduplication as a Threat to the Guest OS - Suzaki and others. <link href=\""https://staff.aist.go.jp/c.artho/papers/EuroSec2011-suzaki.pdf\"">https://staff.aist.go.jp/c.artho/papers/EuroSec2011-suzaki.pdf</link>"" #: ./security-guide/section_hypervisor-selection.xml703(link) msgid ""KVM: Kernel Samepage Merging"" msgstr ""KVM: Kernel Samepage Merging"" #: ./security-guide/section_hypervisor-selection.xml709(link) msgid ""XSM: Xen Security Modules"" msgstr ""XSM: Xen "" #: ./security-guide/section_hypervisor-selection.xml713(link) msgid ""xVirt: Mandatory Access Control for Linux-based virtualization"" msgstr ""xVirt: Linux "" #: ./security-guide/section_hypervisor-selection.xml717(link) msgid ""TXT: Intel Trusted Execution Technology"" msgstr ""TXT: Intel Trusted Execution Technology"" #: ./security-guide/section_hypervisor-selection.xml723(link) msgid ""AppArmor: Linux security module implementing MAC"" msgstr ""AppArmor: MAC  Linux "" #: ./security-guide/section_hypervisor-selection.xml729(link) msgid ""cgroups: Linux kernel feature to control resource usage"" msgstr ""cgroups:  Linux "" ","""POT-Creation-Date: 2014-09-23 21:32+0000\n"" ""PO-Revision-Date: 2014-09-23 21:32+0000\n""#: ./security-guide/section_hypervisor-selection.xml620(title)#: ./security-guide/section_hypervisor-selection.xml601(para) msgid """" ""Fine grain Cross-VM Attacks on Xen and VMware are possible - Apecechea and "" ""others. <link "" ""href=\""https://eprint.iacr.org/2014/248.pdf\"">https://eprint.iacr.org/2014/248.pdf</link>"" msgstr ""Fine grain Cross-VM Attacks on Xen and VMware are possible - Apecechea and others. <link href=\""https://eprint.iacr.org/2014/248.pdf\"">https://eprint.iacr.org/2014/248.pdf</link>"" #: ./security-guide/section_hypervisor-selection.xml606(para) msgid """" ""Memory Deduplication as a Threat to the Guest OS - Suzaki and others. <link "" ""href=\""https://staff.aist.go.jp/c.artho/papers/EuroSec2011-suzaki.pdf\"">https://staff.aist.go.jp/c.artho/papers/EuroSec2011-suzaki.pdf</link>"" msgstr ""Memory Deduplication as a Threat to the Guest OS - Suzaki and others. <link href=\""https://staff.aist.go.jp/c.artho/papers/EuroSec2011-suzaki.pdf\"">https://staff.aist.go.jp/c.artho/papers/EuroSec2011-suzaki.pdf</link>"" "" attack. In academic studies<placeholder-1/><placeholder-2/>attackers were "" ""able to identify software packages and versions running on neighboring "" ""virtual machines as well as software downloads and other sensitive "" ""information through analyzing memory access times on the attacker VM."" msgstr ""KSM  TPS <placeholder-1/><placeholder-2/>"" #: ./security-guide/section_hypervisor-selection.xml614(para)#: ./security-guide/section_hypervisor-selection.xml621(para)#: ./security-guide/section_hypervisor-selection.xml627(para)#: ./security-guide/section_hypervisor-selection.xml642(para)#: ./security-guide/section_hypervisor-selection.xml643(para)#: ./security-guide/section_hypervisor-selection.xml644(para)#: ./security-guide/section_hypervisor-selection.xml645(para)#: ./security-guide/section_hypervisor-selection.xml646(para)#: ./security-guide/section_hypervisor-selection.xml647(para)#: ./security-guide/section_hypervisor-selection.xml650(para)#: ./security-guide/section_hypervisor-selection.xml659(para)#: ./security-guide/section_hypervisor-selection.xml668(para)#: ./security-guide/section_hypervisor-selection.xml677(para)#: ./security-guide/section_hypervisor-selection.xml687(link) msgid ""KVM: Kernel Samepage Merging"" msgstr ""KVM: Kernel Samepage Merging"" #: ./security-guide/section_hypervisor-selection.xml691(link) msgid ""XSM: Xen Security Modules"" msgstr ""XSM: Xen "" #: ./security-guide/section_hypervisor-selection.xml693(link) msgid ""xVirt: Mandatory Access Control for Linux-based virtualization"" msgstr ""xVirt: Linux "" #: ./security-guide/section_hypervisor-selection.xml695(link) msgid ""TXT: Intel Trusted Execution Technology"" msgstr ""TXT: Intel Trusted Execution Technology"" #: ./security-guide/section_hypervisor-selection.xml699(link) msgid ""AppArmor: Linux security module implementing MAC"" msgstr ""AppArmor: MAC  Linux "" #: ./security-guide/section_hypervisor-selection.xml703(link) msgid ""cgroups: Linux kernel feature to control resource usage"" msgstr ""cgroups:  Linux "" #: ./security-guide/section_hypervisor-selection.xml705(para)#: ./security-guide/section_hypervisor-selection.xml707(para)",112,111
openstack%2Fopenstack-manuals~master~Ic52a46a093b4694e9fa175ef4d5bbcb22a2eced1,openstack/openstack-manuals,master,Ic52a46a093b4694e9fa175ef4d5bbcb22a2eced1,Imported Translations from Transifex,MERGED,2014-09-28 06:10:05.000000000,2014-09-28 07:40:18.000000000,2014-09-28 07:40:18.000000000,"[{'_account_id': 3}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-09-28 06:10:05.000000000', 'files': ['doc/config-reference/locale/config-reference.pot', 'doc/common/locale/common.pot', 'doc/user-guide-admin/locale/user-guide-admin.pot', 'doc/install-guide/locale/ja.po', 'doc/common/locale/ja.po', 'doc/user-guide-admin/locale/ja.po', 'doc/cli-reference/locale/cli-reference.pot', 'doc/common/locale/fr.po'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/3548527fe13d38876b1791d5867c68664d20d6bc', 'message': 'Imported Translations from Transifex\n\nChange-Id: Ic52a46a093b4694e9fa175ef4d5bbcb22a2eced1\n'}]",0,124616,3548527fe13d38876b1791d5867c68664d20d6bc,6,2,1,11131,,,0,"Imported Translations from Transifex

Change-Id: Ic52a46a093b4694e9fa175ef4d5bbcb22a2eced1
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/16/124616/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/config-reference/locale/config-reference.pot', 'doc/common/locale/common.pot', 'doc/user-guide-admin/locale/user-guide-admin.pot', 'doc/install-guide/locale/ja.po', 'doc/common/locale/ja.po', 'doc/user-guide-admin/locale/ja.po', 'doc/cli-reference/locale/cli-reference.pot', 'doc/common/locale/fr.po']",8,3548527fe13d38876b1791d5867c68664d20d6bc,transifex/translations,"""POT-Creation-Date: 2014-09-27 10:51+0000\n"" ""PO-Revision-Date: 2014-09-27 10:50+0000\n""#: ./doc/common/section_support-compute.xml7(title)#: ./doc/common/section_support-compute.xml8(para)#: ./doc/common/section_support-compute.xml17(title)#: ./doc/common/section_support-compute.xml18(para)#: ./doc/common/section_support-compute.xml27(literal)#: ./doc/common/section_support-compute.xml30(literal)#: ./doc/common/section_support-compute.xml33(para)#: ./doc/common/section_support-compute.xml40(para)#: ./doc/common/section_support-compute.xml48(title)#: ./doc/common/section_support-compute.xml49(para)#: ./doc/common/section_support-compute.xml53(para)#: ./doc/common/section_support-compute.xml57(para)#: ./doc/common/section_support-compute.xml62(para)""Package: Displays information about the package to which the process "" ""belongs, including version information.""""Threads: Displays stack traces and thread IDs for each of the threads within"" "" the process.""""Green Threads: Displays stack traces for each of the green threads within "" ""the process (green threads do not have thread IDs)."" msgstr """" #: ./doc/common/section_support-compute.xml81(para) msgid """" ""Configuration: Lists all configuration options currently accessible through "" ""the CONF object for the current process."" msgstr """" #: ./doc/common/section_support-compute.xml59(para) msgid ""The report has the following sections: <placeholder-1/>"" msgstr """" #: ./doc/common/section_support-compute.xml88(para) msgid """"#: ./doc/common/section_support-compute.xml93(title)#: ./doc/common/section_support-compute.xml94(para)#: ./doc/common/section_support-compute.xml102(title)#: ./doc/common/section_support-compute.xml109(para)#: ./doc/common/section_support-compute.xml119(para)#: ./doc/common/section_support-compute.xml104(para)#: ./doc/common/section_support-compute.xml126(para)#: ./doc/common/section_support-compute.xml134(para)#: ./doc/common/section_support-compute.xml139(title)#: ./doc/common/section_support-compute.xml140(para)#: ./doc/common/section_support-compute.xml150(para)#: ./doc/common/section_support-compute.xml159(filename)#: ./doc/common/section_support-compute.xml162(filename)#: ./doc/common/section_support-compute.xml165(filename)#: ./doc/common/section_support-compute.xml168(filename)#: ./doc/common/section_support-compute.xml171(filename)#: ./doc/common/section_support-compute.xml174(para)#: ./doc/common/section_support-compute.xml178(para)#: ./doc/common/section_support-compute.xml183(para)#: ./doc/common/section_support-compute.xml186(para)#: ./doc/common/section_support-compute.xml190(para)#: ./doc/common/section_support-compute.xml197(title)#: ./doc/common/section_support-compute.xml198(para)#: ./doc/common/section_support-compute.xml205(para)#: ./doc/common/section_support-compute.xml212(para)#: ./doc/common/section_support-compute.xml217(title)#: ./doc/common/section_support-compute.xml218(para)#: ./doc/common/section_support-compute.xml225(para)#: ./doc/common/section_support-compute.xml231(title)#: ./doc/common/section_support-compute.xml232(para)#: ./doc/common/section_support-compute.xml234(para)#: ./doc/common/section_support-compute.xml239(para)","""POT-Creation-Date: 2014-09-23 05:56+0000\n"" ""PO-Revision-Date: 2014-09-22 15:21+0000\n""#: ./doc/common/section_support-compute.xml11(title)#: ./doc/common/section_support-compute.xml12(para)#: ./doc/common/section_support-compute.xml21(title)#: ./doc/common/section_support-compute.xml22(para)#: ./doc/common/section_support-compute.xml31(literal)#: ./doc/common/section_support-compute.xml34(literal)#: ./doc/common/section_support-compute.xml37(para)#: ./doc/common/section_support-compute.xml44(para)#: ./doc/common/section_support-compute.xml52(title)#: ./doc/common/section_support-compute.xml53(para)#: ./doc/common/section_support-compute.xml57(para)#: ./doc/common/section_support-compute.xml61(para)#: ./doc/common/section_support-compute.xml65(para)""Package Displays information about the package to which the process belongs,"" "" including version information."" msgstr """" #: ./doc/common/section_support-compute.xml67(para) msgid """" ""Threads Displays stack traces and thread IDs for each of the threads within "" ""the process.""""Green Threads Displays stack traces for each of the green threads within the"" "" process (green threads do not have thread IDs)."" msgstr """" #: ./doc/common/section_support-compute.xml71(para) msgid """" ""Configuration Lists all configuration options currently accessible through "" ""the CONF object for the current process."" msgstr """" #: ./doc/common/section_support-compute.xml63(para) msgid ""The report has the following sections: <placeholder-1/>""#: ./doc/common/section_support-compute.xml79(title)#: ./doc/common/section_support-compute.xml80(para)#: ./doc/common/section_support-compute.xml88(title)#: ./doc/common/section_support-compute.xml95(para)#: ./doc/common/section_support-compute.xml105(para)#: ./doc/common/section_support-compute.xml90(para)#: ./doc/common/section_support-compute.xml112(para)#: ./doc/common/section_support-compute.xml120(para)#: ./doc/common/section_support-compute.xml125(title)#: ./doc/common/section_support-compute.xml126(para)#: ./doc/common/section_support-compute.xml136(para)#: ./doc/common/section_support-compute.xml145(filename)#: ./doc/common/section_support-compute.xml148(filename)#: ./doc/common/section_support-compute.xml151(filename)#: ./doc/common/section_support-compute.xml154(filename)#: ./doc/common/section_support-compute.xml157(filename)#: ./doc/common/section_support-compute.xml160(para)#: ./doc/common/section_support-compute.xml164(para)#: ./doc/common/section_support-compute.xml169(para)#: ./doc/common/section_support-compute.xml172(para)#: ./doc/common/section_support-compute.xml176(para)#: ./doc/common/section_support-compute.xml183(title)#: ./doc/common/section_support-compute.xml184(para)#: ./doc/common/section_support-compute.xml191(para)#: ./doc/common/section_support-compute.xml198(para)#: ./doc/common/section_support-compute.xml203(title)#: ./doc/common/section_support-compute.xml204(para)#: ./doc/common/section_support-compute.xml211(para)#: ./doc/common/section_support-compute.xml217(title)#: ./doc/common/section_support-compute.xml218(para)#: ./doc/common/section_support-compute.xml220(para)#: ./doc/common/section_support-compute.xml225(para)",634,622
openstack%2Fpython-openstackclient~master~Idc40428e30e59f71dbdbfa0555c0066fddc441c2,openstack/python-openstackclient,master,Idc40428e30e59f71dbdbfa0555c0066fddc441c2,utils.find_resource does not catch right exception,MERGED,2014-09-24 03:24:24.000000000,2014-09-28 07:39:20.000000000,2014-09-28 07:39:19.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 8736}]","[{'number': 1, 'created': '2014-09-24 03:24:24.000000000', 'files': ['openstackclient/common/utils.py'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/7029cf37e268a789a65bab3b9a16e4491854106e', 'message': 'utils.find_resource does not catch right exception\n\nCurrently, utils.find_resource catch NotFound exception defined in\nopenstackclient. However, different client libraries raise different\nexceptions defined in thire own library.\n\nChange-Id: Idc40428e30e59f71dbdbfa0555c0066fddc441c2\nCloses-Bug: #1371924\n'}]",0,123621,7029cf37e268a789a65bab3b9a16e4491854106e,7,3,1,9101,,,0,"utils.find_resource does not catch right exception

Currently, utils.find_resource catch NotFound exception defined in
openstackclient. However, different client libraries raise different
exceptions defined in thire own library.

Change-Id: Idc40428e30e59f71dbdbfa0555c0066fddc441c2
Closes-Bug: #1371924
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/21/123621/1 && git format-patch -1 --stdout FETCH_HEAD,['openstackclient/common/utils.py'],1,7029cf37e268a789a65bab3b9a16e4491854106e,bug/1371924, # FIXME(dtroyer): The exception to catch here is dependent on which # client library the manager passed in belongs to. # Eventually this should be pulled from a common set # of client exceptions. except Exception as ex: if type(ex).__name__ == 'NotFound': pass else: raise, except exceptions.NotFound: pass,9,2
openstack%2Fneutron~master~Idd770a85a9eabff112d9613e75d8bb524020234a,openstack/neutron,master,Idd770a85a9eabff112d9613e75d8bb524020234a,Use call to report state when ovs_agent starts up,MERGED,2014-07-22 13:32:30.000000000,2014-09-28 07:28:12.000000000,2014-09-19 11:02:09.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 261}, {'_account_id': 841}, {'_account_id': 1923}, {'_account_id': 5170}, {'_account_id': 6502}, {'_account_id': 6659}, {'_account_id': 6788}, {'_account_id': 6854}, {'_account_id': 7016}, {'_account_id': 7805}, {'_account_id': 8124}, {'_account_id': 8213}, {'_account_id': 8645}, {'_account_id': 8646}, {'_account_id': 9681}, {'_account_id': 9732}, {'_account_id': 9820}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 11825}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-07-22 13:32:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/551b8993a2ee0025b3f927c48256e28b56ce524b', 'message': 'Use call to report state when ovs_agent starts up\n\nMake sure to report ovs_agent state when starting up, otherwise\n, ovs_agent start_time would not be updated and thus l2pop does\nnot send fdb entries.\n\nChange-Id: Idd770a85a9eabff112d9613e75d8bb524020234a\n'}, {'number': 2, 'created': '2014-07-22 13:34:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/d96754db111b9362564e23ed8cde87aec84a689d', 'message': 'Use call to report state when ovs_agent starts up\n\nMake sure to report ovs_agent state successfully when starting up,\notherwise, ovs_agent start_time would not be updated and thus l2pop\ndoes not send fdb entries.\n\nChange-Id: Idd770a85a9eabff112d9613e75d8bb524020234a\n'}, {'number': 3, 'created': '2014-07-22 13:44:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/fd457c4cbd1ea239d0f4f4572faee20e3c13ea71', 'message': 'Use call to report state when ovs_agent starts up\n\nMake sure to report ovs_agent state successfully when starting up,\notherwise, if losing start_flag message, ovs_agent start_time would\nnot be updated and thus l2pop does not send fdb entries.\n\nChange-Id: Idd770a85a9eabff112d9613e75d8bb524020234a\n'}, {'number': 4, 'created': '2014-07-23 08:45:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/35f1f7226da0ca1f02487a404d8b1b2949a2b2a4', 'message': 'Use call to report state when ovs_agent starts up\n\nMake sure to report ovs_agent state when starting up, otherwise\n, ovs_agent start_time would not be updated and thus l2pop does\nnot send fdb entries.\n\nChange-Id: Idd770a85a9eabff112d9613e75d8bb524020234a\nCloses-Bug: #1347452\n'}, {'number': 5, 'created': '2014-07-25 02:00:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/cd7b51ecbf43aaab90756877dad888dbcfb62b78', 'message': 'Use call to report state when ovs_agent starts up\n\nMake sure to report ovs_agent state when starting up, otherwise\n, ovs_agent start_time would not be updated and thus l2pop does\nnot send fdb entries.\n\nChange-Id: Idd770a85a9eabff112d9613e75d8bb524020234a\nCloses-Bug: #1347452\n'}, {'number': 6, 'created': '2014-08-06 07:16:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/674eaef06e665e70e83e9fb21922b64c79c44d66', 'message': 'Use call to report state when ovs_agent starts up\n\nMake sure to report ovs_agent state when starting up, otherwise\n, ovs_agent start_time would not be updated and thus l2pop does\nnot send fdb entries.\n\nChange-Id: Idd770a85a9eabff112d9613e75d8bb524020234a\nCloses-Bug: #1347452\n'}, {'number': 7, 'created': '2014-08-12 02:22:51.000000000', 'files': ['neutron/tests/unit/openvswitch/test_ovs_neutron_agent.py', 'neutron/plugins/openvswitch/agent/ovs_neutron_agent.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/46501246c202f1f424f0d2a05769926303a39c89', 'message': 'Use call to report state when ovs_agent starts up\n\nMake sure to report ovs_agent state when starting up, otherwise\novs_agent start_time would not be updated and thus l2pop does\nnot send fdb entries.\n\nChange-Id: Idd770a85a9eabff112d9613e75d8bb524020234a\nCloses-Bug: #1347452\n'}]",21,108693,46501246c202f1f424f0d2a05769926303a39c89,185,36,7,8646,,,0,"Use call to report state when ovs_agent starts up

Make sure to report ovs_agent state when starting up, otherwise
ovs_agent start_time would not be updated and thus l2pop does
not send fdb entries.

Change-Id: Idd770a85a9eabff112d9613e75d8bb524020234a
Closes-Bug: #1347452
",git fetch https://review.opendev.org/openstack/neutron refs/changes/93/108693/7 && git format-patch -1 --stdout FETCH_HEAD,['neutron/plugins/openvswitch/agent/ovs_neutron_agent.py'],1,551b8993a2ee0025b3f927c48256e28b56ce524b,bug/ovsagent_report_startflag," self.use_call = True self.agent_state, self.use_call) self.use_call = False", self.agent_state),4,1
openstack%2Ftaskflow~master~I2a93c8e55f0d692df6d074bd86ab1a5e6d11a03f,openstack/taskflow,master,I2a93c8e55f0d692df6d074bd86ab1a5e6d11a03f,Adjust docs+venv tox environments requirements/dependencies,MERGED,2014-09-27 21:44:12.000000000,2014-09-28 05:42:42.000000000,2014-09-28 05:42:41.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 9608}]","[{'number': 1, 'created': '2014-09-27 21:44:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/7fc1e4c5e2e7ee777149bcf3069430a2f708922b', 'message': 'Adjust docs & venv virtualenv requirements/dependencies\n\nAfter a prior merge these sections are no longer valid and\nneed to be adjusted to reflect the removal of the optional\nrequirements file.\n\nChange-Id: I2a93c8e55f0d692df6d074bd86ab1a5e6d11a03f\n'}, {'number': 2, 'created': '2014-09-27 22:03:30.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/8178e7811ca0f39fdaad2ca70fff9ffc18751534', 'message': 'Adjust docs+venv tox environments requirements/dependencies\n\nAfter a prior merge these sections are no longer valid and\nneed to be adjusted to reflect the removal of the optional\nrequirements file.\n\nChange-Id: I2a93c8e55f0d692df6d074bd86ab1a5e6d11a03f\n'}]",0,124607,8178e7811ca0f39fdaad2ca70fff9ffc18751534,8,3,2,1297,,,0,"Adjust docs+venv tox environments requirements/dependencies

After a prior merge these sections are no longer valid and
need to be adjusted to reflect the removal of the optional
requirements file.

Change-Id: I2a93c8e55f0d692df6d074bd86ab1a5e6d11a03f
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/07/124607/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,7fc1e4c5e2e7ee777149bcf3069430a2f708922b,,deps = {[testenv:py27]deps}basepython = python2.7 deps = {[testenv:py27]deps},deps = -r{toxinidir}/requirements-py2.txt -r{toxinidir}/test-requirements.txt -r{toxinidir}/optional-requirements.txt doc8,3,4
openstack%2Foslo.db~master~I293172684fcff520ae445836bb132bf1a919d127,openstack/oslo.db,master,I293172684fcff520ae445836bb132bf1a919d127,Imported Translations from Transifex,MERGED,2014-09-26 06:04:12.000000000,2014-09-28 05:41:54.000000000,2014-09-28 05:41:54.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 7491}]","[{'number': 1, 'created': '2014-09-26 06:04:12.000000000', 'files': ['oslo.db/locale/fr/LC_MESSAGES/oslo.db-log-critical.po'], 'web_link': 'https://opendev.org/openstack/oslo.db/commit/b42d8f1c13270689081f0ac5000e39ecf10ad754', 'message': 'Imported Translations from Transifex\n\nChange-Id: I293172684fcff520ae445836bb132bf1a919d127\n'}]",0,124283,b42d8f1c13270689081f0ac5000e39ecf10ad754,13,4,1,11131,,,0,"Imported Translations from Transifex

Change-Id: I293172684fcff520ae445836bb132bf1a919d127
",git fetch https://review.opendev.org/openstack/oslo.db refs/changes/83/124283/1 && git format-patch -1 --stdout FETCH_HEAD,['oslo.db/locale/fr/LC_MESSAGES/oslo.db-log-critical.po'],1,b42d8f1c13270689081f0ac5000e39ecf10ad754,transifex/translations,"# Translations template for heat. # Copyright (C) 2014 ORGANIZATION # This file is distributed under the same license as the heat project. # # Translators: # Maxime COQUEREL <max.coquerel@gmail.com>, 2014 msgid """" msgstr """" ""Project-Id-Version: oslo.db\n"" ""Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"" ""POT-Creation-Date: 2014-09-26 06:04+0000\n"" ""PO-Revision-Date: 2014-09-25 08:51+0000\n"" ""Last-Translator: Maxime COQUEREL <max.coquerel@gmail.com>\n"" ""Language-Team: French (http://www.transifex.com/projects/p/oslodb/language/"" ""fr/)\n"" ""Language: fr\n"" ""MIME-Version: 1.0\n"" ""Content-Type: text/plain; charset=UTF-8\n"" ""Content-Transfer-Encoding: 8bit\n"" ""Generated-By: Babel 1.3\n"" ""Plural-Forms: nplurals=2; plural=(n > 1);\n"" ",,21,0
openstack%2Foslo.log~master~I4008881f9626f90d68f0a220702eb1b2d6359419,openstack/oslo.log,master,I4008881f9626f90d68f0a220702eb1b2d6359419,Imported Translations from Transifex,MERGED,2014-09-26 06:06:14.000000000,2014-09-28 05:41:48.000000000,2014-09-28 05:41:47.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-09-26 06:06:14.000000000', 'files': ['oslo.log/locale/oslo.log-log-error.pot', 'oslo.log/locale/oslo.log.pot', 'oslo.log/locale/oslo.log-log-warning.pot', 'oslo.log/locale/oslo.log-log-info.pot', 'oslo.log/locale/fr/LC_MESSAGES/oslo.log-log-critical.po'], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/bc9f5d4a8835428961e865df53134fe5ffe15960', 'message': 'Imported Translations from Transifex\n\nChange-Id: I4008881f9626f90d68f0a220702eb1b2d6359419\n'}]",0,124284,bc9f5d4a8835428961e865df53134fe5ffe15960,12,3,1,11131,,,0,"Imported Translations from Transifex

Change-Id: I4008881f9626f90d68f0a220702eb1b2d6359419
",git fetch https://review.opendev.org/openstack/oslo.log refs/changes/84/124284/1 && git format-patch -1 --stdout FETCH_HEAD,"['oslo.log/locale/oslo.log-log-error.pot', 'oslo.log/locale/oslo.log.pot', 'oslo.log/locale/oslo.log-log-warning.pot', 'oslo.log/locale/oslo.log-log-info.pot', 'oslo.log/locale/fr/LC_MESSAGES/oslo.log-log-critical.po']",5,bc9f5d4a8835428961e865df53134fe5ffe15960,transifex/translations,"# Translations template for heat. # Copyright (C) 2014 ORGANIZATION # This file is distributed under the same license as the heat project. # # Translators: # Maxime COQUEREL <max.coquerel@gmail.com>, 2014 msgid """" msgstr """" ""Project-Id-Version: oslo.log\n"" ""Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"" ""POT-Creation-Date: 2014-09-26 06:06+0000\n"" ""PO-Revision-Date: 2014-09-25 08:51+0000\n"" ""Last-Translator: Maxime COQUEREL <max.coquerel@gmail.com>\n"" ""Language-Team: French (http://www.transifex.com/projects/p/oslolog/language/"" ""fr/)\n"" ""Language: fr\n"" ""MIME-Version: 1.0\n"" ""Content-Type: text/plain; charset=UTF-8\n"" ""Content-Transfer-Encoding: 8bit\n"" ""Generated-By: Babel 1.3\n"" ""Plural-Forms: nplurals=2; plural=(n > 1);\n"" ",,33,469
openstack%2Fswift~master~If1c8aa1240d38354ebc9b1ebca92dc1c8c36cb5f,openstack/swift,master,If1c8aa1240d38354ebc9b1ebca92dc1c8c36cb5f,direct_client not passing args between some functions,MERGED,2014-09-15 13:19:05.000000000,2014-09-28 05:41:35.000000000,2014-09-28 05:41:33.000000000,"[{'_account_id': 3}, {'_account_id': 1253}, {'_account_id': 5189}, {'_account_id': 7479}, {'_account_id': 7847}, {'_account_id': 13052}]","[{'number': 1, 'created': '2014-09-15 13:19:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e76e7032879d808d8a6d19af7e6e6925fb8febd1', 'message': 'direct_client not passing args between some functions\n\nThe call to _get_direct_account_container in direct_get_account\nhas several of its args =None instead of set to the value passed\nto direct_get_account.\n\nThe same applies to _get_direct_account_container in\ndirect_get_container.\n\nThe direct_get_container is only called by the account-reaper\nand this bug will have limited impact on it. The marker,\nmaintained in reap_container, is ignored by direct_get_container.\nThis is not as bad as it sounds, if the account-reaper successfully\ndeletes the first 10K objects, assuming the container has > 10K\nobjects, the next call to direct_get_container will in fact return\nthe next 10K objects even though it sets marker=None (assuming the\nfirst 10K objects were successfully deleted).\n\nChange-Id: If1c8aa1240d38354ebc9b1ebca92dc1c8c36cb5f\n'}, {'number': 2, 'created': '2014-09-15 13:23:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0f9bfa65a6272829c42a27a298a128aff44009c0', 'message': 'direct_client not passing args between some functions\n\nThe call to _get_direct_account_container in direct_get_account\nhas several of its args =None instead of set to the value passed\nto direct_get_account.\n\nThe same applies to _get_direct_account_container in\ndirect_get_container.\n\nThe direct_get_container is only called by the account-reaper\nand this bug will have limited impact on it. The marker,\nmaintained in reap_container, is ignored by direct_get_container.\nThis is not as bad as it sounds, if the account-reaper successfully\ndeletes the first 10K objects, assuming the container has > 10K\nobjects, the next call to direct_get_container will in fact return\nthe next 10K objects even though it sets marker=None (assuming the\nfirst 10K objects were successfully deleted).\n\nCloses-Bug: #1369558\nChange-Id: If1c8aa1240d38354ebc9b1ebca92dc1c8c36cb5f\n'}, {'number': 3, 'created': '2014-09-22 11:16:23.000000000', 'files': ['test/unit/common/test_direct_client.py', 'swift/common/direct_client.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/6354e2da57a8d487caf3605d4005134f584cf935', 'message': 'direct_client not passing args between some functions\n\nThe call to _get_direct_account_container in direct_get_account\nhas several of its args =None instead of set to the value passed\nto direct_get_account.\n\nThe same applies to _get_direct_account_container in\ndirect_get_container.\n\nThe direct_get_container is only called by the account-reaper\nand this bug will have limited impact on it. The marker,\nmaintained in reap_container, is ignored by direct_get_container.\nThis is not as bad as it sounds, if the account-reaper successfully\ndeletes the first 10K objects, assuming the container has > 10K\nobjects, the next call to direct_get_container will in fact return\nthe next 10K objects even though it sets marker=None (assuming the\nfirst 10K objects were successfully deleted).\n\nThis patch also updates test_direct_get_account and\ntest_direct_get_container to ensure the appropriate\nargs are included in the connection query_string.\n\nCloses-Bug: #1369558\nChange-Id: If1c8aa1240d38354ebc9b1ebca92dc1c8c36cb5f\n'}]",0,121535,6354e2da57a8d487caf3605d4005134f584cf935,26,6,3,1253,,,0,"direct_client not passing args between some functions

The call to _get_direct_account_container in direct_get_account
has several of its args =None instead of set to the value passed
to direct_get_account.

The same applies to _get_direct_account_container in
direct_get_container.

The direct_get_container is only called by the account-reaper
and this bug will have limited impact on it. The marker,
maintained in reap_container, is ignored by direct_get_container.
This is not as bad as it sounds, if the account-reaper successfully
deletes the first 10K objects, assuming the container has > 10K
objects, the next call to direct_get_container will in fact return
the next 10K objects even though it sets marker=None (assuming the
first 10K objects were successfully deleted).

This patch also updates test_direct_get_account and
test_direct_get_container to ensure the appropriate
args are included in the connection query_string.

Closes-Bug: #1369558
Change-Id: If1c8aa1240d38354ebc9b1ebca92dc1c8c36cb5f
",git fetch https://review.opendev.org/openstack/swift refs/changes/35/121535/3 && git format-patch -1 --stdout FETCH_HEAD,['swift/common/direct_client.py'],1,e76e7032879d808d8a6d19af7e6e6925fb8febd1,failing_to_pass_args," account, marker=marker, limit=limit, prefix=prefix, delimiter=delimiter, conn_timeout=conn_timeout, response_timeout=response_timeout) part, account, marker=marker, limit=limit, prefix=prefix, delimiter=delimiter, conn_timeout=conn_timeout, response_timeout=response_timeout)"," account, marker=None, limit=None, prefix=None, delimiter=None, conn_timeout=5, response_timeout=15) part, account, marker=None, limit=None, prefix=None, delimiter=None, conn_timeout=5, response_timeout=15)",10,10
openstack%2Foslo.messaging~master~I3ca1464ef5a1dadcc8f030caf7e62135492d05a2,openstack/oslo.messaging,master,I3ca1464ef5a1dadcc8f030caf7e62135492d05a2,Updated from global requirements,MERGED,2014-09-26 03:59:04.000000000,2014-09-28 05:34:46.000000000,2014-09-28 05:34:46.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-09-26 03:59:04.000000000', 'files': ['requirements.txt', 'requirements-py3.txt'], 'web_link': 'https://opendev.org/openstack/oslo.messaging/commit/6b405b9fa6eb707c8e0fe00315cb19264f28b885', 'message': 'Updated from global requirements\n\nChange-Id: I3ca1464ef5a1dadcc8f030caf7e62135492d05a2\n'}]",0,124261,6b405b9fa6eb707c8e0fe00315cb19264f28b885,10,3,1,11131,,,0,"Updated from global requirements

Change-Id: I3ca1464ef5a1dadcc8f030caf7e62135492d05a2
",git fetch https://review.opendev.org/openstack/oslo.messaging refs/changes/61/124261/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'requirements-py3.txt']",2,6b405b9fa6eb707c8e0fe00315cb19264f28b885,openstack/requirements,kombu>=2.5.0,kombu>=2.4.8,2,2
openstack%2Foslo.i18n~master~Id06bb974dba0f29720be3650986bb04e23a2c5e0,openstack/oslo.i18n,master,Id06bb974dba0f29720be3650986bb04e23a2c5e0,Imported Translations from Transifex,MERGED,2014-09-26 06:01:44.000000000,2014-09-28 05:24:15.000000000,2014-09-28 05:24:15.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-09-26 06:01:44.000000000', 'files': ['oslo.i18n/locale/fr/LC_MESSAGES/oslo.i18n-log-critical.po', 'oslo.i18n/locale/fr/LC_MESSAGES/oslo.i18n-log-error.po', 'oslo.i18n/locale/fr/LC_MESSAGES/oslo.i18n-log-info.po', 'oslo.i18n/locale/fr/LC_MESSAGES/oslo.i18n-log-warning.po'], 'web_link': 'https://opendev.org/openstack/oslo.i18n/commit/12f14da97bbe71b679756df80cc9a6a6d79f5821', 'message': 'Imported Translations from Transifex\n\nChange-Id: Id06bb974dba0f29720be3650986bb04e23a2c5e0\n'}]",0,124282,12f14da97bbe71b679756df80cc9a6a6d79f5821,10,3,1,11131,,,0,"Imported Translations from Transifex

Change-Id: Id06bb974dba0f29720be3650986bb04e23a2c5e0
",git fetch https://review.opendev.org/openstack/oslo.i18n refs/changes/82/124282/1 && git format-patch -1 --stdout FETCH_HEAD,"['oslo.i18n/locale/fr/LC_MESSAGES/oslo.i18n-log-critical.po', 'oslo.i18n/locale/fr/LC_MESSAGES/oslo.i18n-log-error.po', 'oslo.i18n/locale/fr/LC_MESSAGES/oslo.i18n-log-info.po', 'oslo.i18n/locale/fr/LC_MESSAGES/oslo.i18n-log-warning.po']",4,12f14da97bbe71b679756df80cc9a6a6d79f5821,transifex/translations,"# Translations template for heat. # Copyright (C) 2014 ORGANIZATION # This file is distributed under the same license as the heat project. # # Translators: # Maxime COQUEREL <max.coquerel@gmail.com>, 2014 msgid """" msgstr """" ""Project-Id-Version: oslo.i18n\n"" ""Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"" ""POT-Creation-Date: 2014-09-26 06:01+0000\n"" ""PO-Revision-Date: 2014-09-25 08:52+0000\n"" ""Last-Translator: Maxime COQUEREL <max.coquerel@gmail.com>\n"" ""Language-Team: French (http://www.transifex.com/projects/p/osloi18n/language/"" ""fr/)\n"" ""Language: fr\n"" ""MIME-Version: 1.0\n"" ""Content-Type: text/plain; charset=UTF-8\n"" ""Content-Transfer-Encoding: 8bit\n"" ""Generated-By: Babel 1.3\n"" ""Plural-Forms: nplurals=2; plural=(n > 1);\n"" ",,84,0
openstack%2Foslo.messaging~master~I076ccff2b34be3ecf07cdcd0bb9753997c1e3218,openstack/oslo.messaging,master,I076ccff2b34be3ecf07cdcd0bb9753997c1e3218,Add documentation explaining how to use the AMQP 1.0 driver,MERGED,2014-09-11 17:49:17.000000000,2014-09-28 04:38:07.000000000,2014-09-28 04:38:06.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 2813}, {'_account_id': 5638}, {'_account_id': 6159}, {'_account_id': 7805}, {'_account_id': 8770}, {'_account_id': 8784}]","[{'number': 1, 'created': '2014-09-11 17:49:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.messaging/commit/4978b8d8d6101b405528f95265d9dd62267a0dbb', 'message': 'Add documentation explaining how to use the AMQP 1.0 driver.\n\nChange-Id: I076ccff2b34be3ecf07cdcd0bb9753997c1e3218\nCloses-Bug: #1367910\n'}, {'number': 2, 'created': '2014-09-12 00:50:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.messaging/commit/c9daa608d398aaa77cab2e315694acac6053695b', 'message': 'Add documentation explaining how to use the AMQP 1.0 driver\n\nChange-Id: I076ccff2b34be3ecf07cdcd0bb9753997c1e3218\nCloses-Bug: #1367910\n'}, {'number': 3, 'created': '2014-09-12 14:59:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.messaging/commit/affe0e7f1d029ec44edd87ee09b85316973ba6b2', 'message': 'Add documentation explaining how to use the AMQP 1.0 driver\n\nChange-Id: I076ccff2b34be3ecf07cdcd0bb9753997c1e3218\nCloses-Bug: #1367910\n'}, {'number': 4, 'created': '2014-09-12 16:04:45.000000000', 'files': ['doc/source/index.rst', 'doc/source/AMQP1.0.rst'], 'web_link': 'https://opendev.org/openstack/oslo.messaging/commit/bc0033a9c06de59cd9343781ee05a92ea34e29e4', 'message': 'Add documentation explaining how to use the AMQP 1.0 driver\n\nChange-Id: I076ccff2b34be3ecf07cdcd0bb9753997c1e3218\nCloses-Bug: #1367910\n'}]",7,120860,bc0033a9c06de59cd9343781ee05a92ea34e29e4,32,8,4,8770,,,0,"Add documentation explaining how to use the AMQP 1.0 driver

Change-Id: I076ccff2b34be3ecf07cdcd0bb9753997c1e3218
Closes-Bug: #1367910
",git fetch https://review.opendev.org/openstack/oslo.messaging refs/changes/60/120860/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/index.rst', 'doc/source/AMQP1.0.rst']",2,4978b8d8d6101b405528f95265d9dd62267a0dbb,bug/1367910,"------------------------- AMQP 1.0 Protocol Support ------------------------- .. currentmodule:: oslo.messaging ============ Introduction ============ This release of oslo.messaging includes an experimental driver that provides support for version 1.0 of the Advanced Message Queuing Protocol (AMQP 1.0, ISO/IEC 19464). The current implementation of this driver is considered *experimental*. It is not recommended that this driver be used in production systems. Rather, this driver is being provided as a *technical preview*, in hopes that it will encourage further testing by the AMQP 1.0 community. More detail regarding the driver's implementation is available from the `specification`_. .. _specification: https://git.openstack.org/cgit/openstack/oslo-specs/tree/specs/juno/amqp10-driver-implementation.rst ============= Prerequisites ============= This driver uses the Apache QPID `Proton`_ AMQP 1.0 protocol engine. This engine consists of a platform specific library and a python binding. The driver does not directly interface with the engine API, as the API is a very low-level interface to the AMQP protocol. Instead, the driver uses the pure python `Pyngus`_ client API, which is layered on top of the protocol engine. .. _Proton: http://qpid.apache.org/proton/index.html .. _Pyngus: https://github.com/kgiusti/pyngus In order to run the driver the Proton Python bindings, Proton library, Proton header files, and Pyngus must be installed. Pyngus is available via `Pypi`__. .. __: https://pypi.python.org/pypi/pyngus While the Proton Python bindings are available via `Pypi`__, it includes a C extension that requires the Proton library and header files be pre-installed in order for the binding to install properly. If the target platform's distribution provides a pre-packaged version of the Proton Python binding (see packages_ below), it is recommended to use these pre-built packages instead of pulling them from Pypi. .. __: https://pypi.python.org/pypi/python-qpid-proton The driver also requires a *broker* that supports version 1.0 of the AMQP protocol. Adoption of AMQP 1.0 is in its early stages, so the choice of brokers is currently limited to the Apache `qpidd`_ broker (C++ only, not the Java broker). The version of qpidd **must** be at least 0.26. qpidd also uses the Proton engine for its AMQP 1.0 support, so the Proton library must be installed on the system hosting the qpidd daemon. .. _qpidd: http://qpid.apache.org/components/cpp-broker/index.html ============= Configuration ============= driver ------ It is recommended to start with the default configuration options supported by the driver. The remaining configuration steps described below assume that none of the driver's options have been manually overridden. **Note Well:** The driver currently does **not** support the generic *amqp* options used by the existing drivers, such as *amqp_durable_queues* or *amqp_auto_delete*. Support for these are TBD. qpidd ----- First, verify that the Proton library has been installed and is imported by the qpidd broker. This can checked by running:: $ qpidd --help and looking for the AMQP 1.0 options in the help text. If no AMQP 1.0 options are listed, verify that the Proton libraries are installed and that the version of qpidd is greater than or equal to 0.26. Second, configure the address patterns used by the driver. This is done by adding the following to /etc/qpid/qpidd.conf:: queue-patterns=exclusive queue-patterns=unicast topic-patterns=broadcast These patterns, *exclusive*, *unicast*, and *broadcast* are the default values used by the driver. These can be overridden via the driver configuration options if desired. If manually overridden, update the qpidd.conf values to match. services -------- The new driver can be loaded and used by existing applications by specifying *amqp* as the RPC backend in the service's configuration file. For example, in nova.conf:: rpc_backend = amqp .. _packages: ====================== Platforms and Packages ====================== Pyngus is available via Pypi. Pre-built packages for the Proton library and qpidd are available for some popular distributions: RHEL and Fedora --------------- Packages exist in EPEL for RHEL/Centos 7, and Fedora 19+. Unfortunately, RHEL/Centos 6 base packages include a very old version of qpidd that does not support AMQP 1.0. EPEL's policy does not allow a newer version of qpidd for RHEL/Centos 6. The following packages must be installed on the system running the qpidd daemon: - qpid-cpp-server (version 0.26+) - qpid-proton-c The following packages must be installed on the systems running the services that use the new driver: - Proton libraries: qpid-proton-c-devel - Proton python bindings: python-qpid-proton - pyngus (via Pypi) Debian and Ubuntu ----------------- Packages for the Proton library, headers, and Python bindings are available in the Debian/Testing repository. Proton packages are not yet available in the Ubuntu repository. The version of qpidd on both platforms is too old and does not support AMQP 1.0. Until the proper package version arrive the latest packages can be pulled from the `Apache Qpid PPA`_ on Launchpad:: sudo add-apt-repository ppa:qpid/released .. _Apache Qpid PPA: https://launchpad.net/~qpid/+archive/ubuntu/released The following packages must be installed on the system running the qpidd daemon: - qpidd (version 0.26+) - libqpid-proton2 The following packages must be installed on the systems running the services that use the new driver: - Proton libraries: libqpid-proton2-dev - Proton python bindings: python-qpid-proton - pyngus (via Pypi) ",,174,0
openstack%2Fnova~stable%2Ficehouse~Icddead6f746d4d3ba652858eaae425acce8c177d,openstack/nova,stable/icehouse,Icddead6f746d4d3ba652858eaae425acce8c177d,Catch missing Glance image attrs with None,MERGED,2014-08-23 02:32:11.000000000,2014-09-28 04:37:54.000000000,2014-09-28 04:37:51.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 5170}, {'_account_id': 8213}, {'_account_id': 8797}, {'_account_id': 8871}, {'_account_id': 9656}]","[{'number': 1, 'created': '2014-08-23 02:32:11.000000000', 'files': ['nova/image/glance.py', 'nova/tests/image/test_glance.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/e1d6e1881b08827d1be95e35ed064364d222659e', 'message': ""Catch missing Glance image attrs with None\n\nGlance is known to drop values from its response if they don't resolve to true.\n Previously in I67b7dd16a94fe60d873c012f6bd246ab24500d5a the None catch was\n removed as this WILL cause a call to glance for missing value and\n optimizations where made to attempt to pre-cache values that can be missing\n in expected ways. However there are still situations that can't be optimized.\n\nNotably this can occur with disk_format, container_format, name, deleted,\n and checksum.\n\nTest adapted from I85c42f8351763da201021a22f5ff0ebd62c6b2db by Mike Perez\n\nCloses-bug #1308058\nCo-Author: Mike Perez <thingee@gmail.com>\n\nChange-Id: Icddead6f746d4d3ba652858eaae425acce8c177d\n(cherry picked from commit ef3f158a5aa1988be8ab2f295252373f3ee85809)\n""}]",0,116425,e1d6e1881b08827d1be95e35ed064364d222659e,29,7,1,67,,,0,"Catch missing Glance image attrs with None

Glance is known to drop values from its response if they don't resolve to true.
 Previously in I67b7dd16a94fe60d873c012f6bd246ab24500d5a the None catch was
 removed as this WILL cause a call to glance for missing value and
 optimizations where made to attempt to pre-cache values that can be missing
 in expected ways. However there are still situations that can't be optimized.

Notably this can occur with disk_format, container_format, name, deleted,
 and checksum.

Test adapted from I85c42f8351763da201021a22f5ff0ebd62c6b2db by Mike Perez

Closes-bug #1308058
Co-Author: Mike Perez <thingee@gmail.com>

Change-Id: Icddead6f746d4d3ba652858eaae425acce8c177d
(cherry picked from commit ef3f158a5aa1988be8ab2f295252373f3ee85809)
",git fetch https://review.opendev.org/openstack/nova refs/changes/25/116425/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/image/glance.py', 'nova/tests/image/test_glance.py']",2,e1d6e1881b08827d1be95e35ed064364d222659e,bug/1308058," def test_extracting_missing_attributes(self): """"""Verify behavior from glance objects that are missing attributes This fakes the image class and is missing attribute as the client can return if they're not set in the database. """""" class MyFakeGlanceImage(glance_stubs.FakeImage): def __init__(self, metadata): IMAGE_ATTRIBUTES = ['size', 'owner', 'id', 'created_at', 'updated_at', 'status', 'min_disk', 'min_ram', 'is_public'] raw = dict.fromkeys(IMAGE_ATTRIBUTES) raw.update(metadata) self.__dict__['raw'] = raw metadata = { 'id': 1, 'created_at': self.NOW_DATETIME, 'updated_at': self.NOW_DATETIME, } image = MyFakeGlanceImage(metadata) observed = glance._extract_attributes(image) expected = { 'id': 1, 'name': None, 'is_public': None, 'size': None, 'min_disk': None, 'min_ram': None, 'disk_format': None, 'container_format': None, 'checksum': None, 'created_at': self.NOW_DATETIME, 'updated_at': self.NOW_DATETIME, 'deleted_at': None, 'deleted': None, 'status': None, 'properties': {}, 'owner': None, } self.assertEqual(expected, observed) ",,47,1
openstack%2Foslo.vmware~master~I8e2abd3abf015733642419cb8a3b86aec09c929f,openstack/oslo.vmware,master,I8e2abd3abf015733642419cb8a3b86aec09c929f,getText can be called only when doc is not None,MERGED,2014-09-22 19:13:53.000000000,2014-09-28 04:37:44.000000000,2014-09-28 04:37:43.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 5638}, {'_account_id': 8759}, {'_account_id': 9172}]","[{'number': 1, 'created': '2014-09-22 19:13:53.000000000', 'files': ['oslo/vmware/service.py'], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/1c2987bc4cb19fd53299abb8783763042348a768', 'message': ""getText can be called only when doc is not None\n\nAdd a check to make sure getText is executed only when doc is\nnot None.\nWithout this check, we can potentially get:\nAttributeError: 'NoneType' object has no attribute 'getText'\n\nChange-Id: I8e2abd3abf015733642419cb8a3b86aec09c929f\n""}]",0,123224,1c2987bc4cb19fd53299abb8783763042348a768,10,5,1,8759,,,0,"getText can be called only when doc is not None

Add a check to make sure getText is executed only when doc is
not None.
Without this check, we can potentially get:
AttributeError: 'NoneType' object has no attribute 'getText'

Change-Id: I8e2abd3abf015733642419cb8a3b86aec09c929f
",git fetch https://review.opendev.org/openstack/oslo.vmware refs/changes/24/123224/1 && git format-patch -1 --stdout FETCH_HEAD,['oslo/vmware/service.py'],1,1c2987bc4cb19fd53299abb8783763042348a768,fix_none_type_gettext," if doc is not None: fault_string = doc.childAtPath(""/Envelope/Body/Fault/"" ""faultstring"").getText() detail = doc.childAtPath('/Envelope/Body/Fault/detail')"," fault_string = doc.childAtPath(""/Envelope/Body/Fault/"" ""faultstring"").getText() detail = doc.childAtPath('/Envelope/Body/Fault/detail')",4,3
openstack%2Fnova~stable%2Ficehouse~I9b3da852b0ab11dc980524e2317ee876a3ec28b8,openstack/nova,stable/icehouse,I9b3da852b0ab11dc980524e2317ee876a3ec28b8,Include next link when default limit is reached,MERGED,2014-08-15 09:56:31.000000000,2014-09-28 03:59:10.000000000,2014-09-28 03:59:07.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 5170}, {'_account_id': 7400}, {'_account_id': 9656}, {'_account_id': 10559}]","[{'number': 1, 'created': '2014-08-15 09:56:31.000000000', 'files': ['nova/tests/api/openstack/test_common.py', 'nova/api/openstack/common.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/e5e6bc77e8fff614d5dff3a4a6fc4f2ddc18d215', 'message': 'Include next link when default limit is reached\n\nThe /servers and /servers/detail APIs support pagination and a\n""next"" link should be included when more data is available, see\nhttp://docs.openstack.org/api/openstack-compute/2/content/\nPaginated_Collections-d1e664.html. In the documentation it states\nthat \'collections are required to contain atom ""next"" links\'.\n\nWhen the default ""osapi_max"" limit is reached then the ""next"" link\nis not included in the API reply. In this case, the caller cannot\ndetermine if there are any more servers and has no marker value\nsuch that they can retrieve the rest of the servers.\n\nFor example, assume that there are 1050 VMs and the osapi_max value\nis the default of 1000. The /servers API will return the first 1000\nVMs but it will not include the ""next"" link; the caller has no way\nto determine if there are more then 1000 VMs and no way to retrieve\nthe other 50 VMs.\n\nThe fix for this is to include the ""next"" link when the number of\nservers being returned is the maximum limit, even if the ""limit""\nparameter is not supplied. Note: the caller could workaround this\nby specifying the exact ""osapi_max"" limit value (ie, limit=1000)\non the API call but this requires that the caller knows the\nconfigured limit.\n\nChange-Id: I9b3da852b0ab11dc980524e2317ee876a3ec28b8\nCloses-bug: 1288466\n(cherry picked from commit 968c01afe7ff43346f5817e2a3adf97fd03106b1)\n'}]",0,114487,e5e6bc77e8fff614d5dff3a4a6fc4f2ddc18d215,23,6,1,5803,,,0,"Include next link when default limit is reached

The /servers and /servers/detail APIs support pagination and a
""next"" link should be included when more data is available, see
http://docs.openstack.org/api/openstack-compute/2/content/
Paginated_Collections-d1e664.html. In the documentation it states
that 'collections are required to contain atom ""next"" links'.

When the default ""osapi_max"" limit is reached then the ""next"" link
is not included in the API reply. In this case, the caller cannot
determine if there are any more servers and has no marker value
such that they can retrieve the rest of the servers.

For example, assume that there are 1050 VMs and the osapi_max value
is the default of 1000. The /servers API will return the first 1000
VMs but it will not include the ""next"" link; the caller has no way
to determine if there are more then 1000 VMs and no way to retrieve
the other 50 VMs.

The fix for this is to include the ""next"" link when the number of
servers being returned is the maximum limit, even if the ""limit""
parameter is not supplied. Note: the caller could workaround this
by specifying the exact ""osapi_max"" limit value (ie, limit=1000)
on the API call but this requires that the caller knows the
configured limit.

Change-Id: I9b3da852b0ab11dc980524e2317ee876a3ec28b8
Closes-bug: 1288466
(cherry picked from commit 968c01afe7ff43346f5817e2a3adf97fd03106b1)
",git fetch https://review.opendev.org/openstack/nova refs/changes/87/114487/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/api/openstack/test_common.py', 'nova/api/openstack/common.py']",2,e5e6bc77e8fff614d5dff3a4a6fc4f2ddc18d215,," """"""Retrieve 'next' link, if applicable. This is included if: 1) 'limit' param is specified and equals the number of items. 2) 'limit' param is specified but it exceeds CONF.osapi_max_limit, in this case the number of items is CONF.osapi_max_limit. 3) 'limit' param is NOT specified but the number of items is CONF.osapi_max_limit. """""" max_items = min( int(request.params.get(""limit"", CONF.osapi_max_limit)), CONF.osapi_max_limit) if max_items and max_items == len(items):"," """"""Retrieve 'next' link, if applicable."""""" limit = int(request.params.get(""limit"", 0)) if limit and limit == len(items):",89,3
openstack%2Fpbr~master~Ic4c888626699eb710e85e3bdc48dd6ee74bdf024,openstack/pbr,master,Ic4c888626699eb710e85e3bdc48dd6ee74bdf024,Retry the integration setup on connection error,MERGED,2014-09-12 12:53:48.000000000,2014-09-28 03:55:39.000000000,2014-09-28 03:55:39.000000000,"[{'_account_id': 2}, {'_account_id': 3}, {'_account_id': 5638}, {'_account_id': 6039}]","[{'number': 1, 'created': '2014-09-12 12:53:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/pbr/commit/c73519d7d9c8498e4bff688079eff529b957a9fb', 'message': 'Retry the integration setup on connection error\n\nSometimes there are network errors communicating with pypi\nduring a long running (1h+) integration gate job. This\ncauses the job to fail, and wastes a lot of time re-running\nthe job to get it to pass. This change makes it less likely\nto fail, as if there is a timeout or socket error, the\nintegration setup will retry.\n\nChange-Id: Ic4c888626699eb710e85e3bdc48dd6ee74bdf024\n'}, {'number': 2, 'created': '2014-09-12 14:44:01.000000000', 'files': ['tools/integration.sh'], 'web_link': 'https://opendev.org/openstack/pbr/commit/22a68b4c25d6b10f53756b419e493995aefc2f2f', 'message': 'Retry the integration setup on connection error\n\nSometimes there are network errors communicating with pypi\nduring a long running (1h+) integration gate job. This\ncauses the job to fail, and wastes a lot of time re-running\nthe job to get it to pass. This change makes it less likely\nto fail, as if there is a timeout or socket error, the\nintegration setup will retry.\n\nChange-Id: Ic4c888626699eb710e85e3bdc48dd6ee74bdf024\n'}]",0,121093,22a68b4c25d6b10f53756b419e493995aefc2f2f,10,4,2,12100,,,0,"Retry the integration setup on connection error

Sometimes there are network errors communicating with pypi
during a long running (1h+) integration gate job. This
causes the job to fail, and wastes a lot of time re-running
the job to get it to pass. This change makes it less likely
to fail, as if there is a timeout or socket error, the
integration setup will retry.

Change-Id: Ic4c888626699eb710e85e3bdc48dd6ee74bdf024
",git fetch https://review.opendev.org/openstack/pbr refs/changes/93/121093/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/integration.sh'],1,c73519d7d9c8498e4bff688079eff529b957a9fb,retry-integration-on-connection-error,"try: from requests import Timeout except ImportError: from pip._vendor.requests import Timeout from socket import error as SocketError # Some environments have network issues that drop connections to pypi # when running integration tests, so we retry here so that hour-long # test runs are less likely to fail randomly. try; setuptools.setup( setup_requires=['pbr'], pbr=True) except (SocketError, Timeout): setuptools.setup( setup_requires=['pbr'], pbr=True) ","setuptools.setup( setup_requires=['pbr'], pbr=True)",19,3
openstack%2Fglance~master~Ib67f16cc3786715fe9a48784162035199991bf4f,openstack/glance,master,Ib67f16cc3786715fe9a48784162035199991bf4f,Fixes v2 Image Property Update Error,ABANDONED,2014-08-05 22:33:02.000000000,2014-09-28 02:49:18.000000000,,"[{'_account_id': 3}, {'_account_id': 616}, {'_account_id': 4992}, {'_account_id': 5202}, {'_account_id': 6484}, {'_account_id': 6549}, {'_account_id': 12114}]","[{'number': 1, 'created': '2014-08-05 22:33:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/9367304c8f4834be510897ccaa3e6d685b853ec6', 'message': 'Fixes v2 image property update error\n\nUpdating a image property no longer returns a 409 error when image\nproperty does not exist. It will now add the property if property\nis not initially present.\n\nChange-Id: Ib67f16cc3786715fe9a48784162035199991bf4f\nCloses-Bug: #1334322\n'}, {'number': 2, 'created': '2014-09-09 01:25:24.000000000', 'files': ['glance/api/v2/images.py'], 'web_link': 'https://opendev.org/openstack/glance/commit/9b644e4b5cd4b68e5e3142c1234b3d5d148ab5be', 'message': ""Fixes v2 Image Property Update Error\n\nUpdating a image with properties from the schema-image.json file\n( specifically the following properties:\n  'architecture', 'os_distro', 'os_version', 'instance_uuid' )\nwould return a 409 Conflict Error.\nThis patch fixes this issue and these properties can be freely\nadded to v2 images.\n\nChange-Id: Ib67f16cc3786715fe9a48784162035199991bf4f\nCloses-Bug: #1334322\n""}]",5,112160,9b644e4b5cd4b68e5e3142c1234b3d5d148ab5be,16,7,2,12114,,,0,"Fixes v2 Image Property Update Error

Updating a image with properties from the schema-image.json file
( specifically the following properties:
  'architecture', 'os_distro', 'os_version', 'instance_uuid' )
would return a 409 Conflict Error.
This patch fixes this issue and these properties can be freely
added to v2 images.

Change-Id: Ib67f16cc3786715fe9a48784162035199991bf4f
Closes-Bug: #1334322
",git fetch https://review.opendev.org/openstack/glance refs/changes/60/112160/2 && git format-patch -1 --stdout FETCH_HEAD,['glance/api/v2/images.py'],1,9367304c8f4834be510897ccaa3e6d685b853ec6,bug/1334322, else: image.extra_properties[path_root] = value," elif path_root in image.extra_properties: image.extra_properties[path_root] = value else: msg = _(""Property %s does not exist."") raise webob.exc.HTTPConflict(msg % path_root)",1,4
openstack%2Ftaskflow~master~If24530c0381d7fb99797acaa582d3be1d7054185,openstack/taskflow,master,If24530c0381d7fb99797acaa582d3be1d7054185,Use the features that the oslotest mock base class provides,MERGED,2014-09-10 23:41:51.000000000,2014-09-28 02:30:00.000000000,2014-09-28 02:29:59.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-09-10 23:41:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/ccc3c0ead0fdce12fc13078b5ebaab9d90b35682', 'message': 'Use the features that the oslotest mock base class provides\n\nInstead of having our own mock subclass that has similar functions\nas the oslotest base mocking class just use the base class functions\nwhere we can and add on our own customizations as we choose to.\n\nThis change moves to using the base classes fixtures and also adjusts\nthe customized subclass method names to match closer to the rest of\nthe unittest classes method name style (camel-case not underscores).\n\nChange-Id: If24530c0381d7fb99797acaa582d3be1d7054185\n'}, {'number': 2, 'created': '2014-09-10 23:47:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/40f02abdf9964535c2fdcdfa3ab200204ba924d0', 'message': 'Use the features that the oslotest mock base class provides\n\nInstead of having our own mock subclass that has similar functions\nas the oslotest base mocking class just use the base class functions\nwhere we can and add on our own customizations as we choose to.\n\nThis change moves to using the base classes fixtures and also adjusts\nthe customized subclass method names to match closer to the rest of\nthe unittest classes method name style (camel-case not underscores).\n\nChange-Id: If24530c0381d7fb99797acaa582d3be1d7054185\n'}, {'number': 3, 'created': '2014-09-12 07:16:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/c296aa5945f358ca2bb4460df6be8ea85c99f0d0', 'message': 'Use the features that the oslotest mock base class provides\n\nInstead of having our own mock subclass that has similar functions\nas the oslotest base mocking class just use the base class functions\nwhere we can and add on our own customizations as we choose to.\n\nThis change moves to using the base classes fixtures and also adjusts\nthe customized subclass method names to match closer to the rest of\nthe unittest classes method name style (camel-case not underscores).\n\nChange-Id: If24530c0381d7fb99797acaa582d3be1d7054185\n'}, {'number': 4, 'created': '2014-09-26 20:18:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/f6a92cdadd2adcc7685d4b5acc546ca7a50609de', 'message': 'Use the features that the oslotest mock base class provides\n\nInstead of having our own mock subclass that has similar functions\nas the oslotest base mocking class just use the base class functions\nwhere we can and add on our own customizations as we choose to.\n\nThis change moves to using the base classes fixtures and also adjusts\nthe customized subclass method names to match closer to the rest of\nthe unittest classes method name style (camel-case not underscores).\n\nChange-Id: If24530c0381d7fb99797acaa582d3be1d7054185\n'}, {'number': 5, 'created': '2014-09-27 21:56:05.000000000', 'files': ['taskflow/test.py', 'taskflow/tests/unit/worker_based/test_proxy.py', 'taskflow/tests/unit/worker_based/test_dispatcher.py', 'taskflow/tests/unit/worker_based/test_pipeline.py', 'taskflow/tests/unit/worker_based/test_executor.py', 'taskflow/tests/unit/worker_based/test_message_pump.py', 'taskflow/tests/unit/worker_based/test_engine.py', 'taskflow/tests/unit/worker_based/test_worker.py', 'taskflow/tests/unit/worker_based/test_server.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/5780a5d77e70443dcf785c03814cf902c6d073cd', 'message': 'Use the features that the oslotest mock base class provides\n\nInstead of having our own mock subclass that has similar functions\nas the oslotest base mocking class just use the base class functions\nwhere we can and add on our own customizations as we choose to.\n\nThis change moves to using the base classes fixtures and also adjusts\nthe customized subclass method names to match closer to the rest of\nthe unittest classes method name style (camel-case not underscores).\n\nChange-Id: If24530c0381d7fb99797acaa582d3be1d7054185\n'}]",0,120620,5780a5d77e70443dcf785c03814cf902c6d073cd,17,3,5,1297,,,0,"Use the features that the oslotest mock base class provides

Instead of having our own mock subclass that has similar functions
as the oslotest base mocking class just use the base class functions
where we can and add on our own customizations as we choose to.

This change moves to using the base classes fixtures and also adjusts
the customized subclass method names to match closer to the rest of
the unittest classes method name style (camel-case not underscores).

Change-Id: If24530c0381d7fb99797acaa582d3be1d7054185
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/20/120620/4 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/test.py', 'taskflow/tests/unit/worker_based/test_proxy.py', 'tools/certain_tests.sh', 'taskflow/tests/unit/worker_based/test_dispatcher.py', 'taskflow/tests/unit/worker_based/test_pipeline.py', 'taskflow/tests/unit/worker_based/test_executor.py', 'taskflow/tests/unit/worker_based/test_message_pump.py', 'taskflow/tests/unit/worker_based/test_engine.py', 'taskflow/tests/unit/worker_based/test_worker.py', 'taskflow/tests/unit/worker_based/test_server.py']",10,ccc3c0ead0fdce12fc13078b5ebaab9d90b35682,," self.proxy_mock, self.proxy_inst_mock = self.patchClass( self.response_mock, self.response_inst_mock = self.patchClass( self.resetMasterMock()"," self.proxy_mock, self.proxy_inst_mock = self._patch_class( self.response_mock, self.response_inst_mock = self._patch_class( self._reset_master_mock()",70,36
openstack%2Ftaskflow~master~I1602d5180ec8649a1899185972750ddddf65990f,openstack/taskflow,master,I1602d5180ec8649a1899185972750ddddf65990f,Use oslotest to provide our base test case class,MERGED,2014-09-10 01:12:00.000000000,2014-09-28 02:24:44.000000000,2014-09-28 02:24:43.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 9608}]","[{'number': 1, 'created': '2014-09-10 01:12:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/c72ea04d9aa363a4861ec89d811d0496e86a7a3d', 'message': 'Use oslotest to provide our base test case class\n\nThe oslotest library has a nice openstack testing integrated\nbase class that can ensure we setup our base test case using\nthe right logging fixtures, test timeouts, and output fixtures\nthat better operate in the openstack ecosystem.\n\nThis will also allow us to remove some of the functionality\nthat we currently have in our base test case and replace it with\nthe equivalent (or better) functionality that oslotest now\nprovides.\n\nPart of blueprint use-oslo-test\n\nChange-Id: I1602d5180ec8649a1899185972750ddddf65990f\n'}, {'number': 2, 'created': '2014-09-10 05:36:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/195c70b1ce73ee6e68134dde821f829d90f7db3e', 'message': 'Use oslotest to provide our base test case class\n\nThe oslotest library has a nice openstack testing integrated\nbase class that can ensure we setup our base test case using\nthe right logging fixtures, test timeouts, and output fixtures\nthat better operate in the openstack ecosystem.\n\nThis will also allow us to remove some of the functionality\nthat we currently have in our base test case and replace it with\nthe equivalent (or better) functionality that oslotest now\nprovides.\n\nPart of blueprint use-oslo-test\n\nChange-Id: I1602d5180ec8649a1899185972750ddddf65990f\n'}, {'number': 3, 'created': '2014-09-12 07:16:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/cf837a49ecd062192b0843e0f6df0006f135fc86', 'message': 'Use oslotest to provide our base test case class\n\nThe oslotest library has a nice openstack testing integrated\nbase class that can ensure we setup our base test case using\nthe right logging fixtures, test timeouts, and output fixtures\nthat better operate in the openstack ecosystem.\n\nThis will also allow us to remove some of the functionality\nthat we currently have in our base test case and replace it with\nthe equivalent (or better) functionality that oslotest now\nprovides.\n\nPart of blueprint use-oslo-test\n\nChange-Id: I1602d5180ec8649a1899185972750ddddf65990f\n'}, {'number': 4, 'created': '2014-09-26 20:17:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/79d9c09cf0fe0cc786826c7a98e0c2493f4bb4a5', 'message': 'Use oslotest to provide our base test case class\n\nThe oslotest library has a nice openstack testing integrated\nbase class that can ensure we setup our base test case using\nthe right logging fixtures, test timeouts, and output fixtures\nthat better operate in the openstack ecosystem.\n\nThis will also allow us to remove some of the functionality\nthat we currently have in our base test case and replace it with\nthe equivalent (or better) functionality that oslotest now\nprovides.\n\nPart of blueprint use-oslo-test\n\nChange-Id: I1602d5180ec8649a1899185972750ddddf65990f\n'}, {'number': 5, 'created': '2014-09-27 21:53:37.000000000', 'files': ['taskflow/tests/unit/test_storage.py', 'taskflow/tests/unit/worker_based/test_proxy.py', 'test-requirements.txt', 'taskflow/tests/unit/test_duration.py', 'taskflow/tests/unit/worker_based/test_dispatcher.py', 'taskflow/tests/unit/worker_based/test_protocol.py', 'taskflow/tests/unit/jobs/base.py', 'taskflow/tests/unit/worker_based/test_executor.py', 'taskflow/tests/unit/worker_based/test_message_pump.py', 'taskflow/tests/unit/worker_based/test_engine.py', 'taskflow/test.py', 'taskflow/tests/unit/worker_based/test_worker.py', 'tox.ini', 'taskflow/tests/unit/test_task.py', 'taskflow/tests/unit/test_engine_helpers.py', 'taskflow/tests/unit/worker_based/test_server.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/ce620c399a9e5cf2fd438d0193ba0d83da667a88', 'message': 'Use oslotest to provide our base test case class\n\nThe oslotest library has a nice openstack testing integrated\nbase class that can ensure we setup our base test case using\nthe right logging fixtures, test timeouts, and output fixtures\nthat better operate in the openstack ecosystem.\n\nThis will also allow us to remove some of the functionality\nthat we currently have in our base test case and replace it with\nthe equivalent (or better) functionality that oslotest now\nprovides.\n\nPart of blueprint use-oslo-test\n\nChange-Id: I1602d5180ec8649a1899185972750ddddf65990f\n'}]",0,120294,ce620c399a9e5cf2fd438d0193ba0d83da667a88,23,5,5,1297,,,0,"Use oslotest to provide our base test case class

The oslotest library has a nice openstack testing integrated
base class that can ensure we setup our base test case using
the right logging fixtures, test timeouts, and output fixtures
that better operate in the openstack ecosystem.

This will also allow us to remove some of the functionality
that we currently have in our base test case and replace it with
the equivalent (or better) functionality that oslotest now
provides.

Part of blueprint use-oslo-test

Change-Id: I1602d5180ec8649a1899185972750ddddf65990f
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/94/120294/1 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/test.py', 'test-requirements.txt']",2,c72ea04d9aa363a4861ec89d811d0496e86a7a3d,bp/use-oslo-test,oslotest>=1.1.0.0a2,discover coverage>=3.6python-subunit>=0.0.18 testrepository>=0.0.18,4,6
openstack%2Fcinder~master~I78501e1d3558bd6c3e6e1abb0c312cec7d11efd4,openstack/cinder,master,I78501e1d3558bd6c3e6e1abb0c312cec7d11efd4,IBM Storwize:Failed to retype from non-type to replication enable,MERGED,2014-09-18 09:04:14.000000000,2014-09-28 02:24:36.000000000,2014-09-28 02:24:35.000000000,"[{'_account_id': 3}, {'_account_id': 2759}, {'_account_id': 4355}, {'_account_id': 4418}, {'_account_id': 5538}, {'_account_id': 6491}, {'_account_id': 7198}, {'_account_id': 8587}, {'_account_id': 8874}, {'_account_id': 9008}, {'_account_id': 10503}, {'_account_id': 10622}, {'_account_id': 11079}, {'_account_id': 11811}, {'_account_id': 12017}, {'_account_id': 12369}, {'_account_id': 12778}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-09-18 09:04:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/564d50fea7cc526f84f03993ca697f1658219f79', 'message': ""Failed to retype from non-type to replication enable\n\nIt used none as volume type to create replication. Storwize Driver\ndid not check whether it's none.\nChange to use new type to create replication.\n\nCloses-bug: #1369815\n\nChange-Id: I78501e1d3558bd6c3e6e1abb0c312cec7d11efd4\n""}, {'number': 2, 'created': '2014-09-19 03:23:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6544251d6e4e3d285b42f52cc547776b7b108954', 'message': ""IBM Storwize:Failed to retype from non-type to replication enable\n\nIt used none as volume type to create replication. Storwize Driver\ndid not check whether it's none.\nChange to use new type to create replication.\n\nCloses-bug: #1369815\n\nChange-Id: I78501e1d3558bd6c3e6e1abb0c312cec7d11efd4\n""}, {'number': 3, 'created': '2014-09-24 05:01:55.000000000', 'files': ['cinder/volume/drivers/ibm/storwize_svc/__init__.py', 'cinder/volume/drivers/ibm/storwize_svc/replication.py', 'cinder/tests/test_storwize_svc.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/ea32e07f36781c2c0c7a0aa28817cc2df23df32d', 'message': ""IBM Storwize:Failed to retype from non-type to replication enable\n\nIt used none as volume type to create replication. Storwize Driver\ndid not check whether it's none.\nChange to use new type to create replication.\n\nCloses-bug: #1369815\n\nChange-Id: I78501e1d3558bd6c3e6e1abb0c312cec7d11efd4\n""}]",8,122365,ea32e07f36781c2c0c7a0aa28817cc2df23df32d,54,18,3,11079,,,0,"IBM Storwize:Failed to retype from non-type to replication enable

It used none as volume type to create replication. Storwize Driver
did not check whether it's none.
Change to use new type to create replication.

Closes-bug: #1369815

Change-Id: I78501e1d3558bd6c3e6e1abb0c312cec7d11efd4
",git fetch https://review.opendev.org/openstack/cinder refs/changes/65/122365/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/ibm/storwize_svc/__init__.py', 'cinder/volume/drivers/ibm/storwize_svc/replication.py', 'cinder/tests/test_storwize_svc.py']",3,564d50fea7cc526f84f03993ca697f1658219f79,bug/1369815," def test_storwize_retype_from_none_to_strech_cluster_replication(self): self._set_flag('storwize_svc_stretched_cluster_partner', 'openstack2') self.driver.do_setup(self.ctxt) loc = ('StorwizeSVCDriver:' + self.driver._state['system_id'] + ':openstack') cap = {'location_info': loc, 'extent_size': '128'} self.driver._stats = {'location_info': loc} host = {'host': 'foo', 'capabilities': cap} ctxt = context.get_admin_context() volume = self._generate_vol_info(None, None) volume['volume_type_id'] = None volume['volume_type'] = None volume['replication_status'] = ""disabled"" volume['replication_extended_status'] = None # Create volume which is not volume replication model_update = self.driver.create_volume(volume) self.assertIsNone(model_update) # volume should be DB object in this parameter model_update = self.driver.get_replication_status(self.ctxt, volume) self.assertIsNone(model_update) enable_type = self._create_replication_volume_type(True) diff, equal = volume_types.volume_types_diff(ctxt, None, enable_type['id']) # Enable replica self.driver.retype(ctxt, volume, enable_type, diff, host) # In DB replication_status will be updated volume['replication_status'] = None model_update = self.driver.get_replication_status(self.ctxt, volume) self.assertIs('copying', model_update['replication_status']) self.driver.delete_volume(volume) ", self.driver.do_setup(None),43,5
openstack%2Fdevstack~master~I85f87b37558a15d1eaaa781b02fec5b02bd2ab44,openstack/devstack,master,I85f87b37558a15d1eaaa781b02fec5b02bd2ab44,Remove Nova v3 API endpoint,MERGED,2014-09-25 00:44:43.000000000,2014-09-28 02:24:27.000000000,2014-09-28 02:24:26.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 2750}, {'_account_id': 5754}, {'_account_id': 6486}, {'_account_id': 8556}, {'_account_id': 10016}]","[{'number': 1, 'created': '2014-09-25 00:44:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/ca220961eae468f7ee7ce56ba3fe94656cef918f', 'message': ""Remove Nova v3 API endpoint\n\nNova v3 API has disappeared in Juno cycle, and we don't test the API\nnow on the gate since If63dcdb2d05aa0fab0b6848a1248b6678f1ee9ad .\nThis patch removes the endpoint of Nova v3 API.\n\nChange-Id: I85f87b37558a15d1eaaa781b02fec5b02bd2ab44\n""}, {'number': 2, 'created': '2014-09-25 02:09:59.000000000', 'files': ['files/default_catalog.templates', 'lib/nova', 'lib/tempest'], 'web_link': 'https://opendev.org/openstack/devstack/commit/fa4ece89f7928bc86b194b62f02304e805ae573b', 'message': ""Remove Nova v3 API endpoint\n\nNova v3 API has disappeared in Juno cycle, and we don't test the API\nnow on the gate since If63dcdb2d05aa0fab0b6848a1248b6678f1ee9ad .\nThis patch removes the endpoint of Nova v3 API.\n\nChange-Id: I85f87b37558a15d1eaaa781b02fec5b02bd2ab44\n""}]",0,123897,fa4ece89f7928bc86b194b62f02304e805ae573b,12,7,2,6167,,,0,"Remove Nova v3 API endpoint

Nova v3 API has disappeared in Juno cycle, and we don't test the API
now on the gate since If63dcdb2d05aa0fab0b6848a1248b6678f1ee9ad .
This patch removes the endpoint of Nova v3 API.

Change-Id: I85f87b37558a15d1eaaa781b02fec5b02bd2ab44
",git fetch https://review.opendev.org/openstack/devstack refs/changes/97/123897/1 && git format-patch -1 --stdout FETCH_HEAD,"['files/default_catalog.templates', 'lib/nova', 'lib/tempest']",3,ca220961eae468f7ee7ce56ba3fe94656cef918f,remove-nova-v3-api,, iniset $TEMPEST_CONFIG compute-feature-enabled api_v3 ${TEMPEST_NOVA_API_V3:-False},0,15
openstack%2Fneutron~master~I5110b24380ad848d03719e94283d69a7bc0ce972,openstack/neutron,master,I5110b24380ad848d03719e94283d69a7bc0ce972,Indicate the begin and end of the sync process to EOS,MERGED,2014-09-19 00:22:26.000000000,2014-09-28 01:53:05.000000000,2014-09-19 21:55:25.000000000,"[{'_account_id': 3}, {'_account_id': 490}, {'_account_id': 841}, {'_account_id': 5170}, {'_account_id': 6558}, {'_account_id': 6854}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9695}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10386}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-19 00:22:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8457a19863300116f1ce21043d5044062868f460', 'message': 'Indicate the begin and end of the sync process to EOS\n\nSend a trigger to EOS when a sync operation is initiated, and,\nanother trigger when the sync operation is complete.\n\nChange-Id: I5110b24380ad848d03719e94283d69a7bc0ce972\nCloses-Bug: 1371324\n'}, {'number': 2, 'created': '2014-09-19 05:41:45.000000000', 'files': ['neutron/plugins/ml2/drivers/arista/mechanism_arista.py', 'neutron/tests/unit/ml2/drivers/arista/test_arista_mechanism_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/ede3fcf062cc60ceb6c1d90c7723887335b17d27', 'message': 'Indicate the begin and end of the sync process to EOS\n\nSend a trigger to EOS when a sync operation is initiated, and,\nanother trigger when the sync operation is complete.\nAdditionally, sync_interval value (from ml2_conf_arista.ini)\nis passed down to EOS. This is used by EOS to timeout the\ntransaction.\n\nChange-Id: I5110b24380ad848d03719e94283d69a7bc0ce972\nCloses-Bug: 1371324\n'}]",10,122575,ede3fcf062cc60ceb6c1d90c7723887335b17d27,54,23,2,6558,,,0,"Indicate the begin and end of the sync process to EOS

Send a trigger to EOS when a sync operation is initiated, and,
another trigger when the sync operation is complete.
Additionally, sync_interval value (from ml2_conf_arista.ini)
is passed down to EOS. This is used by EOS to timeout the
transaction.

Change-Id: I5110b24380ad848d03719e94283d69a7bc0ce972
Closes-Bug: 1371324
",git fetch https://review.opendev.org/openstack/neutron refs/changes/75/122575/2 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ml2/drivers/arista/mechanism_arista.py', 'neutron/tests/unit/ml2/drivers/arista/test_arista_mechanism_driver.py']",2,8457a19863300116f1ce21043d5044062868f460,bug/1371324," cfg.CONF.set_override('sync_interval', 10, ""ml2_arista"") def test_sync_start(self): self.drv.sync_start() cmds = ['enable', 'configure', 'cvx', 'service openstack', 'region RegionOne', 'sync start', 'exit', 'exit', 'exit'] self.drv._server.runCmds.assert_called_once_with(version=1, cmds=cmds) def test_sync_end(self): self.drv.sync_end() cmds = ['enable', 'configure', 'cvx', 'service openstack', 'region RegionOne', 'sync end', 'exit', 'exit', 'exit'] self.drv._server.runCmds.assert_called_once_with(version=1, cmds=cmds) 'sync interval %d' % cfg.CONF.ml2_arista.sync_interval,",,59,2
openstack%2Fdevstack~master~I5250356f782f149f87f3d0ffba3380911fa437be,openstack/devstack,master,I5250356f782f149f87f3d0ffba3380911fa437be,Fix pkg name for mysql-connector-python on SUSE,MERGED,2014-09-25 05:56:55.000000000,2014-09-28 01:22:26.000000000,2014-09-28 01:22:26.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 2750}, {'_account_id': 6547}, {'_account_id': 7102}, {'_account_id': 9009}]","[{'number': 1, 'created': '2014-09-25 05:56:55.000000000', 'files': ['files/rpms-suse/keystone', 'files/rpms-suse/neutron', 'files/rpms-suse/nova'], 'web_link': 'https://opendev.org/openstack/devstack/commit/293869fbacea6939a63a8a83311063af8376cf9c', 'message': 'Fix pkg name for mysql-connector-python on SUSE\n\nThe correct package name on SUSE distros is python-mysql-connector-python.\n\nChange-Id: I5250356f782f149f87f3d0ffba3380911fa437be\n'}]",0,123935,293869fbacea6939a63a8a83311063af8376cf9c,12,6,1,7102,,,0,"Fix pkg name for mysql-connector-python on SUSE

The correct package name on SUSE distros is python-mysql-connector-python.

Change-Id: I5250356f782f149f87f3d0ffba3380911fa437be
",git fetch https://review.opendev.org/openstack/devstack refs/changes/35/123935/1 && git format-patch -1 --stdout FETCH_HEAD,"['files/rpms-suse/keystone', 'files/rpms-suse/neutron', 'files/rpms-suse/nova']",3,293869fbacea6939a63a8a83311063af8376cf9c,mysql-connector-pkg-name,python-mysql-connector-python,python-mysql.connector,3,3
openstack%2Foslo-incubator~master~I639113f4a8da56d078a2a0b5becfc2b0ff0db6c5,openstack/oslo-incubator,master,I639113f4a8da56d078a2a0b5becfc2b0ff0db6c5,Fix i18n import,MERGED,2014-09-26 14:09:11.000000000,2014-09-28 01:09:47.000000000,2014-09-28 01:09:46.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-09-26 14:09:11.000000000', 'files': ['openstack/common/_i18n.py'], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/9ce1d96fb2e075fcd5b9ddbee728c0ee49d2be56', 'message': ""Fix i18n import\n\nWith update.py, the line:\n  from oslo import i18n\n\nis replaced to:\n  from <base> import i18n\n\nLet's avoid that.\n\nChange-Id: I639113f4a8da56d078a2a0b5becfc2b0ff0db6c5\n""}]",0,124411,9ce1d96fb2e075fcd5b9ddbee728c0ee49d2be56,7,3,1,1669,,,0,"Fix i18n import

With update.py, the line:
  from oslo import i18n

is replaced to:
  from <base> import i18n

Let's avoid that.

Change-Id: I639113f4a8da56d078a2a0b5becfc2b0ff0db6c5
",git fetch https://review.opendev.org/openstack/oslo-incubator refs/changes/11/124411/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/common/_i18n.py'],1,9ce1d96fb2e075fcd5b9ddbee728c0ee49d2be56,jd/fix-i18n,import oslo.i18n_translators = oslo.i18n.TranslatorFactory(domain='oslo'),from oslo import i18n_translators = i18n.TranslatorFactory(domain='oslo'),2,2
openstack%2Foslo-cookiecutter~master~I64ae9191863564e278a35d42ec9cd743a233028e,openstack/oslo-cookiecutter,master,I64ae9191863564e278a35d42ec9cd743a233028e,warn against sorting requirements,MERGED,2014-09-03 20:36:19.000000000,2014-09-28 01:04:33.000000000,2014-09-28 01:04:33.000000000,"[{'_account_id': 3}, {'_account_id': 5638}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-03 20:36:19.000000000', 'files': ['oslo.{{cookiecutter.module_name}}/test-requirements.txt', 'oslo.{{cookiecutter.module_name}}/requirements.txt'], 'web_link': 'https://opendev.org/openstack/oslo-cookiecutter/commit/2cc446fa31347e9f929abb6452eca54628070ee9', 'message': 'warn against sorting requirements\n\nChange-Id: I64ae9191863564e278a35d42ec9cd743a233028e\nAddresses-Bug: #1365061\n'}]",3,118858,2cc446fa31347e9f929abb6452eca54628070ee9,8,3,1,2472,,,0,"warn against sorting requirements

Change-Id: I64ae9191863564e278a35d42ec9cd743a233028e
Addresses-Bug: #1365061
",git fetch https://review.opendev.org/openstack/oslo-cookiecutter refs/changes/58/118858/1 && git format-patch -1 --stdout FETCH_HEAD,"['oslo.{{cookiecutter.module_name}}/test-requirements.txt', 'oslo.{{cookiecutter.module_name}}/requirements.txt']",2,2cc446fa31347e9f929abb6452eca54628070ee9,bug/1365061,"# The order of packages is significant, because pip processes them in the order # of appearance. Changing the order has an impact on the overall integration # process, which may cause wedges in the gate later. ",,8,0
openstack%2Foslo-cookiecutter~master~I38a8016ad7348d50b63dc6cd42118948f2cdf4de,openstack/oslo-cookiecutter,master,I38a8016ad7348d50b63dc6cd42118948f2cdf4de,Build universal wheels by default,MERGED,2014-09-08 20:02:43.000000000,2014-09-28 01:04:28.000000000,2014-09-28 01:04:27.000000000,"[{'_account_id': 3}, {'_account_id': 5638}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-08 20:02:43.000000000', 'files': ['oslo.{{cookiecutter.module_name}}/setup.cfg'], 'web_link': 'https://opendev.org/openstack/oslo-cookiecutter/commit/977e19a3da137896e076a57959b8c55fbfec132e', 'message': 'Build universal wheels by default\n\nChange-Id: I38a8016ad7348d50b63dc6cd42118948f2cdf4de\n'}]",0,119896,977e19a3da137896e076a57959b8c55fbfec132e,7,3,1,2472,,,0,"Build universal wheels by default

Change-Id: I38a8016ad7348d50b63dc6cd42118948f2cdf4de
",git fetch https://review.opendev.org/openstack/oslo-cookiecutter refs/changes/96/119896/1 && git format-patch -1 --stdout FETCH_HEAD,['oslo.{{cookiecutter.module_name}}/setup.cfg'],1,977e19a3da137896e076a57959b8c55fbfec132e,universal-wheels, [wheel] universal = true,,3,0
openstack%2Foslo-cookiecutter~master~Ieda600538c1868a64dd2fea7142e13ea3b846de5,openstack/oslo-cookiecutter,master,Ieda600538c1868a64dd2fea7142e13ea3b846de5,Simplify project doc templates,MERGED,2014-09-04 19:31:03.000000000,2014-09-28 01:03:45.000000000,2014-09-28 01:03:44.000000000,"[{'_account_id': 3}, {'_account_id': 5638}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-04 19:31:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-cookiecutter/commit/2f8258c63919cd72fa615074dd74b21643e09183', 'message': ""Simplify project doc templates\n\nRemove the separate file that includes README.rst.\n\nFix up the title string generation at the top of index.rst.\n\nAdd default api.rst.\n\nRemove virtualenvwrapper instructions from usage.rst since we don't need\nthose.\n\nChange-Id: Ieda600538c1868a64dd2fea7142e13ea3b846de5\n""}, {'number': 2, 'created': '2014-09-04 20:23:41.000000000', 'files': ['oslo.{{cookiecutter.module_name}}/doc/source/api.rst', 'oslo.{{cookiecutter.module_name}}/doc/source/installation.rst', 'oslo.{{cookiecutter.module_name}}/doc/source/usage.rst', 'oslo.{{cookiecutter.module_name}}/doc/source/history.rst', 'oslo.{{cookiecutter.module_name}}/doc/source/readme.rst', 'oslo.{{cookiecutter.module_name}}/doc/source/index.rst'], 'web_link': 'https://opendev.org/openstack/oslo-cookiecutter/commit/7861e1f25c06c8fb7256f13984e36df138f4e5ee', 'message': ""Simplify project doc templates\n\nRemove the separate file that includes README.rst.\n\nFix up the title string generation at the top of index.rst.\n\nAdd default api.rst.\n\nRemove virtualenvwrapper instructions from usage.rst since we don't need\nthose.\n\nAdd history.rst file.\n\nChange-Id: Ieda600538c1868a64dd2fea7142e13ea3b846de5\n""}]",0,119179,7861e1f25c06c8fb7256f13984e36df138f4e5ee,8,3,2,2472,,,0,"Simplify project doc templates

Remove the separate file that includes README.rst.

Fix up the title string generation at the top of index.rst.

Add default api.rst.

Remove virtualenvwrapper instructions from usage.rst since we don't need
those.

Add history.rst file.

Change-Id: Ieda600538c1868a64dd2fea7142e13ea3b846de5
",git fetch https://review.opendev.org/openstack/oslo-cookiecutter refs/changes/79/119179/1 && git format-patch -1 --stdout FETCH_HEAD,"['oslo.{{cookiecutter.module_name}}/doc/source/api.rst', 'oslo.{{cookiecutter.module_name}}/doc/source/installation.rst', 'oslo.{{cookiecutter.module_name}}/doc/source/usage.rst', 'oslo.{{cookiecutter.module_name}}/doc/source/index.rst', 'oslo.{{cookiecutter.module_name}}/doc/source/readme.rst']",5,2f8258c63919cd72fa615074dd74b21643e09183,clean-up-doc-templates,,.. include:: ../../README.rst ,25,20
openstack-attic%2Fidentity-api~master~I7374595e0f6ab50c0a004b6de9f7c2896f7c2fbc,openstack-attic/identity-api,master,I7374595e0f6ab50c0a004b6de9f7c2896f7c2fbc,Change all used passwords and secrets to 'secretsecret',MERGED,2014-09-09 07:56:54.000000000,2014-09-28 00:59:12.000000000,2014-09-28 00:59:12.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 964}, {'_account_id': 2448}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 6547}, {'_account_id': 8978}, {'_account_id': 10607}]","[{'number': 1, 'created': '2014-09-09 07:56:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack-attic/identity-api/commit/e2c76565d7f28bb26c2d9d9e9a0379e11f7080bf', 'message': ""Change 'secrete' to 'secret'\n\nRemoval of a spelling error.\n\nChange-Id: I7374595e0f6ab50c0a004b6de9f7c2896f7c2fbc\n""}, {'number': 2, 'created': '2014-09-27 08:29:14.000000000', 'files': ['v2.0/src/samples/credentials.xml', 'v3/src/markdown/identity-api-v3.md'], 'web_link': 'https://opendev.org/openstack-attic/identity-api/commit/31285d36a9e121e13f964605fb76df4d14bd4ed5', 'message': ""Change all used passwords and secrets to 'secretsecret'\n\nChange-Id: I7374595e0f6ab50c0a004b6de9f7c2896f7c2fbc\nPartial-Bug: #1372639\n""}]",0,120029,31285d36a9e121e13f964605fb76df4d14bd4ed5,21,9,2,167,,,0,"Change all used passwords and secrets to 'secretsecret'

Change-Id: I7374595e0f6ab50c0a004b6de9f7c2896f7c2fbc
Partial-Bug: #1372639
",git fetch https://review.opendev.org/openstack-attic/identity-api refs/changes/29/120029/1 && git format-patch -1 --stdout FETCH_HEAD,['v3/src/markdown/identity-api-v3.md'],1,e2c76565d7f28bb26c2d9d9e9a0379e11f7080bf,bug/1372639," ""password"": ""secret"" ""password"": ""secret"" ""password"": ""secret"" ""password"": ""secret"" ""password"": ""secret"" ""password"": ""secret"" ""password"": ""secret"" ""password"": ""secret"" ""original_password"": ""secret"""," ""password"": ""secrete"" ""password"": ""secrete"" ""password"": ""secrete"" ""password"": ""secrete"" ""password"": ""secrete"" ""password"": ""secrete"" ""password"": ""secrete"" ""password"": ""secrete"" ""original_password"": ""secrete""",9,9
openstack%2Fkeystone~master~Id48074b81f062408a6757c632f578837f6a3c87e,openstack/keystone,master,Id48074b81f062408a6757c632f578837f6a3c87e,Imported Translations from Transifex,MERGED,2014-09-25 06:18:45.000000000,2014-09-28 00:00:10.000000000,2014-09-28 00:00:09.000000000,"[{'_account_id': 3}, {'_account_id': 2903}]","[{'number': 1, 'created': '2014-09-25 06:18:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/af833815678ceeb3ab1d443248bcfac477fcf5cd', 'message': 'Imported Translations from Transifex\n\nChange-Id: Id48074b81f062408a6757c632f578837f6a3c87e\n'}, {'number': 2, 'created': '2014-09-26 06:05:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/ab32d431e4b0a2d7ae3ffe7a6b2a718a56a296d5', 'message': 'Imported Translations from Transifex\n\nChange-Id: Id48074b81f062408a6757c632f578837f6a3c87e\n'}, {'number': 3, 'created': '2014-09-27 06:17:29.000000000', 'files': ['keystone/locale/fr/LC_MESSAGES/keystone-log-warning.po'], 'web_link': 'https://opendev.org/openstack/keystone/commit/e0d638466097f984a70e24a78d03cf9736bdf67c', 'message': 'Imported Translations from Transifex\n\nChange-Id: Id48074b81f062408a6757c632f578837f6a3c87e\n'}]",0,123941,e0d638466097f984a70e24a78d03cf9736bdf67c,13,2,3,11131,,,0,"Imported Translations from Transifex

Change-Id: Id48074b81f062408a6757c632f578837f6a3c87e
",git fetch https://review.opendev.org/openstack/keystone refs/changes/41/123941/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/locale/fr/LC_MESSAGES/keystone-log-warning.po'],1,af833815678ceeb3ab1d443248bcfac477fcf5cd,transifex/translations,"# Bruno Cornec <bruno.cornec@hp.com>, 2014""POT-Creation-Date: 2014-09-25 06:18+0000\n"" ""PO-Revision-Date: 2014-09-24 10:06+0000\n"" ""Last-Translator: Bruno Cornec <bruno.cornec@hp.com>\n""msgstr ""Echec d'autorisation. %(exception)s depuis %(remote_addr)s""msgstr ""Jeton invalide dans _get_trust_id_for_request""msgstr ""%s n'est pas un dogpile.proxy.ProxyBackend""msgstr ""Verrou KVS relach (temps limite atteint) pour : %s"" #: keystone/common/ldap/core.py:1003""Le serveur LDAP ne prend pas en charge la pagination. Dsactivez la "" ""pagination dans keystone.conf pour viter de recevoir ce message."" #: keystone/common/ldap/core.py:1202""Mauvais mappage d'attribut additionnel: \""%s\"". Le format doit tre "" ""<ldap_attribute>:<keystone_attribute>"" #: keystone/common/ldap/core.py:1313""L'attribut ID %(id_attr)s pour l'objet LDAP %(dn)s a de multiples valeurs et "" ""par consquent ne peut tre utilis comme un ID. Obtention de l'ID depuis le "" ""DN  la place.""""Le point d'entre %(endpoint_id)s rfrenc en association avec la politique "" ""%(policy_id)s est introuvable.""""L'excution de la tche %(func_name)s a dpass intervalle de %(delay).2f "" ""secondes ""#: keystone/token/persistence/core.py:240","""POT-Creation-Date: 2014-09-19 06:36+0000\n"" ""PO-Revision-Date: 2014-06-20 20:10+0000\n"" ""Last-Translator: openstackjenkins <jenkins@openstack.org>\n""msgstr """"msgstr """"msgstr """"msgstr """" #: keystone/common/ldap/core.py:1002 #: keystone/common/ldap/core.py:1201 #: keystone/common/ldap/core.py:1312#: keystone/token/persistence/core.py:232",23,11
openstack%2Fceilometer~master~Ib31588f962d393cb2c1d9ee96e9d62b8898233cc,openstack/ceilometer,master,Ib31588f962d393cb2c1d9ee96e9d62b8898233cc,Fix bug in the documentation,MERGED,2014-09-23 13:18:34.000000000,2014-09-27 23:02:03.000000000,2014-09-27 23:02:02.000000000,"[{'_account_id': 3}, {'_account_id': 3012}, {'_account_id': 6537}, {'_account_id': 6676}, {'_account_id': 7052}, {'_account_id': 9562}, {'_account_id': 10987}, {'_account_id': 12038}]","[{'number': 1, 'created': '2014-09-23 13:18:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/b391ae09b2c71e655954156e93eca0d75a509e56', 'message': 'Fix bug in the documentation\n\nAdding information about some daemons of Ceilometer.\n\nChange-Id: Ib31588f962d393cb2c1d9ee96e9d62b8898233cc\nCloses-Bug: 1368938\n'}, {'number': 2, 'created': '2014-09-23 13:28:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/2f86caca0140f27fe3ef1cccb74aa3462e455270', 'message': 'Fix bug in the documentation\n\nAdding information about some daemons of Ceilometer.\n\nChange-Id: Ib31588f962d393cb2c1d9ee96e9d62b8898233cc\nCloses-Bug: 1368938\n'}, {'number': 3, 'created': '2014-09-25 09:34:20.000000000', 'files': ['doc/source/glossary.rst', 'doc/source/install/development.rst'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/269c9046cbb54ff019308edf8d09e9394f8523aa', 'message': 'Fix bug in the documentation\n\nAdding information about some daemons of Ceilometer.\n\nChange-Id: Ib31588f962d393cb2c1d9ee96e9d62b8898233cc\nCloses-Bug: 1368938\n'}]",15,123432,269c9046cbb54ff019308edf8d09e9394f8523aa,23,8,3,13273,,,0,"Fix bug in the documentation

Adding information about some daemons of Ceilometer.

Change-Id: Ib31588f962d393cb2c1d9ee96e9d62b8898233cc
Closes-Bug: 1368938
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/32/123432/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/glossary.rst', 'doc/source/install/development.rst']",2,b391ae09b2c71e655954156e93eca0d75a509e56,bug/1368938,"Ceilometer has several daemons. The basic are: :term:`compute agent` runs on the Nova compute node(s) while the :term:`central agent`, :term:`collector` and :term:`notification agent` run on the cloud's management node(s). In a development environment created by devstack_, these two are typicallyThe first group of daemons is necessary for ceilometer to work. The second group just includes alarms. The third group admits to work outside with Ceilometer. Compute agent This agent is responsible for collecting resource usage data of VM instances on individual compute nodes within an OpenStack deployment. A compute agent instance has to be installed on every compute node, installation instructions can be found in the Install the Compute agent for Telemetry section in the OpenStack Installation Guide. Just like the central agent, this component also does not need a direct database access. The samples are sent via AMQP to the collector. Central agent As the name of this agent shows, it is a central component in the Telemetry architecture. This agent is responsible for polling public REST APIs to retrieve additional information on OpenStack resources not already covered via notifications, and also for polling hardware resources over SNMP. The following services can be polled with this agent: OpenStack Compute (Nova) OpenStack Networking (Neutron) OpenStack Object Storage (Swift) OpenStack Block Storage (Cinder) Hardware resources via SNMP Energy consumption metrics via Kwapi framework To install and configure this service use the Install the Telemetry module section in the OpenStack Installation Guide. The central agent does not need direct database connection. The samples collected by this agent are sent via message queue to the collector service, which is responsible for persisting the data into the configured database back end. Notification agent The Telemetry module has a separate agent that is responsible for consuming notifications, named the notification agent. This component is responsible for notifications consuming from the message bus and their transformation into new samples. The different OpenStack services emit several notifications about the various types of events that happen in the system during normal operation. Not all these notifications are consumed by the Telemetry module, as the intention is only to capture the billable events and all those notifications that can be used for monitoring or profiling purposes. The notification agent filters them by the event type. If the ceilometer-agent-notification service is not installed and started, samples originating from notifications won't be generated. In case of the lack of notification based samples, the state of this service and the log file of Telemetry should be checked first. Collector The ceilometer-collector service receives the samples as metering messages from the message bus of the configured AMQP service. It stores these samples without any modification in the configured backend. The service has to run on a host machine from which it has access to the database. Alarms The definition of an alarm provides the rules that govern when a state transition should occur, and the actions to be taken thereon. Alarms provide user-oriented Monitoring-as-a-Service for resources running on OpenStack. This type of monitoring ensures you can automatically scale up or down a group of instances through the Orchestration module, but you can also use alarms for general-purpose awareness of your cloud resources' health. State transitions are detected by the alarm-evaluator, whereas the alarm-notifier effects the actual notification action. API An Application Programming Interface (API) offers a way to use the capabilities of a service by using predefined functions. ","Ceilometer has four daemons. The :term:`compute agent` runs on the Nova compute node(s) while the :term:`central agent` and :term:`collector` run on the cloud's management node(s). In a development environment created by devstack_, these two are typically",92,4
openstack%2Fironic~master~I3606c999fd1a17df975d9dd734c2582841ecc152,openstack/ironic,master,I3606c999fd1a17df975d9dd734c2582841ecc152,Adds openSUSE support for developer documentation,MERGED,2014-09-26 13:02:03.000000000,2014-09-27 22:46:21.000000000,2014-09-27 22:46:20.000000000,"[{'_account_id': 3}, {'_account_id': 2889}, {'_account_id': 6773}, {'_account_id': 12081}, {'_account_id': 13404}]","[{'number': 1, 'created': '2014-09-26 13:02:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fc9840b36836b8201f845d5ea66075561f01ef85', 'message': 'Adds openSUSE support for developer documentation\n\nSupport for openSUSE documentation is updated in this patch.\n\nChange-Id: I3606c999fd1a17df975d9dd734c2582841ecc152\n'}, {'number': 2, 'created': '2014-09-26 13:52:04.000000000', 'files': ['doc/source/dev/dev-quickstart.rst'], 'web_link': 'https://opendev.org/openstack/ironic/commit/3e57528b0b5ea9aa6bcf980c23f4cbaa839d5a9f', 'message': 'Adds openSUSE support for developer documentation\n\nSupport for openSUSE documentation is updated in this patch.\n\nChange-Id: I3606c999fd1a17df975d9dd734c2582841ecc152\n'}]",7,124395,3e57528b0b5ea9aa6bcf980c23f4cbaa839d5a9f,16,5,2,13404,,,0,"Adds openSUSE support for developer documentation

Support for openSUSE documentation is updated in this patch.

Change-Id: I3606c999fd1a17df975d9dd734c2582841ecc152
",git fetch https://review.opendev.org/openstack/ironic refs/changes/95/124395/2 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/dev/dev-quickstart.rst'],1,fc9840b36836b8201f845d5ea66075561f01ef85,, # openSUSE/SLE 12: sudo zypper install git git-review libffi-devel libmysqlclient-devel libopenssl-devel libxml2-devel libxslt-devel postgresql-devel python-devel python-flake8 python-nose python-pip python-setuptools-git python-testrepository python-tox python-virtualenv ,,3,0
openstack%2Fkeystone~master~I308fe2db811cc97185956ef60a739026ba815013,openstack/keystone,master,I308fe2db811cc97185956ef60a739026ba815013,Updated from global requirements,MERGED,2014-09-26 03:54:16.000000000,2014-09-27 22:43:40.000000000,2014-09-27 22:43:39.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 2903}, {'_account_id': 6486}]","[{'number': 1, 'created': '2014-09-26 03:54:16.000000000', 'files': ['test-requirements.txt', 'test-requirements-py3.txt'], 'web_link': 'https://opendev.org/openstack/keystone/commit/8168ce2032cca835489024927b7952b01bf12d0f', 'message': 'Updated from global requirements\n\nChange-Id: I308fe2db811cc97185956ef60a739026ba815013\n'}]",0,124259,8168ce2032cca835489024927b7952b01bf12d0f,14,4,1,11131,,,0,"Updated from global requirements

Change-Id: I308fe2db811cc97185956ef60a739026ba815013
",git fetch https://review.opendev.org/openstack/keystone refs/changes/59/124259/1 && git format-patch -1 --stdout FETCH_HEAD,"['test-requirements.txt', 'test-requirements-py3.txt']",2,8168ce2032cca835489024927b7952b01bf12d0f,openstack/requirements,kombu>=2.5.0,kombu>=2.4.8,2,2
openstack%2Fnova~master~Icccfb1a606bc9e14188d9f5c5f5667e35f2cb413,openstack/nova,master,Icccfb1a606bc9e14188d9f5c5f5667e35f2cb413,Fix Typo in method name - parse_Dom,MERGED,2014-09-26 00:16:38.000000000,2014-09-27 22:43:19.000000000,2014-09-27 22:43:16.000000000,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 5367}, {'_account_id': 5638}, {'_account_id': 8412}, {'_account_id': 8871}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-26 00:16:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/09933ec449fe91823a38e216044b013f2a2af0a1', 'message': 'Fix Typo in method name - parse_Dom\n\nSilly typo\n\nCloses-Bug: #1374158\n\nChange-Id: Icccfb1a606bc9e14188d9f5c5f5667e35f2cb413\n'}, {'number': 2, 'created': '2014-09-26 03:10:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b2f4a49bdb90edc9265c86a402148e1ef0caccd2', 'message': 'Fix Typo in method name - parse_Dom\n\nSilly typo\n\nCloses-Bug: #1374158\n\nChange-Id: Icccfb1a606bc9e14188d9f5c5f5667e35f2cb413\n'}, {'number': 3, 'created': '2014-09-26 10:52:30.000000000', 'files': ['nova/virt/libvirt/config.py', 'nova/tests/virt/libvirt/test_config.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/480a932173219bda0f7ead61fd406fe4a366832c', 'message': 'Fix Typo in method name - parse_Dom\n\nSilly typo\n\nCloses-Bug: #1374158\n\nChange-Id: Icccfb1a606bc9e14188d9f5c5f5667e35f2cb413\n'}]",0,124231,480a932173219bda0f7ead61fd406fe4a366832c,31,9,3,5638,,,0,"Fix Typo in method name - parse_Dom

Silly typo

Closes-Bug: #1374158

Change-Id: Icccfb1a606bc9e14188d9f5c5f5667e35f2cb413
",git fetch https://review.opendev.org/openstack/nova refs/changes/31/124231/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/virt/libvirt/config.py'],1,09933ec449fe91823a38e216044b013f2a2af0a1,bug/1374158," super(LibvirtConfigGuestCPUNUMA, self).parse_dom(xmldoc)"," super(LibvirtConfigGuestCPUNUMA, self).parse_Dom(xmldoc)",1,1
openstack%2Fswift~master~If591375ca9cc87d52c7c9c6dc16c9fb4b49e99fc,openstack/swift,master,If591375ca9cc87d52c7c9c6dc16c9fb4b49e99fc,Fixes unit tests to clean up temporary directories,MERGED,2014-09-24 20:01:06.000000000,2014-09-27 22:43:12.000000000,2014-09-27 22:43:11.000000000,"[{'_account_id': 3}, {'_account_id': 330}, {'_account_id': 597}, {'_account_id': 2622}, {'_account_id': 7479}, {'_account_id': 9051}, {'_account_id': 13052}]","[{'number': 1, 'created': '2014-09-24 20:01:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/25fb790cc545e27a39dc76d42abbf62616c43e99', 'message': 'Fixes unit tests to clean up temporary directories\n\nThis patch fixes the unit tests to remove the temporary directories\ncreated during run of unit tests. Some of unit tests did not tear down\ncorrectly, whatever it had set it up for running. This would over period\nof time bloat up the tmp directory. As on date, there were around 49 tmp\ndirectories left uncleared per round of unit tests. This patch fixes it.\n\nChange-Id: If591375ca9cc87d52c7c9c6dc16c9fb4b49e99fc\n'}, {'number': 2, 'created': '2014-09-25 08:18:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ef2c3e45fa6779f275a5c949e6ec278d2ce6dacb', 'message': 'Fixes unit tests to clean up temporary directories\n\nThis patch fixes the unit tests to remove the temporary directories\ncreated during run of unit tests. Some of unit tests did not tear down\ncorrectly, whatever it had set it up for running. This would over period\nof time bloat up the tmp directory. As on date, there were around 49 tmp\ndirectories left uncleared per round of unit tests. This patch fixes it.\n\nChange-Id: If591375ca9cc87d52c7c9c6dc16c9fb4b49e99fc\n'}, {'number': 3, 'created': '2014-09-26 17:18:40.000000000', 'files': ['test/unit/obj/test_expirer.py', 'test/unit/cli/test_recon.py', 'test/unit/obj/test_server.py', 'test/unit/proxy/test_sysmeta.py', 'test/unit/common/middleware/test_xprofile.py', 'test/unit/common/middleware/test_dlo.py', 'test/unit/common/test_utils.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/0f93fff46ad098a360f719a762dde4d17e852fbc', 'message': 'Fixes unit tests to clean up temporary directories\n\nThis patch fixes the unit tests to remove the temporary directories\ncreated during run of unit tests. Some of unit tests did not tear down\ncorrectly, whatever it had set it up for running. This would over period\nof time bloat up the tmp directory. As on date, there were around 49 tmp\ndirectories left uncleared per round of unit tests. This patch fixes it.\n\nChange-Id: If591375ca9cc87d52c7c9c6dc16c9fb4b49e99fc\n'}]",2,123835,0f93fff46ad098a360f719a762dde4d17e852fbc,24,7,3,9051,,,0,"Fixes unit tests to clean up temporary directories

This patch fixes the unit tests to remove the temporary directories
created during run of unit tests. Some of unit tests did not tear down
correctly, whatever it had set it up for running. This would over period
of time bloat up the tmp directory. As on date, there were around 49 tmp
directories left uncleared per round of unit tests. This patch fixes it.

Change-Id: If591375ca9cc87d52c7c9c6dc16c9fb4b49e99fc
",git fetch https://review.opendev.org/openstack/swift refs/changes/35/123835/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/obj/test_expirer.py', 'test/unit/obj/test_server.py', 'test/unit/proxy/test_sysmeta.py', 'test/unit/common/middleware/test_xprofile.py', 'test/unit/common/middleware/test_dlo.py', 'test/unit/common/test_utils.py']",6,25fb790cc545e27a39dc76d42abbf62616c43e99,fix_unit_tests, rmtree(tmpdir) rmtree(tmpdir) rmtree(tmpdir),,36,11
openstack%2Fneutron~master~Ie7930ebedb12212886d45294132fefff7296e104,openstack/neutron,master,Ie7930ebedb12212886d45294132fefff7296e104,DVR to delete router namespaces for service ports,MERGED,2014-09-03 09:31:53.000000000,2014-09-27 22:37:19.000000000,2014-09-22 23:13:54.000000000,"[{'_account_id': 3}, {'_account_id': 704}, {'_account_id': 748}, {'_account_id': 2592}, {'_account_id': 4187}, {'_account_id': 5170}, {'_account_id': 6659}, {'_account_id': 6788}, {'_account_id': 6876}, {'_account_id': 7016}, {'_account_id': 7448}, {'_account_id': 8645}, {'_account_id': 9361}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9695}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9970}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10386}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10624}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-09-03 09:31:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3654c446765d01391e95b418e1eacf5afac50b0d', 'message': 'DVR to delete router namespaces for service ports\n\nEarlier merge that enabled LBaaS in DVR with review #114141\nhad not covered the removal of DVR router namespace on\nVIP Port deletion in ml2 plugin.\n\nHere we fix the ml2 plugin to attempt namespace removal for\nall dvr serviced ports.\n\nChange-Id: Ie7930ebedb12212886d45294132fefff7296e104\nCloses-Bug: #1364839\n'}, {'number': 2, 'created': '2014-09-04 07:03:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/60b7775563fd904b8068436d6cc870b965a28cd1', 'message': 'DVR to delete router namespaces for service ports\n\nEarlier merge that enabled LBaaS in DVR with review #114141\nhad not covered the removal of DVR router namespace on\nVIP Port deletion in ml2 plugin.\n\nHere we fix the ml2 plugin to attempt namespace removal for\nall dvr serviced ports.\n\nChange-Id: Ie7930ebedb12212886d45294132fefff7296e104\nCloses-Bug: #1364839\n'}, {'number': 3, 'created': '2014-09-05 19:09:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/38639ed1875992c954a51f6e445bead62a09094c', 'message': 'DVR to delete router namespaces for service ports\n\nEarlier merge that enabled LBaaS in DVR with review #114141\nhad not covered the removal of DVR router namespace on\nVIP Port deletion in ml2 plugin.\n\nHere we fix the ml2 plugin to attempt namespace removal for\nall dvr serviced ports.\n\nChange-Id: Ie7930ebedb12212886d45294132fefff7296e104\nCloses-Bug: #1364839\n'}, {'number': 4, 'created': '2014-09-08 18:20:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/113662f0370717208cf61f97e5908f62d71803b7', 'message': 'DVR to delete router namespaces for service ports\n\nEarlier merge that enabled LBaaS in DVR with review #114141\nhad not covered the removal of DVR router namespace on\nVIP Port deletion in ml2 plugin.\n\nHere we fix the ml2 plugin to attempt namespace removal for\nall dvr serviced ports.\n\nChange-Id: Ie7930ebedb12212886d45294132fefff7296e104\nCloses-Bug: #1364839\n'}, {'number': 5, 'created': '2014-09-10 05:06:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9ae015cd53228b1f7ed0d7c8251366ae24be6c54', 'message': 'DVR to delete router namespaces for service ports\n\nEarlier merge that enabled LBaaS in DVR with review #114141\nhad not covered the removal of DVR router namespace on\nVIP Port deletion in ml2 plugin.\n\nHere we fix the ml2 plugin to attempt namespace removal for\nall dvr serviced ports.\n\nChange-Id: Ie7930ebedb12212886d45294132fefff7296e104\nCloses-Bug: #1364839\n'}, {'number': 6, 'created': '2014-09-15 17:27:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/7a37b11323873d7caa9eb8b75213eb6b75b8f01f', 'message': 'DVR to delete router namespaces for service ports\n\nEarlier merge that enabled LBaaS in DVR with review #114141\nhad not covered the removal of DVR router namespace on\nVIP Port deletion in ml2 plugin.\n\nHere we fix the ml2 plugin to attempt namespace removal for\nall dvr serviced ports.\n\nChange-Id: Ie7930ebedb12212886d45294132fefff7296e104\nCloses-Bug: #1364839\n'}, {'number': 7, 'created': '2014-09-17 18:17:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/bb57847cf97439b239c4f2a476bbd5e72bef3490', 'message': 'DVR to delete router namespaces for service ports\n\nEarlier merge that enabled LBaaS in DVR with review #114141\nhad not covered the removal of DVR router namespace on\nVIP Port deletion in ml2 plugin.\n\nHere we fix the ml2 plugin to attempt namespace removal for\nall dvr serviced ports.\n\nChange-Id: Ie7930ebedb12212886d45294132fefff7296e104\nCloses-Bug: #1364839\n'}, {'number': 8, 'created': '2014-09-19 03:18:48.000000000', 'files': ['neutron/db/l3_dvrscheduler_db.py', 'neutron/tests/unit/ml2/test_ml2_plugin.py', 'neutron/tests/unit/test_l3_schedulers.py', 'neutron/plugins/ml2/plugin.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/9cd96fe116ac1d990410cb1135473bd15c1f7dce', 'message': 'DVR to delete router namespaces for service ports\n\nEarlier merge that enabled LBaaS in DVR with review #114141\nhad not covered the removal of DVR router namespace on\nVIP Port deletion in ml2 plugin.\n\nHere we fix the ml2 plugin to attempt namespace removal for\nall dvr serviced ports.\n\nChange-Id: Ie7930ebedb12212886d45294132fefff7296e104\nCloses-Bug: #1364839\n'}]",5,118580,9cd96fe116ac1d990410cb1135473bd15c1f7dce,258,36,8,9361,,,0,"DVR to delete router namespaces for service ports

Earlier merge that enabled LBaaS in DVR with review #114141
had not covered the removal of DVR router namespace on
VIP Port deletion in ml2 plugin.

Here we fix the ml2 plugin to attempt namespace removal for
all dvr serviced ports.

Change-Id: Ie7930ebedb12212886d45294132fefff7296e104
Closes-Bug: #1364839
",git fetch https://review.opendev.org/openstack/neutron refs/changes/80/118580/8 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/db/l3_dvrscheduler_db.py', 'neutron/tests/unit/ml2/test_ml2_plugin.py', 'neutron/plugins/ml2/plugin.py']",3,3654c446765d01391e95b418e1eacf5afac50b0d,bug/1364839," if is_dvr_enabled and utils.is_dvr_serviced( port['device_owner']): router_info = l3plugin.dvr_deletens_if_no_port(context, id)"," if ""compute:"" in port['device_owner'] and is_dvr_enabled: router_info = l3plugin.dvr_deletens_if_no_vm(context, id)",6,5
openstack%2Fhorizon~master~Id81a5e108644f99ee939296b54835f4f4ac2aaa7,openstack/horizon,master,Id81a5e108644f99ee939296b54835f4f4ac2aaa7,Disable buttons opening the modal forms on page unload,MERGED,2014-09-08 15:40:41.000000000,2014-09-27 22:25:04.000000000,2014-09-27 22:25:03.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 2455}, {'_account_id': 6638}, {'_account_id': 6914}, {'_account_id': 8040}, {'_account_id': 8577}, {'_account_id': 9981}, {'_account_id': 11880}, {'_account_id': 11881}, {'_account_id': 12355}]","[{'number': 1, 'created': '2014-09-08 15:40:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/5df90884a0b9872b0f9ab20ce628258a5b69db97', 'message': ""Add atribute for buttons in table\n\nFixed bug when we have ability click to buttons in table,\nduring a spiner works(after click in modal dialog). Add\n'disable' attribute for this buttons.\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\n""}, {'number': 2, 'created': '2014-09-08 19:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/d5aa757ec884aaa64f607822644f3237f72e4a56', 'message': ""Disable buttons opening the modal forms on page unload\n\nFixed bug when we have ability click to buttons in table,\nduring a spiner works(after click in modal dialog). Add\n'disable' attribute for this buttons.\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\n""}, {'number': 3, 'created': '2014-09-08 20:06:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/ca74fce3081dd6332338dbdf0195260bc631e1ff', 'message': 'Disable buttons opening the modal forms on page unload\n\n If click button (which open modal form) when page start reload, form will disappear then page download.  That this would not happens --- remove ability click to button before page load\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\n'}, {'number': 4, 'created': '2014-09-08 20:07:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/1961d491d1b3413d9bfe2e9035d0d2b1658e4b3f', 'message': 'Disable buttons opening the modal forms on page unload\n\n If click button (which open modal form) when page start reload, form\nwill disappear then page download.  That this would not happens --- \nremove ability click to button before page load\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\n'}, {'number': 5, 'created': '2014-09-08 20:41:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/a59adc1e3cf2a601be4993c2b09abc95aa373f77', 'message': 'Disable buttons opening the modal forms on page unload\n\n If button which opens a modal form is clicked after page started reloading, form will disappear once reload is finished. To avoid this glitch make user unable to press such buttons until reload finishes.\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\n'}, {'number': 6, 'created': '2014-09-08 20:42:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/2ddaa905238ad81d2e9ef1aadd41ba392632202d', 'message': 'Disable buttons opening the modal forms on page unload\n\n If button which opens a modal form is clicked after page started \nreloading, form will disappear once reload is finished. To avoid this \nglitch make user unable to press such buttons until reload finishes.\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\n'}, {'number': 7, 'created': '2014-09-17 13:44:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/8f247950ffc7359db8e39071511f39364afaacc8', 'message': 'Disable buttons opening the modal forms on page unload\n\n If button which opens a modal form is clicked after page started \nreloading, form will disappear once reload is finished. To avoid this \nglitch make user unable to press such buttons until reload finishes.\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\nCo-Authored-By: Timur Sufiev <tsufiev@mirantis.com>\n'}, {'number': 8, 'created': '2014-09-18 12:35:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/8e3275db6c5ce096155d019f1d880676bbb3db99', 'message': 'Disable buttons opening the modal forms on page unload\n\n If button which opens a modal form is clicked after page started\nreloading, form will disappear once reload is finished. To avoid this\nglitch make user unable to press such buttons until reload finishes.\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\nCo-Authored-By: Timur Sufiev <tsufiev@mirantis.com>\n'}, {'number': 9, 'created': '2014-09-18 12:35:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/15ec8becf7b80cdbece6601105c2edef545235a5', 'message': 'Disable buttons opening the modal forms on page unload\n\n If button which opens a modal form is clicked after page started\nreloading, form will disappear once reload is finished. To avoid this\nglitch make user unable to press such buttons until reload finishes.\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\nCo-Authored-By: Timur Sufiev <tsufiev@mirantis.com>\n'}, {'number': 10, 'created': '2014-09-18 15:52:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/5881c3b344f62e6373dc8d032855abdf8afd8394', 'message': 'Disable buttons opening the modal forms on page unload\n\n If button which opens a modal form is clicked after page started\nreloading, form will disappear once reload is finished. To avoid this\nglitch make user unable to press such buttons until reload finishes.\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\nCo-Authored-By: Timur Sufiev <tsufiev@mirantis.com>\n'}, {'number': 11, 'created': '2014-09-23 14:11:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/4924531d386f0b0678c159389ca0b853f9b5d7aa', 'message': 'Disable buttons opening the modal forms on page unload\n\n If button which opens a modal form is clicked after page started\nreloading, form will disappear once reload is finished. To avoid this\nglitch make user unable to press such buttons until reload finishes.\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\nCo-Authored-By: Timur Sufiev <tsufiev@mirantis.com>\n'}, {'number': 12, 'created': '2014-09-25 10:00:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/933c8c66676d1781c49bdf9d09d6f3ed2cd38be1', 'message': 'Disable buttons opening the modal forms on page unload\n\n If button which opens a modal form is clicked after page started\nreloading, form will disappear once reload is finished. To avoid this\nglitch make user unable to press such buttons until reload finishes.\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\n'}, {'number': 13, 'created': '2014-09-25 10:53:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/fbd18a343c634a525e923e47b5d99eafd50967a6', 'message': 'Disable buttons opening the modal forms on page unload\n\n If button which opens a modal form is clicked after page started\nreloading, form will disappear once reload is finished. To avoid this\nglitch make user unable to press such buttons until reload finishes.\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\n'}, {'number': 14, 'created': '2014-09-25 11:03:10.000000000', 'files': ['horizon/static/horizon/js/horizon.modals.js'], 'web_link': 'https://opendev.org/openstack/horizon/commit/f84243074851388f3b06ab8ec20eba1d804f75a5', 'message': 'Disable buttons opening the modal forms on page unload\n\nIf button which opens a modal form is clicked after page started\nreloading, form will disappear once reload is finished. To avoid this\nglitch make user unable to press such buttons until reload finishes.\n\nChange-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7\nCloses-Bug: #1357331\n'}]",9,119793,f84243074851388f3b06ab8ec20eba1d804f75a5,47,11,14,12355,,,0,"Disable buttons opening the modal forms on page unload

If button which opens a modal form is clicked after page started
reloading, form will disappear once reload is finished. To avoid this
glitch make user unable to press such buttons until reload finishes.

Change-Id: Id81a5e108644f99ee939296b54835f4f4ac2aaa7
Closes-Bug: #1357331
",git fetch https://review.opendev.org/openstack/horizon refs/changes/93/119793/14 && git format-patch -1 --stdout FETCH_HEAD,['horizon/static/horizon/js/horizon.modals.js'],1,5df90884a0b9872b0f9ab20ce628258a5b69db97,bug/1357331," // Disable buttons and dropdown $(window).on('beforeunload', function() { $('.ajax-modal').attr('disabled', true) }); ",,5,0
openstack%2Fkeystone~master~I46f4b0c45e38cc1315e320369a2ba7d2279eb16f,openstack/keystone,master,I46f4b0c45e38cc1315e320369a2ba7d2279eb16f,Add version attribute to the SAML2 Assertion object.,MERGED,2014-09-25 15:58:01.000000000,2014-09-27 22:24:56.000000000,2014-09-27 22:24:55.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 2903}, {'_account_id': 6482}, {'_account_id': 7725}]","[{'number': 1, 'created': '2014-09-25 15:58:01.000000000', 'files': ['keystone/contrib/federation/idp.py', 'keystone/tests/test_v3_federation.py', 'keystone/tests/saml2/signed_saml2_assertion.xml'], 'web_link': 'https://opendev.org/openstack/keystone/commit/1e985609f7535e340acd1d571d281644785338f2', 'message': 'Add version attribute to the SAML2 Assertion object.\n\nAttribute ``version`` in the SAML2 Assertion should be non-empty.\n\nChange-Id: I46f4b0c45e38cc1315e320369a2ba7d2279eb16f\nCloses-Bug: #1373961\n'}]",3,124092,1e985609f7535e340acd1d571d281644785338f2,19,5,1,8978,,,0,"Add version attribute to the SAML2 Assertion object.

Attribute ``version`` in the SAML2 Assertion should be non-empty.

Change-Id: I46f4b0c45e38cc1315e320369a2ba7d2279eb16f
Closes-Bug: #1373961
",git fetch https://review.opendev.org/openstack/keystone refs/changes/92/124092/1 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/contrib/federation/idp.py', 'keystone/tests/test_v3_federation.py', 'keystone/tests/saml2/signed_saml2_assertion.xml']",3,1e985609f7535e340acd1d571d281644785338f2,bug_1373961,"<ns0:Assertion xmlns:ns0=""urn:oasis:names:tc:SAML:2.0:assertion"" xmlns:ns1=""http://www.w3.org/2000/09/xmldsig#"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" ID=""9a22528bfe194b2880edce5d60414d6a"" IssueInstant=""2014-08-19T10:53:57Z"" Version=""2.0"">","<ns0:Assertion xmlns:ns0=""urn:oasis:names:tc:SAML:2.0:assertion"" xmlns:ns1=""http://www.w3.org/2000/09/xmldsig#"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" ID=""9a22528bfe194b2880edce5d60414d6a"" IssueInstant=""2014-08-19T10:53:57Z"">",19,1
openstack%2Fcinder~master~I7069111dfd22d4fca92615b5730854f6d696ec13,openstack/cinder,master,I7069111dfd22d4fca92615b5730854f6d696ec13,NetApp fix for controller preferred path,MERGED,2014-09-05 07:57:50.000000000,2014-09-27 22:24:47.000000000,2014-09-27 22:24:46.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 2417}, {'_account_id': 5538}, {'_account_id': 6094}, {'_account_id': 6491}, {'_account_id': 9008}, {'_account_id': 10622}, {'_account_id': 11047}, {'_account_id': 11811}, {'_account_id': 12017}, {'_account_id': 13049}]","[{'number': 1, 'created': '2014-09-05 07:57:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b935d80ee94b8df2f7d79f8c61790b03cb9d7bd3', 'message': 'NetApp fix for controller preferred path\n\nThis patch optimizes the data path by choosing\nthe preferred path to access the volume by selecting\nthe controller owning the volume.\n\nCloses-Bug: #1365881\n\nChange-Id: I7069111dfd22d4fca92615b5730854f6d696ec13\n'}, {'number': 2, 'created': '2014-09-17 16:39:47.000000000', 'files': ['cinder/tests/test_netapp_eseries_iscsi.py', 'cinder/volume/drivers/netapp/eseries/iscsi.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/e6ea26ea1272f9407879b822b3913b4308e3f2f8', 'message': 'NetApp fix for controller preferred path\n\nThis patch optimizes the data path by choosing\nthe preferred path to access the volume by selecting\nthe controller owning the volume.\n\nCloses-Bug: #1365881\n\nChange-Id: I7069111dfd22d4fca92615b5730854f6d696ec13\n'}]",4,119311,e6ea26ea1272f9407879b822b3913b4308e3f2f8,22,12,2,6094,,,0,"NetApp fix for controller preferred path

This patch optimizes the data path by choosing
the preferred path to access the volume by selecting
the controller owning the volume.

Closes-Bug: #1365881

Change-Id: I7069111dfd22d4fca92615b5730854f6d696ec13
",git fetch https://review.opendev.org/openstack/cinder refs/changes/11/119311/2 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/test_netapp_eseries_iscsi.py', 'cinder/volume/drivers/netapp/eseries/iscsi.py']",2,b935d80ee94b8df2f7d79f8c61790b03cb9d7bd3,de12," return self._get_latest_volume(uid) def _get_latest_volume(self, uid): label = utils.convert_uuid_to_es_fmt(uid) for vol in self._client.list_volumes(): if vol.get('label') == label: self._cache_volume(vol) return self._get_cached_volume(label) raise exception.NetAppDriverException(_(""Volume %s not found.""), uid) vol = self._get_latest_volume(volume['id']) iscsi_details = self._get_iscsi_service_details() iscsi_det = self._get_iscsi_portal_for_vol(vol, iscsi_details) ports = [] iscsi_det['tcp_port'] = port.get('tcpListenPort') iscsi_det['controller'] = port.get('controllerId') ports.append(iscsi_det) if not ports: msg = _('No good iscsi portals found for %s.') raise exception.NetAppDriverException( msg % self._client.get_system_id()) return ports def _get_iscsi_portal_for_vol(self, volume, portals, anyController=True): """"""Get the iscsi portal info relevant to volume."""""" for portal in portals: if portal.get('controller') == volume.get('currentManager'): return portal if anyController and portals: return portals[0] msg = _('No good iscsi portal found in supplied list for %s.')"," for vol in self._client.list_volumes(): if vol.get('label') == label: self._cache_volume(vol) break return self._get_cached_volume(label) vol = self._get_volume(volume['id']) iscsi_det = self._get_iscsi_service_details() iscsi_det['tcp_port'] = port.get('tcpListenPort', '3260') return iscsi_det msg = _('No good iscsi portal information found for %s.')",75,11
openstack%2Fcinder~master~Ie33a63c6ecd443b900fa7bed3a1ec5b6bb0065e6,openstack/cinder,master,Ie33a63c6ecd443b900fa7bed3a1ec5b6bb0065e6,Fix running unit tests with coverage,MERGED,2014-09-02 12:08:23.000000000,2014-09-27 22:24:38.000000000,2014-09-27 22:24:37.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 7198}, {'_account_id': 9533}, {'_account_id': 11811}, {'_account_id': 11904}]","[{'number': 1, 'created': '2014-09-02 12:08:23.000000000', 'files': ['run_tests.sh'], 'web_link': 'https://opendev.org/openstack/cinder/commit/373fad9dc55c3d8eb7d50d2b2f89664fec61bfae', 'message': 'Fix running unit tests with coverage\n\nThe path to this repo is added to PYTHONPATH when the coverage flag is\nspecified so that the cinder entry points are discovered.\n\nChange-Id: Ie33a63c6ecd443b900fa7bed3a1ec5b6bb0065e6\nCloses-Bug: #1364347\n'}]",0,118317,373fad9dc55c3d8eb7d50d2b2f89664fec61bfae,10,6,1,7219,,,0,"Fix running unit tests with coverage

The path to this repo is added to PYTHONPATH when the coverage flag is
specified so that the cinder entry points are discovered.

Change-Id: Ie33a63c6ecd443b900fa7bed3a1ec5b6bb0065e6
Closes-Bug: #1364347
",git fetch https://review.opendev.org/openstack/cinder refs/changes/17/118317/1 && git format-patch -1 --stdout FETCH_HEAD,['run_tests.sh'],1,373fad9dc55c3d8eb7d50d2b2f89664fec61bfae,bug/1364347," if [ -z ""${PYTHONPATH:-}"" ]; then export PYTHONPATH=./ else export PYTHONPATH=$PYTHONPATH:./ fi",,5,0
openstack%2Fcinder~master~I8819437617d813fbabdb79dcdcaa529e8a30796a,openstack/cinder,master,I8819437617d813fbabdb79dcdcaa529e8a30796a,VMware: Add storage profile related unit tests,MERGED,2014-08-22 14:54:42.000000000,2014-09-27 22:24:29.000000000,2014-09-27 22:24:28.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 9008}, {'_account_id': 11047}, {'_account_id': 11811}, {'_account_id': 12017}, {'_account_id': 12033}]","[{'number': 1, 'created': '2014-08-22 14:54:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/d7c8d430271420e2d1a20ef2021881452b1fe94f', 'message': 'VMware: Add storage profile related unit tests\n\nThis patch adds missing unit tests related to storage profiles.\n\nChange-Id: I8819437617d813fbabdb79dcdcaa529e8a30796a\n'}, {'number': 2, 'created': '2014-08-25 10:14:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/8d6ceb42cf04fcae70e04f3f4c03aea3919c1014', 'message': 'VMware: Add storage profile related unit tests\n\nThis patch adds missing unit tests related to storage profiles.\n\nChange-Id: I8819437617d813fbabdb79dcdcaa529e8a30796a\n'}, {'number': 3, 'created': '2014-09-03 06:25:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/91d108c6d89f6a8f03100f1465bb2bc0f111dfb9', 'message': 'VMware: Add storage profile related unit tests\n\nThis patch adds missing unit tests related to storage profiles.\n\nChange-Id: I8819437617d813fbabdb79dcdcaa529e8a30796a\n'}, {'number': 4, 'created': '2014-09-17 06:50:40.000000000', 'files': ['cinder/tests/test_vmware_volumeops.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/5c4ec8b9083b05aa30cea53109e4f6dfc50d79d4', 'message': 'VMware: Add storage profile related unit tests\n\nThis patch adds missing unit tests related to storage profiles.\n\nChange-Id: I8819437617d813fbabdb79dcdcaa529e8a30796a\n'}]",0,116302,5c4ec8b9083b05aa30cea53109e4f6dfc50d79d4,23,7,4,9171,,,0,"VMware: Add storage profile related unit tests

This patch adds missing unit tests related to storage profiles.

Change-Id: I8819437617d813fbabdb79dcdcaa529e8a30796a
",git fetch https://review.opendev.org/openstack/cinder refs/changes/02/116302/3 && git format-patch -1 --stdout FETCH_HEAD,['cinder/tests/test_vmware_volumeops.py'],1,d7c8d430271420e2d1a20ef2021881452b1fe94f,profile_tests," # Clear side effects. self.session.invoke_api.side_effect = None # Clear side effects. self.session.invoke_api.side_effect = None # Clear side effects. self.session.invoke_api.side_effect = None # Clear side effects. self.session.invoke_api.side_effect = None # Clear side effects. self.session.invoke_api.side_effect = None def test_get_all_profiles(self): profile_ids = [1, 2] methods = ['PbmQueryProfile', 'PbmRetrieveContent'] def invoke_api_side_effect(module, method, *args, **kwargs): self.assertEqual(self.session.pbm, module) self.assertEqual(methods.pop(0), method) self.assertEqual(self.session.pbm.service_content.profileManager, args[0]) if method == 'PbmQueryProfile': self.assertEqual('STORAGE', kwargs['resourceType'].resourceType) return profile_ids self.assertEqual(profile_ids, kwargs['profileIds']) self.session.invoke_api.side_effect = invoke_api_side_effect self.vops.get_all_profiles() self.assertEqual(2, self.session.invoke_api.call_count) # Clear side effects. self.session.invoke_api.side_effect = None def test_get_all_profiles_with_no_profiles(self): self.session.invoke_api.return_value = [] res_type = mock.sentinel.res_type self.session.pbm.client.factory.create.return_value = res_type profiles = self.vops.get_all_profiles() self.session.invoke_api.assert_called_once_with( self.session.pbm, 'PbmQueryProfile', self.session.pbm.service_content.profileManager, resourceType=res_type) self.assertEqual([], profiles) def _create_profile(self, profile_id, name): profile = mock.Mock() profile.profileId = profile_id profile.name = name return profile @mock.patch('cinder.volume.drivers.vmware.volumeops.VMwareVolumeOps.' 'get_all_profiles') def test_retrieve_profile_id(self, get_all_profiles): profiles = [self._create_profile(str(i), 'profile-%d' % i) for i in range(0, 10)] get_all_profiles.return_value = profiles exp_profile_id = '5' profile_id = self.vops.retrieve_profile_id( 'profile-%s' % exp_profile_id) self.assertEqual(exp_profile_id, profile_id) get_all_profiles.assert_called_once_with() @mock.patch('cinder.volume.drivers.vmware.volumeops.VMwareVolumeOps.' 'get_all_profiles') def test_retrieve_profile_id_with_invalid_profile(self, get_all_profiles): profiles = [self._create_profile(str(i), 'profile-%d' % i) for i in range(0, 10)] get_all_profiles.return_value = profiles profile_id = self.vops.retrieve_profile_id('profile-%s' % (i + 1)) self.assertIsNone(profile_id) get_all_profiles.assert_called_once_with() def test_filter_matching_hubs(self): hubs = mock.Mock() profile_id = 'profile-0' self.vops.filter_matching_hubs(hubs, profile_id) self.session.invoke_api.assert_called_once_with( self.session.pbm, 'PbmQueryMatchingHub', self.session.pbm.service_content.placementSolver, hubsToSearch=hubs, profile=profile_id) ",,93,0
openstack%2Fcinder~master~I3865654e219c05dcec3aaab07c4cee0658fe181e,openstack/cinder,master,I3865654e219c05dcec3aaab07c4cee0658fe181e,VMware: Relocate volume to compliant datastore,MERGED,2014-08-28 07:29:15.000000000,2014-09-27 22:24:20.000000000,2014-09-27 22:24:20.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 6491}, {'_account_id': 7198}, {'_account_id': 9008}, {'_account_id': 9171}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12017}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-08-28 07:29:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a20a3a5b19513cdf56c75eb9a44cbeab559cdd0c', 'message': ""VMware: Relocate volume to compliant datastore\n\nDuring attach to a nova instance, the backing VM corresponding to the\nvolume is relocated only if the nova instance's ESX host cannot access\nthe backing's current datastore. The storage profile is ignored and\nthe volume's virtual disk might end up in a non-compliant datastore.\nThis patch fixes the problem by checking storage profile compliance of\nthe current datastore.\n\nChange-Id: I3865654e219c05dcec3aaab07c4cee0658fe181e\nCloses-Bug: #1301348\n""}, {'number': 2, 'created': '2014-09-03 06:19:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/d995b0bebe35bf2bc5b1dc5e5c8c0ebbc5217a34', 'message': ""VMware: Relocate volume to compliant datastore\n\nDuring attach to a nova instance, the backing VM corresponding to the\nvolume is relocated only if the nova instance's ESX host cannot access\nthe backing's current datastore. The storage profile is ignored and\nthe volume's virtual disk might end up in a non-compliant datastore.\nThis patch fixes the problem by checking storage profile compliance of\nthe current datastore.\n\nChange-Id: I3865654e219c05dcec3aaab07c4cee0658fe181e\nCloses-Bug: #1301348\n""}, {'number': 3, 'created': '2014-09-17 06:39:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1c8ac039b35d9346be8a2703729a835c0cfc7a7b', 'message': ""VMware: Relocate volume to compliant datastore\n\nDuring attach to a nova instance, the backing VM corresponding to the\nvolume is relocated only if the nova instance's ESX host cannot access\nthe backing's current datastore. The storage profile is ignored and\nthe volume's virtual disk might end up in a non-compliant datastore.\nThis patch fixes the problem by checking storage profile compliance of\nthe current datastore.\n\nChange-Id: I3865654e219c05dcec3aaab07c4cee0658fe181e\nCloses-Bug: #1301348\n""}, {'number': 4, 'created': '2014-09-25 11:26:21.000000000', 'files': ['cinder/volume/drivers/vmware/error_util.py', 'cinder/tests/test_vmware_volumeops.py', 'cinder/volume/drivers/vmware/vmdk.py', 'cinder/volume/drivers/vmware/volumeops.py', 'cinder/tests/test_vmware_vmdk.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/4be8913520f5e9fe4109ade101da9509e4a83360', 'message': ""VMware: Relocate volume to compliant datastore\n\nDuring attach to a nova instance, the backing VM corresponding to the\nvolume is relocated only if the nova instance's ESX host cannot access\nthe backing's current datastore. The storage profile is ignored and\nthe volume's virtual disk might end up in a non-compliant datastore.\nThis patch fixes the problem by checking storage profile compliance of\nthe current datastore.\n\nChange-Id: I3865654e219c05dcec3aaab07c4cee0658fe181e\nCloses-Bug: #1301348\n""}]",3,117443,4be8913520f5e9fe4109ade101da9509e4a83360,30,11,4,9171,,,0,"VMware: Relocate volume to compliant datastore

During attach to a nova instance, the backing VM corresponding to the
volume is relocated only if the nova instance's ESX host cannot access
the backing's current datastore. The storage profile is ignored and
the volume's virtual disk might end up in a non-compliant datastore.
This patch fixes the problem by checking storage profile compliance of
the current datastore.

Change-Id: I3865654e219c05dcec3aaab07c4cee0658fe181e
Closes-Bug: #1301348
",git fetch https://review.opendev.org/openstack/cinder refs/changes/43/117443/2 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/vmware/error_util.py', 'cinder/tests/test_vmware_volumeops.py', 'cinder/volume/drivers/vmware/vmdk.py', 'cinder/volume/drivers/vmware/volumeops.py', 'cinder/tests/test_vmware_vmdk.py']",5,a20a3a5b19513cdf56c75eb9a44cbeab559cdd0c,bug/1301348," @mock.patch.object(VMDK_DRIVER, 'volumeops') @mock.patch.object(VMDK_DRIVER, '_relocate_backing') @mock.patch.object(VMDK_DRIVER, '_create_backing') def test_initialize_connection_with_instance_and_backing( self, create_backing, relocate_backing, vops): self._test_initialize_connection_with_instance_and_backing( create_backing, relocate_backing, vops) def _test_initialize_connection_with_instance_and_backing( self, create_backing, relocate_backing, vops): instance = mock.sentinel.instance connector = {'instance': instance} backing = mock.Mock(value=mock.sentinel.backing_value) vops.get_backing.return_value = backing host = mock.sentinel.host vops.get_host.return_value = host volume = {'name': 'vol-1', 'id': 1} relocate_backing.assert_called_once_with(volume, backing, host) self.assertFalse(create_backing.called) self.assertEqual('vmdk', conn_info['driver_volume_type']) self.assertEqual(backing.value, conn_info['data']['volume']) self.assertEqual(volume['id'], conn_info['data']['volume_id']) @mock.patch.object(VMDK_DRIVER, 'volumeops') @mock.patch.object(VMDK_DRIVER, '_relocate_backing') @mock.patch.object(VMDK_DRIVER, '_create_backing') def test_initialize_connection_with_instance_and_no_backing( self, create_backing, relocate_backing, vops): self._test_initialize_connection_with_instance_and_no_backing( create_backing, relocate_backing, vops) def _test_initialize_connection_with_instance_and_no_backing( self, create_backing, relocate_backing, vops): instance = mock.sentinel.instance connector = {'instance': instance} vops.get_backing.return_value = None host = mock.sentinel.host vops.get_host.return_value = host backing = mock.Mock(value=mock.sentinel.backing_value) create_backing.return_value = backing volume = {'name': 'vol-1', 'id': 1} conn_info = self._driver.initialize_connection(volume, connector) create_backing.assert_called_once_with(volume, host) self.assertFalse(relocate_backing.called) self.assertEqual('vmdk', conn_info['driver_volume_type']) self.assertEqual(backing.value, conn_info['data']['volume']) self.assertEqual(volume['id'], conn_info['data']['volume_id']) @mock.patch.object(VMDK_DRIVER, 'volumeops') @mock.patch.object(VMDK_DRIVER, '_relocate_backing') @mock.patch.object(VMDK_DRIVER, '_create_backing_in_inventory') def test_initialize_connection_with_no_instance_and_no_backing( self, create_backing_in_inventory, relocate_backing, vops): self._test_initialize_connection_with_no_instance_and_no_backing( create_backing_in_inventory, relocate_backing, vops) def _test_initialize_connection_with_no_instance_and_no_backing( self, create_backing_in_inventory, relocate_backing, vops): vops.get_backing.return_value = None host = mock.sentinel.host vops.get_host.return_value = host backing = mock.Mock(value=mock.sentinel.backing_value) create_backing_in_inventory.return_value = backing connector = {} volume = {'name': 'vol-1', 'id': 1} conn_info = self._driver.initialize_connection(volume, connector) create_backing_in_inventory.assert_called_once_with(volume) self.assertFalse(relocate_backing.called) self.assertEqual('vmdk', conn_info['driver_volume_type']) self.assertEqual(backing.value, conn_info['data']['volume']) self.assertEqual(volume['id'], conn_info['data']['volume_id']) @mock.patch.object(VMDK_DRIVER, 'volumeops') @mock.patch.object(VMDK_DRIVER, '_relocate_backing') @mock.patch.object(VMDK_DRIVER, '_create_backing') def test_initialize_connection_with_instance_and_backing( self, create_backing, relocate_backing, vops): self._test_initialize_connection_with_instance_and_backing( create_backing, relocate_backing, vops) @mock.patch.object(VMDK_DRIVER, 'volumeops') @mock.patch.object(VMDK_DRIVER, '_relocate_backing') @mock.patch.object(VMDK_DRIVER, '_create_backing') def test_initialize_connection_with_instance_and_no_backing( self, create_backing, relocate_backing, vops): self._test_initialize_connection_with_instance_and_no_backing( create_backing, relocate_backing, vops) @mock.patch.object(VMDK_DRIVER, 'volumeops') @mock.patch.object(VMDK_DRIVER, '_relocate_backing') @mock.patch.object(VMDK_DRIVER, '_create_backing_in_inventory') def test_initialize_connection_with_no_instance_and_no_backing( self, create_backing_in_inventory, relocate_backing, vops): self._test_initialize_connection_with_no_instance_and_no_backing( create_backing_in_inventory, relocate_backing, vops) @mock.patch.object(VMDK_DRIVER, 'volumeops') @mock.patch.object(VMDK_DRIVER, 'ds_sel') def test_relocate_backing_nop(self, ds_sel, vops): volume = {'name': 'vol-1', 'size': 1} datastore = mock.sentinel.datastore vops.get_datastore.return_value = datastore profile = mock.sentinel.profile vops.get_profile.return_value = profile vops.is_datastore_accessible.return_value = True ds_sel.is_datastore_compliant.return_value = True backing = mock.sentinel.backing host = mock.sentinel.host self._driver._relocate_backing(volume, backing, host) vops.is_datastore_accessible.assert_called_once_with(datastore, host) ds_sel.is_datastore_compliant.assert_called_once_with(datastore, profile) self.assertFalse(vops.relocate_backing.called) @mock.patch.object(VMDK_DRIVER, 'volumeops') @mock.patch.object(VMDK_DRIVER, 'ds_sel') def test_relocate_backing_with_no_datastore( self, ds_sel, vops): volume = {'name': 'vol-1', 'size': 1} profile = mock.sentinel.profile vops.get_profile.return_value = profile vops.is_datastore_accessible.return_value = True ds_sel.is_datastore_compliant.return_value = False ds_sel.select_datastore.return_value = [] backing = mock.sentinel.backing host = mock.sentinel.host self.assertRaises(error_util.NoValidDatastoreException, self._driver._relocate_backing, volume, backing, host) ds_sel.select_datastore.assert_called_once_with( {hub.DatastoreSelector.SIZE_BYTES: volume['size'] * units.Gi, hub.DatastoreSelector.PROFILE_NAME: profile}, hosts=[host]) self.assertFalse(vops.relocate_backing.called) @mock.patch.object(VMDK_DRIVER, 'volumeops') @mock.patch.object(VMDK_DRIVER, '_get_volume_group_folder') @mock.patch.object(VMDK_DRIVER, 'ds_sel') def test_relocate_backing( self, ds_sel, get_volume_group_folder, vops): volume = {'name': 'vol-1', 'size': 1} vops.is_datastore_accessible.return_value = False ds_sel.is_datastore_compliant.return_value = True backing = mock.sentinel.backing host = mock.sentinel.host rp = mock.sentinel.rp datastore = mock.sentinel.datastore summary = mock.Mock(datastore=datastore) ds_sel.select_datastore.return_value = (host, rp, summary) folder = mock.sentinel.folder get_volume_group_folder.return_value = folder self._driver._relocate_backing(volume, backing, host) vops.relocate_backing.assert_called_once_with(backing, datastore, rp, host) vops.move_backing_to_folder.assert_called_once_with(backing, folder) "," def test_init_conn_with_instance_and_backing(self): """"""Test initialize_connection with instance and backing."""""" m = self.mox m.StubOutWithMock(self._driver.__class__, 'volumeops') self._driver.volumeops = self._volumeops m.StubOutWithMock(self._volumeops, 'get_backing') volume = FakeObject() volume['name'] = 'volume_name' volume['id'] = 'volume_id' volume['size'] = 1 connector = {'instance': 'my_instance'} backing = FakeMor('VirtualMachine', 'my_back') self._volumeops.get_backing(volume['name']).AndReturn(backing) m.StubOutWithMock(self._volumeops, 'get_host') host = FakeMor('HostSystem', 'my_host') self._volumeops.get_host(mox.IgnoreArg()).AndReturn(host) m.ReplayAll() self.assertEqual(conn_info['driver_volume_type'], 'vmdk') self.assertEqual(conn_info['data']['volume'], 'my_back') self.assertEqual(conn_info['data']['volume_id'], 'volume_id') m.UnsetStubs() m.VerifyAll() def test_init_conn_with_instance_no_backing(self): """"""Test initialize_connection with instance and without backing."""""" m = self.mox m.StubOutWithMock(self._driver.__class__, 'volumeops') self._driver.volumeops = self._volumeops m.StubOutWithMock(self._volumeops, 'get_backing') volume = FakeObject() volume['name'] = 'volume_name' volume['id'] = 'volume_id' volume['size'] = 1 volume['volume_type_id'] = None connector = {'instance': 'my_instance'} self._volumeops.get_backing(volume['name']) m.StubOutWithMock(self._volumeops, 'get_host') host = FakeMor('HostSystem', 'my_host') self._volumeops.get_host(mox.IgnoreArg()).AndReturn(host) m.StubOutWithMock(self._volumeops, 'get_dss_rp') resource_pool = FakeMor('ResourcePool', 'my_rp') datastores = [FakeMor('Datastore', 'my_ds')] self._volumeops.get_dss_rp(host).AndReturn((datastores, resource_pool)) m.StubOutWithMock(self._driver, '_get_folder_ds_summary') folder = FakeMor('Folder', 'my_fol') summary = FakeDatastoreSummary(1, 1) self._driver._get_folder_ds_summary(volume, resource_pool, datastores).AndReturn((folder, summary)) backing = FakeMor('VirtualMachine', 'my_back') m.StubOutWithMock(self._volumeops, 'create_backing') self._volumeops.create_backing(volume['name'], volume['size'] * units.Mi, mox.IgnoreArg(), folder, resource_pool, host, mox.IgnoreArg(), mox.IgnoreArg(), mox.IgnoreArg()).AndReturn(backing) m.ReplayAll() conn_info = self._driver.initialize_connection(volume, connector) self.assertEqual(conn_info['driver_volume_type'], 'vmdk') self.assertEqual(conn_info['data']['volume'], 'my_back') self.assertEqual(conn_info['data']['volume_id'], 'volume_id') m.UnsetStubs() m.VerifyAll() def test_init_conn_without_instance(self): """"""Test initialize_connection without instance and a backing."""""" m = self.mox m.StubOutWithMock(self._driver.__class__, 'volumeops') self._driver.volumeops = self._volumeops m.StubOutWithMock(self._volumeops, 'get_backing') backing = FakeMor('VirtualMachine', 'my_back') volume = FakeObject() volume['name'] = 'volume_name' volume['id'] = 'volume_id' connector = {} self._volumeops.get_backing(volume['name']).AndReturn(backing) m.ReplayAll() conn_info = self._driver.initialize_connection(volume, connector) self.assertEqual(conn_info['driver_volume_type'], 'vmdk') self.assertEqual(conn_info['data']['volume'], 'my_back') self.assertEqual(conn_info['data']['volume_id'], 'volume_id') m.UnsetStubs() m.VerifyAll() def test_init_conn_with_instance_and_backing(self): """"""Test initialize_connection with instance and backing."""""" m = self.mox m.StubOutWithMock(self._driver.__class__, 'volumeops') self._driver.volumeops = self._volumeops m.StubOutWithMock(self._volumeops, 'get_backing') volume = FakeObject() volume['name'] = 'volume_name' volume['id'] = 'volume_id' volume['size'] = 1 connector = {'instance': 'my_instance'} backing = FakeMor('VirtualMachine', 'my_back') self._volumeops.get_backing(volume['name']).AndReturn(backing) m.StubOutWithMock(self._volumeops, 'get_host') host = FakeMor('HostSystem', 'my_host') self._volumeops.get_host(mox.IgnoreArg()).AndReturn(host) datastore = FakeMor('Datastore', 'my_ds') resource_pool = FakeMor('ResourcePool', 'my_rp') m.StubOutWithMock(self._volumeops, 'get_dss_rp') self._volumeops.get_dss_rp(host).AndReturn(([datastore], resource_pool)) m.StubOutWithMock(self._volumeops, 'get_datastore') self._volumeops.get_datastore(backing).AndReturn(datastore) m.ReplayAll() conn_info = self._driver.initialize_connection(volume, connector) self.assertEqual(conn_info['driver_volume_type'], 'vmdk') self.assertEqual(conn_info['data']['volume'], 'my_back') self.assertEqual(conn_info['data']['volume_id'], 'volume_id') m.UnsetStubs() m.VerifyAll() def test_init_conn_with_instance_and_backing_and_relocation(self): """"""Test initialize_connection with backing being relocated."""""" m = self.mox m.StubOutWithMock(self._driver.__class__, 'volumeops') self._driver.volumeops = self._volumeops m.StubOutWithMock(self._volumeops, 'get_backing') volume = FakeObject() volume['name'] = 'volume_name' volume['id'] = 'volume_id' volume['size'] = 1 connector = {'instance': 'my_instance'} backing = FakeMor('VirtualMachine', 'my_back') self._volumeops.get_backing(volume['name']).AndReturn(backing) m.StubOutWithMock(self._volumeops, 'get_host') host = FakeMor('HostSystem', 'my_host') self._volumeops.get_host(mox.IgnoreArg()).AndReturn(host) datastore1 = FakeMor('Datastore', 'my_ds_1') datastore2 = FakeMor('Datastore', 'my_ds_2') resource_pool = FakeMor('ResourcePool', 'my_rp') m.StubOutWithMock(self._volumeops, 'get_dss_rp') self._volumeops.get_dss_rp(host).AndReturn(([datastore1], resource_pool)) m.StubOutWithMock(self._volumeops, 'get_datastore') self._volumeops.get_datastore(backing).AndReturn(datastore2) m.StubOutWithMock(self._driver, '_get_folder_ds_summary') folder = FakeMor('Folder', 'my_fol') summary = FakeDatastoreSummary(1, 1, datastore1) self._driver._get_folder_ds_summary(volume, resource_pool, [datastore1]).AndReturn((folder, summary)) m.StubOutWithMock(self._volumeops, 'relocate_backing') self._volumeops.relocate_backing(backing, datastore1, resource_pool, host) m.StubOutWithMock(self._volumeops, 'move_backing_to_folder') self._volumeops.move_backing_to_folder(backing, folder) m.ReplayAll() conn_info = self._driver.initialize_connection(volume, connector) self.assertEqual(conn_info['driver_volume_type'], 'vmdk') self.assertEqual(conn_info['data']['volume'], 'my_back') self.assertEqual(conn_info['data']['volume_id'], 'volume_id') m.UnsetStubs() m.VerifyAll() ",336,184
openstack%2Fcinder~master~If4b868af7b91373b2b36fd5cd2f0dda12d604d99,openstack/cinder,master,If4b868af7b91373b2b36fd5cd2f0dda12d604d99,VMware: Implement retype for VMDK driver,MERGED,2014-08-13 12:11:16.000000000,2014-09-27 22:24:12.000000000,2014-09-27 22:24:11.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 2243}, {'_account_id': 2861}, {'_account_id': 5538}, {'_account_id': 9008}, {'_account_id': 9171}, {'_account_id': 10106}, {'_account_id': 10622}, {'_account_id': 10730}, {'_account_id': 11811}, {'_account_id': 11904}, {'_account_id': 12017}, {'_account_id': 12033}, {'_account_id': 12492}]","[{'number': 1, 'created': '2014-08-13 12:11:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2f7455833fdeb6728a5e0863e0c9715d06474f2c', 'message': ""VMware: Implement retype for VMDK driver\n\nThis patch adds support for retype operation in the VMDK driver.\n\nThe VMDK driver uses the extra spec keys 'vmware:vmdk_type' and\n'vmware:storage_profile' to determine the disk provisioning type\nand storage policy of the backing VM. Currently it is not possible\nto change the disk provisioning type or storage policy after volume\ncreation. With retype support, user can change the disk provisioning\ntype and storage policy by changing the volume's type to another type\nwith extra spec keys set to desired values.\n\nCloses-Bug: #1275682\nPartial-Bug: #1288254\n\nChange-Id: If4b868af7b91373b2b36fd5cd2f0dda12d604d99\n""}, {'number': 2, 'created': '2014-08-22 14:56:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c9f478f5f4ff301668ed7332f2dc0af3fc221e07', 'message': ""VMware: Implement retype for VMDK driver\n\nThis patch adds support for retype operation in the VMDK driver.\n\nThe VMDK driver uses the extra spec keys 'vmware:vmdk_type' and\n'vmware:storage_profile' to determine the disk provisioning type\nand storage policy of the backing VM. Currently it is not possible\nto change the disk provisioning type or storage policy after volume\ncreation. With retype support, user can change the disk provisioning\ntype and storage policy by changing the volume's type to another type\nwith extra spec keys set to desired values.\n\nCloses-Bug: #1275682\nPartial-Bug: #1288254\n\nChange-Id: If4b868af7b91373b2b36fd5cd2f0dda12d604d99\n""}, {'number': 3, 'created': '2014-08-25 10:06:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/23afb7d28b5b077fb87afce16c904a10fae823a5', 'message': ""VMware: Implement retype for VMDK driver\n\nThis patch adds support for retype operation in the VMDK driver.\n\nThe VMDK driver uses the extra spec keys 'vmware:vmdk_type' and\n'vmware:storage_profile' to determine the disk provisioning type\nand storage policy of the backing VM. Currently it is not possible\nto change the disk provisioning type or storage policy after volume\ncreation. With retype support, user can change the disk provisioning\ntype and storage policy by changing the volume's type to another type\nwith extra spec keys set to desired values.\n\nCloses-Bug: #1275682\nPartial-Bug: #1288254\n\nChange-Id: If4b868af7b91373b2b36fd5cd2f0dda12d604d99\n""}, {'number': 4, 'created': '2014-09-03 06:15:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/384056e704c7e9cdf46c364e43f954bbbbc003c7', 'message': ""VMware: Implement retype for VMDK driver\n\nThis patch adds support for retype operation in the VMDK driver.\n\nThe VMDK driver uses the extra spec keys 'vmware:vmdk_type' and\n'vmware:storage_profile' to determine the disk provisioning type\nand storage policy of the backing VM. Currently it is not possible\nto change the disk provisioning type or storage policy after volume\ncreation. With retype support, user can change the disk provisioning\ntype and storage policy by changing the volume's type to another type\nwith extra spec keys set to desired values.\n\nCloses-Bug: #1275682\nPartial-Bug: #1288254\n\nChange-Id: If4b868af7b91373b2b36fd5cd2f0dda12d604d99\n""}, {'number': 5, 'created': '2014-09-17 06:31:30.000000000', 'files': ['cinder/tests/test_vmware_volumeops.py', 'cinder/volume/drivers/vmware/vmdk.py', 'cinder/volume/drivers/vmware/volumeops.py', 'cinder/tests/test_vmware_vmdk.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/86591a24d48fe19be427b5a8981821fcdb85bb37', 'message': ""VMware: Implement retype for VMDK driver\n\nThis patch adds support for retype operation in the VMDK driver.\n\nThe VMDK driver uses the extra spec keys 'vmware:vmdk_type' and\n'vmware:storage_profile' to determine the disk provisioning type\nand storage policy of the backing VM. Currently it is not possible\nto change the disk provisioning type or storage policy after volume\ncreation. With retype support, user can change the disk provisioning\ntype and storage policy by changing the volume's type to another type\nwith extra spec keys set to desired values.\n\nCloses-Bug: #1275682\nPartial-Bug: #1288254\n\nChange-Id: If4b868af7b91373b2b36fd5cd2f0dda12d604d99\n""}]",5,113863,86591a24d48fe19be427b5a8981821fcdb85bb37,45,15,5,9171,,,0,"VMware: Implement retype for VMDK driver

This patch adds support for retype operation in the VMDK driver.

The VMDK driver uses the extra spec keys 'vmware:vmdk_type' and
'vmware:storage_profile' to determine the disk provisioning type
and storage policy of the backing VM. Currently it is not possible
to change the disk provisioning type or storage policy after volume
creation. With retype support, user can change the disk provisioning
type and storage policy by changing the volume's type to another type
with extra spec keys set to desired values.

Closes-Bug: #1275682
Partial-Bug: #1288254

Change-Id: If4b868af7b91373b2b36fd5cd2f0dda12d604d99
",git fetch https://review.opendev.org/openstack/cinder refs/changes/63/113863/5 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/test_vmware_volumeops.py', 'cinder/volume/drivers/vmware/vmdk.py', 'cinder/volume/drivers/vmware/volumeops.py', 'cinder/tests/test_vmware_vmdk.py']",4,2f7455833fdeb6728a5e0863e0c9715d06474f2c,vmdk_retype,"from cinder.volume.drivers.vmware import datastore as hub # Clear side effects. driver._select_ds_for_volume.side_effect = None @mock.patch('cinder.volume.volume_types.get_volume_type_extra_specs') def test_get_disk_type(self, get_volume_type_extra_specs): # Test with no volume type. volume = {'volume_type_id': None} self.assertEqual(vmdk.THIN_VMDK_TYPE, vmdk.VMwareEsxVmdkDriver._get_disk_type(volume)) # Test with valid vmdk_type. volume_type_id = mock.sentinel.volume_type_id volume = {'volume_type_id': volume_type_id} get_volume_type_extra_specs.return_value = vmdk.THICK_VMDK_TYPE self.assertEqual(vmdk.THICK_VMDK_TYPE, vmdk.VMwareEsxVmdkDriver._get_disk_type(volume)) get_volume_type_extra_specs.assert_called_once_with(volume_type_id, 'vmware:vmdk_type') # Test with invalid vmdk_type. get_volume_type_extra_specs.return_value = 'sparse' self.assertRaises(error_util.InvalidDiskTypeException, vmdk.VMwareEsxVmdkDriver._get_disk_type, volume) @mock.patch.object(VMDK_DRIVER, '_delete_temp_backing') @mock.patch('cinder.openstack.common.uuidutils.generate_uuid') @mock.patch.object(VMDK_DRIVER, '_get_volume_group_folder') @mock.patch('cinder.volume.volume_types.get_volume_type_extra_specs') @mock.patch.object(VMDK_DRIVER, 'volumeops') @mock.patch.object(VMDK_DRIVER, 'ds_sel') def test_retype(self, ds_sel, vops, get_volume_type_extra_specs, get_volume_group_folder, generate_uuid, delete_temp_backing): self._test_retype(ds_sel, vops, get_volume_type_extra_specs, get_volume_group_folder, generate_uuid, delete_temp_backing) def _test_retype(self, ds_sel, vops, get_volume_type_extra_specs, get_volume_group_folder, genereate_uuid, delete_temp_backing): self._driver._storage_policy_enabled = True context = mock.sentinel.context diff = mock.sentinel.diff host = mock.sentinel.host new_type = {'id': 'abc'} # Test with in-use volume. vol = {'size': 1, 'status': 'retyping', 'name': 'vol-1', 'volume_type_id': 'def', 'instance_uuid': '583a8dbb'} self.assertFalse(self._driver.retype(context, vol, new_type, diff, host)) # Test with no backing. vops.get_backing.return_value = None vol['instance_uuid'] = None self.assertTrue(self._driver.retype(context, vol, new_type, diff, host)) # Test with no disk type conversion, no profile change and # compliant datastore. ds_value = mock.sentinel.datastore_value datastore = mock.Mock(value=ds_value) vops.get_datastore.return_value = datastore backing = mock.sentinel.backing vops.get_backing.return_value = backing get_volume_type_extra_specs.side_effect = [vmdk.THIN_VMDK_TYPE, vmdk.THIN_VMDK_TYPE, None, None] ds_sel.is_datastore_compliant.return_value = True self.assertTrue(self._driver.retype(context, vol, new_type, diff, host)) # Test with no disk type conversion, profile change and # compliant datastore. new_profile = mock.sentinel.new_profile get_volume_type_extra_specs.side_effect = [vmdk.THIN_VMDK_TYPE, vmdk.THIN_VMDK_TYPE, 'gold-1', new_profile] ds_sel.is_datastore_compliant.return_value = True profile_id = mock.sentinel.profile_id ds_sel.get_profile_id.return_value = profile_id self.assertTrue(self._driver.retype(context, vol, new_type, diff, host)) vops.change_backing_profile.assert_called_once_with(backing, profile_id) # Test with disk type conversion, profile change and a backing with # snapshots. Also test the no candidate datastore case. get_volume_type_extra_specs.side_effect = [vmdk.THICK_VMDK_TYPE, vmdk.THIN_VMDK_TYPE, 'gold-1', new_profile] vops.snapshot_exists.return_value = True ds_sel.select_datastore.return_value = () self.assertFalse(self._driver.retype(context, vol, new_type, diff, host)) exp_req = {hub.DatastoreSelector.HARD_ANTI_AFFINITY_DS: [ds_value], hub.DatastoreSelector.PROFILE_NAME: new_profile, hub.DatastoreSelector.SIZE_BYTES: units.Gi} ds_sel.select_datastore.assert_called_once_with(exp_req) # Modify the previous case with a candidate datastore which is # different than the backing's current datastore. get_volume_type_extra_specs.side_effect = [vmdk.THICK_VMDK_TYPE, vmdk.THIN_VMDK_TYPE, 'gold-1', new_profile] vops.snapshot_exists.return_value = True host = mock.sentinel.host rp = mock.sentinel.rp candidate_ds = mock.Mock(value=mock.sentinel.candidate_ds_value) summary = mock.Mock(datastore=candidate_ds) ds_sel.select_datastore.return_value = (host, rp, summary) folder = mock.sentinel.folder get_volume_group_folder.return_value = folder vops.change_backing_profile.reset_mock() self.assertTrue(self._driver.retype(context, vol, new_type, diff, host)) vops.relocate_backing.assert_called_once_with( backing, candidate_ds, rp, host, vmdk.THIN_VMDK_TYPE) vops.move_backing_to_folder.assert_called_once_with(backing, folder) vops.change_backing_profile.assert_called_once_with(backing, profile_id) # Test with disk type conversion, profile change, backing with # no snapshots and candidate datastore which is same as the backing # datastore. get_volume_type_extra_specs.side_effect = [vmdk.THICK_VMDK_TYPE, vmdk.THIN_VMDK_TYPE, 'gold-1', new_profile] vops.snapshot_exists.return_value = False summary.datastore = datastore uuid = '025b654b-d4ed-47f9-8014-b71a7744eafc' genereate_uuid.return_value = uuid clone = mock.sentinel.clone vops.clone_backing.return_value = clone vops.change_backing_profile.reset_mock() self.assertTrue(self._driver.retype(context, vol, new_type, diff, host)) vops.rename_backing.assert_called_once_with(backing, uuid) vops.clone_backing.assert_called_once_with( vol['name'], backing, None, volumeops.FULL_CLONE_TYPE, datastore, vmdk.THIN_VMDK_TYPE) delete_temp_backing.assert_called_once_with(backing) vops.change_backing_profile.assert_called_once_with(clone, profile_id) # Modify the previous case with exception during clone. get_volume_type_extra_specs.side_effect = [vmdk.THICK_VMDK_TYPE, vmdk.THIN_VMDK_TYPE, 'gold-1', new_profile] vops.clone_backing.side_effect = error_util.VimException('error') vops.rename_backing.reset_mock() vops.change_backing_profile.reset_mock() self.assertRaises( error_util.VimException, self._driver.retype, context, vol, new_type, diff, host) exp_rename_calls = [mock.call(backing, uuid), mock.call(backing, vol['name'])] self.assertEqual(exp_rename_calls, vops.rename_backing.call_args_list) self.assertFalse(vops.change_backing_profile.called) # Clear side effects. driver._filter_ds_by_profile.side_effect = None @mock.patch.object(VMDK_DRIVER, '_delete_temp_backing') @mock.patch('cinder.openstack.common.uuidutils.generate_uuid') @mock.patch.object(VMDK_DRIVER, '_get_volume_group_folder') @mock.patch('cinder.volume.volume_types.get_volume_type_extra_specs') @mock.patch.object(VMDK_DRIVER, 'volumeops') @mock.patch.object(VMDK_DRIVER, 'ds_sel') def test_retype(self, ds_sel, vops, get_volume_type_extra_specs, get_volume_group_folder, generate_uuid, delete_temp_backing): self._test_retype(ds_sel, vops, get_volume_type_extra_specs, get_volume_group_folder, generate_uuid, delete_temp_backing) "," def test_get_disk_type(self): volume = FakeObject() volume['volume_type_id'] = None self.assertEqual(vmdk.VMwareEsxVmdkDriver._get_disk_type(volume), 'thin')",520,32
openstack%2Fcinder~master~I17e90aa09a303fbb8d4ad90037f440c8c4e7d072,openstack/cinder,master,I17e90aa09a303fbb8d4ad90037f440c8c4e7d072,VMware: Improve datastore selection logic,MERGED,2014-08-13 12:08:52.000000000,2014-09-27 22:18:54.000000000,2014-09-27 22:18:53.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 2243}, {'_account_id': 5538}, {'_account_id': 9008}, {'_account_id': 9171}, {'_account_id': 10106}, {'_account_id': 10730}, {'_account_id': 11047}, {'_account_id': 11811}, {'_account_id': 11904}, {'_account_id': 12017}, {'_account_id': 12033}, {'_account_id': 12492}]","[{'number': 1, 'created': '2014-08-13 12:08:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/139f5e3393e72a9647c1c7b106b21cd94625cd49', 'message': ""VMware: Datastore selection\n\nThe current datastore selection logic is not modularized and difficult\nto extend. The retype API implementation has a requirement to specify\nhard anti-affinity requirement with the current backing datastore. Some\nof the bug fixes also need to specify hard affinity with one or more\ndatastore types. To support such requirements and to enable future\nextensions, this patch introduces a new module which contains datastore\nselection logic. The existing code for datastore selection is reused as\nmuch as possible. The dependency on existing datastore selection logic\nwill be removed in a separate patch.\n\nThe current datastore selection iterates over a list of hosts, for each\nhost, queries the connected valid datastores and tries to select a\nsuitable datastore. The filtering is based on space, storage profile,\nnumber of connected hosts and space utilization. The space utilization\nis used only for breaking ties. If a suitable datastore is found, further\nprocessing of list of hosts is skipped, which could result in uneven space\nutilization. To solve this, the new selection logic introduces a requirement\ncalled 'preferred_utilization_threshold' which can be exposed as a driver\nconfig option.\n\nThis patch also adds some of the missing unit tests related to storage\nprofiles.\n\nPartial-bug: #1275682\nPartial-bug: #1301943\nPartial-bug: #1293955\n\nChange-Id: I17e90aa09a303fbb8d4ad90037f440c8c4e7d072\n""}, {'number': 2, 'created': '2014-08-22 14:49:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c1805064c08a542b89527e0d0710dd9084c95d8c', 'message': ""VMware: Datastore selection\n\nThe current datastore selection logic is not modularized and difficult\nto extend. The retype API implementation has a requirement to specify\nhard anti-affinity requirement with the current backing datastore. Some\nof the bug fixes also need to specify hard affinity with one or more\ndatastore types. To support such requirements and to enable future\nextensions, this patch introduces a new module which contains datastore\nselection logic. The existing code for datastore selection is reused as\nmuch as possible. The dependency on existing datastore selection logic\nwill be removed in a separate patch.\n\nThe current datastore selection iterates over a list of hosts, for each\nhost, queries the connected valid datastores and tries to select a\nsuitable datastore. The filtering is based on space, storage profile,\nnumber of connected hosts and space utilization. The space utilization\nis used only for breaking ties. If a suitable datastore is found, further\nprocessing of list of hosts is skipped, which could result in uneven space\nutilization. To solve this, the new selection logic introduces a requirement\ncalled 'preferred_utilization_threshold' which can be exposed as a driver\nconfig option.\n\nPartial-bug: #1275682\nPartial-bug: #1301943\nPartial-bug: #1293955\n\nChange-Id: I17e90aa09a303fbb8d4ad90037f440c8c4e7d072\n""}, {'number': 3, 'created': '2014-08-25 09:30:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/0be3c26a4b630b5249508c7f0974093b709b7584', 'message': ""VMware: Improve datastore selection logic\n\nThe current datastore selection logic is not modularized and difficult\nto extend. The retype API implementation has a requirement to specify\nhard anti-affinity requirement with the current backing datastore. Some\nof the bug fixes also need to specify hard affinity with one or more\ndatastore types. To support such requirements and to enable future\nextensions, this patch introduces a new module which contains datastore\nselection logic. The existing code for datastore selection is reused as\nmuch as possible. The dependency on existing datastore selection logic\nwill be removed in a separate patch.\n\nThe current datastore selection iterates over a list of hosts, for each\nhost, queries the connected valid datastores and tries to select a\nsuitable datastore. The filtering is based on space, storage profile,\nnumber of connected hosts and space utilization. The space utilization\nis used only for breaking ties. If a suitable datastore is found, further\nprocessing of list of hosts is skipped, which could result in uneven space\nutilization. To solve this, the new selection logic introduces a requirement\ncalled 'preferred_utilization_threshold' which can be exposed as a driver\nconfig option.\n\nPartial-bug: #1275682\nPartial-bug: #1301943\nPartial-bug: #1293955\n\nChange-Id: I17e90aa09a303fbb8d4ad90037f440c8c4e7d072\n""}, {'number': 4, 'created': '2014-09-03 06:11:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/d9d514515239f32b39b76a05feb427d872a2dc83', 'message': ""VMware: Improve datastore selection logic\n\nThe current datastore selection logic is not modularized and difficult\nto extend. The retype API implementation has a requirement to specify\nhard anti-affinity requirement with the current backing datastore. Some\nof the bug fixes also need to specify hard affinity with one or more\ndatastore types. To support such requirements and to enable future\nextensions, this patch introduces a new module which contains datastore\nselection logic. The existing code for datastore selection is reused as\nmuch as possible. The dependency on existing datastore selection logic\nwill be removed in a separate patch.\n\nThe current datastore selection iterates over a list of hosts, for each\nhost, queries the connected valid datastores and tries to select a\nsuitable datastore. The filtering is based on space, storage profile,\nnumber of connected hosts and space utilization. The space utilization\nis used only for breaking ties. If a suitable datastore is found, further\nprocessing of list of hosts is skipped, which could result in uneven space\nutilization. To solve this, the new selection logic introduces a requirement\ncalled 'preferred_utilization_threshold' which can be exposed as a driver\nconfig option.\n\nPartial-bug: #1275682\nPartial-bug: #1301943\nPartial-bug: #1293955\n\nChange-Id: I17e90aa09a303fbb8d4ad90037f440c8c4e7d072\n""}, {'number': 5, 'created': '2014-09-17 06:27:06.000000000', 'files': ['cinder/tests/test_vmware_datastore.py', 'cinder/volume/drivers/vmware/error_util.py', 'cinder/volume/drivers/vmware/datastore.py', 'cinder/volume/drivers/vmware/volumeops.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/2c672d1100ad4f44517838e17156b3ea6300b1cc', 'message': ""VMware: Improve datastore selection logic\n\nThe current datastore selection logic is not modularized and difficult\nto extend. The retype API implementation has a requirement to specify\nhard anti-affinity requirement with the current backing datastore. Some\nof the bug fixes also need to specify hard affinity with one or more\ndatastore types. To support such requirements and to enable future\nextensions, this patch introduces a new module which contains datastore\nselection logic. The existing code for datastore selection is reused as\nmuch as possible. The dependency on existing datastore selection logic\nwill be removed in a separate patch.\n\nThe current datastore selection iterates over a list of hosts, for each\nhost, queries the connected valid datastores and tries to select a\nsuitable datastore. The filtering is based on space, storage profile,\nnumber of connected hosts and space utilization. The space utilization\nis used only for breaking ties. If a suitable datastore is found, further\nprocessing of list of hosts is skipped, which could result in uneven space\nutilization. To solve this, the new selection logic introduces a requirement\ncalled 'preferred_utilization_threshold' which can be exposed as a driver\nconfig option.\n\nPartial-bug: #1275682\nPartial-bug: #1301943\nPartial-bug: #1293955\n\nChange-Id: I17e90aa09a303fbb8d4ad90037f440c8c4e7d072\n""}]",6,113862,2c672d1100ad4f44517838e17156b3ea6300b1cc,51,14,5,9171,,,0,"VMware: Improve datastore selection logic

The current datastore selection logic is not modularized and difficult
to extend. The retype API implementation has a requirement to specify
hard anti-affinity requirement with the current backing datastore. Some
of the bug fixes also need to specify hard affinity with one or more
datastore types. To support such requirements and to enable future
extensions, this patch introduces a new module which contains datastore
selection logic. The existing code for datastore selection is reused as
much as possible. The dependency on existing datastore selection logic
will be removed in a separate patch.

The current datastore selection iterates over a list of hosts, for each
host, queries the connected valid datastores and tries to select a
suitable datastore. The filtering is based on space, storage profile,
number of connected hosts and space utilization. The space utilization
is used only for breaking ties. If a suitable datastore is found, further
processing of list of hosts is skipped, which could result in uneven space
utilization. To solve this, the new selection logic introduces a requirement
called 'preferred_utilization_threshold' which can be exposed as a driver
config option.

Partial-bug: #1275682
Partial-bug: #1301943
Partial-bug: #1293955

Change-Id: I17e90aa09a303fbb8d4ad90037f440c8c4e7d072
",git fetch https://review.opendev.org/openstack/cinder refs/changes/62/113862/3 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/test_vmware_datastore.py', 'cinder/volume/drivers/vmware/error_util.py', 'cinder/tests/test_vmware_volumeops.py', 'cinder/volume/drivers/vmware/datastore.py', 'cinder/volume/drivers/vmware/volumeops.py']",5,139f5e3393e72a9647c1c7b106b21cd94625cd49,datastore_sel," profiles = [] if profileIds: profiles = self._session.invoke_api(pbm, 'PbmRetrieveContent', profile_manager, profileIds=profileIds) return profiles"," return self._session.invoke_api(pbm, 'PbmRetrieveContent', profile_manager, profileIds=profileIds)",760,3
openstack%2Fcinder~master~If340980ab2dcc844398254ff368ca6b78ca40ff6,openstack/cinder,master,If340980ab2dcc844398254ff368ca6b78ca40ff6,Failed to re-detach volume when volume detached.,MERGED,2014-09-24 09:15:16.000000000,2014-09-27 22:18:45.000000000,2014-09-27 22:18:43.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 7198}, {'_account_id': 9008}, {'_account_id': 9416}, {'_account_id': 9533}, {'_account_id': 10503}, {'_account_id': 12017}, {'_account_id': 12369}, {'_account_id': 12778}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-09-24 09:15:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/cd6cd7fcfbf7d3516b05ac4325048253c8c12ec8', 'message': 'ProphetStor storage failed to re-detach volume when volume already\ndetached in the back-end storage.\n\nWhen first request command detach the volume, but the back-end\nstorage state is in-processing or busy. Next retry command will\ngot the error code that describe the volume already detached.\n\nChange-Id: If340980ab2dcc844398254ff368ca6b78ca40ff6\nCloses-Bug: 1373317\n'}, {'number': 2, 'created': '2014-09-24 15:07:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/72bf925c89abd175adb576e153abe341827b651f', 'message': 'Failed to re-detach volume when volume detached.\n\nWhen first request command detach the volume, but the back-end\nstorage state is in-processing or busy. Next retry command will\ngot the error code that describe the volume already detached.\n\nChange-Id: If340980ab2dcc844398254ff368ca6b78ca40ff6\nCloses-Bug: 1373317\n'}, {'number': 3, 'created': '2014-09-25 00:53:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/3ea675f1085f7e1d1e40f502e42025a76e7bd9bf', 'message': 'Failed to re-detach volume when volume detached.\n\nWhen first request command detach the volume, but the back-end\nstorage state is in-processing or busy. Next retry command will\ngot the error code that describe the volume already detached.\n\nChange-Id: If340980ab2dcc844398254ff368ca6b78ca40ff6\nCloses-Bug: 1373317\n'}, {'number': 4, 'created': '2014-09-25 01:05:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2c81a6dc81acd94d222d6ad1382ea79800702e3d', 'message': 'Failed to re-detach volume when volume detached.\n\nWhen first request command detach the volume, but the back-end\nstorage state is in-processing or busy. Next retry command will\ngot the error code that describe the volume already detached.\n\nChange-Id: If340980ab2dcc844398254ff368ca6b78ca40ff6\nCloses-Bug: 1373317\n'}, {'number': 5, 'created': '2014-09-25 06:30:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/12451dfee00ba0066c8b51f60cc31945f1f96ffd', 'message': 'Failed to re-detach volume when volume detached.\n\nWhen first request command detach the volume, but the back-end\nstorage state is in-processing or busy. Next retry command will\ngot the error code that describe the volume already detached.\n\nChange-Id: If340980ab2dcc844398254ff368ca6b78ca40ff6\nCloses-Bug: 1373317\n'}, {'number': 6, 'created': '2014-09-25 09:07:06.000000000', 'files': ['cinder/volume/drivers/prophetstor/dpl_iscsi.py', 'cinder/tests/test_prophetstor_dpl.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/7a5383a70adce0a6284c9c10a0d352077aa9edea', 'message': 'Failed to re-detach volume when volume detached.\n\nWhen first request command detach the volume, but the back-end\nstorage state is in-processing or busy. Next retry command will\ngot the error code that describe the volume already detached.\n\nChange-Id: If340980ab2dcc844398254ff368ca6b78ca40ff6\nCloses-Bug: 1373317\n'}]",1,123664,7a5383a70adce0a6284c9c10a0d352077aa9edea,37,11,6,9416,,,0,"Failed to re-detach volume when volume detached.

When first request command detach the volume, but the back-end
storage state is in-processing or busy. Next retry command will
got the error code that describe the volume already detached.

Change-Id: If340980ab2dcc844398254ff368ca6b78ca40ff6
Closes-Bug: 1373317
",git fetch https://review.opendev.org/openstack/cinder refs/changes/64/123664/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/drivers/prophetstor/dpl_iscsi.py'],1,cd6cd7fcfbf7d3516b05ac4325048253c8c12ec8,bug/1373317, elif ret == errno.ENODATA: msg = _('Flexvisor already unassign volume ' '%(id)s.') % {'id': volume['id']} LOG.info(msg),,4,0
openstack%2Fcinder~master~Ic3726f725efa4d29204e8530a8f191b801a4aac8,openstack/cinder,master,Ic3726f725efa4d29204e8530a8f191b801a4aac8,Fixing format for log messages,MERGED,2014-09-03 18:40:16.000000000,2014-09-27 21:58:06.000000000,2014-09-27 21:58:06.000000000,"[{'_account_id': 3}, {'_account_id': 1207}, {'_account_id': 2243}, {'_account_id': 2861}, {'_account_id': 4523}, {'_account_id': 6491}, {'_account_id': 9008}, {'_account_id': 9171}, {'_account_id': 10068}, {'_account_id': 10503}, {'_account_id': 11811}, {'_account_id': 11904}, {'_account_id': 12017}, {'_account_id': 12369}, {'_account_id': 12778}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-09-03 18:40:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/8fcd549a3e53cdfc1f4b68e5e4470f19e87f82c1', 'message': 'Fixing format for log messages\nCloses-Bug: 1362337\n\nChange-Id: Ic3726f725efa4d29204e8530a8f191b801a4aac8\n'}, {'number': 2, 'created': '2014-09-16 19:33:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/66c03e8cc7fac4a12d75597ec65268796f505a53', 'message': 'Fixing format for log messages\n\ncode_cleanup_batching\n\nCloses-Bug: 1362337\n\nChange-Id: Ic3726f725efa4d29204e8530a8f191b801a4aac8\n'}, {'number': 3, 'created': '2014-09-23 22:37:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/fb297be73aaf926de700f9ac81db20084cee3d9a', 'message': 'Fixing format for log messages\n\ncode_cleanup_batching for EQL driver\nFollow log essage format for i18n - http://docs.openstack.org/developer/oslo.i18n/guidelines.html#adding-variables-to-log-messages\n\nCloses-Bug: 1362337\n\nChange-Id: Ic3726f725efa4d29204e8530a8f191b801a4aac8\n'}, {'number': 4, 'created': '2014-09-27 13:25:36.000000000', 'files': ['cinder/volume/drivers/eqlx.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/6225edf2618f051020377b12d9ed69b8ca486bbd', 'message': 'Fixing format for log messages\n\ncode_cleanup_batching for EQL driver\nFollow log essage format for i18n - http://docs.openstack.org/developer/oslo.i18n/guidelines.html#adding-variables-to-log-messages\n\nCloses-Bug: 1362337\n\nChange-Id: Ic3726f725efa4d29204e8530a8f191b801a4aac8\n'}]",23,118737,6225edf2618f051020377b12d9ed69b8ca486bbd,36,16,4,7160,,,0,"Fixing format for log messages

code_cleanup_batching for EQL driver
Follow log essage format for i18n - http://docs.openstack.org/developer/oslo.i18n/guidelines.html#adding-variables-to-log-messages

Closes-Bug: 1362337

Change-Id: Ic3726f725efa4d29204e8530a8f191b801a4aac8
",git fetch https://review.opendev.org/openstack/cinder refs/changes/37/118737/4 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/drivers/eqlx.py'],1,8fcd549a3e53cdfc1f4b68e5e4470f19e87f82c1,bug/1362337," help='Group name to use for creating volumes.'), help='Timeout for the Group Manager cli command execution.'), help='Maximum retry count for reconnection.'), help='Existing CHAP account name.'), help='Password for specified CHAP account name.', help='Pool in which volumes will be created.') LOG.debug(""Setting CLI terminal width: '%s'."", cmd) LOG.debug(""Sending CLI command: '%s'."", command) LOG.info(_('EQL-driver: executing ""%s"".') % command) LOG.error(_('Error running SSH command: ""%s"".') % command) LOG.debug(""Updating volume stats."") LOG.info(_('EQL-driver: Setup is complete, group IP is ""%s"".'), LOG.error(_('Failed to setup the Dell EqualLogic driver.')) LOG.error(_('Failed to create volume ""%s"".'), volume['name']) LOG.error(_('Failed to add multi-host access for volume ""%s"".'), LOG.warn(_('Volume %s was not found while trying to delete it.'), LOG.error(_('Failed to delete volume ""%s"".'), volume['name']) LOG.error(_('Failed to create snapshot of volume ""%s"".'), LOG.error(_('Failed to create volume from snapshot ""%s"".'), LOG.error(_('Failed to create clone of volume ""%s"".'), 'volume %(vol)s.'), LOG.error(_('Failed to initialize connection to volume ""%s"".'), LOG.error(_('Failed to terminate connection to volume ""%s"".'), LOG.warn(_('Volume %s is not found!, it may have been deleted.'), LOG.error(_('Failed to ensure export of volume ""%s"".'), '%(current_size)sGB to %(new_size)sGB.'),"," help='Group name to use for creating volumes'), help='Timeout for the Group Manager cli command execution'), help='Maximum retry count for reconnection'), help='Existing CHAP account name'), help='Password for specified CHAP account name', help='Pool in which volumes will be created') LOG.debug(""Setting CLI terminal width: '%s'"", cmd) LOG.debug(""Sending CLI command: '%s'"", command) LOG.info(_('EQL-driver: executing ""%s""') % command) LOG.error(_(""Error running SSH command: %s"") % command) LOG.debug(""Updating volume stats"") LOG.info(_(""EQL-driver: Setup is complete, group IP is %s""), LOG.error(_('Failed to setup the Dell EqualLogic driver')) LOG.error(_('Failed to create volume %s'), volume['name']) LOG.error(_('Failed to add multi-host access for volume %s'), LOG.warn(_('Volume %s was not found while trying to delete it'), LOG.error(_('Failed to delete volume %s'), volume['name']) LOG.error(_('Failed to create snapshot of volume %s'), LOG.error(_('Failed to create volume from snapshot %s'), LOG.error(_('Failed to create clone of volume %s'), 'volume %(vol)s'), LOG.error(_('Failed to initialize connection to volume %s'), LOG.error(_('Failed to terminate connection to volume %s'), LOG.warn(_('Volume %s is not found!, it may have been deleted'), LOG.error(_('Failed to ensure export of volume %s'), '%(current_size)sGB to %(new_size)sGB'),",26,26
openstack%2Fglance~stable%2Ficehouse~I5552220ce264c0d09780bbd0eb7610e7cff0b820,openstack/glance,stable/icehouse,I5552220ce264c0d09780bbd0eb7610e7cff0b820,Fix image killed after deletion,MERGED,2014-08-14 17:08:43.000000000,2014-09-27 21:57:58.000000000,2014-09-27 21:57:57.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 6549}, {'_account_id': 9656}, {'_account_id': 12395}]","[{'number': 1, 'created': '2014-08-14 17:08:43.000000000', 'files': ['glance/tests/unit/v1/test_upload_utils.py', 'glance/tests/unit/v1/test_api.py', 'glance/api/v1/upload_utils.py', 'glance/api/v1/images.py'], 'web_link': 'https://opendev.org/openstack/glance/commit/fcc937966056d86956cb403a68a81cb9f4fbd0b1', 'message': ""Fix image killed after deletion\n\nIf an image upload fails after the image was deleted during\nthe upload, the image status currently gets set to 'deleted'\nthen 'killed' once the upload fails. This patch fixes this so\nthat if the image was deleted it stays in 'deleted' state.\n\nPermitted from_states for transitioning to 'killed' are\n'queued' and 'saving'. We now enforce this transition and\nraise an exception if it is violated.\n\nCloses-Bug: 1236868\nCo-authored-by: Hirofumi Ichihara <ichihara.hirofumi@lab.ntt.co.jp>\nCo-authored-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>\n(cherry picked from commit 345d8f7766aa06fa6654382efc480da493a3f2c8)\n\nConflicts:\n\n\tglance/api/v1/images.py\n\tglance/api/v1/upload_utils.py\n\nChange-Id: I5552220ce264c0d09780bbd0eb7610e7cff0b820\n""}]",0,114299,fcc937966056d86956cb403a68a81cb9f4fbd0b1,19,5,1,6737,,,0,"Fix image killed after deletion

If an image upload fails after the image was deleted during
the upload, the image status currently gets set to 'deleted'
then 'killed' once the upload fails. This patch fixes this so
that if the image was deleted it stays in 'deleted' state.

Permitted from_states for transitioning to 'killed' are
'queued' and 'saving'. We now enforce this transition and
raise an exception if it is violated.

Closes-Bug: 1236868
Co-authored-by: Hirofumi Ichihara <ichihara.hirofumi@lab.ntt.co.jp>
Co-authored-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>
(cherry picked from commit 345d8f7766aa06fa6654382efc480da493a3f2c8)

Conflicts:

	glance/api/v1/images.py
	glance/api/v1/upload_utils.py

Change-Id: I5552220ce264c0d09780bbd0eb7610e7cff0b820
",git fetch https://review.opendev.org/openstack/glance refs/changes/99/114299/1 && git format-patch -1 --stdout FETCH_HEAD,"['glance/tests/unit/v1/test_upload_utils.py', 'glance/tests/unit/v1/test_api.py', 'glance/api/v1/upload_utils.py', 'glance/api/v1/images.py']",4,fcc937966056d86956cb403a68a81cb9f4fbd0b1,bug/1236868," upload_utils.safe_kill(req, image_meta['id'], 'queued') upload_utils.safe_kill(req, image_meta['id'], 'queued')"," upload_utils.safe_kill(req, image_meta['id']) upload_utils.safe_kill(req, image_meta['id'])",217,58
openstack%2Fnova~stable%2Ficehouse~I94941dfe96c1596c8e9b91df3d3d19d33ae7fe92,openstack/nova,stable/icehouse,I94941dfe96c1596c8e9b91df3d3d19d33ae7fe92,Fixes Hyper-V boot from volume root device issue,MERGED,2014-09-10 09:04:44.000000000,2014-09-27 21:49:49.000000000,2014-09-27 21:49:46.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 1420}, {'_account_id': 5170}, {'_account_id': 8213}, {'_account_id': 8543}, {'_account_id': 9656}, {'_account_id': 10635}]","[{'number': 1, 'created': '2014-09-10 09:04:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3ee06acaf37e6e781af26b00360dfdc45ae67871', 'message': 'Fixes Hyper-V boot from volume root device issue\n\nChecks that instances are booting from volume by checking\nthe correct root device obtained from the block device info.\n\nChange-Id: I94941dfe96c1596c8e9b91df3d3d19d33ae7fe92\nCo-Authored-By: Zsolt Dudas <zdudas@cloudbasesolutions.com>\nCloses-Bug: #1357972\n(cherry picked from commit ea19fb10c5e09ff5df383607654ab9dc2791ec21)\n'}, {'number': 2, 'created': '2014-09-17 14:21:04.000000000', 'files': ['nova/tests/virt/hyperv/db_fakes.py', 'nova/tests/virt/hyperv/test_hypervapi.py', 'nova/virt/hyperv/volumeops.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/d72c0a4f03f56f6b24b86d3740fd721ef0a63080', 'message': 'Fixes Hyper-V boot from volume root device issue\n\nChecks that instances are booting from volume by checking\nthe correct root device obtained from the block device info.\n\nChange-Id: I94941dfe96c1596c8e9b91df3d3d19d33ae7fe92\nCo-Authored-By: Zsolt Dudas <zdudas@cloudbasesolutions.com>\nCloses-Bug: #1357972\n(cherry picked from commit ea19fb10c5e09ff5df383607654ab9dc2791ec21)\n'}]",0,120350,d72c0a4f03f56f6b24b86d3740fd721ef0a63080,36,8,2,3185,,,0,"Fixes Hyper-V boot from volume root device issue

Checks that instances are booting from volume by checking
the correct root device obtained from the block device info.

Change-Id: I94941dfe96c1596c8e9b91df3d3d19d33ae7fe92
Co-Authored-By: Zsolt Dudas <zdudas@cloudbasesolutions.com>
Closes-Bug: #1357972
(cherry picked from commit ea19fb10c5e09ff5df383607654ab9dc2791ec21)
",git fetch https://review.opendev.org/openstack/nova refs/changes/50/120350/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/hyperv/db_fakes.py', 'nova/tests/virt/hyperv/test_hypervapi.py', 'nova/virt/hyperv/volumeops.py']",3,3ee06acaf37e6e781af26b00360dfdc45ae67871,," if block_device_info: root_device = block_device_info.get('root_device_name') if not root_device: root_device = self._default_root_device return self._volutils.volume_in_mapping(root_device, block_device_info)"," return self._volutils.volume_in_mapping(self._default_root_device, block_device_info)",11,18
openstack%2Fnova~master~I37bd7cee7e8879b01a8aee739d74c2dd6e033dc4,openstack/nova,master,I37bd7cee7e8879b01a8aee739d74c2dd6e033dc4,Fix 'os-interface' resource name for Nova V2.1,MERGED,2014-09-11 05:58:46.000000000,2014-09-27 21:49:24.000000000,2014-09-27 21:49:21.000000000,"[{'_account_id': 3}, {'_account_id': 5170}, {'_account_id': 5292}, {'_account_id': 5754}, {'_account_id': 6167}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-11 05:58:46.000000000', 'files': ['nova/tests/integrated/v3/test_attach_interfaces.py', 'nova/api/openstack/compute/plugins/v3/attach_interfaces.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/f7ef2b09a195f853cecaa28db710d643a8e30f78', 'message': ""Fix 'os-interface' resource name for Nova V2.1\n\nV2 and V3 diff for this resource name is below-\nV2 - '/servers/os-interface'\nV3 - '/servers/os-attach-interfaces'\n\nV3 resource name needs to be changed to work that for V2.1\n\nChange-Id: I37bd7cee7e8879b01a8aee739d74c2dd6e033dc4\nCloses-Bug: #1368035\n""}]",0,120692,f7ef2b09a195f853cecaa28db710d643a8e30f78,20,8,1,8556,,,0,"Fix 'os-interface' resource name for Nova V2.1

V2 and V3 diff for this resource name is below-
V2 - '/servers/os-interface'
V3 - '/servers/os-attach-interfaces'

V3 resource name needs to be changed to work that for V2.1

Change-Id: I37bd7cee7e8879b01a8aee739d74c2dd6e033dc4
Closes-Bug: #1368035
",git fetch https://review.opendev.org/openstack/nova refs/changes/92/120692/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/integrated/v3/test_attach_interfaces.py', 'nova/api/openstack/compute/plugins/v3/attach_interfaces.py']",2,f7ef2b09a195f853cecaa28db710d643a8e30f78,attach_interface," res = [extensions.ResourceExtension('os-interface',"," res = [extensions.ResourceExtension(ALIAS,",5,5
openstack%2Fswift~feature%2Fec~I8df5b4c26b3f8c0513119d8e82eca1302c8e2fc1,openstack/swift,feature/ec,I8df5b4c26b3f8c0513119d8e82eca1302c8e2fc1,Merge master to feature/ec,MERGED,2014-09-26 19:53:58.000000000,2014-09-27 21:49:00.000000000,2014-09-27 21:48:58.000000000,"[{'_account_id': 3}, {'_account_id': 7479}, {'_account_id': 13052}]","[{'number': 1, 'created': '2014-09-26 19:53:58.000000000', 'files': ['test/unit/account/test_backend.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/bcaa00f25f3e8bd4123645256c535d0efe8a6733', 'message': 'Merge master to feature/ec\n\nChange-Id: I8df5b4c26b3f8c0513119d8e82eca1302c8e2fc1\n'}]",0,124503,bcaa00f25f3e8bd4123645256c535d0efe8a6733,8,3,1,7479,,,0,"Merge master to feature/ec

Change-Id: I8df5b4c26b3f8c0513119d8e82eca1302c8e2fc1
",git fetch https://review.opendev.org/openstack/swift refs/changes/03/124503/1 && git format-patch -1 --stdout FETCH_HEAD,['test/unit/account/test_backend.py'],1,bcaa00f25f3e8bd4123645256c535d0efe8a6733,ec merge,"@patch_policies([ StoragePolicy.from_conf( REPL_POLICY, {'idx': 0, 'name': 'zero', 'is_default': True}), StoragePolicy.from_conf( REPL_POLICY, {'idx': 1, 'name': 'one'}), StoragePolicy.from_conf( REPL_POLICY, {'idx': 2, 'name': 'two'}), StoragePolicy.from_conf( REPL_POLICY, {'idx': 37, 'name': 'three'}) ])","@patch_policies([StoragePolicy(0, 'zero', False), StoragePolicy(1, 'one', True), StoragePolicy(2, 'two', False), StoragePolicy(3, 'three', False)])",10,4
openstack%2Fneutron~stable%2Ficehouse~I3f5a2fcfec49316fbe06d6221d003aeb2599bca7,openstack/neutron,stable/icehouse,I3f5a2fcfec49316fbe06d6221d003aeb2599bca7,Add missing ml2 plugin to migration 1fcfc149aca4,MERGED,2014-07-30 13:25:18.000000000,2014-09-27 21:48:46.000000000,2014-09-27 21:48:45.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 1313}, {'_account_id': 1420}, {'_account_id': 1653}, {'_account_id': 2592}, {'_account_id': 5170}, {'_account_id': 6072}, {'_account_id': 6854}, {'_account_id': 7787}, {'_account_id': 8213}, {'_account_id': 8645}, {'_account_id': 9008}, {'_account_id': 9656}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9846}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 13308}]","[{'number': 1, 'created': '2014-07-30 13:25:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/1e49d6f44a2de8a5f277d865da3b645c63ca3593', 'message': 'Add missing ml2 plugin to migration 1fcfc149aca4\n\nMigration 1fcfc149aca4_agents_unique_by_type_and_host\nshouldbe applied to ml2 plugin as well.\n\nChange-Id: I3f5a2fcfec49316fbe06d6221d003aeb2599bca7\nCloses-Bug: #1350326\n'}, {'number': 2, 'created': '2014-08-04 07:02:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/aee57ba654d1e8b0fccfe7e6de99226645fb98e3', 'message': 'Add missing ml2 plugin to migration 1fcfc149aca4\n\nMigration 1fcfc149aca4_agents_unique_by_type_and_host\nshouldbe applied to ml2 plugin as well.\n\nChange-Id: I3f5a2fcfec49316fbe06d6221d003aeb2599bca7\nCloses-Bug: #1350326\n'}, {'number': 3, 'created': '2014-09-25 12:05:46.000000000', 'files': ['neutron/db/migration/alembic_migrations/versions/1fcfc149aca4_agents_unique_by_type_and_host.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/b51e2c79a2eb2406ac1c906f299b8c5d2b320ff0', 'message': 'Add missing ml2 plugin to migration 1fcfc149aca4\n\nMigration 1fcfc149aca4_agents_unique_by_type_and_host\nshouldbe applied to ml2 plugin as well.\n\nChange-Id: I3f5a2fcfec49316fbe06d6221d003aeb2599bca7\nCloses-Bug: #1350326\n(cherry picked from commit 30f1a75e790a512ea8b2b4997a634a50070fef9f)\n'}]",2,110642,b51e2c79a2eb2406ac1c906f299b8c5d2b320ff0,120,29,3,6072,,,0,"Add missing ml2 plugin to migration 1fcfc149aca4

Migration 1fcfc149aca4_agents_unique_by_type_and_host
shouldbe applied to ml2 plugin as well.

Change-Id: I3f5a2fcfec49316fbe06d6221d003aeb2599bca7
Closes-Bug: #1350326
(cherry picked from commit 30f1a75e790a512ea8b2b4997a634a50070fef9f)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/42/110642/2 && git format-patch -1 --stdout FETCH_HEAD,['neutron/db/migration/alembic_migrations/versions/1fcfc149aca4_agents_unique_by_type_and_host.py'],1,1e49d6f44a2de8a5f277d865da3b645c63ca3593,," 'neutron.plugins.ml2.plugin.Ml2Plugin',",,1,0
openstack%2Fkeystone~master~Icbd867ff4e385f4430bfa9b7c5680b870475176d,openstack/keystone,master,Icbd867ff4e385f4430bfa9b7c5680b870475176d,Uses session in migration to stop DB locking,MERGED,2014-09-26 22:46:47.000000000,2014-09-27 21:27:09.000000000,2014-09-27 21:27:08.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 792}, {'_account_id': 1916}, {'_account_id': 2218}, {'_account_id': 2903}, {'_account_id': 5046}, {'_account_id': 5707}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 7191}, {'_account_id': 7725}, {'_account_id': 9098}, {'_account_id': 11816}]","[{'number': 1, 'created': '2014-09-26 22:46:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/23e09315c5c728a549fe2940b782c1c7b74d1175', 'message': 'Adds a commit in a migration between SQA calls\n\nThe bugs appears to only be reproducible in SQLite.  It seems\nthat the session is not ready to do a delete immediately after\nusing the session to select the entire table. This change will\ncommit after the SELECT, which seems to unlock the SQLite\ndatabase.\n\nChange-Id: Icbd867ff4e385f4430bfa9b7c5680b870475176d\nCloses-bug: #1371620\n'}, {'number': 2, 'created': '2014-09-26 23:32:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/b626b501e341f527d01b5bcfaec114c664bb2d69', 'message': 'Uses session in migration to stop DB locking\n\nUsing both the engine and session in a migration causes the DB to\nbecome locked.\n\nChange-Id: Icbd867ff4e385f4430bfa9b7c5680b870475176d\nCloses-bug: #1371620\n'}, {'number': 3, 'created': '2014-09-27 03:56:09.000000000', 'files': ['keystone/common/sql/migrate_repo/versions/039_grant_to_assignment.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/4da0bccfef467036d83fc87b51c962438004d9f0', 'message': 'Uses session in migration to stop DB locking\n\nUsing both the engine and session in a migration causes the DB to\nbecome locked.\n\nChange-Id: Icbd867ff4e385f4430bfa9b7c5680b870475176d\nCloses-bug: #1371620\n'}]",0,124533,4da0bccfef467036d83fc87b51c962438004d9f0,13,14,3,7725,,,0,"Uses session in migration to stop DB locking

Using both the engine and session in a migration causes the DB to
become locked.

Change-Id: Icbd867ff4e385f4430bfa9b7c5680b870475176d
Closes-bug: #1371620
",git fetch https://review.opendev.org/openstack/keystone refs/changes/33/124533/3 && git format-patch -1 --stdout FETCH_HEAD,['keystone/common/sql/migrate_repo/versions/039_grant_to_assignment.py'],1,23e09315c5c728a549fe2940b782c1c7b74d1175,bug/1371620, session.commit() session.commit(),,2,0
openstack%2Fneutron~stable%2Ficehouse~Ibdcd74f47043762386b62f3ec0fa1723060446ac,openstack/neutron,stable/icehouse,Ibdcd74f47043762386b62f3ec0fa1723060446ac,call security_groups_member_updated in port_update,MERGED,2014-08-18 17:42:12.000000000,2014-09-27 21:26:55.000000000,2014-09-27 21:26:53.000000000,"[{'_account_id': 3}, {'_account_id': 161}, {'_account_id': 1420}, {'_account_id': 2531}, {'_account_id': 2592}, {'_account_id': 4187}, {'_account_id': 5170}, {'_account_id': 6502}, {'_account_id': 6659}, {'_account_id': 7787}, {'_account_id': 8213}, {'_account_id': 8645}, {'_account_id': 9656}, {'_account_id': 9681}, {'_account_id': 9732}, {'_account_id': 9846}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10672}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-08-18 17:42:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e3454d1dd0b02fdf29c31e249b448cfd667cdb77', 'message': 'call security_groups_member_updated in port_update\n\nWhen a running VM is added to a security group, all existing VMs (ports)\nin the security group should be notified/updated - otherwise they would\nhave incorrect rules, not knowing the new VM/port added.\n\nThe current behavior would only update the port of the added VM. This\npatch forces an security_groups_member_updated() call for all the\nsecurity groups that have ports removed or added.\n\nChange-Id: Ibdcd74f47043762386b62f3ec0fa1723060446ac\nCloses-Bug: 1316618\n(cherry picked from commit e97eea83ae162b9ce0f45f1ff334dbf70608fe3f)\n'}, {'number': 2, 'created': '2014-08-26 13:33:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/5d01b8656aa10f755a23910428258f285d859aec', 'message': 'call security_groups_member_updated in port_update\n\nWhen a running VM is added to a security group, all existing VMs (ports)\nin the security group should be notified/updated - otherwise they would\nhave incorrect rules, not knowing the new VM/port added.\n\nThe current behavior would only update the port of the added VM. This\npatch forces an security_groups_member_updated() call for all the\nsecurity groups that have ports removed or added.\n\nChange-Id: Ibdcd74f47043762386b62f3ec0fa1723060446ac\nCloses-Bug: 1316618\n(cherry picked from commit e97eea83ae162b9ce0f45f1ff334dbf70608fe3f)\n'}, {'number': 3, 'created': '2014-08-26 13:37:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/4e4fedf67f1296de8f01393430e18b37e936635a', 'message': 'call security_groups_member_updated in port_update\n\nWhen a running VM is added to a security group, all existing VMs (ports)\nin the security group should be notified/updated - otherwise they would\nhave incorrect rules, not knowing the new VM/port added.\n\nThe current behavior would only update the port of the added VM. This\npatch forces an security_groups_member_updated() call for all the\nsecurity groups that have ports removed or added.\n\nChange-Id: Ibdcd74f47043762386b62f3ec0fa1723060446ac\nCloses-Bug: 1316618\n(cherry picked from commit e97eea83ae162b9ce0f45f1ff334dbf70608fe3f)\n'}, {'number': 4, 'created': '2014-08-26 13:41:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/a58a8f8e2a8ca72a694df74e40180d3bad38b4e2', 'message': 'call security_groups_member_updated in port_update\n\nWhen a running VM is added to a security group, all existing VMs (ports)\nin the security group should be notified/updated - otherwise they would\nhave incorrect rules, not knowing the new VM/port added.\n\nThe current behavior would only update the port of the added VM. This\npatch forces an security_groups_member_updated() call for all the\nsecurity groups that have ports removed or added.\n\nChange-Id: Ibdcd74f47043762386b62f3ec0fa1723060446ac\nCloses-Bug: 1316618\n(cherry picked from commit e97eea83ae162b9ce0f45f1ff334dbf70608fe3f)\n'}, {'number': 5, 'created': '2014-09-17 22:33:09.000000000', 'files': ['neutron/plugins/openvswitch/ovs_neutron_plugin.py', 'neutron/tests/unit/openvswitch/test_openvswitch_plugin.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/3520e66b8745924ebb9bd2978e8e11b60f58a221', 'message': 'call security_groups_member_updated in port_update\n\nWhen a running VM is added to a security group, all existing VMs (ports)\nin the security group should be notified/updated - otherwise they would\nhave incorrect rules, not knowing the new VM/port added.\n\nThe current behavior would only update the port of the added VM. This\npatch forces an security_groups_member_updated() call for all the\nsecurity groups that have ports removed or added.\n\nChange-Id: Ibdcd74f47043762386b62f3ec0fa1723060446ac\nCloses-Bug: 1316618\n(cherry picked from commit e97eea83ae162b9ce0f45f1ff334dbf70608fe3f)\n'}]",6,115038,3520e66b8745924ebb9bd2978e8e11b60f58a221,118,27,5,161,,,0,"call security_groups_member_updated in port_update

When a running VM is added to a security group, all existing VMs (ports)
in the security group should be notified/updated - otherwise they would
have incorrect rules, not knowing the new VM/port added.

The current behavior would only update the port of the added VM. This
patch forces an security_groups_member_updated() call for all the
security groups that have ports removed or added.

Change-Id: Ibdcd74f47043762386b62f3ec0fa1723060446ac
Closes-Bug: 1316618
(cherry picked from commit e97eea83ae162b9ce0f45f1ff334dbf70608fe3f)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/38/115038/3 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/openvswitch/ovs_neutron_plugin.py', 'neutron/tests/unit/openvswitch/test_openvswitch_plugin.py']",2,e3454d1dd0b02fdf29c31e249b448cfd667cdb77,bug/1316618,"from neutron import contextfrom neutron.extensions import securitygroup as ext_sg from neutron.plugins.openvswitch import ovs_neutron_pluginimport mock class TestOpenvswitchUpdatePort(OpenvswitchPluginV2TestCase, ovs_neutron_plugin.OVSNeutronPluginV2): def test_update_port_add_remove_security_group(self): get_port_func = ( 'neutron.db.db_base_plugin_v2.' 'NeutronDbPluginV2.get_port' ) with mock.patch(get_port_func) as mock_get_port: mock_get_port.return_value = { ext_sg.SECURITYGROUPS: [""sg1"", ""sg2""], ""admin_state_up"": True, ""fixed_ips"": ""fake_ip"", ""network_id"": ""fake_id""} update_port_func = ( 'neutron.db.db_base_plugin_v2.' 'NeutronDbPluginV2.update_port' ) with mock.patch(update_port_func) as mock_update_port: mock_update_port.return_value = { ext_sg.SECURITYGROUPS: [""sg2"", ""sg3""], ""admin_state_up"": True, ""fixed_ips"": ""fake_ip"", ""network_id"": ""fake_id""} fake_func = ( 'neutron.plugins.openvswitch.' 'ovs_db_v2.get_network_binding' ) with mock.patch(fake_func) as mock_func: class MockBinding: network_type = ""fake"" segmentation_id = ""fake"" physical_network = ""fake"" mock_func.return_value = MockBinding() ctx = context.Context('', 'somebody') self.update_port(ctx, ""id"", { ""port"": { ext_sg.SECURITYGROUPS: [ ""sg2"", ""sg3""]}}) sgmu = self.notifier.security_groups_member_updated sgmu.assert_called_with(ctx, set(['sg1', 'sg3'])) def setUp(self): super(TestOpenvswitchUpdatePort, self).setUp() self.update_security_group_on_port = mock.MagicMock(return_value=True) self._process_portbindings_create_and_update = mock.MagicMock( return_value=True) self._update_extra_dhcp_opts_on_port = mock.MagicMock( return_value=True) self.update_address_pairs_on_port = mock.MagicMock( return_value=True) class MockNotifier: def __init__(self): self.port_update = mock.MagicMock(return_value=True) self.security_groups_member_updated = mock.MagicMock( return_value=True) self.notifier = MockNotifier()",,82,1
openstack%2Ftaskflow~master~I443794b83de3f6a196fa7fc29a90620fb51b7f4c,openstack/taskflow,master,I443794b83de3f6a196fa7fc29a90620fb51b7f4c,Move some of the custom requirements out of tox.ini,MERGED,2014-09-09 18:03:57.000000000,2014-09-27 21:21:57.000000000,2014-09-27 21:21:57.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 9608}]","[{'number': 1, 'created': '2014-09-09 18:03:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/b3a41ec294f8fd5ee085e9000fc7aa1db0e75eb8', 'message': ""Move some of the custom requirements out of tox.ini\n\nThis change removes the need for an optional-requirements\nfile which isn't being kept up to date with the rest of the\nopenstack ecosystem and moves most of the customizations to\ntest-requirements and where different versions are still\nneeded (aka for SQLAlchemy) we place those varations into\nthere needed tox environment as required.\n\nChange-Id: I443794b83de3f6a196fa7fc29a90620fb51b7f4c\n""}, {'number': 2, 'created': '2014-09-09 18:28:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/3b45449dbb86344ab97c47e840b73b437623092c', 'message': ""Move some of the custom requirements out of tox.ini\n\nThis change removes the need for an optional-requirements\nfile which isn't being kept up to date with the rest of the\nopenstack ecosystem and moves most of the customizations to\ntest-requirements and where different versions are still\nneeded (aka for SQLAlchemy) we place those varations into\nthere needed tox environment as required.\n\nChange-Id: I443794b83de3f6a196fa7fc29a90620fb51b7f4c\n""}, {'number': 3, 'created': '2014-09-09 23:39:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/c6fa485989ff06a9be32e2270e8368f857342ee3', 'message': ""Move some of the custom requirements out of tox.ini\n\nThis change removes the need for an optional-requirements\nfile which isn't being kept up to date with the rest of the\nopenstack ecosystem and moves most of the customizations to\ntest-requirements and where different versions are still\nneeded (aka for SQLAlchemy) we place those varations into\nthere needed tox environment as required.\n\nChange-Id: I443794b83de3f6a196fa7fc29a90620fb51b7f4c\n""}, {'number': 4, 'created': '2014-09-20 15:28:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/f0c8113e6f6100cfc9b0ac3400d4a406598b8ccc', 'message': ""Move some of the custom requirements out of tox.ini\n\nThis change removes the need for an optional-requirements\nfile which isn't being kept up to date with the rest of the\nopenstack ecosystem and moves most of the customizations to\ntest-requirements and where different versions are still\nneeded (aka for SQLAlchemy) we place those varations into\nthere needed tox environment as required.\n\nChange-Id: I443794b83de3f6a196fa7fc29a90620fb51b7f4c\n""}, {'number': 5, 'created': '2014-09-23 03:09:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/8e5fc40a6d55a5e6c86120958d31374339968f70', 'message': ""Move some of the custom requirements out of tox.ini\n\nThis change removes the need for an optional-requirements\nfile which isn't being kept up to date with the rest of the\nopenstack ecosystem and moves most of the customizations to\ntest-requirements and where different versions are still\nneeded (aka for SQLAlchemy) we place those varations into\nthere needed tox environment as required.\n\nChange-Id: I443794b83de3f6a196fa7fc29a90620fb51b7f4c\n""}, {'number': 6, 'created': '2014-09-26 19:49:35.000000000', 'files': ['test-requirements.txt', 'optional-requirements.txt', 'requirements-py2.txt', 'requirements-py3.txt', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/1e216c61f11ff7efe9cf5e4b205c8f418cf76fc0', 'message': ""Move some of the custom requirements out of tox.ini\n\nThis change removes the need for an optional-requirements\nfile which isn't being kept up to date with the rest of the\nopenstack ecosystem and moves most of the customizations to\ntest-requirements and where different versions are still\nneeded (aka for SQLAlchemy) we place those varations into\nthere needed tox environment as required.\n\nChange-Id: I443794b83de3f6a196fa7fc29a90620fb51b7f4c\n""}]",0,120168,1e216c61f11ff7efe9cf5e4b205c8f418cf76fc0,19,3,6,1297,,,0,"Move some of the custom requirements out of tox.ini

This change removes the need for an optional-requirements
file which isn't being kept up to date with the rest of the
openstack ecosystem and moves most of the customizations to
test-requirements and where different versions are still
needed (aka for SQLAlchemy) we place those varations into
there needed tox environment as required.

Change-Id: I443794b83de3f6a196fa7fc29a90620fb51b7f4c
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/68/120168/6 && git format-patch -1 --stdout FETCH_HEAD,"['test-requirements.txt', 'optional-requirements.txt', 'requirements-py2.txt', 'requirements-py3.txt', 'tox.ini']",5,b3a41ec294f8fd5ee085e9000fc7aa1db0e75eb8,,"basepython = python2.6 -r{toxinidir}/test-requirements.txt SQLAlchemy>=0.7.8,<=0.7.99 MySQL-python eventlet>=0.13.0 SQLAlchemy>=0.7.8,<=0.9.99 -r{toxinidir}/test-requirements.txtbasepython = python2.6basepython = python2.7"," alembic>=0.4.1 psycopg2 kazoo>=1.3.1 kombu>=2.4.8 SQLAlchemy>=0.7.8,<=0.7.99basepython = python2.6 -r{toxinidir}/optional-requirements.txt doc8>=0.3.4basepython = python2.6basepython = python2.7",40,42
openstack%2Fheat~master~I827034bb5d3bed195512dd39f636322e0f5da400,openstack/heat,master,I827034bb5d3bed195512dd39f636322e0f5da400,Add variables to HOT spec,ABANDONED,2014-04-01 15:02:23.000000000,2014-09-27 21:11:31.000000000,,"[{'_account_id': 3}, {'_account_id': 2834}, {'_account_id': 4257}, {'_account_id': 4715}, {'_account_id': 6904}, {'_account_id': 7193}, {'_account_id': 8246}, {'_account_id': 8289}, {'_account_id': 8328}, {'_account_id': 8871}, {'_account_id': 9237}]","[{'number': 1, 'created': '2014-04-01 15:02:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/97dae6c9e6c45e353c01e56c18d216a6e820d2ec', 'message': 'Add conditional creation of resource to the HOT specification\n\nThe following patch proposes the addition of a new conditions\ntop level section and several intrinsics to operate on the conditions.\n\nThe condition result is only used to determine whether a resource\nshould or should not be created.\n\nMy major opposition to conditionals in the past has been that they\nmake HOT a non-declartive language.  With the approach used, even\nthough there are variables delcared (that is what the conditions section\ndoes), all of the variables are resolveable at template parsing time.\n\nMy main concern with conditionals is in changing conditional scope to\ninclude run-time evaluation of variables, which would make HOT imperative\nand more complicated.\n\nChange-Id: I827034bb5d3bed195512dd39f636322e0f5da400\n'}, {'number': 2, 'created': '2014-04-04 23:48:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/1cdd14abaac29eaffc62d9cef4d1078f5f32a6cf', 'message': 'Add variables to HOT spec\n\nThe following patch proposes the addition of a new variables\ntop level section and several intrinsic functions to\noperation on the variables.\n\nThe variables can be used in any number of places, but there is\na limitation on allowing the use of resource objects in variable\nevaluation.\n\nMy major opposition to conditionals in the past has been that they\nmake HOT a non-declartive language.  With the approach used, even\nthough there are variables delcared, all of the variables are\nresolveable at template parsing time.\n\nVariables may not be circular.  Any attempt to use a variable in a\ncircular fashion will result in a validation error.\n\nMy main concern with variables is in changing variable scope to\ninclude run-time evaluation of variables, which would make HOT\n imperative and more complicated.  If this can be avoided in the\nfuture, the change makes sense to me.\n\nChange-Id: I827034bb5d3bed195512dd39f636322e0f5da400\n'}, {'number': 3, 'created': '2014-04-04 23:51:12.000000000', 'files': ['doc/source/template_guide/hot_spec.rst'], 'web_link': 'https://opendev.org/openstack/heat/commit/f58bdfd43d00b16f01434152bf57699a30b6b2ee', 'message': 'Add variables to HOT spec\n\nThe following patch proposes the addition of a new variables\ntop level section and several intrinsic functions to\noperation on the variables.\n\nThe variables can be used in any number of places, but there is\na limitation on not allowing the use of resource objects in variable\nevaluation.\n\nMy major opposition to conditionals in the past has been that they\nmake HOT a non-declartive language.  With the approach used, even\nthough variables delcared, all of the variables are resolveable at\ntemplate validation.\n\nVariables may not be circular.  Any attempt to use a variable in a\ncircular fashion will result in a validation error.\n\nMy main concern with variables is in changing variable scope to\ninclude run-time evaluation of variables, which would make HOT\nimperative and more complicated.  If this can be avoided in the\nfuture, the change makes sense to me.\n\nChange-Id: I827034bb5d3bed195512dd39f636322e0f5da400\n'}]",76,84468,f58bdfd43d00b16f01434152bf57699a30b6b2ee,50,11,3,2834,,,0,"Add variables to HOT spec

The following patch proposes the addition of a new variables
top level section and several intrinsic functions to
operation on the variables.

The variables can be used in any number of places, but there is
a limitation on not allowing the use of resource objects in variable
evaluation.

My major opposition to conditionals in the past has been that they
make HOT a non-declartive language.  With the approach used, even
though variables delcared, all of the variables are resolveable at
template validation.

Variables may not be circular.  Any attempt to use a variable in a
circular fashion will result in a validation error.

My main concern with variables is in changing variable scope to
include run-time evaluation of variables, which would make HOT
imperative and more complicated.  If this can be avoided in the
future, the change makes sense to me.

Change-Id: I827034bb5d3bed195512dd39f636322e0f5da400
",git fetch https://review.opendev.org/openstack/heat refs/changes/68/84468/3 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/template_guide/hot_spec.rst'],1,97dae6c9e6c45e353c01e56c18d216a6e820d2ec,," conditions: # declaration of conditionals ------------------ Conditions Section ------------------ In the *conditions* section, conditionals are defined which return either true or false. Conditionals are created with intrinsic functions (see :ref:`hot_spec_intrinsic_functions`) which evaluate parameters from the parameter section. These conditionals may only be used within the conditional property of a resource. If the condition evaluates to true, the resource is created. If the conditional evalues to false, the resource is not created. :: conditions: <condition ID>: <condition value> condition ID: A condition block is headed by the condition ID, which must be unique within the condition section of a template. condition value: A conditional value created by the evaluation of the conditional intrinsic functions (see :ref:`hot_spec_intrinsic_functions`). condition: <condition id>condition: A condition ID which is evaluated to determine if the resource should be created or should not be created.get_cond -------- The *get_cond* function allows for referencing a conditional result from the conditional section of a template. :: get_cond: <conditional ID> The *conditional ID* of the referenced conditional is retrieved for evaluation by the conditions parameter of the resource. cond_and ------------ The *cond_and* function returns true if all the specified conditions evaluate to true, or return false if any one of the conditions evalues to false. :: cond_and: - <condition_id> - <condition id 2> - <condition id 3> (optional) - ... The example below demonstrates *cond_and* usage and its impact on the creation of a resource. Note the resource is only created if the create_small parameter is set to yes and instance_type is m1.small. :: parameters: create_small: type: string label: Instance Type default: 'yes' description: Instance type to be used. instance_type: type: string label: Instance Type description: Instance type to be used. conditionals: small_desired: { cond_equals: [ { get_param: create_small } , 'yes' ] } type_valid: { cond_equals: [ { get_param: instance_type }, 'm1.small' ] } server_create: { cond_and: [ { get_cond: type_valid }, { get_cond: small_desired } ] } resources: my_instance: type: OS::Nova::Server conditional: { get_cond: server_create } properties: # general properties ... cond_equals ------------ The *cond_equals* function returns true if the evaluated values are equal and false if they are not equal. :: cond_equals: - <value1> - <value2> The example below demonstrates *cond_equals* usage and its impact on the creation of a resource. Note the resource is only created if the create_small parameter is set to yes.small. :: parameters: create_small: type: string label: Instance Type default: 'yes' description: Instance type to be used. conditionals: small_desired: { cond_equals: [ { get_param: create_small } , 'yes' ] } resources: my_instance: type: OS::Nova::Server conditional: { get_cond: small_desired } properties: # general properties ... cond_not ------------ The *cond_not* function returns true for a condition that evaluates to false otherwise returns true. :: cond_not: - <condition ID> The example below demonstrates *cond_not* usage and its impact on the creation of a resource. Note the resource is only created if the create_small parameter is not set to yes.small. :: parameters: create_small: type: string label: Instance Type default: 'yes' description: Instance type to be used. conditionals: no_small_desired: { cond_not: [ { get_param: create_small } , 'yes' ] } resources: my_instance: type: OS::Nova::Server conditional: { get_cond: no_small_desired } properties: # general properties ... cond_or ------------ The *cond_or* function returns true if any of the specified conditions evaluates to true, or returns false otherwise. :: cond_or: - <condition ID> - <value 1> - <value 2> (optional) - ... :: The example below demonstrates *cond_or* usage and its impact on the creation of a resource. Note the resource is created if either the create_small parameter is set to yes or instance_type is m1.small. :: parameters: create_small: type: string label: Instance Type default: 'yes' description: Instance type to be used. instance_type: type: string label: Instance Type description: Instance type to be used. conditionals: small_desired: { cond_equals: [ { get_param: create_small } , 'yes' ] } type_valid: { cond_equals: [ { get_param: instance_type }, 'm1.small' ] } server_create: { cond_or [ { get_cond: type_valid }, { get_cond: small_desired } ] } resources: my_instance: type: OS::Nova::Server conditional: { get_cond: server_create } properties: # general properties ...",,204,0
openstack%2Fmonasca-agent~master~I6d14ddcf3e103b8bc66d3113466c0685fcb92bdd,openstack/monasca-agent,master,I6d14ddcf3e103b8bc66d3113466c0685fcb92bdd,Fix detection and operation of Kafka plugin,MERGED,2014-07-28 20:20:03.000000000,2014-09-27 20:57:11.000000000,2014-09-27 20:57:11.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 2419}, {'_account_id': 11094}, {'_account_id': 12443}]","[{'number': 1, 'created': '2014-07-28 20:20:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/d9cf68c4f1ad6ab3dd2e956bd910336aad730b6d', 'message': 'Fix detection and operation of Kafka plugin\n\nChange-Id: I6d14ddcf3e103b8bc66d3113466c0685fcb92bdd\n'}, {'number': 2, 'created': '2014-07-28 22:16:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/422ba5e17bc205a1a0189823ae05b64671532a7b', 'message': 'Fix detection and operation of Kafka plugin\n\nChange-Id: I6d14ddcf3e103b8bc66d3113466c0685fcb92bdd\n'}, {'number': 3, 'created': '2014-07-29 17:41:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/05736ce78556e73d3a5f681b8a2db43c2679bdbc', 'message': 'Fix detection and operation of Kafka plugin\n\nChange-Id: I6d14ddcf3e103b8bc66d3113466c0685fcb92bdd\n'}, {'number': 4, 'created': '2014-08-14 17:20:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/1d5246ac59771170bce6238cc57add6cd7443e3c', 'message': ""Fix detection and operation of Kafka plugin\n\nResolves the following issues:\n - kafka plugin detection code was assuming 'localhost' for the kafka server\n - kafka_consumer check was choking on the YAML generated by the plugin\n - KafkaClient class expects host:port string for its first parameter\n\nChange-Id: I6d14ddcf3e103b8bc66d3113466c0685fcb92bdd\n""}, {'number': 5, 'created': '2014-08-14 18:19:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/7b13bb81cd2dee90ec93477907204e386c112d2c', 'message': ""Fix detection and operation of Kafka plugin\n\nResolves the following issues:\n - kafka plugin detection code was assuming 'localhost' for the kafka server\n - kafka_consumer check was choking on the YAML generated by the plugin\n - KafkaClient class expects host:port string for its first parameter\n\nChange-Id: I6d14ddcf3e103b8bc66d3113466c0685fcb92bdd\n""}, {'number': 6, 'created': '2014-08-14 21:50:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/6010a991ed554c37f5350de4c39fd2d0f60696d5', 'message': ""Fix detection and operation of Kafka plugin\n\nResolves the following issues:\n - kafka plugin detection code was assuming 'localhost' for the kafka server\n - kafka_consumer check was choking on the YAML generated by the plugin\n - KafkaClient class expects host:port string for its first parameter\n - kafka_consumer.py upstream updates\n\nChange-Id: I6d14ddcf3e103b8bc66d3113466c0685fcb92bdd\n""}, {'number': 7, 'created': '2014-08-21 17:13:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/38a7f6d46d61afca8d464c05b047a4337c99d214', 'message': ""Fix detection and operation of Kafka plugin\n\nResolves the following issues:\n - kafka plugin detection code was assuming 'localhost' for the kafka server\n - kafka_consumer check was choking on the YAML generated by the plugin\n - KafkaClient class expects host:port string for its first parameter\n - kafka_consumer.py upstream updates\n\nChange-Id: I6d14ddcf3e103b8bc66d3113466c0685fcb92bdd\n""}, {'number': 8, 'created': '2014-09-26 23:24:06.000000000', 'files': ['monsetup/detection/utils.py', 'monsetup/detection/plugins/kafka.py', 'monagent/collector/checks_d/kafka_consumer.py', 'monsetup/detection/plugins/kafka_consumer.py', 'conf.d/kafka_consumer.yaml.example', 'monsetup/main.py'], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/1043c7e242d741de4a7cd716f972087abfac6b82', 'message': 'Fix detection and operation of Kafka plugin\n\nChange-Id: I6d14ddcf3e103b8bc66d3113466c0685fcb92bdd\n'}]",6,110124,1043c7e242d741de4a7cd716f972087abfac6b82,35,5,8,12443,,,0,"Fix detection and operation of Kafka plugin

Change-Id: I6d14ddcf3e103b8bc66d3113466c0685fcb92bdd
",git fetch https://review.opendev.org/openstack/monasca-agent refs/changes/24/110124/8 && git format-patch -1 --stdout FETCH_HEAD,"['monsetup/detection/utils.py', 'monsetup/detection/plugins/kafka.py', 'monagent/collector/checks_d/kafka_consumer.py', 'monsetup/main.py']",4,d9cf68c4f1ad6ab3dd2e956bd910336aad730b6d,feature/kafka2," config_file.write(yaml.safe_dump(value, encoding='utf-8', allow_unicode=True))", config_file.write(yaml.dump(value)),21,3
openstack%2Ftaskflow~master~I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932,openstack/taskflow,master,I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932,Add a mandelbrot parallel calculation WBE example,MERGED,2014-07-11 00:32:43.000000000,2014-09-27 20:34:07.000000000,2014-09-27 20:34:06.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 8871}, {'_account_id': 8895}]","[{'number': 1, 'created': '2014-07-11 00:32:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/e8918cf8e7b6895734573f432c248f08deb99b86', 'message': 'Add a mandlebrot parallel calculation WBE example\n\nThe mandlebrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandlebrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}, {'number': 2, 'created': '2014-07-11 00:36:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/03c694fcdc9151e4bae61b76449c1e1260eb5f2a', 'message': 'Add a mandlebrot parallel calculation WBE example\n\nThe mandlebrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandlebrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nSee: http://en.wikipedia.org/wiki/Mandelbrot_set\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}, {'number': 3, 'created': '2014-07-11 01:44:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/7036869cb9e022252db8706d6702fb53b6e7d138', 'message': 'Add a mandlebrot parallel calculation WBE example\n\nThe mandlebrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandlebrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nSee: http://en.wikipedia.org/wiki/Mandelbrot_set\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}, {'number': 4, 'created': '2014-07-11 01:57:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/551ffe4097bbafd5c2803be76ed748a3ae7e65a5', 'message': 'Add a mandlebrot parallel calculation WBE example\n\nThe mandlebrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandlebrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nSee: http://en.wikipedia.org/wiki/Mandelbrot_set\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}, {'number': 5, 'created': '2014-07-11 07:01:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/e2c1757f4780411e22304289694e2c90750496c1', 'message': 'Add a mandlebrot parallel calculation WBE example\n\nThe mandlebrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandlebrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nSee: http://en.wikipedia.org/wiki/Mandelbrot_set\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}, {'number': 6, 'created': '2014-07-11 07:04:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/aa38c2ae9a3aa4381dd0836bbd5a0b05346c1d0f', 'message': 'Add a mandlebrot parallel calculation WBE example\n\nThe mandlebrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandlebrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nSee: http://en.wikipedia.org/wiki/Mandelbrot_set\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}, {'number': 7, 'created': '2014-07-11 23:02:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/d664821601406dfe19c814f7624bbfe6080cb9d8', 'message': 'Add a mandlebrot parallel calculation WBE example\n\nThe mandlebrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandlebrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nSee: http://en.wikipedia.org/wiki/Mandelbrot_set\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}, {'number': 8, 'created': '2014-08-17 02:49:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/1f00a2ee6a8ac1b9baa9fa3a6cd839827b2013a2', 'message': 'Add a mandelbrot parallel calculation WBE example\n\nThe mandelbrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandelbrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nSee: http://en.wikipedia.org/wiki/Mandelbrot_set\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}, {'number': 9, 'created': '2014-08-17 02:51:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/28ab80c37e3a6cedf23be2712c8be8850689a387', 'message': 'Add a mandelbrot parallel calculation WBE example\n\nThe mandelbrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandelbrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nSee: http://en.wikipedia.org/wiki/Mandelbrot_set\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}, {'number': 10, 'created': '2014-08-29 06:34:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/0b60480de6a215b336ec52fb7e01e313a2f333a8', 'message': 'Add a mandelbrot parallel calculation WBE example\n\nThe mandelbrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandelbrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nSee: http://en.wikipedia.org/wiki/Mandelbrot_set\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}, {'number': 11, 'created': '2014-08-31 00:31:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/3f39e644ea6a546bf80c32abc4bdc540df9d31ba', 'message': 'Add a mandelbrot parallel calculation WBE example\n\nThe mandelbrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandelbrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nSee: http://en.wikipedia.org/wiki/Mandelbrot_set\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}, {'number': 12, 'created': '2014-09-04 21:42:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/286fbe5da9a90ad96f190fabaf70aa641191bf12', 'message': 'Add a mandelbrot parallel calculation WBE example\n\nThe mandelbrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandelbrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nSee: http://en.wikipedia.org/wiki/Mandelbrot_set\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}, {'number': 13, 'created': '2014-09-12 22:03:26.000000000', 'files': ['doc/source/examples.rst', 'doc/source/img/mandelbrot.png', 'taskflow/examples/wbe_mandelbrot.py', 'taskflow/examples/wbe_mandelbrot.out.txt'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/adf417cda526f4551079a0ae3fdcdeb8b803be09', 'message': 'Add a mandelbrot parallel calculation WBE example\n\nThe mandelbrot calculation is neat example to provide\nto show how the WBE engine can be used to compute various\ncalculations (the mandelbrot set in this example).\n\nThis will also create an image from the output if a filename\nis provided and the pillow library is installed (that library\ndoes the actual image writing).\n\nSee: http://en.wikipedia.org/wiki/Mandelbrot_set\n\nPart of blueprint more-examples\n\nChange-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932\n'}]",0,106216,adf417cda526f4551079a0ae3fdcdeb8b803be09,51,4,13,1297,,,0,"Add a mandelbrot parallel calculation WBE example

The mandelbrot calculation is neat example to provide
to show how the WBE engine can be used to compute various
calculations (the mandelbrot set in this example).

This will also create an image from the output if a filename
is provided and the pillow library is installed (that library
does the actual image writing).

See: http://en.wikipedia.org/wiki/Mandelbrot_set

Part of blueprint more-examples

Change-Id: I12b9b7a2ce61b17ddaa2930cc9cf266ae0c60932
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/16/106216/4 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/examples.rst', 'taskflow/examples/wbe_mandlebrot.py', 'taskflow/examples/wbe_mandlebrot.out.txt']",3,e8918cf8e7b6895734573f432c248f08deb99b86,bp/more-examples,Calculating your mandlebrot fractal of size 512x512. Running 2 workers. Execution finished. Stopping workers. Writing image... Gathered 262144 results that represents a mandlebrot image (using 8 chunks that are computed jointly by 2 workers). ,,265,0
openstack%2Ftaskflow~master~Ifb17fc1a4b4df2ea7cbc3a49e9815888dd1d7467,openstack/taskflow,master,Ifb17fc1a4b4df2ea7cbc3a49e9815888dd1d7467,"Typos ""searchs""",MERGED,2014-09-18 23:41:06.000000000,2014-09-27 20:30:01.000000000,2014-09-27 20:30:00.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 13300}]","[{'number': 1, 'created': '2014-09-18 23:41:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/3e6366f33124017c1853440ef8fdd7a370b9ea8e', 'message': 'Typos ""occuring"" and ""searchs""\n\nMisspelling of ""occurring"" found in method capture_failure,\nand ""searchs"" in method index.\n\nChange-Id: Ifb17fc1a4b4df2ea7cbc3a49e9815888dd1d7467\n'}, {'number': 2, 'created': '2014-09-20 15:59:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/c7c2082f8b6e29f84a3c23ef1c1e861c3d75702a', 'message': 'Typos ""occuring"" and ""searchs""\n\nMisspelling of ""occurring"" found in method capture_failure,\nand ""searchs"" in method index.\n\nChange-Id: Ifb17fc1a4b4df2ea7cbc3a49e9815888dd1d7467\n'}, {'number': 3, 'created': '2014-09-20 15:59:43.000000000', 'files': ['taskflow/types/tree.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/de652c770aa281214aad7003fc6d4d1a85802e4c', 'message': 'Typos ""searchs""\n\nMisspelling of ""searchs"" in method index.\n\nChange-Id: Ifb17fc1a4b4df2ea7cbc3a49e9815888dd1d7467\n'}]",0,122566,de652c770aa281214aad7003fc6d4d1a85802e4c,15,3,3,13300,,,0,"Typos ""searchs""

Misspelling of ""searchs"" in method index.

Change-Id: Ifb17fc1a4b4df2ea7cbc3a49e9815888dd1d7467
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/66/122566/1 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/types/tree.py', 'taskflow/utils/misc.py']",2,3e6366f33124017c1853440ef8fdd7a370b9ea8e,," """"""Captures the occurring exception and provides a failure back."," """"""Captures the occuring exception and provides a failure back.",2,2
openstack%2Fkeystone~master~If970cdf20cfca8b1dc667eefd030083fdafe9424,openstack/keystone,master,If970cdf20cfca8b1dc667eefd030083fdafe9424,Set issuer value to CONF.saml.idp_entity_id.,MERGED,2014-09-25 20:24:30.000000000,2014-09-27 20:28:39.000000000,2014-09-27 20:28:38.000000000,"[{'_account_id': 3}, {'_account_id': 2903}, {'_account_id': 6482}, {'_account_id': 7725}, {'_account_id': 8978}]","[{'number': 1, 'created': '2014-09-25 20:24:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/a70d5ec3cf6b02479a5be3c9f7d466af690fa57f', 'message': 'Set issuer value to CONF.saml.idp_entity_id.\n\nWhen generating SAML assertion Keystone should always set issuer value\nin federation.controllers.Auth.create_saml_assertion() to\nCONF.saml.idp_entity_id.\n\nChange-Id: If970cdf20cfca8b1dc667eefd030083fdafe9424\nCloses-Bug: #1374033\n'}, {'number': 2, 'created': '2014-09-26 13:25:32.000000000', 'files': ['keystone/contrib/federation/controllers.py', 'keystone/tests/test_v3_federation.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/62099029e9c794c5bc4a780fe34fd51ebffee6e1', 'message': 'Set issuer value to CONF.saml.idp_entity_id.\n\nWhen generating SAML assertion Keystone should always set issuer value\nin federation.controllers.Auth.create_saml_assertion() to\nCONF.saml.idp_entity_id.\n\nChange-Id: If970cdf20cfca8b1dc667eefd030083fdafe9424\nCloses-Bug: #1374033\n'}]",0,124176,62099029e9c794c5bc4a780fe34fd51ebffee6e1,10,5,2,8978,,,0,"Set issuer value to CONF.saml.idp_entity_id.

When generating SAML assertion Keystone should always set issuer value
in federation.controllers.Auth.create_saml_assertion() to
CONF.saml.idp_entity_id.

Change-Id: If970cdf20cfca8b1dc667eefd030083fdafe9424
Closes-Bug: #1374033
",git fetch https://review.opendev.org/openstack/keystone refs/changes/76/124176/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/contrib/federation/controllers.py'],1,a70d5ec3cf6b02479a5be3c9f7d466af690fa57f,bug_1374033, issuer = CONF.saml.idp_entity_id," issuer = wsgi.Application.base_url(context, 'public') ",1,2
openstack%2Ftaskflow~master~Ia21a05fe9249fa019a09c4f30beeb0770ded5150,openstack/taskflow,master,Ia21a05fe9249fa019a09c4f30beeb0770ded5150,Fix multilock concurrency when shared by > 1 threads,MERGED,2014-09-19 22:57:52.000000000,2014-09-27 20:28:31.000000000,2014-09-27 20:28:30.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 1669}]","[{'number': 1, 'created': '2014-09-19 22:57:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/5a307211bc1114f44ec3a7bb6fa0c267ffa8766e', 'message': 'Fix multilock concurrency when shared by many threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 2, 'created': '2014-09-19 22:59:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/0af9412046c7a4226faceeb25aa4608024bda8a0', 'message': 'Fix multilock concurrency when shared by many threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 3, 'created': '2014-09-19 23:05:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/558c34782786fa70cda454c3a1048511cc771efb', 'message': 'Fix multilock concurrency when shared by many threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 4, 'created': '2014-09-19 23:08:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/f0c7be9b4f35d728ae226a9d374d97f306e67de4', 'message': 'Fix multilock concurrency when shared by many threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 5, 'created': '2014-09-19 23:22:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/ac7e89ca02a79ee55fc065714f9826246742cf22', 'message': 'Fix multilock concurrency when shared by many threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 6, 'created': '2014-09-19 23:41:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/54ef2d94c5921fa28708fd6718f0369d49323fe0', 'message': 'Fix multilock concurrency when shared by many threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 7, 'created': '2014-09-19 23:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/0815d3996585a6fe4799de791f357c1d64356abc', 'message': 'Fix multilock concurrency when shared by many threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 8, 'created': '2014-09-20 00:00:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/14a882bff7666e78424636b844885adc09a696f9', 'message': 'Fix multilock concurrency when shared by many threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 9, 'created': '2014-09-20 00:06:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/9ff823f851986823b2945b6af17c84cf2c4a9c8c', 'message': 'Fix multilock concurrency when shared by many threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 10, 'created': '2014-09-20 00:22:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/334aab2eb18ce8cb0ccabaacf2dbb67334515b3c', 'message': 'Fix multilock concurrency when shared by many threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 11, 'created': '2014-09-20 03:25:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/980083ebdbb48a5c1512b26fe12138fe599ec87b', 'message': 'Fix multilock concurrency when shared by > 1 threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nThis makes it so that a single multilock object can be\nshared by many threads and each thread using the object\nwill correctly obtain and release as expected...\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 12, 'created': '2014-09-22 16:22:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/001b75e40408c03f10637d2a58149b6ba6e1d960', 'message': 'Fix multilock concurrency when shared by > 1 threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nThis makes it so that a single multilock object can be\nshared by many threads and each thread using the object\nwill correctly obtain and release as expected...\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 13, 'created': '2014-09-22 18:08:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/d6739cf7c4aba0196767c1597664af961021289d', 'message': 'Fix multilock concurrency when shared by > 1 threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nThis makes it so that a single multilock object can be\nshared by many threads and each thread using the object\nwill correctly obtain and release as expected...\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 14, 'created': '2014-09-22 18:14:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/7c313171d2e5112cbc25a02f949f20a506a471d4', 'message': 'Fix multilock concurrency when shared by > 1 threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nThis makes it so that a single multilock object can be\nshared by many threads and each thread using the object\nwill correctly obtain and release as expected...\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 15, 'created': '2014-09-22 21:37:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/451cd24d08b1f1099494495749254024dffc9b3d', 'message': 'Fix multilock concurrency when shared by > 1 threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nThis makes it so that a single multilock object can be\nshared by many threads and each thread using the object\nwill correctly obtain and release as expected...\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 16, 'created': '2014-09-23 01:51:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/33758b58b620a7ba64b8834cda9cd8967b8b518a', 'message': 'Fix multilock concurrency when shared by > 1 threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nThis makes it so that a single multilock object can be\nshared by many threads and each thread using the object\nwill correctly obtain and release as expected...\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 17, 'created': '2014-09-23 01:54:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/1b4fb77ff4098411d8501699ec89853f341269ef', 'message': 'Fix multilock concurrency when shared by > 1 threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nThis makes it so that a single multilock object can be\nshared by many threads and each thread using the object\nwill correctly obtain and release as expected...\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}, {'number': 18, 'created': '2014-09-23 18:23:11.000000000', 'files': ['taskflow/utils/lock_utils.py', 'taskflow/tests/unit/test_utils_lock_utils.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/83690a20863702c3bcebc042d2edefe7161062a8', 'message': 'Fix multilock concurrency when shared by > 1 threads\n\nInstead of raising thread errors when another thread\nhas locks that the consuming thread wants to use just\nblock and wait and release the correct locks on release\nto match the expected vs observed behavior.\n\nThis makes it so that a single multilock object can be\nshared by many threads and each thread using the object\nwill correctly obtain and release as expected...\n\nFixes bug 1371814\n\nChange-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150\n'}]",12,122887,83690a20863702c3bcebc042d2edefe7161062a8,34,3,18,1297,,,0,"Fix multilock concurrency when shared by > 1 threads

Instead of raising thread errors when another thread
has locks that the consuming thread wants to use just
block and wait and release the correct locks on release
to match the expected vs observed behavior.

This makes it so that a single multilock object can be
shared by many threads and each thread using the object
will correctly obtain and release as expected...

Fixes bug 1371814

Change-Id: Ia21a05fe9249fa019a09c4f30beeb0770ded5150
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/87/122887/17 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/utils/lock_utils.py', 'taskflow/tests/unit/test_utils_lock_utils.py']",2,5a307211bc1114f44ec3a7bb6fa0c267ffa8766e,bug/1371814,"class MultilockTest(test.TestCase): def test_acquired_pass(self): activated = collections.deque() def critical(): start = time.time() time.sleep(0.05) end = time.time() activated.append((start, end)) def run(lock): with lock: critical() lock1 = threading.Lock() lock2 = threading.Lock() lock = lock_utils.MultiLock((lock1, lock2)) threads = [] for _i in range(0, 20): t = threading.Thread(target=run, args=[lock]) t.daemon = True threads.append(t) t.start() while threads: t = threads.pop() t.join() for (start, end) in activated: self.assertEqual(1, _find_overlaps(activated, start, end)) self.assertFalse(lock1.locked()) self.assertFalse(lock2.locked()) def test_acquired_fail(self): activated = collections.deque() lock1 = threading.Lock() lock2 = threading.Lock() def run(lock): with lock: if lock1.locked(): activated.append(1) if lock2.locked(): activated.append(1) def run_fail(lock): try: with lock: raise RuntimeError() except RuntimeError: pass lock = lock_utils.MultiLock((lock1, lock2)) threads = [] for i in range(0, 20): if i % 2 == 1: target = run_fail else: target = run t = threading.Thread(target=target, args=[lock]) threads.append(t) t.daemon = True t.start() while threads: t = threads.pop() t.join() self.assertEqual(20, sum(activated)) self.assertFalse(lock1.locked()) self.assertFalse(lock2.locked()) def test_double_acquire_fail(self): lock1 = threading.Lock() lock2 = threading.Lock() lock = lock_utils.MultiLock((lock1, lock2), double_acquires=False) try: lock.acquire() self.assertRaises(threading.ThreadError, lock.acquire) finally: lock.release() def test_double_release_pass(self): lock1 = threading.Lock() lock2 = threading.Lock() lock = lock_utils.MultiLock((lock1, lock2)) try: lock.acquire() finally: lock.release() lock.release() ",,116,26
openstack%2Fhorizon~master~If876cb5692feb772e5c7ca8fb529033bed9cc5a3,openstack/horizon,master,If876cb5692feb772e5c7ca8fb529033bed9cc5a3,Fix for Data Processing Templates copy,MERGED,2014-08-11 10:21:31.000000000,2014-09-27 20:20:35.000000000,2014-09-27 20:20:34.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 5623}, {'_account_id': 6786}, {'_account_id': 7126}, {'_account_id': 7132}, {'_account_id': 7227}, {'_account_id': 7428}, {'_account_id': 8090}, {'_account_id': 8411}, {'_account_id': 8648}, {'_account_id': 9622}, {'_account_id': 9659}, {'_account_id': 12038}]","[{'number': 1, 'created': '2014-08-11 10:21:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/59ada6c9f44b6a0d8453ac5db61ef1f71c6eaa12', 'message': ""Fix for Data Processing Templates copy\n\nThe Cluster Templates can now be copied even if does not contain\nnode_group_template_id field in it's node groups.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n""}, {'number': 2, 'created': '2014-08-11 10:30:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/d14d4f0dc3e6c2e2e2f26e677f08595e8e67a502', 'message': ""Fix for Data Processing Templates copy\n\nThe Cluster Templates can now be copied even if does not contain\nnode_group_template_id field in it's node groups.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n""}, {'number': 3, 'created': '2014-08-19 10:15:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/4e5cab3c59ece6e87fd8af2e5464a3fae1ca9a0e', 'message': ""Fix for Data Processing Templates copy\n\nThe Cluster Templates can now be copied even if does not contain\nnode_group_template_id field in it's node groups.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n""}, {'number': 4, 'created': '2014-08-22 09:54:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/1c88450730db7a86d4c2403a54bff0b3a4942c9b', 'message': ""Fix for Data Processing Templates copy\n\nThe Cluster Templates can now be copied even if does not contain\nnode_group_template_id field in it's node groups.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n""}, {'number': 5, 'created': '2014-08-22 11:05:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/9ea3559fc4fcf606963c04de173f9dd96cfd9cdb', 'message': ""Fix for Data Processing Templates copy\n\nThe Cluster Templates can now be copied even if does not contain\nnode_group_template_id field in it's node groups.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n""}, {'number': 6, 'created': '2014-08-25 12:50:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/b9569f0053149ea73263cbeab357da212edadc4b', 'message': ""Fix for Data Processing Templates copy\n\nThe Cluster Templates can now be copied even if does not contain\nnode_group_template_id field in it's node groups.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n""}, {'number': 7, 'created': '2014-08-28 10:53:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/49081f2eedd8060aea87ead04ba642dee50e78e5', 'message': ""Fix for Data Processing Templates copy\n\nThe Cluster Templates can now be copied even if does not contain\nnode_group_template_id field in it's node groups.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n""}, {'number': 8, 'created': '2014-08-28 14:14:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/307113407691ac2210bbf4a25979dbaf6db868c2', 'message': ""Fix for Data Processing Templates copy\n\nThe Cluster Templates can now be copied even if does not contain\nnode_group_template_id field in it's node groups.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n""}, {'number': 9, 'created': '2014-09-01 10:10:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/dc991e9abcfe5b832bd62ff5d8161b09f26cdf5a', 'message': ""Fix for Data Processing Templates copy\n\nThe Cluster Templates can now be copied even if does not contain\nnode_group_template_id field in it's node groups.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n""}, {'number': 10, 'created': '2014-09-02 11:55:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/0df4684b47796d47432f9643700a6d9ee10da711', 'message': ""Fix for Data Processing Templates copy\n\nThe Cluster Templates can now be copied even if does not contain\nnode_group_template_id field in it's node groups.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n""}, {'number': 11, 'created': '2014-09-02 12:02:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/afd6336d3f03f5acc0ddde6b3679b024f9c404d8', 'message': 'Fix for Data Processing Templates copy\n\nThe Cluster Template can now be copied even if it does not contain\nthe node_group_template_id field.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n'}, {'number': 12, 'created': '2014-09-03 13:06:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/7f7b98fc3285a5730c898f7d1c63d968956b7f64', 'message': 'Fix for Data Processing Templates copy\n\nThe Cluster Template can now be copied even if it does not contain\nthe node_group_template_id field.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n'}, {'number': 13, 'created': '2014-09-03 13:14:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/e056cb704bd13d1ac2268826d649fa6f8aa6a214', 'message': 'Fix for Data Processing Templates create and copy\n\nThe Cluster Template can now be copied even if it does not contain\nthe node_group_template_id field.\nThe create workflow does not lose node groups anymore.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\nCloses-Bug: #1364648\n'}, {'number': 14, 'created': '2014-09-09 10:47:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/954ac48a17c706c60ba3ccb7f3a4df6c5e37f7ef', 'message': 'Fix for Data Processing Templates copy\n\nThe Cluster Template can now be copied even if it does not contain\nthe node_group_template_id field.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n'}, {'number': 15, 'created': '2014-09-19 10:32:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/7f448eca79f4eb0e435962486915d0a6032f03a0', 'message': 'Fix for Data Processing Templates copy[WIP]\n\nThe Cluster Template can now be copied even if it does not contain\nthe node_group_template_id field.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n'}, {'number': 16, 'created': '2014-09-19 10:41:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/e903d6e25d78ce1d3dec41b218cc0bb8f3af159c', 'message': 'Fix for Data Processing Templates copy\n\nThe Cluster Template can now be copied even if it does not contain\nthe node_group_template_id field.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n'}, {'number': 17, 'created': '2014-09-19 12:26:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/5329180b60181ca90e82fae800621a232bf0164d', 'message': 'Fix for Data Processing Templates copy\n\nThe Cluster Template can now be copied even if it does not contain\nthe node_group_template_id field.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n'}, {'number': 18, 'created': '2014-09-22 11:47:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/fabd768cddccffccdb66b71c0f1132dfe9e31310', 'message': 'Fix for Data Processing Templates copy\n\nThe Cluster Template can now be copied even if it does not contain\nthe node_group_template_id field.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n'}, {'number': 19, 'created': '2014-09-23 09:52:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/24d94e7db040363b4a46ef0b45e25831cd175e8b', 'message': 'Fix for Data Processing Templates copy\n\nThe Cluster Template can now be copied even if it does not contain\nthe node_group_template_id field.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n'}, {'number': 20, 'created': '2014-09-23 13:52:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/91392e67d7963697dff26b3ef31ea45fa1e12d7b', 'message': 'Fix for Data Processing Templates copy\n\nThe Cluster Template can now be copied even if it does not contain\nthe node_group_template_id field.\n\nUtils refactored to use Sahara client correctly.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n'}, {'number': 21, 'created': '2014-09-25 07:55:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/050702d9458cb63cae91d980c374e0c7717ca56e', 'message': 'Fix for Data Processing Templates copy\n\nThe Cluster Template can now be copied even if it does not contain\nthe node_group_template_id field.\n\nUtils refactored to use Sahara client correctly.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n'}, {'number': 22, 'created': '2014-09-25 12:25:50.000000000', 'files': ['openstack_dashboard/dashboards/project/data_processing/utils/workflow_helpers.py', 'openstack_dashboard/dashboards/project/data_processing/cluster_templates/templates/data_processing.cluster_templates/cluster_node_groups_template.html', 'openstack_dashboard/dashboards/project/data_processing/nodegroup_templates/workflows/create.py', 'openstack_dashboard/dashboards/project/data_processing/cluster_templates/workflows/create.py', 'openstack_dashboard/dashboards/project/data_processing/clusters/workflows/scale.py', 'openstack_dashboard/dashboards/project/data_processing/cluster_templates/tests.py', 'openstack_dashboard/dashboards/project/data_processing/cluster_templates/workflows/copy.py', 'openstack_dashboard/test/test_data/sahara_data.py', 'openstack_dashboard/dashboards/project/data_processing/utils/helpers.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/90f8be4eb46041223f4818dcc765c94b33eabcef', 'message': 'Fix for Data Processing Templates copy\n\nThe Cluster Template can now be copied even if it does not contain\nthe node_group_template_id field.\n\nUtils refactored to use Sahara client correctly.\n\nChange-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3\nCloses-Bug: #1349807\n'}]",8,113213,90f8be4eb46041223f4818dcc765c94b33eabcef,103,14,22,7132,,,0,"Fix for Data Processing Templates copy

The Cluster Template can now be copied even if it does not contain
the node_group_template_id field.

Utils refactored to use Sahara client correctly.

Change-Id: If876cb5692feb772e5c7ca8fb529033bed9cc5a3
Closes-Bug: #1349807
",git fetch https://review.opendev.org/openstack/horizon refs/changes/13/113213/21 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/data_processing/utils/workflow_helpers.py', 'openstack_dashboard/dashboards/project/data_processing/cluster_templates/templates/data_processing.cluster_templates/cluster_node_groups_template.html', 'openstack_dashboard/dashboards/project/data_processing/cluster_templates/workflows/create.py', 'openstack_dashboard/dashboards/project/data_processing/clusters/workflows/scale.py', 'openstack_dashboard/dashboards/project/data_processing/cluster_templates/workflows/copy.py']",5,59ada6c9f44b6a0d8453ac5db61ef1f71c6eaa12,bug/1349807,"import base64 import json serialized = ""serialized_%d"" % i # save the original node group with all it's fields in # case the template id is missing serialized_val = base64.urlsafe_b64encode( json.dumps(wf_helpers.clean_ng(templ_ng))) ng = { ""serialized"": serialized_val } if ""node_group_template_id"" in templ_ng: ng[""template_id""] = templ_ng[ ""node_group_template_id""] ng_action.groups.append(ng) ng_action, group_name, template_id, count, serialized)"," ng_action.groups.append({ ""template_id"": templ_ng[""node_group_template_id""], }) ng_action, group_name, template_id, count)",64,14
openstack%2Ftaskflow~master~I0545683f83402097f54c34a6b737904e6edd85b3,openstack/taskflow,master,I0545683f83402097f54c34a6b737904e6edd85b3,Ensure the cachedproperty creation/setting is thread-safe,MERGED,2014-09-05 18:52:04.000000000,2014-09-27 20:20:27.000000000,2014-09-27 20:20:26.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 6648}, {'_account_id': 11024}]","[{'number': 1, 'created': '2014-09-05 18:52:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/881be09155d0fea8f4df62df7eb17cd9da7bc824', 'message': 'Ensure the cachedproperty attribute creation/setting is thread-safe\n\nWhen the cachedproperty descriptor is attached to an object that\nneeds to be only created/set by one thread at a time we should ensure\nthat this is done safely.\n\nFixes bug 1366156\n\nChange-Id: I0545683f83402097f54c34a6b737904e6edd85b3\n'}, {'number': 2, 'created': '2014-09-05 18:53:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/767af9e57fd95a47f7f47dd61e7c4c5d745b8226', 'message': 'Ensure the cachedproperty creation/setting is thread-safe\n\nWhen the cachedproperty descriptor is attached to an object\nthat needs to be only created/set by one thread at a time we\nshould ensure that this is done safely by using a lock to\nprevent multiple threads from creating and assigning the\nassociated attribute.\n\nFixes bug 1366156\n\nChange-Id: I0545683f83402097f54c34a6b737904e6edd85b3\n'}, {'number': 3, 'created': '2014-09-05 19:02:32.000000000', 'files': ['taskflow/tests/unit/test_utils.py', 'taskflow/utils/misc.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/a96f49b9a5c73985187e75ba4eba6b946875c92a', 'message': 'Ensure the cachedproperty creation/setting is thread-safe\n\nWhen the cachedproperty descriptor is attached to an object\nthat needs to be only created/set by one thread at a time we\nshould ensure that this is done safely by using a lock to\nprevent multiple threads from creating and assigning the\nassociated attribute.\n\nFixes bug 1366156\n\nChange-Id: I0545683f83402097f54c34a6b737904e6edd85b3\n'}]",0,119443,a96f49b9a5c73985187e75ba4eba6b946875c92a,12,4,3,1297,,,0,"Ensure the cachedproperty creation/setting is thread-safe

When the cachedproperty descriptor is attached to an object
that needs to be only created/set by one thread at a time we
should ensure that this is done safely by using a lock to
prevent multiple threads from creating and assigning the
associated attribute.

Fixes bug 1366156

Change-Id: I0545683f83402097f54c34a6b737904e6edd85b3
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/43/119443/1 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/tests/unit/test_utils.py', 'taskflow/utils/misc.py']",2,881be09155d0fea8f4df62df7eb17cd9da7bc824,bug/1366156,"import threading """"""A *thread-safe* descriptor property that is only evaluated once. self._lock = threading.RLock() with self._lock: try: return getattr(instance, self._attr_name) except AttributeError: value = self._fget(instance) setattr(instance, self._attr_name, value) return value"," """"""A descriptor property that is only evaluated once.. try: return getattr(instance, self._attr_name) except AttributeError: value = self._fget(instance) setattr(instance, self._attr_name, value) return value",43,7
openstack%2Ftaskflow~master~I05151c3f827d6a8a0b6a7f0aeab17bab2c8ce440,openstack/taskflow,master,I05151c3f827d6a8a0b6a7f0aeab17bab2c8ce440,Example which shows how to move values from one task to another,MERGED,2014-09-10 18:36:39.000000000,2014-09-27 20:12:40.000000000,2014-09-27 20:12:40.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 6648}, {'_account_id': 8759}, {'_account_id': 11024}]","[{'number': 1, 'created': '2014-09-10 18:36:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/22b7992614e50a6074ed2a627bbe72dffbe21c8a', 'message': 'Add a example that shows how to pass values from one task to another\n\nChange-Id: I05151c3f827d6a8a0b6a7f0aeab17bab2c8ce440\n'}, {'number': 2, 'created': '2014-09-10 18:37:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/2ce706b91719378c316909ad669fb5fb71095b33', 'message': 'Example which shows how to move values from one task to another\n\nChange-Id: I05151c3f827d6a8a0b6a7f0aeab17bab2c8ce440\n'}, {'number': 3, 'created': '2014-09-10 18:38:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/689a1e32c511e7713fd4fe95d43ff1ba1e7ae052', 'message': 'Example which shows how to move values from one task to another\n\nChange-Id: I05151c3f827d6a8a0b6a7f0aeab17bab2c8ce440\n'}, {'number': 4, 'created': '2014-09-12 22:03:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/7dd7bec6742a170c89ba939a56555b1052a24215', 'message': 'Example which shows how to move values from one task to another\n\nPart of blueprint more-examples\n\nChange-Id: I05151c3f827d6a8a0b6a7f0aeab17bab2c8ce440\n'}, {'number': 5, 'created': '2014-09-13 01:20:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/38403d608d1d20e72f35b81dc88b2f37296e3bb5', 'message': 'Example which shows how to move values from one task to another\n\nPart of blueprint more-examples\n\nChange-Id: I05151c3f827d6a8a0b6a7f0aeab17bab2c8ce440\n'}, {'number': 6, 'created': '2014-09-13 01:22:31.000000000', 'files': ['doc/source/examples.rst', 'taskflow/examples/simple_linear_pass.py', 'taskflow/examples/simple_linear_pass.out.txt'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/452652431a32ef4bc6dbb549efe858b0c3e71466', 'message': 'Example which shows how to move values from one task to another\n\nPart of blueprint more-examples\n\nChange-Id: I05151c3f827d6a8a0b6a7f0aeab17bab2c8ce440\n'}]",0,120515,452652431a32ef4bc6dbb549efe858b0c3e71466,19,5,6,1297,,,0,"Example which shows how to move values from one task to another

Part of blueprint more-examples

Change-Id: I05151c3f827d6a8a0b6a7f0aeab17bab2c8ce440
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/15/120515/6 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/examples.rst', 'taskflow/examples/simple_linear_pass.py', 'taskflow/examples/simple_linear_pass.out.txt']",3,22b7992614e50a6074ed2a627bbe72dffbe21c8a,bp/more-examples,Constructing... Loading... Running... Executing 'a' Executing 'b' Got input 'a' Done... ,,80,0
openstack%2Ftaskflow~master~Idaf04fb1a6a622af292511bbcf25329c9a5aab53,openstack/taskflow,master,Idaf04fb1a6a622af292511bbcf25329c9a5aab53,Mention issue with more than one thread and reduce workers,MERGED,2014-08-29 09:11:18.000000000,2014-09-27 20:08:13.000000000,2014-09-27 20:08:12.000000000,"[{'_account_id': 3}, {'_account_id': 6648}, {'_account_id': 9608}]","[{'number': 1, 'created': '2014-08-29 09:11:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/7cdc2b76c180bc76f5e3412acde379e9d86cb42c', 'message': 'Mention issue with more than one thread and reduce workers\n\nDue to how it appears the filesystem transport and the memory\ntransport in kombu are not thread-safe we will work around this\nby not having more than one worker active at the same time in\nthis example.\n\nChange-Id: Idaf04fb1a6a622af292511bbcf25329c9a5aab53\n'}, {'number': 2, 'created': '2014-08-29 09:15:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/b8c4cababdee42737513740a1ba7cc340931ee23', 'message': ""Mention issue with more than one thread and reduce workers\n\nDue to how it appears the filesystem transport in kombu is\nnot thread-safe we will work around this by not having more\nthan one worker active at the same time in this example.\n\nOddly it appears the memory transport is unaffected (but\nfrom looking at the code it doesn't look safe either), this\nmay just be due to how the python memory model works though.\n\nChange-Id: Idaf04fb1a6a622af292511bbcf25329c9a5aab53\n""}, {'number': 3, 'created': '2014-08-29 17:45:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/74dfa6a90721e71780961aaac19bf33ede30db15', 'message': ""Mention issue with more than one thread and reduce workers\n\nDue to how it appears the filesystem transport in kombu is\nnot thread-safe we will work around this by not having more\nthan one worker active at the same time in this example.\n\nOddly it appears the memory transport is unaffected (but\nfrom looking at the code it doesn't look safe either), this\nmay just be due to how the python memory model works though.\n\nChange-Id: Idaf04fb1a6a622af292511bbcf25329c9a5aab53\n""}, {'number': 4, 'created': '2014-08-31 00:32:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/3f3d7ac2d70856cf9a3e79ad67dae8f86d4e4651', 'message': ""Mention issue with more than one thread and reduce workers\n\nDue to how it appears the filesystem transport in kombu is\nnot thread-safe we will work around this by not having more\nthan one worker active at the same time in this example.\n\nOddly it appears the memory transport is unaffected (but\nfrom looking at the code it doesn't look safe either), this\nmay just be due to how the python memory model works though.\n\nChange-Id: Idaf04fb1a6a622af292511bbcf25329c9a5aab53\n""}, {'number': 5, 'created': '2014-09-12 22:04:09.000000000', 'files': ['taskflow/examples/wbe_simple_linear.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/f8fbb30412edc41a1df05c7938db9dc973357f5c', 'message': ""Mention issue with more than one thread and reduce workers\n\nDue to how it appears the filesystem transport in kombu is\nnot thread-safe we will work around this by not having more\nthan one worker active at the same time in this example.\n\nOddly it appears the memory transport is unaffected (but\nfrom looking at the code it doesn't look safe either), this\nmay just be due to how the python memory model works though.\n\nPart of blueprint more-examples\n\nChange-Id: Idaf04fb1a6a622af292511bbcf25329c9a5aab53\n""}]",0,117729,f8fbb30412edc41a1df05c7938db9dc973357f5c,16,3,5,1297,,,0,"Mention issue with more than one thread and reduce workers

Due to how it appears the filesystem transport in kombu is
not thread-safe we will work around this by not having more
than one worker active at the same time in this example.

Oddly it appears the memory transport is unaffected (but
from looking at the code it doesn't look safe either), this
may just be due to how the python memory model works though.

Part of blueprint more-examples

Change-Id: Idaf04fb1a6a622af292511bbcf25329c9a5aab53
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/29/117729/4 && git format-patch -1 --stdout FETCH_HEAD,['taskflow/examples/wbe_simple_linear.py'],1,7cdc2b76c180bc76f5e3412acde379e9d86cb42c,bp/more-examples, # Until https://github.com/celery/kombu/issues/398 is resolved it is not # recommended to run many worker threads in this example due to the types # of errors mentioned in that issue. WORKERS = 1,WORKERS = 2,5,1
openstack%2Ftrove~master~I3ea1bd255514438c3d356d23d42f3fb3ddd45274,openstack/trove,master,I3ea1bd255514438c3d356d23d42f3fb3ddd45274,Document Trove configuration options,MERGED,2014-09-03 20:12:10.000000000,2014-09-27 20:08:08.000000000,2014-09-27 20:08:07.000000000,"[{'_account_id': 3}, {'_account_id': 5293}, {'_account_id': 7092}, {'_account_id': 8415}, {'_account_id': 8871}, {'_account_id': 9664}, {'_account_id': 10215}, {'_account_id': 10295}]","[{'number': 1, 'created': '2014-09-03 20:12:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/c1407473fdc119ce29b3f150fc59bf0310422f39', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: http://docs.openstack.org/trunk/config-reference/content/\n                database-configuring-db.html\n\nAll options now have help strings to clarify how/when they can be used.\n\nNote: This is a first cut of the changes, pushed up to facilitate\nviewing the diffs.  These lines will eventually be removed.\n\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\nCloses-Bug: #1334465\n'}, {'number': 2, 'created': '2014-09-03 20:38:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/6dbb8e814156c3eafbcfeff9c4c1322599e8bc97', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: http://docs.openstack.org/trunk/config-reference/content/\n                database-configuring-db.html\n\nAll options now have help strings to clarify how/when they can be used.\n\nCloses-Bug: #1334465\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\n'}, {'number': 3, 'created': '2014-09-03 20:44:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/f47325e8ca64decd84ed8947ac141c0d790ee449', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: http://docs.openstack.org/trunk/config-reference/content/\n                database-configuring-db.html\n\nAll options now have help strings to clarify how/when they can be used.\n\nDocImpact: A separate patch set will be created for the actual doc\nchanges.\n\nCloses-Bug: #1334465\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\n'}, {'number': 4, 'created': '2014-09-04 14:06:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/52aaa386002fdbd89c04ede8de9c65f15e5d9c15', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: http://docs.openstack.org/trunk/config-reference/content/\n                database-configuring-db.html\n\nAll options now have help strings to clarify how/when they can be used.\n\nDocImpact: A separate patch set will be created for the actual doc\nchanges.\n\nCloses-Bug: #1334465\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\n'}, {'number': 5, 'created': '2014-09-08 14:19:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/a7b06ba41ae05ba2e20be8b64d4e95983c6ecee2', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: http://docs.openstack.org/trunk/config-reference/content/\n                database-configuring-db.html\n\nAll options now have help strings to clarify how/when they can be used.\n\nDocImpact: A separate patch set will be created for the actual doc\nchanges.\n\nCloses-Bug: #1334465\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\n'}, {'number': 6, 'created': '2014-09-10 21:07:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/ce4c5ebb7ff6f1206ec6abdfbe0e855b068cb87d', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: http://docs.openstack.org/trunk/config-reference/content/\n                database-configuring-db.html\n\nAll options now have help strings to clarify how/when they can be used.\n\nDocImpact: A separate patch set will be created for the actual doc\nchanges.\n\nCloses-Bug: #1334465\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\n'}, {'number': 7, 'created': '2014-09-11 01:44:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/e57aab8f923060c1bf5153430bb88e3fc00431c6', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: http://docs.openstack.org/trunk/config-reference/content/\n                database-configuring-db.html\n\nAll options now have help strings to clarify how/when they can be used.\n\nDocImpact: A separate patch set will be created for the actual doc\nchanges.\n\nCloses-Bug: #1334465\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\n'}, {'number': 8, 'created': '2014-09-11 14:40:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/b5525822b4fd295409c954d6e53dd038812195a1', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: http://docs.openstack.org/trunk/config-reference/content/\n                database-configuring-db.html\n\nAll options now have help strings to clarify how/when they can be used.\n\nDocImpact: A separate patch set will be created for the actual doc\nchanges.\n\nCloses-Bug: #1334465\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\n'}, {'number': 9, 'created': '2014-09-17 18:32:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/811d5232b257b82b0854f2d200f87a861457d436', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: http://docs.openstack.org/trunk/config-reference/content/\n                database-configuring-db.html\n\nAll options now have help strings to clarify how/when they can be used.\n\nAlso removed guest_id option (defined elsewhere) and periodic_interval\noption (not referenced in the code).\n\nDocImpact: A separate patch set will be created for the actual doc\nchanges.\n\nCloses-Bug: #1334465\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\n'}, {'number': 10, 'created': '2014-09-18 16:29:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/51ff2a0779c0b06cbfcd06ff16a96260eb6bf115', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: http://docs.openstack.org/trunk/config-reference/content/\n                database-configuring-db.html\n\nAll options now have help strings to clarify how/when they can be used.\n\nAlso removed periodic_interval option (not referenced in the code).\n\nDocImpact: A separate patch set will be created for the actual doc\nchanges.\n\nCloses-Bug: #1334465\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\n'}, {'number': 11, 'created': '2014-09-23 19:11:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/0060369d7c6e11e31bcbce1eeacf7bf618546d0c', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: http://docs.openstack.org/trunk/config-reference/content/\n                database-configuring-db.html\n\nAll options now have help strings to clarify how/when they can be used.\n\nAlso removed periodic_interval option (not referenced in the code).\n\nDocImpact: A separate patch set will be created for the actual doc\nchanges.\n\nCloses-Bug: #1334465\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\n'}, {'number': 12, 'created': '2014-09-25 16:21:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/e98a159f0e4f19b665a21f1436a15f66c278bb05', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: http://docs.openstack.org/trunk/config-reference/content/\n                database-configuring-db.html\n\nAll options now have help strings to clarify how/when they can be used.\n\nAlso removed periodic_interval option (not referenced in the code).\n\nDocImpact: A separate patch set will be created for the actual doc\nchanges.\n\nCloses-Bug: #1334465\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\n'}, {'number': 13, 'created': '2014-09-25 16:57:05.000000000', 'files': ['trove/common/cfg.py', 'trove/cmd/guest.py'], 'web_link': 'https://opendev.org/openstack/trove/commit/e6b6973a05b08687cec493a233787de4802c91ec', 'message': 'Document Trove configuration options\n\nMany of the Trove configuration options do not have adequate help text\nto describe what the configuration option can be used for, and/or what\nvalid values for the option are.\n\nSince the auto-generated docs are based on this help text, they\ncurrently look very empty and are not very helpful.\n\nexample: See ""Configure the database"" in chapter 4. Database Service\n         under http://docs.openstack.org/trunk/config-reference/content\n\nAll options now have help strings to clarify how/when they can be used.\n\nAlso removed periodic_interval option (not referenced in the code).\n\nDocImpact: A separate patch set will be created for the actual doc\nchanges.\n\nCloses-Bug: #1334465\nChange-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274\n'}]",29,118759,e6b6973a05b08687cec493a233787de4802c91ec,128,8,13,10215,,,0,"Document Trove configuration options

Many of the Trove configuration options do not have adequate help text
to describe what the configuration option can be used for, and/or what
valid values for the option are.

Since the auto-generated docs are based on this help text, they
currently look very empty and are not very helpful.

example: See ""Configure the database"" in chapter 4. Database Service
         under http://docs.openstack.org/trunk/config-reference/content

All options now have help strings to clarify how/when they can be used.

Also removed periodic_interval option (not referenced in the code).

DocImpact: A separate patch set will be created for the actual doc
changes.

Closes-Bug: #1334465
Change-Id: I3ea1bd255514438c3d356d23d42f3fb3ddd45274
",git fetch https://review.opendev.org/openstack/trove refs/changes/59/118759/12 && git format-patch -1 --stdout FETCH_HEAD,['trove/common/cfg.py'],1,c1407473fdc119ce29b3f150fc59bf0310422f39,bug/1334465," cfg.IntOpt('sql_idle_timeout', default=3600, help='Idle time after which a database reconnect is ' 'performed.'), cfg.BoolOpt('sql_query_log', default=False, help='Write all SQL queries to a log.'), help='The IP address the API server will listen on.'), cfg.IntOpt('bind_port', default=8779, help='Port the API server will listen on.'), cfg.StrOpt('api_extensions_path', default='$pybasedir/extensions/routes', help='Path to extensions.'), cfg.ListOpt('admin_roles', default=['admin'], help='Roles to add to an admin user.'), cfg.StrOpt('trove_auth_url', default='http://0.0.0.0:5000/v2.0', help='Trove authentication URL.'), cfg.StrOpt('host', default='0.0.0.0', help='Host to listen for RPC messages.'), cfg.BoolOpt('trove_dns_support', default=False, help='Whether Trove should add DNS entries on create ' '(using Designate DNSaaS).'), cfg.StrOpt('db_api_implementation', default='trove.db.sqlalchemy.api', help='API Implementation for Trove database access.'), cfg.StrOpt('dns_driver', default='trove.dns.driver.DnsDriver', help='Driver for DNSaaS.'), default='trove.dns.driver.DnsInstanceEntryFactory', help='Factory for adding DNS entries.'), cfg.StrOpt('dns_hostname', default="""", help='Hostname used for adding DNS entries.'), cfg.StrOpt('dns_account_id', default="""", help='Tenant ID for DNSaaS.'), cfg.StrOpt('dns_endpoint_url', default=""0.0.0.0"", help='Endpoint URL for DNSaaS.'), cfg.StrOpt('dns_service_type', default="""", help='Service Type for DNSaaS.'), cfg.StrOpt('dns_region', default="""", help='Region name for DNSaaS.'), cfg.StrOpt('dns_auth_url', default="""", help='Authentication URL for DNSaaS.'), cfg.StrOpt('dns_domain_name', default="""", help='Domain name used for adding DNS entries.'), cfg.StrOpt('dns_username', default="""", secret=True, help='Username for DNSaaS.'), cfg.StrOpt('dns_passkey', default="""", secret=True, help='Passkey for DNSaaS.'), cfg.StrOpt('dns_management_base_url', default="""", help='Management URL for DNSaaS.'), cfg.IntOpt('dns_ttl', default=300, help='Time in seconds before a refresh of DNS information ' 'occurs.'), cfg.StrOpt('dns_domain_id', default="""", help='Domain ID used for adding DNS entries.'), cfg.IntOpt('users_page_size', default=20, help='Page size for listing users.'), cfg.IntOpt('databases_page_size', default=20, help='Page size for listing databases.'), cfg.IntOpt('instances_page_size', default=20, help='Page size for listing instances.'), cfg.IntOpt('clusters_page_size', default=20, help='Page size for listing clusters.'), cfg.IntOpt('backups_page_size', default=20, help='Page size for listing backups.'), cfg.IntOpt('configurations_page_size', default=20, help='Page size for listing configurations.'), cfg.ListOpt('ignore_users', default=['os_admin', 'root'], help='Users to exclude from list.'), cfg.ListOpt('ignore_dbs', default=['lost+found', 'mysql', 'information_schema'], help='Databases to exclude from list.'), cfg.IntOpt('agent_call_low_timeout', default=5, help=""Maximum time to wait for Guest Agent 'quick' requests""), cfg.IntOpt('agent_call_high_timeout', default=60, help=""Maximum time to wait for Guest Agent 'slow' requests""), cfg.StrOpt('guest_id', default=None, help='Instance ID of the Guest.'), cfg.IntOpt('state_change_wait_time', default=3 * 60, help=""Maximum time to wait for a state change""), cfg.IntOpt('agent_heartbeat_time', default=10, help='Maximum time for the Guest Agent to reply to a ' 'heartbeat request.'), cfg.IntOpt('num_tries', default=3, help='Number of times to check if a volume exists.'), cfg.StrOpt('volume_fstype', default='ext3', help='File system type used to format a volume.'), cfg.StrOpt('format_options', default='-m 5', help='Options to use when formatting a volume.'), cfg.IntOpt('volume_format_timeout', default=120, help='Maximum time to wait to format a volume.'), cfg.StrOpt('mount_options', default='defaults,noatime', help='Options to use when mounting a volume.'), cfg.StrOpt('taskmanager_queue', default='taskmanager', help='Message queue name the Taskmanager will listen to.'), cfg.StrOpt('conductor_queue', default='trove-conductor', help='Message queue name the Conductor will listen to.'), cfg.BoolOpt('use_nova_server_volume', default=False, help='Whether to provision a cinder volume for the ' 'Nova instance.'), cfg.BoolOpt('use_heat', default=False, help='Use Heat for provisioning.'), cfg.StrOpt('device_path', default='/dev/vdb', help='Device path for volume if volume support is enabled.'), cfg.StrOpt('block_device_mapping', default='vdb', help='Block device to map onto the created instance.'), cfg.IntOpt('server_delete_time_out', default=60, help='Maximum time to wait to delete a server.'), cfg.IntOpt('volume_time_out', default=60, help='Maximum time to wait to attach a volume.'), cfg.IntOpt('heat_time_out', default=60, help='Maximum time to wait for a Heat request to complete.'), cfg.IntOpt('reboot_time_out', default=60 * 2, help='Maximum time to wait to reboot a server.'), cfg.IntOpt('dns_time_out', default=60 * 2, help='Maximum time to wait add a DNS entry.'), cfg.IntOpt('resize_time_out', default=60 * 10, help='Maximum time to wait to resize a server.'), cfg.IntOpt('revert_time_out', default=60 * 10, help='Maximum time to wait to revert a server resize.'), help='Maximum time to wait to delete a cluster.'), cfg.ListOpt('root_grant', default=['ALL'], help=""Permissions to grant to the 'root' user.""), cfg.BoolOpt('root_grant_option', default=True, help=""Assign the 'root' user GRANT permissions""), cfg.IntOpt('default_password_length', default=36, help='Character length of generated passwords.'), cfg.IntOpt('http_get_rate', default=200, help=""Maximum number of HTTP 'GET' requests per minute.""), cfg.IntOpt('http_post_rate', default=200, help=""Maximum number of HTTP 'POST' requests per minute.""), cfg.IntOpt('http_delete_rate', default=200, help=""Maximum number of HTTP 'DELETE' requests per minute.""), cfg.IntOpt('http_put_rate', default=200, help=""Maximum number of HTTP 'PUT' requests per minute.""), cfg.IntOpt('http_mgmt_post_rate', default=200, help=""Maximum number of management HTTP 'POST' requests "" ""per minute.""), cfg.BoolOpt('trove_security_groups_support', default=True, help='Whether Trove should add Security Groups on create.'), cfg.StrOpt('trove_security_group_name_prefix', default='SecGroup', help='Prefix to use when creating Security Groups.'), cfg.StrOpt('trove_security_group_rule_cidr', default='0.0.0.0/0', help='CIDR to use when creating Security Group Rules.'), default='trove.guestagent.backup.backup_types.InnoBackupEx', help='Runner to use for backups.'), cfg.StrOpt('backup_swift_container', default='database_backups', help='Swift container to put backups in.'), default='trove.common.remote.dns_client', help='Client to send DNS calls to.'), default='trove.common.remote.guest_client', help='Client to send Guest Agent calls to.'), default='trove.common.remote.nova_client', help='Client to send Nova calls to.'), default='trove.common.remote.neutron_client', help='Client to send Neutron calls to.'), default='trove.common.remote.cinder_client', help='Client to send Cinder calls to.'), default='trove.common.remote.heat_client', help='Client to send Heat calls to.'), default='trove.common.remote.swift_client', help='Client to send Swift calls to.'), cfg.StrOpt('network_label_regex', default='^private$', help='Regular expression to match Trove network labels.'), cfg.StrOpt('ip_regex', default=None, help='List IP addresses that match this regular expression.'), cfg.StrOpt('black_list_regex', default=None, help='Exclude IP addresses that match this regular ' 'expression.'), default='trove.guestagent.strategies.backup.mysql_impl', help='Namespace to load backup strategies from.'), default='trove.guestagent.strategies.restore.mysql_impl', help='Namespace to load restore strategies from.'), cfg.StrOpt('device_path', default='/dev/vdb', help='Device path for volume if volume support is enabled.'), default='trove.guestagent.strategies.backup.mysql_impl', help='Namespace to load backup strategies from.'), default='trove.guestagent.strategies.restore.mysql_impl', help='Namespace to load restore strategies from.'), cfg.StrOpt('device_path', default='/dev/vdb', help='Device path for volume if volume support is enabled.'), cfg.StrOpt('device_path', default=None, help='Device path for volume if volume support is enabled.'), cfg.StrOpt('device_path', default='/dev/vdb', help='Device path for volume if volume support is enabled.'), default='trove.guestagent.strategies.backup.couchbase_impl', help='Namespace to load backup strategies from.'), default='trove.guestagent.strategies.restore.couchbase_impl', help='Namespace to load restore strategies from.'), cfg.StrOpt('device_path', default='/dev/vdb', help='Device path for volume if volume support is enabled.'), cfg.StrOpt('device_path', default='/dev/vdb', help='Device path for volume if volume support is enabled.'),"," cfg.IntOpt('sql_idle_timeout', default=3600), cfg.BoolOpt('sql_query_log', default=False), help='The IP address the API will listen on.'), cfg.IntOpt('bind_port', default=8779), cfg.ListOpt('admin_roles', default=['admin']), cfg.StrOpt('trove_auth_url', default='http://0.0.0.0:5000/v2.0'), cfg.StrOpt('host', default='0.0.0.0'), cfg.IntOpt('periodic_interval', default=60), cfg.BoolOpt('trove_dns_support', default=False), cfg.StrOpt('db_api_implementation', default='trove.db.sqlalchemy.api'), cfg.StrOpt('dns_driver', default='trove.dns.driver.DnsDriver'), default='trove.dns.driver.DnsInstanceEntryFactory'), cfg.StrOpt('dns_hostname', default=""""), cfg.StrOpt('dns_account_id', default=""""), cfg.StrOpt('dns_endpoint_url', default=""0.0.0.0""), cfg.StrOpt('dns_service_type', default=""""), cfg.StrOpt('dns_region', default=""""), cfg.StrOpt('dns_auth_url', default=""""), cfg.StrOpt('dns_domain_name', default=""""), cfg.StrOpt('dns_username', default="""", secret=True), cfg.StrOpt('dns_passkey', default="""", secret=True), cfg.StrOpt('dns_management_base_url', default=""""), cfg.IntOpt('dns_ttl', default=300), cfg.StrOpt('dns_domain_id', default=""""), cfg.IntOpt('users_page_size', default=20), cfg.IntOpt('databases_page_size', default=20), cfg.IntOpt('instances_page_size', default=20), cfg.IntOpt('clusters_page_size', default=20), cfg.IntOpt('backups_page_size', default=20), cfg.IntOpt('configurations_page_size', default=20), cfg.ListOpt('ignore_users', default=['os_admin', 'root']), cfg.ListOpt('ignore_dbs', default=['lost+found', 'mysql', 'information_schema']), cfg.IntOpt('agent_call_low_timeout', default=5), cfg.IntOpt('agent_call_high_timeout', default=1000), cfg.StrOpt('guest_id', default=None), cfg.IntOpt('state_change_wait_time', default=3 * 60), cfg.IntOpt('agent_heartbeat_time', default=10), cfg.IntOpt('num_tries', default=3), cfg.StrOpt('volume_fstype', default='ext3'), cfg.StrOpt('format_options', default='-m 5'), cfg.IntOpt('volume_format_timeout', default=120), cfg.StrOpt('mount_options', default='defaults,noatime'), cfg.StrOpt('taskmanager_queue', default='taskmanager'), cfg.StrOpt('conductor_queue', default='trove-conductor'), cfg.BoolOpt('use_nova_server_volume', default=False), cfg.BoolOpt('use_heat', default=False), cfg.StrOpt('device_path', default='/dev/vdb'), cfg.StrOpt('block_device_mapping', default='vdb'), cfg.IntOpt('server_delete_time_out', default=60), cfg.IntOpt('volume_time_out', default=60), cfg.IntOpt('heat_time_out', default=60), cfg.IntOpt('reboot_time_out', default=60 * 2), cfg.IntOpt('dns_time_out', default=60 * 2), cfg.IntOpt('resize_time_out', default=60 * 10), cfg.IntOpt('revert_time_out', default=60 * 10), cfg.ListOpt('root_grant', default=['ALL']), cfg.BoolOpt('root_grant_option', default=True), cfg.IntOpt('default_password_length', default=36), cfg.IntOpt('http_get_rate', default=200), cfg.IntOpt('http_post_rate', default=200), cfg.IntOpt('http_delete_rate', default=200), cfg.IntOpt('http_put_rate', default=200), cfg.IntOpt('http_mgmt_post_rate', default=200), cfg.BoolOpt('trove_security_groups_support', default=True), cfg.StrOpt('trove_security_group_name_prefix', default='SecGroup'), cfg.StrOpt('trove_security_group_rule_cidr', default='0.0.0.0/0'), default='trove.guestagent.backup.backup_types.InnoBackupEx'), cfg.StrOpt('backup_swift_container', default='database_backups'), default='trove.common.remote.dns_client'), default='trove.common.remote.guest_client'), default='trove.common.remote.nova_client'), default='trove.common.remote.neutron_client'), default='trove.common.remote.cinder_client'), default='trove.common.remote.heat_client'), default='trove.common.remote.swift_client'), cfg.StrOpt('network_label_regex', default='^private$'), cfg.StrOpt('ip_regex', default=None), cfg.StrOpt('black_list_regex', default=None), default='trove.guestagent.strategies.backup.mysql_impl'), default='trove.guestagent.strategies.restore.mysql_impl'), cfg.StrOpt('device_path', default='/dev/vdb'), default='trove.guestagent.strategies.backup.mysql_impl'), default='trove.guestagent.strategies.restore.mysql_impl'), cfg.StrOpt('device_path', default='/dev/vdb'), cfg.StrOpt('device_path', default=None), cfg.StrOpt('device_path', default='/dev/vdb'), default='trove.guestagent.strategies.backup.couchbase_impl'), default='trove.guestagent.strategies.restore.couchbase_impl'), cfg.StrOpt('device_path', default='/dev/vdb'), cfg.StrOpt('device_path', default='/dev/vdb'),",188,93
openstack%2Fnova~master~I8800006508b2e18de657aa0a8d135bcd17cb89f1,openstack/nova,master,I8800006508b2e18de657aa0a8d135bcd17cb89f1,Give context to the warning in _sync_power_states,MERGED,2014-09-24 16:40:53.000000000,2014-09-27 20:06:27.000000000,2014-09-27 20:06:24.000000000,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 1063}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 6873}, {'_account_id': 9008}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-24 16:40:53.000000000', 'files': ['nova/compute/manager.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/79cd9a7b10fbe697b3f06cf7d6b286bd293c5a72', 'message': 'Give context to the warning in _sync_power_states\n\nThe ""Found x in the database and y on the hypervisor""\nwarning in the logs doesn\'t give any context to what\nis happening when that is logged, this adds the context.\n\nChange-Id: I8800006508b2e18de657aa0a8d135bcd17cb89f1\n'}]",0,123784,79cd9a7b10fbe697b3f06cf7d6b286bd293c5a72,31,8,1,6873,,,0,"Give context to the warning in _sync_power_states

The ""Found x in the database and y on the hypervisor""
warning in the logs doesn't give any context to what
is happening when that is logged, this adds the context.

Change-Id: I8800006508b2e18de657aa0a8d135bcd17cb89f1
",git fetch https://review.opendev.org/openstack/nova refs/changes/84/123784/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/compute/manager.py'],1,79cd9a7b10fbe697b3f06cf7d6b286bd293c5a72,sync-power-state-found-warning," LOG.warn(_(""While synchronizing instance power states, found "" ""%(num_db_instances)s instances in the database and "" ""%(num_vm_instances)s instances on the hypervisor.""),"," LOG.warn(_(""Found %(num_db_instances)s in the database and "" ""%(num_vm_instances)s on the hypervisor.""),",3,2
openstack%2Fsahara~master~I74d0241bc4b4f4ef2b812ed448c0a9cb6ddae1bd,openstack/sahara,master,I74d0241bc4b4f4ef2b812ed448c0a9cb6ddae1bd,Fixed broken pep8 after keystone update,MERGED,2014-09-26 20:47:25.000000000,2014-09-27 19:49:37.000000000,2014-09-27 19:49:36.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 8091}, {'_account_id': 10670}]","[{'number': 1, 'created': '2014-09-26 20:47:25.000000000', 'files': ['etc/sahara/sahara.conf.sample'], 'web_link': 'https://opendev.org/openstack/sahara/commit/e5e2596e1d3d39e515da7aceb9d58e68bde4848e', 'message': 'Fixed broken pep8 after keystone update\n\nKeystone added memcache options to config. Added them to sahara\nsample.\n\nChange-Id: I74d0241bc4b4f4ef2b812ed448c0a9cb6ddae1bd\n'}]",0,124511,e5e2596e1d3d39e515da7aceb9d58e68bde4848e,11,5,1,8411,,,0,"Fixed broken pep8 after keystone update

Keystone added memcache options to config. Added them to sahara
sample.

Change-Id: I74d0241bc4b4f4ef2b812ed448c0a9cb6ddae1bd
",git fetch https://review.opendev.org/openstack/sahara refs/changes/11/124511/1 && git format-patch -1 --stdout FETCH_HEAD,['etc/sahara/sahara.conf.sample'],1,e5e2596e1d3d39e515da7aceb9d58e68bde4848e,,# (optional) number of seconds memcached server is considered # dead before it is tried again. (integer value) #memcache_pool_dead_retry=300 # (optional) max total number of open connections to every # memcached server. (integer value) #memcache_pool_maxsize=10 # (optional) socket timeout in seconds for communicating with # a memcache server. (integer value) #memcache_pool_socket_timeout=3 # (optional) number of seconds a connection to memcached is # held unused in the pool before it is closed. (integer value) #memcache_pool_unused_timeout=60 # (optional) number of seconds that an operation will wait to # get a memcache client connection from the pool. (integer # value) #memcache_pool_conn_get_timeout=10 # (optional) use the advanced (eventlet safe) memcache client # pool. The advanced pool will only work under python 2.x. # (boolean value) #memcache_use_advanced_pool=false ,,26,0
openstack%2Ftempest~master~Id0c172e247ecfcd4eec85bafd4787e33944cee1a,openstack/tempest,master,Id0c172e247ecfcd4eec85bafd4787e33944cee1a,Log deletion errors in  clean servers,MERGED,2014-09-23 19:38:41.000000000,2014-09-27 19:49:27.000000000,2014-09-27 19:49:25.000000000,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 1921}, {'_account_id': 5196}, {'_account_id': 5292}, {'_account_id': 6167}]","[{'number': 1, 'created': '2014-09-23 19:38:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8e5116b229654ddd85bb659ddaa7959793036ea3', 'message': ""Log deletion errors in  clean servers\n\nAlthough we don't want to cause tempest to fail on error while deleting\na server in tearDownClass as these types of issues are hard to debug. We\nshould at least log the issues instead of hiding them completely.\n\nChange-Id: Id0c172e247ecfcd4eec85bafd4787e33944cee1a\nRelated-Bug: #1372696\n""}, {'number': 2, 'created': '2014-09-23 19:55:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/99cc5b2c04d882a195bfaf202e88983e5e4672d9', 'message': ""Log deletion errors in  clean servers\n\nAlthough we don't want to cause tempest to fail on error while deleting\na server in tearDownClass as these types of issues are hard to debug. We\nshould at least log the issues instead of hiding them completely.\n\nChange-Id: Id0c172e247ecfcd4eec85bafd4787e33944cee1a\nRelated-Bug: #1372696\n""}, {'number': 3, 'created': '2014-09-24 18:04:30.000000000', 'files': ['tempest/api/compute/base.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/0c33579fe94712f84e4641ae9f1ea66033144454', 'message': ""Log deletion errors in  clean servers\n\nAlthough we don't want to cause tempest to fail on error while deleting\na server in tearDownClass as these types of issues are hard to debug. We\nshould at least log the issues instead of hiding them completely.\n\nChange-Id: Id0c172e247ecfcd4eec85bafd4787e33944cee1a\nRelated-Bug: #1372696\n""}]",2,123555,0c33579fe94712f84e4641ae9f1ea66033144454,22,6,3,1849,,,0,"Log deletion errors in  clean servers

Although we don't want to cause tempest to fail on error while deleting
a server in tearDownClass as these types of issues are hard to debug. We
should at least log the issues instead of hiding them completely.

Change-Id: Id0c172e247ecfcd4eec85bafd4787e33944cee1a
Related-Bug: #1372696
",git fetch https://review.opendev.org/openstack/tempest refs/changes/55/123555/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/compute/base.py'],1,8e5116b229654ddd85bb659ddaa7959793036ea3,bug/1372696," except Exception.NotFound: # Something else already cleaned up the server, nothing to be # worried about except Exception: LOG.exception('Deleting server %s failed' % server['id']) except Exception.NotFound: # Something else already cleaned up the server, nothing to be # worried about except Exception: LOG.exception('Waiting for deletion of server %s failed' % server['id'])", except Exception: except Exception:,11,2
openstack%2Fhorizon~stable%2Ficehouse~Ia561c2db1c59e51fb669e1ad6636d9b3bc9f7676,openstack/horizon,stable/icehouse,Ia561c2db1c59e51fb669e1ad6636d9b3bc9f7676,"template to rely on the the ""id"" attribute",MERGED,2014-08-07 14:31:38.000000000,2014-09-27 19:49:16.000000000,2014-09-27 19:49:15.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 6282}, {'_account_id': 6638}, {'_account_id': 7976}, {'_account_id': 9622}, {'_account_id': 9656}, {'_account_id': 9981}, {'_account_id': 12000}, {'_account_id': 13308}]","[{'number': 1, 'created': '2014-08-07 14:31:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/ffbffa678cdef38d0f850f24328902d49555a605', 'message': 'template to rely on the the ""id"" attribute\n\nThe exception fixed is the following:\nNoReverseMatch: Reverse for \'detail\' with arguments \'(\'\',)\' and keyword\narguments \'{}\' not found. 1 pattern(s) tried:\n[u\'project/volumes/(?P<volume_id>[^/]+)/$\']\n\nThe test data doesn\'t include a volumeId attribute to volumes but they\ncontain an id attribute.\n\nHere I modify the template to rely on the the ""id"" attribute but maybe\nreal data has a volumeId attribute and it\'s the test data in\nopenstack_dashboard/test/test_data/nova_data.py that needs to be\nmodified instead.\n\nNote that it is required to support Django 1.7 in Icehouse, because\nwe\'re planning to release Debian Jessie with both Django 1.7 and\nIcehouse. This is to *support* Django 1.7, not at all to *switch* to\nit, which is very different. There\'s not so many patches needed,\nand they are non-disruptive/simple. Also, please don\'t just -1 on the\nreivew because of the freeze for next point release, just delay it\nuntil tomorrow after 2014.1.2 is released. Cheers! :)\n\nChange-Id: Ia561c2db1c59e51fb669e1ad6636d9b3bc9f7676\n(cherry picked from commit 149f842017f44b24f83a904498a4df6c6d2b6cc9)\n'}, {'number': 2, 'created': '2014-09-25 11:34:21.000000000', 'files': ['openstack_dashboard/dashboards/project/instances/templates/instances/_detail_overview.html'], 'web_link': 'https://opendev.org/openstack/horizon/commit/ba524b0abdb0ed905e765f7a2bbf7cca304bc818', 'message': 'template to rely on the the ""id"" attribute\n\nThe exception fixed is the following:\nNoReverseMatch: Reverse for \'detail\' with arguments \'(\'\',)\' and keyword\narguments \'{}\' not found. 1 pattern(s) tried:\n[u\'project/volumes/(?P<volume_id>[^/]+)/$\']\n\nThe test data doesn\'t include a volumeId attribute to volumes but they\ncontain an id attribute.\n\nHere I modify the template to rely on the the ""id"" attribute but maybe\nreal data has a volumeId attribute and it\'s the test data in\nopenstack_dashboard/test/test_data/nova_data.py that needs to be\nmodified instead.\n\nNote that it is required to support Django 1.7 in Icehouse, because\nwe\'re planning to release Debian Jessie with both Django 1.7 and\nIcehouse. This is to *support* Django 1.7, not at all to *switch* to\nit, which is very different.\n\nChange-Id: Ia561c2db1c59e51fb669e1ad6636d9b3bc9f7676\n(cherry picked from commit 149f842017f44b24f83a904498a4df6c6d2b6cc9)\n'}]",0,112591,ba524b0abdb0ed905e765f7a2bbf7cca304bc818,22,10,2,6476,,,0,"template to rely on the the ""id"" attribute

The exception fixed is the following:
NoReverseMatch: Reverse for 'detail' with arguments '('',)' and keyword
arguments '{}' not found. 1 pattern(s) tried:
[u'project/volumes/(?P<volume_id>[^/]+)/$']

The test data doesn't include a volumeId attribute to volumes but they
contain an id attribute.

Here I modify the template to rely on the the ""id"" attribute but maybe
real data has a volumeId attribute and it's the test data in
openstack_dashboard/test/test_data/nova_data.py that needs to be
modified instead.

Note that it is required to support Django 1.7 in Icehouse, because
we're planning to release Debian Jessie with both Django 1.7 and
Icehouse. This is to *support* Django 1.7, not at all to *switch* to
it, which is very different.

Change-Id: Ia561c2db1c59e51fb669e1ad6636d9b3bc9f7676
(cherry picked from commit 149f842017f44b24f83a904498a4df6c6d2b6cc9)
",git fetch https://review.opendev.org/openstack/horizon refs/changes/91/112591/2 && git format-patch -1 --stdout FETCH_HEAD,['openstack_dashboard/dashboards/project/instances/templates/instances/_detail_overview.html'],1,ffbffa678cdef38d0f850f24328902d49555a605,fix-volume.volumeId-in-detail-overview-icehouse," <a href=""{% url 'horizon:project:volumes:volumes:detail' volume.id %}""> {{ volume.id }}"," <a href=""{% url 'horizon:project:volumes:volumes:detail' volume.volumeId %}""> {{ volume.volumeId }}",2,2
openstack%2Fnova~stable%2Ficehouse~I17f15852c098af88afd270084c62eb87693c60d4,openstack/nova,stable/icehouse,I17f15852c098af88afd270084c62eb87693c60d4,Fix live-migration failure in FC multipath case,MERGED,2014-09-22 06:44:06.000000000,2014-09-27 19:48:54.000000000,2014-09-27 19:48:52.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 5170}, {'_account_id': 6491}, {'_account_id': 9656}, {'_account_id': 9796}, {'_account_id': 9924}, {'_account_id': 12175}]","[{'number': 1, 'created': '2014-09-22 06:44:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1c62457c7699cb92a21fa0d37c9a7da1d6125a0a', 'message': ""Fix live-migration failure in FC multipath case\n\nCurrently, /dev/dm-<NUM> instead of /dev/mapper/<multipath_id> is\nused to access multipath FC volumes by Compute Node and\nmultipath_id in connection_info is not maintained properly and\nmay be lost during connection refreshing.\n\nThis implementation will make source Compute Node and destination\nCompute Node fail to disconnect/connect to volumes properly and\nresult in live-migration failure.\n\nTo fix it, /dev/mapper<multipath_id> will be used instead of\n/dev/dm-<NUM> to access multipath devices, just like iSCSI multipath\nimplementation, and logic to preserve the unique (across Compute\nNodes) multipath_id is also added.\n\nCloses-Bug: #1327497\n(cherry picked from commit 3ea14e8a70a946dbb162ecafa848e4f2fa29772a)\n\nConflicts:\n\tnova/storage/linuxscsi.py\n\tnova/tests/virt/libvirt/test_libvirt_volume.py\n\tnova/virt/block_device.py\n\tnova/virt/libvirt/volume.py\n\nThis backport adjust oslo.i18n usage to oslo-incubator common code,\ndue to we didn't have oslo.i18n in icehouse.\n\nChange-Id: I17f15852c098af88afd270084c62eb87693c60d4\n""}, {'number': 2, 'created': '2014-09-23 02:27:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3517d67db575832aaa4902e2e68e58b43b188616', 'message': ""Fix live-migration failure in FC multipath case\n\nCurrently, /dev/dm-<NUM> instead of /dev/mapper/<multipath_id> is\nused to access multipath FC volumes by Compute Node and\nmultipath_id in connection_info is not maintained properly and\nmay be lost during connection refreshing.\n\nThis implementation will make source Compute Node and destination\nCompute Node fail to disconnect/connect to volumes properly and\nresult in live-migration failure.\n\nTo fix it, /dev/mapper<multipath_id> will be used instead of\n/dev/dm-<NUM> to access multipath devices, just like iSCSI multipath\nimplementation, and logic to preserve the unique (across Compute\nNodes) multipath_id is also added.\n\nCloses-Bug: #1327497\n(cherry picked from commit 3ea14e8a70a946dbb162ecafa848e4f2fa29772a)\n\nConflicts:\n\tnova/storage/linuxscsi.py\n\tnova/tests/virt/libvirt/test_libvirt_volume.py\n\tnova/virt/block_device.py\n\tnova/virt/libvirt/volume.py\n\nThis backport commit adjust oslo.i18n usage to oslo-incubator common code,\ndue to we didn't have oslo.i18n in icehouse.\nAnd remove unused  variable value dev_str in test_libvirt_volume.py,\nit should be deleted but not worth a specific commit in stable/icehouse.\n\nChange-Id: I17f15852c098af88afd270084c62eb87693c60d4\n""}, {'number': 3, 'created': '2014-09-23 03:50:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/4aa3b471b86dff7ec72f4f83364ab92989c1bb9b', 'message': ""Fix live-migration failure in FC multipath case\n\nCurrently, /dev/dm-<NUM> instead of /dev/mapper/<multipath_id> is\nused to access multipath FC volumes by Compute Node and\nmultipath_id in connection_info is not maintained properly and\nmay be lost during connection refreshing.\n\nThis implementation will make source Compute Node and destination\nCompute Node fail to disconnect/connect to volumes properly and\nresult in live-migration failure.\n\nTo fix it, /dev/mapper<multipath_id> will be used instead of\n/dev/dm-<NUM> to access multipath devices, just like iSCSI multipath\nimplementation, and logic to preserve the unique (across Compute\nNodes) multipath_id is also added.\n\nCloses-Bug: #1327497\n(cherry picked from commit 3ea14e8a70a946dbb162ecafa848e4f2fa29772a)\n\nConflicts:\n\tnova/storage/linuxscsi.py\n\tnova/tests/virt/libvirt/test_libvirt_volume.py\n\tnova/virt/block_device.py\n\tnova/virt/libvirt/volume.py\n\nThis backport commit adjust oslo.i18n usage to oslo-incubator common code,\ndue to we didn't have oslo.i18n in icehouse.\nAnd remove unused  variable value dev_str in test_libvirt_volume.py,\nit should be deleted but not worth a specific commit in stable/icehouse.\n\nChange-Id: I17f15852c098af88afd270084c62eb87693c60d4\n""}, {'number': 4, 'created': '2014-09-24 01:56:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/6b34be869dad70460610114e7ff1c10f23c8860b', 'message': ""Fix live-migration failure in FC multipath case\n\nCurrently, /dev/dm-<NUM> instead of /dev/mapper/<multipath_id> is\nused to access multipath FC volumes by Compute Node and\nmultipath_id in connection_info is not maintained properly and\nmay be lost during connection refreshing.\n\nThis implementation will make source Compute Node and destination\nCompute Node fail to disconnect/connect to volumes properly and\nresult in live-migration failure.\n\nTo fix it, /dev/mapper<multipath_id> will be used instead of\n/dev/dm-<NUM> to access multipath devices, just like iSCSI multipath\nimplementation, and logic to preserve the unique (across Compute\nNodes) multipath_id is also added.\n\nCloses-Bug: #1327497\n(cherry picked from commit 3ea14e8a70a946dbb162ecafa848e4f2fa29772a)\n\nConflicts:\n\tnova/storage/linuxscsi.py\n\tnova/tests/virt/libvirt/test_libvirt_volume.py\n\tnova/virt/block_device.py\n\tnova/virt/libvirt/volume.py\n\nThis backport commit adjust oslo.i18n usage to oslo-incubator common code,\ndue to we didn't have oslo.i18n in icehouse.\nAnd remove unused  variable value dev_str in test_libvirt_volume.py,\nit should be deleted but not worth a specific commit in stable/icehouse.\n\nChange-Id: I17f15852c098af88afd270084c62eb87693c60d4\n""}, {'number': 5, 'created': '2014-09-24 07:40:46.000000000', 'files': ['nova/tests/test_linuxscsi.py', 'nova/storage/linuxscsi.py', 'nova/tests/virt/test_block_device.py', 'nova/tests/virt/libvirt/test_libvirt_volume.py', 'nova/virt/libvirt/volume.py', 'nova/compute/manager.py', 'tox.ini', 'nova/virt/block_device.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/74e0ba7e658fcd2c6d1b7a92dcee564098d0a1ff', 'message': ""Fix live-migration failure in FC multipath case\n\nCurrently, /dev/dm-<NUM> instead of /dev/mapper/<multipath_id> is\nused to access multipath FC volumes by Compute Node and\nmultipath_id in connection_info is not maintained properly and\nmay be lost during connection refreshing.\n\nThis implementation will make source Compute Node and destination\nCompute Node fail to disconnect/connect to volumes properly and\nresult in live-migration failure.\n\nTo fix it, /dev/mapper<multipath_id> will be used instead of\n/dev/dm-<NUM> to access multipath devices, just like iSCSI multipath\nimplementation, and logic to preserve the unique (across Compute\nNodes) multipath_id is also added.\n\nCloses-Bug: #1327497\n(cherry picked from commit 3ea14e8a70a946dbb162ecafa848e4f2fa29772a)\n\nConflicts:\n\tnova/storage/linuxscsi.py\n\tnova/tests/virt/libvirt/test_libvirt_volume.py\n\tnova/virt/block_device.py\n\tnova/virt/libvirt/volume.py\n\nThis backport commit adjust oslo.i18n usage to oslo-incubator common code,\ndue to we didn't have oslo.i18n in icehouse.\nAnd remove unused  variable value dev_str in test_libvirt_volume.py,\nit should be deleted but not worth a specific commit in stable/icehouse.\n\nChange-Id: I17f15852c098af88afd270084c62eb87693c60d4\n""}]",2,123056,74e0ba7e658fcd2c6d1b7a92dcee564098d0a1ff,37,8,5,9796,,,0,"Fix live-migration failure in FC multipath case

Currently, /dev/dm-<NUM> instead of /dev/mapper/<multipath_id> is
used to access multipath FC volumes by Compute Node and
multipath_id in connection_info is not maintained properly and
may be lost during connection refreshing.

This implementation will make source Compute Node and destination
Compute Node fail to disconnect/connect to volumes properly and
result in live-migration failure.

To fix it, /dev/mapper<multipath_id> will be used instead of
/dev/dm-<NUM> to access multipath devices, just like iSCSI multipath
implementation, and logic to preserve the unique (across Compute
Nodes) multipath_id is also added.

Closes-Bug: #1327497
(cherry picked from commit 3ea14e8a70a946dbb162ecafa848e4f2fa29772a)

Conflicts:
	nova/storage/linuxscsi.py
	nova/tests/virt/libvirt/test_libvirt_volume.py
	nova/virt/block_device.py
	nova/virt/libvirt/volume.py

This backport commit adjust oslo.i18n usage to oslo-incubator common code,
due to we didn't have oslo.i18n in icehouse.
And remove unused  variable value dev_str in test_libvirt_volume.py,
it should be deleted but not worth a specific commit in stable/icehouse.

Change-Id: I17f15852c098af88afd270084c62eb87693c60d4
",git fetch https://review.opendev.org/openstack/nova refs/changes/56/123056/3 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/test_linuxscsi.py', 'nova/storage/linuxscsi.py', 'nova/tests/virt/test_block_device.py', 'nova/tests/virt/libvirt/test_libvirt_volume.py', 'nova/virt/libvirt/volume.py', 'nova/compute/manager.py', 'nova/virt/block_device.py']",7,1c62457c7699cb92a21fa0d37c9a7da1d6125a0a,FC_fix,"from nova.openstack.common.gettextutils import _LI def _preserve_multipath_id(self, connection_info): if self['connection_info'] and 'data' in self['connection_info']: if 'multipath_id' in self['connection_info']['data']: connection_info['data']['multipath_id'] =\ self['connection_info']['data']['multipath_id'] LOG.info(_LI('preserve multipath_id %s'), connection_info['data']['multipath_id']) self._preserve_multipath_id(connection_info) self._preserve_multipath_id(connection_info)",,48,18
openstack%2Fnova~stable%2Ficehouse~I8ebb5f3c2e7a81b11d776f8c0a15f3491ed273be,openstack/nova,stable/icehouse,I8ebb5f3c2e7a81b11d776f8c0a15f3491ed273be,libvirt: Save device_path in connection_info when booting from volume,MERGED,2014-09-23 03:57:24.000000000,2014-09-27 19:42:09.000000000,2014-09-27 19:42:06.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 5170}, {'_account_id': 8247}, {'_account_id': 9656}, {'_account_id': 9796}]","[{'number': 1, 'created': '2014-09-23 03:57:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/85448710a0fbadb3cb16cfa4e1207eb211f3fa00', 'message': ""libvirt: Save device_path in connection_info when booting from volume\n\nIf you boot an instance from a volume and later terminate it, the\nlibvirt volume driver disconnect_volume method does not have the\n'device_path' key in connection_info['data'].  However, if you\nattach a volume to an existing instance and then detach it,\nthe disconnect_volume method would have the 'device_path' key in\nconnection_info['data'].  Having the 'device_path' key would be\nuseful for some volume drivers to determine the device path of the\nvolume.  This patch saves the 'device_path' in connection_info['data']\nwhen _create_domain_and_network is called, so it could be later used.\n\nChange-Id: I8ebb5f3c2e7a81b11d776f8c0a15f3491ed273be\nCloses-Bug: #1291007\n(cherry picked from commit d19c75c19d2de8b20e82e6de9413ba53671ad7fb)\n""}, {'number': 2, 'created': '2014-09-23 05:36:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/946e6dea682e060e9c482914fea6b724846a1692', 'message': ""libvirt: Save device_path in connection_info when booting from volume\n\nIf you boot an instance from a volume and later terminate it, the\nlibvirt volume driver disconnect_volume method does not have the\n'device_path' key in connection_info['data'].  However, if you\nattach a volume to an existing instance and then detach it,\nthe disconnect_volume method would have the 'device_path' key in\nconnection_info['data'].  Having the 'device_path' key would be\nuseful for some volume drivers to determine the device path of the\nvolume.  This patch saves the 'device_path' in connection_info['data']\nwhen _create_domain_and_network is called, so it could be later used.\n\nChange-Id: I8ebb5f3c2e7a81b11d776f8c0a15f3491ed273be\nCloses-Bug: #1291007\n(cherry picked from commit d19c75c19d2de8b20e82e6de9413ba53671ad7fb)\n""}, {'number': 3, 'created': '2014-09-23 06:47:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f2a03131fe9be24a1f956548d136f9e0771873fc', 'message': ""libvirt: Save device_path in connection_info when booting from volume\n\nIf you boot an instance from a volume and later terminate it, the\nlibvirt volume driver disconnect_volume method does not have the\n'device_path' key in connection_info['data'].  However, if you\nattach a volume to an existing instance and then detach it,\nthe disconnect_volume method would have the 'device_path' key in\nconnection_info['data'].  Having the 'device_path' key would be\nuseful for some volume drivers to determine the device path of the\nvolume.  This patch saves the 'device_path' in connection_info['data']\nwhen _create_domain_and_network is called, so it could be later used.\n\nChange-Id: I8ebb5f3c2e7a81b11d776f8c0a15f3491ed273be\nCloses-Bug: #1291007\n(cherry picked from commit d19c75c19d2de8b20e82e6de9413ba53671ad7fb)\n""}, {'number': 4, 'created': '2014-09-24 01:54:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ef375b16a59384f870623aef9f2ff697a14f1077', 'message': ""libvirt: Save device_path in connection_info when booting from volume\n\nIf you boot an instance from a volume and later terminate it, the\nlibvirt volume driver disconnect_volume method does not have the\n'device_path' key in connection_info['data'].  However, if you\nattach a volume to an existing instance and then detach it,\nthe disconnect_volume method would have the 'device_path' key in\nconnection_info['data'].  Having the 'device_path' key would be\nuseful for some volume drivers to determine the device path of the\nvolume.  This patch saves the 'device_path' in connection_info['data']\nwhen _create_domain_and_network is called, so it could be later used.\n\nChange-Id: I8ebb5f3c2e7a81b11d776f8c0a15f3491ed273be\nCloses-Bug: #1291007\n(cherry picked from commit d19c75c19d2de8b20e82e6de9413ba53671ad7fb)\n""}, {'number': 5, 'created': '2014-09-24 06:16:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f316b0fd6083fa6c3050cebd8ed5ee66bfa849c4', 'message': ""libvirt: Save device_path in connection_info when booting from volume\n\nIf you boot an instance from a volume and later terminate it, the\nlibvirt volume driver disconnect_volume method does not have the\n'device_path' key in connection_info['data'].  However, if you\nattach a volume to an existing instance and then detach it,\nthe disconnect_volume method would have the 'device_path' key in\nconnection_info['data'].  Having the 'device_path' key would be\nuseful for some volume drivers to determine the device path of the\nvolume.  This patch saves the 'device_path' in connection_info['data']\nwhen _create_domain_and_network is called, so it could be later used.\n\nChange-Id: I8ebb5f3c2e7a81b11d776f8c0a15f3491ed273be\nCloses-Bug: #1291007\n(cherry picked from commit d19c75c19d2de8b20e82e6de9413ba53671ad7fb)\n""}, {'number': 6, 'created': '2014-09-24 06:43:44.000000000', 'files': ['nova/tests/virt/libvirt/test_libvirt.py', 'nova/virt/libvirt/driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/b61aa4d09889c0e10dcbdc8838b0bb1128b0af06', 'message': ""libvirt: Save device_path in connection_info when booting from volume\n\nIf you boot an instance from a volume and later terminate it, the\nlibvirt volume driver disconnect_volume method does not have the\n'device_path' key in connection_info['data'].  However, if you\nattach a volume to an existing instance and then detach it,\nthe disconnect_volume method would have the 'device_path' key in\nconnection_info['data'].  Having the 'device_path' key would be\nuseful for some volume drivers to determine the device path of the\nvolume.  This patch saves the 'device_path' in connection_info['data']\nwhen _create_domain_and_network is called, so it could be later used.\n\nThis commit also adjusts unit test to pass the check test.\n\nChange-Id: I8ebb5f3c2e7a81b11d776f8c0a15f3491ed273be\nCloses-Bug: #1291007\n(cherry picked from commit d19c75c19d2de8b20e82e6de9413ba53671ad7fb)\n""}]",0,123323,b61aa4d09889c0e10dcbdc8838b0bb1128b0af06,29,6,6,9796,,,0,"libvirt: Save device_path in connection_info when booting from volume

If you boot an instance from a volume and later terminate it, the
libvirt volume driver disconnect_volume method does not have the
'device_path' key in connection_info['data'].  However, if you
attach a volume to an existing instance and then detach it,
the disconnect_volume method would have the 'device_path' key in
connection_info['data'].  Having the 'device_path' key would be
useful for some volume drivers to determine the device path of the
volume.  This patch saves the 'device_path' in connection_info['data']
when _create_domain_and_network is called, so it could be later used.

This commit also adjusts unit test to pass the check test.

Change-Id: I8ebb5f3c2e7a81b11d776f8c0a15f3491ed273be
Closes-Bug: #1291007
(cherry picked from commit d19c75c19d2de8b20e82e6de9413ba53671ad7fb)
",git fetch https://review.opendev.org/openstack/nova refs/changes/23/123323/4 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/libvirt/test_libvirt.py', 'nova/virt/libvirt/driver.py']",2,85448710a0fbadb3cb16cfa4e1207eb211f3fa00,Save_device_path, if 'data' in connection_info: if (not reboot and 'data' in connection_info and 'volume_id' in connection_info['data']):, if (not reboot and 'data' in connection_info and 'volume_id' in connection_info['data']):,85,2
openstack%2Ftaskflow~master~I1d8309ce87114a0890dfc93a0a2c4b68f80ef828,openstack/taskflow,master,I1d8309ce87114a0890dfc93a0a2c4b68f80ef828,Increase/adjust the logging of the WBE response/send activities,MERGED,2014-06-26 22:21:36.000000000,2014-09-27 18:54:15.000000000,2014-09-27 18:54:14.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 2472}, {'_account_id': 6648}, {'_account_id': 8871}, {'_account_id': 9608}]","[{'number': 1, 'created': '2014-06-26 22:21:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/fffbf511f36ec61aeb6079c81a15d8fc70b86174', 'message': 'Increase the logging of the WBE response/send activities\n\nNow that the cache does not emit its own data about when\nits being updated we should increase the logging that is\nemitted by the current user of the cache types.\n\nAlso removes less useful str() and repr() tests since\nthose do not add that much meaningful value.\n\nChange-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828\n'}, {'number': 2, 'created': '2014-06-27 00:07:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/151c3a599bb6bd4d308091d55630045b06a05d55', 'message': 'Increase the logging of the WBE response/send activities\n\nNow that the cache does not emit its own data about when\nits being updated we should increase the logging that is\nemitted by the current user of the cache types.\n\nAlso removes less useful str() and repr() tests since\nthose do not add that much meaningful value.\n\nChange-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828\n'}, {'number': 3, 'created': '2014-06-27 01:15:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/66ce982f26a5be01bb345e70e785e98b34858507', 'message': 'Increase the logging of the WBE response/send activities\n\nNow that the cache does not emit its own data about when\nits being updated we should increase the logging that is\nemitted by the current user of the cache types.\n\nAlso removes less useful str() and repr() tests since\nthose do not add that much meaningful value.\n\nChange-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828\n'}, {'number': 4, 'created': '2014-06-29 20:15:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/2e443e6f2ebcac26a42c90767369d8a2a411e93b', 'message': 'Increase the logging of the WBE response/send activities\n\nNow that the cache does not emit its own data about when\nits being updated we should increase the logging that is\nemitted by the current user of the cache types.\n\nAlso removes less useful str() and repr() tests since\nthose do not add that much meaningful value.\n\nChange-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828\n'}, {'number': 5, 'created': '2014-07-03 03:57:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/e7004dc5c1c7f8f28b83de865bf662bfbf2f5993', 'message': 'Increase the logging of the WBE response/send activities\n\nNow that the cache does not emit its own data about when\nits being updated we should increase the logging that is\nemitted by the current user of the cache types.\n\nAlso removes less useful str() and repr() tests since\nthose do not add that much meaningful value.\n\nChange-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828\n'}, {'number': 6, 'created': '2014-08-15 17:39:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/be9c27014dca02c3d704b7c2202a65479250b275', 'message': 'Increase/adjust the logging of the WBE response/send activities\n\nChange-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828\n'}, {'number': 7, 'created': '2014-08-16 02:06:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/f5419be110d90491ec491524c480ab740cfd1f44', 'message': 'Increase/adjust the logging of the WBE response/send activities\n\nChange-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828\n'}, {'number': 8, 'created': '2014-08-28 17:31:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/798dd47a514c874663419545cedead330563d847', 'message': 'Increase/adjust the logging of the WBE response/send activities\n\nChange-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828\n'}, {'number': 9, 'created': '2014-09-04 23:57:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/a39b0d743d4396bb3a6dc3464501fe40b900859b', 'message': 'Increase/adjust the logging of the WBE response/send activities\n\nChange-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828\n'}, {'number': 10, 'created': '2014-09-05 01:19:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/5913cdda39391c9699f452f774b1e26f138e05e5', 'message': 'Increase/adjust the logging of the WBE response/send activities\n\nChange-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828\n'}, {'number': 11, 'created': '2014-09-12 07:11:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/50691ec4a2b5abb319f13631c48a49d630365f6e', 'message': 'Increase/adjust the logging of the WBE response/send activities\n\nChange-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828\n'}, {'number': 12, 'created': '2014-09-23 02:51:11.000000000', 'files': ['taskflow/engines/worker_based/proxy.py', 'taskflow/engines/worker_based/executor.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/d3d66083f43398a1db6c58fa1c48bb58110e276c', 'message': 'Increase/adjust the logging of the WBE response/send activities\n\nChange-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828\n'}]",2,102959,d3d66083f43398a1db6c58fa1c48bb58110e276c,54,6,12,1297,,,0,"Increase/adjust the logging of the WBE response/send activities

Change-Id: I1d8309ce87114a0890dfc93a0a2c4b68f80ef828
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/59/102959/7 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/engines/worker_based/executor.py', 'taskflow/engines/worker_based/proxy.py', 'taskflow/tests/unit/worker_based/test_protocol.py', 'taskflow/engines/worker_based/protocol.py']",4,fffbf511f36ec61aeb6079c81a15d8fc70b86174,better-wbe-logging," def __str__(self): return ""<%s> %s:%s, [%s, %s]"" % (self.TYPE, self.uuid, self.task_cls, self.action, self.state) @property def action(self): return self._action"," def __repr__(self): return ""%s:%s"" % (self._task_cls, self._action)",33,28
openstack%2Fglance_store~master~Ib3df99b13abb6aa1f579c1dce9e2b64d6e72da91,openstack/glance_store,master,Ib3df99b13abb6aa1f579c1dce9e2b64d6e72da91,Recover from errors while deleting image segments,MERGED,2014-09-23 20:48:36.000000000,2014-09-27 18:54:13.000000000,2014-09-27 18:54:12.000000000,"[{'_account_id': 3}, {'_account_id': 2537}, {'_account_id': 5347}, {'_account_id': 6549}, {'_account_id': 11864}]","[{'number': 1, 'created': '2014-09-23 20:48:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/b2072ece5055ea5d5578a14bb1fb11baf5dd9bd4', 'message': ""Recover from errors while deleting image segments\n\nWhen Glance attempts to delete an image, which is stored as a\nsegmented object, if an error occurs while deleting one of the\nsegments, delete operation fails while leaving behind rest of\nthe segments.\nThis patch enables Glance to recover from one or more errors during\nsegment deletion and proceed with deleting other segments. However,\nthis patch doesn't do anything with the failed segments themselves.\nWith this patch it'd be possible that even after an image is\ndeleted, certain chunks are left behind in the store.\n\nChange-Id: Ib3df99b13abb6aa1f579c1dce9e2b64d6e72da91\nCloses-Bug: 1256364\n""}, {'number': 2, 'created': '2014-09-25 20:06:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/c4000030483315acfcf8432edd3eda4648329fe8', 'message': ""Recover from errors while deleting image segments\n\nWhen Glance attempts to delete an image, which is stored as a\nsegmented object, if an error occurs while deleting one of the\nsegments, delete operation fails while leaving behind rest of\nthe segments.\nThis patch enables Glance to recover from one or more errors during\nsegment deletion and proceed with deleting other segments. However,\nthis patch doesn't do anything with the failed segments themselves.\nWith this patch it'd be possible that even after an image is\ndeleted, certain chunks are left behind in the store.\n\nChange-Id: Ib3df99b13abb6aa1f579c1dce9e2b64d6e72da91\nCloses-Bug: 1256364\n""}, {'number': 3, 'created': '2014-09-26 15:23:01.000000000', 'files': ['glance_store/_drivers/swift/store.py', 'tests/unit/test_swift_store.py'], 'web_link': 'https://opendev.org/openstack/glance_store/commit/8f99cce9b12a419d4627a4d15624b9b9df4cf099', 'message': ""Recover from errors while deleting image segments\n\nWhen Glance attempts to delete an image, which is stored as a\nsegmented object, if an error occurs while deleting one of the\nsegments, delete operation fails while leaving behind rest of\nthe segments.\nThis patch enables Glance to recover from one or more errors during\nsegment deletion and proceed with deleting other segments. However,\nthis patch doesn't do anything with the failed segments themselves.\nWith this patch it'd be possible that even after an image is\ndeleted, certain chunks are left behind in the store.\n\nChange-Id: Ib3df99b13abb6aa1f579c1dce9e2b64d6e72da91\nCloses-Bug: 1256364\n""}]",0,123584,8f99cce9b12a419d4627a4d15624b9b9df4cf099,14,5,3,8158,,,0,"Recover from errors while deleting image segments

When Glance attempts to delete an image, which is stored as a
segmented object, if an error occurs while deleting one of the
segments, delete operation fails while leaving behind rest of
the segments.
This patch enables Glance to recover from one or more errors during
segment deletion and proceed with deleting other segments. However,
this patch doesn't do anything with the failed segments themselves.
With this patch it'd be possible that even after an image is
deleted, certain chunks are left behind in the store.

Change-Id: Ib3df99b13abb6aa1f579c1dce9e2b64d6e72da91
Closes-Bug: 1256364
",git fetch https://review.opendev.org/openstack/glance_store refs/changes/84/123584/3 && git format-patch -1 --stdout FETCH_HEAD,"['glance_store/_drivers/swift/store.py', 'tests/unit/test_swift_store.py']",2,b2072ece5055ea5d5578a14bb1fb11baf5dd9bd4,bug/1256364," manifest = kwargs.get('headers').get('X-Object-Manifest') 'etag': etag, 'x-object-manifest': manifest} fixture_objects[fixture_key] = None def test_delete_with_some_segments_failing(self): """""" Tests that delete of a segmented object recovers from error(s) while deleting one or more segments. To test this we add a segmented object first and then delete it, while simulating errors on one or more segments. """""" test_image_id = str(uuid.uuid4()) def fake_head_object(container, object_name): object_manifest = '/'.join([container, object_name])+'-' return {'x-object-manifest': object_manifest} def fake_get_container(container, **kwargs): # Returning 5 fake segments return None, [{'name': '%s-%05d' % (test_image_id, x)} for x in range(1, 6)] def fake_delete_object(container, object_name): # Simulate error on 1st and 3rd segments global SWIFT_DELETE_OBJECT_CALLS SWIFT_DELETE_OBJECT_CALLS += 1 if object_name.endswith('001') or object_name.endswith('003'): raise swiftclient.ClientException('Object DELETE failed') else: pass location = ""swift+https://%s:key@localhost:8080/glance/%s"" location = location % (self.swift_store_user, test_image_id) loc = get_location_from_uri(location) conn = self.store.get_connection(loc.store_location) conn.delete_object = fake_delete_object conn.head_object = fake_head_object conn.get_container = fake_get_container global SWIFT_DELETE_OBJECT_CALLS SWIFT_DELETE_OBJECT_CALLS = 0 self.store.delete(loc, connection=conn) # Expecting 6 delete calls, 5 for the segments and 1 for the manifest self.assertEqual(SWIFT_DELETE_OBJECT_CALLS, 6) ", 'etag': etag},56,3
openstack%2Fnova~master~I7d0f2571e80a4e55133f823d2a04feaf4dddf2e5,openstack/nova,master,I7d0f2571e80a4e55133f823d2a04feaf4dddf2e5,Log original error when attaching volume fails,MERGED,2014-09-26 15:48:13.000000000,2014-09-27 18:53:52.000000000,2014-09-27 18:53:50.000000000,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 6873}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-26 15:48:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/4a589b89f37d36124dbf21085d63bda051cdd881', 'message': 'Log original error when attaching volume fails\n\nWe have a race in the gate to attach an encrypted volume and the only\nthing we see in the logs is the DeviceIsBusy error which makes this hard\nto debug.\n\nThis change logs the original error so we can dig deeper into the root\ncause.\n\nRelated-Bug: #1348204\n\nChange-Id: I7d0f2571e80a4e55133f823d2a04feaf4dddf2e5\n'}, {'number': 2, 'created': '2014-09-26 15:54:55.000000000', 'files': ['nova/virt/libvirt/driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/6c7d4c91911ccb2c79d293302cd944a24079af2f', 'message': 'Log original error when attaching volume fails\n\nWe have a race in the gate to attach an encrypted volume and the only\nthing we see in the logs is the DeviceIsBusy error which makes this hard\nto debug.\n\nThis change logs the original error so we can dig deeper into the root\ncause.\n\nRelated-Bug: #1348204\n\nChange-Id: I7d0f2571e80a4e55133f823d2a04feaf4dddf2e5\n'}]",3,124448,6c7d4c91911ccb2c79d293302cd944a24079af2f,13,6,2,6873,,,0,"Log original error when attaching volume fails

We have a race in the gate to attach an encrypted volume and the only
thing we see in the logs is the DeviceIsBusy error which makes this hard
to debug.

This change logs the original error so we can dig deeper into the root
cause.

Related-Bug: #1348204

Change-Id: I7d0f2571e80a4e55133f823d2a04feaf4dddf2e5
",git fetch https://review.opendev.org/openstack/nova refs/changes/48/124448/2 && git format-patch -1 --stdout FETCH_HEAD,['nova/virt/libvirt/driver.py'],1,4a589b89f37d36124dbf21085d63bda051cdd881,bug/1348204," LOG.error(_('Failed to attach volume at mountpoint %(mountpoint)s.' ' Error: %(ex)s'), {'mountpoint': mountpoint, 'ex': ex}, instance=instance)",,3,0
openstack%2Ftempest~master~I56bc2bfb5a3019cf4a2979b40c7d7cbe3c8cf71a,openstack/tempest,master,I56bc2bfb5a3019cf4a2979b40c7d7cbe3c8cf71a,Unskip test_create_and_get_delete_bucket(),MERGED,2014-09-15 14:15:16.000000000,2014-09-27 18:53:39.000000000,2014-09-27 18:53:38.000000000,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 5196}, {'_account_id': 6167}, {'_account_id': 10016}, {'_account_id': 10385}, {'_account_id': 11224}]","[{'number': 1, 'created': '2014-09-15 14:15:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/27c4a358ff6c8030c59bfe143afa33e8bdb9c3aa', 'message': 'Unskip test_create_and_get_delete_bucket()\n\ntest_create_and_get_delete_bucket() was skipped because of bug 1076965\nwhich is now in a state which should allow us to unskip it.\n\nChange-Id: I56bc2bfb5a3019cf4a2979b40c7d7cbe3c8cf71a\nRelated-Bug: #1076965\n'}, {'number': 2, 'created': '2014-09-15 18:32:08.000000000', 'files': ['tempest/thirdparty/boto/test_s3_buckets.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/57ab5de4a453341a0d886a884b0156d1816061b5', 'message': 'Unskip test_create_and_get_delete_bucket()\n\ntest_create_and_get_delete_bucket() was skipped because of bug 1076965\nwhich is now in a state which should allow us to unskip it.\n\nChange-Id: I56bc2bfb5a3019cf4a2979b40c7d7cbe3c8cf71a\nRelated-Bug: #1076965\n'}]",0,121569,57ab5de4a453341a0d886a884b0156d1816061b5,19,7,2,5196,,,0,"Unskip test_create_and_get_delete_bucket()

test_create_and_get_delete_bucket() was skipped because of bug 1076965
which is now in a state which should allow us to unskip it.

Change-Id: I56bc2bfb5a3019cf4a2979b40c7d7cbe3c8cf71a
Related-Bug: #1076965
",git fetch https://review.opendev.org/openstack/tempest refs/changes/69/121569/2 && git format-patch -1 --stdout FETCH_HEAD,['tempest/thirdparty/boto/test_s3_buckets.py'],1,27c4a358ff6c8030c59bfe143afa33e8bdb9c3aa,bug/1076965,," @test.skip_because(bug=""1076965"")",0,1
openstack%2Fneutron~master~Ie6b44d1d820ebb186036ffcd04ea2104dd384e94,openstack/neutron,master,Ie6b44d1d820ebb186036ffcd04ea2104dd384e94,Do not assume order of device_ids set elements,MERGED,2014-08-06 13:26:21.000000000,2014-09-27 18:07:43.000000000,2014-09-22 14:46:04.000000000,"[{'_account_id': 3}, {'_account_id': 490}, {'_account_id': 704}, {'_account_id': 841}, {'_account_id': 5170}, {'_account_id': 5948}, {'_account_id': 6524}, {'_account_id': 6620}, {'_account_id': 6635}, {'_account_id': 6637}, {'_account_id': 6638}, {'_account_id': 6659}, {'_account_id': 6854}, {'_account_id': 7787}, {'_account_id': 8124}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9751}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10370}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10624}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 12737}, {'_account_id': 12826}, {'_account_id': 12857}]","[{'number': 1, 'created': '2014-08-06 13:26:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/298783b01821cf84b544e7ff8d6ee052e4572787', 'message': ""Do not assume order of ovs_bridges set elements\n\nThis fixes the test_ancillary_bridges_multiple unit test that breaks with a randomized\nPYTHONHASHSEED (see the bug report).\nThe test assumed that the ovs_bridges set had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\n\nThe fix refactors the setup_ancillary_bridges function so that it sorts the\novs_bridges set before filtering the list of bridges to those that have external\nbridge-id's configured.\n\nPartial-bug: #1348818\n\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: Ie6b44d1d820ebb186036ffcd04ea2104dd384e94\n""}, {'number': 2, 'created': '2014-08-07 16:38:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/61c69d6d4f7c8b64209a11d9039f296355b508f0', 'message': 'Do not assume order of device_ids set elements\n\nThis fixes the test_ancillary_bridges_multiple unit test that breaks with a randomized\nPYTHONHASHSEED (see the bug report).\nThe test assumed that the device_ids set had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\n\nThe fix refactors the pullup_side_effect function so that it checks if the\ndevice_id exists before returning the bridge.\n\nPartial-bug: #1348818\n\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: Ie6b44d1d820ebb186036ffcd04ea2104dd384e94\n'}, {'number': 3, 'created': '2014-08-07 23:37:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/fa1cfbcc813fc1726cf80bdba823c22b9a68eef6', 'message': ':x\n\nThis fixes the test_ancillary_bridges_multiple unit test that breaks with a randomized\nPYTHONHASHSEED (see the bug report).\nThe test assumed that the device_ids set had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\n\nThe fix refactors the pullup_side_effect function so that it checks if the\ndevice_id exists before returning the bridge.\n\nPartial-bug: #1348818\n\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: Ie6b44d1d820ebb186036ffcd04ea2104dd384e94\n'}, {'number': 4, 'created': '2014-08-08 09:08:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ed7ced8b079b395a1eb0ddd22009f7190a5879c6', 'message': 'Do not assume order of device_ids set elements\n\nThis fixes the test_ancillary_bridges_multiple unit test that breaks with a randomized\nPYTHONHASHSEED (see the bug report).\nThe test assumed that the device_ids set had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\n\nThe fix refactors the pullup_side_effect function so that it checks if the\ndevice_id exists before returning the bridge.\n\nPartial-bug: #1348818\n\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: Ie6b44d1d820ebb186036ffcd04ea2104dd384e94\n'}, {'number': 5, 'created': '2014-09-03 15:33:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3d38e0aff815c9f6986a94f7995f2b6b94e0641b', 'message': 'Do not assume order of device_ids set elements\n\nThis fixes the test_ancillary_bridges_multiple unit test that breaks with a randomized\nPYTHONHASHSEED (see the bug report).\nThe test assumed that the device_ids set had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\n\nThe fix refactors the pullup_side_effect function so that it checks if the\ndevice_id exists before returning the bridge.\n\nPartial-bug: #1348818\n\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: Ie6b44d1d820ebb186036ffcd04ea2104dd384e94\n'}, {'number': 6, 'created': '2014-09-09 16:18:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/bb11b2ad52e0b43e224b5b3c9ba8239c3a9ecea1', 'message': 'Do not assume order of device_ids set elements\n\nThis fixes the test_ancillary_bridges_multiple unit test that breaks with a randomized\nPYTHONHASHSEED (see the bug report).\nThe test assumed that the device_ids set had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\n\nThe fix refactors the pullup_side_effect function so that it checks if the\ndevice_id exists before returning the bridge.\n\nPartial-bug: #1348818\n\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: Ie6b44d1d820ebb186036ffcd04ea2104dd384e94\n'}, {'number': 7, 'created': '2014-09-18 23:38:53.000000000', 'files': ['neutron/tests/unit/openvswitch/test_ovs_neutron_agent.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/979d8118577c59a2dfe8c3ebd268fb3863aa4b26', 'message': 'Do not assume order of device_ids set elements\n\nThis fixes the test_ancillary_bridges_multiple unit test that breaks with a randomized\nPYTHONHASHSEED (see the bug report).\nThe test assumed that the device_ids set had\nelements in a particular order. Found with PYTHONHASHSEED=2455351445.\n\nThe fix refactors the pullup_side_effect function so that it checks if the\ndevice_id exists before returning the bridge.\n\nPartial-bug: #1348818\n\nNote: There are several other unrelated unit tests that also break with a\nrandomized PYTHONHASHSEED, but they are not addressed here. They will be\naddressed in separate patches.\n\nChange-Id: Ie6b44d1d820ebb186036ffcd04ea2104dd384e94\n'}]",6,112296,979d8118577c59a2dfe8c3ebd268fb3863aa4b26,177,40,7,6638,,,0,"Do not assume order of device_ids set elements

This fixes the test_ancillary_bridges_multiple unit test that breaks with a randomized
PYTHONHASHSEED (see the bug report).
The test assumed that the device_ids set had
elements in a particular order. Found with PYTHONHASHSEED=2455351445.

The fix refactors the pullup_side_effect function so that it checks if the
device_id exists before returning the bridge.

Partial-bug: #1348818

Note: There are several other unrelated unit tests that also break with a
randomized PYTHONHASHSEED, but they are not addressed here. They will be
addressed in separate patches.

Change-Id: Ie6b44d1d820ebb186036ffcd04ea2104dd384e94
",git fetch https://review.opendev.org/openstack/neutron refs/changes/96/112296/2 && git format-patch -1 --stdout FETCH_HEAD,['neutron/plugins/openvswitch/agent/ovs_neutron_agent.py'],1,298783b01821cf84b544e7ff8d6ee052e4572787,bug/1348818, for bridge in sorted(ovs_bridges):, for bridge in ovs_bridges:,1,1
openstack%2Fdevstack~master~I30392adff5e3250a1c4f9f1f04fc7e0587007226,openstack/devstack,master,I30392adff5e3250a1c4f9f1f04fc7e0587007226,Enable security group extension in PLUMgrid plugin,MERGED,2014-07-16 00:52:32.000000000,2014-09-27 17:54:56.000000000,2014-09-27 17:54:55.000000000,"[{'_account_id': 3}, {'_account_id': 704}, {'_account_id': 970}, {'_account_id': 1653}, {'_account_id': 1935}, {'_account_id': 2592}, {'_account_id': 2750}, {'_account_id': 4395}, {'_account_id': 6835}, {'_account_id': 6854}, {'_account_id': 7175}, {'_account_id': 8279}, {'_account_id': 8871}, {'_account_id': 9009}, {'_account_id': 9536}]","[{'number': 1, 'created': '2014-07-16 00:52:32.000000000', 'files': ['lib/neutron_plugins/plumgrid'], 'web_link': 'https://opendev.org/openstack/devstack/commit/242c098498a12c5af60b97818d4c547ff90614d6', 'message': 'Enable security group extension in PLUMgrid plugin\n\nImplements: blueprint plumgrid-neutron-security-groups\nChange-Id: I30392adff5e3250a1c4f9f1f04fc7e0587007226\n'}]",0,107216,242c098498a12c5af60b97818d4c547ff90614d6,35,15,1,8279,,,0,"Enable security group extension in PLUMgrid plugin

Implements: blueprint plumgrid-neutron-security-groups
Change-Id: I30392adff5e3250a1c4f9f1f04fc7e0587007226
",git fetch https://review.opendev.org/openstack/devstack refs/changes/16/107216/1 && git format-patch -1 --stdout FETCH_HEAD,['lib/neutron_plugins/plumgrid'],1,242c098498a12c5af60b97818d4c547ff90614d6,bp/plumgrid-neutron-security-groups, # return 0 means enabled return 0, # False return 1,2,2
openstack%2Fdevstack~master~I82ebb09c64589fc9b7bb790982541bc87c66e6e3,openstack/devstack,master,I82ebb09c64589fc9b7bb790982541bc87c66e6e3,Remove the Cisco Nexus monolithic plugin support,MERGED,2014-09-15 17:03:22.000000000,2014-09-27 17:44:24.000000000,2014-09-27 17:44:23.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 2592}, {'_account_id': 5892}, {'_account_id': 6524}, {'_account_id': 7018}, {'_account_id': 7118}, {'_account_id': 8940}, {'_account_id': 9009}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-15 17:03:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/3ad716cd9a5abb1286c979497bc4e26d691dabc7', 'message': ""Remove the Cisco Nexus monolithic plugin support\n\nThe Cisco Nexus monolithic plugin does not work without the Open\nvSwitch plugin. The Open vSwitch plugin is scheduled to be removed\nas per #1323729. This patch removes the Nexus Hardware switch\nrelated code from devstack. The N1KV virtual switch related code\nwill still remain in the tree as it doesn't depend on Open vSwitch\nplugin.\n\nCloses-Bug: #1350387\nChange-Id: I82ebb09c64589fc9b7bb790982541bc87c66e6e3\n""}, {'number': 2, 'created': '2014-09-16 16:17:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/4ad028d24ee5e183af135f199c63382202ee8eb6', 'message': ""Remove the Cisco Nexus monolithic plugin support\n\nThe Cisco Nexus monolithic plugin does not work without the Open\nvSwitch plugin. The Open vSwitch plugin is scheduled to be removed\nas per #1323729. This patch removes the Nexus Hardware switch\nrelated code from devstack. The N1KV virtual switch related code\nwill still remain in the tree as it doesn't depend on Open vSwitch\nplugin.\n\nCloses-Bug: #1350387\nChange-Id: I82ebb09c64589fc9b7bb790982541bc87c66e6e3\n""}, {'number': 3, 'created': '2014-09-18 16:44:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/d05b9c05103e538172a0934e1ff918a924b6c2b7', 'message': ""Remove the Cisco Nexus monolithic plugin support\n\nThe Cisco Nexus monolithic plugin does not work without the Open\nvSwitch plugin. The Open vSwitch plugin is scheduled to be removed\nas per #1323729. This patch removes the Nexus Hardware switch\nrelated code from devstack. The N1KV virtual switch related code\nwill still remain in the tree as it doesn't depend on Open vSwitch\nplugin.\n\nCloses-Bug: #1350387\nChange-Id: I82ebb09c64589fc9b7bb790982541bc87c66e6e3\n""}, {'number': 4, 'created': '2014-09-19 17:50:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/878187ae03e1ef34d98a0c92859b899238e616a0', 'message': ""Remove the Cisco Nexus monolithic plugin support\n\nThe Cisco Nexus monolithic plugin does not work without the Open\nvSwitch plugin. The Open vSwitch plugin is scheduled to be removed\nas per #1323729. This patch removes the Nexus Hardware switch\nrelated code from devstack. The N1KV virtual switch related code\nwill still remain in the tree as it doesn't depend on Open vSwitch\nplugin.\n\nCloses-Bug: #1350387\nChange-Id: I82ebb09c64589fc9b7bb790982541bc87c66e6e3\n""}, {'number': 5, 'created': '2014-09-22 16:59:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/4b115a82b98a3d9d1c819ae8e5976e1d7fe93bbc', 'message': ""Remove the Cisco Nexus monolithic plugin support\n\nThe Cisco Nexus monolithic plugin does not work without the Open\nvSwitch plugin. The Open vSwitch plugin is scheduled to be removed\nas per #1323729. This patch removes the Nexus Hardware switch\nrelated code from devstack. The N1KV virtual switch related code\nwill still remain in the tree as it doesn't depend on Open vSwitch\nplugin.\n\nCloses-Bug: #1350387\nChange-Id: I82ebb09c64589fc9b7bb790982541bc87c66e6e3\n""}, {'number': 6, 'created': '2014-09-23 16:53:09.000000000', 'files': ['lib/neutron_plugins/cisco'], 'web_link': 'https://opendev.org/openstack/devstack/commit/107278fa5b2b70669c46237da971e0a9ff84482a', 'message': ""Remove the Cisco Nexus monolithic plugin support\n\nThe Cisco Nexus monolithic plugin does not work without the Open\nvSwitch plugin. The Open vSwitch plugin is scheduled to be removed\nas per #1323729. This patch removes the Nexus Hardware switch\nrelated code from devstack. The N1KV virtual switch related code\nwill still remain in the tree as it doesn't depend on Open vSwitch\nplugin.\n\nCloses-Bug: #1350387\nChange-Id: I82ebb09c64589fc9b7bb790982541bc87c66e6e3\n""}]",4,121623,107278fa5b2b70669c46237da971e0a9ff84482a,33,10,6,5892,,,0,"Remove the Cisco Nexus monolithic plugin support

The Cisco Nexus monolithic plugin does not work without the Open
vSwitch plugin. The Open vSwitch plugin is scheduled to be removed
as per #1323729. This patch removes the Nexus Hardware switch
related code from devstack. The N1KV virtual switch related code
will still remain in the tree as it doesn't depend on Open vSwitch
plugin.

Closes-Bug: #1350387
Change-Id: I82ebb09c64589fc9b7bb790982541bc87c66e6e3
",git fetch https://review.opendev.org/openstack/devstack refs/changes/23/121623/4 && git format-patch -1 --stdout FETCH_HEAD,['lib/neutron_plugins/cisco'],1,3ad716cd9a5abb1286c979497bc4e26d691dabc7,bug/1350387,function has_neutron_plugin_security_group { return 1 _neutron_ovs_base_configure_nova_vif_driver Q_CISCO_PLUGIN_SUBPLUGINS=(n1kv) Q_PLUGIN_CONF_PATH=etc/neutron/plugins/cisco Q_PLUGIN_CONF_FILENAME=cisco_plugins.ini cisco_cfg_file=/$Q_PLUGIN_CONF_FILE,"# Specify ncclient package information NCCLIENT_DIR=$DEST/ncclient NCCLIENT_VERSION=${NCCLIENT_VERSION:-0.3.1} NCCLIENT_REPO=${NCCLIENT_REPO:-git://github.com/CiscoSystems/ncclient.git} NCCLIENT_BRANCH=${NCCLIENT_BRANCH:-master} function _has_ovs_subplugin { local subplugin for subplugin in ${Q_CISCO_PLUGIN_SUBPLUGINS[@]}; do if [[ ""$subplugin"" == ""openvswitch"" ]]; then return 0 fi done return 1 } function _has_nexus_subplugin { local subplugin for subplugin in ${Q_CISCO_PLUGIN_SUBPLUGINS[@]}; do if [[ ""$subplugin"" == ""nexus"" ]]; then return 0 fi done return 1 } # This routine populates the cisco config file with the information for # a particular nexus switch function _config_switch { local cisco_cfg_file=$1 local switch_ip=$2 local username=$3 local password=$4 local ssh_port=$5 shift 5 local section=""NEXUS_SWITCH:$switch_ip"" iniset $cisco_cfg_file $section username $username iniset $cisco_cfg_file $section password $password iniset $cisco_cfg_file $section ssh_port $ssh_port while [[ ${#@} != 0 ]]; do iniset $cisco_cfg_file $section $1 $2 shift 2 done } # Check the version of the installed ncclient package function check_ncclient_version { python << EOF version = '$NCCLIENT_VERSION' import sys try: import pkg_resources import ncclient module_version = pkg_resources.get_distribution('ncclient').version if version != module_version: sys.exit(1) except: sys.exit(1) EOF } # Install the ncclient package function install_ncclient { git_clone $NCCLIENT_REPO $NCCLIENT_DIR $NCCLIENT_BRANCH (cd $NCCLIENT_DIR; sudo python setup.py install) } # Check if the required version of ncclient has been installed function is_ncclient_installed { # Check if the Cisco ncclient repository exists if [[ -d $NCCLIENT_DIR ]]; then remotes=$(cd $NCCLIENT_DIR; git remote -v | grep fetch | awk '{ print $2}') for remote in $remotes; do if [[ $remote == $NCCLIENT_REPO ]]; then break; fi done if [[ $remote != $NCCLIENT_REPO ]]; then return 1 fi else return 1 fi # Check if the ncclient is installed with the right version if ! check_ncclient_version; then return 1 fi return 0 } function has_neutron_plugin_security_group { if _has_ovs_subplugin; then ovs_has_neutron_plugin_security_group else return 1 fi # Cisco uses OVS if openvswitch subplugin is deployed _has_ovs_subplugin if _has_ovs_subplugin; then ovs_neutron_plugin_create_nova_conf else _neutron_ovs_base_configure_nova_vif_driver fi Q_CISCO_PLUGIN_SUBPLUGINS=(openvswitch nexus) if _has_ovs_subplugin; then ovs_neutron_plugin_configure_common Q_PLUGIN_EXTRA_CONF_PATH=etc/neutron/plugins/cisco Q_PLUGIN_EXTRA_CONF_FILES=(cisco_plugins.ini) # Copy extra config files to /etc so that they can be modified # later according to Cisco-specific localrc settings. mkdir -p /$Q_PLUGIN_EXTRA_CONF_PATH local f local extra_conf_file for (( f=0; $f < ${#Q_PLUGIN_EXTRA_CONF_FILES[@]}; f+=1 )); do extra_conf_file=$Q_PLUGIN_EXTRA_CONF_PATH/${Q_PLUGIN_EXTRA_CONF_FILES[$f]} cp $NEUTRON_DIR/$extra_conf_file /$extra_conf_file done else Q_PLUGIN_CONF_PATH=etc/neutron/plugins/cisco Q_PLUGIN_CONF_FILENAME=cisco_plugins.ini fi if _has_ovs_subplugin; then ovs_neutron_plugin_configure_debug_command fi if _has_ovs_subplugin; then ovs_neutron_plugin_configure_l3_agent fi } function _configure_nexus_subplugin { local cisco_cfg_file=$1 # Install a known compatible ncclient from the Cisco repository if necessary if ! is_ncclient_installed; then # Preserve the two global variables local offline=$OFFLINE local reclone=$RECLONE # Change their values to allow installation OFFLINE=False RECLONE=yes install_ncclient # Restore their values OFFLINE=$offline RECLONE=$reclone fi # Setup default nexus switch information if [ ! -v Q_CISCO_PLUGIN_SWITCH_INFO ]; then declare -A Q_CISCO_PLUGIN_SWITCH_INFO HOST_NAME=$(hostname) Q_CISCO_PLUGIN_SWITCH_INFO=([1.1.1.1]=stack:stack:22:${HOST_NAME}:1/10) else iniset $cisco_cfg_file CISCO nexus_driver neutron.plugins.cisco.nexus.cisco_nexus_network_driver_v2.CiscoNEXUSDriver fi # Setup the switch configurations local nswitch local sw_info local segment local sw_info_array declare -i count=0 for nswitch in ${!Q_CISCO_PLUGIN_SWITCH_INFO[@]}; do sw_info=${Q_CISCO_PLUGIN_SWITCH_INFO[$nswitch]} sw_info_array=${sw_info//:/ } sw_info_array=( $sw_info_array ) count=${#sw_info_array[@]} if [[ $count < 5 || $(( ($count-3) % 2 )) != 0 ]]; then die $LINENO ""Incorrect switch configuration: ${Q_CISCO_PLUGIN_SWITCH_INFO[$nswitch]}"" fi _config_switch $cisco_cfg_file $nswitch ${sw_info_array[@]} done if _has_ovs_subplugin; then ovs_neutron_plugin_configure_plugin_agent fi if _has_ovs_subplugin; then ovs_neutron_plugin_configure_service cisco_cfg_file=/${Q_PLUGIN_EXTRA_CONF_FILES[0]} else cisco_cfg_file=/$Q_PLUGIN_CONF_FILE fi if _has_ovs_subplugin && _has_n1kv_subplugin; then die $LINENO ""OVS subplugin and n1kv subplugin cannot coexist"" fi inicomment $cisco_cfg_file CISCO_PLUGINS nexus_plugin nexus) iniset $cisco_cfg_file CISCO_PLUGINS nexus_plugin neutron.plugins.cisco.nexus.cisco_nexus_plugin_v2.NexusPlugin;; openvswitch) iniset $cisco_cfg_file CISCO_PLUGINS vswitch_plugin neutron.plugins.openvswitch.ovs_neutron_plugin.OVSNeutronPluginV2;; if _has_nexus_subplugin; then _configure_nexus_subplugin $cisco_cfg_file fi ",6,193
openstack%2Fdevstack~master~I37073d73e4cba0103ab1a3d935302f1cd0ef73c5,openstack/devstack,master,I37073d73e4cba0103ab1a3d935302f1cd0ef73c5,Update Horizon Neutron-related settings,MERGED,2014-09-23 19:50:25.000000000,2014-09-27 17:44:04.000000000,2014-09-27 17:44:03.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 2592}, {'_account_id': 7118}, {'_account_id': 9009}]","[{'number': 1, 'created': '2014-09-23 19:50:25.000000000', 'files': ['lib/horizon'], 'web_link': 'https://opendev.org/openstack/devstack/commit/0843f0ab05a7f477cfc83a70f7711f438e7e8cbd', 'message': 'Update Horizon Neutron-related settings\n\n- Horizon no longer has ""enable_security_group"" setting\n  so we need to remove it.\n- There is no need to set enable_lb/firewall/vpn to True\n  when q-lbaas/q-fwaas/q-vpn is enabled because Horizon now checks if\n  Neutron ext-list and enables corresponding dashboards accordingly.\n\nChange-Id: I37073d73e4cba0103ab1a3d935302f1cd0ef73c5\n'}]",0,123561,0843f0ab05a7f477cfc83a70f7711f438e7e8cbd,9,5,1,841,,,0,"Update Horizon Neutron-related settings

- Horizon no longer has ""enable_security_group"" setting
  so we need to remove it.
- There is no need to set enable_lb/firewall/vpn to True
  when q-lbaas/q-fwaas/q-vpn is enabled because Horizon now checks if
  Neutron ext-list and enables corresponding dashboards accordingly.

Change-Id: I37073d73e4cba0103ab1a3d935302f1cd0ef73c5
",git fetch https://review.opendev.org/openstack/devstack refs/changes/61/123561/1 && git format-patch -1 --stdout FETCH_HEAD,['lib/horizon'],1,0843f0ab05a7f477cfc83a70f7711f438e7e8cbd,remove-horizon-enable-secgroup,, if is_service_enabled neutron; then _horizon_config_set $local_settings OPENSTACK_NEUTRON_NETWORK enable_security_group $Q_USE_SECGROUP fi # enable loadbalancer dashboard in case service is enabled if is_service_enabled q-lbaas; then _horizon_config_set $local_settings OPENSTACK_NEUTRON_NETWORK enable_lb True fi # enable firewall dashboard in case service is enabled if is_service_enabled q-fwaas; then _horizon_config_set $local_settings OPENSTACK_NEUTRON_NETWORK enable_firewall True fi # enable VPN dashboard in case service is enabled if is_service_enabled q-vpn; then _horizon_config_set $local_settings OPENSTACK_NEUTRON_NETWORK enable_vpn True fi ,0,18
openstack%2Fheat~master~Icfeff0e30a9c4bd7fa0279e156709e078b81fcab,openstack/heat,master,Icfeff0e30a9c4bd7fa0279e156709e078b81fcab,Fixed adopt failure for stack with resource group,MERGED,2014-09-05 12:05:38.000000000,2014-09-27 17:43:45.000000000,2014-09-27 17:43:44.000000000,"[{'_account_id': 3}, {'_account_id': 1633}, {'_account_id': 4257}, {'_account_id': 4328}, {'_account_id': 4571}, {'_account_id': 4715}, {'_account_id': 9542}]","[{'number': 1, 'created': '2014-09-05 12:05:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/70d15878b74e1a3776dd8863d51d803f9ac9ace7', 'message': 'Fixed adopt failure for stack with resource group\n\nAdded support to adopt a resource group and corresponding\nunit test.\n\nChange-Id: Icfeff0e30a9c4bd7fa0279e156709e078b81fcab\nCloses-bug: #1353942\n'}, {'number': 2, 'created': '2014-09-09 17:08:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/548c2ab62879dcbb9ee5efd6bd6e8d19859d3806', 'message': 'Fixed adopt failure for stack with resource group\n\nAdded support to adopt a resource group and corresponding\nunit test.\n\nChange-Id: Icfeff0e30a9c4bd7fa0279e156709e078b81fcab\nCloses-bug: #1353942\n'}, {'number': 3, 'created': '2014-09-10 05:49:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/ba8919416f876b12733ae30499f2c1304ff5e1f2', 'message': 'Fixed adopt failure for stack with resource group\n\nAdded support to adopt a resource group and corresponding\nunit test.\n\nChange-Id: Icfeff0e30a9c4bd7fa0279e156709e078b81fcab\nCloses-bug: #1353942\n'}, {'number': 4, 'created': '2014-09-18 12:35:39.000000000', 'files': ['heat/engine/resources/resource_group.py', 'heat/tests/test_resource_group.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/fc8dd31ff79ba2c0c610e7a6fad450eb10acae93', 'message': 'Fixed adopt failure for stack with resource group\n\nAdded support to adopt a resource group and corresponding\nunit test.\n\nChange-Id: Icfeff0e30a9c4bd7fa0279e156709e078b81fcab\nCloses-bug: #1353942\n'}]",6,119355,fc8dd31ff79ba2c0c610e7a6fad450eb10acae93,23,7,4,1633,,,0,"Fixed adopt failure for stack with resource group

Added support to adopt a resource group and corresponding
unit test.

Change-Id: Icfeff0e30a9c4bd7fa0279e156709e078b81fcab
Closes-bug: #1353942
",git fetch https://review.opendev.org/openstack/heat refs/changes/55/119355/3 && git format-patch -1 --stdout FETCH_HEAD,"['heat/engine/resources/resource_group.py', 'heat/tests/test_resource_group.py']",2,70d15878b74e1a3776dd8863d51d803f9ac9ace7,bug/1353942," def test_adopt(self): resg = self._create_dummy_stack() adopt_data = { ""status"": ""COMPLETE"", ""name"": ""group1"", ""resource_data"": {}, ""metadata"": {}, ""resource_id"": ""test-group1-id"", ""action"": ""CREATE"", ""type"": ""OS::Heat::ResourceGroup"", ""resources"": { ""0"": { ""status"": ""COMPLETE"", ""name"": ""0"", ""resource_data"": {}, ""resource_id"": ""dummy-0"", ""action"": ""CREATE"", ""type"": ""dummy.resource"", ""metadata"": {} }, ""1"": { ""status"": ""COMPLETE"", ""name"": ""1"", ""resource_data"": {}, ""resource_id"": ""dummy-1"", ""action"": ""CREATE"", ""type"": ""dummy.resource"", ""metadata"": {} } } } adopt = scheduler.TaskRunner(resg.adopt, adopt_data) adopt() self.assertEqual((resg.ADOPT, resg.COMPLETE), resg.state)",,54,0
openstack%2Fkeystone~master~I6db54d8f6114337d37be3cab20f60d7905243cba,openstack/keystone,master,I6db54d8f6114337d37be3cab20f60d7905243cba,Fail on empty userId/username before query,MERGED,2014-09-11 06:31:35.000000000,2014-09-27 17:43:36.000000000,2014-09-27 17:43:35.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 5046}, {'_account_id': 7787}, {'_account_id': 10765}]","[{'number': 1, 'created': '2014-09-11 06:31:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/29535d0e736969f5968b9336e4b5aab360ba24aa', 'message': ""Fail on empty userId/username before query\n\nRather than attempting to query the database for an empty userId\nthat results in an error, just raise an error immediately that\nhas a better error message than one indicating a missing 'len()'\nattribute.\n\nChange-Id: I6db54d8f6114337d37be3cab20f60d7905243cba\nCloses-Bug: #1368046\n""}, {'number': 2, 'created': '2014-09-15 05:45:36.000000000', 'files': ['keystone/tests/test_auth.py', 'keystone/token/controllers.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/ad312be44f32b6db1c2182baae67b8c4e840f406', 'message': ""Fail on empty userId/username before query\n\nRather than attempting to query the database for an empty userId\nthat results in an error, just raise an error immediately that\nhas a better error message than one indicating a missing 'len()'\nattribute.\n\nChange-Id: I6db54d8f6114337d37be3cab20f60d7905243cba\nCloses-Bug: #1368046\n""}]",0,120705,ad312be44f32b6db1c2182baae67b8c4e840f406,15,5,2,7787,,,0,"Fail on empty userId/username before query

Rather than attempting to query the database for an empty userId
that results in an error, just raise an error immediately that
has a better error message than one indicating a missing 'len()'
attribute.

Change-Id: I6db54d8f6114337d37be3cab20f60d7905243cba
Closes-Bug: #1368046
",git fetch https://review.opendev.org/openstack/keystone refs/changes/05/120705/2 && git format-patch -1 --stdout FETCH_HEAD,['keystone/token/controllers.py'],1,29535d0e736969f5968b9336e4b5aab360ba24aa,bug/1368046," if (not auth['passwordCredentials'].get(""userId"") and not auth['passwordCredentials'].get(""username"")):"," if (""userId"" not in auth['passwordCredentials'] and ""username"" not in auth['passwordCredentials']):",2,2
openstack%2Fheat~master~I34e662c554ea66c6df6ebee6ce3bfb0dc3f8a948,openstack/heat,master,I34e662c554ea66c6df6ebee6ce3bfb0dc3f8a948,Include credentials for heat calling self,MERGED,2014-09-23 00:14:33.000000000,2014-09-27 16:39:07.000000000,2014-09-27 16:39:06.000000000,"[{'_account_id': 3}, {'_account_id': 4257}, {'_account_id': 4571}, {'_account_id': 4715}, {'_account_id': 8289}, {'_account_id': 8871}, {'_account_id': 9542}]","[{'number': 1, 'created': '2014-09-23 00:14:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/ffa85ee1f844b469fcb917bc27346b5da4b155e6', 'message': 'Include credentials for heat calling self\n\nWhen in standalone mode, heat needs to include heat credentials\nwhen calling its own REST API.\n\nThis change assumes that heat is running in standalone mode if\nheat.conf [clients_heat] url has been set.\n\nChange-Id: I34e662c554ea66c6df6ebee6ce3bfb0dc3f8a948\nCloses-Bug: #1302578\n'}, {'number': 2, 'created': '2014-09-23 04:46:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/a708cf9ad859d394a39d7f8be7254fd0e91c05c3', 'message': 'Include credentials for heat calling self\n\nWhen in standalone mode, heat needs to include heat credentials\nwhen calling its own REST API.\n\nThis change assumes that heat is running in standalone mode if\nheat.conf [clients_heat] url has been set.\n\nChange-Id: I34e662c554ea66c6df6ebee6ce3bfb0dc3f8a948\nCloses-Bug: #1302578\n'}, {'number': 3, 'created': '2014-09-26 02:07:16.000000000', 'files': ['heat/engine/clients/os/heat_plugin.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/b61a45c13ffa82356f896f0914d2f28dabea7a7f', 'message': 'Include credentials for heat calling self\n\nWhen in standalone mode, heat needs to include heat credentials\nwhen calling its own REST API.\n\nThis change assumes that heat is running in standalone mode if\nheat.conf [clients_heat] url has been set.\n\nChange-Id: I34e662c554ea66c6df6ebee6ce3bfb0dc3f8a948\nCloses-Bug: #1302578\n'}]",0,123292,b61a45c13ffa82356f896f0914d2f28dabea7a7f,20,7,3,4571,,,0,"Include credentials for heat calling self

When in standalone mode, heat needs to include heat credentials
when calling its own REST API.

This change assumes that heat is running in standalone mode if
heat.conf [clients_heat] url has been set.

Change-Id: I34e662c554ea66c6df6ebee6ce3bfb0dc3f8a948
Closes-Bug: #1302578
",git fetch https://review.opendev.org/openstack/heat refs/changes/92/123292/1 && git format-patch -1 --stdout FETCH_HEAD,['heat/engine/clients/os/heat_plugin.py'],1,ffa85ee1f844b469fcb917bc27346b5da4b155e6,bug/1302578," if self._get_client_option('heat', 'url'): # assume that the heat API URL is manually configured because # it is not in the keystone catalog, so include the credentials # for the standalone auth_password middleware args['username'] = self.context.username args['password'] = self.context.password del(args['token']) ",,8,0
openstack%2Fheat~master~I9c19c1ca49f72ff1e760e849712c8c9d5fa7541f,openstack/heat,master,I9c19c1ca49f72ff1e760e849712c8c9d5fa7541f,Add keystone v2 keypair methods,MERGED,2014-09-23 00:14:32.000000000,2014-09-27 16:38:58.000000000,2014-09-27 16:38:57.000000000,"[{'_account_id': 3}, {'_account_id': 3098}, {'_account_id': 4257}, {'_account_id': 4571}, {'_account_id': 4715}, {'_account_id': 8289}]","[{'number': 1, 'created': '2014-09-23 00:14:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/71c682f263e412559bdf2b3b0a84fe3544424740', 'message': 'Add keystone v2 keypair methods\n\nTo get KeystoneClientV2 to work with StackUser resources, the\nmissing methods create_stack_domain_user_keypair and\ncreate_stack_domain_user_keypair have been added.\n\nStackUser._create_keypair has also been modified to tolerate\nhaving a missing id attribute, which is what the v2 API returns.\n\nChange-Id: I9c19c1ca49f72ff1e760e849712c8c9d5fa7541f\nCloses-Bug: #1302578\n'}, {'number': 2, 'created': '2014-09-23 04:46:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/3a8a4a9d86a2975f72ab36038a8344e5d45c1b27', 'message': 'Add keystone v2 keypair methods\n\nTo get KeystoneClientV2 to work with StackUser resources, the\nmissing methods create_stack_domain_user_keypair and\ncreate_stack_domain_user_keypair have been added.\n\nStackUser._create_keypair has also been modified to tolerate\nhaving a missing id attribute, which is what the v2 API returns.\n\nChange-Id: I9c19c1ca49f72ff1e760e849712c8c9d5fa7541f\nCloses-Bug: #1372687\n'}, {'number': 3, 'created': '2014-09-26 02:07:16.000000000', 'files': ['heat/engine/stack_user.py', 'contrib/heat_keystoneclient_v2/heat_keystoneclient_v2/client.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/d9eebfdbc6116f9e1442aa5d201d7e961225360a', 'message': 'Add keystone v2 keypair methods\n\nTo get KeystoneClientV2 to work with StackUser resources, the\nmissing methods create_stack_domain_user_keypair and\ndelete_stack_domain_user_keypair have been added.\n\nStackUser._create_keypair has also been modified to tolerate\nhaving a missing id attribute, which is what the v2 API returns.\n\nChange-Id: I9c19c1ca49f72ff1e760e849712c8c9d5fa7541f\nCloses-Bug: #1372687\n'}]",2,123291,d9eebfdbc6116f9e1442aa5d201d7e961225360a,19,6,3,4571,,,0,"Add keystone v2 keypair methods

To get KeystoneClientV2 to work with StackUser resources, the
missing methods create_stack_domain_user_keypair and
delete_stack_domain_user_keypair have been added.

StackUser._create_keypair has also been modified to tolerate
having a missing id attribute, which is what the v2 API returns.

Change-Id: I9c19c1ca49f72ff1e760e849712c8c9d5fa7541f
Closes-Bug: #1372687
",git fetch https://review.opendev.org/openstack/heat refs/changes/91/123291/1 && git format-patch -1 --stdout FETCH_HEAD,"['heat/engine/stack_user.py', 'contrib/heat_keystoneclient_v2/heat_keystoneclient_v2/client.py']",2,71c682f263e412559bdf2b3b0a84fe3544424740,bug/1302578," def create_stack_domain_user_keypair(self, user_id, project_id): return self.create_ec2_keypair(user_id) def delete_stack_domain_user_keypair(self, user_id, project_id, credential_id): return self.delete_ec2_keypair(user_id, credential_id) ",,14,1
openstack%2Fheat~master~Iedff00b382b4fcda741ca5c9b4adc23b176ec48c,openstack/heat,master,Iedff00b382b4fcda741ca5c9b4adc23b176ec48c,Associate floating IP with router interface,MERGED,2014-09-19 02:09:50.000000000,2014-09-27 16:38:49.000000000,2014-09-27 16:38:48.000000000,"[{'_account_id': 3}, {'_account_id': 4257}, {'_account_id': 4571}, {'_account_id': 7193}, {'_account_id': 8289}, {'_account_id': 9542}]","[{'number': 1, 'created': '2014-09-19 02:09:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/8b3e5c0e61da03311c66c4fe54f5de82129cece8', 'message': ""Associate floating IP with router interface\n\nThis change will create a dependency from a FloatingIP to a RouterInterface in\nthis template which interfaces with the same subnet that this FloatingIP's\nport is assigned to.\n\nIt would be preferable to add the dependency based on matching FloatingIP\nfloating_network_id and Router external_gateway_info network, but there is\na valid use-case for the Router being external to the template, so the\ndependency is matched on the internal subnet instead, which is available\nfrom the RouterInterface property.\n\nChange-Id: Iedff00b382b4fcda741ca5c9b4adc23b176ec48c\nCloses-Bug: #1299259\n""}, {'number': 2, 'created': '2014-09-21 21:23:58.000000000', 'files': ['heat/engine/resources/neutron/floatingip.py', 'heat/tests/test_neutron.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/5c158edac6950899c41d3090cde8c636e63c4a2f', 'message': ""Associate floating IP with router interface\n\nThis change will create a dependency from a FloatingIP to a RouterInterface in\nthis template which interfaces with the same subnet that this FloatingIP's\nport is assigned to.\n\nIt would be preferable to add the dependency based on matching FloatingIP\nfloating_network_id and Router external_gateway_info network, but there is\na valid use-case for the Router being external to the template, so the\ndependency is matched on the internal subnet instead, which is available\nfrom the RouterInterface property.\n\nChange-Id: Iedff00b382b4fcda741ca5c9b4adc23b176ec48c\nCloses-Bug: #1299259\n""}]",2,122592,5c158edac6950899c41d3090cde8c636e63c4a2f,18,6,2,4571,,,0,"Associate floating IP with router interface

This change will create a dependency from a FloatingIP to a RouterInterface in
this template which interfaces with the same subnet that this FloatingIP's
port is assigned to.

It would be preferable to add the dependency based on matching FloatingIP
floating_network_id and Router external_gateway_info network, but there is
a valid use-case for the Router being external to the template, so the
dependency is matched on the internal subnet instead, which is available
from the RouterInterface property.

Change-Id: Iedff00b382b4fcda741ca5c9b4adc23b176ec48c
Closes-Bug: #1299259
",git fetch https://review.opendev.org/openstack/heat refs/changes/92/122592/2 && git format-patch -1 --stdout FETCH_HEAD,"['heat/engine/resources/neutron/floatingip.py', 'heat/tests/test_neutron.py']",2,8b3e5c0e61da03311c66c4fe54f5de82129cece8,bug/1299259," ""router_interface"": { ""Type"": ""OS::Neutron::RouterInterface"", ""Properties"": { ""router_id"": { ""Ref"" : ""router"" }, ""subnet"": ""sub1234"" } }, ""router_interface"": { ""Type"": ""OS::Neutron::RouterInterface"", ""Properties"": { ""router_id"": { ""Ref"" : ""router"" }, ""subnet"": ""sub1234"" } }, deps = stack.dependencies[stack['router_interface']] self.assertIn(stack['floating_ip'], deps) ",,48,2
openstack%2Fheat~master~I0a05b8cf916267338e4bcb6a3e12bc2cdcdc3d48,openstack/heat,master,I0a05b8cf916267338e4bcb6a3e12bc2cdcdc3d48,"FloatingIP updateable port_id, fixed_ip_address",MERGED,2014-09-18 23:47:15.000000000,2014-09-27 16:38:39.000000000,2014-09-27 16:38:38.000000000,"[{'_account_id': 3}, {'_account_id': 4257}, {'_account_id': 4571}, {'_account_id': 4715}, {'_account_id': 7193}, {'_account_id': 7227}, {'_account_id': 8289}]","[{'number': 1, 'created': '2014-09-18 23:47:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/2713fb2e06212c4364462e9a1b799d2097dc68d0', 'message': 'FloatingIP updateable port_id, fixed_ip_address\n\nThis change implements handle_update for FloatingIP.\n\nFloatingIPAssociation already has handle_update implemented, so\nthis change is just fixing a gap.\n\nUpdateable FloatingIP will be vital if ports end up getting replaced\non every update as per I558db6bac196f49e5c488a577f0580c934b06747\n\nChange-Id: I0a05b8cf916267338e4bcb6a3e12bc2cdcdc3d48\nCloses-Bug: #1369748\nRelated-Bug: #1301486\n'}, {'number': 2, 'created': '2014-09-21 21:53:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/fe96dd8baf6b02217a6cb6bbc61a2888d09c5243', 'message': 'FloatingIP updateable port_id, fixed_ip_address\n\nThis change implements handle_update for FloatingIP.\n\nFloatingIPAssociation already has handle_update implemented, so\nthis change is just fixing a gap.\n\nUpdateable FloatingIP will be vital if ports end up getting replaced\non every update as per I558db6bac196f49e5c488a577f0580c934b06747\n\nChange-Id: I0a05b8cf916267338e4bcb6a3e12bc2cdcdc3d48\nCloses-Bug: #1369748\nRelated-Bug: #1301486\n'}, {'number': 3, 'created': '2014-09-21 23:20:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/77a7dde2a95c84c878fc814582cf144ecc5d7098', 'message': 'FloatingIP updateable port_id, fixed_ip_address\n\nThis change implements handle_update for FloatingIP.\n\nFloatingIPAssociation already has handle_update implemented, so\nthis change is just fixing a gap.\n\nUpdateable FloatingIP will be vital if ports end up getting replaced\non every update as per I558db6bac196f49e5c488a577f0580c934b06747\n\nChange-Id: I0a05b8cf916267338e4bcb6a3e12bc2cdcdc3d48\nCloses-Bug: #1369748\nRelated-Bug: #1301486\n'}, {'number': 4, 'created': '2014-09-26 01:43:14.000000000', 'files': ['heat/engine/resources/neutron/floatingip.py', 'heat/tests/test_neutron.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/d0d925f45170634a7ef8b11f15b7971513fa9227', 'message': 'FloatingIP updateable port_id, fixed_ip_address\n\nThis change implements handle_update for FloatingIP.\n\nFloatingIPAssociation already has handle_update implemented, so\nthis change is just fixing a gap.\n\nUpdateable FloatingIP will be vital if ports end up getting replaced\non every update as per I558db6bac196f49e5c488a577f0580c934b06747\n\nChange-Id: I0a05b8cf916267338e4bcb6a3e12bc2cdcdc3d48\nCloses-Bug: #1369748\nRelated-Bug: #1301486\n'}]",4,122568,d0d925f45170634a7ef8b11f15b7971513fa9227,24,7,4,4571,,,0,"FloatingIP updateable port_id, fixed_ip_address

This change implements handle_update for FloatingIP.

FloatingIPAssociation already has handle_update implemented, so
this change is just fixing a gap.

Updateable FloatingIP will be vital if ports end up getting replaced
on every update as per I558db6bac196f49e5c488a577f0580c934b06747

Change-Id: I0a05b8cf916267338e4bcb6a3e12bc2cdcdc3d48
Closes-Bug: #1369748
Related-Bug: #1301486
",git fetch https://review.opendev.org/openstack/heat refs/changes/68/122568/4 && git format-patch -1 --stdout FETCH_HEAD,"['heat/engine/resources/neutron/floatingip.py', 'heat/tests/test_neutron.py']",2,2713fb2e06212c4364462e9a1b799d2097dc68d0,bug/1369748,"neutron_floating_no_assoc_template = ''' { ""AWSTemplateFormatVersion"" : ""2010-09-09"", ""Description"" : ""Template to test Neutron resources"", ""Parameters"" : {}, ""Resources"" : { ""port_floating"": { ""Type"": ""OS::Neutron::Port"", ""Properties"": { ""network"": ""xyz1234"", ""fixed_ips"": [{ ""subnet"": ""sub1234"", ""ip_address"": ""10.0.0.10"" }] } }, ""floating_ip"": { ""Type"": ""OS::Neutron::FloatingIP"", ""Properties"": { ""floating_network"": ""abcd1234"", ""port_id"": { ""Ref"" : ""port_floating"" } } }, ""router"": { ""Type"": ""OS::Neutron::Router"" }, ""gateway"": { ""Type"": ""OS::Neutron::RouterGateway"", ""Properties"": { ""router_id"": { ""Ref"" : ""router"" }, ""network"": ""abcd1234"" } } } } ''' def test_floatip_association_port(self): def test_floatip_port(self): neutronV20.find_resourceid_by_name_or_id( mox.IsA(neutronclient.Client), 'network', 'xyz1234' ).AndReturn('xyz1234') neutronV20.find_resourceid_by_name_or_id( mox.IsA(neutronclient.Client), 'subnet', 'sub1234' ).AndReturn('sub1234') neutronclient.Client.create_port({'port': { 'network_id': u'xyz1234', 'fixed_ips': [ {'subnet_id': u'sub1234', 'ip_address': u'10.0.0.10'} ], 'name': utils.PhysName('test_stack', 'port_floating'), 'admin_state_up': True}} ).AndReturn({'port': { ""status"": ""BUILD"", ""id"": ""fc68ea2c-b60b-4b4f-bd82-94ec81110766"" }}) neutronclient.Client.show_port( 'fc68ea2c-b60b-4b4f-bd82-94ec81110766' ).AndReturn({'port': { ""status"": ""ACTIVE"", ""id"": ""fc68ea2c-b60b-4b4f-bd82-94ec81110766"" }}) neutronV20.find_resourceid_by_name_or_id( mox.IsA(neutronclient.Client), 'network', 'abcd1234' ).AndReturn('abcd1234') neutronclient.Client.create_floatingip({ 'floatingip': { 'floating_network_id': u'abcd1234', 'port_id': u'fc68ea2c-b60b-4b4f-bd82-94ec81110766' } }).AndReturn({'floatingip': { ""status"": ""ACTIVE"", ""id"": ""fc68ea2c-b60b-4b4f-bd82-94ec81110766"" }}) # update as with port_id neutronclient.Client.update_floatingip( 'fc68ea2c-b60b-4b4f-bd82-94ec81110766', { 'floatingip': { 'port_id': u'2146dfbf-ba77-4083-8e86-d052f671ece5', 'fixed_ip_address': None}} ).AndReturn({'floatingip': { ""status"": ""ACTIVE"", ""id"": ""fc68ea2c-b60b-4b4f-bd82-94ec81110766"" }}) neutronclient.Client.delete_floatingip( 'fc68ea2c-b60b-4b4f-bd82-94ec81110766' ).AndReturn(None) neutronclient.Client.delete_port( 'fc68ea2c-b60b-4b4f-bd82-94ec81110766' ).AndReturn(None) neutronclient.Client.show_port( 'fc68ea2c-b60b-4b4f-bd82-94ec81110766' ).AndRaise(qe.PortNotFoundClient(status_code=404)) self.m.ReplayAll() t = template_format.parse(neutron_floating_no_assoc_template) stack = utils.parse_stack(t) p = stack['port_floating'] scheduler.TaskRunner(p.create)() self.assertEqual((p.CREATE, p.COMPLETE), p.state) fip = stack['floating_ip'] scheduler.TaskRunner(fip.create)() self.assertEqual((fip.CREATE, fip.COMPLETE), fip.state) # test update FloatingIp with port_id props = copy.deepcopy(fip.properties.data) update_port_id = '2146dfbf-ba77-4083-8e86-d052f671ece5' props['port_id'] = update_port_id update_snippet = rsrc_defn.ResourceDefinition(fip.name, fip.type(), stack.t.parse(stack, props)) scheduler.TaskRunner(fip.update, update_snippet)() self.assertEqual((fip.UPDATE, fip.COMPLETE), fip.state) scheduler.TaskRunner(fip.delete)() scheduler.TaskRunner(p.delete)() self.m.VerifyAll() ", def test_floatip_port(self):,154,3
openstack%2Ftempest~master~Ie3bba6d3c744a19380af49a1b81b34bce98f93c3,openstack/tempest,master,Ie3bba6d3c744a19380af49a1b81b34bce98f93c3,test_tokens_nocatalog to identity v3 test_tokens,ABANDONED,2014-05-10 05:20:24.000000000,2014-09-27 16:35:20.000000000,,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 1795}, {'_account_id': 2222}, {'_account_id': 2238}, {'_account_id': 5196}, {'_account_id': 5689}, {'_account_id': 5803}, {'_account_id': 6455}, {'_account_id': 7227}, {'_account_id': 7293}, {'_account_id': 7350}, {'_account_id': 7428}, {'_account_id': 8556}, {'_account_id': 8824}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 12355}]","[{'number': 1, 'created': '2014-05-10 05:20:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f0299063f70f00be0d5c5559724c48373d7a8f1c', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 2, 'created': '2014-05-12 12:47:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/56573cfb983b37e8b4cd93416ac138c67cfc674a', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 3, 'created': '2014-05-12 18:41:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/e9afe4915f8d47039ea0e33c5f9eb26ff55576c3', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 4, 'created': '2014-05-16 15:05:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f656207c2bb5e6bce86e013e763fcd05b28b85e3', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\nTo 'get_users' method in XML identity_client added a argument\n'params' to provide an option to filter the response of\nget_users response based on the input parameter sent from\ntest case such as user name.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 5, 'created': '2014-05-29 11:35:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5f692859049399b4680c8b2f87248ecb0e93832d', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\nTo 'get_users' method in XML identity_client added a argument\n'params' to provide an option to filter the response of\nget_users response based on the input parameter sent from\ntest case such as user name.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 6, 'created': '2014-06-02 08:35:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0134aeb0e5a5c91fbb30811f4ce7852c731c73d0', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\nTo 'get_users' method in XML identity_client added a argument\n'params' to provide an option to filter the response of\nget_users response based on the input parameter sent from\ntest case such as user name.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 7, 'created': '2014-06-11 11:32:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/bbd3cc29c44c74717bb7ea07664febe1668ca738', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\nTo 'get_users' method in XML identity_client added a argument\n'params' to provide an option to filter the response of\nget_users response based on the input parameter sent from\ntest case such as user name.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 8, 'created': '2014-06-16 14:11:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/142e6bc0dbc124c362e71803f577d3851ea95e0c', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function everywhere in the file.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\nTo 'get_users' method in XML identity_client added a argument\n'params' to provide an option to filter the response of\nget_users response based on the input parameter sent from\ntest case such as user name.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 9, 'created': '2014-06-16 16:47:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d67e1d31592652893227993089ced9d9c1cb15a5', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case 'test_tokens'.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\nTo 'get_users' method in XML identity_client added a argument\n'params' to provide an option to filter the response of\nget_users response based on the input parameter sent from\ntest case such as user name.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 10, 'created': '2014-06-24 11:33:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/bdd2acbe9019e0ffe821bcbaecb7e36e361b6e34', 'message': 'test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case \'test_tokens\'.\n\nAlso adds related logic to \'get_token\' and \'auth\' methods of JSON and\nXML identity_clients.\nModified \'get_users\' and added \'_parse_users\' method to XML\nidentity client.\nTo \'get_users\' method in XML identity_client added a argument\n\'params\' to provide an option to filter the response of\nget_users response based on the input parameter sent from\ntest case such as user name.\nModified self.addCleanup and also refactored assert\nstatements of existing testcase ""test_tokens"" as\n(expected, actual) format.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n'}, {'number': 11, 'created': '2014-06-24 12:28:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/a61519609edf91468e9a7ae440fe39f4f335344e', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case 'test_tokens' ans also refactored assert\nstatements as (expected, actual) format.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\nTo 'get_users' method in XML identity_client added a argument\n'params' to provide an option to filter the response of\nget_users response based on the input parameter sent from\ntest case such as user name.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 12, 'created': '2014-06-24 12:34:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8f95f47603752b64e55af8b626eeb4653f5d5ebf', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case 'test_tokens' and also assert\nstatements as (expected, actual) format.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\nTo 'get_users' method in XML identity_client added a argument\n'params' to provide an option to filter the response of\nget_users response based on the input parameter sent from\ntest case such as user name.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 13, 'created': '2014-06-25 08:39:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8d9724e5263cc434f752a63ae9fa1d0edc81dd3c', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case 'test_tokens' and also assert\nstatements as (expected, actual) format.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\nTo 'get_users' method in XML identity_client added a argument\n'params' to provide an option to filter the response of\nget_users response based on the input parameter sent from\ntest case such as user name.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 14, 'created': '2014-07-15 12:41:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c02befb8c38f599aca71a6f9e468d4de15df7d18', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case 'test_tokens' and also assert\nstatements as (expected, actual) format.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\nTo 'get_users' method in XML identity_client added a argument\n'params' to provide an option to filter the response of\nget_users response based on the input parameter sent from\ntest case such as user name.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 15, 'created': '2014-07-15 12:49:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d2b58f0907c7a57544a4eb799ec301eebba1f8a3', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case 'test_tokens' and also assert\nstatements as (expected, actual) format.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\nModified 'get_users' and added '_parse_users' method to XML\nidentity client.\nTo 'get_users' method in XML identity_client added a argument\n'params' to provide an option to filter the response of\nget_users response based on the input parameter sent from\ntest case such as user name.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 16, 'created': '2014-07-24 16:40:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/502fcc993dac31065316741b780d3dd5c60fa291', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case 'test_tokens' and also assert\nstatements as (expected, actual) format.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 17, 'created': '2014-08-04 05:28:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/eda542421a51194904aa058a0d98309f62835899', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case 'test_tokens' and also assert\nstatements as (expected, actual) format.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 18, 'created': '2014-08-12 05:33:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ba15e523242d4755fb188481e1e39d75e37ce5f5', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case 'test_tokens' and also assert\nstatements as (expected, actual) format.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 19, 'created': '2014-09-01 06:16:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/05fe9c0ca3e82c02a790dc7fb973db7b0134e0ff', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case 'test_tokens' and also assert\nstatements as (expected, actual) format.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 20, 'created': '2014-09-10 08:19:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/87aba37a09bf3785d0c84d93ccd9d39e9a0747fc', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case 'test_tokens' and also assert\nstatements as (expected, actual) format.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}, {'number': 21, 'created': '2014-09-15 06:15:47.000000000', 'files': ['tempest/api/identity/admin/v3/test_tokens.py', 'tempest/services/identity/v3/json/identity_client.py', 'tempest/services/identity/v3/xml/identity_client.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/c8087bf94e69395263485111efb932956ae68098', 'message': ""test_tokens_nocatalog to identity v3 test_tokens\n\nAdds test_tokens_nocatalog test case to identity v3 test_tokens script.\n\nAdds a method _delete_token to delete token and assert\nresponse code and NotFound exception after delete of token.\nThis method is called as part of addCleanup when\ntoken is posted in test case test_tokens_nocatalog. Hence\ndid refactoring to use this helper function also in the existing\ntest case 'test_tokens' and also assert\nstatements as (expected, actual) format.\n\nAlso adds related logic to 'get_token' and 'auth' methods of JSON and\nXML identity_clients.\n\nChange-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3\n""}]",53,93172,c8087bf94e69395263485111efb932956ae68098,184,19,21,1795,,,0,"test_tokens_nocatalog to identity v3 test_tokens

Adds test_tokens_nocatalog test case to identity v3 test_tokens script.

Adds a method _delete_token to delete token and assert
response code and NotFound exception after delete of token.
This method is called as part of addCleanup when
token is posted in test case test_tokens_nocatalog. Hence
did refactoring to use this helper function also in the existing
test case 'test_tokens' and also assert
statements as (expected, actual) format.

Also adds related logic to 'get_token' and 'auth' methods of JSON and
XML identity_clients.

Change-Id: Ie3bba6d3c744a19380af49a1b81b34bce98f93c3
",git fetch https://review.opendev.org/openstack/tempest refs/changes/72/93172/20 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/identity/admin/v3/test_tokens.py', 'tempest/services/identity/v3/json/identity_client.py', 'tempest/services/identity/v3/xml/identity_client.py']",3,f0299063f70f00be0d5c5559724c48373d7a8f1c,keystone/token_nocatalog," def _parse_users(self, node): array = [] for child in node.getchildren(): tag_list = child.tag.split('}', 1) if tag_list[1] == ""user"": array.append(common.xml_to_json(child)) return array def get_users(self, params=None): url = 'users' if params: url += '?%s' % urllib.urlencode(params) resp, body = self.get(url) body = self._parse_users(etree.fromstring(body)) def get_token(self, resp_token, **kwargs): url = ""auth/tokens"" if kwargs.get('nocatalog'): url = ""auth/tokens?nocatalog"" resp, body = self.get(url, headers=headers) domain=None, token=None, nocatalog=False): if nocatalog: self.auth_url = self.auth_url + '?nocatalog'"," def get_users(self): resp, body = self.get(""users"") body = self._parse_array(etree.fromstring(body)) def get_token(self, resp_token): resp, body = self.get(""auth/tokens"", headers=headers) domain=None, token=None):",60,9
openstack%2Fheat~master~I2a10c14772bdafc001e211d7e94502ac1f6b32b1,openstack/heat,master,I2a10c14772bdafc001e211d7e94502ac1f6b32b1,Add an option to disable cloud watch lite,MERGED,2014-09-22 03:29:21.000000000,2014-09-27 16:27:12.000000000,2014-09-27 16:27:11.000000000,"[{'_account_id': 3}, {'_account_id': 4257}, {'_account_id': 4328}, {'_account_id': 4571}, {'_account_id': 4715}, {'_account_id': 6899}, {'_account_id': 8246}, {'_account_id': 8289}, {'_account_id': 9542}]","[{'number': 1, 'created': '2014-09-22 03:29:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/522980cb9175e6860b806d97197ae1be48964496', 'message': 'Add an option to disable cloud watch lite\n\nThis also adds a deprecation warning saying it will be\nremoved in Kilo.\n\nRelease message:\nAnyone deploying Heat should not be using OS::Heat::CWLiteAlarm, but\nOS::Ceilometer::Alarm.\nCWLiteAlarm should be explictly disabled in /etc/heat/heat.conf by\nsetting ""enable_cloud_watch_lite=false"". This will stop Heat from\nrunning a period task check for alarms.\n\nDocImpact\nChange-Id: I2a10c14772bdafc001e211d7e94502ac1f6b32b1\nCloses-bug: #1322128\n'}, {'number': 2, 'created': '2014-09-24 12:43:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/86affe42edbb139513d42e6cdd5e8d3f6585d02f', 'message': 'Add an option to disable cloud watch lite\n\nThis also adds a deprecation warning.\n\nRelease message:\nAnyone deploying Heat should not be using OS::Heat::CWLiteAlarm, but\nOS::Ceilometer::Alarm.\nCWLiteAlarm should be explictly disabled in /etc/heat/heat.conf by\nsetting ""enable_cloud_watch_lite=false"". This will stop Heat from\nrunning a period task check for alarms.\n\nDocImpact\nChange-Id: I2a10c14772bdafc001e211d7e94502ac1f6b32b1\nCloses-bug: #1322128\n'}, {'number': 3, 'created': '2014-09-24 22:27:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/056ef483a53dadc52f3d7276710393987cdf9833', 'message': 'Add an option to disable cloud watch lite\n\nThis also adds a deprecation warning.\nThis also changes the default to use Ceilometer.\n\nRelease message:\nAnyone deploying Heat should not be using OS::Heat::CWLiteAlarm, but\nOS::Ceilometer::Alarm.\nCWLiteAlarm should be explictly disabled in /etc/heat/heat.conf by\nsetting ""enable_cloud_watch_lite=false"". This will stop Heat from\nrunning a period task check for alarms.\n\nDocImpact\nChange-Id: I2a10c14772bdafc001e211d7e94502ac1f6b32b1\nCloses-bug: #1322128\n'}, {'number': 4, 'created': '2014-09-25 09:42:57.000000000', 'files': ['etc/heat/heat.conf.sample', 'heat/engine/resources/cloud_watch.py', 'heat/tests/common.py', 'heat/tests/test_engine_service.py', 'heat/engine/environment.py', 'heat/tests/test_provider_template.py', 'etc/heat/environment.d/default.yaml', 'bin/heat-engine', 'heat/common/config.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/33eb87b3e2d8da3ba11901c4762c19b0fc740dab', 'message': 'Add an option to disable cloud watch lite\n\nThis also adds a deprecation warning.\nThis also changes the default to use Ceilometer.\n\nRelease message:\nAnyone deploying Heat should not be using OS::Heat::CWLiteAlarm, but\nOS::Ceilometer::Alarm.\nCWLiteAlarm should be explictly disabled in /etc/heat/heat.conf by\nsetting ""enable_cloud_watch_lite=false"". This will stop Heat from\nrunning a period task check for alarms.\n\nDocImpact\nChange-Id: I2a10c14772bdafc001e211d7e94502ac1f6b32b1\nCloses-bug: #1322128\n'}]",2,123039,33eb87b3e2d8da3ba11901c4762c19b0fc740dab,27,9,4,4715,,,0,"Add an option to disable cloud watch lite

This also adds a deprecation warning.
This also changes the default to use Ceilometer.

Release message:
Anyone deploying Heat should not be using OS::Heat::CWLiteAlarm, but
OS::Ceilometer::Alarm.
CWLiteAlarm should be explictly disabled in /etc/heat/heat.conf by
setting ""enable_cloud_watch_lite=false"". This will stop Heat from
running a period task check for alarms.

DocImpact
Change-Id: I2a10c14772bdafc001e211d7e94502ac1f6b32b1
Closes-bug: #1322128
",git fetch https://review.opendev.org/openstack/heat refs/changes/39/123039/2 && git format-patch -1 --stdout FETCH_HEAD,"['etc/heat/heat.conf.sample', 'heat/engine/resources/cloud_watch.py', 'bin/heat-engine', 'heat/common/config.py']",4,522980cb9175e6860b806d97197ae1be48964496,ceilo-alarm," cfg.BoolOpt('enable_cloud_watch_lite', default=True, help=_('Enable the legacy OS::Heat::CWLiteAlarm resource.')),",,25,6
openstack%2Fheat~master~Id3577c8af3f0433577b1ec7b5068585a74f4c3cc,openstack/heat,master,Id3577c8af3f0433577b1ec7b5068585a74f4c3cc,Pass the correct matching_metadata to Ceilometer,MERGED,2014-09-25 09:42:57.000000000,2014-09-27 16:27:03.000000000,2014-09-27 16:27:02.000000000,"[{'_account_id': 3}, {'_account_id': 4257}, {'_account_id': 4571}]","[{'number': 1, 'created': '2014-09-25 09:42:57.000000000', 'files': ['heat/engine/resources/ceilometer/alarm.py', 'heat/tests/test_ceilometer_alarm.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/7555fc9f0446e83aa5964293ab36e741ea06f618', 'message': 'Pass the correct matching_metadata to Ceilometer\n\nWhen using the Cloud Watch Resource Template don\'t really have\nthe json functions to prepend a prefix to the matching_metadata.\n\nThe issue here is the nova metadata get converted by Ceilometer from\n""metering"" to ""user_metadata"" so depending on the meter name we need to\nsearch for different metadata.\n\nChange-Id: Id3577c8af3f0433577b1ec7b5068585a74f4c3cc\n'}]",0,123991,7555fc9f0446e83aa5964293ab36e741ea06f618,8,3,1,4715,,,0,"Pass the correct matching_metadata to Ceilometer

When using the Cloud Watch Resource Template don't really have
the json functions to prepend a prefix to the matching_metadata.

The issue here is the nova metadata get converted by Ceilometer from
""metering"" to ""user_metadata"" so depending on the meter name we need to
search for different metadata.

Change-Id: Id3577c8af3f0433577b1ec7b5068585a74f4c3cc
",git fetch https://review.opendev.org/openstack/heat refs/changes/91/123991/1 && git format-patch -1 --stdout FETCH_HEAD,"['heat/engine/resources/ceilometer/alarm.py', 'heat/tests/test_ceilometer_alarm.py']",2,7555fc9f0446e83aa5964293ab36e741ea06f618,ceilo-alarm," al['matching_metadata'] = dict( ('metadata.metering.%s' % k, v) for k, v in al['matching_metadata'].items())",,45,2
openstack%2Fheat~master~I084d28dfbbd0f2d7a6a18a09600f86880fa394d5,openstack/heat,master,I084d28dfbbd0f2d7a6a18a09600f86880fa394d5,Don't pass empty action strings to ceilometer,MERGED,2014-09-25 09:42:57.000000000,2014-09-27 16:09:33.000000000,2014-09-27 16:09:32.000000000,"[{'_account_id': 3}, {'_account_id': 4257}, {'_account_id': 4571}]","[{'number': 1, 'created': '2014-09-25 09:42:57.000000000', 'files': ['heat/engine/resources/ceilometer/alarm.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/3f73f82a0d4370b6a938fd5d5df6b7c91639d023', 'message': ""Don't pass empty action strings to ceilometer\n\nNote: this only really happens when using the cloud watch\nresource template.\n\nPartial-bug: #1373247\nChange-Id: I084d28dfbbd0f2d7a6a18a09600f86880fa394d5\n""}]",0,123990,3f73f82a0d4370b6a938fd5d5df6b7c91639d023,8,3,1,4715,,,0,"Don't pass empty action strings to ceilometer

Note: this only really happens when using the cloud watch
resource template.

Partial-bug: #1373247
Change-Id: I084d28dfbbd0f2d7a6a18a09600f86880fa394d5
",git fetch https://review.opendev.org/openstack/heat refs/changes/90/123990/1 && git format-patch -1 --stdout FETCH_HEAD,['heat/engine/resources/ceilometer/alarm.py'],1,3f73f82a0d4370b6a938fd5d5df6b7c91639d023,ceilo-alarm, if act: kwargs[k].append(act), kwargs[k].append(act),2,1
openstack%2Fheat~master~I6f4c099bde83fba5ad89e43297503943136b717e,openstack/heat,master,I6f4c099bde83fba5ad89e43297503943136b717e,Make sure that AutoScaling group has it's tag set correctly,MERGED,2014-09-25 09:42:57.000000000,2014-09-27 16:09:24.000000000,2014-09-27 16:09:23.000000000,"[{'_account_id': 3}, {'_account_id': 4257}, {'_account_id': 4571}]","[{'number': 1, 'created': '2014-09-25 09:42:57.000000000', 'files': ['heat/engine/resources/autoscaling.py', 'heat/tests/test_server_tags.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/b0903a5598b6812342e230a03ff0310d068cb450', 'message': ""Make sure that AutoScaling group has it's tag set correctly\n\nAt the moment we have to rely on the base classes metering.groupname\nand this AutoScalingGroupName is effectively lost.\n\nChange-Id: I6f4c099bde83fba5ad89e43297503943136b717e\nPartial-bug: #1373247\n""}]",0,123989,b0903a5598b6812342e230a03ff0310d068cb450,8,3,1,4715,,,0,"Make sure that AutoScaling group has it's tag set correctly

At the moment we have to rely on the base classes metering.groupname
and this AutoScalingGroupName is effectively lost.

Change-Id: I6f4c099bde83fba5ad89e43297503943136b717e
Partial-bug: #1373247
",git fetch https://review.opendev.org/openstack/heat refs/changes/89/123989/1 && git format-patch -1 --stdout FETCH_HEAD,"['heat/engine/resources/autoscaling.py', 'heat/tests/test_server_tags.py']",2,b0903a5598b6812342e230a03ff0310d068cb450,ceilo-alarm, nova_tags['metering.AutoScalingGroupName'] = group_refid, nova_tags['AutoScalingGroupName'] = group_refid,2,2
openstack%2Fheat~master~Ie9ac19f5ab5933054c65c64c53e555d278c3229b,openstack/heat,master,Ie9ac19f5ab5933054c65c64c53e555d278c3229b,Fix barbican plugin registration,MERGED,2014-09-26 11:37:41.000000000,2014-09-27 16:08:31.000000000,2014-09-27 16:08:30.000000000,"[{'_account_id': 3}, {'_account_id': 4715}, {'_account_id': 6577}, {'_account_id': 7385}, {'_account_id': 13009}]","[{'number': 1, 'created': '2014-09-26 11:37:41.000000000', 'files': ['contrib/heat_barbican/setup.cfg'], 'web_link': 'https://opendev.org/openstack/heat/commit/246737bdd0338d3cb448dc3f8136b02f84c1bfc7', 'message': 'Fix barbican plugin registration\n\nAdd client plugin registration and fix packages and files section.\n\nChange-Id: Ie9ac19f5ab5933054c65c64c53e555d278c3229b\nCloses-Bug: 1374097\n'}]",1,124374,246737bdd0338d3cb448dc3f8136b02f84c1bfc7,13,5,1,7385,,,0,"Fix barbican plugin registration

Add client plugin registration and fix packages and files section.

Change-Id: Ie9ac19f5ab5933054c65c64c53e555d278c3229b
Closes-Bug: 1374097
",git fetch https://review.opendev.org/openstack/heat refs/changes/74/124374/1 && git format-patch -1 --stdout FETCH_HEAD,['contrib/heat_barbican/setup.cfg'],1,246737bdd0338d3cb448dc3f8136b02f84c1bfc7,bug/1374097,[files][entry_points] heat.clients = barbican = heat_barbican.client:BarbicanClientPlugin ,[files],5,1
openstack%2Ftrove~master~Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3,openstack/trove,master,Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3,Isolate unit tests from integration tests data,MERGED,2014-09-23 14:34:27.000000000,2014-09-27 16:08:28.000000000,2014-09-27 16:08:27.000000000,"[{'_account_id': 3}, {'_account_id': 4240}, {'_account_id': 6268}, {'_account_id': 6476}, {'_account_id': 7092}, {'_account_id': 8214}, {'_account_id': 8415}, {'_account_id': 9664}]","[{'number': 1, 'created': '2014-09-23 14:34:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/9ff1cd57dfd2f6a7a26d9b6f239f9ed478a77afb', 'message': 'Isolate unit tests from integration tests data\n\nReasons:\n - unit tests are depending on integration tests data, such as:\n  - datastore and its versions;\n - distro package maintainers are not able to package Trove since ice house rc1.\n\nGate behaviour:\n - gate tests are not failing because unit tests are running right after\n   fake-mode tests, so sqlite still exist.\n\nChanges:\n - fixing those unit tests that are failing while building Trove package\n   (more info see at bug-report).\n\nChange-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3\nCloses-Bug: #1302784\n'}, {'number': 2, 'created': '2014-09-23 15:02:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/6819bd72d41f8dc8b2aeba9e172187c0ac7e6734', 'message': 'Isolate unit tests from integration tests data\n\nReasons:\n - unit tests are depending on integration tests data, such as:\n  - datastore and its versions;\n - distro package maintainers are not able to package Trove since ice house rc1.\n\nGate behaviour:\n - gate tests are not failing because unit tests are running right after\n   fake-mode tests, so sqlite still exist.\n\nChanges:\n - fixing those unit tests that are failing while building Trove package\n   (more info see at bug-report).\n\nChange-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3\nCloses-Bug: #1302784\n'}, {'number': 3, 'created': '2014-09-24 10:59:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/4eb5fe7fffffdb96ad3ae04cb3772e64b4eb2185', 'message': 'Isolate unit tests from integration tests data\n\nReasons:\n - unit tests are depending on integration tests data, such as:\n  - datastore and its versions;\n - distro package maintainers are not able to package Trove since ice house rc1.\n\nGate behaviour:\n - gate tests are not failing because unit tests are running right after\n   fake-mode tests, so sqlite still exist.\n\nChanges:\n - fixing those unit tests that are failing while building Trove package\n   (more info see at bug-report).\n\nChange-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3\nCloses-Bug: #1302784\n'}, {'number': 4, 'created': '2014-09-24 12:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/9875bec798bb9f5187f44b34a569cfb6bd29d414', 'message': 'Isolate unit tests from integration tests data\n\nReasons:\n - unit tests are depending on integration tests data, such as:\n  - datastore and its versions;\n - distro package maintainers are not able to package Trove since ice house rc1.\n\nGate behaviour:\n - gate tests are not failing because unit tests are running right after\n   fake-mode tests, so sqlite still exist.\n\nChanges:\n - fixing those unit tests that are failing while building Trove package\n   (more info see at bug-report).\n\nChange-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3\nCloses-Bug: #1302784\n'}, {'number': 5, 'created': '2014-09-24 13:51:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/a4d5766df58e0e37d0de19be9d02509fd5791c59', 'message': 'Isolate unit tests from integration tests data\n\nReasons:\n - unit tests are depending on integration tests data, such as:\n  - datastore and its versions;\n - distro package maintainers are not able to package Trove since ice house rc1.\n\nGate behaviour:\n - gate tests are not failing because unit tests are running right after\n   fake-mode tests, so sqlite still exist.\n\nChanges:\n - fixing those unit tests that are failing while building Trove package\n   (more info see at bug-report).\n\nChange-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3\nCloses-Bug: #1302784\n'}, {'number': 6, 'created': '2014-09-24 15:28:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/f1a42aa42915b7dc68fa4b7023bed1fa95e537d9', 'message': 'Isolate unit tests from integration tests data\n\nReasons:\n - unit tests are depending on integration tests data, such as:\n  - datastore and its versions;\n - distro package maintainers are not able to package Trove since ice house rc1.\n\nGate behaviour:\n - gate tests are not failing because unit tests are running right after\n   fake-mode tests, so sqlite still exist.\n\nChanges:\n - fixing those unit tests that are failing while building Trove package\n   (more info see at bug-report).\n\nChange-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3\nCloses-Bug: #1302784\n'}, {'number': 7, 'created': '2014-09-24 15:45:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/b8c8a2a0e11c03e6c37fa606604236e079ee7025', 'message': 'Isolate unit tests from integration tests data\n\nReasons:\n - unit tests are depending on integration tests data, such as:\n  - datastore and its versions;\n - distro package maintainers are not able to package Trove since ice house rc1.\n\nGate behaviour:\n - gate tests are not failing because unit tests are running right after\n   fake-mode tests, so sqlite still exist.\n\nChanges:\n - fixing those unit tests that are failing while building Trove package\n   (more info see at bug-report).\n\nChange-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3\nCloses-Bug: #1302784\n'}, {'number': 8, 'created': '2014-09-24 21:08:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/e9890297bef9b77b3e14ade40168d282b1fc5a45', 'message': 'Isolate unit tests from integration tests data\n\nReasons:\n - unit tests are depending on integration tests data, such as:\n  - datastore and its versions;\n - distribution package maintainers are not able to package Trove since ice house RC1.\n\nGate behaviour:\n - gate tests are not failing because unit tests are running right after\n   fake-mode tests, so sqlite still exist.\n\nChanges:\n - fixing those unit tests that are failing while building Trove package\n   (more info see at bug-report).\n\nChange-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3\nCloses-Bug: #1302784\n'}, {'number': 9, 'created': '2014-09-24 21:27:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/4be27154d46110cf2a0debe1817384df3cc79265', 'message': 'Isolate unit tests from integration tests data\n\nReasons:\n - unit tests are depending on integration tests data, such as:\n  - datastore and its versions;\n - distribution package maintainers are not able to package Trove since ice house RC1.\n\nGate behaviour:\n - gate tests are not failing because unit tests are running right after\n   fake-mode tests, so sqlite still exist.\n\nChanges:\n - fixing those unit tests that are failing while building Trove package\n   (more info see at bug-report).\n\nChange-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3\nCloses-Bug: #1302784\n'}, {'number': 10, 'created': '2014-09-24 21:48:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/f185f5b3c38b38bd3ca102b1f0fbbd5790aad445', 'message': 'Isolate unit tests from integration tests data\n\nReasons:\n - unit tests are depending on integration tests data, such as:\n  - datastore and its versions;\n - distribution package maintainers are not able to package Trove since ice house RC1.\n\nGate behaviour:\n - gate tests are not failing because unit tests are running right after\n   fake-mode tests, so sqlite still exist.\n\nChanges:\n - fixing those unit tests that are failing while building Trove package\n   (more info see at bug-report).\n\nChange-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3\nCloses-Bug: #1302784\n'}, {'number': 11, 'created': '2014-09-25 07:55:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/f4ccea479b5c656ebff354e312d178764bb93fe4', 'message': 'Isolate unit tests from integration tests data\n\nReasons:\n - unit tests are depending on integration tests data, such as:\n  - datastore and its versions;\n - distribution package maintainers are not able to package Trove since ice house RC1.\n\nGate behaviour:\n - gate tests are not failing because unit tests are running right after\n   fake-mode tests, so sqlite still exist.\n\nChanges:\n - fixing those unit tests that are failing while building Trove package\n   (more info see at bug-report).\n\nChange-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3\nCloses-Bug: #1302784\n'}, {'number': 12, 'created': '2014-09-25 13:18:24.000000000', 'files': ['trove/tests/unittests/mgmt/test_models.py', 'trove/tests/unittests/instance/test_instance_status.py'], 'web_link': 'https://opendev.org/openstack/trove/commit/2de5c0f66d938a320bb79093180bef3fb5983b4a', 'message': 'Isolate unit tests from integration tests data\n\nReasons:\n - unit tests are depending on integration tests data, such as:\n  - datastore and its versions;\n - distribution package maintainers are not able to package Trove since ice house RC1.\n\nGate behaviour:\n - gate tests are not failing because unit tests are running right after\n   fake-mode tests, so sqlite still exist.\n\nChanges:\n - fixing those unit tests that are failing while building Trove package\n   (more info see at bug-report).\n\nChange-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3\nCloses-Bug: #1302784\n'}]",2,123450,2de5c0f66d938a320bb79093180bef3fb5983b4a,67,8,12,8415,,,0,"Isolate unit tests from integration tests data

Reasons:
 - unit tests are depending on integration tests data, such as:
  - datastore and its versions;
 - distribution package maintainers are not able to package Trove since ice house RC1.

Gate behaviour:
 - gate tests are not failing because unit tests are running right after
   fake-mode tests, so sqlite still exist.

Changes:
 - fixing those unit tests that are failing while building Trove package
   (more info see at bug-report).

Change-Id: Ib9a41d68481d32cdd81fc4d7c015a63c4da33bb3
Closes-Bug: #1302784
",git fetch https://review.opendev.org/openstack/trove refs/changes/50/123450/9 && git format-patch -1 --stdout FETCH_HEAD,"['trove/extensions/mgmt/instances/models.py', 'trove/tests/unittests/mgmt/test_models.py', 'trove/tests/unittests/instance/test_instance_status.py']",3,9ff1cd57dfd2f6a7a26d9b6f239f9ed478a77afb,bug/123450," import uuid from trove.datastore import modelsfrom trove.tests.unittests.util import util self.id = str(uuid.uuid4()) self.datastore_version_id = str(uuid.uuid4())class BaseInstanceStatusTestCase(TestCase): util.init_db() self.db_info = FakeDBInstance() self.status = InstanceServiceStatus( ServiceStatuses.RUNNING) self.datastore = models.DBDatastore.create( id=str(uuid.uuid4()), name='mysql', default_version_id=self.db_info.datastore_version_id ) self.version = models.DBDatastoreVersion.create( id=self.db_info.datastore_version_id, datastore_id=self.datastore.id, name='5.5', manager='mysql', image_id=str(uuid.uuid4()), active=1, packages=""mysql-server-5.5"" ) super(BaseInstanceStatusTestCase, self).setUp() self.datastore.delete() self.version.delete() super(BaseInstanceStatusTestCase, self).tearDown() class InstanceStatusTest(BaseInstanceStatusTestCase): self.db_info.task_status.is_error = True instance = SimpleInstance('dummy context', self.db_info, self.status) self.db_info.task_status.action = ""BUILDING"" instance = SimpleInstance('dummy context', self.db_info, self.status) self.db_info.task_status.action = ""REBOOTING"" instance = SimpleInstance('dummy context', self.db_info, self.status) self.db_info.task_status.action = ""RESIZING"" instance = SimpleInstance('dummy context', self.db_info, self.status) self.db_info.task_status.action = ""DELETING"" instance = SimpleInstance('dummy context', self.db_info, self.status) self.db_info.server_status = ""BUILD"" instance = SimpleInstance('dummy context', self.db_info, self.status) self.db_info.server_status = ""ERROR"" instance = SimpleInstance('dummy context', self.db_info, self.status) self.db_info.server_status = ""REBOOT"" instance = SimpleInstance('dummy context', self.db_info, self.status) self.db_info.server_status = ""RESIZE"" instance = SimpleInstance('dummy context', self.db_info, self.status) self.db_info.server_status = ""VERIFY_RESIZE"" instance = SimpleInstance('dummy context', self.db_info, self.status) self.status.set_status(ServiceStatuses.PAUSED) instance = SimpleInstance('dummy context', self.db_info, self.status) self.status.set_status(ServiceStatuses.NEW) instance = SimpleInstance('dummy context', self.db_info, self.status) self.status.set_status(ServiceStatuses.RUNNING) instance = SimpleInstance('dummy context', self.db_info, self.status)","from trove.tests.util import test_config self.id = None self.datastore_version_id = test_config.dbaas_datastore_version_idclass InstanceStatusTest(TestCase): super(InstanceStatusTest, self).setUp() super(InstanceStatusTest, self).tearDown() fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_db_info.task_status.is_error = True instance = SimpleInstance('dummy context', fake_db_info, fake_status) fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_db_info.task_status.action = ""BUILDING"" instance = SimpleInstance('dummy context', fake_db_info, fake_status) fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_db_info.task_status.action = ""REBOOTING"" instance = SimpleInstance('dummy context', fake_db_info, fake_status) fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_db_info.task_status.action = ""RESIZING"" instance = SimpleInstance('dummy context', fake_db_info, fake_status) fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_db_info.task_status.action = ""DELETING"" instance = SimpleInstance('dummy context', fake_db_info, fake_status) fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_db_info.server_status = ""BUILD"" instance = SimpleInstance('dummy context', fake_db_info, fake_status) fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_db_info.server_status = ""ERROR"" instance = SimpleInstance('dummy context', fake_db_info, fake_status) fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_db_info.server_status = ""REBOOT"" instance = SimpleInstance('dummy context', fake_db_info, fake_status) fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_db_info.server_status = ""RESIZE"" instance = SimpleInstance('dummy context', fake_db_info, fake_status) fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_db_info.server_status = ""VERIFY_RESIZE"" instance = SimpleInstance('dummy context', fake_db_info, fake_status) fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_status.set_status(ServiceStatuses.PAUSED) instance = SimpleInstance('dummy context', fake_db_info, fake_status) fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_status.set_status(ServiceStatuses.NEW) instance = SimpleInstance('dummy context', fake_db_info, fake_status) fake_db_info = FakeDBInstance() fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING) fake_status.set_status(ServiceStatuses.RUNNING) instance = SimpleInstance('dummy context', fake_db_info, fake_status)",203,231
openstack%2Fheat~master~I023f75a2b4c12f0e968249a70b327bf8ffc5378d,openstack/heat,master,I023f75a2b4c12f0e968249a70b327bf8ffc5378d,Fix heat.engine.clients.has_client,MERGED,2014-09-26 11:35:01.000000000,2014-09-27 16:08:18.000000000,2014-09-27 16:08:17.000000000,"[{'_account_id': 3}, {'_account_id': 4715}, {'_account_id': 6577}, {'_account_id': 7385}, {'_account_id': 8246}]","[{'number': 1, 'created': '2014-09-26 11:35:01.000000000', 'files': ['heat/tests/test_clients.py', 'heat/engine/clients/__init__.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/f6982f6c950441f53739a6147588b59f306f705b', 'message': 'Fix heat.engine.clients.has_client\n\nProperly check client plugins existence by looking into\nExtensionManager.names().\n\nChange-Id: I023f75a2b4c12f0e968249a70b327bf8ffc5378d\nCloses-Bug: #1374096\n'}]",0,124372,f6982f6c950441f53739a6147588b59f306f705b,13,5,1,7385,,,0,"Fix heat.engine.clients.has_client

Properly check client plugins existence by looking into
ExtensionManager.names().

Change-Id: I023f75a2b4c12f0e968249a70b327bf8ffc5378d
Closes-Bug: #1374096
",git fetch https://review.opendev.org/openstack/heat refs/changes/72/124372/1 && git format-patch -1 --stdout FETCH_HEAD,"['heat/tests/test_clients.py', 'heat/engine/clients/__init__.py']",2,f6982f6c950441f53739a6147588b59f306f705b,bug/1374096, return _mgr and name in _mgr.names(), return _mgr and name in _mgr,2,1
openstack%2Ftempest~master~Idd225af4e30fd1ab94570ed29d76664826cf965a,openstack/tempest,master,Idd225af4e30fd1ab94570ed29d76664826cf965a,Add test caller to test_server_cfn_init timeout error message,MERGED,2014-09-25 21:52:10.000000000,2014-09-27 16:08:08.000000000,2014-09-27 16:08:07.000000000,"[{'_account_id': 3}, {'_account_id': 5196}, {'_account_id': 6167}, {'_account_id': 6873}]","[{'number': 1, 'created': '2014-09-25 21:52:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/be553ffdc17559dc43ba3705bf9c6a2530ed5ce7', 'message': 'Add test caller to test_server_cfn_init timeout error message\n\nWhen test_server_cfn_init times out trying to reach the server IP the\nerror message is pretty useless as far as being able to fingerprint the\nfailure in logstash.\n\nThis adds the test caller to the timeout error message like have in the\ncommon waiters code.\n\nRelated-Bug: #1374175\n\nChange-Id: Idd225af4e30fd1ab94570ed29d76664826cf965a\n'}, {'number': 2, 'created': '2014-09-25 22:15:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/7fa9e83d00aa66a308aa32281713a0ca806634b9', 'message': 'Add test caller to test_server_cfn_init timeout error message\n\nWhen test_server_cfn_init times out trying to reach the server IP the\nerror message is pretty useless as far as being able to fingerprint the\nfailure in logstash.\n\nThis adds the test caller to the timeout error message like have in the\ncommon waiters code.\n\nRelated-Bug: #1374175\n\nChange-Id: Idd225af4e30fd1ab94570ed29d76664826cf965a\n'}, {'number': 3, 'created': '2014-09-25 22:17:31.000000000', 'files': ['tempest/scenario/orchestration/test_server_cfn_init.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/a9d6535c856d6f4976e8a35f9a185c3995e50b29', 'message': 'Add test caller to test_server_cfn_init timeout error message\n\nWhen test_server_cfn_init times out trying to reach the server IP the\nerror message is pretty useless as far as being able to fingerprint the\nfailure in logstash.\n\nThis adds the test caller to the timeout error message like have in the\ncommon waiters code.\n\nRelated-Bug: #1374175\n\nChange-Id: Idd225af4e30fd1ab94570ed29d76664826cf965a\n'}]",2,124198,a9d6535c856d6f4976e8a35f9a185c3995e50b29,16,4,3,6873,,,0,"Add test caller to test_server_cfn_init timeout error message

When test_server_cfn_init times out trying to reach the server IP the
error message is pretty useless as far as being able to fingerprint the
failure in logstash.

This adds the test caller to the timeout error message like have in the
common waiters code.

Related-Bug: #1374175

Change-Id: Idd225af4e30fd1ab94570ed29d76664826cf965a
",git fetch https://review.opendev.org/openstack/tempest refs/changes/98/124198/2 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/orchestration/test_server_cfn_init.py'],1,be553ffdc17559dc43ba3705bf9c6a2530ed5ce7,bug/1374175,"from tempest.common.utils import misc as misc_utils msg = ""Timed out waiting for %s to become reachable"" % server_ip caller = misc_utils.find_test_caller() if caller: msg = ""(%s) %s"" % (caller, msg) self.fail(msg)"," self.fail( ""Timed out waiting for %s to become reachable"" % server_ip)",6,2
openstack%2Foslo.utils~master~Ie3fdc73924ef99691879ceb5efd9d5d9bb97a836,openstack/oslo.utils,master,Ie3fdc73924ef99691879ceb5efd9d5d9bb97a836,Fix how it appears we need to use mock_anything to avoid 'self' errors,MERGED,2014-09-19 18:39:57.000000000,2014-09-27 16:08:05.000000000,2014-09-27 16:08:04.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-09-19 18:39:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.utils/commit/01d0d54cff9c609da4d9e58173365001c27b39d9', 'message': ""Fix how it appears we need to use mock_anything to avoid 'self' errors\n\nFixes bug 1371724\n\nChange-Id: Ie3fdc73924ef99691879ceb5efd9d5d9bb97a836\n""}, {'number': 2, 'created': '2014-09-21 01:43:55.000000000', 'files': ['tests/test_excutils.py'], 'web_link': 'https://opendev.org/openstack/oslo.utils/commit/5d3b3da8cbac5ddf0d7ba3d11efeced4a63f16a6', 'message': ""Fix how it appears we need to use mock_anything to avoid 'self' errors\n\nCloses-Bug: #1371724\n\nChange-Id: Ie3fdc73924ef99691879ceb5efd9d5d9bb97a836\n""}]",0,122823,5d3b3da8cbac5ddf0d7ba3d11efeced4a63f16a6,10,3,2,1297,,,0,"Fix how it appears we need to use mock_anything to avoid 'self' errors

Closes-Bug: #1371724

Change-Id: Ie3fdc73924ef99691879ceb5efd9d5d9bb97a836
",git fetch https://review.opendev.org/openstack/oslo.utils refs/changes/23/122823/2 && git format-patch -1 --stdout FETCH_HEAD,['tests/test_excutils.py'],1,01d0d54cff9c609da4d9e58173365001c27b39d9,bug/1371724," self.mox.StubOutWithMock(time, 'time', use_mock_anything=True)"," self.mox.StubOutWithMock(time, 'time')",2,1
openstack%2Fneutron~master~I6b6153ea577bd685576690559b1c389490ee525d,openstack/neutron,master,I6b6153ea577bd685576690559b1c389490ee525d,Add unit tests covering single operations to ODL,MERGED,2014-08-22 08:07:48.000000000,2014-09-27 15:20:18.000000000,2014-09-27 15:20:17.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 490}, {'_account_id': 2888}, {'_account_id': 5170}, {'_account_id': 6659}, {'_account_id': 6854}, {'_account_id': 7787}, {'_account_id': 8213}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9695}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10386}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 11692}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-08-22 08:07:48.000000000', 'files': ['neutron/plugins/ml2/drivers/mechanism_odl.py', 'neutron/tests/unit/ml2/test_mechanism_odl.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/903e2a8cd1dd9169048d1ad9dd8a566b2ae52395', 'message': 'Add unit tests covering single operations to ODL\n\nThis commit adds the remaining test cases (create and update\noperations) to fully cover sync_single_resource. It also defines the\nfilter_* methods as static or class methods and removes their duplicate\narguments.\n\nChange-Id: I6b6153ea577bd685576690559b1c389490ee525d\nCloses-Bug: #1325184\n'}]",6,116199,903e2a8cd1dd9169048d1ad9dd8a566b2ae52395,49,30,1,11692,,,0,"Add unit tests covering single operations to ODL

This commit adds the remaining test cases (create and update
operations) to fully cover sync_single_resource. It also defines the
filter_* methods as static or class methods and removes their duplicate
arguments.

Change-Id: I6b6153ea577bd685576690559b1c389490ee525d
Closes-Bug: #1325184
",git fetch https://review.opendev.org/openstack/neutron refs/changes/99/116199/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ml2/drivers/mechanism_odl.py', 'neutron/tests/unit/ml2/test_mechanism_odl.py']",2,903e2a8cd1dd9169048d1ad9dd8a566b2ae52395,bug/1325184,"from neutron.openstack.common import jsonutilsclass DataMatcher(object): def __init__(self, operation, object_type, context): self._data = context.current.copy() self._object_type = object_type filter_map = getattr(mechanism_odl.OpenDaylightMechanismDriver, '%s_object_map' % operation) attr_filter = filter_map[""%ss"" % object_type] attr_filter(self._data, context) def __eq__(self, s): data = jsonutils.loads(s) return self._data == data[self._object_type] def _get_mock_network_operation_context(): current = {'status': 'ACTIVE', 'subnets': [], 'name': 'net1', 'provider:physical_network': None, 'admin_state_up': True, 'tenant_id': 'test-tenant', 'provider:network_type': 'local', 'router:external': False, 'shared': False, 'id': 'd897e21a-dfd6-4331-a5dd-7524fa421c3e', 'provider:segmentation_id': None} @staticmethod def _get_mock_subnet_operation_context(): current = {'ipv6_ra_mode': None, 'allocation_pools': [{'start': '10.0.0.2', 'end': '10.0.1.254'}], 'host_routes': [], 'ipv6_address_mode': None, 'cidr': '10.0.0.0/23', 'id': '72c56c48-e9b8-4dcf-b3a7-0813bb3bd839', 'name': '', 'enable_dhcp': True, 'network_id': 'd897e21a-dfd6-4331-a5dd-7524fa421c3e', 'tenant_id': 'test-tenant', 'dns_nameservers': [], 'gateway_ip': '10.0.0.1', 'ip_version': 4, 'shared': False} context = mock.Mock(current=current) return context @staticmethod def _get_mock_port_operation_context(): current = {'status': 'DOWN', 'binding:host_id': '', 'allowed_address_pairs': [], 'device_owner': 'fake_owner', 'binding:profile': {}, 'fixed_ips': [], 'id': '72c56c48-e9b8-4dcf-b3a7-0813bb3bd839', 'security_groups': ['2f9244b4-9bee-4e81-bc4a-3f3c2045b3d7'], 'device_id': 'fake_device', 'name': '', 'admin_state_up': True, 'network_id': 'c13bba05-eb07-45ba-ace2-765706b2d701', 'tenant_id': 'bad_tenant_id', 'binding:vif_details': {}, 'binding:vnic_type': 'normal', 'binding:vif_type': 'unbound', 'mac_address': '12:34:56:78:21:b6'} context = mock.Mock(current=current) context._plugin.get_security_group = mock.Mock(return_value={}) return context @classmethod def _get_mock_operation_context(cls, object_type): getter = getattr(cls, '_get_mock_%s_operation_context' % object_type) return getter() _status_code_msgs = { 200: '', 201: '', 400: '400 Client Error: Bad Request', 501: '501 Server Error: Not Implemented', 503: '503 Server Error: Service Unavailable', def _test_single_operation(self, method, context, status_code, exc_class=None, *args, **kwargs): mock_method.assert_called_once_with( headers={'Content-Type': 'application/json'}, auth=AuthMatcher(), timeout=config.cfg.CONF.ml2_odl.timeout, *args, **kwargs) def _test_create_resource_postcommit(self, object_type, status_code, exc_class=None): method = getattr(self.mech, 'create_%s_postcommit' % object_type) context = self._get_mock_operation_context(object_type) url = '%s/%ss' % (config.cfg.CONF.ml2_odl.url, object_type) kwargs = {'url': url, 'data': DataMatcher('create', object_type, context)} self._test_single_operation(method, context, status_code, exc_class, 'post', **kwargs) def _test_update_resource_postcommit(self, object_type, status_code, exc_class=None): method = getattr(self.mech, 'update_%s_postcommit' % object_type) context = self._get_mock_operation_context(object_type) kwargs = {'url': url, 'data': DataMatcher('update', object_type, context)} self._test_single_operation(method, context, status_code, exc_class, 'put', **kwargs) def _test_delete_resource_postcommit(self, object_type, status_code, exc_class=None): method = getattr(self.mech, 'delete_%s_postcommit' % object_type) context = self._get_mock_operation_context(object_type) url = '%s/%ss/%s' % (config.cfg.CONF.ml2_odl.url, object_type, context.current['id']) kwargs = {'url': url, 'data': None} self._test_single_operation(method, context, status_code, exc_class, 'delete', **kwargs) def test_create_network_postcommit(self): for status_code in (requests.codes.created, requests.codes.bad_request): self._test_create_resource_postcommit('network', status_code) self._test_create_resource_postcommit( 'network', requests.codes.unauthorized, requests.exceptions.HTTPError) def test_create_subnet_postcommit(self): for status_code in (requests.codes.created, requests.codes.bad_request): self._test_create_resource_postcommit('subnet', status_code) for status_code in (requests.codes.unauthorized, requests.codes.forbidden, requests.codes.not_found, requests.codes.conflict, requests.codes.not_implemented): self._test_create_resource_postcommit( 'subnet', status_code, requests.exceptions.HTTPError) def test_create_port_postcommit(self): for status_code in (requests.codes.created, requests.codes.bad_request): self._test_create_resource_postcommit('port', status_code) for status_code in (requests.codes.unauthorized, requests.codes.forbidden, requests.codes.not_found, requests.codes.conflict, requests.codes.not_implemented, requests.codes.service_unavailable): self._test_create_resource_postcommit( 'port', status_code, requests.exceptions.HTTPError) def test_update_network_postcommit(self): for status_code in (requests.codes.ok, requests.codes.bad_request): self._test_update_resource_postcommit('network', status_code) for status_code in (requests.codes.forbidden, requests.codes.not_found): self._test_update_resource_postcommit( 'network', status_code, requests.exceptions.HTTPError) def test_update_subnet_postcommit(self): for status_code in (requests.codes.ok, requests.codes.bad_request): self._test_update_resource_postcommit('subnet', status_code) for status_code in (requests.codes.unauthorized, requests.codes.forbidden, requests.codes.not_found, requests.codes.not_implemented): self._test_update_resource_postcommit( 'subnet', status_code, requests.exceptions.HTTPError) def test_update_port_postcommit(self): for status_code in (requests.codes.ok, requests.codes.bad_request): self._test_update_resource_postcommit('port', status_code) for status_code in (requests.codes.unauthorized, requests.codes.forbidden, requests.codes.not_found, requests.codes.conflict, requests.codes.not_implemented): self._test_update_resource_postcommit( 'port', status_code, requests.exceptions.HTTPError)"," def _get_mock_delete_resource_context(): current = {'id': '00000000-1111-2222-3333-444444444444'} _status_code_msgs = { 501: '501 Server Error: Not Implemented' def _test_delete_resource_postcommit(self, object_type, status_code, exc_class=None): method = getattr(self.mech, 'delete_%s_postcommit' % object_type) context = self._get_mock_delete_resource_context() mock_method.assert_called_once_with( 'delete', url=url, headers={'Content-Type': 'application/json'}, data=None, auth=AuthMatcher(), timeout=config.cfg.CONF.ml2_odl.timeout)",231,64
openstack%2Fhorizon~stable%2Ficehouse~I4b37d04fec462b2b2baad5a3c76db0d0f48e5387,openstack/horizon,stable/icehouse,I4b37d04fec462b2b2baad5a3c76db0d0f48e5387,Long container names no longer break the page,MERGED,2014-08-13 05:57:10.000000000,2014-09-27 15:14:15.000000000,2014-09-27 15:14:14.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 1941}, {'_account_id': 4264}, {'_account_id': 4978}, {'_account_id': 5623}, {'_account_id': 5733}, {'_account_id': 6610}, {'_account_id': 6914}, {'_account_id': 8213}, {'_account_id': 8871}, {'_account_id': 9576}, {'_account_id': 9622}, {'_account_id': 9656}, {'_account_id': 9659}, {'_account_id': 9981}]","[{'number': 1, 'created': '2014-08-13 05:57:10.000000000', 'files': ['openstack_dashboard/static/dashboard/less/horizon.less'], 'web_link': 'https://opendev.org/openstack/horizon/commit/9a5894b0c569231453ff3e7ea432116759813b82', 'message': ""Long container names no longer break the page\n\nIn the containers page, if the name of a container is too long, the\nobjects table is no longer visible and the table is out of the screen.\nThere's a screenshot in the bug itself.\n\nConflicts:\n\topenstack_dashboard/static/dashboard/less/horizon.less\n\nChange-Id: I4b37d04fec462b2b2baad5a3c76db0d0f48e5387\nCloses-Bug: 1314145\n(cherry picked from commit d20bde3d7be621642faa300ba3ae28ab65c2c6b5)\n""}]",0,113779,9a5894b0c569231453ff3e7ea432116759813b82,41,16,1,5733,,,0,"Long container names no longer break the page

In the containers page, if the name of a container is too long, the
objects table is no longer visible and the table is out of the screen.
There's a screenshot in the bug itself.

Conflicts:
	openstack_dashboard/static/dashboard/less/horizon.less

Change-Id: I4b37d04fec462b2b2baad5a3c76db0d0f48e5387
Closes-Bug: 1314145
(cherry picked from commit d20bde3d7be621642faa300ba3ae28ab65c2c6b5)
",git fetch https://review.opendev.org/openstack/horizon refs/changes/79/113779/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack_dashboard/static/dashboard/less/horizon.less'],1,9a5894b0c569231453ff3e7ea432116759813b82,bug/1314145,@navigationTableWidth: 40%; @contentTableWidth: @browserWrapperWidth - @navigationTableWidth; &.anchor { word-wrap: break-word; white-space: normal; max-width: 145px; },@contentTableWidth: 70%; @navigationTableWidth: 30%;,7,2
openstack%2Fneutron~master~I1b9886f508c4c8b96cf50c50f157c1960da433fc,openstack/neutron,master,I1b9886f508c4c8b96cf50c50f157c1960da433fc,Eliminate OrderedDict from test_api_v2.py,MERGED,2014-09-22 17:18:24.000000000,2014-09-27 15:04:51.000000000,2014-09-27 15:04:49.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 532}, {'_account_id': 679}, {'_account_id': 748}, {'_account_id': 5170}, {'_account_id': 5948}, {'_account_id': 6502}, {'_account_id': 6524}, {'_account_id': 6637}, {'_account_id': 6659}, {'_account_id': 6854}, {'_account_id': 7491}, {'_account_id': 7787}, {'_account_id': 7924}, {'_account_id': 8645}, {'_account_id': 8655}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12000}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-22 17:18:24.000000000', 'files': ['neutron/tests/unit/test_api_v2.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/c756d3c3ca9e90fab5e1d3ff37af33972747ba70', 'message': 'Eliminate OrderedDict from test_api_v2.py\n\nNeutron cannot possibly be passing tests under Python 2.6, as\nneutron/tests/unit/test_api_v2.py is referencing\ncollections.OrderedDict, which does not exist in Python 2.6.\nSince there is no reason to use an OrderedDict in this case,\nthis replaces it with a simple dict.\n\nChange-Id: I1b9886f508c4c8b96cf50c50f157c1960da433fc\nCloses-Bug: #1372571\n'}]",6,123189,c756d3c3ca9e90fab5e1d3ff37af33972747ba70,47,32,1,679,,,0,"Eliminate OrderedDict from test_api_v2.py

Neutron cannot possibly be passing tests under Python 2.6, as
neutron/tests/unit/test_api_v2.py is referencing
collections.OrderedDict, which does not exist in Python 2.6.
Since there is no reason to use an OrderedDict in this case,
this replaces it with a simple dict.

Change-Id: I1b9886f508c4c8b96cf50c50f157c1960da433fc
Closes-Bug: #1372571
",git fetch https://review.opendev.org/openstack/neutron refs/changes/89/123189/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/unit/test_api_v2.py'],1,c756d3c3ca9e90fab5e1d3ff37af33972747ba70,bug/1372571, args_dict = dict(,import collections args_dict = collections.OrderedDict(,1,2
openstack%2Fpython-keystoneclient~master~I06389c854e83b145bb36848158a085756ab6fa46,openstack/python-keystoneclient,master,I06389c854e83b145bb36848158a085756ab6fa46,Add HTTP_X_AUTH_URI variable for use by the OpenStack service,ABANDONED,2014-08-12 16:26:21.000000000,2014-09-27 14:46:23.000000000,,"[{'_account_id': 3}, {'_account_id': 860}, {'_account_id': 7191}]","[{'number': 1, 'created': '2014-08-12 16:26:21.000000000', 'files': ['keystoneclient/middleware/auth_token.py'], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/781924eaf846ecab80c9d386ce2b538fb44c7ecd', 'message': ""Add HTTP_X_AUTH_URI variable for use by the OpenStack service\n\nIf Middleware is configured to run in 'delay_auth_decision' mode and\nkeystone auhtentication fails (HTTP_X_IDENTITY_STATUS set to 'invalid'),\nthe Middleware may grant or not service access according to its own\ninternal rules. If Middleware denies access, it needs the Keystone\nauth_uri to generate a proper Www-Authenticate header in the 401\nresponce.\n\nThis patch adds a HTTP_X_AUTH_URI header for the Middleware\nto build up proper 401 responce in case 'delay_auth_decision' is\nenabled\n\nRelated-Bug: 1349364\nChange-Id: I06389c854e83b145bb36848158a085756ab6fa46\n""}]",1,113579,781924eaf846ecab80c9d386ce2b538fb44c7ecd,12,3,1,12654,,,0,"Add HTTP_X_AUTH_URI variable for use by the OpenStack service

If Middleware is configured to run in 'delay_auth_decision' mode and
keystone auhtentication fails (HTTP_X_IDENTITY_STATUS set to 'invalid'),
the Middleware may grant or not service access according to its own
internal rules. If Middleware denies access, it needs the Keystone
auth_uri to generate a proper Www-Authenticate header in the 401
responce.

This patch adds a HTTP_X_AUTH_URI header for the Middleware
to build up proper 401 responce in case 'delay_auth_decision' is
enabled

Related-Bug: 1349364
Change-Id: I06389c854e83b145bb36848158a085756ab6fa46
",git fetch https://review.opendev.org/openstack/python-keystoneclient refs/changes/79/113579/1 && git format-patch -1 --stdout FETCH_HEAD,['keystoneclient/middleware/auth_token.py'],1,781924eaf846ecab80c9d386ce2b538fb44c7ecd,bug/1349364,"HTTP_X_AUTH_URI Keystone authorization URI for the underlyiing service to generate a proper WWW-Authenticate responce. Provided if the Middleware is configured to run in 'delay_auth_decision' mode. self._add_headers(env, {'X-Identity-Status': 'Invalid', 'X-Auth-Uri': self.auth_uri}) 'X-Auth-Uri',"," self._add_headers(env, {'X-Identity-Status': 'Invalid'})",8,1
openstack%2Fceilometer~master~Ie973625325ba3e25c76c90e4792eeaf466ada657,openstack/ceilometer,master,Ie973625325ba3e25c76c90e4792eeaf466ada657,Per-source separation of static resources & discovery,MERGED,2014-09-25 11:52:34.000000000,2014-09-27 14:40:56.000000000,2014-09-27 14:40:55.000000000,"[{'_account_id': 3}, {'_account_id': 2284}, {'_account_id': 4491}, {'_account_id': 6537}, {'_account_id': 8052}, {'_account_id': 9562}]","[{'number': 1, 'created': '2014-09-25 11:52:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/84b339ca618a8d7ae5841b4bac8885b3287f72e1', 'message': 'Per-source separation of static resources & discovery\n\nPreviously, the amalgamation of static resources and discovery\nextensions defined for all matching pipeline sources were passed\nto each pollster on each polling cycle.\n\nThis led to unintended duplication of the samples emitted when\nan individual pollster matched several sources.\n\nNow, we relate the static resources and discovery extensions to\nthe originating sources and only pass these when a pollster is\ntraversed in the context of that source.\n\nSimilarly, sinks are now related to the originating source and\nsamples are only published over the sinks corresponding to the\ncurrent sources.\n\nCloses-Bug: #1357869\n\nChange-Id: Ie973625325ba3e25c76c90e4792eeaf466ada657\n'}, {'number': 2, 'created': '2014-09-25 14:12:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/9e8ee681055f3561efbbfed4215b42df985e4183', 'message': 'Per-source separation of static resources & discovery\n\nPreviously, the amalgamation of static resources and discovery\nextensions defined for all matching pipeline sources were passed\nto each pollster on each polling cycle.\n\nThis led to unintended duplication of the samples emitted when\nan individual pollster matched several sources.\n\nNow, we relate the static resources and discovery extensions to\nthe originating sources and only pass these when a pollster is\ntraversed in the context of that source.\n\nSimilarly, sinks are now related to the originating source and\nsamples are only published over the sinks corresponding to the\ncurrent sources.\n\nCloses-Bug: #1357869\n\nChange-Id: Ie973625325ba3e25c76c90e4792eeaf466ada657\n'}, {'number': 3, 'created': '2014-09-26 11:29:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/5fbfba22d8c3bb3479b93d87321849975d39f447', 'message': 'Per-source separation of static resources & discovery\n\nPreviously, the amalgamation of static resources and discovery\nextensions defined for all matching pipeline sources were passed\nto each pollster on each polling cycle.\n\nThis led to unintended duplication of the samples emitted when\nan individual pollster matched several sources.\n\nNow, we relate the static resources and discovery extensions to\nthe originating sources and only pass these when a pollster is\ntraversed in the context of that source.\n\nSimilarly, sinks are now related to the originating source and\nsamples are only published over the sinks corresponding to the\ncurrent sources.\n\nCloses-Bug: #1357869\n\nChange-Id: Ie973625325ba3e25c76c90e4792eeaf466ada657\n'}, {'number': 4, 'created': '2014-09-26 11:44:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/a0cd4a372cdca0973a5400acc016bfa1e52a1482', 'message': 'Per-source separation of static resources & discovery\n\nPreviously, the amalgamation of static resources and discovery\nextensions defined for all matching pipeline sources were passed\nto each pollster on each polling cycle.\n\nThis led to unintended duplication of the samples emitted when\nan individual pollster matched several sources.\n\nNow, we relate the static resources and discovery extensions to\nthe originating sources and only pass these when a pollster is\ntraversed in the context of that source.\n\nSimilarly, sinks are now related to the originating source and\nsamples are only published over the sinks corresponding to the\ncurrent sources.\n\nCloses-Bug: #1357869\n\nChange-Id: Ie973625325ba3e25c76c90e4792eeaf466ada657\n'}, {'number': 5, 'created': '2014-09-26 11:46:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/64a5f4ecfb84edd9bded7e9966801192a1d8d25b', 'message': 'Per-source separation of static resources & discovery\n\nPreviously, the amalgamation of static resources and discovery\nextensions defined for all matching pipeline sources were passed\nto each pollster on each polling cycle.\n\nThis led to unintended duplication of the samples emitted when\nan individual pollster matched several sources.\n\nNow, we relate the static resources and discovery extensions to\nthe originating sources and only pass these when a pollster is\ntraversed in the context of that source.\n\nSimilarly, sinks are now related to the originating source and\nsamples are only published over the sinks corresponding to the\ncurrent sources.\n\nCloses-Bug: #1357869\n\nChange-Id: Ie973625325ba3e25c76c90e4792eeaf466ada657\n'}, {'number': 6, 'created': '2014-09-26 12:30:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/55b16deb38144707b7afa6a898bdfc9e32aebcce', 'message': 'Per-source separation of static resources & discovery\n\nPreviously, the amalgamation of static resources and discovery\nextensions defined for all matching pipeline sources were passed\nto each pollster on each polling cycle.\n\nThis led to unintended duplication of the samples emitted when\nan individual pollster matched several sources.\n\nNow, we relate the static resources and discovery extensions to\nthe originating sources and only pass these when a pollster is\ntraversed in the context of that source.\n\nSimilarly, sinks are now related to the originating source and\nsamples are only published over the sinks corresponding to the\ncurrent sources.\n\nCloses-Bug: #1357869\n\nChange-Id: Ie973625325ba3e25c76c90e4792eeaf466ada657\n'}, {'number': 7, 'created': '2014-09-26 12:48:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/fbde9358ac16ebf40df749b90f3424a0fcaac19a', 'message': 'Per-source separation of static resources & discovery\n\nPreviously, the amalgamation of static resources and discovery\nextensions defined for all matching pipeline sources were passed\nto each pollster on each polling cycle.\n\nThis led to unintended duplication of the samples emitted when\nan individual pollster matched several sources.\n\nNow, we relate the static resources and discovery extensions to\nthe originating sources and only pass these when a pollster is\ntraversed in the context of that source.\n\nSimilarly, sinks are now related to the originating source and\nsamples are only published over the sinks corresponding to the\ncurrent sources.\n\nCloses-Bug: #1357869\n\nChange-Id: Ie973625325ba3e25c76c90e4792eeaf466ada657\n'}, {'number': 8, 'created': '2014-09-26 12:50:06.000000000', 'files': ['ceilometer/tests/agentbase.py', 'ceilometer/agent.py'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/d8317189e554c8378eefc615b73726f4b89791cb', 'message': 'Per-source separation of static resources & discovery\n\nPreviously, the amalgamation of static resources and discovery\nextensions defined for all matching pipeline sources were passed\nto each pollster on each polling cycle.\n\nThis led to unintended duplication of the samples emitted when\nan individual pollster matched several sources.\n\nNow, we relate the static resources and discovery extensions to\nthe originating sources and only pass these when a pollster is\ntraversed in the context of that source.\n\nSimilarly, sinks are now related to the originating source and\nsamples are only published over the sinks corresponding to the\ncurrent sources.\n\nCloses-Bug: #1357869\n\nChange-Id: Ie973625325ba3e25c76c90e4792eeaf466ada657\n'}]",13,124027,d8317189e554c8378eefc615b73726f4b89791cb,28,6,8,2284,,,0,"Per-source separation of static resources & discovery

Previously, the amalgamation of static resources and discovery
extensions defined for all matching pipeline sources were passed
to each pollster on each polling cycle.

This led to unintended duplication of the samples emitted when
an individual pollster matched several sources.

Now, we relate the static resources and discovery extensions to
the originating sources and only pass these when a pollster is
traversed in the context of that source.

Similarly, sinks are now related to the originating source and
samples are only published over the sinks corresponding to the
current sources.

Closes-Bug: #1357869

Change-Id: Ie973625325ba3e25c76c90e4792eeaf466ada657
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/27/124027/8 && git format-patch -1 --stdout FETCH_HEAD,"['ceilometer/tests/agentbase.py', 'ceilometer/agent.py']",2,84b339ca618a8d7ae5841b4bac8885b3287f72e1,,"from ceilometer import pipeline as publish_pipeline self._discovery = [] def setup(self, pipeline): self._resources = pipeline.resources self._discovery = pipeline.discovery def get(self, discovery_cache=None): source_discovery = (self.agent_manager.discover(self._discovery, discovery_cache) @staticmethod def key(source, pollster): return '%s-%s' % (source.name, pollster.name) # elements of the Cartesian product of pipelines X pollsters # with a common interval self.pollster_matches = set() # per-sink publisher contexts associated with each source self.publishers = {} # we relate the static resources and per-source discovery to # each combination of pollster and matching source def add(self, pollster, pipeline): if pipeline.source.name not in self.publishers: publish_context = publish_pipeline.PublishContext( self.manager.context) publish_context.add_pipelines([pipeline]) self.publishers[pipeline.source.name] = publish_context else: # this is a multi-sink source self.publishers[pipeline.source.name].add_pipelines([pipeline]) self.pollster_matches.update([(pipeline.source, pollster)]) key = Resources.key(pipeline.source, pollster) self.resources[key].setup(pipeline) cache = {} discovery_cache = {} for source, pollster in self.pollster_matches: LOG.info(_(""Polling pollster %(poll)s in the context of %(src)s""), dict(poll=pollster.name, src=source)) pollster_resources = None if pollster.obj.default_discovery: pollster_resources = self.manager.discover( [pollster.obj.default_discovery], discovery_cache) key = Resources.key(source, pollster) source_resources = list(self.resources[key].get(discovery_cache)) with self.publishers[source.name] as publisher: for pipeline, pollster in itertools.product( if pipeline.support_meter(pollster.name): polling_task = polling_tasks.get(pipeline.get_interval()) polling_tasks[pipeline.get_interval()] = polling_task polling_task.add(pollster, pipeline) self.pipeline_manager = publish_pipeline.setup_pipeline()","from ceilometer import pipeline self._discovery = set([]) def extend(self, pipeline): self._resources.extend(pipeline.resources) self._discovery.update(set(pipeline.discovery)) @property def resources(self): source_discovery = (self.agent_manager.discover(self._discovery) self.pollsters = set() # we extend the amalgamation of all static resources for this # set of pollsters with a common interval, so as to also # include any dynamically discovered resources specific to # the matching pipelines (if either is present, the per-agent # default discovery is overridden) self.publish_context = pipeline.PublishContext( agent_manager.context) def add(self, pollster, pipelines): self.publish_context.add_pipelines(pipelines) for pipe_line in pipelines: self.resources[pollster.name].extend(pipe_line) self.pollsters.update([pollster]) with self.publish_context as publisher: cache = {} discovery_cache = {} for pollster in self.pollsters: key = pollster.name LOG.info(_(""Polling pollster %s""), key) pollster_resources = None if pollster.obj.default_discovery: pollster_resources = self.manager.discover( [pollster.obj.default_discovery], discovery_cache) source_resources = list(self.resources[key].resources) for pipe_line, pollster in itertools.product( if pipe_line.support_meter(pollster.name): polling_task = polling_tasks.get(pipe_line.get_interval()) polling_tasks[pipe_line.get_interval()] = polling_task polling_task.add(pollster, [pipe_line]) self.pipeline_manager = pipeline.setup_pipeline()",69,54
openstack%2Fcinder~stable%2Ficehouse~I0268d374f7529d89068dcbf3c1cb9ab3d60d4115,openstack/cinder,stable/icehouse,I0268d374f7529d89068dcbf3c1cb9ab3d60d4115,Prevent tenant viewing volumes owned by another,MERGED,2014-08-28 16:01:33.000000000,2014-09-27 14:40:47.000000000,2014-09-27 14:40:46.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 2759}, {'_account_id': 5538}, {'_account_id': 6491}, {'_account_id': 7198}, {'_account_id': 7219}, {'_account_id': 8213}, {'_account_id': 9656}, {'_account_id': 11811}]","[{'number': 1, 'created': '2014-08-28 16:01:33.000000000', 'files': ['cinder/tests/api/v1/test_volume_metadata.py', 'cinder/tests/api/v2/test_volume_metadata.py', 'cinder/tests/api/v1/test_snapshot_metadata.py', 'cinder/tests/api/v2/test_snapshot_metadata.py', 'cinder/tests/test_volume.py', 'cinder/tests/api/fakes.py', 'cinder/tests/api/test_router.py', 'cinder/tests/api/v2/test_volumes.py', 'cinder/volume/api.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/da65d08f20021ac60a8c6fc2b5577b60911aeb79', 'message': 'Prevent tenant viewing volumes owned by another\n\nBug introduced by 0505bb268942534ad5d6ecd5e34a4d9b0e7f5c04 allows any\ntenant to get the details of a volume belonging to any other tenant\nif the UUID is known.\n\nThis commit allows only the tenant or an admin to get a volume.\n\nChange-Id: I0268d374f7529d89068dcbf3c1cb9ab3d60d4115\nCloses-Bug: #1356368\n(cherry picked from commit 5868e8f285d23b56ca6123dab760342c57bf8c80)\n'}]",0,117546,da65d08f20021ac60a8c6fc2b5577b60911aeb79,22,10,1,7198,,,0,"Prevent tenant viewing volumes owned by another

Bug introduced by 0505bb268942534ad5d6ecd5e34a4d9b0e7f5c04 allows any
tenant to get the details of a volume belonging to any other tenant
if the UUID is known.

This commit allows only the tenant or an admin to get a volume.

Change-Id: I0268d374f7529d89068dcbf3c1cb9ab3d60d4115
Closes-Bug: #1356368
(cherry picked from commit 5868e8f285d23b56ca6123dab760342c57bf8c80)
",git fetch https://review.opendev.org/openstack/cinder refs/changes/46/117546/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/api/v1/test_volume_metadata.py', 'cinder/tests/api/v2/test_volume_metadata.py', 'cinder/tests/api/v1/test_snapshot_metadata.py', 'cinder/tests/api/v2/test_snapshot_metadata.py', 'cinder/tests/test_volume.py', 'cinder/tests/api/fakes.py', 'cinder/tests/api/test_router.py', 'cinder/tests/api/v2/test_volumes.py', 'cinder/volume/api.py']",9,da65d08f20021ac60a8c6fc2b5577b60911aeb79,bug/1356368," ctxt = context.elevated() else: ctxt = context rv = self.db.volume_get(ctxt, volume_id) if not context.is_admin and volume['project_id'] != context.project_id: raise exception.VolumeNotFound(volume_id=volume_id)"," context = context.elevated() rv = self.db.volume_get(context, volume_id)",67,35
openstack%2Foslo.log~master~Ied5ca29b29ab241ae926417278a54c7ba551fb79,openstack/oslo.log,master,Ied5ca29b29ab241ae926417278a54c7ba551fb79,Use oslo.utils and oslo.serialization,MERGED,2014-09-25 20:13:07.000000000,2014-09-27 14:40:45.000000000,2014-09-27 14:40:44.000000000,"[{'_account_id': 3}, {'_account_id': 5638}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-25 20:13:07.000000000', 'files': ['oslo/log/openstack/common/importutils.py', 'oslo/log/openstack/common/timeutils.py', 'oslo/log/openstack/common/jsonutils.py', 'oslo/log/openstack/common/local.py', 'oslo/log/openstack/common/log.py', 'oslo/log/log.py', 'oslo/log/openstack/common/fileutils.py', 'oslo/log/openstack/common/__init__.py', 'requirements.txt', 'oslo/log/openstack/common/gettextutils.py', 'openstack-common.conf', 'oslo/log/openstack/common/excutils.py', 'oslo/log/formatters.py', 'tests/unit/test_log.py', 'oslo/log/openstack/__init__.py', 'oslo/log/openstack/common/strutils.py'], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/ef46e20ec166fabd9614929866d705b7fc12d6db', 'message': 'Use oslo.utils and oslo.serialization\n\nReplace the incubated modules from oslo.utils and oslo.serialization\nwith references to the libraries.\n\nChange-Id: Ied5ca29b29ab241ae926417278a54c7ba551fb79\n'}]",0,124172,ef46e20ec166fabd9614929866d705b7fc12d6db,7,3,1,2472,,,0,"Use oslo.utils and oslo.serialization

Replace the incubated modules from oslo.utils and oslo.serialization
with references to the libraries.

Change-Id: Ied5ca29b29ab241ae926417278a54c7ba551fb79
",git fetch https://review.opendev.org/openstack/oslo.log refs/changes/72/124172/1 && git format-patch -1 --stdout FETCH_HEAD,"['oslo/log/openstack/common/importutils.py', 'oslo/log/openstack/common/timeutils.py', 'oslo/log/openstack/common/jsonutils.py', 'oslo/log/openstack/common/local.py', 'oslo/log/openstack/common/log.py', 'oslo/log/log.py', 'oslo/log/openstack/common/fileutils.py', 'oslo/log/openstack/common/__init__.py', 'requirements.txt', 'oslo/log/openstack/common/gettextutils.py', 'openstack-common.conf', 'oslo/log/openstack/common/excutils.py', 'oslo/log/formatters.py', 'tests/unit/test_log.py', 'oslo/log/openstack/__init__.py', 'oslo/log/openstack/common/strutils.py']",16,ef46e20ec166fabd9614929866d705b7fc12d6db,use-libs-not-incubated-modules,,"# Copyright 2011 OpenStack Foundation. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """""" System-level utilities and helper functions. """""" import math import re import sys import unicodedata import six from oslo.log.openstack.common.gettextutils import _ UNIT_PREFIX_EXPONENT = { 'k': 1, 'K': 1, 'Ki': 1, 'M': 2, 'Mi': 2, 'G': 3, 'Gi': 3, 'T': 4, 'Ti': 4, } UNIT_SYSTEM_INFO = { 'IEC': (1024, re.compile(r'(^[-+]?\d*\.?\d+)([KMGT]i?)?(b|bit|B)$')), 'SI': (1000, re.compile(r'(^[-+]?\d*\.?\d+)([kMGT])?(b|bit|B)$')), } TRUE_STRINGS = ('1', 't', 'true', 'on', 'y', 'yes') FALSE_STRINGS = ('0', 'f', 'false', 'off', 'n', 'no') SLUGIFY_STRIP_RE = re.compile(r""[^\w\s-]"") SLUGIFY_HYPHENATE_RE = re.compile(r""[-\s]+"") # NOTE(flaper87): The following globals are used by `mask_password` _SANITIZE_KEYS = ['adminPass', 'admin_pass', 'password', 'admin_password'] # NOTE(ldbragst): Let's build a list of regex objects using the list of # _SANITIZE_KEYS we already have. This way, we only have to add the new key # to the list of _SANITIZE_KEYS and we can generate regular expressions # for XML and JSON automatically. _SANITIZE_PATTERNS_2 = [] _SANITIZE_PATTERNS_1 = [] # NOTE(amrith): Some regular expressions have only one parameter, some # have two parameters. Use different lists of patterns here. _FORMAT_PATTERNS_1 = [r'(%(key)s\s*[=]\s*)[^\s^\'^\""]+'] _FORMAT_PATTERNS_2 = [r'(%(key)s\s*[=]\s*[\""\']).*?([\""\'])', r'(%(key)s\s+[\""\']).*?([\""\'])', r'([-]{2}%(key)s\s+)[^\'^\""^=^\s]+([\s]*)', r'(<%(key)s>).*?(</%(key)s>)', r'([\""\']%(key)s[\""\']\s*:\s*[\""\']).*?([\""\'])', r'([\'""].*?%(key)s[\'""]\s*:\s*u?[\'""]).*?([\'""])', r'([\'""].*?%(key)s[\'""]\s*,\s*\'--?[A-z]+\'\s*,\s*u?' '[\'""]).*?([\'""])', r'(%(key)s\s*--?[A-z]+\s*)\S+(\s*)'] for key in _SANITIZE_KEYS: for pattern in _FORMAT_PATTERNS_2: reg_ex = re.compile(pattern % {'key': key}, re.DOTALL) _SANITIZE_PATTERNS_2.append(reg_ex) for pattern in _FORMAT_PATTERNS_1: reg_ex = re.compile(pattern % {'key': key}, re.DOTALL) _SANITIZE_PATTERNS_1.append(reg_ex) def int_from_bool_as_string(subject): """"""Interpret a string as a boolean and return either 1 or 0. Any string value in: ('True', 'true', 'On', 'on', '1') is interpreted as a boolean True. Useful for JSON-decoded stuff and config file parsing """""" return bool_from_string(subject) and 1 or 0 def bool_from_string(subject, strict=False, default=False): """"""Interpret a string as a boolean. A case-insensitive match is performed such that strings matching 't', 'true', 'on', 'y', 'yes', or '1' are considered True and, when `strict=False`, anything else returns the value specified by 'default'. Useful for JSON-decoded stuff and config file parsing. If `strict=True`, unrecognized values, including None, will raise a ValueError which is useful when parsing values passed in from an API call. Strings yielding False are 'f', 'false', 'off', 'n', 'no', or '0'. """""" if not isinstance(subject, six.string_types): subject = six.text_type(subject) lowered = subject.strip().lower() if lowered in TRUE_STRINGS: return True elif lowered in FALSE_STRINGS: return False elif strict: acceptable = ', '.join( ""'%s'"" % s for s in sorted(TRUE_STRINGS + FALSE_STRINGS)) msg = _(""Unrecognized value '%(val)s', acceptable values are:"" "" %(acceptable)s"") % {'val': subject, 'acceptable': acceptable} raise ValueError(msg) else: return default def safe_decode(text, incoming=None, errors='strict'): """"""Decodes incoming text/bytes string using `incoming` if they're not already unicode. :param incoming: Text's current encoding :param errors: Errors handling policy. See here for valid values http://docs.python.org/2/library/codecs.html :returns: text or a unicode `incoming` encoded representation of it. :raises TypeError: If text is not an instance of str """""" if not isinstance(text, (six.string_types, six.binary_type)): raise TypeError(""%s can't be decoded"" % type(text)) if isinstance(text, six.text_type): return text if not incoming: incoming = (sys.stdin.encoding or sys.getdefaultencoding()) try: return text.decode(incoming, errors) except UnicodeDecodeError: # Note(flaper87) If we get here, it means that # sys.stdin.encoding / sys.getdefaultencoding # didn't return a suitable encoding to decode # text. This happens mostly when global LANG # var is not set correctly and there's no # default encoding. In this case, most likely # python will use ASCII or ANSI encoders as # default encodings but they won't be capable # of decoding non-ASCII characters. # # Also, UTF-8 is being used since it's an ASCII # extension. return text.decode('utf-8', errors) def safe_encode(text, incoming=None, encoding='utf-8', errors='strict'): """"""Encodes incoming text/bytes string using `encoding`. If incoming is not specified, text is expected to be encoded with current python's default encoding. (`sys.getdefaultencoding`) :param incoming: Text's current encoding :param encoding: Expected encoding for text (Default UTF-8) :param errors: Errors handling policy. See here for valid values http://docs.python.org/2/library/codecs.html :returns: text or a bytestring `encoding` encoded representation of it. :raises TypeError: If text is not an instance of str """""" if not isinstance(text, (six.string_types, six.binary_type)): raise TypeError(""%s can't be encoded"" % type(text)) if not incoming: incoming = (sys.stdin.encoding or sys.getdefaultencoding()) if isinstance(text, six.text_type): return text.encode(encoding, errors) elif text and encoding != incoming: # Decode text before encoding it with `encoding` text = safe_decode(text, incoming, errors) return text.encode(encoding, errors) else: return text def string_to_bytes(text, unit_system='IEC', return_int=False): """"""Converts a string into an float representation of bytes. The units supported for IEC :: Kb(it), Kib(it), Mb(it), Mib(it), Gb(it), Gib(it), Tb(it), Tib(it) KB, KiB, MB, MiB, GB, GiB, TB, TiB The units supported for SI :: kb(it), Mb(it), Gb(it), Tb(it) kB, MB, GB, TB Note that the SI unit system does not support capital letter 'K' :param text: String input for bytes size conversion. :param unit_system: Unit system for byte size conversion. :param return_int: If True, returns integer representation of text in bytes. (default: decimal) :returns: Numerical representation of text in bytes. :raises ValueError: If text has an invalid value. """""" try: base, reg_ex = UNIT_SYSTEM_INFO[unit_system] except KeyError: msg = _('Invalid unit system: ""%s""') % unit_system raise ValueError(msg) match = reg_ex.match(text) if match: magnitude = float(match.group(1)) unit_prefix = match.group(2) if match.group(3) in ['b', 'bit']: magnitude /= 8 else: msg = _('Invalid string format: %s') % text raise ValueError(msg) if not unit_prefix: res = magnitude else: res = magnitude * pow(base, UNIT_PREFIX_EXPONENT[unit_prefix]) if return_int: return int(math.ceil(res)) return res def to_slug(value, incoming=None, errors=""strict""): """"""Normalize string. Convert to lowercase, remove non-word characters, and convert spaces to hyphens. Inspired by Django's `slugify` filter. :param value: Text to slugify :param incoming: Text's current encoding :param errors: Errors handling policy. See here for valid values http://docs.python.org/2/library/codecs.html :returns: slugified unicode representation of `value` :raises TypeError: If text is not an instance of str """""" value = safe_decode(value, incoming, errors) # NOTE(aababilov): no need to use safe_(encode|decode) here: # encodings are always ""ascii"", error handling is always ""ignore"" # and types are always known (first: unicode; second: str) value = unicodedata.normalize(""NFKD"", value).encode( ""ascii"", ""ignore"").decode(""ascii"") value = SLUGIFY_STRIP_RE.sub("""", value).strip().lower() return SLUGIFY_HYPHENATE_RE.sub(""-"", value) def mask_password(message, secret=""***""): """"""Replace password with 'secret' in message. :param message: The string which includes security information. :param secret: value with which to replace passwords. :returns: The unicode value of message with the password fields masked. For example: >>> mask_password(""'adminPass' : 'aaaaa'"") ""'adminPass' : '***'"" >>> mask_password(""'admin_pass' : 'aaaaa'"") ""'admin_pass' : '***'"" >>> mask_password('""password"" : ""aaaaa""') '""password"" : ""***""' >>> mask_password(""'original_password' : 'aaaaa'"") ""'original_password' : '***'"" >>> mask_password(""u'original_password' : u'aaaaa'"") ""u'original_password' : u'***'"" """""" message = six.text_type(message) # NOTE(ldbragst): Check to see if anything in message contains any key # specified in _SANITIZE_KEYS, if not then just return the message since # we don't have to mask any passwords. if not any(key in message for key in _SANITIZE_KEYS): return message substitute = r'\g<1>' + secret + r'\g<2>' for pattern in _SANITIZE_PATTERNS_2: message = re.sub(pattern, substitute, message) substitute = r'\g<1>' + secret for pattern in _SANITIZE_PATTERNS_1: message = re.sub(pattern, substitute, message) return message ",13,2280
openstack%2Foslo.log~master~Ia0f86adf9dc6d28e5397d79bff95e7a3895f772e,openstack/oslo.log,master,Ia0f86adf9dc6d28e5397d79bff95e7a3895f772e,Fix test env order for testrepository db format,MERGED,2014-09-25 20:13:07.000000000,2014-09-27 14:40:38.000000000,2014-09-27 14:40:38.000000000,"[{'_account_id': 3}, {'_account_id': 5638}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-25 20:13:07.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/50d2dbaad5fdfe972c08b51e00168aa2e1bbb5ef', 'message': 'Fix test env order for testrepository db format\n\nRun the python 3 tests before python 2 so the test repository database\nis created in a format available to both.\n\nChange-Id: Ia0f86adf9dc6d28e5397d79bff95e7a3895f772e\n'}]",0,124171,50d2dbaad5fdfe972c08b51e00168aa2e1bbb5ef,7,3,1,2472,,,0,"Fix test env order for testrepository db format

Run the python 3 tests before python 2 so the test repository database
is created in a format available to both.

Change-Id: Ia0f86adf9dc6d28e5397d79bff95e7a3895f772e
",git fetch https://review.opendev.org/openstack/oslo.log refs/changes/71/124171/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,50d2dbaad5fdfe972c08b51e00168aa2e1bbb5ef,use-libs-not-incubated-modules,"envlist = py33,py34,py26,py27,pypy,pep8","envlist = py26,py27,py33,py34,pypy,pep8",1,1
openstack%2Foslo.log~master~If3094c28f500dc189dbbcd15926e71ece9e7fde3,openstack/oslo.log,master,If3094c28f500dc189dbbcd15926e71ece9e7fde3,log: add missing space in error message,MERGED,2014-09-21 01:56:15.000000000,2014-09-27 14:30:56.000000000,2014-09-27 14:30:56.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-21 01:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/b1538f5b3c8e5c0c4bf88c50902fe20cc0997730', 'message': 'log: add missing space in error message\n\nChange-Id: If3094c28f500dc189dbbcd15926e71ece9e7fde3\n'}, {'number': 2, 'created': '2014-09-21 02:04:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/aa31e9337ab238f12a9301e11ca805fc81bf5b9e', 'message': 'log: add missing space in error message\n\nChange-Id: If3094c28f500dc189dbbcd15926e71ece9e7fde3\n'}, {'number': 3, 'created': '2014-09-21 04:06:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/ab10c394f8eeef8f978b85bf2554e983e6a46034', 'message': 'log: add missing space in error message\n\nChange-Id: If3094c28f500dc189dbbcd15926e71ece9e7fde3\n'}, {'number': 4, 'created': '2014-09-21 04:10:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/ba8f862384afaf13facbe83a875fafc94cffe6f0', 'message': 'log: add missing space in error message\n\nChange-Id: If3094c28f500dc189dbbcd15926e71ece9e7fde3\n'}, {'number': 5, 'created': '2014-09-24 11:14:47.000000000', 'files': ['oslo/log/log.py'], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/635108c1e95c93bfb56cae8d536782f78f4feb7a', 'message': 'log: add missing space in error message\n\nChange-Id: If3094c28f500dc189dbbcd15926e71ece9e7fde3\n'}]",0,122976,635108c1e95c93bfb56cae8d536782f78f4feb7a,14,3,5,5638,,,0,"log: add missing space in error message

Change-Id: If3094c28f500dc189dbbcd15926e71ece9e7fde3
",git fetch https://review.opendev.org/openstack/oslo.log refs/changes/76/122976/4 && git format-patch -1 --stdout FETCH_HEAD,['oslo/log/log.py'],1,b1538f5b3c8e5c0c4bf88c50902fe20cc0997730,, log_root.error('Unable to add syslog handler. Verify that syslog ', log_root.error('Unable to add syslog handler. Verify that syslog',1,1
openstack%2Foslo.log~master~Ie53eb4d2e85601feb281c57c569048e2c533320a,openstack/oslo.log,master,Ie53eb4d2e85601feb281c57c569048e2c533320a,fix typo and formatting in contributing docs,MERGED,2014-09-10 12:06:44.000000000,2014-09-27 14:30:54.000000000,2014-09-27 14:30:54.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-10 12:06:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/5a1da386e80eb7363d21686c1a4f39da500e2ff8', 'message': 'fix typo and formatting in contributing docs\n\nChange-Id: Ie53eb4d2e85601feb281c57c569048e2c533320a\n'}, {'number': 2, 'created': '2014-09-21 02:04:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/56a71fff04573d6755f96a2ffc929fa62734ba0e', 'message': 'fix typo and formatting in contributing docs\n\nChange-Id: Ie53eb4d2e85601feb281c57c569048e2c533320a\n'}, {'number': 3, 'created': '2014-09-21 04:04:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/c171da611c72f5d60ce538b357bf3d10d0499f50', 'message': 'fix typo and formatting in contributing docs\n\nChange-Id: Ie53eb4d2e85601feb281c57c569048e2c533320a\n'}, {'number': 4, 'created': '2014-09-21 04:10:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/cfa359b32cdc98bd7934698fe514dfdc72d85436', 'message': 'fix typo and formatting in contributing docs\n\nChange-Id: Ie53eb4d2e85601feb281c57c569048e2c533320a\n'}, {'number': 5, 'created': '2014-09-24 11:13:54.000000000', 'files': ['doc/source/contributing.rst', 'CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/505e34af217c978ac69f73ffd37601c64ccaf780', 'message': 'fix typo and formatting in contributing docs\n\nChange-Id: Ie53eb4d2e85601feb281c57c569048e2c533320a\n'}]",0,120393,505e34af217c978ac69f73ffd37601c64ccaf780,22,4,5,5638,,,0,"fix typo and formatting in contributing docs

Change-Id: Ie53eb4d2e85601feb281c57c569048e2c533320a
",git fetch https://review.opendev.org/openstack/oslo.log refs/changes/93/120393/3 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'doc/source/contributing.rst']",2,5a1da386e80eb7363d21686c1a4f39da500e2ff8,fix-docs,============ Contributing ============,============== Contributing ==============,4,4
openstack%2Fkeystone-specs~master~I71e941e2a639641a662a163c682eb86d51de42fb,openstack/keystone-specs,master,I71e941e2a639641a662a163c682eb86d51de42fb,Stop using intersphinx,MERGED,2014-09-13 07:26:26.000000000,2014-09-27 13:36:59.000000000,2014-09-27 13:36:58.000000000,"[{'_account_id': 3}, {'_account_id': 6482}, {'_account_id': 6486}]","[{'number': 1, 'created': '2014-09-13 07:26:26.000000000', 'files': ['doc/source/conf.py'], 'web_link': 'https://opendev.org/openstack/keystone-specs/commit/004b010c22e0d3f23dd3646b9b7277d6be0f8a26', 'message': ""Stop using intersphinx\n\nRemove intersphinx from the docs build as it triggers network calls that\noccasionally fail, and we don't really use intersphinx (links other\nsphinx documents out on the internet)\n\nThis also removes the requirement for internet access during docs build.\n\nThis can cause docs jobs to fail if the project errors out on\nwarnings.\n\nChange-Id: I71e941e2a639641a662a163c682eb86d51de42fb\nRelated-Bug: #1368910\n""}]",0,121298,004b010c22e0d3f23dd3646b9b7277d6be0f8a26,7,3,1,6547,,,0,"Stop using intersphinx

Remove intersphinx from the docs build as it triggers network calls that
occasionally fail, and we don't really use intersphinx (links other
sphinx documents out on the internet)

This also removes the requirement for internet access during docs build.

This can cause docs jobs to fail if the project errors out on
warnings.

Change-Id: I71e941e2a639641a662a163c682eb86d51de42fb
Related-Bug: #1368910
",git fetch https://review.opendev.org/openstack/keystone-specs refs/changes/98/121298/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/conf.py'],1,004b010c22e0d3f23dd3646b9b7277d6be0f8a26,bug/1368910,," 'sphinx.ext.intersphinx',",0,1
openstack%2Fneutron~master~I3eac35abbb1d6f8429b4331948aecfc952284219,openstack/neutron,master,I3eac35abbb1d6f8429b4331948aecfc952284219,Delete a broken subnet delete unit test,MERGED,2014-09-18 20:48:26.000000000,2014-09-27 13:34:53.000000000,2014-09-21 00:20:33.000000000,"[{'_account_id': 3}, {'_account_id': 748}, {'_account_id': 841}, {'_account_id': 5170}, {'_account_id': 5948}, {'_account_id': 6524}, {'_account_id': 6638}, {'_account_id': 6854}, {'_account_id': 7787}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-18 20:48:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/91fdcd50c4e234340bac9532f5359e4f858e7f15', 'message': ""Fix a broken subnet delete unit test\n\nA test to delete a subnet in use was incorrectly\ncalling 'subnet' instead of 'subnets' in the API request\nand asserting a 404 instead of a 409.\n\nChange-Id: I3eac35abbb1d6f8429b4331948aecfc952284219\n""}, {'number': 2, 'created': '2014-09-18 23:12:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/a2ff494779196c3e2cb793ae5826315b931bdc8b', 'message': ""Fix a broken subnet delete unit test\n\nA test to delete a subnet in use was incorrectly\ncalling 'subnet' instead of 'subnets' in the API request\nand asserting a 404 instead of a 409.\n\nChange-Id: I3eac35abbb1d6f8429b4331948aecfc952284219\n""}, {'number': 3, 'created': '2014-09-18 23:37:22.000000000', 'files': ['neutron/tests/unit/test_db_plugin.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/ea52cb9b677dc05e976b4a04907bf17768503789', 'message': ""Delete a broken subnet delete unit test\n\nA test to delete a subnet in use was incorrectly\ncalling 'subnet' instead of 'subnets' in the API request\nand asserting a 404 instead of a 409. Even the correct\nversion of this test is already covered by the\n'test_port_prevents_subnet_deletion' method so this\ncommit just removes the broken test.\n\nChange-Id: I3eac35abbb1d6f8429b4331948aecfc952284219\n""}]",5,122521,ea52cb9b677dc05e976b4a04907bf17768503789,62,24,3,7787,,,0,"Delete a broken subnet delete unit test

A test to delete a subnet in use was incorrectly
calling 'subnet' instead of 'subnets' in the API request
and asserting a 404 instead of a 409. Even the correct
version of this test is already covered by the
'test_port_prevents_subnet_deletion' method so this
commit just removes the broken test.

Change-Id: I3eac35abbb1d6f8429b4331948aecfc952284219
",git fetch https://review.opendev.org/openstack/neutron refs/changes/21/122521/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/unit/test_db_plugin.py'],1,91fdcd50c4e234340bac9532f5359e4f858e7f15,brokenunittest," req = self.new_delete_request('subnets', self.assertEqual(res.status_int, webob.exc.HTTPConflict.code)"," req = self.new_delete_request('subnet', self.assertEqual(res.status_int, webob.exc.HTTPNotFound.code)",2,2
openstack%2Fneutron~master~Ic5bcbb0cf941e0745890abc776d719e58bb42e35,openstack/neutron,master,Ic5bcbb0cf941e0745890abc776d719e58bb42e35,Delete DB records instead of tables to speedup UT,MERGED,2014-09-17 00:31:12.000000000,2014-09-27 12:43:13.000000000,2014-09-23 22:47:40.000000000,"[{'_account_id': 3}, {'_account_id': 748}, {'_account_id': 1689}, {'_account_id': 2035}, {'_account_id': 5170}, {'_account_id': 5948}, {'_account_id': 6524}, {'_account_id': 6598}, {'_account_id': 6659}, {'_account_id': 6854}, {'_account_id': 7787}, {'_account_id': 8645}, {'_account_id': 8873}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 11816}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-09-17 00:31:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/cd4ef50f336d2ab14c505a20c4b390483fae5242', 'message': 'Speedup the unit tests\n\nThe unit tests were slow before. This commit makes them\nslightly faster.\n\nChange-Id: Ic5bcbb0cf941e0745890abc776d719e58bb42e35\n'}, {'number': 2, 'created': '2014-09-17 23:37:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3560cd6539990cd613002d5d23ef160b375c8999', 'message': ""Delete DB records instead of tables to speedup UT\n\nNow that the schema is fixed for all of the plugins,\nthere isn't a need to delete and recreate the entire\nschema for every unit test.\n\nThis patch clears the tables at the end of each test\ninstead of deleting them. This eliminated overhead seems\nto save 10%+ execution time of the entire set of unit\ntests.\n\nExample of performance gain from tox -epy27 tests.unit.ml2:\nBefore: Ran 2495 tests in 284.186s\nAfter: Ran 2495 tests in 223.299s\n\nChange-Id: Ic5bcbb0cf941e0745890abc776d719e58bb42e35\n""}, {'number': 3, 'created': '2014-09-18 19:49:23.000000000', 'files': ['neutron/tests/unit/testlib_api.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/34cf04a0ea84f4e5b32aa8d45108bd2a38bcff58', 'message': ""Delete DB records instead of tables to speedup UT\n\nNow that the schema is fixed for all of the plugins,\nthere isn't a need to delete and recreate the entire\nschema for every unit test.\n\nThis patch clears the tables at the end of each test\ninstead of deleting them. This eliminated overhead seems\nto save 10%+ execution time of the entire set of unit\ntests.\n\nExample of performance gain from tox -epy27 tests.unit.ml2:\nBefore: Ran 2495 tests in 284.186s\nAfter: Ran 2495 tests in 223.299s\n\nChange-Id: Ic5bcbb0cf941e0745890abc776d719e58bb42e35\n""}]",4,122028,34cf04a0ea84f4e5b32aa8d45108bd2a38bcff58,131,30,3,7787,,,0,"Delete DB records instead of tables to speedup UT

Now that the schema is fixed for all of the plugins,
there isn't a need to delete and recreate the entire
schema for every unit test.

This patch clears the tables at the end of each test
instead of deleting them. This eliminated overhead seems
to save 10%+ execution time of the entire set of unit
tests.

Example of performance gain from tox -epy27 tests.unit.ml2:
Before: Ran 2495 tests in 284.186s
After: Ran 2495 tests in 223.299s

Change-Id: Ic5bcbb0cf941e0745890abc776d719e58bb42e35
",git fetch https://review.opendev.org/openstack/neutron refs/changes/28/122028/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/unit/testlib_api.py'],1,cd4ef50f336d2ab14c505a20c4b390483fae5242,vroomvroom, # flag to indicate that the models have been loaded _TABLES_ESTABLISHED = False if not SqlTestCase._TABLES_ESTABLISHED: model_base.BASEV2.metadata.create_all(engine) SqlTestCase._TABLES_ESTABLISHED = True def clear_tables(): with db_api.get_session().begin(subtransactions=True): for table in reversed( model_base.BASEV2.metadata.sorted_tables): engine.execute(table.delete()) self.addCleanup(clear_tables)," model_base.BASEV2.metadata.create_all(engine) def unregister_models(): """"""Unregister all data models."""""" model_base.BASEV2.metadata.drop_all(engine) self.addCleanup(unregister_models)",12,5
openstack%2Fneutron~master~Ifb495f9e8929062a0c24d090c3e702109a38803a,openstack/neutron,master,Ifb495f9e8929062a0c24d090c3e702109a38803a,Fix 500 error on retrieving metadata by invalid URI,MERGED,2014-09-18 18:03:00.000000000,2014-09-27 11:31:34.000000000,2014-09-25 04:58:13.000000000,"[{'_account_id': 3}, {'_account_id': 704}, {'_account_id': 748}, {'_account_id': 841}, {'_account_id': 1131}, {'_account_id': 1923}, {'_account_id': 4395}, {'_account_id': 5170}, {'_account_id': 6659}, {'_account_id': 7293}, {'_account_id': 7787}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10257}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-18 18:03:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/41da0205d6ea8b015b44e383aba60ae06866121f', 'message': 'Fix 500 error on retrieving metadata by invalid URI\n\nAn invalid URI should return a BadRequest error rather\nthan making the metadata proxy bomb out.\n\nCloses-bug: #1371160\n\nChange-Id: Ifb495f9e8929062a0c24d090c3e702109a38803a\n'}, {'number': 2, 'created': '2014-09-18 19:03:18.000000000', 'files': ['neutron/tests/unit/test_metadata_namespace_proxy.py', 'neutron/agent/metadata/agent.py', 'neutron/agent/metadata/namespace_proxy.py', 'neutron/tests/unit/test_metadata_agent.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/f92d1300e1aca68b39b927282af038acc68c5013', 'message': 'Fix 500 error on retrieving metadata by invalid URI\n\nAn invalid URI should return a BadRequest error rather\nthan making the metadata proxy bomb out.\n\nCloses-bug: #1371160\n\nChange-Id: Ifb495f9e8929062a0c24d090c3e702109a38803a\n'}]",7,122483,f92d1300e1aca68b39b927282af038acc68c5013,54,28,2,748,,,0,"Fix 500 error on retrieving metadata by invalid URI

An invalid URI should return a BadRequest error rather
than making the metadata proxy bomb out.

Closes-bug: #1371160

Change-Id: Ifb495f9e8929062a0c24d090c3e702109a38803a
",git fetch https://review.opendev.org/openstack/neutron refs/changes/83/122483/2 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/agent/metadata/agent.py', 'neutron/tests/unit/test_metadata_agent.py']",2,41da0205d6ea8b015b44e383aba60ae06866121f,bug/1371160," def test_proxy_request_400(self): self.assertIsInstance(self._proxy_request_test_helper(400), webob.exc.HTTPBadRequest) ",,6,0
openstack%2Fsecurity-doc~master~I6ed0e0d96a2bb86b0bcbd479aae3ddb7901ee3e1,openstack/security-doc,master,I6ed0e0d96a2bb86b0bcbd479aae3ddb7901ee3e1,Use reference section,MERGED,2014-09-23 19:14:14.000000000,2014-09-27 10:54:08.000000000,2014-09-27 10:54:08.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 2807}]","[{'number': 1, 'created': '2014-09-23 19:14:14.000000000', 'files': ['security-guide/section_hypervisor-selection.xml'], 'web_link': 'https://opendev.org/openstack/security-doc/commit/2a99811de6edb874be3c450a5e6845732616e61d', 'message': 'Use reference section\n\nLike in other places in the guide, use a separate reference section with\nall links in the hypervisor section.\n\nChange-Id: I6ed0e0d96a2bb86b0bcbd479aae3ddb7901ee3e1\n'}]",0,123548,2a99811de6edb874be3c450a5e6845732616e61d,7,3,1,6547,,,0,"Use reference section

Like in other places in the guide, use a separate reference section with
all links in the hypervisor section.

Change-Id: I6ed0e0d96a2bb86b0bcbd479aae3ddb7901ee3e1
",git fetch https://review.opendev.org/openstack/security-doc refs/changes/48/123548/1 && git format-patch -1 --stdout FETCH_HEAD,['security-guide/section_hypervisor-selection.xml'],1,2a99811de6edb874be3c450a5e6845732616e61d,fix-reference," academic studies attackers were able to identify software packages <section xml:id=""hypversisor-selection-references""> <title>References</title> <itemizedlist> <listitem> <para> Fine grain Cross-VM Attacks on Xen and VMware are possible - Apecechea and others. <link xlink:href=""https://eprint.iacr.org/2014/248.pdf"" >https://eprint.iacr.org/2014/248.pdf</link> </para> </listitem> <listitem> <para> Memory Deduplication as a Threat to the Guest OS - Suzaki and others. <link xlink:href=""https://staff.aist.go.jp/c.artho/papers/EuroSec2011-suzaki.pdf"" >https://staff.aist.go.jp/c.artho/papers/EuroSec2011-suzaki.pdf</link> </para> </listitem> <listitem> <para><link xlink:href=""http://www.linux-kvm.org/page/KSM"">KVM: Kernel Samepage Merging</link></para> </listitem> <listitem> <para><link xlink:href=""http://wiki.xen.org/wiki/Xen_Security_Modules_:_XSM-FLASK"" >XSM: Xen Security Modules</link></para> </listitem> <listitem> <para><link xlink:href=""http://selinuxproject.org/page/SVirt"" >xVirt: Mandatory Access Control for Linux-based virtualization</link></para> </listitem> <listitem> <para><link xlink:href=""http://www.intel.com/txt"">TXT: Intel Trusted Execution Technology</link></para> </listitem> <listitem> <para><link xlink:href=""http://wiki.apparmor.net/index.php/Main_Page"" >AppArmor: Linux security module implementing MAC</link></para> </listitem> <listitem> <para><link xlink:href=""https://www.kernel.org/doc/Documentation/cgroups/cgroups.txt"" >cgroups: Linux kernel feature to control resource usage</link></para> </listitem> </itemizedlist> </section>"," academic studies<footnote> <para>Fine grain Cross-VM Attacks on Xen and VMware are possible - Apecechea and others. <link xlink:href=""https://eprint.iacr.org/2014/248.pdf"" >https://eprint.iacr.org/2014/248.pdf</link></para> </footnote><footnote> <para>Memory Deduplication as a Threat to the Guest OS - Suzaki and others. <link xlink:href=""https://staff.aist.go.jp/c.artho/papers/EuroSec2011-suzaki.pdf"" >https://staff.aist.go.jp/c.artho/papers/EuroSec2011-suzaki.pdf</link></para> </footnote>attackers were able to identify software packages <para><link xlink:href=""http://www.linux-kvm.org/page/KSM"">KVM: Kernel Samepage Merging</link></para> <para><link xlink:href=""http://wiki.xen.org/wiki/Xen_Security_Modules_:_XSM-FLASK"" >XSM: Xen Security Modules</link></para> <para><link xlink:href=""http://selinuxproject.org/page/SVirt"" >xVirt: Mandatory Access Control for Linux-based virtualization</link></para> <para><link xlink:href=""http://www.intel.com/txt"">TXT: Intel Trusted Execution Technology</link></para> <para><link xlink:href=""http://wiki.apparmor.net/index.php/Main_Page"" >AppArmor: Linux security module implementing MAC</link></para> <para><link xlink:href=""https://www.kernel.org/doc/Documentation/cgroups/cgroups.txt"" >cgroups: Linux kernel feature to control resource usage</link></para>",52,29
openstack%2Fopenstack-manuals~master~I63490e15fe0eceff448867fdaf4cfd04e19a89ea,openstack/openstack-manuals,master,I63490e15fe0eceff448867fdaf4cfd04e19a89ea,Update for python-heatclient 0.2.12,MERGED,2014-09-26 06:07:37.000000000,2014-09-27 10:48:46.000000000,2014-09-27 10:48:45.000000000,"[{'_account_id': 3}, {'_account_id': 167}]","[{'number': 1, 'created': '2014-09-26 06:07:37.000000000', 'files': ['doc/cli-reference/generated/ch_cli_heat_commands.xml'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/bae0c5061839c539013255a8e341c76b6c1e8c96', 'message': 'Update for python-heatclient 0.2.12\n\nChange-Id: I63490e15fe0eceff448867fdaf4cfd04e19a89ea\n'}]",0,124286,bae0c5061839c539013255a8e341c76b6c1e8c96,6,2,1,6547,,,0,"Update for python-heatclient 0.2.12

Change-Id: I63490e15fe0eceff448867fdaf4cfd04e19a89ea
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/86/124286/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/cli-reference/generated/ch_cli_heat_commands.xml'],1,bae0c5061839c539013255a8e341c76b6c1e8c96,heat-0.2.12," <literal>0.2.12</literal>. <term><command>action-check</command></term> <listitem> <para> Check that stack resources are in expected states. </para> </listitem> </varlistentry> <varlistentry> <section xml:id=""heatclient_subcommand_action-check""> <title>heat action-check</title> <screen><computeroutput>usage: heat action-check &lt;NAME or ID&gt;</computeroutput></screen> <para> Check that stack resources are in expected states. </para> <variablelist wordsize=""10""> <title>Positional arguments</title> <varlistentry> <term><command>&lt;NAME or ID&gt;</command></term> <listitem> <para> Name or ID of stack to check. </para> </listitem> </varlistentry> </variablelist> </section>", <literal>0.2.11</literal>.,26,1
openstack%2Fopenstack-manuals~master~I8e7107e86de43569fb706d2bf8b38483b19c93d4,openstack/openstack-manuals,master,I8e7107e86de43569fb706d2bf8b38483b19c93d4,Emdash fixes,MERGED,2014-09-25 17:57:51.000000000,2014-09-27 10:48:29.000000000,2014-09-27 10:48:28.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 964}]","[{'number': 1, 'created': '2014-09-25 17:57:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/86e392e6269ca81f058d546009326b45d545bcc6', 'message': 'Emdash fixes\n\nUse &mdash; instead of unicode character.\nUse colon instead of &mdash in section_support-compute.xml.\n\nChange-Id: I8e7107e86de43569fb706d2bf8b38483b19c93d4\n'}, {'number': 2, 'created': '2014-09-25 18:08:55.000000000', 'files': ['doc/config-reference/block-storage/drivers/coraid-driver.xml', 'doc/config-reference/block-storage/drivers/hds-hnas-driver.xml', 'doc/common/section_support-compute.xml', 'doc/user-guide-admin/section_dashboard_admin_manage_projects_users.xml'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/347d84bfee43adf508bb84cd9581e5b68fad152d', 'message': 'Emdash fixes\n\nUse &mdash; instead of unicode character.\nUse colon instead of &mdash in section_support-compute.xml.\nRemove wrong usage of ""--"" in hds-hnas-driver.xml.\n\nChange-Id: I8e7107e86de43569fb706d2bf8b38483b19c93d4\n'}]",0,124120,347d84bfee43adf508bb84cd9581e5b68fad152d,8,3,2,6547,,,0,"Emdash fixes

Use &mdash; instead of unicode character.
Use colon instead of &mdash in section_support-compute.xml.
Remove wrong usage of ""--"" in hds-hnas-driver.xml.

Change-Id: I8e7107e86de43569fb706d2bf8b38483b19c93d4
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/20/124120/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/config-reference/block-storage/drivers/coraid-driver.xml', 'doc/common/section_support-compute.xml', 'doc/user-guide-admin/section_dashboard_admin_manage_projects_users.xml']",3,86e392e6269ca81f058d546009326b45d545bcc6,entities,"<!DOCTYPE section [ <!ENTITY % openstack SYSTEM ""../common/entities/openstack.ent""> %openstack; ]> not automatically terminated though&mdash;you must stop", not automatically terminated thoughyou must stop,36,14
openstack%2Fneutron~stable%2Ficehouse~I9f0a5149d24a4c003409728e50376569c97e7325,openstack/neutron,stable/icehouse,I9f0a5149d24a4c003409728e50376569c97e7325,Don't convert numeric protocol values to int,MERGED,2014-06-26 08:00:23.000000000,2014-09-27 07:52:10.000000000,2014-09-27 07:52:08.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 1420}, {'_account_id': 2592}, {'_account_id': 5170}, {'_account_id': 5948}, {'_account_id': 6659}, {'_account_id': 6788}, {'_account_id': 6854}, {'_account_id': 7293}, {'_account_id': 8124}, {'_account_id': 8213}, {'_account_id': 8279}, {'_account_id': 8645}, {'_account_id': 9656}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9846}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12040}, {'_account_id': 12737}, {'_account_id': 13308}]","[{'number': 1, 'created': '2014-06-26 08:00:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/73b19a9a1796102f9ad968ce2daa18303a4fbcd7', 'message': ""Don't convert numeric protocol values to int\n\nThey are treated as strings everywhere. Converting them to int causes problems\nwhen using postgresql as the database backend because it doesn't automatically\ncast them back to integer.\n\nChange-Id: I9f0a5149d24a4c003409728e50376569c97e7325\nCloses-bug: 1330490\n""}, {'number': 2, 'created': '2014-07-08 09:22:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/892f93856d3b30ccc12b89b21b490912d5cde8ac', 'message': ""Don't convert numeric protocol values to int\n\nThey are treated as strings everywhere. Converting them to int causes problems\nwhen using postgresql as the database backend because it doesn't automatically\ncast them back to integer.\n\nChange-Id: I9f0a5149d24a4c003409728e50376569c97e7325\nCloses-bug: 1330490\n(cherry picked from commit 3be62f878e8165cf3bce357a00b8a583773f7d3b)\n""}, {'number': 3, 'created': '2014-09-25 09:16:25.000000000', 'files': ['neutron/tests/unit/test_extension_security_group.py', 'neutron/extensions/securitygroup.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/a17a50057594027739c819264cebb3405ee8be42', 'message': ""Don't convert numeric protocol values to int\n\nThey are treated as strings everywhere. Converting them to int causes problems\nwhen using postgresql as the database backend because it doesn't automatically\ncast them back to integer.\n\nChange-Id: I9f0a5149d24a4c003409728e50376569c97e7325\nCloses-bug: 1330490\n(cherry picked from commit 3be62f878e8165cf3bce357a00b8a583773f7d3b)\n""}]",7,102754,a17a50057594027739c819264cebb3405ee8be42,80,29,3,2062,,,0,"Don't convert numeric protocol values to int

They are treated as strings everywhere. Converting them to int causes problems
when using postgresql as the database backend because it doesn't automatically
cast them back to integer.

Change-Id: I9f0a5149d24a4c003409728e50376569c97e7325
Closes-bug: 1330490
(cherry picked from commit 3be62f878e8165cf3bce357a00b8a583773f7d3b)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/54/102754/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/test_extension_security_group.py', 'neutron/extensions/securitygroup.py']",2,73b19a9a1796102f9ad968ce2daa18303a4fbcd7,bug/1330490, return value, return val,11,1
openstack%2Fsecurity-doc~master~Ia6027eb30db15bc244a6e0f0d57309f3e63efeb2,openstack/security-doc,master,Ia6027eb30db15bc244a6e0f0d57309f3e63efeb2,Updated from openstack-manuals,MERGED,2014-09-25 13:02:28.000000000,2014-09-27 07:34:11.000000000,2014-09-27 07:34:11.000000000,"[{'_account_id': 3}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-09-25 13:02:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/security-doc/commit/8024f28534b2ff5cdf5156964ef4a89955a7e996', 'message': 'Updated from openstack-manuals\n\nChange-Id: Ia6027eb30db15bc244a6e0f0d57309f3e63efeb2\n'}, {'number': 2, 'created': '2014-09-26 06:16:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/security-doc/commit/c9efe952731314ff40f1853d44e89db213193ed8', 'message': 'Updated from openstack-manuals\n\nChange-Id: Ia6027eb30db15bc244a6e0f0d57309f3e63efeb2\n'}, {'number': 3, 'created': '2014-09-26 14:31:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/security-doc/commit/20b7c0e04ebc3eab3a5bbcdc261d00f636a5b72b', 'message': 'Updated from openstack-manuals\n\nChange-Id: Ia6027eb30db15bc244a6e0f0d57309f3e63efeb2\n'}, {'number': 4, 'created': '2014-09-26 19:13:45.000000000', 'files': ['security-guide/openstack.ent'], 'web_link': 'https://opendev.org/openstack/security-doc/commit/d4d1eb6698d948fb2808b7b35ca34061a71e4cfe', 'message': 'Updated from openstack-manuals\n\nChange-Id: Ia6027eb30db15bc244a6e0f0d57309f3e63efeb2\n'}]",0,124046,d4d1eb6698d948fb2808b7b35ca34061a71e4cfe,13,2,4,11131,,,0,"Updated from openstack-manuals

Change-Id: Ia6027eb30db15bc244a6e0f0d57309f3e63efeb2
",git fetch https://review.opendev.org/openstack/security-doc refs/changes/46/124046/4 && git format-patch -1 --stdout FETCH_HEAD,['security-guide/openstack.ent'],1,8024f28534b2ff5cdf5156964ef4a89955a7e996,openstack/openstack-manuals,"<!ENTITY times ""&#215;""><imagedata fileref=""../common/figures/Check_mark_23x20_02.svg""","<imagedata fileref=""static/Check_mark_23x20_02.svg""",2,1
openstack%2Foperations-guide~master~I88d0a614396a011ca5505a68600b980fecd35204,openstack/operations-guide,master,I88d0a614396a011ca5505a68600b980fecd35204,Updated from openstack-manuals,MERGED,2014-09-25 13:02:25.000000000,2014-09-27 07:34:03.000000000,2014-09-27 07:34:03.000000000,"[{'_account_id': 3}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-09-25 13:02:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/operations-guide/commit/87c72c74f746e8453fa9bf4c23dc0f2251fc6d61', 'message': 'Updated from openstack-manuals\n\nChange-Id: I88d0a614396a011ca5505a68600b980fecd35204\n'}, {'number': 2, 'created': '2014-09-26 06:16:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/operations-guide/commit/b19ff3e77857a2b73c61e0fd0f4704e7246ecfa5', 'message': 'Updated from openstack-manuals\n\nChange-Id: I88d0a614396a011ca5505a68600b980fecd35204\n'}, {'number': 3, 'created': '2014-09-26 09:25:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/operations-guide/commit/3bb5c60333f9decf178b98c3cb3c7795b938e035', 'message': 'Updated from openstack-manuals\n\nChange-Id: I88d0a614396a011ca5505a68600b980fecd35204\n'}, {'number': 4, 'created': '2014-09-26 14:31:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/operations-guide/commit/de1e0500124b8d2ea646513a9f1be558503951e6', 'message': 'Updated from openstack-manuals\n\nChange-Id: I88d0a614396a011ca5505a68600b980fecd35204\n'}, {'number': 5, 'created': '2014-09-26 19:13:42.000000000', 'files': ['doc/openstack-ops/openstack.ent'], 'web_link': 'https://opendev.org/openstack/operations-guide/commit/f41100f7a4ada068b0712ef2c8bebadae061736e', 'message': 'Updated from openstack-manuals\n\nChange-Id: I88d0a614396a011ca5505a68600b980fecd35204\n'}]",0,124045,f41100f7a4ada068b0712ef2c8bebadae061736e,15,2,5,11131,,,0,"Updated from openstack-manuals

Change-Id: I88d0a614396a011ca5505a68600b980fecd35204
",git fetch https://review.opendev.org/openstack/operations-guide refs/changes/45/124045/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/openstack-ops/openstack.ent'],1,87c72c74f746e8453fa9bf4c23dc0f2251fc6d61,openstack/openstack-manuals,"<!ENTITY times ""&#215;""><imagedata fileref=""../common/figures/Check_mark_23x20_02.svg""","<imagedata fileref=""figures/Check_mark_23x20_02.svg""",2,1
openstack%2Fha-guide~master~I5a995612bf595dd1fa934f838b70c8371059fcc0,openstack/ha-guide,master,I5a995612bf595dd1fa934f838b70c8371059fcc0,Updated from openstack-manuals,MERGED,2014-09-26 06:16:01.000000000,2014-09-27 07:29:22.000000000,2014-09-27 07:29:22.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-09-26 06:16:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ha-guide/commit/a77b3b8a6d98f62994bf0da9930687f848a37333', 'message': 'Updated from openstack-manuals\n\nChange-Id: I5a995612bf595dd1fa934f838b70c8371059fcc0\n'}, {'number': 2, 'created': '2014-09-26 14:30:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ha-guide/commit/cd6c9d54d944db2aae829c15994dd32dc958c4dd', 'message': 'Updated from openstack-manuals\n\nChange-Id: I5a995612bf595dd1fa934f838b70c8371059fcc0\n'}, {'number': 3, 'created': '2014-09-26 19:13:37.000000000', 'files': ['doc/high-availability-guide/openstack.ent'], 'web_link': 'https://opendev.org/openstack/ha-guide/commit/02455f1c9d2e9f8ad7a3ec3a05ddc16a6db045d5', 'message': 'Updated from openstack-manuals\n\nChange-Id: I5a995612bf595dd1fa934f838b70c8371059fcc0\n'}]",3,124297,02455f1c9d2e9f8ad7a3ec3a05ddc16a6db045d5,13,3,3,11131,,,0,"Updated from openstack-manuals

Change-Id: I5a995612bf595dd1fa934f838b70c8371059fcc0
",git fetch https://review.opendev.org/openstack/ha-guide refs/changes/97/124297/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/high-availability-guide/openstack.ent'],1,a77b3b8a6d98f62994bf0da9930687f848a37333,openstack/openstack-manuals,"<!-- The master of this file is in openstack-manuals repository, file doc/common/entities/openstack.ent. Any changes to the master file will override changs in other repositories. --><!ENTITY hellip ""&#133;"">",,5,1
openstack%2Fneutron~master~I7fee9728a41b126990918f2271e85924a6746772,openstack/neutron,master,I7fee9728a41b126990918f2271e85924a6746772,Cisco VPNaaS and L3 router plugin integration,ABANDONED,2014-09-26 12:53:55.000000000,2014-09-27 06:59:35.000000000,,"[{'_account_id': 5170}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10503}]","[{'number': 1, 'created': '2014-09-26 12:53:55.000000000', 'files': ['neutron/plugins/cisco/l3/configdrive_templates/csr1kv_cfg_template', 'neutron/tests/unit/services/vpn/service_drivers/test_cisco_ipsec.py', 'neutron/plugins/cisco/cfg_agent/device_drivers/csr1kv/cisco_csr1kv_snippets.py', 'neutron/tests/unit/services/vpn/device_drivers/test_cisco_ipsec.py', 'neutron/tests/unit/services/vpn/service_drivers/test_cisco_config_loader.py', 'neutron/services/vpn/service_drivers/cisco_cfg_loader.py', 'neutron/plugins/cisco/db/l3/l3_router_appliance_db.py', 'neutron/services/vpn/service_drivers/cisco_ipsec.py', 'neutron/services/vpn/device_drivers/cisco_csr_rest_client.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/6369aeb043bbae78e832def7987ea9f048909544', 'message': 'Cisco VPNaaS and L3 router plugin integration\n\nPlease provide review comments for this change in the upstream review:\n\nhttps://review.openstack.org/#/c/123877/\n\nChanges made there, will be reflected in this repo as well.\n\nChange-Id: I7fee9728a41b126990918f2271e85924a6746772\nImplements: blueprint cisco-vpnaas-and-router-integration\n'}]",0,124392,6369aeb043bbae78e832def7987ea9f048909544,15,13,1,6659,,,0,"Cisco VPNaaS and L3 router plugin integration

Please provide review comments for this change in the upstream review:

https://review.openstack.org/#/c/123877/

Changes made there, will be reflected in this repo as well.

Change-Id: I7fee9728a41b126990918f2271e85924a6746772
Implements: blueprint cisco-vpnaas-and-router-integration
",git fetch https://review.opendev.org/openstack/neutron refs/changes/92/124392/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/cisco/l3/configdrive_templates/csr1kv_cfg_template', 'neutron/tests/unit/services/vpn/service_drivers/test_cisco_ipsec.py', 'neutron/plugins/cisco/cfg_agent/device_drivers/csr1kv/cisco_csr1kv_snippets.py', 'neutron/tests/unit/services/vpn/device_drivers/test_cisco_ipsec.py', 'neutron/services/vpn/service_drivers/cisco_cfg_loader.py', 'neutron/tests/unit/services/vpn/service_drivers/test_cisco_config_loader.py', 'neutron/plugins/cisco/db/l3/l3_router_appliance_db.py', 'neutron/services/vpn/service_drivers/cisco_ipsec.py', 'neutron/services/vpn/device_drivers/cisco_csr_rest_client.py']",9,6369aeb043bbae78e832def7987ea9f048909544,bp/cisco-vpnaas-and-router-integration," self.inner_if_name = settings.get('inner_if_name', '') self.outer_if_name = settings.get('outer_if_name', '') vrf = settings.get('vrf') self.vrf_prefix = 'vrf/%s/' % vrf if vrf else """" # VPN Specific APIs return self.post_request(self.vrf_prefix + URI_VPN_IKE_KEYRINGS, payload=psk_info) u'tunnel-ip-address': self.outer_if_name, u'ip-address': self.inner_if_name return self.post_request(self.vrf_prefix + URI_VPN_SITE_TO_SITE, return self.post_request(self.vrf_prefix + URI_ROUTING_STATIC_ROUTES, return self.delete_request( self.vrf_prefix + URI_ROUTING_STATIC_ROUTES_ID % route_id) return self.put_request( self.vrf_prefix + URI_VPN_SITE_TO_SITE_STATE % tunnel, info) return self.delete_request( self.vrf_prefix + URI_VPN_SITE_TO_SITE_ID % conn_id) return self.delete_request( self.vrf_prefix + URI_VPN_IKE_KEYRINGS_ID % key_id) results = self.get_request(self.vrf_prefix + URI_VPN_SITE_ACTIVE_SESSIONS)"," self.tunnel_ip = settings.get('external_ip', '') self.tunnel_if_name = settings.get('tunnel_if_name', '') return self.post_request(URI_VPN_IKE_KEYRINGS, payload=psk_info) u'tunnel-ip-address': self.tunnel_ip, u'ip-address': self.tunnel_if_name return self.post_request(URI_VPN_SITE_TO_SITE, return self.post_request(URI_ROUTING_STATIC_ROUTES, return self.delete_request(URI_ROUTING_STATIC_ROUTES_ID % route_id) return self.put_request(URI_VPN_SITE_TO_SITE_STATE % tunnel, info) return self.delete_request(URI_VPN_SITE_TO_SITE_ID % conn_id) return self.delete_request(URI_VPN_IKE_KEYRINGS_ID % key_id) results = self.get_request(URI_VPN_SITE_ACTIVE_SESSIONS)",201,747
openstack%2Fnova~stable%2Ficehouse~I874ade4456b92a63959a765c7851bcd001befa32,openstack/nova,stable/icehouse,I874ade4456b92a63959a765c7851bcd001befa32,Fixes Hyper-V agent force_hyperv_utils_v1 flag issue,MERGED,2014-09-10 10:34:05.000000000,2014-09-27 06:19:02.000000000,2014-09-27 06:19:00.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 1653}, {'_account_id': 3185}, {'_account_id': 5170}, {'_account_id': 8213}, {'_account_id': 9656}, {'_account_id': 10814}, {'_account_id': 11531}, {'_account_id': 12604}]","[{'number': 1, 'created': '2014-09-10 10:34:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/34bda8a403240df8220f7b436e8dba492cbdf129', 'message': ""Fixes Hyper-V agent force_hyperv_utils_v1 flag issue\n\nWMI root\\virtualization namespace v1 (in Hyper-V) has been removed\nfrom Windows Server / Hyper-V Server 2012 R2.\n\nHyper-V compute agent now creates instances which uses\nroot\\virtualization\\v2 namespace if the agent's OS is\nWindows Server / Hyper-V Server 2012 R2 or newer.\n\nCloses-Bug: #1344036\n\nChange-Id: I874ade4456b92a63959a765c7851bcd001befa32\n(cherry picked from 52de9395e5fe4f328f6dab0b35d660a700787c76)\n\nConflicts:\n\tnova/tests/virt/hyperv/test_migrationops.py\n\tnova/tests/virt/hyperv/test_vmops.py\n\nChange-Id: I874ade4456b92a63959a765c7851bcd001befa32\n""}, {'number': 2, 'created': '2014-09-17 12:48:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d9b0852ea88d4bd3d3a8219713979685df1631c0', 'message': ""Fixes Hyper-V agent force_hyperv_utils_v1 flag issue\n\nWMI root\\virtualization namespace v1 (in Hyper-V) has been removed\nfrom Windows Server / Hyper-V Server 2012 R2.\n\nHyper-V compute agent now creates instances which uses\nroot\\virtualization\\v2 namespace if the agent's OS is\nWindows Server / Hyper-V Server 2012 R2 or newer.\n\nCloses-Bug: #1344036\n\nChange-Id: I874ade4456b92a63959a765c7851bcd001befa32\n(cherry picked from 52de9395e5fe4f328f6dab0b35d660a700787c76)\n\nConflicts:\n\tnova/tests/virt/hyperv/test_migrationops.py\n\tnova/tests/virt/hyperv/test_vmops.py\n\nChange-Id: I874ade4456b92a63959a765c7851bcd001befa32\n""}, {'number': 3, 'created': '2014-09-24 11:43:20.000000000', 'files': ['nova/tests/virt/hyperv/test_migrationops.py', 'nova/tests/virt/hyperv/test_vmops.py', 'nova/tests/virt/hyperv/test_hypervapi.py', 'nova/tests/virt/hyperv/test_utilsfactory.py', 'nova/virt/hyperv/utilsfactory.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/7e091733040e9e7dc29dd1ecfce52ee27d6efd99', 'message': ""Fixes Hyper-V agent force_hyperv_utils_v1 flag issue\n\nWMI root\\virtualization namespace v1 (in Hyper-V) has been removed\nfrom Windows Server / Hyper-V Server 2012 R2.\n\nHyper-V compute agent now creates instances which uses\nroot\\virtualization\\v2 namespace if the agent's OS is\nWindows Server / Hyper-V Server 2012 R2 or newer.\n\nCloses-Bug: #1344036\n(cherry picked from 52de9395e5fe4f328f6dab0b35d660a700787c76)\n\nChange-Id: I874ade4456b92a63959a765c7851bcd001befa32\n""}]",0,120372,7e091733040e9e7dc29dd1ecfce52ee27d6efd99,45,10,3,3185,,,0,"Fixes Hyper-V agent force_hyperv_utils_v1 flag issue

WMI root\virtualization namespace v1 (in Hyper-V) has been removed
from Windows Server / Hyper-V Server 2012 R2.

Hyper-V compute agent now creates instances which uses
root\virtualization\v2 namespace if the agent's OS is
Windows Server / Hyper-V Server 2012 R2 or newer.

Closes-Bug: #1344036
(cherry picked from 52de9395e5fe4f328f6dab0b35d660a700787c76)

Change-Id: I874ade4456b92a63959a765c7851bcd001befa32
",git fetch https://review.opendev.org/openstack/nova refs/changes/72/120372/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/hyperv/test_migrationops.py', 'nova/tests/virt/hyperv/test_vmops.py', 'nova/tests/virt/hyperv/test_hypervapi.py', 'nova/tests/virt/hyperv/test_utilsfactory.py', 'nova/virt/hyperv/utilsfactory.py']",5,34bda8a403240df8220f7b436e8dba492cbdf129,bug/1344036,"from nova.i18n import _def _get_virt_utils_class(v1_class, v2_class): # The ""root/virtualization"" WMI namespace is no longer supported on # Windows Server / Hyper-V Server 2012 R2 / Windows 8.1 # (kernel version 6.3) or above. if (CONF.hyperv.force_hyperv_utils_v1 and get_hostutils().check_min_windows_version(6, 3)): raise vmutils.HyperVException( _('The ""force_hyperv_utils_v1"" option cannot be set to ""True"" ' 'on Windows Server / Hyper-V Server 2012 R2 or above as the WMI ' '""root/virtualization"" namespace is no longer supported.')) return _get_class(v1_class, v2_class, CONF.hyperv.force_hyperv_utils_v1) def get_vmutils(host='.'): return _get_virt_utils_class(vmutils.VMUtils, vmutilsv2.VMUtilsV2)(host) return _get_virt_utils_class(vhdutils.VHDUtils, vhdutilsv2.VHDUtilsV2)() return _get_virt_utils_class(networkutils.NetworkUtils, networkutilsv2.NetworkUtilsV2)() return _get_virt_utils_class(rdpconsoleutils.RDPConsoleUtils, rdpconsoleutilsv2.RDPConsoleUtilsV2)()","def get_vmutils(host='.'): return _get_class(vmutils.VMUtils, vmutilsv2.VMUtilsV2, CONF.hyperv.force_hyperv_utils_v1)(host) return _get_class(vhdutils.VHDUtils, vhdutilsv2.VHDUtilsV2, CONF.hyperv.force_hyperv_utils_v1)() return _get_class(networkutils.NetworkUtils, networkutilsv2.NetworkUtilsV2, CONF.hyperv.force_hyperv_utils_v1)() return _get_class(rdpconsoleutils.RDPConsoleUtils, rdpconsoleutilsv2.RDPConsoleUtilsV2, CONF.hyperv.force_hyperv_utils_v1)()",175,10
openstack%2Fnova~stable%2Ficehouse~I7d73b891545492e3e247eebdf02507cb630eb250,openstack/nova,stable/icehouse,I7d73b891545492e3e247eebdf02507cb630eb250,Fix nova image-show with queued image,MERGED,2014-05-27 22:58:30.000000000,2014-09-27 05:17:01.000000000,2014-09-27 05:16:58.000000000,"[{'_account_id': 3}, {'_account_id': 67}, {'_account_id': 308}, {'_account_id': 1653}, {'_account_id': 4395}, {'_account_id': 5170}, {'_account_id': 8213}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9656}, {'_account_id': 10118}]","[{'number': 1, 'created': '2014-05-27 22:58:30.000000000', 'files': ['nova/image/glance.py', 'nova/tests/image/test_glance.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/cce6d22c6430048d6789dec04b43c1483f4a0b8d', 'message': ""Fix nova image-show with queued image\n\nCurrently, the nova image-show command cannot be used with queued images,\nbecause they don't have certain attributes set yet. Nova will raise\nAttributeError, which causes a 500 to be returned from the API.\n\nChange-Id: I7d73b891545492e3e247eebdf02507cb630eb250\nCloses-bug: #1321186\n(cherry picked from commit 4c5dea6a5103a141b0ca13dcd57b32b3261180d1)\n""}]",0,95960,cce6d22c6430048d6789dec04b43c1483f4a0b8d,47,11,1,67,,,0,"Fix nova image-show with queued image

Currently, the nova image-show command cannot be used with queued images,
because they don't have certain attributes set yet. Nova will raise
AttributeError, which causes a 500 to be returned from the API.

Change-Id: I7d73b891545492e3e247eebdf02507cb630eb250
Closes-bug: #1321186
(cherry picked from commit 4c5dea6a5103a141b0ca13dcd57b32b3261180d1)
",git fetch https://review.opendev.org/openstack/nova refs/changes/60/95960/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/image/glance.py', 'nova/tests/image/test_glance.py']",2,cce6d22c6430048d6789dec04b43c1483f4a0b8d,bp-94338," @mock.patch('nova.image.glance._is_image_available') def test_show_queued_image_without_some_attrs(self, is_avail_mock): is_avail_mock.return_value = True client = mock.MagicMock() # fake image cls without disk_format, container_format, name attributes class fake_image_cls(object): id = 'b31aa5dd-f07a-4748-8f15-398346887584' deleted = False protected = False min_disk = 0 created_at = '2014-05-20T08:16:48' size = 0 status = 'queued' is_public = False min_ram = 0 owner = '980ec4870033453ead65c0470a78b8a8' updated_at = '2014-05-20T08:16:48' glance_image = fake_image_cls() client.call.return_value = glance_image ctx = mock.sentinel.ctx service = glance.GlanceImageService(client) image_info = service.show(ctx, glance_image.id) client.call.assert_called_once_with(ctx, 1, 'get', glance_image.id) NOVA_IMAGE_ATTRIBUTES = set(['size', 'disk_format', 'owner', 'container_format', 'status', 'id', 'name', 'created_at', 'updated_at', 'deleted', 'deleted_at', 'checksum', 'min_disk', 'min_ram', 'is_public', 'properties']) self.assertEqual(NOVA_IMAGE_ATTRIBUTES, set(image_info.keys())) ",,43,0
openstack%2Fnova~stable%2Ficehouse~I67b7dd16a94fe60d873c012f6bd246ab24500d5a,openstack/nova,stable/icehouse,I67b7dd16a94fe60d873c012f6bd246ab24500d5a,_translate_from_glance() can cause an unnecessary HTTP request,MERGED,2014-05-27 22:56:28.000000000,2014-09-27 05:16:36.000000000,2014-09-27 05:16:33.000000000,"[{'_account_id': 3}, {'_account_id': 67}, {'_account_id': 1420}, {'_account_id': 5170}, {'_account_id': 8213}, {'_account_id': 9008}, {'_account_id': 9126}, {'_account_id': 9656}, {'_account_id': 10118}]","[{'number': 1, 'created': '2014-05-27 22:56:28.000000000', 'files': ['nova/image/glance.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/aff80d58bb85f52d33d1562b76dfd6f2c8b025d3', 'message': ""_translate_from_glance() can cause an unnecessary HTTP request\n\nAfter returning from a get() call to python-glanceclient, nova runs a\ntranslation function on the returned Image to get the data it wants. Part of\nthis process is checking for an expected set of attributes, one of which is\nthe deletion time ('deleted_at'). However, if the image has not been deleted,\ndeleted_at key will not exist. This forces another call to glance to occur for\nthe same image. A similar problem exists for the checksum attribute, which does\nnot exist before an image is active. The fix here is to only consider\ndeleted_at and checksum if they are expected to be present.\n\nChange-Id: I67b7dd16a94fe60d873c012f6bd246ab24500d5a\nCloses-Bug: #1275173\n(cherry picked from commit 8fcefef5c6db3bb7f182df62de9c5bf986f97303)\n""}]",0,95958,aff80d58bb85f52d33d1562b76dfd6f2c8b025d3,30,9,1,67,,,0,"_translate_from_glance() can cause an unnecessary HTTP request

After returning from a get() call to python-glanceclient, nova runs a
translation function on the returned Image to get the data it wants. Part of
this process is checking for an expected set of attributes, one of which is
the deletion time ('deleted_at'). However, if the image has not been deleted,
deleted_at key will not exist. This forces another call to glance to occur for
the same image. A similar problem exists for the checksum attribute, which does
not exist before an image is active. The fix here is to only consider
deleted_at and checksum if they are expected to be present.

Change-Id: I67b7dd16a94fe60d873c012f6bd246ab24500d5a
Closes-Bug: #1275173
(cherry picked from commit 8fcefef5c6db3bb7f182df62de9c5bf986f97303)
",git fetch https://review.opendev.org/openstack/nova refs/changes/58/95958/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/image/glance.py'],1,aff80d58bb85f52d33d1562b76dfd6f2c8b025d3,bp-70518," #NOTE(hdd): If a key is not found, base.Resource.__getattr__() may perform # a get(), resulting in a useless request back to glance. This list is # therefore sorted, with dependent attributes as the end # 'deleted_at' depends on 'deleted' # 'checksum' depends on 'status' == 'active' 'container_format', 'status', 'id', 'deleted', 'deleted_at', 'checksum', for attr in IMAGE_ATTRIBUTES: if attr == 'deleted_at' and not output['deleted']: output[attr] = None elif attr == 'checksum' and output['status'] != 'active': output[attr] = None else: output[attr] = getattr(image, attr)"," 'container_format', 'checksum', 'id', 'deleted_at', 'deleted', 'status', for attr in IMAGE_ATTRIBUTES: output[attr] = getattr(image, attr, None)",14,3
openstack%2Fhorizon~master~I0b834e58b373c7c1179a3d56fe3f78415131b345,openstack/horizon,master,I0b834e58b373c7c1179a3d56fe3f78415131b345,Imported Translations from Transifex,MERGED,2014-09-25 06:17:49.000000000,2014-09-27 05:16:23.000000000,2014-09-27 05:16:22.000000000,"[{'_account_id': 3}, {'_account_id': 1941}, {'_account_id': 4264}, {'_account_id': 6547}, {'_account_id': 6914}, {'_account_id': 9576}]","[{'number': 1, 'created': '2014-09-25 06:17:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/0024620b8c54561fbb656a1d21e2582964186950', 'message': 'Imported Translations from Transifex\n\nChange-Id: I0b834e58b373c7c1179a3d56fe3f78415131b345\n'}, {'number': 2, 'created': '2014-09-26 06:04:02.000000000', 'files': ['horizon/locale/zh_CN/LC_MESSAGES/django.po', 'horizon/locale/zh_TW/LC_MESSAGES/django.po', 'openstack_dashboard/locale/fr/LC_MESSAGES/django.po', 'horizon/locale/zh_TW/LC_MESSAGES/djangojs.po', 'openstack_dashboard/locale/en_GB/LC_MESSAGES/django.po', 'horizon/locale/ja/LC_MESSAGES/django.po', 'openstack_dashboard/locale/de/LC_MESSAGES/django.po', 'horizon/locale/pl_PL/LC_MESSAGES/django.po', 'horizon/locale/ja/LC_MESSAGES/djangojs.po', 'openstack_dashboard/locale/en/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ja/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ko_KR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/es/LC_MESSAGES/django.po', 'horizon/locale/fr/LC_MESSAGES/django.po', 'horizon/locale/sr/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_CN/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_TW/LC_MESSAGES/django.po', 'openstack_dashboard/locale/pt_BR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en_AU/LC_MESSAGES/django.po'], 'web_link': 'https://opendev.org/openstack/horizon/commit/f73e76674414099948794ef2e89217e38e9916d8', 'message': 'Imported Translations from Transifex\n\nChange-Id: I0b834e58b373c7c1179a3d56fe3f78415131b345\n'}]",0,123939,f73e76674414099948794ef2e89217e38e9916d8,22,6,2,11131,,,0,"Imported Translations from Transifex

Change-Id: I0b834e58b373c7c1179a3d56fe3f78415131b345
",git fetch https://review.opendev.org/openstack/horizon refs/changes/39/123939/1 && git format-patch -1 --stdout FETCH_HEAD,"['horizon/locale/zh_CN/LC_MESSAGES/django.po', 'horizon/locale/zh_TW/LC_MESSAGES/django.po', 'openstack_dashboard/locale/fr/LC_MESSAGES/django.po', 'horizon/locale/zh_TW/LC_MESSAGES/djangojs.po', 'openstack_dashboard/locale/en_GB/LC_MESSAGES/django.po', 'horizon/locale/ja/LC_MESSAGES/django.po', 'openstack_dashboard/locale/pl_PL/LC_MESSAGES/django.po', 'openstack_dashboard/locale/de/LC_MESSAGES/django.po', 'horizon/locale/pl_PL/LC_MESSAGES/django.po', 'horizon/locale/ja/LC_MESSAGES/djangojs.po', 'openstack_dashboard/locale/en/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ja/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ko_KR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/es/LC_MESSAGES/django.po', 'horizon/locale/fr/LC_MESSAGES/django.po', 'horizon/locale/sr/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_CN/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_TW/LC_MESSAGES/django.po', 'openstack_dashboard/locale/pt_BR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en_AU/LC_MESSAGES/django.po']",20,0024620b8c54561fbb656a1d21e2582964186950,transifex/translations,"""POT-Creation-Date: 2014-09-24 23:17-0500\n"" ""PO-Revision-Date: 2014-09-25 03:57+0000\n"" ""Last-Translator: openstackjenkins <jenkins@openstack.org>\n""#: dashboards/settings/password/forms.py:62#: dashboards/admin/info/tables.py:170 dashboards/admin/instances/tables.py:82 #: dashboards/admin/instances/tables.py:125#: dashboards/admin/networks/ports/tables.py:91 #: dashboards/admin/networks/subnets/tables.py:97 #: dashboards/admin/routers/tables.py:54#: dashboards/project/access_and_security/security_groups/tables.py:122#: dashboards/project/data_processing/cluster_templates/tables.py:100#: dashboards/project/firewalls/forms.py:117 #: dashboards/project/firewalls/forms.py:144 #: dashboards/project/firewalls/tables.py:160 #: dashboards/project/firewalls/tables.py:192 #: dashboards/project/firewalls/tables.py:209#: dashboards/project/loadbalancers/tables.py:221#: dashboards/project/networks/tables.py:103#: dashboards/project/networks/ports/tables.py:55 #: dashboards/project/networks/subnets/tables.py:98#: dashboards/project/routers/tables.py:153 #: dashboards/project/routers/ports/tables.py:94#: dashboards/project/volumes/snapshots/tables.py:124#: dashboards/project/volumes/volumes/tables.py:311 #: dashboards/project/volumes/volumes/tables.py:340#: dashboards/project/instances/tables.py:902#: dashboards/project/volumes/volumes/tables.py:348#: dashboards/admin/aggregates/views.py:51#: dashboards/admin/aggregates/views.py:63#: dashboards/admin/aggregates/views.py:96#: dashboards/admin/aggregates/views.py:130#: dashboards/admin/aggregates/views.py:143#: dashboards/project/access_and_security/security_groups/tables.py:123#: dashboards/project/data_processing/cluster_templates/tables.py:111#: dashboards/project/firewalls/forms.py:119 #: dashboards/project/firewalls/forms.py:147#: dashboards/project/loadbalancers/tables.py:223#: dashboards/project/volumes/volumes/tables.py:314#: dashboards/project/volumes/volumes/tables.py:245#: dashboards/admin/info/tables.py:214 dashboards/admin/overview/views.py:33 #: dashboards/identity/projects/workflows.py:57 #: dashboards/project/overview/views.py:32 usage/quotas.py:67 msgid ""RAM (MB)"" msgstr ""RAM (MB)""msgid ""Root Disk (GB)"" msgstr """"msgid ""Ephemeral Disk (GB)"" msgstr """"msgid ""Swap Disk (MB)"" msgstr """"#: dashboards/project/routers/ports/tables.py:100#: dashboards/project/volumes/volumes/tables.py:343#: dashboards/admin/instances/tables.py:37 #: dashboards/admin/instances/tables.py:159#: dashboards/project/instances/tables.py:921#: dashboards/project/instances/tables.py:863 #: dashboards/project/instances/tables.py:885#: dashboards/admin/instances/tables.py:121#: dashboards/admin/instances/tables.py:137 #: dashboards/admin/networks/tables.py:95#: dashboards/admin/networks/ports/tables.py:97#: dashboards/project/firewalls/tables.py:215#: dashboards/project/instances/tables.py:897#: dashboards/project/loadbalancers/tables.py:228 #: dashboards/project/loadbalancers/tables.py:262#: dashboards/project/networks/tables.py:109 #: dashboards/project/networks/ports/tables.py:59#: dashboards/project/routers/tables.py:157 #: dashboards/project/routers/ports/tables.py:98#: dashboards/project/volumes/volumes/tables.py:321#: dashboards/admin/instances/tables.py:85 #: dashboards/project/instances/tables.py:864#: dashboards/admin/instances/tables.py:127#: dashboards/project/instances/tables.py:887#: dashboards/admin/images/views.py:80#: dashboards/admin/images/views.py:177#: dashboards/admin/images/views.py:190#: dashboards/identity/users/tables.py:178#: dashboards/project/firewalls/tables.py:178#: dashboards/admin/info/tables.py:205 usage/quotas.py:66#: dashboards/project/volumes/volumes/tables.py:64 #: dashboards/project/volumes/volumes/tables.py:357 #: dashboards/project/volumes/volumes/tables.py:371 usage/quotas.py:74#: dashboards/project/volumes/snapshots/tables.py:53 #: dashboards/project/volumes/snapshots/tables.py:133 usage/quotas.py:75#: dashboards/project/access_and_security/security_groups/tables.py:130#: dashboards/project/instances/workflows/create_instance.py:489#: dashboards/project/access_and_security/security_groups/tables.py:258#: dashboards/admin/networks/ports/tables.py:94#: dashboards/project/networks/ports/tables.py:57 #: dashboards/project/routers/ports/tables.py:97 usage/quotas.py:69#: dashboards/admin/instances/tables.py:34#: dashboards/admin/instances/tables.py:35#: dashboards/admin/instances/tables.py:36#: dashboards/project/volumes/volumes/tables.py:396#: dashboards/admin/instances/tables.py:53#: dashboards/admin/instances/tables.py:80 #: dashboards/admin/instances/tables.py:114#: dashboards/admin/networks/tables.py:86 #: dashboards/admin/routers/tables.py:52#: dashboards/admin/instances/tables.py:81#: dashboards/admin/instances/tables.py:83#: dashboards/admin/instances/tables.py:84#: dashboards/admin/instances/tables.py:86 #: dashboards/project/instances/tables.py:865#: dashboards/admin/instances/tables.py:87 #: dashboards/project/instances/tables.py:866#: dashboards/admin/instances/tables.py:129#: dashboards/project/instances/tables.py:889 #: dashboards/project/loadbalancers/tables.py:253#: dashboards/admin/instances/tables.py:132#: dashboards/project/instances/tables.py:892#: dashboards/project/volumes/volumes/tables.py:317#: dashboards/admin/instances/tables.py:143 #: dashboards/project/instances/tables.py:904#: dashboards/admin/instances/tables.py:150 #: dashboards/project/instances/tables.py:911#: dashboards/admin/instances/tables.py:152 #: dashboards/project/instances/tables.py:914#: dashboards/admin/networks/tables.py:97#: dashboards/admin/networks/ports/tables.py:99#: dashboards/project/firewalls/forms.py:152#: dashboards/project/networks/tables.py:111#: dashboards/project/networks/ports/tables.py:61#: dashboards/project/routers/ports/tables.py:102#: dashboards/admin/networks/tables.py:93#: dashboards/project/firewalls/forms.py:120#: dashboards/project/networks/tables.py:107#: dashboards/project/routers/tables.py:163#: dashboards/project/instances/workflows/create_instance.py:626#: dashboards/project/instances/workflows/create_instance.py:636#: dashboards/admin/networks/tables.py:101#: dashboards/project/instances/workflows/create_instance.py:580#: dashboards/project/networks/tables.py:43 #: dashboards/project/networks/tables.py:115#: dashboards/admin/networks/tables.py:56 #: dashboards/project/networks/tables.py:59#: dashboards/admin/networks/tables.py:64#: dashboards/project/networks/tables.py:67#: dashboards/admin/networks/tables.py:73#: dashboards/project/networks/tables.py:77#: dashboards/admin/networks/tables.py:87#: dashboards/admin/networks/tables.py:90 #: dashboards/project/networks/tables.py:106#: dashboards/admin/networks/tables.py:92#: dashboards/project/instances/tables.py:617#: dashboards/project/volumes/snapshots/tables.py:112#: dashboards/admin/networks/subnets/tables.py:113#: dashboards/project/networks/subnets/tables.py:112#: dashboards/project/networks/ports/tables.py:63#: dashboards/admin/networks/ports/tables.py:55#: dashboards/admin/networks/ports/tables.py:65#: dashboards/admin/networks/ports/tables.py:78#: dashboards/project/networks/ports/tables.py:42#: dashboards/admin/networks/ports/tables.py:96#: dashboards/admin/networks/ports/tables.py:101#: dashboards/admin/networks/ports/tables.py:105#: dashboards/project/networks/ports/tables.py:70 usage/quotas.py:79#: dashboards/admin/networks/subnets/tables.py:36#: dashboards/admin/networks/subnets/tables.py:44#: dashboards/admin/networks/subnets/tables.py:56 #: dashboards/project/networks/subnets/tables.py:55#: dashboards/admin/networks/subnets/tables.py:66#: dashboards/project/networks/subnets/tables.py:66#: dashboards/admin/networks/subnets/tables.py:84 #: dashboards/project/networks/subnets/tables.py:85#: dashboards/admin/networks/subnets/tables.py:99#: dashboards/admin/networks/subnets/tables.py:100#: dashboards/project/networks/subnets/tables.py:101#: dashboards/admin/networks/subnets/tables.py:101#: dashboards/project/networks/subnets/tables.py:102#: dashboards/admin/networks/subnets/tables.py:120#: dashboards/project/networks/subnets/tables.py:47 #: dashboards/project/networks/subnets/tables.py:119 usage/quotas.py:78#: dashboards/project/data_processing/cluster_templates/tables.py:65#: dashboards/admin/routers/panel.py:24 dashboards/admin/routers/tables.py:59#: dashboards/project/routers/tables.py:179#: dashboards/project/routers/extensions/routerrules/tables.py:55#: dashboards/project/routers/extensions/routerrules/tables.py:57#: dashboards/project/firewalls/tables.py:176#: dashboards/project/routers/extensions/routerrules/tables.py:58#: dashboards/project/routers/extensions/routerrules/tables.py:59#: dashboards/project/routers/extensions/routerrules/tables.py:44 #: dashboards/project/routers/extensions/routerrules/tables.py:66#: dashboards/project/routers/ports/tables.py:109#: dashboards/project/routers/tables.py:161#: dashboards/project/routers/tables.py:83#: dashboards/project/volumes/snapshots/tables.py:128#: dashboards/project/instances/workflows/create_instance.py:363#: dashboards/project/volumes/volumes/tables.py:63 #: dashboards/project/volumes/volumes/tables.py:370"" It is used to map to a set of quality of service capabilities requested\n"""" click the \""Manage Specs\"" button to manage the key-value specs for the QOS Spec.\n""msgstr """"#: dashboards/identity/users/tables.py:45#: dashboards/project/volumes/volumes/tables.py:369#: dashboards/identity/users/tables.py:185#: dashboards/identity/users/tables.py:167#: dashboards/identity/users/tables.py:168#: dashboards/identity/users/tables.py:177#: dashboards/identity/projects/workflows.py:44 msgid ""Injected File Content (Bytes)"" msgstr """" #: dashboards/project/instances/workflows/create_instance.py:538 #: dashboards/settings/password/forms.py:47#: dashboards/identity/users/tables.py:30#: dashboards/identity/users/tables.py:64#: dashboards/identity/users/tables.py:69#: dashboards/identity/users/tables.py:79#: dashboards/identity/users/tables.py:84#: dashboards/identity/users/tables.py:114#: dashboards/identity/users/tables.py:129#: dashboards/identity/users/tables.py:137#: dashboards/project/dashboard.py:32 dashboards/project/networks/tables.py:42#: dashboards/project/instances/workflows/create_instance.py:496#: dashboards/project/loadbalancers/tables.py:91 #: dashboards/project/loadbalancers/tables.py:261#: dashboards/project/instances/tables.py:329 #: dashboards/project/volumes/volumes/tables.py:110 #: dashboards/project/volumes/volumes/tables.py:178#: dashboards/project/instances/tables.py:611#: dashboards/project/firewalls/tables.py:55#: dashboards/project/access_and_security/security_groups/tables.py:239#: dashboards/project/access_and_security/security_groups/tables.py:244#: dashboards/project/access_and_security/security_groups/tables.py:247#: dashboards/project/access_and_security/security_groups/tables.py:248#: dashboards/project/access_and_security/security_groups/tables.py:242#: dashboards/project/access_and_security/security_groups/tables.py:213#: dashboards/project/access_and_security/security_groups/tables.py:215#: dashboards/project/access_and_security/security_groups/tables.py:36#: dashboards/project/access_and_security/security_groups/tables.py:44#: dashboards/project/access_and_security/security_groups/tables.py:69#: dashboards/project/access_and_security/security_groups/tables.py:85#: dashboards/project/access_and_security/security_groups/tables.py:107#: dashboards/project/access_and_security/security_groups/tables.py:137#: dashboards/project/firewalls/tables.py:27#: dashboards/project/access_and_security/security_groups/tables.py:158#: dashboards/project/access_and_security/security_groups/tables.py:166#: dashboards/project/access_and_security/security_groups/tables.py:220#: dashboards/project/data_processing/cluster_templates/tables.py:66#: dashboards/project/data_processing/cluster_templates/tables.py:115#: dashboards/project/data_processing/cluster_templates/tables.py:38#: dashboards/project/data_processing/cluster_templates/tables.py:54#: dashboards/project/data_processing/cluster_templates/tables.py:61#: dashboards/project/data_processing/cluster_templates/tables.py:64#: dashboards/project/firewalls/tables.py:53 #: dashboards/project/firewalls/tables.py:62 #: dashboards/project/firewalls/tables.py:72 #: dashboards/project/loadbalancers/tables.py:75 #: dashboards/project/loadbalancers/tables.py:89 #: dashboards/project/loadbalancers/tables.py:104 #: dashboards/project/loadbalancers/tables.py:113#: dashboards/project/data_processing/cluster_templates/tables.py:67#: dashboards/project/data_processing/cluster_templates/tables.py:75#: dashboards/project/data_processing/cluster_templates/tables.py:84#: dashboards/project/data_processing/cluster_templates/tables.py:103#: dashboards/project/data_processing/cluster_templates/tables.py:105#: dashboards/project/data_processing/cluster_templates/tables.py:107#: dashboards/project/instances/workflows/create_instance.py:131 #: dashboards/project/loadbalancers/tables.py:288#: dashboards/project/instances/workflows/create_instance.py:385"" To add required tags, select a plugin and a Data Processing version and click &quot;Add plugin tags&quot; button."" msgstr """"#: dashboards/project/instances/workflows/create_instance.py:698#: dashboards/project/instances/workflows/create_instance.py:622#: dashboards/project/volumes/volumes/tables.py:184#: dashboards/project/instances/tables.py:732 #: dashboards/project/instances/tables.py:739#: dashboards/project/instances/workflows/create_instance.py:700#: dashboards/project/instances/workflows/create_instance.py:716#: dashboards/project/instances/tables.py:303 #: dashboards/project/instances/tables.py:331 #: dashboards/project/instances/workflows/create_instance.py:697#: dashboards/project/loadbalancers/tables.py:278#: dashboards/project/instances/tables.py:347#: dashboards/project/instances/workflows/create_instance.py:584#: dashboards/project/instances/workflows/create_instance.py:586#: dashboards/project/instances/workflows/create_instance.py:608#: dashboards/project/instances/workflows/create_instance.py:610#: dashboards/project/instances/workflows/create_instance.py:699#: dashboards/project/firewalls/tables.py:165#: dashboards/project/loadbalancers/tables.py:227#: dashboards/project/firewalls/forms.py:81#: dashboards/project/firewalls/forms.py:81#: dashboards/project/firewalls/forms.py:81#: dashboards/project/firewalls/forms.py:82 #: dashboards/project/firewalls/tables.py:163#: dashboards/project/firewalls/forms.py:88 #: dashboards/project/firewalls/workflows.py:46 msgid ""ALLOW"" msgstr ""ALLOW"" #: dashboards/project/firewalls/forms.py:88 #: dashboards/project/firewalls/workflows.py:47 msgid ""DENY"" msgstr ""DENY"" #: dashboards/project/firewalls/forms.py:104 #, python-format msgid ""Rule %s was successfully updated."" msgstr ""Rule %s was successfully updated."" #: dashboards/project/firewalls/forms.py:109 #, python-format msgid ""Failed to update rule %(name)s: %(reason)s"" msgstr ""Failed to update rule %(name)s: %(reason)s"" #: dashboards/project/firewalls/forms.py:121 #: dashboards/project/firewalls/tables.py:197 #: dashboards/project/firewalls/workflows.py:184 #: dashboards/project/firewalls/templates/firewalls/_policy_details.html:34 msgid ""Audited"" msgstr ""Audited"" #: dashboards/project/firewalls/forms.py:130 #, python-format msgid ""Policy %s was successfully updated."" msgstr ""Policy %s was successfully updated."" #: dashboards/project/firewalls/forms.py:135 #, python-format msgid ""Failed to update policy %(name)s: %(reason)s"" msgstr ""Failed to update policy %(name)s: %(reason)s"" #: dashboards/project/firewalls/forms.py:149 #: dashboards/project/firewalls/tables.py:64 #: dashboards/project/firewalls/tables.py:213 #: dashboards/project/firewalls/workflows.py:239 msgid ""Policy"" msgstr ""Policy"" #: dashboards/project/firewalls/forms.py:165 msgid ""Unable to retrieve policy list."" msgstr ""Unable to retrieve policy list."" #: dashboards/project/firewalls/forms.py:186 #, python-format msgid ""Firewall %s was successfully updated."" msgstr ""Firewall %s was successfully updated."" #: dashboards/project/firewalls/forms.py:191 #, python-format msgid ""Failed to update firewall %(name)s: %(reason)s"" msgstr ""Failed to update firewall %(name)s: %(reason)s"" #: dashboards/project/firewalls/forms.py:199 #: dashboards/project/firewalls/tables.py:118 msgid ""Insert Rule"" msgstr ""Insert Rule"" #: dashboards/project/firewalls/forms.py:200 msgid ""Before"" msgstr ""Before"" #: dashboards/project/firewalls/forms.py:202 msgid ""After"" msgstr ""After"" #: dashboards/project/firewalls/forms.py:229 #, python-format msgid ""Failed to retrieve available rules: %s"" msgstr ""Failed to retrieve available rules: %s"" #: dashboards/project/firewalls/forms.py:248 #, python-format msgid ""Rule %(rule)s was successfully inserted to policy %(policy)s."" msgstr ""Rule %(rule)s was successfully inserted to policy %(policy)s."" #: dashboards/project/firewalls/forms.py:256 #, python-format msgid ""Failed to insert rule to policy %(name)s: %(reason)s"" msgstr ""Failed to insert rule to policy %(name)s: %(reason)s"" #: dashboards/project/firewalls/forms.py:264 #: dashboards/project/firewalls/tables.py:132 msgid ""Remove Rule"" msgstr ""Remove Rule"" #: dashboards/project/firewalls/forms.py:284 #, python-format msgid ""Failed to retrieve current rules in policy %(name)s: %(reason)s"" msgstr ""Failed to retrieve current rules in policy %(name)s: %(reason)s"" #: dashboards/project/firewalls/forms.py:300 #, python-format msgid ""Rule %(rule)s was successfully removed from policy %(policy)s."" msgstr ""Rule %(rule)s was successfully removed from policy %(policy)s."" #: dashboards/project/firewalls/forms.py:308 #, python-format msgid ""Failed to remove rule from policy %(name)s: %(reason)s"" msgstr ""Failed to remove rule from policy %(name)s: %(reason)s"" #: dashboards/project/firewalls/panel.py:22 #: dashboards/project/firewalls/tables.py:75 #: dashboards/project/firewalls/tables.py:219 #: dashboards/project/firewalls/tabs.py:75 #: dashboards/project/firewalls/templates/firewalls/details_tabs.html:3 #: dashboards/project/firewalls/templates/firewalls/details_tabs.html:6 msgid ""Firewalls"" msgstr ""Firewalls"" #: dashboards/project/firewalls/tables.py:36 #: dashboards/project/firewalls/workflows.py:212 msgid ""Add Policy"" msgstr ""Add Policy"" #: dashboards/project/firewalls/tables.py:44 msgid ""Create Firewall"" msgstr ""Create Firewall"" #: dashboards/project/firewalls/tables.py:54 #: dashboards/project/firewalls/tables.py:63 #: dashboards/project/firewalls/tables.py:73 #: dashboards/project/loadbalancers/tables.py:76 #: dashboards/project/loadbalancers/tables.py:90 #: dashboards/project/loadbalancers/tables.py:105 #: dashboards/project/loadbalancers/tables.py:114 #: dashboards/project/volumes/backups/tables.py:49 #: dashboards/project/volumes/snapshots/tables.py:54 #: dashboards/project/volumes/volumes/tables.py:65 #: dashboards/project/vpn/tables.py:68 dashboards/project/vpn/tables.py:82 #: dashboards/project/vpn/tables.py:96 dashboards/project/vpn/tables.py:110 #, python-format msgid ""Scheduled deletion of %(data_type)s"" msgstr ""Scheduled deletion of %(data_type)s"" #: dashboards/project/firewalls/tables.py:56 #: dashboards/project/firewalls/tables.py:185 #: dashboards/project/firewalls/tables.py:195 #: dashboards/project/firewalls/workflows.py:133 #: dashboards/project/firewalls/workflows.py:139 #: dashboards/project/firewalls/templates/firewalls/_policy_details.html:19 msgid ""Rules"" msgstr ""Rules"" #: dashboards/project/firewalls/tables.py:65 #: dashboards/project/firewalls/tables.py:201 msgid ""Policies"" msgstr ""Policies"" #: dashboards/project/firewalls/tables.py:74 msgid ""Firewall"" msgstr ""Firewall"" #: dashboards/project/firewalls/tables.py:81 #: dashboards/project/firewalls/templates/firewalls/_updaterule.html:8 #: dashboards/project/firewalls/templates/firewalls/updaterule.html:3 msgid ""Edit Rule"" msgstr ""Edit Rule"" #: dashboards/project/firewalls/tables.py:93 #: dashboards/project/firewalls/templates/firewalls/_updatepolicy.html:8 #: dashboards/project/firewalls/templates/firewalls/updatepolicy.html:3 msgid ""Edit Policy"" msgstr ""Edit Policy"" #: dashboards/project/firewalls/tables.py:105 #: dashboards/project/firewalls/templates/firewalls/_updatefirewall.html:8 #: dashboards/project/firewalls/templates/firewalls/updatefirewall.html:3 msgid ""Edit Firewall"" msgstr ""Edit Firewall"" #: dashboards/project/firewalls/tables.py:167#: dashboards/project/firewalls/tables.py:169#: dashboards/project/firewalls/tables.py:171#: dashboards/project/firewalls/tables.py:173#: dashboards/project/firewalls/tables.py:181#: dashboards/project/volumes/snapshots/tables.py:79#: dashboards/project/volumes/volumes/tables.py:87 #: dashboards/project/volumes/volumes/tables.py:112#: dashboards/project/instances/tables.py:383 #: dashboards/project/volumes/volumes/tables.py:159#: dashboards/project/instances/workflows/create_instance.py:667#: dashboards/project/instances/workflows/create_instance.py:387#: dashboards/project/instances/workflows/create_instance.py:677#: dashboards/project/instances/workflows/create_instance.py:678#: dashboards/project/instances/workflows/create_instance.py:155 #: dashboards/project/instances/workflows/create_instance.py:681#: dashboards/project/instances/tables.py:90#: dashboards/project/instances/tables.py:98#: dashboards/project/instances/tables.py:119#: dashboards/project/instances/tables.py:127#: dashboards/project/instances/tables.py:150#: dashboards/project/instances/tables.py:158#: dashboards/project/instances/tables.py:175#: dashboards/project/instances/tables.py:180 #: dashboards/project/instances/tables.py:247#: dashboards/project/instances/tables.py:190#: dashboards/project/instances/tables.py:195 #: dashboards/project/instances/tables.py:262#: dashboards/project/instances/tables.py:242#: dashboards/project/instances/tables.py:257#: dashboards/project/instances/tables.py:370#: dashboards/project/instances/tables.py:396#: dashboards/project/instances/tables.py:416#: dashboards/project/instances/tables.py:433#: dashboards/project/instances/tables.py:457#: dashboards/project/instances/tables.py:470#: dashboards/project/instances/tables.py:483#: dashboards/project/instances/tables.py:500#: dashboards/project/instances/tables.py:523 #: dashboards/project/instances/tables.py:547#: dashboards/project/instances/tables.py:566#: dashboards/project/instances/tables.py:570#: dashboards/project/instances/tables.py:576#: dashboards/project/instances/tables.py:605#: dashboards/project/instances/tables.py:608#: dashboards/project/instances/tables.py:618#: dashboards/project/instances/tables.py:620#: dashboards/project/instances/tables.py:632#: dashboards/project/instances/tables.py:658#: dashboards/project/instances/tables.py:666#: dashboards/project/instances/tables.py:687#: dashboards/project/instances/tables.py:696#: dashboards/project/instances/tables.py:723#: dashboards/project/instances/tables.py:747 #: dashboards/project/instances/tables.py:769#: dashboards/project/instances/tables.py:748#: dashboards/project/instances/tables.py:749#: dashboards/project/instances/tables.py:751#: dashboards/project/instances/tables.py:752#: dashboards/project/instances/tables.py:753#: dashboards/project/instances/tables.py:755#: dashboards/project/instances/tables.py:757#: dashboards/project/instances/tables.py:759#: dashboards/project/instances/tables.py:760#: dashboards/project/instances/tables.py:762#: dashboards/project/instances/tables.py:763#: dashboards/project/instances/tables.py:764#: dashboards/project/instances/tables.py:766#: dashboards/project/instances/tables.py:767#: dashboards/project/instances/tables.py:768#: dashboards/project/instances/tables.py:771#: dashboards/project/instances/tables.py:772#: dashboards/project/instances/tables.py:774#: dashboards/project/instances/tables.py:780#: dashboards/project/instances/tables.py:782#: dashboards/project/instances/tables.py:784#: dashboards/project/instances/tables.py:785#: dashboards/project/instances/tables.py:787#: dashboards/project/instances/tables.py:789#: dashboards/project/instances/tables.py:791#: dashboards/project/instances/tables.py:793#: dashboards/project/instances/tables.py:795#: dashboards/project/instances/tables.py:797#: dashboards/project/instances/tables.py:799#: dashboards/project/instances/tables.py:801#: dashboards/project/instances/tables.py:803#: dashboards/project/instances/tables.py:805#: dashboards/project/instances/tables.py:807#: dashboards/project/instances/tables.py:809#: dashboards/project/instances/tables.py:810#: dashboards/project/instances/tables.py:812#: dashboards/project/instances/tables.py:813#: dashboards/project/instances/tables.py:814 #: dashboards/project/instances/tables.py:817#: dashboards/project/instances/tables.py:816#: dashboards/project/instances/tables.py:819#: dashboards/project/instances/tables.py:821#: dashboards/project/instances/tables.py:822#: dashboards/project/instances/tables.py:824#: dashboards/project/instances/tables.py:826#: dashboards/project/instances/tables.py:828#: dashboards/project/instances/tables.py:830#: dashboards/project/instances/tables.py:831#: dashboards/project/instances/tables.py:832#: dashboards/project/instances/tables.py:834#: dashboards/project/instances/tables.py:835#: dashboards/project/instances/tables.py:836#: dashboards/project/instances/tables.py:838#: dashboards/project/instances/tables.py:840#: dashboards/project/instances/tables.py:842#: dashboards/project/instances/tables.py:844#: dashboards/project/instances/tables.py:848#: dashboards/project/instances/tables.py:849#: dashboards/project/instances/tables.py:850#: dashboards/project/instances/tables.py:851#: dashboards/project/instances/tables.py:852#: dashboards/project/instances/tables.py:853#: dashboards/project/instances/tables.py:854#: dashboards/project/instances/tables.py:855#: dashboards/project/instances/tables.py:856#: dashboards/project/instances/tables.py:857#: dashboards/project/instances/tables.py:894 #: dashboards/project/instances/workflows/create_instance.py:474#: dashboards/project/loadbalancers/tables.py:225#: dashboards/project/volumes/volumes/tables.py:346#: dashboards/project/volumes/snapshots/tables.py:52#: dashboards/project/instances/workflows/create_instance.py:115#: dashboards/project/instances/workflows/create_instance.py:118#: dashboards/project/instances/workflows/create_instance.py:121#: dashboards/project/instances/workflows/create_instance.py:124#: dashboards/project/instances/workflows/create_instance.py:127#: dashboards/project/instances/workflows/create_instance.py:142#: dashboards/project/instances/workflows/create_instance.py:143#: dashboards/project/instances/workflows/create_instance.py:144#: dashboards/project/instances/workflows/create_instance.py:147#: dashboards/project/instances/workflows/create_instance.py:153#: dashboards/project/instances/workflows/create_instance.py:159#: dashboards/project/instances/workflows/create_instance.py:170#: dashboards/project/instances/workflows/create_instance.py:197#: dashboards/project/instances/workflows/create_instance.py:204#: dashboards/project/instances/workflows/create_instance.py:211#: dashboards/project/instances/workflows/create_instance.py:225#: dashboards/project/instances/workflows/create_instance.py:228 #: dashboards/project/instances/workflows/create_instance.py:302#: dashboards/project/instances/workflows/create_instance.py:231#: dashboards/project/instances/workflows/create_instance.py:254#: dashboards/project/instances/workflows/create_instance.py:271#: dashboards/project/instances/workflows/create_instance.py:282 #: dashboards/project/instances/workflows/create_instance.py:299#: dashboards/project/instances/workflows/create_instance.py:287#: dashboards/project/instances/workflows/create_instance.py:293#: dashboards/project/instances/workflows/create_instance.py:319#: dashboards/project/instances/workflows/create_instance.py:325#: dashboards/project/instances/workflows/create_instance.py:327#: dashboards/project/instances/workflows/create_instance.py:350#: dashboards/project/instances/workflows/create_instance.py:360#: dashboards/project/instances/workflows/create_instance.py:365#: dashboards/project/instances/workflows/create_instance.py:399#: dashboards/project/instances/workflows/create_instance.py:401#: dashboards/project/instances/workflows/create_instance.py:413#: dashboards/project/instances/workflows/create_instance.py:415#: dashboards/project/instances/workflows/create_instance.py:417#: dashboards/project/instances/workflows/create_instance.py:428#: dashboards/project/instances/workflows/create_instance.py:431#: dashboards/project/instances/workflows/create_instance.py:433#: dashboards/project/instances/workflows/create_instance.py:476#: dashboards/project/instances/workflows/create_instance.py:480#: dashboards/project/instances/workflows/create_instance.py:486#: dashboards/project/instances/workflows/create_instance.py:492#: dashboards/project/instances/workflows/create_instance.py:497#: dashboards/project/instances/workflows/create_instance.py:513#: dashboards/project/instances/workflows/create_instance.py:517#: dashboards/project/instances/workflows/create_instance.py:519#: dashboards/project/instances/workflows/create_instance.py:528#: dashboards/project/instances/workflows/create_instance.py:560#: dashboards/project/instances/workflows/create_instance.py:562#: dashboards/project/instances/workflows/create_instance.py:569#: dashboards/project/instances/workflows/create_instance.py:592#: dashboards/project/instances/workflows/create_instance.py:595#: dashboards/project/instances/workflows/create_instance.py:685#: dashboards/project/instances/workflows/create_instance.py:713#: dashboards/project/instances/workflows/create_instance.py:771#: dashboards/project/loadbalancers/tables.py:259#: dashboards/project/loadbalancers/tables.py:285#: dashboards/project/loadbalancers/tables.py:286#: dashboards/project/loadbalancers/tables.py:30#: dashboards/project/loadbalancers/tables.py:39#: dashboards/project/loadbalancers/tables.py:57#: dashboards/project/loadbalancers/tables.py:66#: dashboards/project/loadbalancers/tables.py:77 #: dashboards/project/loadbalancers/tables.py:229#: dashboards/project/loadbalancers/tables.py:78#: dashboards/project/loadbalancers/tables.py:92 #: dashboards/project/loadbalancers/tables.py:234#: dashboards/project/loadbalancers/tables.py:106#: dashboards/project/loadbalancers/tables.py:107 #: dashboards/project/loadbalancers/tables.py:292#: dashboards/project/loadbalancers/tables.py:115#: dashboards/project/loadbalancers/tables.py:116 #: dashboards/project/loadbalancers/tables.py:266#: dashboards/project/loadbalancers/tables.py:122#: dashboards/project/loadbalancers/tables.py:134#: dashboards/project/loadbalancers/tables.py:151#: dashboards/project/loadbalancers/tables.py:163#: dashboards/project/loadbalancers/tables.py:184#: dashboards/project/loadbalancers/tables.py:200#: dashboards/project/loadbalancers/tables.py:207#: dashboards/project/loadbalancers/tables.py:224#: dashboards/project/loadbalancers/tables.py:226#: dashboards/project/networks/subnets/tables.py:46#: dashboards/project/loadbalancers/tables.py:257#: dashboards/project/loadbalancers/tables.py:283#: dashboards/project/loadbalancers/tables.py:287#: dashboards/project/routers/tables.py:74#: dashboards/project/networks/tables.py:87#: dashboards/project/networks/subnets/tables.py:100#: dashboards/project/networks/ports/tables.py:35#: dashboards/project/networks/ports/tables.py:37#: dashboards/project/networks/ports/tables.py:58#: dashboards/project/routers/tables.py:58#: dashboards/project/routers/tables.py:92#: dashboards/project/routers/tables.py:106#: dashboards/project/routers/tables.py:107#: dashboards/project/routers/tables.py:108#: dashboards/project/routers/tables.py:109#: dashboards/project/routers/tables.py:120#: dashboards/project/routers/extensions/routerrules/tables.py:31#: dashboards/project/routers/extensions/routerrules/tables.py:43#: dashboards/project/routers/ports/tables.py:33#: dashboards/project/routers/ports/tables.py:35#: dashboards/project/routers/ports/tables.py:42#: dashboards/project/routers/ports/tables.py:57#: dashboards/project/routers/ports/tables.py:65#: dashboards/project/routers/ports/tables.py:84#: dashboards/project/volumes/snapshots/tables.py:65#: dashboards/project/volumes/volumes/tables.py:417#: dashboards/project/volumes/volumes/tables.py:124#: dashboards/project/volumes/volumes/tables.py:224#: dashboards/project/volumes/volumes/tables.py:212#: dashboards/project/volumes/volumes/tables.py:42#: dashboards/project/volumes/volumes/tables.py:74#: dashboards/project/volumes/volumes/tables.py:135#: dashboards/project/volumes/volumes/tables.py:169#: dashboards/project/volumes/volumes/tables.py:196#: dashboards/project/volumes/volumes/tables.py:258#: dashboards/project/volumes/volumes/tables.py:276#: dashboards/project/volumes/volumes/tables.py:297#: dashboards/project/volumes/volumes/tables.py:299#: dashboards/project/volumes/volumes/tables.py:350#: dashboards/project/volumes/volumes/tables.py:353#: dashboards/project/volumes/volumes/tables.py:368#: dashboards/project/volumes/volumes/tables.py:398#: dashboards/project/volumes/volumes/tables.py:407#: dashboards/settings/password/forms.py:67#: dashboards/settings/password/forms.py:70","""POT-Creation-Date: 2014-09-21 14:06-0500\n"" ""PO-Revision-Date: 2014-09-21 14:51+0000\n"" ""Last-Translator: Tom Fifield <tom@openstack.org>\n""#: dashboards/settings/password/forms.py:61#: dashboards/admin/info/tables.py:170 dashboards/admin/instances/tables.py:92 #: dashboards/admin/instances/tables.py:135#: dashboards/admin/networks/ports/tables.py:103 #: dashboards/admin/networks/subnets/tables.py:108 #: dashboards/admin/routers/tables.py:60#: dashboards/project/access_and_security/security_groups/tables.py:139#: dashboards/project/data_processing/cluster_templates/tables.py:99#: dashboards/project/firewalls/forms.py:113 #: dashboards/project/firewalls/forms.py:140 #: dashboards/project/firewalls/tables.py:204 #: dashboards/project/firewalls/tables.py:236 #: dashboards/project/firewalls/tables.py:253#: dashboards/project/loadbalancers/tables.py:277#: dashboards/project/networks/tables.py:117#: dashboards/project/networks/ports/tables.py:60 #: dashboards/project/networks/subnets/tables.py:106#: dashboards/project/routers/tables.py:177 #: dashboards/project/routers/ports/tables.py:105#: dashboards/project/volumes/snapshots/tables.py:128#: dashboards/project/volumes/volumes/tables.py:351 #: dashboards/project/volumes/volumes/tables.py:380#: dashboards/project/instances/tables.py:991#: dashboards/project/volumes/volumes/tables.py:388#: dashboards/admin/aggregates/views.py:50#: dashboards/admin/aggregates/views.py:62#: dashboards/admin/aggregates/views.py:95#: dashboards/admin/aggregates/views.py:116 msgid ""Unable to retrieve aggregate metadata."" msgstr ""Unable to retrieve aggregate metadata."" #: dashboards/admin/aggregates/views.py:134#: dashboards/admin/aggregates/views.py:146#: dashboards/project/access_and_security/security_groups/tables.py:140#: dashboards/project/data_processing/cluster_templates/tables.py:110#: dashboards/project/firewalls/forms.py:115 #: dashboards/project/firewalls/forms.py:143#: dashboards/project/loadbalancers/tables.py:279#: dashboards/project/volumes/volumes/tables.py:354#: dashboards/project/volumes/volumes/tables.py:285msgid ""RAM MB"" msgstr ""RAM MB""msgid ""Root Disk GB"" msgstr ""Root Disk GB""msgid ""Ephemeral Disk GB"" msgstr ""Ephemeral Disk GB""msgid ""Swap Disk MB"" msgstr ""Swap Disk MB""#: dashboards/project/routers/ports/tables.py:111#: dashboards/project/volumes/volumes/tables.py:383#: dashboards/admin/instances/tables.py:36 #: dashboards/admin/instances/tables.py:169#: dashboards/project/instances/tables.py:1010#: dashboards/project/instances/tables.py:952 #: dashboards/project/instances/tables.py:974#: dashboards/admin/instances/tables.py:131#: dashboards/admin/instances/tables.py:147 #: dashboards/admin/networks/tables.py:107#: dashboards/admin/networks/ports/tables.py:109#: dashboards/project/firewalls/tables.py:259#: dashboards/project/instances/tables.py:986#: dashboards/project/loadbalancers/tables.py:284 #: dashboards/project/loadbalancers/tables.py:318#: dashboards/project/networks/tables.py:123 #: dashboards/project/networks/ports/tables.py:64#: dashboards/project/routers/tables.py:181 #: dashboards/project/routers/ports/tables.py:109#: dashboards/project/volumes/volumes/tables.py:361#: dashboards/admin/instances/tables.py:95 #: dashboards/project/instances/tables.py:953#: dashboards/admin/instances/tables.py:137#: dashboards/project/instances/tables.py:976#: dashboards/admin/images/views.py:79#: dashboards/admin/images/views.py:144 msgid ""Unable to retrieve image properties."" msgstr ""Unable to retrieve image properties."" #: dashboards/admin/images/views.py:181#: dashboards/admin/images/views.py:194#: dashboards/identity/users/tables.py:183#: dashboards/project/firewalls/tables.py:222#: dashboards/admin/info/tables.py:205 #: dashboards/identity/projects/workflows.py:44 usage/quotas.py:66#: dashboards/project/volumes/volumes/tables.py:60 #: dashboards/project/volumes/volumes/tables.py:397 #: dashboards/project/volumes/volumes/tables.py:411 usage/quotas.py:74#: dashboards/project/volumes/snapshots/tables.py:45 #: dashboards/project/volumes/snapshots/tables.py:137 usage/quotas.py:75#: dashboards/admin/info/tables.py:214 dashboards/admin/overview/views.py:33 #: dashboards/identity/projects/workflows.py:57 #: dashboards/project/overview/views.py:32 usage/quotas.py:67 msgid ""RAM (MB)"" msgstr ""RAM (MB)"" #: dashboards/project/access_and_security/security_groups/tables.py:147#: dashboards/project/instances/workflows/create_instance.py:488#: dashboards/project/access_and_security/security_groups/tables.py:275#: dashboards/admin/networks/ports/tables.py:106#: dashboards/project/networks/ports/tables.py:62 #: dashboards/project/routers/ports/tables.py:108 usage/quotas.py:69#: dashboards/admin/instances/tables.py:33#: dashboards/admin/instances/tables.py:34#: dashboards/admin/instances/tables.py:35#: dashboards/project/volumes/volumes/tables.py:436#: dashboards/admin/instances/tables.py:57#: dashboards/admin/instances/tables.py:90 #: dashboards/admin/instances/tables.py:124#: dashboards/admin/networks/tables.py:98 #: dashboards/admin/routers/tables.py:58#: dashboards/admin/instances/tables.py:91#: dashboards/admin/instances/tables.py:93#: dashboards/admin/instances/tables.py:94#: dashboards/admin/instances/tables.py:96 #: dashboards/project/instances/tables.py:954#: dashboards/admin/instances/tables.py:97 #: dashboards/project/instances/tables.py:955#: dashboards/admin/instances/tables.py:139#: dashboards/project/instances/tables.py:978 #: dashboards/project/loadbalancers/tables.py:309#: dashboards/admin/instances/tables.py:142#: dashboards/project/instances/tables.py:981#: dashboards/project/volumes/volumes/tables.py:357#: dashboards/admin/instances/tables.py:153 #: dashboards/project/instances/tables.py:993#: dashboards/admin/instances/tables.py:160 #: dashboards/project/instances/tables.py:1000#: dashboards/admin/instances/tables.py:162 #: dashboards/project/instances/tables.py:1003#: dashboards/admin/networks/tables.py:109#: dashboards/admin/networks/ports/tables.py:111#: dashboards/project/firewalls/forms.py:148#: dashboards/project/networks/tables.py:125#: dashboards/project/networks/ports/tables.py:66#: dashboards/project/routers/ports/tables.py:113#: dashboards/admin/networks/tables.py:105#: dashboards/project/firewalls/forms.py:116#: dashboards/project/networks/tables.py:121#: dashboards/project/routers/tables.py:187#: dashboards/project/instances/workflows/create_instance.py:625#: dashboards/project/instances/workflows/create_instance.py:635#: dashboards/admin/networks/tables.py:113#: dashboards/project/instances/workflows/create_instance.py:579#: dashboards/project/networks/tables.py:42 #: dashboards/project/networks/tables.py:129#: dashboards/admin/networks/tables.py:62 #: dashboards/project/networks/tables.py:64#: dashboards/admin/networks/tables.py:70#: dashboards/project/networks/tables.py:72#: dashboards/admin/networks/tables.py:79#: dashboards/project/networks/tables.py:81#: dashboards/admin/networks/tables.py:99#: dashboards/admin/networks/tables.py:102 #: dashboards/project/networks/tables.py:120#: dashboards/admin/networks/tables.py:104#: dashboards/project/instances/tables.py:694#: dashboards/project/volumes/snapshots/tables.py:116#: dashboards/admin/networks/subnets/tables.py:124#: dashboards/project/networks/subnets/tables.py:120#: dashboards/project/networks/ports/tables.py:68#: dashboards/admin/networks/ports/tables.py:61#: dashboards/admin/networks/ports/tables.py:71#: dashboards/admin/networks/ports/tables.py:84#: dashboards/project/networks/ports/tables.py:41#: dashboards/admin/networks/ports/tables.py:108#: dashboards/admin/networks/ports/tables.py:113#: dashboards/admin/networks/ports/tables.py:117#: dashboards/project/networks/ports/tables.py:75 usage/quotas.py:79#: dashboards/admin/networks/subnets/tables.py:35#: dashboards/admin/networks/subnets/tables.py:43#: dashboards/admin/networks/subnets/tables.py:60 #: dashboards/project/networks/subnets/tables.py:58#: dashboards/admin/networks/subnets/tables.py:70#: dashboards/project/networks/subnets/tables.py:68#: dashboards/admin/networks/subnets/tables.py:90 #: dashboards/project/networks/subnets/tables.py:88#: dashboards/admin/networks/subnets/tables.py:110#: dashboards/admin/networks/subnets/tables.py:111#: dashboards/project/networks/subnets/tables.py:109#: dashboards/admin/networks/subnets/tables.py:112#: dashboards/project/networks/subnets/tables.py:110#: dashboards/admin/networks/subnets/tables.py:131#: dashboards/project/networks/subnets/tables.py:45 #: dashboards/project/networks/subnets/tables.py:127 usage/quotas.py:78#: dashboards/project/data_processing/cluster_templates/tables.py:64#: dashboards/admin/routers/panel.py:24 dashboards/admin/routers/tables.py:65#: dashboards/project/routers/tables.py:203#: dashboards/project/routers/extensions/routerrules/tables.py:66#: dashboards/project/routers/extensions/routerrules/tables.py:68#: dashboards/project/firewalls/tables.py:220#: dashboards/project/routers/extensions/routerrules/tables.py:69#: dashboards/project/routers/extensions/routerrules/tables.py:70#: dashboards/project/routers/extensions/routerrules/tables.py:49 #: dashboards/project/routers/extensions/routerrules/tables.py:77#: dashboards/project/routers/ports/tables.py:120#: dashboards/project/routers/tables.py:185#: dashboards/project/routers/tables.py:89#: dashboards/project/volumes/snapshots/tables.py:132#: dashboards/project/instances/workflows/create_instance.py:362#: dashboards/project/volumes/volumes/tables.py:59 #: dashboards/project/volumes/volumes/tables.py:410"" It is used to map to a set of Quality of Service capabilities requested\n"""" click the \""Manage\"" button to manage the key-value specs for the QOS Spec.\n""msgstr ""\n QOS Specs can be associated with volume types.\n It is used to map to a set of Quality of Service capabilities requested\n by the volume owner. This is equivalent to the\n <tt>cinder qos-create</tt> command. Once the QOS Spec gets created,\n click the \""Manage\"" button to manage the key-value specs for the QOS Spec.\n <br>\n <br>\n Each QOS Specs entity will have a \""consumer\"" value which indicates where the\n administrator would like the QOS policy to be enforced. This value can be \""front-end\""\n (Nova Compute), \""back-end\"" (Cinder back-end), or both.\n ""#: dashboards/identity/users/tables.py:44#: dashboards/project/volumes/volumes/tables.py:409#: dashboards/identity/users/tables.py:190#: dashboards/identity/users/tables.py:172#: dashboards/identity/users/tables.py:173#: dashboards/identity/users/tables.py:182#: dashboards/project/instances/workflows/create_instance.py:537 #: dashboards/settings/password/forms.py:46#: dashboards/identity/users/tables.py:29#: dashboards/identity/users/tables.py:65#: dashboards/identity/users/tables.py:70#: dashboards/identity/users/tables.py:80#: dashboards/identity/users/tables.py:85#: dashboards/identity/users/tables.py:119#: dashboards/identity/users/tables.py:134#: dashboards/identity/users/tables.py:142#: dashboards/project/dashboard.py:32 dashboards/project/networks/tables.py:41#: dashboards/project/instances/workflows/create_instance.py:495#: dashboards/project/loadbalancers/tables.py:96 #: dashboards/project/loadbalancers/tables.py:317#: dashboards/project/instances/tables.py:340 #: dashboards/project/volumes/volumes/tables.py:112 #: dashboards/project/volumes/volumes/tables.py:192#: dashboards/project/instances/tables.py:688#: dashboards/project/firewalls/tables.py:54#: dashboards/project/access_and_security/security_groups/tables.py:256#: dashboards/project/access_and_security/security_groups/tables.py:261#: dashboards/project/access_and_security/security_groups/tables.py:264#: dashboards/project/access_and_security/security_groups/tables.py:265#: dashboards/project/access_and_security/security_groups/tables.py:259#: dashboards/project/access_and_security/security_groups/tables.py:230#: dashboards/project/access_and_security/security_groups/tables.py:232#: dashboards/project/access_and_security/security_groups/tables.py:35#: dashboards/project/access_and_security/security_groups/tables.py:43#: dashboards/project/access_and_security/security_groups/tables.py:74#: dashboards/project/access_and_security/security_groups/tables.py:90#: dashboards/project/access_and_security/security_groups/tables.py:118#: dashboards/project/access_and_security/security_groups/tables.py:154#: dashboards/project/firewalls/tables.py:26#: dashboards/project/access_and_security/security_groups/tables.py:175#: dashboards/project/access_and_security/security_groups/tables.py:183#: dashboards/project/access_and_security/security_groups/tables.py:237#: dashboards/project/data_processing/cluster_templates/tables.py:65#: dashboards/project/data_processing/cluster_templates/tables.py:114#: dashboards/project/data_processing/cluster_templates/tables.py:37#: dashboards/project/data_processing/cluster_templates/tables.py:53#: dashboards/project/data_processing/cluster_templates/tables.py:60#: dashboards/project/data_processing/cluster_templates/tables.py:63#: dashboards/project/firewalls/tables.py:52 #: dashboards/project/firewalls/tables.py:67 #: dashboards/project/firewalls/tables.py:82 #: dashboards/project/loadbalancers/tables.py:74 #: dashboards/project/loadbalancers/tables.py:94 #: dashboards/project/loadbalancers/tables.py:114 #: dashboards/project/loadbalancers/tables.py:129#: dashboards/project/data_processing/cluster_templates/tables.py:66#: dashboards/project/data_processing/cluster_templates/tables.py:74#: dashboards/project/data_processing/cluster_templates/tables.py:83#: dashboards/project/data_processing/cluster_templates/tables.py:102#: dashboards/project/data_processing/cluster_templates/tables.py:104#: dashboards/project/data_processing/cluster_templates/tables.py:106#: dashboards/project/instances/workflows/create_instance.py:130 #: dashboards/project/loadbalancers/tables.py:344#: dashboards/project/instances/workflows/create_instance.py:384"" To add required tags, select a plugin and a Data Processing version and click &quot;Add all&quot; button."" msgstr ""Tags are used for filtering images suitable for each plugin and each Data Processing version.\n To add required tags, select a plugin and a Data Processing version and click &quot;Add all&quot; button.""#: dashboards/project/instances/workflows/create_instance.py:697#: dashboards/project/instances/workflows/create_instance.py:621#: dashboards/project/volumes/volumes/tables.py:198#: dashboards/project/instances/tables.py:821 #: dashboards/project/instances/tables.py:828#: dashboards/project/instances/workflows/create_instance.py:699#: dashboards/project/instances/workflows/create_instance.py:715#: dashboards/project/instances/tables.py:314 #: dashboards/project/instances/tables.py:342 #: dashboards/project/instances/workflows/create_instance.py:696#: dashboards/project/loadbalancers/tables.py:334#: dashboards/project/instances/tables.py:358#: dashboards/project/instances/workflows/create_instance.py:583#: dashboards/project/instances/workflows/create_instance.py:585#: dashboards/project/instances/workflows/create_instance.py:607#: dashboards/project/instances/workflows/create_instance.py:609#: dashboards/project/instances/workflows/create_instance.py:698#: dashboards/project/firewalls/tables.py:209#: dashboards/project/loadbalancers/tables.py:283#: dashboards/project/firewalls/forms.py:80#: dashboards/project/firewalls/forms.py:80#: dashboards/project/firewalls/forms.py:80#: dashboards/project/firewalls/forms.py:86 #: dashboards/project/firewalls/workflows.py:46 msgid ""ALLOW"" msgstr ""ALLOW"" #: dashboards/project/firewalls/forms.py:86 #: dashboards/project/firewalls/workflows.py:47 msgid ""DENY"" msgstr ""DENY"" #: dashboards/project/firewalls/forms.py:100 #, python-format msgid ""Rule %s was successfully updated."" msgstr ""Rule %s was successfully updated."" #: dashboards/project/firewalls/forms.py:105 #, python-format msgid ""Failed to update rule %(name)s: %(reason)s"" msgstr ""Failed to update rule %(name)s: %(reason)s"" #: dashboards/project/firewalls/forms.py:117 #: dashboards/project/firewalls/tables.py:241 #: dashboards/project/firewalls/workflows.py:184 #: dashboards/project/firewalls/templates/firewalls/_policy_details.html:34 msgid ""Audited"" msgstr ""Audited"" #: dashboards/project/firewalls/forms.py:126 #, python-format msgid ""Policy %s was successfully updated."" msgstr ""Policy %s was successfully updated."" #: dashboards/project/firewalls/forms.py:131 #, python-format msgid ""Failed to update policy %(name)s: %(reason)s"" msgstr ""Failed to update policy %(name)s: %(reason)s"" #: dashboards/project/firewalls/forms.py:145 #: dashboards/project/firewalls/tables.py:69 #: dashboards/project/firewalls/tables.py:257 #: dashboards/project/firewalls/workflows.py:239 msgid ""Policy"" msgstr ""Policy"" #: dashboards/project/firewalls/forms.py:161 msgid ""Unable to retrieve policy list."" msgstr ""Unable to retrieve policy list."" #: dashboards/project/firewalls/forms.py:182 #, python-format msgid ""Firewall %s was successfully updated."" msgstr ""Firewall %s was successfully updated."" #: dashboards/project/firewalls/forms.py:187 #, python-format msgid ""Failed to update firewall %(name)s: %(reason)s"" msgstr ""Failed to update firewall %(name)s: %(reason)s"" #: dashboards/project/firewalls/forms.py:195 #: dashboards/project/firewalls/tables.py:151 msgid ""Insert Rule"" msgstr ""Insert Rule"" #: dashboards/project/firewalls/forms.py:196 msgid ""Before"" msgstr ""Before"" #: dashboards/project/firewalls/forms.py:198 msgid ""After"" msgstr ""After"" #: dashboards/project/firewalls/forms.py:225 #, python-format msgid ""Failed to retrieve available rules: %s"" msgstr ""Failed to retrieve available rules: %s"" #: dashboards/project/firewalls/forms.py:244 #, python-format msgid ""Rule %(rule)s was successfully inserted to policy %(policy)s."" msgstr ""Rule %(rule)s was successfully inserted to policy %(policy)s."" #: dashboards/project/firewalls/forms.py:252 #, python-format msgid ""Failed to insert rule to policy %(name)s: %(reason)s"" msgstr ""Failed to insert rule to policy %(name)s: %(reason)s"" #: dashboards/project/firewalls/forms.py:260 #: dashboards/project/firewalls/tables.py:170 msgid ""Remove Rule"" msgstr ""Remove Rule"" #: dashboards/project/firewalls/forms.py:280 #, python-format msgid ""Failed to retrieve current rules in policy %(name)s: %(reason)s"" msgstr ""Failed to retrieve current rules in policy %(name)s: %(reason)s"" #: dashboards/project/firewalls/forms.py:296 #, python-format msgid ""Rule %(rule)s was successfully removed from policy %(policy)s."" msgstr ""Rule %(rule)s was successfully removed from policy %(policy)s."" #: dashboards/project/firewalls/forms.py:304 #, python-format msgid ""Failed to remove rule from policy %(name)s: %(reason)s"" msgstr ""Failed to remove rule from policy %(name)s: %(reason)s"" #: dashboards/project/firewalls/panel.py:22 #: dashboards/project/firewalls/tables.py:85 #: dashboards/project/firewalls/tables.py:263 #: dashboards/project/firewalls/tabs.py:75 #: dashboards/project/firewalls/templates/firewalls/details_tabs.html:3 #: dashboards/project/firewalls/templates/firewalls/details_tabs.html:6 msgid ""Firewalls"" msgstr ""Firewalls"" #: dashboards/project/firewalls/tables.py:35 #: dashboards/project/firewalls/workflows.py:212 msgid ""Add Policy"" msgstr ""Add Policy"" #: dashboards/project/firewalls/tables.py:43 msgid ""Create Firewall"" msgstr ""Create Firewall"" #: dashboards/project/firewalls/tables.py:53 #: dashboards/project/firewalls/tables.py:68 #: dashboards/project/firewalls/tables.py:83 #: dashboards/project/loadbalancers/tables.py:75 #: dashboards/project/loadbalancers/tables.py:95 #: dashboards/project/loadbalancers/tables.py:115 #: dashboards/project/loadbalancers/tables.py:130 #: dashboards/project/volumes/backups/tables.py:49 #: dashboards/project/volumes/snapshots/tables.py:46 #: dashboards/project/volumes/volumes/tables.py:61 #: dashboards/project/vpn/tables.py:68 dashboards/project/vpn/tables.py:82 #: dashboards/project/vpn/tables.py:96 dashboards/project/vpn/tables.py:110 #, python-format msgid ""Scheduled deletion of %(data_type)s"" msgstr ""Scheduled deletion of %(data_type)s"" #: dashboards/project/firewalls/tables.py:55 #: dashboards/project/firewalls/tables.py:229 #: dashboards/project/firewalls/tables.py:239 #: dashboards/project/firewalls/workflows.py:133 #: dashboards/project/firewalls/workflows.py:139 #: dashboards/project/firewalls/templates/firewalls/_policy_details.html:19 msgid ""Rules"" msgstr ""Rules"" #: dashboards/project/firewalls/tables.py:70 #: dashboards/project/firewalls/tables.py:245 msgid ""Policies"" msgstr ""Policies"" #: dashboards/project/firewalls/tables.py:84 msgid ""Firewall"" msgstr ""Firewall"" #: dashboards/project/firewalls/tables.py:97 #: dashboards/project/firewalls/templates/firewalls/_updaterule.html:8 #: dashboards/project/firewalls/templates/firewalls/updaterule.html:3 msgid ""Edit Rule"" msgstr ""Edit Rule"" #: dashboards/project/firewalls/tables.py:115 #: dashboards/project/firewalls/templates/firewalls/_updatepolicy.html:8 #: dashboards/project/firewalls/templates/firewalls/updatepolicy.html:3 msgid ""Edit Policy"" msgstr ""Edit Policy"" #: dashboards/project/firewalls/tables.py:133 #: dashboards/project/firewalls/templates/firewalls/_updatefirewall.html:8 #: dashboards/project/firewalls/templates/firewalls/updatefirewall.html:3 msgid ""Edit Firewall"" msgstr ""Edit Firewall"" #: dashboards/project/firewalls/tables.py:207#: dashboards/project/firewalls/tables.py:211#: dashboards/project/firewalls/tables.py:213#: dashboards/project/firewalls/tables.py:215#: dashboards/project/firewalls/tables.py:217#: dashboards/project/firewalls/tables.py:225#: dashboards/project/volumes/snapshots/tables.py:83#: dashboards/project/volumes/volumes/tables.py:89 #: dashboards/project/volumes/volumes/tables.py:114#: dashboards/project/instances/tables.py:400 #: dashboards/project/volumes/volumes/tables.py:167#: dashboards/project/instances/workflows/create_instance.py:666#: dashboards/project/instances/workflows/create_instance.py:386#: dashboards/project/instances/workflows/create_instance.py:676#: dashboards/project/instances/workflows/create_instance.py:677#: dashboards/project/instances/workflows/create_instance.py:154 #: dashboards/project/instances/workflows/create_instance.py:680#: dashboards/project/instances/tables.py:89#: dashboards/project/instances/tables.py:97#: dashboards/project/instances/tables.py:124#: dashboards/project/instances/tables.py:132#: dashboards/project/instances/tables.py:161#: dashboards/project/instances/tables.py:169#: dashboards/project/instances/tables.py:186#: dashboards/project/instances/tables.py:191 #: dashboards/project/instances/tables.py:258#: dashboards/project/instances/tables.py:201#: dashboards/project/instances/tables.py:206 #: dashboards/project/instances/tables.py:273#: dashboards/project/instances/tables.py:253#: dashboards/project/instances/tables.py:268#: dashboards/project/instances/tables.py:387#: dashboards/project/instances/tables.py:419#: dashboards/project/instances/tables.py:445#: dashboards/project/instances/tables.py:468#: dashboards/project/instances/tables.py:498#: dashboards/project/instances/tables.py:517#: dashboards/project/instances/tables.py:536#: dashboards/project/instances/tables.py:559#: dashboards/project/instances/tables.py:582 #: dashboards/project/instances/tables.py:612#: dashboards/project/instances/tables.py:637#: dashboards/project/instances/tables.py:641#: dashboards/project/instances/tables.py:647#: dashboards/project/instances/tables.py:682#: dashboards/project/instances/tables.py:685#: dashboards/project/instances/tables.py:695#: dashboards/project/instances/tables.py:697#: dashboards/project/instances/tables.py:709#: dashboards/project/instances/tables.py:735#: dashboards/project/instances/tables.py:743#: dashboards/project/instances/tables.py:770#: dashboards/project/instances/tables.py:779#: dashboards/project/instances/tables.py:812#: dashboards/project/instances/tables.py:836 #: dashboards/project/instances/tables.py:858#: dashboards/project/instances/tables.py:837#: dashboards/project/instances/tables.py:838#: dashboards/project/instances/tables.py:840#: dashboards/project/instances/tables.py:841#: dashboards/project/instances/tables.py:842#: dashboards/project/instances/tables.py:844#: dashboards/project/instances/tables.py:846#: dashboards/project/instances/tables.py:848#: dashboards/project/instances/tables.py:849#: dashboards/project/instances/tables.py:851#: dashboards/project/instances/tables.py:852#: dashboards/project/instances/tables.py:853#: dashboards/project/instances/tables.py:855#: dashboards/project/instances/tables.py:856#: dashboards/project/instances/tables.py:857#: dashboards/project/instances/tables.py:860#: dashboards/project/instances/tables.py:861#: dashboards/project/instances/tables.py:863#: dashboards/project/instances/tables.py:869#: dashboards/project/instances/tables.py:871#: dashboards/project/instances/tables.py:873#: dashboards/project/instances/tables.py:874#: dashboards/project/instances/tables.py:876#: dashboards/project/instances/tables.py:878#: dashboards/project/instances/tables.py:880#: dashboards/project/instances/tables.py:882#: dashboards/project/instances/tables.py:884#: dashboards/project/instances/tables.py:886#: dashboards/project/instances/tables.py:888#: dashboards/project/instances/tables.py:890#: dashboards/project/instances/tables.py:892#: dashboards/project/instances/tables.py:894#: dashboards/project/instances/tables.py:896#: dashboards/project/instances/tables.py:898#: dashboards/project/instances/tables.py:899#: dashboards/project/instances/tables.py:901#: dashboards/project/instances/tables.py:902#: dashboards/project/instances/tables.py:903 #: dashboards/project/instances/tables.py:906#: dashboards/project/instances/tables.py:905#: dashboards/project/instances/tables.py:908#: dashboards/project/instances/tables.py:910#: dashboards/project/instances/tables.py:911#: dashboards/project/instances/tables.py:913#: dashboards/project/instances/tables.py:915#: dashboards/project/instances/tables.py:917#: dashboards/project/instances/tables.py:919#: dashboards/project/instances/tables.py:920#: dashboards/project/instances/tables.py:921#: dashboards/project/instances/tables.py:923#: dashboards/project/instances/tables.py:924#: dashboards/project/instances/tables.py:925#: dashboards/project/instances/tables.py:927#: dashboards/project/instances/tables.py:929#: dashboards/project/instances/tables.py:931#: dashboards/project/instances/tables.py:933#: dashboards/project/instances/tables.py:937#: dashboards/project/instances/tables.py:938#: dashboards/project/instances/tables.py:939#: dashboards/project/instances/tables.py:940#: dashboards/project/instances/tables.py:941#: dashboards/project/instances/tables.py:942#: dashboards/project/instances/tables.py:943#: dashboards/project/instances/tables.py:944#: dashboards/project/instances/tables.py:945#: dashboards/project/instances/tables.py:946#: dashboards/project/instances/tables.py:983 #: dashboards/project/instances/workflows/create_instance.py:473#: dashboards/project/loadbalancers/tables.py:281#: dashboards/project/volumes/volumes/tables.py:386#: dashboards/project/volumes/snapshots/tables.py:44#: dashboards/project/instances/workflows/create_instance.py:114#: dashboards/project/instances/workflows/create_instance.py:117#: dashboards/project/instances/workflows/create_instance.py:120#: dashboards/project/instances/workflows/create_instance.py:123#: dashboards/project/instances/workflows/create_instance.py:126#: dashboards/project/instances/workflows/create_instance.py:141#: dashboards/project/instances/workflows/create_instance.py:142#: dashboards/project/instances/workflows/create_instance.py:143#: dashboards/project/instances/workflows/create_instance.py:146#: dashboards/project/instances/workflows/create_instance.py:152#: dashboards/project/instances/workflows/create_instance.py:158#: dashboards/project/instances/workflows/create_instance.py:169#: dashboards/project/instances/workflows/create_instance.py:196#: dashboards/project/instances/workflows/create_instance.py:203#: dashboards/project/instances/workflows/create_instance.py:210#: dashboards/project/instances/workflows/create_instance.py:224#: dashboards/project/instances/workflows/create_instance.py:227 #: dashboards/project/instances/workflows/create_instance.py:301#: dashboards/project/instances/workflows/create_instance.py:230#: dashboards/project/instances/workflows/create_instance.py:253#: dashboards/project/instances/workflows/create_instance.py:270#: dashboards/project/instances/workflows/create_instance.py:281 #: dashboards/project/instances/workflows/create_instance.py:298#: dashboards/project/instances/workflows/create_instance.py:286#: dashboards/project/instances/workflows/create_instance.py:292#: dashboards/project/instances/workflows/create_instance.py:318#: dashboards/project/instances/workflows/create_instance.py:324#: dashboards/project/instances/workflows/create_instance.py:326#: dashboards/project/instances/workflows/create_instance.py:349#: dashboards/project/instances/workflows/create_instance.py:359#: dashboards/project/instances/workflows/create_instance.py:364#: dashboards/project/instances/workflows/create_instance.py:398#: dashboards/project/instances/workflows/create_instance.py:400#: dashboards/project/instances/workflows/create_instance.py:412#: dashboards/project/instances/workflows/create_instance.py:414#: dashboards/project/instances/workflows/create_instance.py:416#: dashboards/project/instances/workflows/create_instance.py:427#: dashboards/project/instances/workflows/create_instance.py:430#: dashboards/project/instances/workflows/create_instance.py:432#: dashboards/project/instances/workflows/create_instance.py:475#: dashboards/project/instances/workflows/create_instance.py:479#: dashboards/project/instances/workflows/create_instance.py:485#: dashboards/project/instances/workflows/create_instance.py:491#: dashboards/project/instances/workflows/create_instance.py:496#: dashboards/project/instances/workflows/create_instance.py:512#: dashboards/project/instances/workflows/create_instance.py:516#: dashboards/project/instances/workflows/create_instance.py:518#: dashboards/project/instances/workflows/create_instance.py:527#: dashboards/project/instances/workflows/create_instance.py:559#: dashboards/project/instances/workflows/create_instance.py:561#: dashboards/project/instances/workflows/create_instance.py:568#: dashboards/project/instances/workflows/create_instance.py:591#: dashboards/project/instances/workflows/create_instance.py:594#: dashboards/project/instances/workflows/create_instance.py:684#: dashboards/project/instances/workflows/create_instance.py:712#: dashboards/project/instances/workflows/create_instance.py:770#: dashboards/project/loadbalancers/tables.py:315#: dashboards/project/loadbalancers/tables.py:341#: dashboards/project/loadbalancers/tables.py:342#: dashboards/project/loadbalancers/tables.py:29#: dashboards/project/loadbalancers/tables.py:38#: dashboards/project/loadbalancers/tables.py:56#: dashboards/project/loadbalancers/tables.py:65#: dashboards/project/loadbalancers/tables.py:76 #: dashboards/project/loadbalancers/tables.py:285#: dashboards/project/loadbalancers/tables.py:77#: dashboards/project/loadbalancers/tables.py:97 #: dashboards/project/loadbalancers/tables.py:290#: dashboards/project/loadbalancers/tables.py:116#: dashboards/project/loadbalancers/tables.py:117 #: dashboards/project/loadbalancers/tables.py:348#: dashboards/project/loadbalancers/tables.py:131#: dashboards/project/loadbalancers/tables.py:132 #: dashboards/project/loadbalancers/tables.py:322#: dashboards/project/loadbalancers/tables.py:144#: dashboards/project/loadbalancers/tables.py:162#: dashboards/project/loadbalancers/tables.py:185#: dashboards/project/loadbalancers/tables.py:203#: dashboards/project/loadbalancers/tables.py:229#: dashboards/project/loadbalancers/tables.py:251#: dashboards/project/loadbalancers/tables.py:257#: dashboards/project/loadbalancers/tables.py:280#: dashboards/project/loadbalancers/tables.py:282#: dashboards/project/networks/subnets/tables.py:44#: dashboards/project/loadbalancers/tables.py:313#: dashboards/project/loadbalancers/tables.py:339#: dashboards/project/loadbalancers/tables.py:343#: dashboards/project/routers/tables.py:80#: dashboards/project/networks/tables.py:96#: dashboards/project/networks/subnets/tables.py:108#: dashboards/project/networks/ports/tables.py:34#: dashboards/project/networks/ports/tables.py:36#: dashboards/project/networks/ports/tables.py:63#: dashboards/project/routers/tables.py:70#: dashboards/project/routers/tables.py:104#: dashboards/project/routers/tables.py:124#: dashboards/project/routers/tables.py:125#: dashboards/project/routers/tables.py:126#: dashboards/project/routers/tables.py:127#: dashboards/project/routers/tables.py:144#: dashboards/project/routers/extensions/routerrules/tables.py:30#: dashboards/project/routers/extensions/routerrules/tables.py:48#: dashboards/project/routers/ports/tables.py:32#: dashboards/project/routers/ports/tables.py:34#: dashboards/project/routers/ports/tables.py:41#: dashboards/project/routers/ports/tables.py:62#: dashboards/project/routers/ports/tables.py:70#: dashboards/project/routers/ports/tables.py:95#: dashboards/project/volumes/snapshots/tables.py:63#: dashboards/project/volumes/volumes/tables.py:457#: dashboards/project/volumes/volumes/tables.py:126#: dashboards/project/volumes/volumes/tables.py:257#: dashboards/project/volumes/volumes/tables.py:239#: dashboards/project/volumes/volumes/tables.py:38#: dashboards/project/volumes/volumes/tables.py:76#: dashboards/project/volumes/volumes/tables.py:143#: dashboards/project/volumes/volumes/tables.py:183#: dashboards/project/volumes/volumes/tables.py:216#: dashboards/project/volumes/volumes/tables.py:298#: dashboards/project/volumes/volumes/tables.py:316#: dashboards/project/volumes/volumes/tables.py:337#: dashboards/project/volumes/volumes/tables.py:339#: dashboards/project/volumes/volumes/tables.py:390#: dashboards/project/volumes/volumes/tables.py:393#: dashboards/project/volumes/volumes/tables.py:408#: dashboards/project/volumes/volumes/tables.py:438#: dashboards/project/volumes/volumes/tables.py:447#: dashboards/settings/password/forms.py:66#: dashboards/settings/password/forms.py:69",13758,12719
openstack%2Foslo.middleware~master~I4e3d80607017822b5aebf316595ea1497e46706b,openstack/oslo.middleware,master,I4e3d80607017822b5aebf316595ea1497e46706b,Imported Translations from Transifex,MERGED,2014-09-26 06:00:57.000000000,2014-09-27 05:16:19.000000000,2014-09-27 05:16:19.000000000,"[{'_account_id': 3}, {'_account_id': 1669}]","[{'number': 1, 'created': '2014-09-26 06:00:57.000000000', 'files': ['oslo.middleware/locale/fr/LC_MESSAGES/oslo.middleware-log-critical.po', 'oslo.middleware/locale/fr/LC_MESSAGES/oslo.middleware-log-info.po', 'oslo.middleware/locale/fr/LC_MESSAGES/oslo.middleware-log-warning.po'], 'web_link': 'https://opendev.org/openstack/oslo.middleware/commit/e18de4afbe0e0d48d4dfc5bb8b351eec2accd9f9', 'message': 'Imported Translations from Transifex\n\nChange-Id: I4e3d80607017822b5aebf316595ea1497e46706b\n'}]",0,124280,e18de4afbe0e0d48d4dfc5bb8b351eec2accd9f9,7,2,1,11131,,,0,"Imported Translations from Transifex

Change-Id: I4e3d80607017822b5aebf316595ea1497e46706b
",git fetch https://review.opendev.org/openstack/oslo.middleware refs/changes/80/124280/1 && git format-patch -1 --stdout FETCH_HEAD,"['oslo.middleware/locale/fr/LC_MESSAGES/oslo.middleware-log-critical.po', 'oslo.middleware/locale/fr/LC_MESSAGES/oslo.middleware-log-info.po', 'oslo.middleware/locale/fr/LC_MESSAGES/oslo.middleware-log-warning.po']",3,e18de4afbe0e0d48d4dfc5bb8b351eec2accd9f9,transifex/translations,"# Translations template for heat. # Copyright (C) 2014 ORGANIZATION # This file is distributed under the same license as the heat project. # # Translators: # Maxime COQUEREL <max.coquerel@gmail.com>, 2014 msgid """" msgstr """" ""Project-Id-Version: oslo.middleware\n"" ""Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"" ""POT-Creation-Date: 2014-09-26 06:00+0000\n"" ""PO-Revision-Date: 2014-09-25 08:48+0000\n"" ""Last-Translator: Maxime COQUEREL <max.coquerel@gmail.com>\n"" ""Language-Team: French (http://www.transifex.com/projects/p/oslomiddleware/"" ""language/fr/)\n"" ""Language: fr\n"" ""MIME-Version: 1.0\n"" ""Content-Type: text/plain; charset=UTF-8\n"" ""Content-Transfer-Encoding: 8bit\n"" ""Generated-By: Babel 1.3\n"" ""Plural-Forms: nplurals=2; plural=(n > 1);\n"" ",,63,0
openstack%2Fhorizon~master~I318057a9f5870d4a25ebf461d5fc67e4a03d54d8,openstack/horizon,master,I318057a9f5870d4a25ebf461d5fc67e4a03d54d8,Fixing issue with editing N1K network profiles,MERGED,2014-03-06 18:49:53.000000000,2014-09-27 05:16:09.000000000,2014-09-27 05:16:07.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 1941}, {'_account_id': 4264}, {'_account_id': 4978}, {'_account_id': 6620}, {'_account_id': 6650}, {'_account_id': 9659}, {'_account_id': 9680}, {'_account_id': 9981}, {'_account_id': 11880}, {'_account_id': 12000}]","[{'number': 1, 'created': '2014-03-06 18:49:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/0c72635071d6e699adbcc5f93e63b981e31ee6d1', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding a new unit test.\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\nCloses-bug: #1260435\n'}, {'number': 2, 'created': '2014-03-12 22:26:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/676496b2c1058505d1b36097b66e3c153a3f199f', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding a new unit test.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 3, 'created': '2014-03-13 00:29:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/a4571b183ed39c1a18641c6dbbd7192e7be2fc20', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding a new unit test.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 4, 'created': '2014-03-13 04:49:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/becbe9d1c0aade8bea8497b0f9cc10a7312e9719', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding a new unit test.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 5, 'created': '2014-03-13 21:47:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/eb453061a1323d0dcf0b5104b460bcb5ee3c3239', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding a new unit test.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 6, 'created': '2014-03-13 22:21:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/4adbcc9a1e67b876765be4e2a1c66b79a017d254', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding a new unit test.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 7, 'created': '2014-03-13 22:28:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/c3f05daf546fed79aac7f7d18627c8954cf4ac94', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding a new unit test.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 8, 'created': '2014-03-14 06:38:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/5fed218fc43a7785dce209a42689766cd98c52ba', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding a new unit test.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 9, 'created': '2014-03-14 06:53:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/eae59e72d6821ff0355d7eebe03daf4ee5613516', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding a new unit test.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 10, 'created': '2014-03-17 19:28:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/803fbbe0781bc5774a8c8f400b75c160045fb3c3', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 11, 'created': '2014-03-24 21:52:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/09f2895c8b955dbd70f3c5d468b86aa2ea13c9e7', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 12, 'created': '2014-05-27 18:32:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/2a66abcc7cc7c5318f2eba71a98380cb39aa9d30', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 13, 'created': '2014-07-17 18:05:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/5951afc250f46bf96bd43786f2a932b7b9bdc604', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 14, 'created': '2014-07-22 18:00:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/3c39bf284a6a5f59be1260b5aa379f16efeca983', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 15, 'created': '2014-07-22 18:31:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/763e4bab01602bd6e9a37578993c12afa3d0b6dc', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 16, 'created': '2014-07-25 02:07:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/89ce95d7a0f7e8a3302a35d3d10cc8ff114417ad', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 17, 'created': '2014-07-30 00:11:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/97da89b797c8293c09bdaf80d3d1a315f9cbb032', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 18, 'created': '2014-08-05 16:48:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/fd9e4117732b0d61d7c184a2240256ab5b59f254', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 19, 'created': '2014-08-15 01:15:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/28aa08fb4ce7a901a29aab21755b4df740e83c2e', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 20, 'created': '2014-09-18 22:14:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/c8a4eaefd7202cb2ab8cbdb9b0eea93d32268899', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 21, 'created': '2014-09-18 22:51:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/63a6f0d16163e669f18db85e90eaac5bae351d9a', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 22, 'created': '2014-09-22 13:30:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/23bf49d45d1e0d335faefd6f11ca86eba823a05c', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\n(Pulled from gate: looks like all Horizon unit tests are very error prone right now)\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 23, 'created': '2014-09-22 17:42:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/db0e30cebb88a26249e21b8a71bcf9148cb7fd8b', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\n(Pulled from gate: looks like all Horizon unit tests are very error prone right now)\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 24, 'created': '2014-09-23 14:38:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/995e51cf21b71c0fcbb4587d6e68249fe71774d4', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 25, 'created': '2014-09-25 15:20:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/f9a6ddca0956b80cb85f851bc337834a5a0fcd28', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}, {'number': 26, 'created': '2014-09-25 17:29:15.000000000', 'files': ['openstack_dashboard/dashboards/router/nexus1000v/tests.py', 'openstack_dashboard/dashboards/router/nexus1000v/templates/nexus1000v/_update_network_profile.html', 'openstack_dashboard/dashboards/router/nexus1000v/views.py', 'openstack_dashboard/dashboards/router/nexus1000v/templates/nexus1000v/update_network_profile.html', 'openstack_dashboard/dashboards/router/nexus1000v/forms.py', 'openstack_dashboard/test/test_data/neutron_data.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/c946139e4e2e81bfdb8f301a35fd1893893bbcb3', 'message': 'Fixing issue with editing N1K network profiles\n\nThis resolves issues surrounding the update network profiles\nfor the Cisco N1K profiles when used with the N1K neutron\nplugin.\n\nIssue resolved include the edit form opening up properly\nand also addressing which fields are editable and which not.\n\nAlso adding new unit tests.\n\nCloses-bug: #1260435\n\nChange-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8\n'}]",48,78708,c946139e4e2e81bfdb8f301a35fd1893893bbcb3,123,12,26,6620,,,0,"Fixing issue with editing N1K network profiles

This resolves issues surrounding the update network profiles
for the Cisco N1K profiles when used with the N1K neutron
plugin.

Issue resolved include the edit form opening up properly
and also addressing which fields are editable and which not.

Also adding new unit tests.

Closes-bug: #1260435

Change-Id: I318057a9f5870d4a25ebf461d5fc67e4a03d54d8
",git fetch https://review.opendev.org/openstack/horizon refs/changes/08/78708/24 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/router/nexus1000v/tests.py', 'openstack_dashboard/dashboards/router/nexus1000v/templates/nexus1000v/_update_network_profile.html', 'openstack_dashboard/dashboards/router/nexus1000v/views.py', 'openstack_dashboard/dashboards/router/nexus1000v/templates/nexus1000v/update_network_profile.html', 'openstack_dashboard/dashboards/router/nexus1000v/forms.py']",5,0c72635071d6e699adbcc5f93e63b981e31ee6d1,bug/1260435," choices=[('vlan', _('VLAN')), ('overlay', _('OVERLAY')), ('trunk', _('TRUNK'))], (attrs={'class': 'switchable', 'data-slug': 'segtype', 'readonly': 'readonly'})) # Sub type options available for Overlay segment type sub_type = forms.ChoiceField(label=_('Sub Type'), choices=[('native_vxlan', _('NATIVE VXLAN')), ('enhanced', _('ENHANCED')), ('other', _('OTHER'))], required=False, widget=forms.Select (attrs={'class': 'switchable switched', 'data-slug': 'subtype', 'data-switch-on': 'segtype', 'data-segtype-overlay': _(""Sub Type""), 'readonly': 'readonly'})) # Sub type options available for Trunk segment type sub_type_trunk = forms.ChoiceField(label=_('Sub Type'), choices=[('vlan', _('VLAN'))], required=False, widget=forms.Select (attrs={'class': 'switched', 'data-switch-on': 'segtype', 'data-segtype-trunk': _(""Sub Type""), 'readonly': 'readonly'})) required=False, widget=forms.TextInput (attrs={'class': 'switched', 'data-switch-on': 'segtype', 'data-segtype-vlan': _(""Segment Range""), 'data-segtype-overlay': _(""Segment Range""), 'readonly': 'readonly'}), help_text=_(""1-4093 for VLAN; "" ""5000-10000 for Overlay"")) multicast_ip_range = forms.CharField(max_length=30, label=_(""Multicast IP Range""), required=False, widget=forms.TextInput (attrs={'class': 'switched', 'data-switch-on': 'subtype', 'data-subtype-native_vxlan': _(""Multicast IP Range"")}), help_text=_(""Multicast IPv4 range"" ""(e.g. 132.0.0.0-"" ""132.0.0.100)"")) other_subtype = forms.CharField(max_length=255, label=_(""Sub Type Value (Manual Input)""), required=False, widget=forms.TextInput (attrs={'class': 'switched', 'data-switch-on': 'subtype', 'data-subtype-other': _(""Sub Type Value "" ""(Manual Input)"")}), help_text=_(""Enter parameter (e.g. GRE)"")) required=False, widget=forms.TextInput (attrs={'class': 'switched', 'data-switch-on': 'segtype', 'data-segtype-vlan': _(""Physical Network""), 'readonly': 'readonly'})) def clean(self): # If sub_type is 'other' or 'trunk' then # assign this new value for sub_type cleaned_data = super(CreateNetworkProfile, self).clean() segment_type = cleaned_data.get('segment_type') if segment_type == 'overlay': sub_type = cleaned_data.get('sub_type') if sub_type == 'other': other_subtype = cleaned_data.get('other_subtype') cleaned_data['sub_type'] = other_subtype LOG.debug('subtype is now %(params)s', {'params': other_subtype}) elif segment_type == 'trunk': sub_type_trunk = cleaned_data.get('sub_type_trunk') cleaned_data['sub_type'] = sub_type_trunk LOG.debug('subtype is now %(params)s', {'params': sub_type_trunk}) return cleaned_data params = {'name': data['name'], 'segment_range': data['segment_range'], 'multicast_ip_range': data['multicast_ip_range']} **params) 'was successfully updated.') % data['name'] msg = _('Failed to update network profile (%s).', LOG.error(msg)"," choices=[('vlan', 'VLAN'), ('vxlan', 'VXLAN')], (attrs={'class': 'switchable'})) required=True) required=False) name=data['name'], segment_type= data['segment_type'], segment_range= data['segment_range'], physical_network= data['physical_network']) 'was successfully updated.') % data['profile_id'] LOG.error('Failed to update network profile (%s).',",106,19
openstack%2Fkeystonemiddleware~master~I4f65adb523a591816a6cd807070d92ce6e414a1b,openstack/keystonemiddleware,master,I4f65adb523a591816a6cd807070d92ce6e414a1b,Support service user and project in non-default domain,MERGED,2014-09-21 18:52:18.000000000,2014-09-27 05:16:02.000000000,2014-09-27 05:16:01.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 2218}, {'_account_id': 5707}, {'_account_id': 6486}, {'_account_id': 7725}]","[{'number': 1, 'created': '2014-09-21 18:52:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/8f44b91c8e077c8ea571a0d697a2ee93deb40868', 'message': ""Support service user and project in non-default domain\n\nThe domain for the service user and project couldn't be configured,\nso the auth_token middleware always used the v2 API and the default\ndomain. With this change, the deployer can configure the domain for\nthe service user and project and the auth_token middleware will then\nuse the v3 API to get the service token.\n\nDocImpact\nCloses-Bug: #1372142\n\nChange-Id: I4f65adb523a591816a6cd807070d92ce6e414a1b\n""}, {'number': 2, 'created': '2014-09-22 20:09:55.000000000', 'files': ['keystonemiddleware/auth_token.py', 'keystonemiddleware/tests/test_opts.py', 'keystonemiddleware/tests/test_auth_token_middleware.py', 'doc/source/middlewarearchitecture.rst'], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/bb00caf15be9336663521a913984795af9bed185', 'message': ""Support service user and project in non-default domain\n\nThe domain for the service user and project couldn't be configured,\nso the auth_token middleware always used the v2 API and the default\ndomain. With this change, the deployer can configure the domain for\nthe service user and project and the auth_token middleware will then\nuse the v3 API to get the service token.\n\nDocImpact\nCloses-Bug: #1372142\n\nChange-Id: I4f65adb523a591816a6cd807070d92ce6e414a1b\n""}]",3,123011,bb00caf15be9336663521a913984795af9bed185,22,6,2,6486,,,0,"Support service user and project in non-default domain

The domain for the service user and project couldn't be configured,
so the auth_token middleware always used the v2 API and the default
domain. With this change, the deployer can configure the domain for
the service user and project and the auth_token middleware will then
use the v3 API to get the service token.

DocImpact
Closes-Bug: #1372142

Change-Id: I4f65adb523a591816a6cd807070d92ce6e414a1b
",git fetch https://review.opendev.org/openstack/keystonemiddleware refs/changes/11/123011/1 && git format-patch -1 --stdout FETCH_HEAD,"['keystonemiddleware/auth_token.py', 'keystonemiddleware/tests/test_opts.py', 'keystonemiddleware/tests/test_auth_token_middleware.py', 'doc/source/middlewarearchitecture.rst']",4,8f44b91c8e077c8ea571a0d697a2ee93deb40868,bug/1335726," # Keystone service account user domain ID. (string value) #admin_user_domain_id=<None> # Keystone service account user domain name. (string value) #admin_user_domain_name=<None> # Keystone service account project domain ID. (string value) #admin_project_domain_id=<None> # Keystone service account project domain name. (string value) #admin_project_domain_name=<None> * ``admin_user``, ``admin_user_domain_name``, ``admin_user_domain_id``, ``admin_password``, ``admin_tenant_name``, ``admin_project_domain_id``, ``admin_project_domain_name``: if ``admin_token`` is not set, or invalid, then ``admin_user``, ``admin_password``, and ``admin_tenant_name`` are defined as a service account which is expected to have been previously configured in Keystone to validate user tokens. If the service user isn't in the default domain, set ``admin_user_domain_name`` or ``admin_user_domain_id``. If the service project isn't in the default domain, set ``admin_project_domain_id`` or ``admin_project_domain_name``.","* ``admin_user``, ``admin_password``, ``admin_tenant_name``: if ``admin_token`` is not set, or invalid, then admin_user, admin_password, and admin_tenant_name are defined as a service account which is expected to have been previously configured in Keystone to validate user tokens.",90,6
openstack%2Fceilometer~master~I02f439ae89f265e088ca539ab355561d1b7b1c31,openstack/ceilometer,master,I02f439ae89f265e088ca539ab355561d1b7b1c31,dbsync: Acknowledge 'metering_connection' option,MERGED,2014-09-24 07:42:31.000000000,2014-09-27 05:15:53.000000000,2014-09-27 05:15:53.000000000,"[{'_account_id': 3}, {'_account_id': 3012}, {'_account_id': 4491}, {'_account_id': 6537}, {'_account_id': 6676}, {'_account_id': 7729}, {'_account_id': 8052}]","[{'number': 1, 'created': '2014-09-24 07:42:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/79322a4e48d7be37c770083a09adf63933454567', 'message': ""dbsync: Acknowledge 'metering_connection' option\n\nSupport 'metering_connection' option in ceilometer-dbsync.\n\nChange-Id: I02f439ae89f265e088ca539ab355561d1b7b1c31\n""}, {'number': 2, 'created': '2014-09-24 07:43:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/673502eecc7fba53bb5ce697cc57ce2638dfc057', 'message': ""dbsync: Acknowledge 'metering_connection' option\n\nSupport 'metering_connection' option in ceilometer-dbsync.\nCloses-Bug: #1373269\n\nChange-Id: I02f439ae89f265e088ca539ab355561d1b7b1c31\n""}, {'number': 3, 'created': '2014-09-26 00:57:07.000000000', 'files': ['ceilometer/cmd/storage.py'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/7beccc684d74472949d54b14b72d29ce74479931', 'message': ""dbsync: Acknowledge 'metering_connection' option\n\nSupport 'metering_connection' option in ceilometer-dbsync.\n\nCloses-Bug: #1373269\n\nChange-Id: I02f439ae89f265e088ca539ab355561d1b7b1c31\n""}]",5,123653,7beccc684d74472949d54b14b72d29ce74479931,20,7,3,4491,,,0,"dbsync: Acknowledge 'metering_connection' option

Support 'metering_connection' option in ceilometer-dbsync.

Closes-Bug: #1373269

Change-Id: I02f439ae89f265e088ca539ab355561d1b7b1c31
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/53/123653/3 && git format-patch -1 --stdout FETCH_HEAD,['ceilometer/cmd/storage.py'],1,79322a4e48d7be37c770083a09adf63933454567,bug/1373269," storage.get_connection_from_config(cfg.CONF, 'metering').upgrade()", storage.get_connection_from_config(cfg.CONF).upgrade(),1,1
openstack%2Fnova~stable%2Ficehouse~I3b49b1d667f6ade9ae3f6765d735440a3e838917,openstack/nova,stable/icehouse,I3b49b1d667f6ade9ae3f6765d735440a3e838917,Sync process and str utils from oslo,MERGED,2014-09-13 19:37:21.000000000,2014-09-27 05:11:52.000000000,2014-09-27 05:11:49.000000000,"[{'_account_id': 3}, {'_account_id': 308}, {'_account_id': 1955}, {'_account_id': 2271}, {'_account_id': 5170}, {'_account_id': 6802}, {'_account_id': 8802}, {'_account_id': 9311}, {'_account_id': 9656}]","[{'number': 1, 'created': '2014-09-13 19:37:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/839bd70a47b43aa72a43321a5b308b29ef986330', 'message': 'Sync latest process and str utils from oslo\n\nThis backport the necessary changes to fix both issues:\n* Make execute method clean password in exception\n* Make sure mask_password works properly\n\nBackport in oslo-incubator: https://review.openstack.org/121365\n\nCloses-Bug: 1343604\nCloses-Bug: 1345233\nSecurityImpact\n\nChange-Id: I3b49b1d667f6ade9ae3f6765d735440a3e838917\n'}, {'number': 2, 'created': '2014-09-22 16:59:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/cdd95d60eab262cc6ff1a3f95d79e5fabb2137e0', 'message': 'Sync latest process and str utils from oslo\n\nThis backport the necessary changes to fix both issues:\n* Make execute method clean password in exception\n* Make sure mask_password works properly\n\nBackport in oslo-incubator: https://review.openstack.org/121365\n\nCloses-Bug: 1343604\nCloses-Bug: 1345233\nSecurityImpact\n\nChange-Id: I3b49b1d667f6ade9ae3f6765d735440a3e838917\n'}, {'number': 3, 'created': '2014-09-24 19:36:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/2a4897216c544cd05ab8422aa0caa3a955bad95e', 'message': 'Sync latest process and str utils from oslo\n\nThis backport the necessary changes to fix both issues:\n* Make execute method clean password in exception\n* Make sure mask_password works properly\n\n------------------------------------------------\nThe sync pulls in the following changes (newest to oldest):\n\n63c99a0f - Mask passwords in exceptions and error messages\n66142c34 - Make strutils.mask_password more secure\nd6b55fb2 - Remove `processutils` dependency on `log`\ncb5a804b - Move `mask_password` to strutils\n\n-----------------------------------------------\n\nBackport in oslo-incubator: https://review.openstack.org/121365\n\nCloses-Bug: 1343604\nCloses-Bug: 1345233\nSecurityImpact\n\nChange-Id: I3b49b1d667f6ade9ae3f6765d735440a3e838917\n'}, {'number': 4, 'created': '2014-09-24 19:36:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a0e414f1126916a1547415b2379ebfb18f45973f', 'message': 'Sync latest process and str utils from oslo\n\nThis backport the necessary changes to fix both issues:\n* Make execute method clean password in exception\n* Make sure mask_password works properly\n\n------------------------------------------------\nThe sync pulls in the following changes (newest to oldest):\n\n63c99a0f - Mask passwords in exceptions and error messages\n66142c34 - Make strutils.mask_password more secure\nd6b55fb2 - Remove `processutils` dependency on `log`\ncb5a804b - Move `mask_password` to strutils\n\n-----------------------------------------------\n\nBackport in oslo-incubator: https://review.openstack.org/121365\n\nCloses-Bug: 1343604\nCloses-Bug: 1345233\nSecurityImpact\n\nChange-Id: I3b49b1d667f6ade9ae3f6765d735440a3e838917\n'}, {'number': 5, 'created': '2014-09-25 09:04:50.000000000', 'files': ['nova/openstack/common/strutils.py', 'nova/openstack/common/processutils.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/f58d95c964cb9a42f573596d1cc80d2034ddb23e', 'message': 'Sync process and str utils from oslo\n\nThis patch backports the necessary changes to fix both issues:\n* Make execute method clean password in exception\n* Make sure mask_password works properly\n\n------------------------------------------------\nThe sync pulls in the following changes (newest to oldest):\n\n63c99a0f - Mask passwords in exceptions and error messages\n66142c34 - Make strutils.mask_password more secure\nd6b55fb2 - Remove `processutils` dependency on `log`\ncb5a804b - Move `mask_password` to strutils\n\n-----------------------------------------------\n\nBackport in oslo-incubator: https://review.openstack.org/121365\n\nCloses-Bug: 1343604\nCloses-Bug: 1345233\nSecurityImpact\n\nChange-Id: I3b49b1d667f6ade9ae3f6765d735440a3e838917\n'}]",4,121383,f58d95c964cb9a42f573596d1cc80d2034ddb23e,38,9,5,9311,,,0,"Sync process and str utils from oslo

This patch backports the necessary changes to fix both issues:
* Make execute method clean password in exception
* Make sure mask_password works properly

------------------------------------------------
The sync pulls in the following changes (newest to oldest):

63c99a0f - Mask passwords in exceptions and error messages
66142c34 - Make strutils.mask_password more secure
d6b55fb2 - Remove `processutils` dependency on `log`
cb5a804b - Move `mask_password` to strutils

-----------------------------------------------

Backport in oslo-incubator: https://review.openstack.org/121365

Closes-Bug: 1343604
Closes-Bug: 1345233
SecurityImpact

Change-Id: I3b49b1d667f6ade9ae3f6765d735440a3e838917
",git fetch https://review.opendev.org/openstack/nova refs/changes/83/121383/4 && git format-patch -1 --stdout FETCH_HEAD,"['nova/openstack/common/strutils.py', 'nova/openstack/common/processutils.py']",2,839bd70a47b43aa72a43321a5b308b29ef986330,bug/1343604,"import loggingfrom nova.openstack.common import strutils :type loglevel: int. (Should be logging.DEBUG or logging.INFO) loglevel = kwargs.pop('loglevel', logging.DEBUG) sanitized_cmd = strutils.mask_password(' '.join(cmd)) LOG.log(loglevel, _('Running cmd (subprocess): %s'), sanitized_cmd) sanitized_stdout = strutils.mask_password(stdout) sanitized_stderr = strutils.mask_password(stderr) stdout=sanitized_stdout, stderr=sanitized_stderr, cmd=sanitized_cmd) LOG.log(loglevel, _('%r failed. Retrying.'), sanitized_cmd)","import logging as stdlib_loggingfrom nova.openstack.common import log as logging :type loglevel: int. (Should be stdlib_logging.DEBUG or stdlib_logging.INFO) loglevel = kwargs.pop('loglevel', stdlib_logging.DEBUG) LOG.log(loglevel, _('Running cmd (subprocess): %s'), ' '.join(cmd)) stdout=stdout, stderr=stderr, cmd=' '.join(cmd)) LOG.log(loglevel, _('%r failed. Retrying.'), cmd)",86,10
openstack%2Fhorizon~stable%2Ficehouse~I4d9aa9e716e4cf6c2b56fd25ab8fde9187fc5108,openstack/horizon,stable/icehouse,I4d9aa9e716e4cf6c2b56fd25ab8fde9187fc5108,TEMPLATE_DIRS must be a tuple,MERGED,2014-08-07 14:23:57.000000000,2014-09-27 05:08:34.000000000,2014-09-27 05:08:34.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 1955}, {'_account_id': 4978}, {'_account_id': 5623}, {'_account_id': 6282}, {'_account_id': 6638}, {'_account_id': 7976}, {'_account_id': 9656}, {'_account_id': 9981}, {'_account_id': 12000}, {'_account_id': 13161}]","[{'number': 1, 'created': '2014-08-07 14:23:57.000000000', 'files': ['horizon/test/settings.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/c9cb128307ab1fa14585fd82f34f8f602f1d8d47', 'message': ""TEMPLATE_DIRS must be a tuple\n\nWith Django 1.7, TEMPLATE_DIRS must be a tuple. The\ndefinition in horizon/test/settings.py must therefore\nhave a leading comma. Note that other definitions of\nTEMPLATE_DIRS are already like this in Horizon, so\nthis just fixes the one instance which is wrong.\n\nNote that it is required to support Django 1.7 in\nIcehouse, because we're planning to release Debian\nJessie with both Django 1.7 and Icehouse. This is to\n*support* Django 1.7, not at all to *switch* to it,\nwhich is very different.\n\nChange-Id: I4d9aa9e716e4cf6c2b56fd25ab8fde9187fc5108\n(cherry picked from commit daf9e5c772f82b783ce2255f7cd347ef397efcaf)\n""}]",0,112589,c9cb128307ab1fa14585fd82f34f8f602f1d8d47,19,12,1,6476,,,0,"TEMPLATE_DIRS must be a tuple

With Django 1.7, TEMPLATE_DIRS must be a tuple. The
definition in horizon/test/settings.py must therefore
have a leading comma. Note that other definitions of
TEMPLATE_DIRS are already like this in Horizon, so
this just fixes the one instance which is wrong.

Note that it is required to support Django 1.7 in
Icehouse, because we're planning to release Debian
Jessie with both Django 1.7 and Icehouse. This is to
*support* Django 1.7, not at all to *switch* to it,
which is very different.

Change-Id: I4d9aa9e716e4cf6c2b56fd25ab8fde9187fc5108
(cherry picked from commit daf9e5c772f82b783ce2255f7cd347ef397efcaf)
",git fetch https://review.opendev.org/openstack/horizon refs/changes/89/112589/1 && git format-patch -1 --stdout FETCH_HEAD,['horizon/test/settings.py'],1,c9cb128307ab1fa14585fd82f34f8f602f1d8d47,fix-template-dirs-icehouse,"TEMPLATE_DIRS = (os.path.join(ROOT_PATH, 'tests', 'templates'),)","TEMPLATE_DIRS = (os.path.join(ROOT_PATH, 'tests', 'templates'))",1,1
openstack%2Fcinder~stable%2Ficehouse~I38f3db336db22bb096a8855bcbbfeba16e517b80,openstack/cinder,stable/icehouse,I38f3db336db22bb096a8855bcbbfeba16e517b80,Fixes terminate_connection live migration issue,MERGED,2014-08-26 23:27:13.000000000,2014-09-27 05:03:48.000000000,2014-09-27 05:03:47.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 3185}, {'_account_id': 6491}, {'_account_id': 8213}, {'_account_id': 8871}, {'_account_id': 9533}, {'_account_id': 9656}, {'_account_id': 10503}, {'_account_id': 11811}, {'_account_id': 12780}, {'_account_id': 13308}]","[{'number': 1, 'created': '2014-08-26 23:27:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/ecb5e929c1e6bc22f86cc45901fc90a9264cc567', 'message': 'Fixes terminate_connection live migration issue\n\nReverts the changes to cinder/volume/manager.py added in\ncommit b868ae707f9ecbe254101e21d9d7ffa0b05b17d1 as calling\nremove_export in terminate_connection causes Nova live\nmigration to fail when volumes are attached.\n\nConflicts:\n        cinder/volume/manager.py\n\nChange-Id: I38f3db336db22bb096a8855bcbbfeba16e517b80\nCloses-Bug: #1361738\n'}, {'number': 2, 'created': '2014-08-27 01:05:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9abafff5ba6a545cb39097e4752e7b0cf346bfd6', 'message': 'Fixes terminate_connection live migration issue\n\nReverts the changes to cinder/volume/manager.py added in\ncommit b868ae707f9ecbe254101e21d9d7ffa0b05b17d1 as calling\nremove_export in terminate_connection causes Nova live\nmigration to fail when volumes are attached.\n\nConflicts:\n        cinder/volume/manager.py\n\nChange-Id: I38f3db336db22bb096a8855bcbbfeba16e517b80\nCloses-Bug: #1361738\n'}, {'number': 3, 'created': '2014-09-25 12:27:05.000000000', 'files': ['cinder/volume/manager.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/8f7823d300d8e85ac8cc5d80437eec736f83aad6', 'message': 'Fixes terminate_connection live migration issue\n\nReverts the changes to cinder/volume/manager.py added in\ncommit b868ae707f9ecbe254101e21d9d7ffa0b05b17d1 as calling\nremove_export in terminate_connection causes Nova live\nmigration to fail when volumes are attached.\n\nConflicts:\n        cinder/volume/manager.py\n\nChange-Id: I38f3db336db22bb096a8855bcbbfeba16e517b80\nCloses-Bug: #1361738\n(cherry picked from ddcad011db507cde66b6b1e655d5ffc91ab8880f)\n'}]",1,117050,8f7823d300d8e85ac8cc5d80437eec736f83aad6,36,12,3,3185,,,0,"Fixes terminate_connection live migration issue

Reverts the changes to cinder/volume/manager.py added in
commit b868ae707f9ecbe254101e21d9d7ffa0b05b17d1 as calling
remove_export in terminate_connection causes Nova live
migration to fail when volumes are attached.

Conflicts:
        cinder/volume/manager.py

Change-Id: I38f3db336db22bb096a8855bcbbfeba16e517b80
Closes-Bug: #1361738
(cherry picked from ddcad011db507cde66b6b1e655d5ffc91ab8880f)
",git fetch https://review.opendev.org/openstack/cinder refs/changes/50/117050/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/manager.py'],1,ecb5e929c1e6bc22f86cc45901fc90a9264cc567,," LOG.debug(_(""volume %s: removing export""), volume_id) self.driver.remove_export(context.elevated(), volume_ref) except Exception as ex: LOG.exception(_(""Error detaching volume %(volume)s, "" ""due to remove export failure.""), {""volume"": volume_id}) raise exception.RemoveExportException(volume=volume_id, reason=ex)"," try: LOG.debug(_(""volume %s: removing export""), volume_id) self.driver.remove_export(context.elevated(), volume_ref) except Exception as ex: LOG.exception(_(""Error detaching volume %(volume)s, "" ""due to remove export failure.""), {""volume"": volume_id}) raise exception.RemoveExportException(volume=volume_id, reason=ex) ",7,9
openstack%2Ftripleo-image-elements~master~Ied7cbd1db2bf5a0638324dcb8ac7135d03ed9aad,openstack/tripleo-image-elements,master,Ied7cbd1db2bf5a0638324dcb8ac7135d03ed9aad,Fix swift object-server cffi version mismatch bug,MERGED,2014-09-05 14:17:34.000000000,2014-09-27 04:28:55.000000000,2014-09-27 04:28:54.000000000,"[{'_account_id': 3}, {'_account_id': 215}, {'_account_id': 1005}, {'_account_id': 4330}, {'_account_id': 6969}, {'_account_id': 7847}, {'_account_id': 8688}, {'_account_id': 10035}]","[{'number': 1, 'created': '2014-09-05 14:17:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/07659ec9c065351a7e874109fd0e91859ceb5539', 'message': 'Fix swift object-server cffi version mismatch bug\n\nThis patch is to fix a recent issue that we saw with Swift in\nthe overcloud.  The basic problem is a version mismatch between\nthe version of cffi installed in the Swift venv and the version\nof cffi for which the python xattr module in the Swift venv\nhas pre-built support. The version of xattr installed has\npre-built support for cffi 0.8.2, whereas cffi 0.8.6 is installed.\nBecause of this, at swift-object start-up the cffi 0.8.6 module\nattempts to build the appropriate C file in the python xattr\ndirectory and fails because it doesn\'t have write permission\non the python xattr directory.  The reason is that the Swift venv\nis installed by root, whereas Swift is run as the ""swift"" user.\n\nTo fix this we\'ve added a new module 76-swift-cffi which is run at\ninstall time and executes a simple python command:\n\nfrom xattr import getxattr, setxattr\n\nin the Swift venv. This forces a build of the correct cffi library\nin the xattr module.\n\nChange-Id: Ied7cbd1db2bf5a0638324dcb8ac7135d03ed9aad\nCloses-Bug: #1340776\n'}, {'number': 2, 'created': '2014-09-08 09:03:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/2d77fe2a7f7b31b980f300390ed0c7de98593230', 'message': 'Fix swift object-server cffi version mismatch bug\n\nThis patch is to fix a recent issue that we saw with Swift in\nthe overcloud.  The basic problem is a version mismatch between\nthe version of cffi installed in the Swift venv and the version\nof cffi for which the python xattr module in the Swift venv\nhas pre-built support. The version of xattr installed has\npre-built support for cffi 0.8.2, whereas cffi 0.8.6 is installed.\nBecause of this, at swift-object start-up the cffi 0.8.6 module\nattempts to build the appropriate C file in the python xattr\ndirectory and fails because it doesn\'t have write permission\non the python xattr directory.  The reason is that the Swift venv\nis installed by root, whereas Swift is run as the ""swift"" user.\n\nTo fix this we\'ve added a new module 76-swift-cffi which is run at\ninstall time and executes a simple python command:\n\nfrom xattr import getxattr, setxattr\n\nin the Swift venv. This forces a build of the correct cffi library\nin the xattr module.\n\nChange-Id: Ied7cbd1db2bf5a0638324dcb8ac7135d03ed9aad\nCloses-Bug: #1340776\n'}, {'number': 3, 'created': '2014-09-08 12:16:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/3df6b9480a0a97e654807e82d2a509c910716ea0', 'message': 'Fix swift object-server cffi version mismatch bug\n\nThis patch is to fix a recent issue that we saw with Swift in\nthe overcloud.  The basic problem is a version mismatch between\nthe version of cffi installed in the Swift venv and the version\nof cffi for which the python xattr module in the Swift venv\nhas pre-built support. The version of xattr installed has\npre-built support for cffi 0.8.2, whereas cffi 0.8.6 is installed.\nBecause of this, at swift-object start-up the cffi 0.8.6 module\nattempts to build the appropriate C file in the python xattr\ndirectory and fails because it doesn\'t have write permission\non the python xattr directory.  The reason is that the Swift venv\nis installed by root, whereas Swift is run as the ""swift"" user.\n\nTo fix this we\'ve added a new module 76-swift-cffi which is run at\ninstall time and executes a simple python command:\n\nfrom xattr import getxattr, setxattr\n\nin the Swift venv. This forces a build of the correct cffi library\nin the xattr module.\n\nChange-Id: Ied7cbd1db2bf5a0638324dcb8ac7135d03ed9aad\nCloses-Bug: #1340776\n'}, {'number': 4, 'created': '2014-09-22 08:14:04.000000000', 'files': ['elements/swift/install.d/swift-source-install/76-swift-cffi'], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/e20862dfed707a1b534e6cd953a673eff7ffa4b4', 'message': 'Fix swift object-server cffi version mismatch bug\n\nThis patch is to fix a recent issue that we saw with Swift in\nthe overcloud.  The basic problem is a version mismatch between\nthe version of cffi installed in the Swift venv and the version\nof cffi for which the python xattr module in the Swift venv\nhas pre-built support. The version of xattr installed has\npre-built support for cffi 0.8.2, whereas cffi 0.8.6 is installed.\nBecause of this, at swift-object start-up the cffi 0.8.6 module\nattempts to build the appropriate C file in the python xattr\ndirectory and fails because it doesn\'t have write permission\non the python xattr directory.  The reason is that the Swift venv\nis installed by root, whereas Swift is run as the ""swift"" user.\n\nTo fix this we\'ve added a new module 76-swift-cffi which is run at\ninstall time and executes a simple python command:\n\nfrom xattr import getxattr, setxattr\n\nin the Swift venv. This forces a build of the correct cffi library\nin the xattr module.\n\nChange-Id: Ied7cbd1db2bf5a0638324dcb8ac7135d03ed9aad\nCloses-Bug: #1340776\n'}]",12,119381,e20862dfed707a1b534e6cd953a673eff7ffa4b4,37,8,4,1005,,,0,"Fix swift object-server cffi version mismatch bug

This patch is to fix a recent issue that we saw with Swift in
the overcloud.  The basic problem is a version mismatch between
the version of cffi installed in the Swift venv and the version
of cffi for which the python xattr module in the Swift venv
has pre-built support. The version of xattr installed has
pre-built support for cffi 0.8.2, whereas cffi 0.8.6 is installed.
Because of this, at swift-object start-up the cffi 0.8.6 module
attempts to build the appropriate C file in the python xattr
directory and fails because it doesn't have write permission
on the python xattr directory.  The reason is that the Swift venv
is installed by root, whereas Swift is run as the ""swift"" user.

To fix this we've added a new module 76-swift-cffi which is run at
install time and executes a simple python command:

from xattr import getxattr, setxattr

in the Swift venv. This forces a build of the correct cffi library
in the xattr module.

Change-Id: Ied7cbd1db2bf5a0638324dcb8ac7135d03ed9aad
Closes-Bug: #1340776
",git fetch https://review.opendev.org/openstack/tripleo-image-elements refs/changes/81/119381/1 && git format-patch -1 --stdout FETCH_HEAD,['elements/swift/install.d/swift-source-install/76-swift-cffi'],1,07659ec9c065351a7e874109fd0e91859ceb5539,swift-cffi-fix,"#! /bin/bash set -eux # This is to avoid an issue with version mismatch between the version of cffi installed # and the version of cffi for which xattr has pre-built support. This mismatch will # prevent the object-server from running. See Openstack Bug #1340776 install_dir=${SWIFT_VENV_DIR:-'/opt/stack/venvs/swift'} set +u source $install_dir/bin/activate set -u python -c ""from xattr import getxattr, setxattr"" set +u deactivate set -u ",,19,0
openstack%2Fnova~stable%2Ficehouse~I15103b8edb3ff8934c835826ed00e08593cdeacc,openstack/nova,stable/icehouse,I15103b8edb3ff8934c835826ed00e08593cdeacc,Fixes a Hyper-V list_instances localization issue,MERGED,2014-09-18 14:01:01.000000000,2014-09-27 04:00:01.000000000,2014-09-27 03:59:58.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 3185}, {'_account_id': 5170}, {'_account_id': 8213}, {'_account_id': 8543}, {'_account_id': 9656}, {'_account_id': 12604}]","[{'number': 1, 'created': '2014-09-18 14:01:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ecb0e4b35ec2d681c4ec2617ef903a05ed548eaa', 'message': 'Fixes a Hyper-V list_instances localization issue\n\nThe Hyper-V WMI MSVM_ComputerSystem class Caption property can\nhave different values on various locales. This patch uses a\ndifferent query to avoid any localization issue.\n\nCo-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>\nCloses-Bug: #1296478\n(cherry picked from commit a5405fa3532d9dd3d33e848f36cc6530e74e6bb7)\n\nChange-Id: I15103b8edb3ff8934c835826ed00e08593cdeacc\n'}, {'number': 2, 'created': '2014-09-24 12:34:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8db0b4fb71f415c131cbf0942114ccb74f20efbe', 'message': 'Fixes a Hyper-V list_instances localization issue\n\nThe Hyper-V WMI MSVM_ComputerSystem class Caption property can\nhave different values on various locales. This patch uses a\ndifferent query to avoid any localization issue.\n\nCo-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>\nCloses-Bug: #1296478\n(cherry picked from commit a5405fa3532d9dd3d33e848f36cc6530e74e6bb7)\n\nChange-Id: I15103b8edb3ff8934c835826ed00e08593cdeacc\n'}, {'number': 3, 'created': '2014-09-26 09:21:54.000000000', 'files': ['nova/tests/virt/hyperv/test_vmutilsv2.py', 'nova/virt/hyperv/vmutilsv2.py', 'nova/virt/hyperv/vmutils.py', 'nova/tests/virt/hyperv/test_vmutils.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/4f41d37499d5e34f3a220ab1276280caa3fdb9bf', 'message': 'Fixes a Hyper-V list_instances localization issue\n\nThe Hyper-V WMI MSVM_ComputerSystem class Caption property can\nhave different values on various locales. This patch uses a\ndifferent query to avoid any localization issue.\n\nCo-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>\nCloses-Bug: #1296478\n(cherry picked from commit a5405fa3532d9dd3d33e848f36cc6530e74e6bb7)\n\nChange-Id: I15103b8edb3ff8934c835826ed00e08593cdeacc\n'}]",0,122424,4f41d37499d5e34f3a220ab1276280caa3fdb9bf,34,8,3,12604,,,0,"Fixes a Hyper-V list_instances localization issue

The Hyper-V WMI MSVM_ComputerSystem class Caption property can
have different values on various locales. This patch uses a
different query to avoid any localization issue.

Co-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>
Closes-Bug: #1296478
(cherry picked from commit a5405fa3532d9dd3d33e848f36cc6530e74e6bb7)

Change-Id: I15103b8edb3ff8934c835826ed00e08593cdeacc
",git fetch https://review.opendev.org/openstack/nova refs/changes/24/122424/3 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/hyperv/test_vmutilsv2.py', 'nova/virt/hyperv/vmutilsv2.py', 'nova/virt/hyperv/vmutils.py', 'nova/tests/virt/hyperv/test_vmutils.py']",4,ecb0e4b35ec2d681c4ec2617ef903a05ed548eaa,bug/1296478," def test_list_instances(self): vs = mock.MagicMock() attrs = {'ElementName': 'fake_name'} vs.configure_mock(**attrs) self._vmutils._conn.Msvm_VirtualSystemSettingData.return_value = [vs] response = self._vmutils.list_instances() self.assertEqual([(attrs['ElementName'])], response) self._vmutils._conn.Msvm_VirtualSystemSettingData.assert_called_with( ['ElementName'], SettingType=self._vmutils._VIRTUAL_SYSTEM_CURRENT_SETTINGS)",,35,4
openstack%2Fnova~stable%2Ficehouse~I9c58168c012b342bb5dfa0c62a7c39a327f442b3,openstack/nova,stable/icehouse,I9c58168c012b342bb5dfa0c62a7c39a327f442b3,Adds list_instance_uuids to the Hyper-V driver,MERGED,2014-09-10 11:57:10.000000000,2014-09-27 03:59:35.000000000,2014-09-27 03:59:33.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 3185}, {'_account_id': 5170}, {'_account_id': 8213}, {'_account_id': 9656}, {'_account_id': 11531}, {'_account_id': 12604}]","[{'number': 1, 'created': '2014-09-10 11:57:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/029e580df5954dd1b572baa6efac06f2e0580962', 'message': 'Adds list_instance_uuids to the Hyper-V driver\n\nIn case of large number of servers, the _destroy_evacuated_instances\nfails randomly. Implementing list_instance_uuids solves the issue.\n\nCo-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>\nCo-Authored-By: Ionut Balutoiu <ibalutoiu@cloudbasesolutions.com>\nCloses-bug: #1291364\n(cherry picked from commit 73da55e4ef626283ae58a97c7ad89854ec77daa3)\n\nConflicts:\n\tnova/tests/virt/hyperv/test_vmops.py\n\tnova/tests/virt/hyperv/test_vmutils.py\n\tnova/tests/virt/hyperv/test_vmutilsv2.py\n\tnova/virt/hyperv/vmutils.py\n\nChange-Id: I9c58168c012b342bb5dfa0c62a7c39a327f442b3\n'}, {'number': 2, 'created': '2014-09-18 14:01:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/0aa96adcd461801b1868dc600555ae4f862797c6', 'message': 'Adds list_instance_uuids to the Hyper-V driver\n\nIn case of large number of servers, the _destroy_evacuated_instances\nfails randomly. Implementing list_instance_uuids solves the issue.\n\nCo-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>\nCo-Authored-By: Ionut Balutoiu <ibalutoiu@cloudbasesolutions.com>\nCloses-bug: #1291364\n(cherry picked from commit 73da55e4ef626283ae58a97c7ad89854ec77daa3)\n\nConflicts:\n\tnova/tests/virt/hyperv/test_vmops.py\n\tnova/tests/virt/hyperv/test_vmutils.py\n\tnova/tests/virt/hyperv/test_vmutilsv2.py\n\tnova/virt/hyperv/vmutils.py\n\nChange-Id: I9c58168c012b342bb5dfa0c62a7c39a327f442b3\n'}, {'number': 3, 'created': '2014-09-24 09:10:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f5ecfdfed528c1982e27af2c5e61c2582ae437e0', 'message': 'Adds list_instance_uuids to the Hyper-V driver\n\nIn case of large number of servers, the _destroy_evacuated_instances\nfails randomly. Implementing list_instance_uuids solves the issue.\n\nCo-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>\nCo-Authored-By: Ionut Balutoiu <ibalutoiu@cloudbasesolutions.com>\nCloses-bug: #1291364\n(cherry picked from commit 73da55e4ef626283ae58a97c7ad89854ec77daa3)\n\nConflicts:\n\tnova/tests/virt/hyperv/test_vmops.py\n\tnova/tests/virt/hyperv/test_vmutils.py\n\tnova/tests/virt/hyperv/test_vmutilsv2.py\n\tnova/virt/hyperv/vmutils.py\n\nChange-Id: I9c58168c012b342bb5dfa0c62a7c39a327f442b3\n'}, {'number': 4, 'created': '2014-09-24 10:42:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/209ea2de5bf9836e068c373393242baedb24046e', 'message': 'Adds list_instance_uuids to the Hyper-V driver\n\nIn case of large number of servers, the _destroy_evacuated_instances\nfails randomly. Implementing list_instance_uuids solves the issue.\n\nCo-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>\nCo-Authored-By: Ionut Balutoiu <ibalutoiu@cloudbasesolutions.com>\nCloses-bug: #1291364\n(cherry picked from commit 73da55e4ef626283ae58a97c7ad89854ec77daa3)\n\nConflicts:\n\tnova/tests/virt/hyperv/test_vmops.py\n\tnova/tests/virt/hyperv/test_vmutils.py\n\tnova/tests/virt/hyperv/test_vmutilsv2.py\n\tnova/virt/hyperv/vmutils.py\n\nChange-Id: I9c58168c012b342bb5dfa0c62a7c39a327f442b3\n'}, {'number': 5, 'created': '2014-09-24 12:34:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/82e3add7d9712ef04cea911abff5dd3cb7ae15ec', 'message': 'Adds list_instance_uuids to the Hyper-V driver\n\nIn case of large number of servers, the _destroy_evacuated_instances\nfails randomly. Implementing list_instance_uuids solves the issue.\n\nCo-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>\nCo-Authored-By: Ionut Balutoiu <ibalutoiu@cloudbasesolutions.com>\nCloses-bug: #1291364\n(cherry picked from commit 73da55e4ef626283ae58a97c7ad89854ec77daa3)\n\nConflicts:\n\tnova/tests/virt/hyperv/test_vmops.py\n\tnova/tests/virt/hyperv/test_vmutils.py\n\tnova/tests/virt/hyperv/test_vmutilsv2.py\n\tnova/virt/hyperv/vmutils.py\n\nChange-Id: I9c58168c012b342bb5dfa0c62a7c39a327f442b3\n'}, {'number': 6, 'created': '2014-09-26 08:18:05.000000000', 'files': ['nova/virt/hyperv/driver.py', 'nova/tests/virt/hyperv/test_vmops.py', 'nova/virt/hyperv/vmops.py', 'nova/tests/virt/hyperv/test_hypervapi.py', 'nova/tests/virt/hyperv/test_vmutilsv2.py', 'nova/virt/hyperv/vmutilsv2.py', 'nova/virt/hyperv/vmutils.py', 'nova/tests/virt/hyperv/test_vmutils.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/9015410ec95fe7dc23fb3cb2be432915a9733cbd', 'message': 'Adds list_instance_uuids to the Hyper-V driver\n\nIn case of large number of servers, the _destroy_evacuated_instances\nfails randomly. Implementing list_instance_uuids solves the issue.\n\nCo-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>\nCo-Authored-By: Ionut Balutoiu <ibalutoiu@cloudbasesolutions.com>\nCloses-bug: #1291364\n(cherry picked from commit 73da55e4ef626283ae58a97c7ad89854ec77daa3)\n\nConflicts:\n\tnova/tests/virt/hyperv/test_vmops.py\n\tnova/tests/virt/hyperv/test_vmutils.py\n\tnova/tests/virt/hyperv/test_vmutilsv2.py\n\tnova/virt/hyperv/vmutils.py\n\nChange-Id: I9c58168c012b342bb5dfa0c62a7c39a327f442b3\n'}]",3,120390,9015410ec95fe7dc23fb3cb2be432915a9733cbd,36,8,6,12604,,,0,"Adds list_instance_uuids to the Hyper-V driver

In case of large number of servers, the _destroy_evacuated_instances
fails randomly. Implementing list_instance_uuids solves the issue.

Co-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>
Co-Authored-By: Ionut Balutoiu <ibalutoiu@cloudbasesolutions.com>
Closes-bug: #1291364
(cherry picked from commit 73da55e4ef626283ae58a97c7ad89854ec77daa3)

Conflicts:
	nova/tests/virt/hyperv/test_vmops.py
	nova/tests/virt/hyperv/test_vmutils.py
	nova/tests/virt/hyperv/test_vmutilsv2.py
	nova/virt/hyperv/vmutils.py

Change-Id: I9c58168c012b342bb5dfa0c62a7c39a327f442b3
",git fetch https://review.opendev.org/openstack/nova refs/changes/90/120390/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/hyperv/driver.py', 'nova/tests/virt/hyperv/test_vmops.py', 'nova/virt/hyperv/vmops.py', 'nova/tests/virt/hyperv/test_hypervapi.py', 'nova/tests/virt/hyperv/test_vmutilsv2.py', 'nova/virt/hyperv/vmutilsv2.py', 'nova/virt/hyperv/vmutils.py', 'nova/tests/virt/hyperv/test_vmutils.py']",8,029e580df5954dd1b572baa6efac06f2e0580962,bug/1291364," def test_list_instance_notes(self): vs = mock.MagicMock() attrs = {'ElementName': 'fake_name', 'Notes': '4f54fb69-d3a2-45b7-bb9b-b6e6b3d893b3'} vs.configure_mock(**attrs) self._vmutils._conn.Msvm_VirtualSystemSettingData.return_value = [vs] response = self._vmutils.list_instance_notes() self.assertEqual([(attrs['ElementName'], [attrs['Notes']])], response) self._vmutils._conn.Msvm_VirtualSystemSettingData.assert_called_with( ['ElementName', 'Notes'], SettingType=self._vmutils._VIRTUAL_SYSTEM_CURRENT_SETTINGS) @mock.patch('nova.virt.hyperv.vmutils.VMUtils.check_ret_val') def test_modify_virtual_system(self, mock_check_ret_val): mock_vs_man_svc = mock.MagicMock() mock_vmsetting = mock.MagicMock() fake_path = 'fake path' fake_job_path = 'fake job path' fake_ret_val = 'fake return value' mock_vs_man_svc.ModifyVirtualSystem.return_value = (0, fake_job_path, fake_ret_val) self._vmutils._modify_virtual_system(vs_man_svc=mock_vs_man_svc, vm_path=fake_path, vmsetting=mock_vmsetting) mock_vs_man_svc.ModifyVirtualSystem.assert_called_once_with( ComputerSystem=fake_path, SystemSettingData=mock_vmsetting.GetText_(1)) mock_check_ret_val.assert_called_once_with(fake_ret_val, fake_job_path) @mock.patch('nova.virt.hyperv.vmutils.VMUtils.check_ret_val') @mock.patch('nova.virt.hyperv.vmutils.VMUtils._get_wmi_obj') @mock.patch('nova.virt.hyperv.vmutils.VMUtils._modify_virtual_system') @mock.patch('nova.virt.hyperv.vmutils.VMUtils._get_vm_setting_data') def test_create_vm_obj(self, mock_get_vm_setting_data, mock_modify_virtual_system, mock_get_wmi_obj, mock_check_ret_val): mock_vs_man_svc = mock.MagicMock() mock_vs_gs_data = mock.MagicMock() fake_vm_path = 'fake vm path' fake_job_path = 'fake job path' fake_ret_val = 'fake return value' _conn = self._vmutils._conn.Msvm_VirtualSystemGlobalSettingData _conn.new.return_value = mock_vs_gs_data mock_vs_man_svc.DefineVirtualSystem.return_value = (fake_vm_path, fake_job_path, fake_ret_val) response = self._vmutils._create_vm_obj(vs_man_svc=mock_vs_man_svc, vm_name='fake vm', notes='fake notes') _conn.new.assert_called_once_with() self.assertEqual(mock_vs_gs_data.ElementName, 'fake vm') mock_vs_man_svc.DefineVirtualSystem.assert_called_once_with( [], None, mock_vs_gs_data.GetText_(1)) mock_check_ret_val.assert_called_once_with(fake_ret_val, fake_job_path) mock_get_wmi_obj.assert_called_with(fake_vm_path) mock_get_vm_setting_data.assert_called_once_with(mock_get_wmi_obj()) mock_modify_virtual_system.assert_called_once_with( mock_vs_man_svc, fake_vm_path, mock_get_vm_setting_data()) self.assertEqual(mock_get_vm_setting_data().Notes, '\n'.join('fake notes')) self.assertEqual(response, mock_get_wmi_obj())",,241,9
openstack%2Fhorizon~master~Ib4e4e18251f4f024a61dabb646cda79a7c88582d,openstack/horizon,master,Ib4e4e18251f4f024a61dabb646cda79a7c88582d,Allow setting config drive option when launching instance,MERGED,2014-09-09 13:02:37.000000000,2014-09-27 03:59:22.000000000,2014-09-27 03:59:21.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 2455}, {'_account_id': 9622}, {'_account_id': 9647}, {'_account_id': 10295}]","[{'number': 1, 'created': '2014-09-09 13:02:37.000000000', 'files': ['openstack_dashboard/dashboards/project/instances/workflows/create_instance.py', 'openstack_dashboard/dashboards/project/instances/tests.py', 'openstack_dashboard/api/nova.py', 'openstack_dashboard/dashboards/project/instances/templates/instances/_launch_advanced_help.html'], 'web_link': 'https://opendev.org/openstack/horizon/commit/69e6676588a7f972a0818bdc7233d75f8fd9b296', 'message': 'Allow setting config drive option when launching instance\n\nThis patch adds a ""Configuration Drive"" checkbox to the Advanced tab\nof the Launch Instance workflow, if the config drive extension is\nsupported. This allows setting the config drive option when launching\nan instance.\n\nChange-Id: Ib4e4e18251f4f024a61dabb646cda79a7c88582d\nCloses-Bug: #1366842\n'}]",0,120081,69e6676588a7f972a0818bdc7233d75f8fd9b296,11,6,1,9647,,,0,"Allow setting config drive option when launching instance

This patch adds a ""Configuration Drive"" checkbox to the Advanced tab
of the Launch Instance workflow, if the config drive extension is
supported. This allows setting the config drive option when launching
an instance.

Change-Id: Ib4e4e18251f4f024a61dabb646cda79a7c88582d
Closes-Bug: #1366842
",git fetch https://review.opendev.org/openstack/horizon refs/changes/81/120081/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/instances/workflows/create_instance.py', 'openstack_dashboard/dashboards/project/instances/tests.py', 'openstack_dashboard/api/nova.py', 'openstack_dashboard/dashboards/project/instances/templates/instances/_launch_advanced_help.html']",4,69e6676588a7f972a0818bdc7233d75f8fd9b296,bug/1366842,<p>{% blocktrans %}Specify advanced options to use when launching an instance.{% endblocktrans %}</p>,<p>{% blocktrans %}Automatic: Entire disk is single partition and automatically resizes.{% endblocktrans %}</p> <p>{% blocktrans %}Manual: Faster build times but requires manual partitioning.{% endblocktrans %}</p>,97,17
openstack%2Foslo.utils~master~I5404d559a2b0c51eed4e9c7ba68e36e12640163a,openstack/oslo.utils,master,I5404d559a2b0c51eed4e9c7ba68e36e12640163a,Imported Translations from Transifex,MERGED,2014-09-26 06:11:33.000000000,2014-09-27 03:59:18.000000000,2014-09-27 03:59:17.000000000,"[{'_account_id': 3}, {'_account_id': 1669}]","[{'number': 1, 'created': '2014-09-26 06:11:33.000000000', 'files': ['oslo.utils/locale/fr/LC_MESSAGES/oslo.utils-log-critical.po', 'oslo.utils/locale/fr/LC_MESSAGES/oslo.utils-log-info.po'], 'web_link': 'https://opendev.org/openstack/oslo.utils/commit/b5ab4d03f35e3721ac0167e1d1140fe203b903e5', 'message': 'Imported Translations from Transifex\n\nChange-Id: I5404d559a2b0c51eed4e9c7ba68e36e12640163a\n'}]",0,124294,b5ab4d03f35e3721ac0167e1d1140fe203b903e5,6,2,1,11131,,,0,"Imported Translations from Transifex

Change-Id: I5404d559a2b0c51eed4e9c7ba68e36e12640163a
",git fetch https://review.opendev.org/openstack/oslo.utils refs/changes/94/124294/1 && git format-patch -1 --stdout FETCH_HEAD,"['oslo.utils/locale/fr/LC_MESSAGES/oslo.utils-log-critical.po', 'oslo.utils/locale/fr/LC_MESSAGES/oslo.utils-log-info.po']",2,b5ab4d03f35e3721ac0167e1d1140fe203b903e5,transifex/translations,"# Translations template for heat. # Copyright (C) 2014 ORGANIZATION # This file is distributed under the same license as the heat project. # # Translators: # Maxime COQUEREL <max.coquerel@gmail.com>, 2014 msgid """" msgstr """" ""Project-Id-Version: oslo.utils\n"" ""Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"" ""POT-Creation-Date: 2014-09-26 06:11+0000\n"" ""PO-Revision-Date: 2014-09-25 08:49+0000\n"" ""Last-Translator: Maxime COQUEREL <max.coquerel@gmail.com>\n"" ""Language-Team: French (http://www.transifex.com/projects/p/osloutils/"" ""language/fr/)\n"" ""Language: fr\n"" ""MIME-Version: 1.0\n"" ""Content-Type: text/plain; charset=UTF-8\n"" ""Content-Transfer-Encoding: 8bit\n"" ""Generated-By: Babel 1.3\n"" ""Plural-Forms: nplurals=2; plural=(n > 1);\n"" ",,42,0
openstack%2Foslo.concurrency~master~I68d736457e762e388d149de8ce039fb639e2b5bc,openstack/oslo.concurrency,master,I68d736457e762e388d149de8ce039fb639e2b5bc,Imported Translations from Transifex,MERGED,2014-09-26 06:08:33.000000000,2014-09-27 03:59:16.000000000,2014-09-27 03:59:15.000000000,"[{'_account_id': 3}, {'_account_id': 1669}]","[{'number': 1, 'created': '2014-09-26 06:08:33.000000000', 'files': ['oslo.concurrency/locale/fr/LC_MESSAGES/oslo.concurrency-log-critical.po', 'oslo.concurrency/locale/fr/LC_MESSAGES/oslo.concurrency-log-warning.po'], 'web_link': 'https://opendev.org/openstack/oslo.concurrency/commit/12bd40f91ddfc98ac9e8c13ee8beb6e0a50e0042', 'message': 'Imported Translations from Transifex\n\nChange-Id: I68d736457e762e388d149de8ce039fb639e2b5bc\n'}]",0,124288,12bd40f91ddfc98ac9e8c13ee8beb6e0a50e0042,6,2,1,11131,,,0,"Imported Translations from Transifex

Change-Id: I68d736457e762e388d149de8ce039fb639e2b5bc
",git fetch https://review.opendev.org/openstack/oslo.concurrency refs/changes/88/124288/1 && git format-patch -1 --stdout FETCH_HEAD,"['oslo.concurrency/locale/fr/LC_MESSAGES/oslo.concurrency-log-critical.po', 'oslo.concurrency/locale/fr/LC_MESSAGES/oslo.concurrency-log-warning.po']",2,12bd40f91ddfc98ac9e8c13ee8beb6e0a50e0042,transifex/translations,"# Translations template for heat. # Copyright (C) 2014 ORGANIZATION # This file is distributed under the same license as the heat project. # # Translators: # Maxime COQUEREL <max.coquerel@gmail.com>, 2014 msgid """" msgstr """" ""Project-Id-Version: oslo.concurrency\n"" ""Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"" ""POT-Creation-Date: 2014-09-26 06:08+0000\n"" ""PO-Revision-Date: 2014-09-25 08:53+0000\n"" ""Last-Translator: Maxime COQUEREL <max.coquerel@gmail.com>\n"" ""Language-Team: French (http://www.transifex.com/projects/p/osloconcurrency/"" ""language/fr/)\n"" ""Language: fr\n"" ""MIME-Version: 1.0\n"" ""Content-Type: text/plain; charset=UTF-8\n"" ""Content-Transfer-Encoding: 8bit\n"" ""Generated-By: Babel 1.3\n"" ""Plural-Forms: nplurals=2; plural=(n > 1);\n"" ",,42,0
openstack%2Ftrove~master~I240d81afc3e43fd3711de8c156cfb43fd14850bf,openstack/trove,master,I240d81afc3e43fd3711de8c156cfb43fd14850bf,Fixed database migration script issues,MERGED,2014-08-12 16:41:04.000000000,2014-09-27 03:52:00.000000000,2014-09-27 03:51:59.000000000,"[{'_account_id': 3}, {'_account_id': 4463}, {'_account_id': 5293}, {'_account_id': 6268}, {'_account_id': 7092}, {'_account_id': 8415}, {'_account_id': 9664}, {'_account_id': 9683}, {'_account_id': 9746}, {'_account_id': 9782}, {'_account_id': 10266}]","[{'number': 1, 'created': '2014-08-12 16:41:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/d5db9990fb0ef007cf4fe65c8ed667c63f7267fc', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade. Some upgrade scripts are also not catching and\ndealing with exceptions thrown by migrations on PostgreSQL, causing\nerrors.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 2, 'created': '2014-08-12 20:18:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/6dec7c4f8ab182247dc41f486c9d7dd7cbd6adae', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade. Some upgrade scripts are also not catching and\ndealing with exceptions thrown by migrations on PostgreSQL, causing\nerrors.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 3, 'created': '2014-08-14 22:25:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/719a0f8e12264c49d20ede4efc604597ba87cf1b', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade. Some upgrade scripts are also not catching and\ndealing with exceptions thrown by migrations on PostgreSQL, causing\nerrors.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 4, 'created': '2014-08-15 18:48:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/2bba857a7b56bb30f81a5d80d02717be256e4fb2', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade. Some upgrade scripts are also not catching and\ndealing with exceptions thrown by migrations on PostgreSQL, causing\nerrors.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 5, 'created': '2014-08-15 18:59:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/50bd989df6afb747ce48b8ecec2fb11cea8163bd', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade. Some upgrade scripts are also not catching and\ndealing with exceptions thrown by migrations on PostgreSQL, causing\nerrors.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 6, 'created': '2014-08-21 14:58:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/e9913d61bdc5c8fdd77f6d5a50c149f5f87693df', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade. Some upgrade scripts are also not catching and\ndealing with exceptions thrown by migrations on PostgreSQL, causing\nerrors.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 7, 'created': '2014-08-21 15:12:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/74f243e9232824bbd48e8f4b040992d3cd7ebbe1', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade. Some upgrade scripts are also not catching and\ndealing with exceptions thrown by migrations on PostgreSQL, causing\nerrors.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 8, 'created': '2014-08-26 16:55:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/c90912795b69d72b3e360e3ddcdc6c3b928d21ed', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade. Some upgrade scripts are also not catching and\ndealing with exceptions thrown by migrations on PostgreSQL, causing\nerrors.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 9, 'created': '2014-08-26 17:00:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/13145182e3d7014e84b2eec6b0bf2214ed8ef62c', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 10, 'created': '2014-08-26 17:04:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/b70dad4cd4d86291dc8d8dee43118bf827af0242', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 11, 'created': '2014-08-27 18:34:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/0d595e3fa7d878c051db35f729b2b6030273fdae', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 12, 'created': '2014-09-02 14:44:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/7d14410d38aa8431320fa7626b71e4cb4e3ba9ee', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 13, 'created': '2014-09-02 18:12:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/d34098d25eec4b26b3c803f0f35324a99578f02a', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 14, 'created': '2014-09-16 16:44:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/5583beebfdd62c2402e99c04b3347c724df072bb', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 15, 'created': '2014-09-16 22:11:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/6c93eeee9663a80d1d9f8dfd10de1cbe73e62418', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}, {'number': 16, 'created': '2014-09-19 21:06:14.000000000', 'files': ['trove/db/sqlalchemy/migrate_repo/versions/032_clusters.py', 'trove/db/sqlalchemy/utils.py', 'trove/db/sqlalchemy/migrate_repo/versions/020_configurations.py', 'trove/tests/db/migrations.py', 'trove/db/sqlalchemy/migrate_repo/versions/027_add_datastore_capabilities.py', 'trove/tests/unittests/db/test_migration_utils.py', 'run_tests.py', 'trove/db/sqlalchemy/migrate_repo/versions/030_add_master_slave.py', 'trove/tests/db/__init__.py', 'trove/db/sqlalchemy/migrate_repo/versions/016_add_datastore_type.py', 'trove/db/sqlalchemy/migrate_repo/versions/029_add_backup_datastore.py', 'trove/tests/unittests/db/__init__.py'], 'web_link': 'https://opendev.org/openstack/trove/commit/06e0aa25a066cb889fb8cb72708dd504cf84a4bd', 'message': 'Fixed database migration script issues\n\nThe db downgrade scripts are currently not dropping foreign key\nconstraints, causing errors when the script tries to drop related\ntables on downgrade.\n\nThis commit address issues in the migration scripts, and also\nintroduces a new test script to test the migration scripts, so that\nissues can be prevented in the future. The new test script is based\non the existing migration test script implementated in Nova.\n\nChange-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf\nCloses-Bug: #1347114\n'}]",66,113582,06e0aa25a066cb889fb8cb72708dd504cf84a4bd,118,11,16,10266,,,0,"Fixed database migration script issues

The db downgrade scripts are currently not dropping foreign key
constraints, causing errors when the script tries to drop related
tables on downgrade.

This commit address issues in the migration scripts, and also
introduces a new test script to test the migration scripts, so that
issues can be prevented in the future. The new test script is based
on the existing migration test script implementated in Nova.

Change-Id: I240d81afc3e43fd3711de8c156cfb43fd14850bf
Closes-Bug: #1347114
",git fetch https://review.opendev.org/openstack/trove refs/changes/82/113582/2 && git format-patch -1 --stdout FETCH_HEAD,"['trove/db/sqlalchemy/utils.py', 'trove/db/sqlalchemy/migrate_repo/versions/020_configurations.py', 'trove/tests/db/migrations.py', 'trove/db/sqlalchemy/migrate_repo/versions/027_add_datastore_capabilities.py', 'trove/db/sqlalchemy/migrate_repo/versions/029_add_backup_datastore.py', 'trove/db/sqlalchemy/migrate_repo/versions/026_datastore_versions_unique_fix.py', 'run_tests.py', 'trove/db/sqlalchemy/migrate_repo/versions/030_add_master_slave.py', 'trove/tests/db/__init__.py', 'trove/db/sqlalchemy/migrate_repo/versions/016_add_datastore_type.py']",10,d5db9990fb0ef007cf4fe65c8ed667c63f7267fc,bug/1347114,"from migrate.changeset.constraint import ForeignKeyConstraint from trove.db.sqlalchemy import utils as db_utils params = {'columns': [instances.c.datastore_version_id], 'refcolumns': [datastore_versions.c.id]} if migrate_engine.name == 'mysql': params['name'] = db_utils.get_foreign_key_constraint_name( engine=migrate_engine, table='instances', columns=['datastore_version_id'], ref_table='datastore_versions', ref_columns=['id']) fkey_constraint = ForeignKeyConstraint(**params) fkey_constraint.drop() drop_tables([datastore_versions, datastores])"," drop_tables([datastores, datastore_versions])",298,8
openstack%2Fnova~stable%2Ficehouse~I91510a2b864f0c1b73cfae18f271e94334714dce,openstack/nova,stable/icehouse,I91510a2b864f0c1b73cfae18f271e94334714dce,Add _wrap_db_error() support to Session.commit(),MERGED,2014-09-25 16:54:24.000000000,2014-09-27 03:51:39.000000000,2014-09-27 03:51:37.000000000,"[{'_account_id': 3}, {'_account_id': 1561}, {'_account_id': 1812}, {'_account_id': 5170}, {'_account_id': 5511}]","[{'number': 1, 'created': '2014-09-25 16:54:24.000000000', 'files': ['nova/openstack/common/db/sqlalchemy/session.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/3371ad81ba7f2e8b1a9391dae3f0844d3dba916f', 'message': ""Add _wrap_db_error() support to Session.commit()\n\nThis patch adds _wrap_db_error() to session.commit(),\nwhich has been observed to be a common point of failure for\ndeadlock exceptions.   In order to achieve this, the\n_wrap_db_error() decorator itself also needed to propagate an\nexisting DBError, as it is the case that SQLAlchemy's\nsession.commit() calls into the session.flush() method.\nTests are added to exercise both the nesting of _wrap_db_error()\nwhen a flush() inside commit() raises an exception, as well\nas when commit() alone raises an exception that the error\nis wrapped as expected.\n\nTests are omitted here as we are relying upon the tests\nthat were added to the corresponding oslo-incubator code.\n\nCloses-bug: #1370191\nChange-Id: I91510a2b864f0c1b73cfae18f271e94334714dce\n""}]",0,124112,3371ad81ba7f2e8b1a9391dae3f0844d3dba916f,11,5,1,11816,,,0,"Add _wrap_db_error() support to Session.commit()

This patch adds _wrap_db_error() to session.commit(),
which has been observed to be a common point of failure for
deadlock exceptions.   In order to achieve this, the
_wrap_db_error() decorator itself also needed to propagate an
existing DBError, as it is the case that SQLAlchemy's
session.commit() calls into the session.flush() method.
Tests are added to exercise both the nesting of _wrap_db_error()
when a flush() inside commit() raises an exception, as well
as when commit() alone raises an exception that the error
is wrapped as expected.

Tests are omitted here as we are relying upon the tests
that were added to the corresponding oslo-incubator code.

Closes-bug: #1370191
Change-Id: I91510a2b864f0c1b73cfae18f271e94334714dce
",git fetch https://review.opendev.org/openstack/nova refs/changes/12/124112/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/openstack/common/db/sqlalchemy/session.py'],1,3371ad81ba7f2e8b1a9391dae3f0844d3dba916f,bug/1370191," except exception.DBError: # note(zzzeek) - if _wrap_db_error is applied to nested functions, # ensure an existing DBError is propagated outwards raise @_wrap_db_error def commit(self, *args, **kwargs): return super(Session, self).commit(*args, **kwargs) ",,8,0
openstack%2Fkeystone~stable%2Ficehouse~Ie452c95dab4fcd04f65f6378c735490ee8cf4c70,openstack/keystone,stable/icehouse,Ie452c95dab4fcd04f65f6378c735490ee8cf4c70,Fix typo on cache backend module,MERGED,2014-08-26 21:56:38.000000000,2014-09-27 03:48:20.000000000,2014-09-27 03:48:19.000000000,"[{'_account_id': 3}, {'_account_id': 91}, {'_account_id': 1420}, {'_account_id': 2903}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 8831}, {'_account_id': 9656}, {'_account_id': 13300}, {'_account_id': 13308}]","[{'number': 1, 'created': '2014-08-26 21:56:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/0f3abfd2893dda2bdd61ec12642db349167f7172', 'message': ""Fix typo on cache backend module\n\nThe correct module is 'dogpile.cache.memcached' and not\n'dogpile.cache.memcache'.\n\nChange-Id: Ie452c95dab4fcd04f65f6378c735490ee8cf4c70\n(cherry picked from commit b8bbab790227445d90e4e9201127413bc85c1649)\n""}, {'number': 2, 'created': '2014-08-26 22:01:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/c20f38fd635a96cfb539b46bc3aa3d5fc0d5f06e', 'message': ""Fix typo on cache backend module\n\nThe correct module is 'dogpile.cache.memcached' and not\n'dogpile.cache.memcache'.\n\nIt's important to backport this to Icehouse as all Juno docs use the icehouse sample at this point. For example: http://docs.openstack.org/trunk/config-reference/content/section_keystone.conf.html\n\nChange-Id: Ie452c95dab4fcd04f65f6378c735490ee8cf4c70\n(cherry picked from commit b8bbab790227445d90e4e9201127413bc85c1649)\n""}, {'number': 3, 'created': '2014-08-26 22:01:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/8209885b82c704a76bcc6618b11d7e229b8dbed3', 'message': ""Fix typo on cache backend module\n\nThe correct module is 'dogpile.cache.memcached' and not\n'dogpile.cache.memcache'.\n\nIt's important to backport this to Icehouse as all Juno \ndocs use the icehouse sample at this point. For example:\nhttp://docs.openstack.org/trunk/config-reference/content/section_keystone.conf.html\n\nChange-Id: Ie452c95dab4fcd04f65f6378c735490ee8cf4c70\n(cherry picked from commit b8bbab790227445d90e4e9201127413bc85c1649)\n""}, {'number': 4, 'created': '2014-09-25 11:54:03.000000000', 'files': ['etc/keystone.conf.sample', 'keystone/common/config.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/6bf25d0759e24a863aad932c5633c30ac6842225', 'message': ""Fix typo on cache backend module\n\nThe correct module is 'dogpile.cache.memcached' and not\n'dogpile.cache.memcache'.\n\nIt's important to backport this to Icehouse as all Juno\ndocs use the icehouse sample at this point. For example:\nhttp://docs.openstack.org/trunk/config-reference/content/section_keystone.conf.html\n\nChange-Id: Ie452c95dab4fcd04f65f6378c735490ee8cf4c70\n(cherry picked from commit b8bbab790227445d90e4e9201127413bc85c1649)\n""}]",0,117035,6bf25d0759e24a863aad932c5633c30ac6842225,26,10,4,8831,,,0,"Fix typo on cache backend module

The correct module is 'dogpile.cache.memcached' and not
'dogpile.cache.memcache'.

It's important to backport this to Icehouse as all Juno
docs use the icehouse sample at this point. For example:
http://docs.openstack.org/trunk/config-reference/content/section_keystone.conf.html

Change-Id: Ie452c95dab4fcd04f65f6378c735490ee8cf4c70
(cherry picked from commit b8bbab790227445d90e4e9201127413bc85c1649)
",git fetch https://review.opendev.org/openstack/keystone refs/changes/35/117035/1 && git format-patch -1 --stdout FETCH_HEAD,"['etc/keystone.conf.sample', 'keystone/common/config.py']",2,0f3abfd2893dda2bdd61ec12642db349167f7172,, 'that Memcache (dogpile.cache.memcached) or Redis ', 'that Memcache (dogpile.cache.memcache) or Redis ',2,2
openstack%2Fneutron~master~I6ef040dae955cf92c64a5743197e66141f30e617,openstack/neutron,master,I6ef040dae955cf92c64a5743197e66141f30e617,Move DB_MAX_ENTRIES to constants,ABANDONED,2014-09-18 16:08:37.000000000,2014-09-27 03:46:08.000000000,,"[{'_account_id': 704}, {'_account_id': 4187}, {'_account_id': 6524}, {'_account_id': 6854}, {'_account_id': 7448}, {'_account_id': 9681}, {'_account_id': 9695}, {'_account_id': 9732}, {'_account_id': 9845}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10386}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-18 16:08:37.000000000', 'files': ['neutron/common/constants.py', 'neutron/plugins/ml2/drivers/helpers.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/7950c4aeacb01dba0c0c07ed9f8dda1674e769ef', 'message': 'Move DB_MAX_ENTRIES to constants\n\nChange-Id: I6ef040dae955cf92c64a5743197e66141f30e617\n'}]",2,122456,7950c4aeacb01dba0c0c07ed9f8dda1674e769ef,23,19,1,6788,,,0,"Move DB_MAX_ENTRIES to constants

Change-Id: I6ef040dae955cf92c64a5743197e66141f30e617
",git fetch https://review.opendev.org/openstack/neutron refs/changes/56/122456/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/common/constants.py', 'neutron/plugins/ml2/drivers/helpers.py']",2,7950c4aeacb01dba0c0c07ed9f8dda1674e769ef,opt-locking-PS17-1,"from neutron.common import constants for attempt in range(1, constants.DB_MAX_RETRIES + 1): {""type"": network_type, ""number"": constants.DB_MAX_RETRIES})","# Number of retries to find a valid segment candidate and allocate it DB_MAX_RETRIES = 10 for attempt in range(1, DB_MAX_RETRIES + 1): {""type"": network_type, ""number"": DB_MAX_RETRIES})",6,6
openstack%2Ftempest~master~Ic1f7f324c04fc2d2529c7065b0dcda1640d4fb53,openstack/tempest,master,Ic1f7f324c04fc2d2529c7065b0dcda1640d4fb53,reenable full request/response logging at DEBUG,MERGED,2014-09-25 14:29:08.000000000,2014-09-27 03:40:56.000000000,2014-09-27 03:40:55.000000000,"[{'_account_id': 3}, {'_account_id': 6167}, {'_account_id': 7872}]","[{'number': 1, 'created': '2014-09-25 14:29:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/407744635d3a8079bfa542303dddd6d081fd8948', 'message': ""reenable full request/response logging at DEBUG\n\nFor the difficult bugs we are often missing information if we don't\nhave the full request / response traffic. Provide a new\n_log_request_full which functions at a debug level and provides\nthis. Pretty print the bodies for resp and req for readability.\n\nChange-Id: Ic1f7f324c04fc2d2529c7065b0dcda1640d4fb53\n""}, {'number': 2, 'created': '2014-09-25 16:44:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/16b87cca7c3c896d22b622d88886d54380667c55', 'message': ""reenable full request/response logging at DEBUG\n\nFor the difficult bugs we are often missing information if we don't\nhave the full request / response traffic. Provide a new\n_log_request_full which functions at a debug level and provides\nthis. Pretty print the bodies for resp and req for readability.\n\nChange-Id: Ic1f7f324c04fc2d2529c7065b0dcda1640d4fb53\n""}, {'number': 3, 'created': '2014-09-25 20:29:52.000000000', 'files': ['tempest/common/rest_client.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/4f8d702d5cd62d3a10619e9b301eac38d5b86130', 'message': ""reenable full request/response logging at DEBUG\n\nFor the difficult bugs we are often missing information if we don't\nhave the full request / response traffic. Provide a new\n_log_request_full which functions at a debug level and provides\nthis. Pretty print the bodies for resp and req for readability.\n\nChange-Id: Ic1f7f324c04fc2d2529c7065b0dcda1640d4fb53\n""}]",0,124069,4f8d702d5cd62d3a10619e9b301eac38d5b86130,10,3,3,2750,,,0,"reenable full request/response logging at DEBUG

For the difficult bugs we are often missing information if we don't
have the full request / response traffic. Provide a new
_log_request_full which functions at a debug level and provides
this. Pretty print the bodies for resp and req for readability.

Change-Id: Ic1f7f324c04fc2d2529c7065b0dcda1640d4fb53
",git fetch https://review.opendev.org/openstack/tempest refs/changes/69/124069/3 && git format-patch -1 --stdout FETCH_HEAD,['tempest/common/rest_client.py'],1,407744635d3a8079bfa542303dddd6d081fd8948,client_trace,"import six# convert a structure into a string safely def safe_body(body, maxlen=2048): text = six.text_type(body) if len(text) > maxlen: return text[:maxlen] try: return json.dumps(body, sort_keys=True, indent=4, separators=(',', ': ')) except TypeError: return text def _log_request_full(self, method, req_url, resp, secs="""", req_headers=None, req_body=None, resp_body=None, caller_name=None, extra=None): if 'X-Auth-Token' in req_headers: req_headers['X-Auth-Token'] = '<omitted>' log_fmt = """"""Request (%s): %s %s %s%s Request - Headers: %s Body: %s Response - Headers: %s Body: %s"""""" self.LOG.debug( log_fmt % ( caller_name, resp['status'], method, req_url, secs, str(req_headers), safe_body(req_body), str(resp), safe_body(resp_body)), extra=extra) # Also look everything at DEBUG if you want to filter this # out, don't run at debug. self._log_full_request(method, req_url, resp, secs, req_headers, req_body, resp_body, caller_name, extra)","import string # We intentionally duplicate the info content because in a parallel # world this is important to match trace_regex = CONF.debug.trace_requests if trace_regex and re.search(trace_regex, caller_name): if 'X-Auth-Token' in req_headers: req_headers['X-Auth-Token'] = '<omitted>' log_fmt = """"""Request (%s): %s %s %s%s Request - Headers: %s Body: %s Response - Headers: %s Body: %s"""""" self.LOG.debug( log_fmt % ( caller_name, resp['status'], method, req_url, secs, str(req_headers), filter(lambda x: x in string.printable, str(req_body)[:2048]), str(resp), filter(lambda x: x in string.printable, str(resp_body)[:2048])), extra=extra)",42,27
openstack%2Fmurano-dashboard~master~I24990a87082fbe3587e02da44ee61f0554e68045,openstack/murano-dashboard,master,I24990a87082fbe3587e02da44ee61f0554e68045,Remove include of non-existing stylesheet,MERGED,2014-09-05 13:25:33.000000000,2014-09-27 03:25:41.000000000,2014-09-27 03:25:40.000000000,"[{'_account_id': 3}, {'_account_id': 7225}, {'_account_id': 7226}, {'_account_id': 7549}, {'_account_id': 7600}, {'_account_id': 7821}, {'_account_id': 11098}, {'_account_id': 13149}]","[{'number': 1, 'created': '2014-09-05 13:25:33.000000000', 'files': ['muranodashboard/templates/services/_page_header.html'], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/b978f1e0afc57088719b040c44e1e1689ea2e4ac', 'message': 'Remove include of non-existing stylesheet\n\nChange-Id: I24990a87082fbe3587e02da44ee61f0554e68045\nCloses-Bug: #1365670\n'}]",0,119372,b978f1e0afc57088719b040c44e1e1689ea2e4ac,13,8,1,7225,,,0,"Remove include of non-existing stylesheet

Change-Id: I24990a87082fbe3587e02da44ee61f0554e68045
Closes-Bug: #1365670
",git fetch https://review.opendev.org/openstack/murano-dashboard refs/changes/72/119372/1 && git format-patch -1 --stdout FETCH_HEAD,['muranodashboard/templates/services/_page_header.html'],1,b978f1e0afc57088719b040c44e1e1689ea2e4ac,bug/1365670,," <link rel=""stylesheet"" type=""text/css"" href=""{% static 'muranodashboard/css/topology.css' %}"" />",0,1
openstack%2Ftempest~master~I380a7f057a6b6df94cc029f65f357de7368dc2f7,openstack/tempest,master,I380a7f057a6b6df94cc029f65f357de7368dc2f7,Lock test_resize_server_using_overlimit_* tests on 'compute_quotas',MERGED,2014-09-23 16:48:25.000000000,2014-09-27 03:24:42.000000000,2014-09-27 00:41:19.000000000,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 5196}, {'_account_id': 6167}, {'_account_id': 6873}]","[{'number': 1, 'created': '2014-09-23 16:48:25.000000000', 'files': ['tempest/api/compute/v3/admin/test_servers_negative.py', 'tempest/api/compute/admin/test_servers_negative.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/ce6b14eda3bf55d35741deced72c8dfe95229e7f', 'message': ""Lock test_resize_server_using_overlimit_* tests on 'compute_quotas'\n\nThe test_resize_server_using_overlimit_* negative tests are getting the\ndefault quotas and creating a flavor that bumps one of the quota values\njust over the limit, expecting an OverLimit failure when trying to\nresize an instance to that temporary new flavor.\n\nThis can race with the os-quota-class-set admin test which changes the\ndefault quotas, so lock the server negative tests using the same lock\nfixture used in the quota class set tests.\n\nCloses-Bug: #1372093\n\nChange-Id: I380a7f057a6b6df94cc029f65f357de7368dc2f7\n""}]",0,123510,ce6b14eda3bf55d35741deced72c8dfe95229e7f,14,5,1,6873,,,0,"Lock test_resize_server_using_overlimit_* tests on 'compute_quotas'

The test_resize_server_using_overlimit_* negative tests are getting the
default quotas and creating a flavor that bumps one of the quota values
just over the limit, expecting an OverLimit failure when trying to
resize an instance to that temporary new flavor.

This can race with the os-quota-class-set admin test which changes the
default quotas, so lock the server negative tests using the same lock
fixture used in the quota class set tests.

Closes-Bug: #1372093

Change-Id: I380a7f057a6b6df94cc029f65f357de7368dc2f7
",git fetch https://review.opendev.org/openstack/tempest refs/changes/10/123510/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/compute/v3/admin/test_servers_negative.py', 'tempest/api/compute/admin/test_servers_negative.py']",2,ce6b14eda3bf55d35741deced72c8dfe95229e7f,bug/1372093,from tempest.common import tempest_fixtures as fixtures # NOTE(mriedem): Avoid conflicts with os-quota-class-sets tests. self.useFixture(fixtures.LockFixture('compute_quotas')) # NOTE(mriedem): Avoid conflicts with os-quota-class-sets tests. self.useFixture(fixtures.LockFixture('compute_quotas')),,10,0
openstack%2Fzaqar~master~I2b202ac018435b7c671490f9f036e700685e85f3,openstack/zaqar,master,I2b202ac018435b7c671490f9f036e700685e85f3,Added checks enforcements rule to the developers manual,MERGED,2014-09-14 22:02:59.000000000,2014-09-27 03:12:05.000000000,2014-09-27 03:12:05.000000000,"[{'_account_id': 3}, {'_account_id': 6159}, {'_account_id': 6413}, {'_account_id': 6484}, {'_account_id': 6944}]","[{'number': 1, 'created': '2014-09-14 22:02:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zaqar/commit/a6fd9c2d9f00856ed5fa230c7b3c4cce8bd94216', 'message': 'Added checks enforcements rule to the developers manual\n\nThis change adds the convention we established for checks\nduring the weekly meeting on 01-09-2014 to the developers docs.\n\nChange-Id: I2b202ac018435b7c671490f9f036e700685e85f3\n'}, {'number': 2, 'created': '2014-09-17 21:28:46.000000000', 'files': ['doc/source/index.rst'], 'web_link': 'https://opendev.org/openstack/zaqar/commit/dd553f6b2fcbc60bbec92d8fbdcaf6666ed68abe', 'message': 'Added checks enforcements rule to the developers manual\n\nThis change adds the convention we established for checks\nduring the weekly meeting on 01-09-2014 to the developers docs.\n\nChange-Id: I2b202ac018435b7c671490f9f036e700685e85f3\n'}]",16,121424,dd553f6b2fcbc60bbec92d8fbdcaf6666ed68abe,17,5,2,6413,,,0,"Added checks enforcements rule to the developers manual

This change adds the convention we established for checks
during the weekly meeting on 01-09-2014 to the developers docs.

Change-Id: I2b202ac018435b7c671490f9f036e700685e85f3
",git fetch https://review.opendev.org/openstack/zaqar refs/changes/24/121424/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/index.rst'],1,a6fd9c2d9f00856ed5fa230c7b3c4cce8bd94216,doc-rock-day,"Zaqar is composed mainly by two layers: The **transport drivers** are responsible of interacting with Zaqar clients. Every query done by the clients is processed by the transport layer, which is in charge of passing this information to the backend and then returning the operation response in a format understandable by the client. The **storage drivers** are responsible of interacting with the storage backends and, that way, store or retrieve the data coming from the transport layer. In order to keep this layers well defined, we have established that **checks should stay in the data plane they belong too**. In other words, transport drivers must guarantee that the incoming data is well-formed and storage drivers must enforce their data model stays consistent. ",,15,0
openstack%2Fhorizon~master~I471e0c1d7b15835ef16cf11c72401335b3a834e4,openstack/horizon,master,I471e0c1d7b15835ef16cf11c72401335b3a834e4,Updated from global requirements,MERGED,2014-09-25 04:06:14.000000000,2014-09-27 02:37:27.000000000,2014-09-27 02:37:26.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 2455}, {'_account_id': 6914}, {'_account_id': 9576}, {'_account_id': 9981}]","[{'number': 1, 'created': '2014-09-25 04:06:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/29ca91133944f166941f05312974afe550418ceb', 'message': 'Updated from global requirements\n\nChange-Id: I471e0c1d7b15835ef16cf11c72401335b3a834e4\n'}, {'number': 2, 'created': '2014-09-25 19:02:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/9a769d292b9768010fe47781426caaa24142846f', 'message': 'Updated from global requirements\n\nChange-Id: I471e0c1d7b15835ef16cf11c72401335b3a834e4\n'}, {'number': 3, 'created': '2014-09-26 03:53:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/f9e7fc3624c380fa51753cae9b125543fc8effcf', 'message': 'Updated from global requirements\n\nChange-Id: I471e0c1d7b15835ef16cf11c72401335b3a834e4\n'}, {'number': 4, 'created': '2014-09-26 04:02:09.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/horizon/commit/e0fdcc09bce82021979d02954fe8df77a91fd0c3', 'message': 'Updated from global requirements\n\nChange-Id: I471e0c1d7b15835ef16cf11c72401335b3a834e4\n'}]",0,123929,e0fdcc09bce82021979d02954fe8df77a91fd0c3,19,6,4,11131,,,0,"Updated from global requirements

Change-Id: I471e0c1d7b15835ef16cf11c72401335b3a834e4
",git fetch https://review.opendev.org/openstack/horizon refs/changes/29/123929/4 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,29ca91133944f166941f05312974afe550418ceb,openstack/requirements,django-pyscss>=1.0.3 # BSD License (2 clause),django-pyscss>=1.0.2 # BSD License (2 clause),1,1
openstack%2Fironic~master~I607a1f810d948659ee7332f99d02c8288aecda2d,openstack/ironic,master,I607a1f810d948659ee7332f99d02c8288aecda2d,WIP: Add conductor_id index to nodes,ABANDONED,2014-07-25 19:07:08.000000000,2014-09-27 02:24:47.000000000,,"[{'_account_id': 3}, {'_account_id': 2889}, {'_account_id': 3099}, {'_account_id': 6773}, {'_account_id': 7711}, {'_account_id': 7882}, {'_account_id': 10035}, {'_account_id': 12081}]","[{'number': 1, 'created': '2014-07-25 19:07:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/491db129480733a930077e6b2f9a7f356ffa4a15', 'message': 'WIP: Add conductor_hostname property to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}, {'number': 2, 'created': '2014-07-29 23:32:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6092d61a1e1de5e78716ed3d2e2cc0e94173f4d6', 'message': 'WIP: Add conductor_hostname property to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}, {'number': 3, 'created': '2014-08-20 04:56:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7f5593fed14eebe3f6f7dde2053fba7fd1b05bfa', 'message': 'WIP: Add conductor_hostname property to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}, {'number': 4, 'created': '2014-08-20 23:33:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9070325ca8989ff717afc7efa45fae16768e72ca', 'message': 'WIP: Add conductor_hostname property to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}, {'number': 5, 'created': '2014-08-21 00:41:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/803fe2d46556b7182cb94e7cf008533700328292', 'message': 'WIP: Add conductor_hostname property to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nPartial-Bug: #1355510\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}, {'number': 6, 'created': '2014-08-21 02:02:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9d28e208ca5877fee7cadda7188c12256eedab1b', 'message': 'WIP: Add conductor_hostname property to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nPartial-Bug: #1355510\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}, {'number': 7, 'created': '2014-08-22 04:43:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cc14d33e11f2e1a91d59571b59e3a46de04745fc', 'message': 'Add conductor_hostname property to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nPartial-Bug: #1355510\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}, {'number': 8, 'created': '2014-08-25 04:27:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d78c7000f244b54166526d4f91fce760d3f1e339', 'message': 'Add conductor_hostname property to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nPartial-Bug: #1355510\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}, {'number': 9, 'created': '2014-09-02 18:09:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/88a171e3af666f5a9094f1f908824b839e7578a3', 'message': 'Add conductor_hostname property to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nPartial-Bug: #1355510\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}, {'number': 10, 'created': '2014-09-02 21:09:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f7aeea2d10b4567de82c0a2f4302da2a47a5c882', 'message': 'Add conductor_hostname property to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nPartial-Bug: #1355510\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}, {'number': 11, 'created': '2014-09-09 23:56:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a875684d0fe881f5cc0577db86fcae8157ed29e6', 'message': 'Add conductor_hostname property to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nPartial-Bug: #1355510\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}, {'number': 12, 'created': '2014-09-11 22:25:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/738b82ad63054e5feab84aa81635188c2f5a3eb1', 'message': 'Add conductor_hostname property to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nPartial-Bug: #1355510\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}, {'number': 13, 'created': '2014-09-16 19:24:36.000000000', 'files': ['ironic/objects/node.py', 'ironic/objects/conductor.py', 'ironic/db/sqlalchemy/alembic/versions/14d174ce8796_add_conductor_id_to_nodes.py', 'ironic/db/sqlalchemy/api.py', 'ironic/db/sqlalchemy/models.py', 'ironic/db/api.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/3a4075a1d4b7ae0272710cee13c0ad09b861d36b', 'message': 'WIP: Add conductor_id index to nodes\n\nWe need an index for retrieving nodes who belong to a specific conductor\nin order to rebalance our hash ring without walking all nodes.\n\nPartial-Bug: #1355510\n\nChange-Id: I607a1f810d948659ee7332f99d02c8288aecda2d\n'}]",10,109688,3a4075a1d4b7ae0272710cee13c0ad09b861d36b,60,8,13,10035,,,0,"WIP: Add conductor_id index to nodes

We need an index for retrieving nodes who belong to a specific conductor
in order to rebalance our hash ring without walking all nodes.

Partial-Bug: #1355510

Change-Id: I607a1f810d948659ee7332f99d02c8288aecda2d
",git fetch https://review.opendev.org/openstack/ironic refs/changes/88/109688/7 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/objects/node.py', 'ironic/tests/db/utils.py', 'ironic/db/sqlalchemy/api.py', 'ironic/db/sqlalchemy/models.py', 'ironic/api/controllers/v1/node.py', 'ironic/db/api.py']",6,491db129480733a930077e6b2f9a7f356ffa4a15,feature/store-node-conductor-in-db, 'conductor_hostname': hostname of the conductor the node belongs to 'conductor_hostname': hostname of the conductor the node belongs to,,19,3
openstack%2Fbarbican-specs~master~I076d6cc0b641ef4c48baabdc34ec75ab73453b0b,openstack/barbican-specs,master,I076d6cc0b641ef4c48baabdc34ec75ab73453b0b,spec to add type field on secrets,ABANDONED,2014-09-02 17:07:06.000000000,2014-09-27 01:58:02.000000000,,"[{'_account_id': 3}, {'_account_id': 7973}]","[{'number': 1, 'created': '2014-09-02 17:07:06.000000000', 'files': ['specs/juno/add-type-field-to-secrets.rst'], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/9f32a797dc5797d7d7a848f3d515dbc474cc4daf', 'message': 'spec to add type field on secrets\n\nChange-Id: I076d6cc0b641ef4c48baabdc34ec75ab73453b0b\n'}]",0,118409,9f32a797dc5797d7d7a848f3d515dbc474cc4daf,4,2,1,994,,,0,"spec to add type field on secrets

Change-Id: I076d6cc0b641ef4c48baabdc34ec75ab73453b0b
",git fetch https://review.opendev.org/openstack/barbican-specs refs/changes/09/118409/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/add-type-field-to-secrets.rst'],1,9f32a797dc5797d7d7a848f3d515dbc474cc4daf,add-type-field-on-secret,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================= Add-type-field-to-secrets ========================= Include the URL of your launchpad blueprint: https://blueprints.launchpad.net/barbican/+spec/add-type-field-to-secrets Currently Secret is generic resource, there is no way to distinguish what kind of secret is encapsulated in it. The type field on secrets will help identifying the Secrets and also improve the searches for particular type of secrets. Problem Description =================== * Secrets are generic resource, there is no way to identify what is stored in it. (SYMMETRIC, ASYMMETRIC, Password .....) * There is no way to search secrets based on type. Proposed Change =============== * Add a new field 'type' on Secret model to identify the secret type. * Enhance REST API to support filter based on secret type. Alternatives ------------ None Data model impact ----------------- * We have to add a new field on Secret model. * Data migration is required. REST API impact --------------- Following API to List of Secrets Per Tenant will be enhanced to support type filter. https://github.com/cloudkeep/barbican/wiki/Application-Programming-Interface#get---list-of-secrets-per-tenant Security impact --------------- None Notifications & Audit Impact ---------------------------- None Other end user impact --------------------- * python-novaclient need enhancement to incorporate the type filter. Performance Impact ------------------ None Other deployer impact --------------------- None Developer impact ---------------- Unit and functional tests need enhancement to accommodate tests for type filter. Implementation ============== Assignee(s) ----------- Primary assignee: <Arvind Tiwari atiwari> Other contributors: <launchpad-id or None> Work Items ---------- * Secret Data model change to add type field and unit test update. * Enhance REST API to support new filter based on type. Dependencies ============ None Testing ======= New tests has to be added in Tempest tests. Documentation Impact ==================== Below API doc will be enhanced to explain the use of new filter. https://github.com/cloudkeep/barbican/wiki/Application-Programming-Interface References ========== https://blueprints.launchpad.net/barbican/+spec/add-type-field-to-secrets ",,130,0
openstack%2Fbarbican-specs~master~I99c35f736708d94397e2b5a923f8c18728578b76,openstack/barbican-specs,master,I99c35f736708d94397e2b5a923f8c18728578b76,Spec to address master key encryption key management,ABANDONED,2014-06-30 21:14:28.000000000,2014-09-27 01:57:48.000000000,,"[{'_account_id': 3}, {'_account_id': 994}, {'_account_id': 7063}, {'_account_id': 7354}, {'_account_id': 7789}, {'_account_id': 8004}]","[{'number': 1, 'created': '2014-06-30 21:14:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/ad446af3e5d5043e45268a08e5daa8deebd9d47c', 'message': 'Spec to address master key management\n\nChange-Id: I99c35f736708d94397e2b5a923f8c18728578b76\n'}, {'number': 2, 'created': '2014-06-30 21:17:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/561d1726538ed5843ecc04b01b33647c4eda3dfb', 'message': 'Spec to address master key management\n\nChange-Id: I99c35f736708d94397e2b5a923f8c18728578b76\n'}, {'number': 3, 'created': '2014-06-30 21:19:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/70b9b9b501bebe1819eb7a002b2ae9ba5c0c3ff1', 'message': 'Spec to address master key management\n\nChange-Id: I99c35f736708d94397e2b5a923f8c18728578b76\n'}, {'number': 4, 'created': '2014-06-30 21:22:04.000000000', 'files': ['specs/juno/master-encryption-key-plugin.rst'], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/804e507323842df6261ee74ffabfa5ec879f1ec6', 'message': 'Spec to address master key encryption key management\n\nChange-Id: I99c35f736708d94397e2b5a923f8c18728578b76\n'}]",2,103662,804e507323842df6261ee74ffabfa5ec879f1ec6,19,6,4,994,,,0,"Spec to address master key encryption key management

Change-Id: I99c35f736708d94397e2b5a923f8c18728578b76
",git fetch https://review.opendev.org/openstack/barbican-specs refs/changes/62/103662/3 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/master-encryption-key-plugin.rst'],1,ad446af3e5d5043e45268a08e5daa8deebd9d47c,master-kek-plugin,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Master Encryption Key Plug-in ========================================== Include the URL of your launchpad blueprint: https://blueprints.launchpad.net/barbican/+spec/store-master-encryption-key-in-hsm Barbican generates key encryption key (a.k.a. KEK) per tenant to encrypt the secrets scoped to tenants. Confidentiality of these secrets depends on the confidentiality of the KEKs. In the current implementation KEKs are encrypted using single Master Encryption Key (a.k.a. MKEK) stored in Barbican configuration file. This blueprint proposes a plug-in framework to manage and consume MKEK. This way MKEKs can be stored in is separate and more secured system, hence provide more confidentiality to secrets which is stored in Barbican database. This blueprint also proposes manageability (Creation, Generation, Retrieval, Rotation and Decommissioning) aspects of MKEKs. Problem Description =================== A detailed description of the problem: * Master Key Encryption key is not secured, there has to be framework to store MKEKs in separate system (e.g. HSM, Database, File system ....). * Life cycle (Create, Generation, Retrieval, Deactivation and Rotation) management of MKEKs is missing which is an important aspect. * No REST APIs for Master Key Encryption key management. Proposed Change =============== Following are list of changes to address above mentioned problems: * A plug-in framework will be added in Barbican system to manage Master Key Encryption Key. This plug-in will introducing required contracts for life cycle (Creation, Generation, Retrieval, Rotation and Deactivation) events. The plug-in will abstracts the underlying implementation, that means underlying system to store the MKEKs can be HSM, Database, etc. * REST APIs will be added to manage Master Key Encryption Keys. The REST API will be integrated with plug in. * A reference implementation of the plug-in will be provided in the code base, which will use local file system to store the MKEK. Alternatives ------------ None Data model impact ----------------- * A new model `mkek_meta_data` will be introduced to hold MKEK meta data. following are the fields of `mkek_meta_data` data model - id - Local id of the MKEK entity. - mkek_ref_id - Reference Id of the a cryptographic key stored in underlying system. - expiration - Expiration of the MKEK. - status - Status (Active, De-Active) Note: - At a given time there will be only one Active MKEK in system. - Rotation will De-activate the current active MKEK. - De-Activated MKEK will be used only for decryption. * `kek_data` model will be modified, a new field will be added to store reference to `mkek_meta_data`. REST API impact --------------- A new REST resource `masterkeys` will be added to represent MKEK entity. Note: This REST API will be only exposed through ADMIN port. (HOW? TBD) * `POST /masterkeys` - This resource will store a key to be used for MKEK. Key will be stored in remote system. * `GET /masterkeys/<id>` - Returns the MKEK meta-data only, specified by id. Note: Actual MKEK will never be exposed through REST API. * `GET /masterkeys` - If id is not provided this API will list all the mkek_meta_data stored in system. Note: Actual MKEK will never be exposed through REST API. * `Delete` - Delete the a MKEK specified by mkek_id. Existing `Orders` resource will be used for MKEK generation and rotation purpose. * `POST /Orders` - This will generate the new MKEK in remote system and set up the mkek_meta_data. <Post request structure - TBD> Note: Generation will fail if there exist an active MKEK. * `POST /Orders` - This will used to rotate the MKEK in remote system. <Post request structure - TBD> Note: Rotation is 2 step process which includes de-activation the existing MKEK and Generate a new MKEK. Security impact --------------- Master key management APIs are reserved for Barbican deployer only. The APIs has to be protected so that it would not be exposed to tenants. How? <TBD, admin API, any suggestions?> Notifications & Audit Impact ---------------------------- None Other end user impact --------------------- None Performance Impact ------------------ None Other deployer impact --------------------- None Developer impact ---------------- None Implementation ============== Assignee(s) ----------- This will be multi phase implementation Phase 1 - Design and implement the plug-in framework Phase 2 - Design and implement the REST API. Primary assignee: <arvind-tiwari> <arunkant-uws> Work Items ---------- * Design and implement the plug-in framework. * Design and implement the REST API. Dependencies ============ None Testing ======= * Unit test has to be added to test the MKEK plug-in. * Unit and functional test will be required to REST API. Documentation Impact ==================== * Documentation will be added to explain the use of REST APIs. References ========== * https://blueprints.launchpad.net/barbican/+spec/store-master-encryption-key-in-hsm",,199,0
openstack%2Fbarbican-specs~master~I1c935a0d0c7b991a17c3a67c0619202bf1e41f26,openstack/barbican-specs,master,I1c935a0d0c7b991a17c3a67c0619202bf1e41f26,Spec for certificate api addition,ABANDONED,2014-07-21 16:37:56.000000000,2014-09-27 01:57:21.000000000,,"[{'_account_id': 3}, {'_account_id': 994}, {'_account_id': 1528}, {'_account_id': 7063}, {'_account_id': 7789}, {'_account_id': 7973}, {'_account_id': 9234}, {'_account_id': 9914}, {'_account_id': 10873}]","[{'number': 1, 'created': '2014-07-21 16:37:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/db53aab4fb32822712ad0f2e94537da44eea2ade', 'message': 'Spec for certificate api addition\n\nChange-Id: I1c935a0d0c7b991a17c3a67c0619202bf1e41f26\n'}, {'number': 2, 'created': '2014-07-21 16:39:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/5e6180d166883c32b3674e66673042c1206f61a9', 'message': 'Spec for certificate api addition\n\nChange-Id: I1c935a0d0c7b991a17c3a67c0619202bf1e41f26\n'}, {'number': 3, 'created': '2014-08-04 16:18:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/e8bc8460dbe7d40dde4d31a8331c4c9e3d3f67da', 'message': 'Spec for certificate api addition\n\nChange-Id: I1c935a0d0c7b991a17c3a67c0619202bf1e41f26\n'}, {'number': 4, 'created': '2014-08-05 12:36:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/3bd22ebd8add287eb606b0ab1de7a60b6e65aae6', 'message': 'Spec for certificate api addition\n\nChange-Id: I1c935a0d0c7b991a17c3a67c0619202bf1e41f26\n'}, {'number': 5, 'created': '2014-08-05 12:39:08.000000000', 'files': ['specs/juno/add-certificate-type-to-order-resource.rst'], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/d1f1b5b21a15ddef793da959377d6a52f0a9a5c3', 'message': 'Spec for certificate api addition\n\nChange-Id: I1c935a0d0c7b991a17c3a67c0619202bf1e41f26\n'}]",29,108429,d1f1b5b21a15ddef793da959377d6a52f0a9a5c3,29,9,5,1528,,,0,"Spec for certificate api addition

Change-Id: I1c935a0d0c7b991a17c3a67c0619202bf1e41f26
",git fetch https://review.opendev.org/openstack/barbican-specs refs/changes/29/108429/3 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/add-certificate-type-to-order-resource.rst'],1,db53aab4fb32822712ad0f2e94537da44eea2ade,(detached,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode =========================================== Add certificate type to the orders resource =========================================== https://blueprints.launchpad.net/barbican/+spec/api-orders-add-more-types Barbican's orders resource is used to generate secrets on behalf of clients. This blueprint addresses how the orders resources can be modified to generate signed certificates. Problem Description =================== Barbican is currently lacking generation support for the certificate order type. This should be generated via a separate order type (in addition to keys types also defined in the blueprint). Proposed Change =============== Barbican Order API will be updated to support certificate signing. This change does not talk about the required backend implementation, only the required API changes and how to interpret the submitted data. Alternatives ------------ None. Data model impact ----------------- Barbican Order model requires the attributes added in the add-more-types-to-the-order-resource spec. REST API impact --------------- The ordering resource will be updated to allow generating certificates. Same as with secret keys, the process is asynchronous and the submission of an order only initiates the process of certificate generation. Resource Structure For Certificate Signing:: { ""type"": ""--type--"", ""meta"": { ""name"": ""--name--"", ""mode"": ""--mode--"", ""pass_phrase"": ""--pass_phrase--"", ""expiration"": ""--expiration--"", ""status"": ""--status--"", ""payload_content_type"": ""--type--"", ""issuer"": ""--certificate_issuer--"", ""request"": ""--csr--"", ""valid_not_before"": ""--ISO-8601 time--"", ""valid_not_after"": ""--ISO-8601 time--"" } } Resource Structure For Certificate Generation:: { ""type"": ""--type--"", ""meta"": { ""name"": ""--name--"", ""mode"": ""--mode--"", ""pass_phrase"": ""--pass_phrase--"", ""expiration"": ""--expiration--"", ""status"": ""--status--"", ""payload_content_type"": ""--type--"", ""issuer"": ""--certificate_issuer--"", ""certificate_key"": ""--pem_key--"", ""certificate_key_ref"": ""--secret_name--"", ""key_usage"": [""--key_usage_id--""], ""subject"": ""--common_name--"", ""alternative_names"": { ""dns"": [""--domain_name--""], ""email"": [""--email_address--""], ""principal"": [""--principal_name--""] }, ""valid_not_before"": ""--ISO-8601 time--"", ""valid_not_after"": ""--ISO-8601 time--"" } } Required Attribute:: type (string) Type of the secret, supported option described here is certificate:: meta (string) Describes additional information regarding the secret:: meta.name (string) Human readable name for the secret:: meta.issuer (string) Chosen signing authority. This value should be interpreted by the backend. A certificate fingerprint must be accepted, but the backend may also accept specific names defined by the service provider.:: Required Attribute (for certificate signing only):: meta.request (string) The contents of the PKCS#10 signing request, encoded in PEM format with the usual header/footer:: Required Attribute (for certificate generation only):: meta.certificate_key_ref or meta.certificate_key (string) Id of the secret holding the key to be used by the certificate, or the key itself. The key must be in PEM format:: meta.subject (string) Value of the subject name in the generated certificate, encoded as RFC2253 compatible string (for example CN=the_person,O=org):: Optional Attribute:: meta.key_usage (array of string) Used when generating a new certificate without a CSR. The list contains key usage names as defined in standards documents. The following are guaranteed to be supported: id-kp-serverAuth, id-kp-clientAuth, id-kp-codeSigning, id-kp-emailProtection, id-kp-timeStamping, id-kp-OCSPSigning The key usage field is set to match this selection. If left out, the default is ""id-kp-serverAuth"".:: meta.alternative_names (dictionary) Can be provided when generating a new certificate without a CSR. It may contain any of the keys: ""dns"", ""email"", ""principal"". The values are arrays of strings and mean, respectively, which dns names, email addresses and kerberos principals should be used in the alternative names section of the certificate. Default is empty dictionary and no alternative names requested.:: meta.valid_not_before (string) The requested time in ISO-8601 format, since when the generated certificate should be valid:: meta.valid_not_after (string) The requested time in ISO-8601 format, when the certificate should expire:: meta.mode (string) The type/mode of the algorithm associated with the secret information:: meta.pass_phrase (string) Pass phrase to be associated with secret. Used in conjunction when type is asymmetric:: meta.expiration (string) The expiration date for the secret in ISO-8601 format. Once the secret has expired, it will no longer be returned by the API or agent. If this field is not supplied, then the secret has no expiration date:: meta.status (string) Status of the generated secret. The default is `Active`:: meta.payload_content_type (string) API --- Order Create Order: POST /v1/{tenant_id}/orders Example 01 - Certificate signing:: Request: POST /v1/{tenant_id}/orders { ""type"": ""certificate"", ""meta"": { ""name"": ""secretname"", ""issuer"": ""deadbeefdeadbeefdeadbeefdeadbeefdeadbeef"", ""request"": ""-----BEGIN CERTIFICATE REQUEST-----\nMIIBETCBvAIBADBXMQswCQYDVQQGEwJBVTETMBEGA1UECAwKU29tZS1TdGF0ZTEh\nMB8GA1UECgwYSW50ZXJuZXQgV2lkZ2l0cyBQdHkgTHRkMRAwDgYDVQQDDAdleGFt\ncGxlMFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBALBLBQ3esWfS6HbJhNEUndZB8b/A\nl8oNGf+0DKkEeUhSu2NC0sq6pVl3Qmsp7tKrnxUXo+2oWmnmTYtWJ4Sc5wECAwEA\nAaAAMA0GCSqGSIb3DQEBCwUAA0EALmuXZIutdWhyMRM8kBE1C6ovZYzq5VjH6qpC\npUbbgDX3OFpbgVA1vCeMAPRhiQsRp8GQWqGl0tb+lbq2QIB4wg==\n-----END CERTIFICATE REQUEST-----\n"", ""valid_not_before"": ""2014-01-01T00:00:00.0"", ""valid_not_after"": ""2015-01-01T00:00:00.0"", ""expiration"": ""2015-02-28T19:14:44.180394"", ""payload_content_type"": ""application/octet-stream"" } } Response: Status: 201 Created { ""order_ref"": ""http://localhost:9311/v1/1234/orders/439e1b22-3f48-48c7-86f4-84930eb23c11"" } Example 02 - Certificate generation:: Request: POST /v1/{tenant_id}/orders { ""type"": ""certificate"", ""meta"": { ""name"": ""secretname"", ""issuer"": ""deadbeefdeadbeefdeadbeefdeadbeefdeadbeef"", ""certificate_key_ref"": ""a8957047-16c6-4b05-ac57-8621edd0e9ee"", ""key_usage"": [""id-kp-serverAuth""], ""subject"": ""CN=host.example.net"", ""alternative_names"": { ""dns"": [""host.example.net"", ""alternative.example.net""], }, ""valid_not_before"": ""2014-01-01T00:00:00.0"", ""valid_not_after"": ""2015-01-01T00:00:00.0"", ""expiration"": ""2015-02-28T19:14:44.180394"", ""payload_content_type"": ""application/octet-stream"" } } Response: Status: 201 Created { ""order_ref"": ""http://localhost:9311/v1/1234/orders/8dbe75fb-0074-49b8-a23d-90a84d1e6a73"" } Listing orders works the same as in the add-more-types-to-the-order-resource spec. Security impact --------------- None Notifications & Audit Impact ---------------------------- Not directly. Backend service signing the certificate request should handle appropriate operator notification and provide audit log of executed actions. Other end user impact --------------------- * Barbican-python client will need to be enhanced to accommodate API changes. Performance Impact ------------------ None Other deployer impact --------------------- None. Developer impact ---------------- None. Implementation ============== Assignee(s) ----------- Stanislaw Pitucha is leading this feature enhancement. Work Items ---------- * Enhance Order REST API to support certificate order type. (TBD) * Enhance Barbican python client to support certificate order type. (TBD) Dependencies ============ * Update to Orders model to support type, meta, container_id (done) * Full functionality testing will require certificate signing plugin (in progress) Testing ======= * Unit tests required Documentation Impact ==================== * Order API request and response structure has to be explained in the docs. https://github.com/cloudkeep/barbican/wiki/Application-Programming-Interface#orders-resource * Barbican WADL and developer guide will be update for this API change. References ========== - https://blueprints.launchpad.net/barbican/+spec/api-orders-add-more-types ",,322,0
openstack%2Fbarbican-specs~master~I7e451d6166e8ae3277c21eda8b52bf2c7b01af52,openstack/barbican-specs,master,I7e451d6166e8ae3277c21eda8b52bf2c7b01af52,Add Version Responses Consistent with Openstack,ABANDONED,2014-07-18 22:37:19.000000000,2014-09-27 01:57:09.000000000,,"[{'_account_id': 3}, {'_account_id': 7262}, {'_account_id': 7973}, {'_account_id': 8004}, {'_account_id': 10873}]","[{'number': 1, 'created': '2014-07-18 22:37:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/7ce2ab60ae2dae63a5f6c4fa289fb4b5182e93f9', 'message': 'Add Version Responses Consistent with Openstack\n\nBarbican currently returns one-off version information for GETs to the\nroot resource. This blueprint calls for replacing that response with\nan OpenStack consistent response, that includes which API versions are\ncurrently supported. Such version information could facilitate\nautomatic discovery of services when appended with the endpoint\ninformation retrieved from Keystone service catalogs for example.\n\nChange-Id: I7e451d6166e8ae3277c21eda8b52bf2c7b01af52\nImplements: blueprint fix-version-api\n'}, {'number': 2, 'created': '2014-07-21 20:34:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/1f175d3494afdf436a7e97fd42486d7c6be6b13f', 'message': 'Add Version Responses Consistent with Openstack\n\nBarbican currently returns one-off version information for GETs to the\nroot resource. This blueprint calls for replacing that response with\nan OpenStack consistent response, that includes which API versions are\ncurrently supported. Such version information could facilitate\nautomatic discovery of services when appended with the endpoint\ninformation retrieved from Keystone service catalogs for example.\n\nChange-Id: I7e451d6166e8ae3277c21eda8b52bf2c7b01af52\nImplements: blueprint fix-version-api\n'}, {'number': 3, 'created': '2014-08-12 13:22:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/c10e3fc345c446cdd22be5e0e2a8480f0ec515cb', 'message': 'Add Version Responses Consistent with Openstack\n\nBarbican currently returns one-off version information for GETs to the\nroot resource. This blueprint calls for replacing that response with\nan OpenStack consistent response based on json-home, that includes\nwhich API versions are currently supported. Such version information\ncould facilitate automatic discovery of services when appended with\nthe endpoint information retrieved from Keystone service catalogs for\nexample.\n\nChange-Id: I7e451d6166e8ae3277c21eda8b52bf2c7b01af52\nImplements: blueprint fix-version-api\n'}, {'number': 4, 'created': '2014-08-12 13:38:42.000000000', 'files': ['specs/juno/fix-version-api.rst'], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/e5123cd7a6230540c798c77dd9c77aafafc06342', 'message': 'Add Version Responses Consistent with Openstack\n\nBarbican currently returns one-off version information for GETs to the\nroot resource. This blueprint calls for replacing that response with\nan OpenStack consistent response based on json-home, that includes\nwhich API versions are currently supported. Such version information\ncould facilitate automatic discovery of services when appended with\nthe endpoint information retrieved from Keystone service catalogs for\nexample.\n\nChange-Id: I7e451d6166e8ae3277c21eda8b52bf2c7b01af52\nImplements: blueprint fix-version-api\n'}]",5,108163,e5123cd7a6230540c798c77dd9c77aafafc06342,22,5,4,7789,,,0,"Add Version Responses Consistent with Openstack

Barbican currently returns one-off version information for GETs to the
root resource. This blueprint calls for replacing that response with
an OpenStack consistent response based on json-home, that includes
which API versions are currently supported. Such version information
could facilitate automatic discovery of services when appended with
the endpoint information retrieved from Keystone service catalogs for
example.

Change-Id: I7e451d6166e8ae3277c21eda8b52bf2c7b01af52
Implements: blueprint fix-version-api
",git fetch https://review.opendev.org/openstack/barbican-specs refs/changes/63/108163/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/fix-version-api.rst'],1,7ce2ab60ae2dae63a5f6c4fa289fb4b5182e93f9,bp/calls,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode =============================================== Add Version Responses Consistent with Openstack =============================================== Launchpad blueprint: https://blueprints.launchpad.net/barbican/+spec/fix-version-api Barbican currently returns one-off version information for GETs to the root resource. This blueprint calls for replacing that response with an OpenStack consistent response, that includes which API versions are currently supported. Such version information could facilitate automatic discovery of services when appended with the endpoint information retrieved from Keystone service catalogs for example. Problem description =================== When the root resource of Barbican is queried with a GET request, it should respond with a version response consistent with other OpenStack projects such as this example from Keystone (documented here: http://docs.openstack.org/api/openstack-identity-service/2.0/content/\ Versions-d1e472.html): { ""choices"": [ { ""id"": ""v1.0"", ""status"": ""DEPRECATED"", ""links"": [ { ""rel"": ""self"", ""href"": ""http://identity.api.openstack.org/v1.0"" } ], ""media-types"": { ""values"": [ { ""base"": ""application/xml"", ""type"": ""application/vnd.openstack.identity+xml;\ version=1.0"" }, { ""base"": ""application/json"", ""type"": ""application/vnd.openstack.identity+json;\ version=1.0"" } ] } }, { ""id"": ""v1.1"", ""status"": ""CURRENT"", ""links"": [ { ""rel"": ""self"", ""href"": ""http://identity.api.openstack.org/v1.1"" } ], ""media-types"": { ""values"": [ { ""base"": ""application/xml"", ""type"": ""application/vnd.openstack.identity+xml;\ version=1.1"" }, { ""base"": ""application/json"", ""type"": ""application/vnd.openstack.identity+json;version=1.1"" } ] } }, { ""id"": ""v2.0"", ""status"": ""BETA"", ""links"": [ { ""rel"": ""self"", ""href"": ""http://identity.api.openstack.org/v2.0"" } ], ""media-types"": { ""values"": [ { ""base"": ""application/xml"", ""type"": ""application/vnd.openstack.identity+xml;\ version=2.0"" }, { ""base"": ""application/json"", ""type"": ""application/vnd.openstack.identity+json;\ version=2.0"" } ] } } ], ""choices_links"": """" } When a GET is performed for a specific version, version details should be provided in response such as per this Keystone example: { ""version"": { ""status"": ""stable"", ""updated"": ""2014-04-17T00:00:00Z"", ""media-types"": [ { ""base"": ""application/json"", ""type"": ""application/vnd.openstack.identity-v2.0+json"" }, { ""base"": ""application/xml"", ""type"": ""application/vnd.openstack.identity-v2.0+xml"" } ], ""id"": ""v2.0"", ""links"": [ { ""href"": ""http://23.253.228.211:5000/v2.0/"", ""rel"": ""self"" }, { ""href"": ""http://docs.openstack.org/api/\ openstack-identity-service/2.0/content/"", ""type"": ""text/html"", ""rel"": ""describedby"" }, { ""href"": ""http://docs.openstack.org/api/\ openstack-identity-service/2.0/identity-dev-guide-2.0.pdf"", ""type"": ""application/pdf"", ""rel"": ""describedby"" } ] } } Proposed change =============== As Chad Lung pointed out in the Launchpad blueprint, several projects document and implement the version responses shown above, so Barbican can follow the same approach. Barbican only supports requests and response in the JSON format, so XML formats will not be supported or implemented via this blueprint. Keystone's keystone/controllers.py implements the GET requests and builds the version responses. Note that the MEDIA_TYPE_JSON should be 'application/vnd.openstack.keymanagement-%s+json'. The 'Version' controller defined in this file is mapped to the URI path via the keystone/routers.py module. Finally the keystone/service.py module builds the final WSGI application, and includes the version routers defined in routers.py. For Barbican then, the barbican/api/controllers/versions.py VersionController class could be modified similarly to Keystone's controllers.py, but should be renamed to VersionsController to handle the root requests, with a new VersionController added to return specific version detail responses. The barbican/api/app.py file should be modified to use the new VersionsController wherever VersionController is used now. Note that while the root version resource would support unauthenticated requests, specific version resource requests would require authentication. Also, the build version information that is responded with the root resource now would still need to be provided to support automated deployment and testing processes. This blueprint proposes adding a query parameter to the root resource such as ?build_version, that would return a JSON response similar to the current one but without the version information, such as this: { build: ""2014.1.dev43.g22d1a96"" } Finally, the Barbican Python client should be updated to take advantage of this new version discovery information to augment the endpoint information it retrieves from Keystone service catalogs. Alternatives ------------ None. Data model impact ----------------- None. REST API impact --------------- The root version response will be changed from the one-off version information returned now to the OpenStack consistent response. A specific version details response (i.e. when performing a GET on v1/ for example) will need to be added. The current root response that includes the build version would need to be modified to only return this information if a query parameter is specified. Security impact --------------- None. Notifications impact -------------------- None. Other end user impact --------------------- Any processes that rely on the current build version response would need to be modified. Clients using the Barbican Python client will need to update to the release after the changes in this blueprint are made. Performance Impact ------------------ None. Other deployer impact --------------------- None. Developer impact ---------------- None. Implementation ============== Assignee(s) ----------- Primary assignee: TBD Other potential contributors: john-wood-w chad-lung Work Items ---------- The following CRs would build out this blueprint: 1) Modify version-related modules per proposal above. 2) Implement the build_version query parameter on the root resource to return the build version. 3) Update Barbican Python client code base to query the root resource for version information and then apply that to the endpoint information retrieved from the Keystone service catalog. Dependencies ============ None. Testing ======= Unit testing of the version resources will be added. Documentation Impact ==================== Update this page with API changes: https://github.com/cloudkeep/barbican/wiki/Application-Programming-Interface References ========== See code and documentation references embedded in this blueprint above. ",,297,0
openstack%2Fbarbican-specs~master~I1fe4337b4a60629eed0aba5be308534580c7a830,openstack/barbican-specs,master,I1fe4337b4a60629eed0aba5be308534580c7a830,Split validator module in to smaller modules,ABANDONED,2014-08-19 20:15:14.000000000,2014-09-27 01:57:01.000000000,,"[{'_account_id': 3}, {'_account_id': 7973}]","[{'number': 1, 'created': '2014-08-19 20:15:14.000000000', 'files': ['specs/juno/split-validator-module.rst'], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/86f9de0df743cb51b5daec8b8dd49fe27aca8915', 'message': 'Split validator module in to smaller modules\n\nChange-Id: I1fe4337b4a60629eed0aba5be308534580c7a830\n'}]",0,115394,86f9de0df743cb51b5daec8b8dd49fe27aca8915,5,2,1,994,,,0,"Split validator module in to smaller modules

Change-Id: I1fe4337b4a60629eed0aba5be308534580c7a830
",git fetch https://review.opendev.org/openstack/barbican-specs refs/changes/94/115394/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/split-validator-module.rst'],1,86f9de0df743cb51b5daec8b8dd49fe27aca8915,split-validator,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode =========================================== Split validator module in to smaller modules =========================================== Include the URL of your launchpad blueprint: https://blueprints.launchpad.net/barbican/+spec/split-validator-module-in-to-smaller-modules Current validator module is getting difficult to manage as it is grown very big. We have to split this module in to smaller units to make it more manageable. Problem Description =================== * Barbican REST request validation module 'barbican.common.validators' is growing big and getting unmanageable. * Current package for validator module is not appropriate. Proposed Change =============== * Split the validator module in to smaller manageable modules. * Refactor the validator in to multiple validators mapped to corresponding controllers. * Define new package ""barbican.api.controllers.validators"" and mode the new validators to new package. The new package structure for validators would be as below. barbican/ api/ controllers/ validators/ BaseValidator.py OrderValidator.py SecretValidator.py .... .... Alternatives ------------ None. Data model impact ----------------- None REST API impact --------------- None Security impact --------------- None Notifications & Audit Impact ---------------------------- None Other end user impact --------------------- None Performance Impact ------------------ None Other deployer impact --------------------- None Developer impact ---------------- Developer has to create new validators for each new REST resources. Implementation ============== Assignee(s) ----------- Primary assignee: <atiwari> Other contributors: <launchpad-id or None> Work Items ---------- * Refactor validator module and move to new package structure. * Define new validator modules for each controllers. * Rearrange the unit tests. Dependencies ============ None Testing ======= None Documentation Impact ==================== None References ========== https://review.openstack.org/#/c/87405/23/barbican/common/validators.py ",,135,0
openstack%2Fbarbican-specs~master~Ifff539c11b1650b8c652f1c4a3dc35a2d70754f8,openstack/barbican-specs,master,Ifff539c11b1650b8c652f1c4a3dc35a2d70754f8,Blueprint for supporting binary secret retrieval in text format,ABANDONED,2014-08-19 23:56:48.000000000,2014-09-27 01:56:50.000000000,,"[{'_account_id': 3}, {'_account_id': 1091}, {'_account_id': 7789}, {'_account_id': 7973}, {'_account_id': 8004}]","[{'number': 1, 'created': '2014-08-19 23:56:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/30e237c634d4f406d55bc20571657a6e5b9f181c', 'message': 'Blueprint for supporting binary secret retrival in text format\n\nChange-Id: Ifff539c11b1650b8c652f1c4a3dc35a2d70754f8\n'}, {'number': 2, 'created': '2014-08-19 23:58:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/d21e5aef21d8e088f60cbc3073d35309244906c8', 'message': 'Blueprint for supporting binary secret retrival in text format\n\nChange-Id: Ifff539c11b1650b8c652f1c4a3dc35a2d70754f8\n'}, {'number': 3, 'created': '2014-08-20 12:34:17.000000000', 'files': ['specs/juno/support-for-text-plain-order.rst'], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/944a5e96337a64412230394c425f28b07d078cbe', 'message': 'Blueprint for supporting binary secret retrieval in text format\n\nChange-Id: Ifff539c11b1650b8c652f1c4a3dc35a2d70754f8\n'}]",4,115435,944a5e96337a64412230394c425f28b07d078cbe,14,5,3,1091,,,0,"Blueprint for supporting binary secret retrieval in text format

Change-Id: Ifff539c11b1650b8c652f1c4a3dc35a2d70754f8
",git fetch https://review.opendev.org/openstack/barbican-specs refs/changes/35/115435/2 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/support-for-text-plain-order.rst'],1,30e237c634d4f406d55bc20571657a6e5b9f181c,bp/for,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ==================================== Support for text/plain order request ==================================== https://blueprints.launchpad.net/barbican/+spec/support-for-text-plain-in-order-request Barbican currently supports secret generation in binary format. This blueprint is to add optional support for getting secrets data in text format. Problem Description =================== Barbican allows retrieval of secret data in binary format which works good when its consumed directly by client application with no user (human) interaction. In cases where user interaction is involved either via a workflow or via UI, binary representation will not work as some of character may not be printable, can be copied by editors etc. In those cases, it will be useful to have a standard way of representing binary secret data in human readable format. This blueprint is to address this issue. Proposed Change =============== Base64 encoding scheme is widely used to encode binary data to store and transport and is a standard mechanism for binary content encoding to ASCII string format. The proposal is to return binary secret data in base64 encoded format when secret retrieve requests sends `Accept` header as `text/plain`. This change can be useful in testing via curl where there is need to copy secret read from barbican and to pass it as data point in another openstack service curl request. There is no change expected in secret data storage format. In barbican, secret can be uploaded in `text/plain` format as well. For those cases, there would not be any encoding change needed in secret retrieval API. For wrapped secret retrieval, binary secret is going to be base64 encoded if `Accept` header is passed as `text/plain`. Alternatives ------------ This is an optional feature which is currently missing in barbican. In absence of this, application and end-user which needs to print/share the secret data, they may have to wrap data internally which can lead to various encoding scheme/ representation. In some cases, this alternative apporach may not be practical like curl testing case mentioned above. Base64 encoding is a well understood format to transport binary data and end-client can decode it easily when it plans to use it. Same, Base64, encoding is used in Barbican internally to store encrypted data. Data model impact ----------------- None REST API impact --------------- Change is expected for secret retrieval API where actual secret is returned. Get secret: GET /v1/secrets/<secret_ref> * Existing API returns secret metadata or actual secret depending on ``Accept`` header value. In case of missing ``Accept`` header, secret metadata is returned. * With this change, support for ``Accept`` header with ``text/plain`` is going to return binary secret as Base64 encoded ASCII string. * Example: Binary secret with `text/plain` Accept header:: Request: curl -i -H ""Accept:text/plain"" -H ""X-Project-Id:12345"" \ http://localhost:9311/v1/secrets/acdc59ec-00e9-4643-839e-4f3d71fb81dc Response: HTTP/1.1 200 OK Content-Length: 64 Content-Type: text/plain; charset=UTF-8 5005EBD409C2F1E772B5784559751B75C1CA0E49ABC8CC931F94512613264136 * Example: Uploaded text secret with `text/plain` Accept header:: Request: curl -i -H ""Accept:text/plain"" -H ""X-Project-Id:12345"" \ http://localhost:9311/v1/secrets/b2abdd29-337f-4a98-a172-a4535d7d3bdd Response: HTTP/1.1 200 OK Content-Length: 14 Content-Type: text/plain; charset=UTF-8 my-secret-here Security impact --------------- None Notifications & Audit Impact ---------------------------- None. Other end user impact --------------------- There should not be any impact other than that returned value is Base64 encoded in case of binary secret. Where secret is to be used for encrpyting some openstack service artifact, client can either request secret with `Accept` header as `application/octet-stream` or decode base64 encoded string value. Performance Impact ------------------ None Other deployer impact --------------------- None Developer impact ---------------- Documentation needs to reflect this optional Accept header support. Implementation ============== Assignee(s) ----------- Primary assignee: arun.kant@hp.com Work Items ---------- * For GET secret API, wrap with base64 encoding for binary secret data. * Enhance documentation to reflect the change. Dependencies ============ None Testing ======= Unit test will be added to show behvaior for binary secret and plain secret. Documentation Impact ==================== Update `Barbican Developer Guide` to reflect support for this header and provide sample to illustrate difference between binary and plain secret response. References ========== * https://blueprints.launchpad.net/barbican/+spec/support-for-text-plain-in-order-request ",,171,0
openstack%2Fnova~stable%2Ficehouse~I904b6751dec740e001f5ec29f637ef456528746f,openstack/nova,stable/icehouse,I904b6751dec740e001f5ec29f637ef456528746f,libvirt: return the correct instance path while cleanup_resize,MERGED,2014-07-02 01:28:53.000000000,2014-09-27 01:39:07.000000000,2014-09-27 01:39:04.000000000,"[{'_account_id': 3}, {'_account_id': 308}, {'_account_id': 1313}, {'_account_id': 1420}, {'_account_id': 1779}, {'_account_id': 4468}, {'_account_id': 5170}, {'_account_id': 7400}, {'_account_id': 8247}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9656}, {'_account_id': 10118}]","[{'number': 1, 'created': '2014-07-02 01:28:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/01661e1d85cac342b2a5a477c4986241c11c5cda', 'message': 'libvirt: return the correct instance path while cleanup_resize\n\nIf we resized a pre-grizzly instance with grizzly or later nova\nto another host, while the resize confirmation process,\n_cleanup_resize will find the instance resize backup dir and\ndelete it, but a wrong xxx_resize dir like ${uuid}_resize,\ninstead of the correct ${name}_resize will be found.\nThis is because the instance is a resized one which original\ninstance dir exists on another host(the dest host),\nget_instance_path method could not find the original instance\ndir on the source host, so the path with uuid will be returned,\nand the `target` existing check in _cleanup_resize is failed,\nthen the ${name}_resize dir will never be deleted.\n\nCloses-bug: #1290294\nChange-Id: I904b6751dec740e001f5ec29f637ef456528746f\n'}, {'number': 2, 'created': '2014-07-02 01:31:50.000000000', 'files': ['nova/tests/virt/libvirt/test_libvirt.py', 'nova/virt/libvirt/driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/aeb71a88ae8d05ff6f5f3f092965f12369fec07a', 'message': 'libvirt: return the correct instance path while cleanup_resize\n\nIf we resized a pre-grizzly instance with grizzly or later nova\nto another host, while the resize confirmation process,\n_cleanup_resize will find the instance resize backup dir and\ndelete it, but a wrong xxx_resize dir like ${uuid}_resize,\ninstead of the correct ${name}_resize will be found.\nThis is because the instance is a resized one which original\ninstance dir exists on another host(the dest host),\nget_instance_path method could not find the original instance\ndir on the source host, so the path with uuid will be returned,\nand the `target` existing check in _cleanup_resize is failed,\nthen the ${name}_resize dir will never be deleted.\n\nCloses-bug: #1290294\nChange-Id: I904b6751dec740e001f5ec29f637ef456528746f\n(cherry picked from b4964eb6a570e290545f95d45411dc8441985cd5)\n'}]",0,104050,aeb71a88ae8d05ff6f5f3f092965f12369fec07a,44,13,2,4468,,,0,"libvirt: return the correct instance path while cleanup_resize

If we resized a pre-grizzly instance with grizzly or later nova
to another host, while the resize confirmation process,
_cleanup_resize will find the instance resize backup dir and
delete it, but a wrong xxx_resize dir like ${uuid}_resize,
instead of the correct ${name}_resize will be found.
This is because the instance is a resized one which original
instance dir exists on another host(the dest host),
get_instance_path method could not find the original instance
dir on the source host, so the path with uuid will be returned,
and the `target` existing check in _cleanup_resize is failed,
then the ${name}_resize dir will never be deleted.

Closes-bug: #1290294
Change-Id: I904b6751dec740e001f5ec29f637ef456528746f
(cherry picked from b4964eb6a570e290545f95d45411dc8441985cd5)
",git fetch https://review.opendev.org/openstack/nova refs/changes/50/104050/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/libvirt/test_libvirt.py', 'nova/virt/libvirt/driver.py']",2,01661e1d85cac342b2a5a477c4986241c11c5cda,icehouse," # NOTE(wangpan): we get the pre-grizzly instance path firstly, # so the backup dir of pre-grizzly instance can # be deleted correctly with grizzly or later nova. pre_grizzly_name = libvirt_utils.get_instance_path(instance, forceold=True) target = pre_grizzly_name + '_resize' if not os.path.exists(target): target = libvirt_utils.get_instance_path(instance) + '_resize' "," target = libvirt_utils.get_instance_path(instance) + ""_resize""",13,3
openstack%2Ftaskflow~master~I9eeb674f7064aaa02a99418ca2b33c99debcf6e1,openstack/taskflow,master,I9eeb674f7064aaa02a99418ca2b33c99debcf6e1,Always return copies from memory backend,ABANDONED,2014-09-26 01:32:06.000000000,2014-09-27 01:11:38.000000000,,[{'_account_id': 3}],"[{'number': 1, 'created': '2014-09-26 01:32:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/88273f57664d5409bcc4bfcb246320274f83f42c', 'message': 'Always return copies from memory backend\n\nTo avoid the issue with conductors having raw access to the logbook\nor flow details and atom details in the memory backend, and this causing\nperiodic inconsistencies with stored results ensure that we *always* return\ncopies from the memory backend instead to prevent this.\n\nThis matches the behavior of other backends which always return copies\nof there underlying objects as well (partially because they do not\nstore the objects in a compatible format).\n\nFixes bug 1365830\n\nAlso fixes a retry test case issue that was discovered due to this more\nrobust copy usage.\n\nChange-Id: I9eeb674f7064aaa02a99418ca2b33c99debcf6e1\n'}, {'number': 2, 'created': '2014-09-26 01:35:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/6e3181f58a39ff5d69ae25dc579800d74c2281a9', 'message': 'Always return copies from memory backend\n\nTo avoid the issue with conductors having raw access to the logbook\nor flow details and atom details in the memory backend, and this causing\nperiodic inconsistencies with stored data ensure that we *always* return\ncopies from the memory backend instead to prevent this.\n\nThis matches the behavior of other backends which always return copies\nof there underlying objects as well (partially because they do not\nstore the objects in a compatible format, ie in a database for the\ndatabase backend, or on a filesystem for the dir/file backend...).\n\nFixes bug 1365830\n\nAlso fixes a retry test case issue that was discovered due to this more\nrobust memory backend copy usage.\n\nChange-Id: I9eeb674f7064aaa02a99418ca2b33c99debcf6e1\n'}, {'number': 3, 'created': '2014-09-26 05:45:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/255c3cafc07571527efe9d3de01cdf32cc25455e', 'message': 'Always return copies from memory backend\n\nTo avoid the issue with conductors having raw access to the logbook\nor flow details and atom details in the memory backend, and this causing\nperiodic inconsistencies with stored data ensure that we *always* return\ncopies from the memory backend instead to prevent this.\n\nThis matches the behavior of other backends which always return copies\nof there underlying objects as well (partially because they do not\nstore the objects in a compatible format, ie in a database for the\ndatabase backend, or on a filesystem for the dir/file backend...).\n\nFixes bug 1365830\n\nAlso fixes a retry test case issue that was discovered due to this more\nrobust memory backend copy usage.\n\nChange-Id: I9eeb674f7064aaa02a99418ca2b33c99debcf6e1\n'}, {'number': 4, 'created': '2014-09-26 06:40:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/27575d365d500a080f35d1e3c33a29499dfaa53b', 'message': 'Always return copies from memory backend\n\nTo avoid the issue with conductors having raw access to the logbook\nor flow details and atom details in the memory backend, and this causing\nperiodic inconsistencies with stored data ensure that we *always* return\ncopies from the memory backend instead to prevent this.\n\nThis matches the behavior of other backends which always return copies\nof there underlying objects as well (partially because they do not\nstore the objects in a compatible format, ie in a database for the\ndatabase backend, or on a filesystem for the dir/file backend...).\n\nFixes bug 1365830\n\nAlso fixes a retry test case issue that was discovered due to this more\nrobust memory backend copy usage.\n\nChange-Id: I9eeb674f7064aaa02a99418ca2b33c99debcf6e1\n'}, {'number': 5, 'created': '2014-09-26 07:05:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/be834bc0992dcc618ef44a6e0f868550230234e7', 'message': 'Always return copies from memory backend\n\nTo avoid the issue with conductors having raw access to the logbook\nor flow details and atom details in the memory backend, and this causing\nperiodic inconsistencies with stored data ensure that we *always* return\ncopies from the memory backend instead to prevent this.\n\nThis matches the behavior of other backends which always return copies\nof there underlying objects as well (partially because they do not\nstore the objects in a compatible format, ie in a database for the\ndatabase backend, or on a filesystem for the dir/file backend...).\n\nFixes bug 1365830\n\nAlso fixes a retry test case issue that was discovered due to this more\nrobust memory backend copy usage.\n\nChange-Id: I9eeb674f7064aaa02a99418ca2b33c99debcf6e1\n'}, {'number': 6, 'created': '2014-09-26 07:26:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/bb569ce4889bd38e02d7040439247ed014cefbbb', 'message': 'Always return copies from memory backend\n\nTo avoid the issue with conductors having raw access to the logbook\nor flow details and atom details in the memory backend, and this causing\nperiodic inconsistencies with stored data ensure that we *always* return\ncopies from the memory backend instead to prevent this.\n\nThis matches the behavior of other backends which always return copies\nof there underlying objects as well (partially because they do not\nstore the objects in a compatible format, ie in a database for the\ndatabase backend, or on a filesystem for the dir/file backend...).\n\nFixes bug 1365830\n\nAlso fixes a retry test case issue that was discovered due to this more\nrobust memory backend copy usage.\n\nChange-Id: I9eeb674f7064aaa02a99418ca2b33c99debcf6e1\n'}, {'number': 7, 'created': '2014-09-26 21:28:07.000000000', 'files': ['taskflow/persistence/logbook.py', 'taskflow/persistence/backends/impl_memory.py', 'taskflow/tests/unit/test_utils.py', 'taskflow/tests/unit/test_retries.py', 'taskflow/utils/misc.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/b6d923991ca4e61e2da34ab4d82072b353b748c1', 'message': 'Always return copies from memory backend\n\nTo avoid the issue with conductors having raw access to the logbook\nor flow details and atom details in the memory backend, and this causing\nperiodic inconsistencies with stored data ensure that we *always* return\ncopies from the memory backend instead to prevent this.\n\nThis matches the behavior of other backends which always return copies\nof there underlying objects as well (partially because they do not\nstore the objects in a compatible format, ie in a database for the\ndatabase backend, or on a filesystem for the dir/file backend...).\n\nFixes bug 1365830\n\nAlso fixes a retry test case issue that was discovered due to this more\nrobust memory backend copy usage.\n\nChange-Id: I9eeb674f7064aaa02a99418ca2b33c99debcf6e1\n'}]",0,124242,b6d923991ca4e61e2da34ab4d82072b353b748c1,11,1,7,1297,,,0,"Always return copies from memory backend

To avoid the issue with conductors having raw access to the logbook
or flow details and atom details in the memory backend, and this causing
periodic inconsistencies with stored data ensure that we *always* return
copies from the memory backend instead to prevent this.

This matches the behavior of other backends which always return copies
of there underlying objects as well (partially because they do not
store the objects in a compatible format, ie in a database for the
database backend, or on a filesystem for the dir/file backend...).

Fixes bug 1365830

Also fixes a retry test case issue that was discovered due to this more
robust memory backend copy usage.

Change-Id: I9eeb674f7064aaa02a99418ca2b33c99debcf6e1
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/42/124242/2 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/persistence/backends/impl_memory.py', 'taskflow/persistence/logbook.py', 'taskflow/tests/unit/test_retries.py']",3,88273f57664d5409bcc4bfcb246320274f83f42c,bug/1365830, fail = misc.Failure.from_exception(RuntimeError('foo'))," fail = misc.Failure.from_exception(RuntimeError('foo')),",58,11
openstack%2Foslo.vmware~master~Id55b5deb5535ba5352b449a3b78099aa912a0b54,openstack/oslo.vmware,master,Id55b5deb5535ba5352b449a3b78099aa912a0b54,VimExceptions need to support i18n objects,MERGED,2014-09-17 16:15:15.000000000,2014-09-27 01:00:33.000000000,2014-09-27 01:00:32.000000000,"[{'_account_id': 3}, {'_account_id': 5638}, {'_account_id': 6873}, {'_account_id': 7198}, {'_account_id': 9171}]","[{'number': 1, 'created': '2014-09-17 16:15:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/9f824e182327b69d5f571cf1670a5e358afeb6ee', 'message': 'VimExceptions need to support i18n objects\n\nWhen lazy is enabled the i18n translation object does not support\nstr() which causes failures like:\n  UnicodeError: Message objects do not support str() because they may\n  contain non-ascii characters. Please use unicode() or translate()\n  instead.\n\nRemoved str() in __init__ and updated __str__ and __unicode__\nin VimException and VimFaultsException to correctly handle this object.\nAlso updated a couple of test cases to use an i18n translation object.\n\nChange-Id: Id55b5deb5535ba5352b449a3b78099aa912a0b54\n'}, {'number': 2, 'created': '2014-09-25 23:50:06.000000000', 'files': ['oslo/vmware/exceptions.py', 'tests/test_vim.py'], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/ee4d9c02d3a7f404aed967c6f9212da263b84c58', 'message': 'VimExceptions need to support i18n objects\n\nWhen lazy is enabled the i18n translation object does not support\nstr() which causes failures like:\n  UnicodeError: Message objects do not support str() because they may\n  contain non-ascii characters. Please use unicode() or translate()\n  instead.\n\nRemoved str() in __init__ and updated __str__ and __unicode__\nin VimException and VimFaultsException to correctly handle this object.\nAlso updated a couple of test cases to use an i18n translation object.\n\nCloses-Bug: #1374210\nChange-Id: Id55b5deb5535ba5352b449a3b78099aa912a0b54\n'}]",0,122193,ee4d9c02d3a7f404aed967c6f9212da263b84c58,13,5,2,6601,,,0,"VimExceptions need to support i18n objects

When lazy is enabled the i18n translation object does not support
str() which causes failures like:
  UnicodeError: Message objects do not support str() because they may
  contain non-ascii characters. Please use unicode() or translate()
  instead.

Removed str() in __init__ and updated __str__ and __unicode__
in VimException and VimFaultsException to correctly handle this object.
Also updated a couple of test cases to use an i18n translation object.

Closes-Bug: #1374210
Change-Id: Id55b5deb5535ba5352b449a3b78099aa912a0b54
",git fetch https://review.opendev.org/openstack/oslo.vmware refs/changes/93/122193/1 && git format-patch -1 --stdout FETCH_HEAD,"['oslo/vmware/exceptions.py', 'tests/test_vim.py']",2,9f824e182327b69d5f571cf1670a5e358afeb6ee,i18n_lazy,"from oslo import i18n from oslo.vmware._i18n import _ back_use_lazy = i18n._lazy.USE_LAZY i18n.enable_lazy() self.addCleanup(self._restore_use_lazy, back_use_lazy) def _restore_use_lazy(self, back_use_lazy): i18n._lazy.USE_LAZY = back_use_lazy e = exceptions.VimException(_(""string""), ValueError(""foo"")) vfe = exceptions.VimFaultException([ValueError(""example"")], _(""cause""))"," e = exceptions.VimException(""string"", ValueError(""foo"")) vfe = exceptions.VimFaultException([ValueError(""example"")], ""cause"")",20,9
openstack%2Fnova~master~I414637a31b0b540fefa6c1314be6e973e2d4a178,openstack/nova,master,I414637a31b0b540fefa6c1314be6e973e2d4a178,Fix SecurityGroupExists error when booting instances,MERGED,2014-09-18 01:18:20.000000000,2014-09-27 00:40:59.000000000,2014-09-27 00:40:56.000000000,"[{'_account_id': 3}, {'_account_id': 67}, {'_account_id': 1653}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 5292}, {'_account_id': 5367}, {'_account_id': 5754}, {'_account_id': 6172}, {'_account_id': 6849}, {'_account_id': 6873}, {'_account_id': 7369}, {'_account_id': 9008}, {'_account_id': 9545}, {'_account_id': 9578}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-18 01:18:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/eb2d616564e7e9ac8cda5f6c89308af623adce48', 'message': ""Fix SecurityGroupExists error when booting instances\n\nMake sure we properly handle the case when multiple threads check\nand create the default security group, if it hasn't existed yet.\nOne thread will succeed and others will fail (we have a unique\nconstraint in db), so just handle the exception to let instance\nboot proceed.\n\nChange-Id: I414637a31b0b540fefa6c1314be6e973e2d4a178\nCloses-Bug: #1370782\n""}, {'number': 2, 'created': '2014-09-25 00:06:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b02b390f9b049f543887b35fe7fdc8cead757e2f', 'message': ""Fix SecurityGroupExists error when booting instances\n\nMake sure we properly handle the case when multiple threads check\nand create the default security group, if it hasn't existed yet.\nOne thread will succeed and others will fail (we have a unique\nconstraint in db), so just handle the exception to let instance\nboot proceed.\n\nCloses-Bug: #1370782\n\nChange-Id: I414637a31b0b540fefa6c1314be6e973e2d4a178\n""}, {'number': 3, 'created': '2014-09-25 00:39:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/40bd5666e76a9edc85f62b094a2b8f2f69562d46', 'message': ""Fix SecurityGroupExists error when booting instances\n\nMake sure we properly handle the case when multiple threads check\nand create the default security group, if it hasn't existed yet.\nOne thread will succeed and others will fail (we have a unique\nconstraint in db to prevent a race condition), so just handle the\nexception to let instance boot proceed.\n\nSteps to reproduce:\n\n1) install OpenStack with Rally using DevStack:\n\n    https://github.com/stackforge/rally/tree/master/contrib/devstack\n\n2) fetch the Rally task:\n\n    http://boris-42.github.io/boot.yaml\n\n3) run it\n\n    rally -v task start boot.yaml\n\nCloses-Bug: #1370782\n\nChange-Id: I414637a31b0b540fefa6c1314be6e973e2d4a178\n""}, {'number': 4, 'created': '2014-09-25 07:38:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/670a8419f2750fab8e645f2d04182f76f5909f8f', 'message': ""Fix SecurityGroupExists error when booting instances\n\nMake sure we properly handle the case when multiple threads check\nand create the default security group, if it hasn't existed yet.\nOne thread will succeed and others will fail (we have a unique\nconstraint in db to prevent a race condition), so just handle the\nexception to let instance boot proceed.\n\nSteps to reproduce:\n\n1) install OpenStack with Rally using DevStack:\n\n    https://github.com/stackforge/rally/tree/master/contrib/devstack\n\n2) fetch the Rally task:\n\n    http://boris-42.github.io/boot.yaml\n\n3) run it\n\n    rally -v task start boot.yaml\n\nCloses-Bug: #1370782\n\nChange-Id: I414637a31b0b540fefa6c1314be6e973e2d4a178\n""}, {'number': 5, 'created': '2014-09-25 17:58:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b9291528826c2d60e178c1aa29b2addc244c0c84', 'message': ""Fix SecurityGroupExists error when booting instances\n\nMake sure we properly handle the case when multiple threads check\nand create the default security group, if it hasn't existed yet.\nOne thread will succeed and others will fail (we have a unique\nconstraint in db to prevent a race condition), so just handle the\nexception to let instance boot proceed.\n\nSteps to reproduce:\n\n1) install OpenStack with Rally using DevStack:\n\n    https://github.com/stackforge/rally/tree/master/contrib/devstack\n\n2) fetch the Rally task:\n\n    http://boris-42.github.io/boot.yaml\n\n3) run it\n\n    rally -v task start boot.yaml\n\nCloses-Bug: #1370782\n\nChange-Id: I414637a31b0b540fefa6c1314be6e973e2d4a178\n""}, {'number': 6, 'created': '2014-09-25 18:55:03.000000000', 'files': ['nova/tests/db/test_db_api.py', 'nova/db/sqlalchemy/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/b0e48fdaf7442a665d096bc934306d9517d921f0', 'message': ""Fix SecurityGroupExists error when booting instances\n\nMake sure we properly handle the case when multiple threads check\nand create the default security group, if it hasn't existed yet.\nOne thread will succeed and others will fail (we have a unique\nconstraint in db to prevent a race condition), so just handle the\nexception to let instance boot proceed.\n\nSteps to reproduce:\n\n1) install OpenStack with Rally using DevStack:\n\n    https://github.com/stackforge/rally/tree/master/contrib/devstack\n\n2) fetch the Rally task:\n\n    http://boris-42.github.io/boot.yaml\n\n3) run it\n\n    rally -v task start boot.yaml\n\nCloses-Bug: #1370782\n\nChange-Id: I414637a31b0b540fefa6c1314be6e973e2d4a178\n""}]",31,122291,b0e48fdaf7442a665d096bc934306d9517d921f0,74,16,6,6849,,,0,"Fix SecurityGroupExists error when booting instances

Make sure we properly handle the case when multiple threads check
and create the default security group, if it hasn't existed yet.
One thread will succeed and others will fail (we have a unique
constraint in db to prevent a race condition), so just handle the
exception to let instance boot proceed.

Steps to reproduce:

1) install OpenStack with Rally using DevStack:

    https://github.com/stackforge/rally/tree/master/contrib/devstack

2) fetch the Rally task:

    http://boris-42.github.io/boot.yaml

3) run it

    rally -v task start boot.yaml

Closes-Bug: #1370782

Change-Id: I414637a31b0b540fefa6c1314be6e973e2d4a178
",git fetch https://review.opendev.org/openstack/nova refs/changes/91/122291/3 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/db/test_db_api.py', 'nova/db/sqlalchemy/api.py']",2,eb2d616564e7e9ac8cda5f6c89308af623adce48,bug/1370782,"class suppress_security_group_exists_error(object): def __init__(self, name): self.name = name def __call__(self, func): @functools.wraps(func) def wrapper(context, *args, **kwargs): try: return func(context, *args, **kwargs) except exception.SecurityGroupExists as e: if e.security_group_name == self.name: return security_group_get_by_name(context, context.project_id, e.security_group_name) raise return wrapper @suppress_security_group_exists_error(name='default')",,43,0
openstack%2Fdesignate~master~I64c20a5b34bcf64d66d283215d32d8189f5a4d41,openstack/designate,master,I64c20a5b34bcf64d66d283215d32d8189f5a4d41,Updated from global requirements,MERGED,2014-09-26 03:53:15.000000000,2014-09-26 23:44:34.000000000,2014-09-26 23:44:34.000000000,"[{'_account_id': 3}, {'_account_id': 741}, {'_account_id': 8130}]","[{'number': 1, 'created': '2014-09-26 03:53:15.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/designate/commit/2f87b9456e6a26ad9018657698b9ea8e3cc89ead', 'message': 'Updated from global requirements\n\nChange-Id: I64c20a5b34bcf64d66d283215d32d8189f5a4d41\n'}]",0,124257,2f87b9456e6a26ad9018657698b9ea8e3cc89ead,7,3,1,11131,,,0,"Updated from global requirements

Change-Id: I64c20a5b34bcf64d66d283215d32d8189f5a4d41
",git fetch https://review.opendev.org/openstack/designate refs/changes/57/124257/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,2f87b9456e6a26ad9018657698b9ea8e3cc89ead,openstack/requirements,kombu>=2.5.0,kombu>=2.4.8,1,1
openstack%2Fneutron~master~I3a6d2fd5b18a97138ac5b699ecd4d4b30dbdbacb,openstack/neutron,master,I3a6d2fd5b18a97138ac5b699ecd4d4b30dbdbacb,Only setup dhcp interface if dhcp is not active on network,MERGED,2014-09-18 14:57:58.000000000,2014-09-26 23:37:47.000000000,2014-09-21 00:21:03.000000000,"[{'_account_id': 3}, {'_account_id': 704}, {'_account_id': 841}, {'_account_id': 4187}, {'_account_id': 5170}, {'_account_id': 6854}, {'_account_id': 7787}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-18 14:57:58.000000000', 'files': ['neutron/agent/linux/dhcp.py', 'neutron/tests/unit/test_linux_dhcp.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/d144cb8ff422e16c5917c87471a46fd095fd1856', 'message': ""Only setup dhcp interface if dhcp is not active on network\n\nWhen enabling (DhcpLocalProcess.enable()) dhcp for a network the agent\nfirst sets dhcp interface, then checks if dhcp is curently active and\nif it's true then the agent restarts dhcp.\nRestart (DhcpBase.restart()) first disables dhcp and then enables it again\nby calling DhcpLocalProcess.enable() recursively which in turn sets\ndhcp interface again (it doesn't see the port created earlier as network\nis not re-fetched from db). This leads to duplicate dhcp interface\nfor the network.\n\nThe fix is to only setup dhcp interface if dhcp is not active.\n\nCloses-Bug: #1339968\n\nChange-Id: I3a6d2fd5b18a97138ac5b699ecd4d4b30dbdbacb\n""}]",0,122442,d144cb8ff422e16c5917c87471a46fd095fd1856,28,21,1,5948,,,0,"Only setup dhcp interface if dhcp is not active on network

When enabling (DhcpLocalProcess.enable()) dhcp for a network the agent
first sets dhcp interface, then checks if dhcp is curently active and
if it's true then the agent restarts dhcp.
Restart (DhcpBase.restart()) first disables dhcp and then enables it again
by calling DhcpLocalProcess.enable() recursively which in turn sets
dhcp interface again (it doesn't see the port created earlier as network
is not re-fetched from db). This leads to duplicate dhcp interface
for the network.

The fix is to only setup dhcp interface if dhcp is not active.

Closes-Bug: #1339968

Change-Id: I3a6d2fd5b18a97138ac5b699ecd4d4b30dbdbacb
",git fetch https://review.opendev.org/openstack/neutron refs/changes/42/122442/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/agent/linux/dhcp.py', 'neutron/tests/unit/test_linux_dhcp.py']",2,d144cb8ff422e16c5917c87471a46fd095fd1856,bug/1339968, self.assertFalse(self.mock_mgr.return_value.setup.called),,2,1
openstack%2Fheat~master~I558db6bac196f49e5c488a577f0580c934b06747,openstack/heat,master,I558db6bac196f49e5c488a577f0580c934b06747,Default port policy to force replacement,MERGED,2014-09-15 22:25:54.000000000,2014-09-26 23:36:32.000000000,2014-09-26 23:36:31.000000000,"[{'_account_id': 3}, {'_account_id': 4257}, {'_account_id': 4571}, {'_account_id': 4715}, {'_account_id': 7193}, {'_account_id': 8289}]","[{'number': 1, 'created': '2014-09-15 22:25:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/75e235375330d4bd324a60884eb8f75f8c27939f', 'message': ""Default port update policy to force replacement\n\nThis change adds an update_policy property to OS::Neutron::Port\nwhich defaults to always replacing the port on stack-update\nregardless of any changes to the template.\n\nCurrently when a server is replaced, the new server is booted with\nthe port that is still attached to the old server, which raises a\nport-still-in-use error.\n\nEven if heat managed to detach and attach a single port, Nova currently\ndeletes all ports on interface-detach and server delete, so the port\nwould no longer be available anyway.\n\nThis change ensures that a new server is always booted with a new port,\nso it fixes the above 2 issues, however there are implications for\nother scenarios.\n\nIf the server is not replaced during the stack-update, the server\nhandle_update will detach/attach to the new port, so this is fine.\n\nIf the port specifies a fixed_ips ip_address which doesn't change\nduring stack-update, an error will be raised that 2 ports exist\nwith the same IP address. The only current workaround would be\nto set update_policy:AUTO and not make any changes to the server\nwhich results in server replacement.\n\nLikewise, update_policy:AUTO will need to be set on any port\nconsumed by resources which don't support update without replacement\n(such as OS::Neutron::FloatingIP and OS::Trove::Instance)\n\nChange-Id: I558db6bac196f49e5c488a577f0580c934b06747\nCloses-Bug: #1301486\nRelated-Bug: #1158684\nRelated-Bug: #1369748\n""}, {'number': 2, 'created': '2014-09-18 21:40:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/7eadc5ac0637219af37bcad24a2f83e7d05409e3', 'message': ""Default port policy to force replacement\n\nThis change adds a replacement_policy property to OS::Neutron::Port\nwhich defaults to always replacing the port on stack-update\nregardless of any changes to the template.\n\nCurrently when a server is replaced, the new server is booted with\nthe port that is still attached to the old server, which raises a\nport-still-in-use error.\n\nEven if heat managed to detach and attach a single port, Nova currently\ndeletes all ports on interface-detach and server delete, so the port\nwould no longer be available anyway.\n\nThis change ensures that a new server is always booted with a new port,\nso it fixes the above 2 issues, however there are implications for\nother scenarios.\n\nIf the server is not replaced during the stack-update, the server\nhandle_update will detach/attach to the new port, so this is fine.\n\nIf the port specifies a fixed_ips ip_address which doesn't change\nduring stack-update, an error will be raised that 2 ports exist\nwith the same IP address. The only current workaround would be\nto set update_policy:AUTO and not make any changes to the server\nwhich results in server replacement (or do 2 stack updates using\na transition ip_address).\n\nLikewise, update_policy:AUTO will need to be set on any port\nconsumed by resources which don't support update without replacement\n(such as OS::Neutron::FloatingIP and OS::Trove::Instance)\n\nChange-Id: I558db6bac196f49e5c488a577f0580c934b06747\nCloses-Bug: #1301486\nRelated-Bug: #1158684\nRelated-Bug: #1369748\n""}, {'number': 3, 'created': '2014-09-25 00:31:48.000000000', 'files': ['heat/engine/resources/neutron/port.py', 'heat/tests/test_neutron.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/30ece56a2118bff073e36a108324d12ee646fab6', 'message': ""Default port policy to force replacement\n\nThis change adds a replacement_policy property to OS::Neutron::Port\nwhich defaults to always replacing the port on stack-update\nregardless of any changes to the template.\n\nCurrently when a server is replaced, the new server is booted with\nthe port that is still attached to the old server, which raises a\nport-still-in-use error.\n\nEven if heat managed to detach and attach a single port, Nova currently\ndeletes all ports on interface-detach and server delete, so the port\nwould no longer be available anyway.\n\nThis change ensures that a new server is always booted with a new port,\nso it fixes the above 2 issues, however there are implications for\nother scenarios.\n\nIf the server is not replaced during the stack-update, the server\nhandle_update will detach/attach to the new port, so this is fine.\n\nIf the port specifies a fixed_ips ip_address which doesn't change\nduring stack-update, an error will be raised that 2 ports exist\nwith the same IP address. The only current workaround would be\nto set update_policy:AUTO and not make any changes to the server\nwhich results in server replacement (or do 2 stack updates using\na transition ip_address).\n\nLikewise, update_policy:AUTO will need to be set on any port\nconsumed by resources which don't support update without replacement\n(such as OS::Neutron::FloatingIP and OS::Trove::Instance)\n\nChange-Id: I558db6bac196f49e5c488a577f0580c934b06747\nCloses-Bug: #1301486\nRelated-Bug: #1158684\nRelated-Bug: #1369748\n""}]",5,121693,30ece56a2118bff073e36a108324d12ee646fab6,22,6,3,4571,,,0,"Default port policy to force replacement

This change adds a replacement_policy property to OS::Neutron::Port
which defaults to always replacing the port on stack-update
regardless of any changes to the template.

Currently when a server is replaced, the new server is booted with
the port that is still attached to the old server, which raises a
port-still-in-use error.

Even if heat managed to detach and attach a single port, Nova currently
deletes all ports on interface-detach and server delete, so the port
would no longer be available anyway.

This change ensures that a new server is always booted with a new port,
so it fixes the above 2 issues, however there are implications for
other scenarios.

If the server is not replaced during the stack-update, the server
handle_update will detach/attach to the new port, so this is fine.

If the port specifies a fixed_ips ip_address which doesn't change
during stack-update, an error will be raised that 2 ports exist
with the same IP address. The only current workaround would be
to set update_policy:AUTO and not make any changes to the server
which results in server replacement (or do 2 stack updates using
a transition ip_address).

Likewise, update_policy:AUTO will need to be set on any port
consumed by resources which don't support update without replacement
(such as OS::Neutron::FloatingIP and OS::Trove::Instance)

Change-Id: I558db6bac196f49e5c488a577f0580c934b06747
Closes-Bug: #1301486
Related-Bug: #1158684
Related-Bug: #1369748
",git fetch https://review.opendev.org/openstack/heat refs/changes/93/121693/2 && git format-patch -1 --stdout FETCH_HEAD,['heat/engine/resources/neutron/port.py'],1,75e235375330d4bd324a60884eb8f75f8c27939f,bug/1301486,"from heat.engine import constraintsfrom heat.engine import resource DEVICE_OWNER, UPDATE_POLICY, 'device_owner', 'update_policy', UPDATE_POLICY: properties.Schema( properties.Schema.STRING, _('Policy on how to respond to a stack-update for this resource. ' 'REPLACE_ALWAYS will replace the port regardless of any ' 'property changes. AUTO will update the existing port for any ' 'changed update-allowed property.'), default='REPLACE_ALWAYS', constraints=[ constraints.AllowedValues(['REPLACE_ALWAYS', 'AUTO']), ], update_allowed=True ), self._prepare_port_properties(props) def _prepare_port_properties(self, props): del(props[self.UPDATE_POLICY]) def _needs_update(self, after, before, after_props, before_props, prev_resource): if after_props.get(self.UPDATE_POLICY) == 'REPLACE_ALWAYS': raise resource.UpdateReplace(self.name) return super(Port, self)._needs_update( after, before, after_props, before_props, prev_resource) self._prepare_port_properties(props)"," DEVICE_OWNER, 'device_owner', self._prepare_list_properties(props) def _prepare_list_properties(self, props): self._prepare_list_properties(props)",30,5
openstack%2Ftempest~master~Id24857ee95445570d45aac3e1a25e607d8df78d1,openstack/tempest,master,Id24857ee95445570d45aac3e1a25e607d8df78d1,Add response schema for V2 sec grp default rule,MERGED,2014-09-09 04:22:27.000000000,2014-09-26 23:35:35.000000000,2014-09-26 23:35:34.000000000,"[{'_account_id': 3}, {'_account_id': 5292}, {'_account_id': 6167}, {'_account_id': 8556}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-09 04:22:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/3aa53614c2fbca931afd1dc7265c1f12ad1796bb', 'message': 'Add response schema for V2 sec grp default rule\n\nThis patch adds response schema for nova V2\n\'os-security-group-default-rules\' create/delete API.\n\nResponse body of create \'os-security-group-default-rules\' -\n{\n    ""security_group_default_rule"": {\n        ""from_port"": 80,\n        ""id"": 1,\n        ""ip_protocol"": ""TCP"",\n        ""ip_range"":{\n            ""cidr"": ""10.10.10.0/24""\n        },\n        ""to_port"": 80\n    }\n}\n\nDelete API only returns 204 status code.\n\nChange-Id: Id24857ee95445570d45aac3e1a25e607d8df78d1\n'}, {'number': 2, 'created': '2014-09-09 05:48:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/32fcc1ad9679c1c97ebbdbac2331916e73853246', 'message': 'Add response schema for V2 sec grp default rule\n\nThis patch adds response schema for nova V2\n\'os-security-group-default-rules\' APIs.\n\nResponse body of create/show \'os-security-group-default-rules\' -\n{\n    ""security_group_default_rule"": {\n        ""from_port"": 80,\n        ""id"": 1,\n        ""ip_protocol"": ""TCP"",\n        ""ip_range"":{\n            ""cidr"": ""10.10.10.0/24""\n        },\n        ""to_port"": 80\n    }\n}\n\nResponse body of list \'os-security-group-default-rules\' -\n{\n    ""security_group_default_rules"": [\n        {\n            ""from_port"": 80,\n            ""id"": 1,\n            ""ip_protocol"": ""TCP"",\n            ""ip_range"": {\n                ""cidr"": ""10.10.10.0/24""\n            },\n            ""to_port"": 80\n        }\n    ]\n}\n\nDelete API only returns 204 status code.\n\nChange-Id: Id24857ee95445570d45aac3e1a25e607d8df78d1\n'}, {'number': 3, 'created': '2014-09-16 06:57:14.000000000', 'files': ['tempest/api_schema/response/compute/v2/security_group_default_rule.py', 'tempest/services/compute/json/security_group_default_rules_client.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/addb0ea421f00f76b03eda0cbb70ae43dd27275a', 'message': 'Add response schema for V2 sec grp default rule\n\nThis patch adds response schema for nova V2\n\'os-security-group-default-rules\' APIs.\n\nResponse body of create/show \'os-security-group-default-rules\' -\n{\n    ""security_group_default_rule"": {\n        ""from_port"": 80,\n        ""id"": 1,\n        ""ip_protocol"": ""TCP"",\n        ""ip_range"":{\n            ""cidr"": ""10.10.10.0/24""\n        },\n        ""to_port"": 80\n    }\n}\n\nResponse body of list \'os-security-group-default-rules\' -\n{\n    ""security_group_default_rules"": [\n        {\n            ""from_port"": 80,\n            ""id"": 1,\n            ""ip_protocol"": ""TCP"",\n            ""ip_range"": {\n                ""cidr"": ""10.10.10.0/24""\n            },\n            ""to_port"": 80\n        }\n    ]\n}\n\nDelete API only returns 204 status code.\n\nChange-Id: Id24857ee95445570d45aac3e1a25e607d8df78d1\n'}]",5,119986,addb0ea421f00f76b03eda0cbb70ae43dd27275a,16,5,3,8556,,,0,"Add response schema for V2 sec grp default rule

This patch adds response schema for nova V2
'os-security-group-default-rules' APIs.

Response body of create/show 'os-security-group-default-rules' -
{
    ""security_group_default_rule"": {
        ""from_port"": 80,
        ""id"": 1,
        ""ip_protocol"": ""TCP"",
        ""ip_range"":{
            ""cidr"": ""10.10.10.0/24""
        },
        ""to_port"": 80
    }
}

Response body of list 'os-security-group-default-rules' -
{
    ""security_group_default_rules"": [
        {
            ""from_port"": 80,
            ""id"": 1,
            ""ip_protocol"": ""TCP"",
            ""ip_range"": {
                ""cidr"": ""10.10.10.0/24""
            },
            ""to_port"": 80
        }
    ]
}

Delete API only returns 204 status code.

Change-Id: Id24857ee95445570d45aac3e1a25e607d8df78d1
",git fetch https://review.opendev.org/openstack/tempest refs/changes/86/119986/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api_schema/response/compute/v2/security_group_default_rule.py', 'tempest/services/compute/json/security_group_default_rules_client.py']",2,3aa53614c2fbca931afd1dc7265c1f12ad1796bb,(detached,"from tempest.api_schema.response.compute.v2 import \ security_group_default_rule as schema self.validate_response(schema.create_security_group_default_rule, resp, body) self.validate_response(schema.delete_security_group_default_rule, resp, body)"," self.expected_success(200, resp.status) self.expected_success(204, resp.status)",53,2
openstack%2Fglance_store~master~Ie01bbf33db5a5ba519c5701ef9475fc0437cd6de,openstack/glance_store,master,Ie01bbf33db5a5ba519c5701ef9475fc0437cd6de,Backporting S3 multi-part upload functionality to glace_store,MERGED,2014-09-11 07:57:02.000000000,2014-09-26 23:35:32.000000000,2014-09-26 23:35:31.000000000,"[{'_account_id': 3}, {'_account_id': 616}, {'_account_id': 6159}, {'_account_id': 6549}, {'_account_id': 8517}]","[{'number': 1, 'created': '2014-09-11 07:57:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/131be614ff2fdb604d2233dfd4adfc54f681ee1d', 'message': 'Backporting S3 multi-part upload functionality to glace_store\n\nThis commits is to backport S3 multi-part upload functionality\nfrom the old glance store package to glace_store library.\n\nChange-Id: Ie01bbf33db5a5ba519c5701ef9475fc0437cd6de\n'}, {'number': 2, 'created': '2014-09-11 08:12:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/caaf1643a6aa4c515685a59515e6dff3ea8f35ad', 'message': 'Backporting S3 multi-part upload functionality to glace_store\n\nThis commits is to backport S3 multi-part upload functionality\nfrom the old glance store package to glace_store library.\n\nChange-Id: Ie01bbf33db5a5ba519c5701ef9475fc0437cd6de\n'}, {'number': 3, 'created': '2014-09-16 07:06:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/a36924bb045f046d59b28ae4896f821dc2a516d6', 'message': 'Backporting S3 multi-part upload functionality to glace_store\n\nThis commits is to backport S3 multi-part upload functionality\nfrom the old glance store package to glace_store library.\n\nChange-Id: Ie01bbf33db5a5ba519c5701ef9475fc0437cd6de\n'}, {'number': 4, 'created': '2014-09-16 07:50:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/ba6cf861785dc8fbe7bb25f5c311daaa2fc095c3', 'message': 'Backporting S3 multi-part upload functionality to glace_store\n\nThis commits is to backport S3 multi-part upload functionality\nfrom the old glance store package to glace_store library.\n\nChange-Id: Ie01bbf33db5a5ba519c5701ef9475fc0437cd6de\n'}, {'number': 5, 'created': '2014-09-17 06:31:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/eb22e4a7ee00eb8ec8ee7f76efb63d7158d48a28', 'message': 'Backporting S3 multi-part upload functionality to glace_store\n\nThis commits is to backport S3 multi-part upload functionality\nfrom the old glance store package to glace_store library.\n\nChange-Id: Ie01bbf33db5a5ba519c5701ef9475fc0437cd6de\n'}, {'number': 6, 'created': '2014-09-18 07:08:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/8d067443954835968ede93fc55e69e274d00d67a', 'message': 'Backporting S3 multi-part upload functionality to glace_store\n\nThis commits is to backport S3 multi-part upload functionality\nfrom the old glance store package to glace_store library.\n\nIn addition, refactored the write chunk processing to S3 for\na large object and dynamic upload part size calculation to\nbe able to store large objects via S3 multi-part upload.\n\nChange-Id: Ie01bbf33db5a5ba519c5701ef9475fc0437cd6de\n'}, {'number': 7, 'created': '2014-09-25 09:21:19.000000000', 'files': ['glance_store/_drivers/s3.py', 'tests/unit/test_s3_store.py', 'tests/unit/test_opts.py'], 'web_link': 'https://opendev.org/openstack/glance_store/commit/615641698e61e4759cdfe51031e18a892ccec3a8', 'message': 'Backporting S3 multi-part upload functionality to glace_store\n\nThis commits is to backport S3 multi-part upload functionality\nfrom the old glance store package to glace_store library.\n\nIn addition, refactored the write chunk processing to S3 for\na large object and dynamic upload part size calculation to\nbe able to store large objects via S3 multi-part upload.\n\nChange-Id: Ie01bbf33db5a5ba519c5701ef9475fc0437cd6de\n'}]",6,120714,615641698e61e4759cdfe51031e18a892ccec3a8,19,5,7,8517,,,0,"Backporting S3 multi-part upload functionality to glace_store

This commits is to backport S3 multi-part upload functionality
from the old glance store package to glace_store library.

In addition, refactored the write chunk processing to S3 for
a large object and dynamic upload part size calculation to
be able to store large objects via S3 multi-part upload.

Change-Id: Ie01bbf33db5a5ba519c5701ef9475fc0437cd6de
",git fetch https://review.opendev.org/openstack/glance_store refs/changes/14/120714/4 && git format-patch -1 --stdout FETCH_HEAD,"['glance_store/_drivers/s3.py', 'tests/unit/test_s3_store.py']",2,131be614ff2fdb604d2233dfd4adfc54f681ee1d,backport/s3multipart,"import xml.etree.ElementTree 's3_store_bucket': 'glance', 's3_store_large_object_size': 5, # over 5MB is large 's3_store_large_object_chunk_size': 6} # part size is 6MB # ensure that mpu api is used and parts are uploaded as expected mpu_parts_uploaded = 0 self.etag = None max_read = kwargs.get('size') checksum = hashlib.md5() while True: if max_read is None or max_read > self.BufferSize: read_size = self.BufferSize elif max_read <= 0: break else: read_size = max_read chunk = fp.read(read_size) if not chunk: break checksum.update(chunk) self.data.write(chunk) if max_read is not None: max_read -= len(chunk) self.etag = checksum.hexdigest()class FakeMPU: """""" Acts like a ``boto.s3.multipart.MultiPartUpload`` """""" def __init__(self, bucket, key_name): self.bucket = bucket self.id = str(uuid.uuid4()) self.key_name = key_name self.parts = {} # pnum -> FakeKey global mpu_parts_uploaded mpu_parts_uploaded = 0 def upload_part_from_file(self, fp, part_num, **kwargs): size = kwargs.get('size') part = FakeKey(self.bucket, self.key_name) part.set_contents_from_file(fp, size=size) self.parts[part_num] = part global mpu_parts_uploaded mpu_parts_uploaded += 1 return part def verify_xml(self, xml_body): """""" Verify xml matches our part info. """""" xmlparts = {} cmuroot = xml.etree.ElementTree.fromstring(xml_body) for cmupart in cmuroot: pnum = int(cmupart.findtext('PartNumber')) etag = cmupart.findtext('ETag') xmlparts[pnum] = etag if len(xmlparts) != len(self.parts): return False for pnum in xmlparts.keys(): if self.parts[pnum] is None: return False if xmlparts[pnum] != self.parts[pnum].etag: return False return True def complete_key(self): """""" Complete the parts into one big FakeKey """""" key = FakeKey(self.bucket, self.key_name) key.data = StringIO.StringIO() checksum = hashlib.md5() cnt = 0 for pnum in sorted(self.parts.keys()): cnt += 1 part = self.parts[pnum] chunk = part.data.read(key.BufferSize) while chunk: checksum.update(chunk) key.data.write(chunk) chunk = part.data.read(key.BufferSize) key.size = key.data.len key.data.seek(0) key.etag = checksum.hexdigest() + '-%d' % cnt key.read = key.data.read return key self.mpus = {} # {key_name -> {id -> FakeMPU}} def initiate_multipart_upload(self, key_name, **kwargs): mpu = FakeMPU(self, key_name) if key_name not in self.mpus: self.mpus[key_name] = {} self.mpus[key_name][mpu.id] = mpu return mpu def cancel_multipart_upload(self, key_name, upload_id, **kwargs): if key_name in self.mpus: if upload_id in self.mpus[key_name]: del self.mpus[key_name][upload_id] if not self.mpus[key_name]: del self.mpus[key_name] def complete_multipart_upload(self, key_name, upload_id, xml_body, **kwargs): if key_name in self.mpus: if upload_id in self.mpus[key_name]: mpu = self.mpus[key_name][upload_id] if mpu.verify_xml(xml_body): key = mpu.complete_key() self.cancel_multipart_upload(key_name, upload_id) self.keys[key_name] = key cmpu = mock.Mock() cmpu.bucket = self cmpu.bucket_name = self.name cmpu.key_name = key_name cmpu.etag = key.etag return cmpu return None # tho raising an exception might be better def test_add_size_variations(self): """""" Test that adding images of various sizes which exercise both S3 single uploads and the multipart upload apis. We've configured the big upload threshold to 5MB and the part size to 6MB. """""" variations = [(FIVE_KB, 0), # simple put (5KB < 5MB) (5242880, 1), # 1 part (5MB <= 5MB < 6MB) (6291456, 1), # 1 part exact (5MB <= 6MB <= 6MB) (7340032, 2)] # 2 parts (6MB < 7MB <= 12MB) for (vsize, vcnt) in variations: expected_image_id = str(uuid.uuid4()) expected_s3_size = vsize expected_s3_contents = ""12345678"" * (expected_s3_size / 8) expected_chksum = hashlib.md5(expected_s3_contents).hexdigest() expected_location = format_s3_location( S3_CONF['s3_store_access_key'], S3_CONF['s3_store_secret_key'], S3_CONF['s3_store_host'], S3_CONF['s3_store_bucket'], expected_image_id) image_s3 = StringIO.StringIO(expected_s3_contents) # add image location, size, chksum, _ = self.store.add(expected_image_id, image_s3, expected_s3_size) self.assertEqual(expected_location, location) self.assertEqual(expected_s3_size, size) self.assertEqual(expected_chksum, chksum) self.assertEqual(vcnt, mpu_parts_uploaded) # get image loc = get_location_from_uri(expected_location) (new_image_s3, new_image_s3_size) = self.store.get(loc) new_image_contents = StringIO.StringIO() for chunk in new_image_s3: new_image_contents.write(chunk) new_image_size = new_image_contents.len self.assertEqual(expected_s3_size, new_image_s3_size) self.assertEqual(expected_s3_size, new_image_size) self.assertEqual(expected_s3_contents, new_image_contents.getvalue()) ", 's3_store_bucket': 'glance'} for bytes in fp: self.data.write(bytes),391,48
openstack%2Fnova~master~Id8033fb8880306babebc14b319bce5b3b8798774,openstack/nova,master,Id8033fb8880306babebc14b319bce5b3b8798774,Fix `confirmResize` action status code in V2,MERGED,2014-09-12 08:28:34.000000000,2014-09-26 23:35:11.000000000,2014-09-26 23:35:08.000000000,"[{'_account_id': 3}, {'_account_id': 5170}, {'_account_id': 5292}, {'_account_id': 5754}, {'_account_id': 6062}, {'_account_id': 6167}, {'_account_id': 8556}, {'_account_id': 9008}, {'_account_id': 9545}, {'_account_id': 9578}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-12 08:28:34.000000000', 'files': ['nova/api/openstack/compute/servers.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/21baedfc3185111589535cdc24fff83603a5e3fc', 'message': 'Fix `confirmResize` action status code in V2\n\nIn server`s action `confirmResize` status code in @wsgi.response\ndecorator is set as 202 but this is overridden/ignored by return\nstatement (return exc.HTTPNoContent()) which return 204 status code.\n\nThis is very confusing and we should have expected status code in\n@wsgi.response decorator as consistence with other APIs.\n\nNOTE- There is no change in API return status code. API returns the\nsame 204 return code as previously.\n\nChange-Id: Id8033fb8880306babebc14b319bce5b3b8798774\nCloses-Bug: #1368597\n'}]",0,121025,21baedfc3185111589535cdc24fff83603a5e3fc,17,11,1,8556,,,0,"Fix `confirmResize` action status code in V2

In server`s action `confirmResize` status code in @wsgi.response
decorator is set as 202 but this is overridden/ignored by return
statement (return exc.HTTPNoContent()) which return 204 status code.

This is very confusing and we should have expected status code in
@wsgi.response decorator as consistence with other APIs.

NOTE- There is no change in API return status code. API returns the
same 204 return code as previously.

Change-Id: Id8033fb8880306babebc14b319bce5b3b8798774
Closes-Bug: #1368597
",git fetch https://review.opendev.org/openstack/nova refs/changes/25/121025/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/api/openstack/compute/servers.py'],1,21baedfc3185111589535cdc24fff83603a5e3fc,confirm_resize_status_code, @wsgi.response(204), @wsgi.response(202) return exc.HTTPNoContent(),1,2
openstack%2Fneutron~master~I984eb76069bd1493a77bf523bec2bd81abb14abb,openstack/neutron,master,I984eb76069bd1493a77bf523bec2bd81abb14abb,Stop admin using other tenants unshared rules,MERGED,2014-07-23 10:43:04.000000000,2014-09-26 23:22:01.000000000,2014-09-26 23:02:22.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 261}, {'_account_id': 490}, {'_account_id': 704}, {'_account_id': 748}, {'_account_id': 841}, {'_account_id': 2592}, {'_account_id': 2711}, {'_account_id': 5170}, {'_account_id': 6659}, {'_account_id': 7293}, {'_account_id': 7448}, {'_account_id': 7787}, {'_account_id': 8124}, {'_account_id': 8213}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10068}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10370}, {'_account_id': 10386}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10624}, {'_account_id': 10692}, {'_account_id': 10781}, {'_account_id': 10797}, {'_account_id': 11614}, {'_account_id': 11774}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-07-23 10:43:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/65998207b95b14f0d3317abd9b623f949bca80e0', 'message': ""Admin should not be able to use tenant's rule\n\nAdmin tenant should not be able to create Firewall policy with tenant's\nfirewall rule\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 2, 'created': '2014-07-24 09:04:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/2c2629a9b9d445904bac2ec06f65c3db48c575d5', 'message': ""Admin should not be able to use tenant's rule\n\nAdmin tenant should not be able to create Firewall policy with tenant's\nfirewall rule\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 3, 'created': '2014-08-04 07:13:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/15162e88eeed976be6d019bbc05def5362aa4032', 'message': ""Admin should not be able to use tenant's rule\n\nAdmin tenant should not be able to create Firewall policy with tenant's\nfirewall rule\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 4, 'created': '2014-08-05 10:28:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/426487cf97616ac774653c5d04bc13e69ebcc6d3', 'message': ""Admin should not be able to use tenant's rule\n\nAdmin tenant should not be able to create Firewall policy with tenant's\nfirewall rule\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 5, 'created': '2014-08-11 10:40:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/a6707ed3086cc139dcd2d182c554e3922afc58be', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if it belongs to different tenants,\nadmin should not be able to create policy using these rules. An exception\nshould be raised in such case. Added new FirewallRuleConflict exception to\nhandle this condition.\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 6, 'created': '2014-08-12 06:01:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/b42e8ea781f391b8ed5fe917c39949655ae404c3', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if it belongs to different tenants,\nthen admin should not be able to create a policy using these rules. An\nexception should be raised in such case. Added new FirewallRuleConflict\nexception to handle this condition.\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 7, 'created': '2014-08-18 09:55:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ad305fca971fdc4108c369a863dc78d29775db06', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if it belongs to different tenants,\nthen admin should not be able to create a policy using these rules and he\nshould not be able to insert such rules to policies.An exception should\nbe raised in such case. Added new FirewallRuleConflictexception to\nhandle such conditions.\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 8, 'created': '2014-08-20 03:39:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/03a5337a27b0215c27199b20262f11c59926fc59', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if they belong to different tenants,\nthen admin should not be able to create a policy using these rules and he\nshould not be able to insert such rules into policies. An exception should\nbe raised in such case. Added new FirewallRuleConflict exception to\nhandle such conditions.\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 9, 'created': '2014-08-21 05:52:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/a0b5856dc7cde5ecdf7efff07464e6caa1969fac', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if they belong to different tenants,\nthen admin should not be able to create a policy using these rules and he\nshould not be able to insert such rules into policies. An exception should\nbe raised in such case. Added new FirewallRuleConflict exception to\nhandle such conditions.\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 10, 'created': '2014-08-25 06:00:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/23557965e64b1b7add7f060ad88a2226650b25a8', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if they belong to different tenants,\nthen admin should not be able to create a policy using these rules and he\nshould not be able to insert such rules into policies. An exception should\nbe raised in such case. Added new FirewallRuleConflict exception to\nhandle such conditions.\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 11, 'created': '2014-08-25 06:18:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ce93fe72ea3ba664beae13a9f768a9fbae4a8837', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if they belong to different tenants,\nthen admin should not be able to create a policy using these rules and he\nshould not be able to insert such rules into policies. An exception should\nbe raised in such case. Added new FirewallRuleConflict exception to\nhandle such conditions.\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 12, 'created': '2014-09-01 11:37:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/a29ce5fdfea4b7249e28534debfab0102126d39d', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if they belong to different tenants,\nthen admin should not be able to create a policy using these rules and he\nshould not be able to insert such rules into policies. An exception should\nbe raised in such case. Added new FirewallRuleConflict exception to\nhandle such conditions.\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 13, 'created': '2014-09-04 06:02:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/6a219b71e187c88cbba7d465d9dc55be512bae00', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if they belong to different tenants,\nthen admin should not be able to create a policy using these rules and he\nshould not be able to insert such rules into policies. An exception should\nbe raised in such case. Added new FirewallRuleConflict exception to\nhandle such conditions.\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 14, 'created': '2014-09-04 06:07:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/acd5d61aa29430918636282fe5922b06c7bb582a', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if they belong to different\ntenants, then admin should not be able to create a policy using\nthese rules and he should not be able to insert such rules into\npolicies. An exception should be raised in such case. Added new\nFirewallRuleConflict exception to handle such conditions.\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 15, 'created': '2014-09-17 10:17:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/b354fcd3cb212ecd7dc7f4dd5b99d7185a363759', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if they belong to different\ntenants, then admin should not be able to create a policy using\nthese rules and he should not be able to insert such rules into\npolicies. An exception should be raised in such case. Added new\nexception FirewallRuleConflict to handle such conditions.\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 16, 'created': '2014-09-17 13:40:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/689f5f5c4b6c91c522fc66af7f0e1a026d9047e0', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if they belong to different\ntenants, then admin should not be able to create a policy using\nthese rules and he should not be able to insert such rules into\npolicies. An exception should be raised in such case. Added new\nexception FirewallRuleConflict to handle such conditions.\n\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 17, 'created': '2014-09-18 06:39:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/87037326e1637902a0dbb2875e15a1d3158d7b48', 'message': ""Adm policy can't use other tenants unshared rules\n\nIf the firewall rules are not shared and if they belong to different\ntenants, then admin should not be able to create a policy using\nthese rules and he should not be able to insert such rules into\npolicies. An exception should be raised in such case. Added new\nexception FirewallRuleConflict to handle such conditions.\n\nCo-Authored-By: Koteswara Rao Kelam<koteswara.kelam@hp.com>\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n""}, {'number': 18, 'created': '2014-09-23 05:14:56.000000000', 'files': ['neutron/db/firewall/firewall_db.py', 'neutron/extensions/firewall.py', 'neutron/tests/unit/db/firewall/test_db_firewall.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/2f5aca31b508da9c673c0209ad40efc2b5b2d16d', 'message': 'Stop admin using other tenants unshared rules\n\nIf the firewall rules are not shared and if they belong to different\ntenants, then admin should not be able to create a policy using\nthese rules and he should not be able to insert such rules into\npolicies. An exception should be raised in such case. Added new\nexception FirewallRuleConflict to handle such conditions.\n\nCo-Authored-By: Koteswara Rao Kelam<koteswara.kelam@hp.com>\nChange-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb\nCloses-bug: 1327057\n'}]",79,108952,2f5aca31b508da9c673c0209ad40efc2b5b2d16d,451,45,18,10797,,,0,"Stop admin using other tenants unshared rules

If the firewall rules are not shared and if they belong to different
tenants, then admin should not be able to create a policy using
these rules and he should not be able to insert such rules into
policies. An exception should be raised in such case. Added new
exception FirewallRuleConflict to handle such conditions.

Co-Authored-By: Koteswara Rao Kelam<koteswara.kelam@hp.com>
Change-Id: I984eb76069bd1493a77bf523bec2bd81abb14abb
Closes-bug: 1327057
",git fetch https://review.opendev.org/openstack/neutron refs/changes/52/108952/16 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/db/firewall/firewall_db.py', 'neutron/tests/unit/db/firewall/test_db_firewall.py']",2,65998207b95b14f0d3317abd9b623f949bca80e0,bug/1327057," firewall_rules, audited, tenant_id=None, expected_res_status=None, **kwargs): if tenant_id is None: tenant_id = kwargs.get('tenant_id', self._tenant_id) tenant_id=None, firewall_rules, audited, tenant_id, def test_create_admin_firewall_policy_with_other_tenant_rules(self): name = ""firewall_policy1"" attrs = self._get_test_firewall_policy_attrs(name) with contextlib.nested(self.firewall_rule(name='fwr1'), self.firewall_rule(name='fwr2'), self.firewall_rule(name='fwr3')) as fr: fw_rule_ids = [r['firewall_rule']['id'] for r in fr] attrs['firewall_rules'] = fw_rule_ids res = self._create_firewall_policy(None, 'firewall_policy1', description=DESCRIPTION, shared=SHARED, firewall_rules=fw_rule_ids, audited=AUDITED, tenant_id='admin-tenant') self.assertEqual(res.status_int, 404) "," firewall_rules, audited, expected_res_status=None, **kwargs): tenant_id = kwargs.get('tenant_id', self._tenant_id) firewall_rules, audited,",25,3
openstack%2Foctavia~master~I9d7682b4cb89bc74303593d6739d4196ed0d19e6,openstack/octavia,master,I9d7682b4cb89bc74303593d6739d4196ed0d19e6,Spec for the amphora driver interface,MERGED,2014-09-15 22:39:15.000000000,2014-09-26 23:11:15.000000000,2014-09-26 22:58:37.000000000,"[{'_account_id': 3}, {'_account_id': 6951}, {'_account_id': 7010}, {'_account_id': 10850}, {'_account_id': 10980}, {'_account_id': 11628}, {'_account_id': 11685}]","[{'number': 1, 'created': '2014-09-15 22:39:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a4748c8d443f57d88ebd291a4ed4e2001a80b776', 'message': 'First verison of amphora driver spec\n\nChange-Id: I9d7682b4cb89bc74303593d6739d4196ed0d19e6\n'}, {'number': 2, 'created': '2014-09-17 22:17:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/93c628d1304e4c3016b161e99f533340e6e98dfa', 'message': 'Second verison of amphora driver spec: New name, blongans and sbalukoffs, comments, hopefully pep8\n\nChange-Id: I9d7682b4cb89bc74303593d6739d4196ed0d19e6\n'}, {'number': 3, 'created': '2014-09-24 19:25:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/906ffc586b0253e407cebafe4dd837a39e1c6662', 'message': '3rd version - with listeners\nChange-Id: I9d7682b4cb89bc74303593d6739d4196ed0d19e6\n'}, {'number': 4, 'created': '2014-09-26 20:48:22.000000000', 'files': ['specs/version0.5/amphora-driver-interface.rst'], 'web_link': 'https://opendev.org/openstack/octavia/commit/fc3141dbdc662b7b0706153b794e37e19e3d7189', 'message': 'Spec for the amphora driver interface\n\nThis is using listeners this time.\n\nChange-Id: I9d7682b4cb89bc74303593d6739d4196ed0d19e6\n'}]",57,121694,fc3141dbdc662b7b0706153b794e37e19e3d7189,37,7,4,10850,,,0,"Spec for the amphora driver interface

This is using listeners this time.

Change-Id: I9d7682b4cb89bc74303593d6739d4196ed0d19e6
",git fetch https://review.opendev.org/openstack/octavia refs/changes/94/121694/4 && git format-patch -1 --stdout FETCH_HEAD,['specs/version0.5/amphora-driver-interface.rst'],1,a4748c8d443f57d88ebd291a4ed4e2001a80b776,,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Amphora Driver Interface ========================================== https://blueprints.launchpad.net/octavia/+spec/amphora-driver-interface This blueprint describes how a driver will interface with the controller. It will describe the base class and other classes required. It will not describe the REST interface needed to talk to an amphora nor how health information or statistsics are gathered from the amphora. Problem description =================== The controller needs to talk through a driver to the amphora to allow for custom APIs and custom rendering of configuration data for different amphora implementations. Proposed change =============== Establish a base class to model the desire functionality: .. code:: python class AmphoraeDriver(object): def getLogger(self): #return the logger to use - this is a way to inject a custom logger for testing, etc def update(self, lbs): #update the amphorae with a new configuration for each load balancer specified in the array lbs raise NotImplementedError def suspend(self, lb): #stop the loadbalancer - OPTIONAL raise NotImplementedError def enable(self, lb): #start/enable the loadbalancer raise NotImplementedError def delete(self, lb): #delete a load balancer on the amphora raise NotImplementedError def delete(self, amphora): #delete all load balancers and the amphora raise NotImplementedError def info(self, amphora): #returns information about the amphora, e.g. {""Rest Interface"": ""1.0"", ""Amphorae"": ""1.0"", ""packages"":{""ha proxy"":""1.5""}} #some information might come from querying the amphora raise NotImplementedError def archive_logs(self, loadbalancer, listener=ALL, target=SWIFT): #archives logs for a specific load balancer, listener, etc. raise NotImplementedError def get_metrics(self, amphora): #return ceilometer ready metrics - some amphora might choose to send them straight to ceilometer raise NotImplementedError def get_statistics(self, amphora): #return statistics, e.g, listener health, node health, etc. raise NotImplementedError def get_diagnostics(self, amphora): #run some self tests, e.g. connectivity, etc. - to determine if that amphorae is healthy raise NotImplementedError The referenced lb is a load balancer object as described in our model. The model is detached from the DB so the driver can't write to the DB. Because our initial goal is to render a whole config no special methods for adding nodes, health monitors, etc. are supported at this juncture. This might be added in later versions. Exception Model --------------- The driver is expected to raise the follow well defined exceptions * NotImplementedError - this functionality is not implemented/not supported * AmphoraDriverError - a super class for all other exceptions and the catch all if no specific exception can be determined. * NotFoundError - this amphora couldn't be found/ was deleted by nova * DeleteFailed - the amphora couldn't be deleted * AmphoraLoadBalancerError - errors to a specific load balancer * DeleteFailed - this load balancer couldn't be deleted * SuspendFaied - this load balancer couldn't be suspended * EnableFailed - this load balancer couldn't be enabled * ArchiveException - couldn't argcive the logs * TargetException - the target is not accessible * QuotaException - the target has no space left * UnauthorizedException - unauthorized to write to the target * InfoException - gathering information about this amphora failed * MetricsException - gathering metrics failed * UnauthorizedException - the driver can't access the amphora * StatisticsException - gathering statistics failed * TimeOutException - contacting the amphora timed out * UnavailableException - the amphora is temporary unavailable Health and Stat Mixin --------------------- It has been suggested to gather health and statistic information via UDP packets emitted from the amphora. This requires each driver to spin up a thread to listen on a UDP port and then hand the information to the controller as a mixin to make sense of it. Here is the mixin definition: .. code:: python class HealthMixIn(object): def update_health(health): #map: {""amphora-status"":HEALTHY, loadbalancers: {""loadbalancer-id"": {""loadbalancer-status"": HEALTHY, ""listeners"":{""listener-id"":{""listener-status"":HEALTHY, ""nodes"":{""node-id"":HEALTHY, ...}}, ...}, ...}} #awesome update code pass class StatsMixIn(object): def update_stats(stats): #uses map {""loadbalancer-id"":{""listener-id"": {""bytes-in"": 123, ""bytes_out"":123, ""active_connections"":123, ""total_connections"", 123}, ...} #awesome update code and code to send to ceilometer pass Things a good driver should do: ------------------------------- * Non blocking IO - throw an appropriate exception instead to wait forever; use timeouts on sockets * We might employ a circuit breaker to insulate driver problems from controller problems [1] * Use appropriate logging * Use the preferred threading model This will be demonstrated in the Noop-driver code. Alternatives ------------ Require all amphora to implement a common REST interface and use that as the integration point. Data model impact ----------------- None REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None - since initial version Other end user impact --------------------- None Performance Impact ------------------ Minimal Other deployer impact --------------------- Deployers need to make sure to bundle the compatible versions of amphora, driver, controller -- Developer impact ---------------- Need to write towards this clean interface. Implementation ============== Assignee(s) ----------- German Eichberger Work Items ---------- * Write abstract interface * Write Noop driver * Write tests Dependencies ============ None Testing ======= * Unit tests with tox and Noop-Driver * tempest tests with Noop-Driver Documentation Impact ==================== None - we won't document the interface for 0.5. If that changes we need to write an interface documentation so 3rd party drivers know what we expect. References ========== [1] http://martinfowler.com/bliki/CircuitBreaker.html ",,212,0
openstack%2Fmanila~master~I0743c92b55e123409133ea623e0f8ec03d8e9aaa,openstack/manila,master,I0743c92b55e123409133ea623e0f8ec03d8e9aaa,Fix creation of share from snapshot,MERGED,2014-09-25 15:54:03.000000000,2014-09-26 23:09:21.000000000,2014-09-26 23:09:20.000000000,"[{'_account_id': 3}, {'_account_id': 2417}, {'_account_id': 6491}, {'_account_id': 7534}, {'_account_id': 8851}, {'_account_id': 11878}]","[{'number': 1, 'created': '2014-09-25 15:54:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/f9243b6048430c412fd34c4937ed5aab160450b6', 'message': ""Add HostFilter to be able to use specific host\n\nIf we try create share from snapshot, source host is not chosen for sure.\nIn case we have multibackend installation, other hosts won't be filtered\nand error will be raised.\n\nAs solution for it HostFilter is added:\n- enabled by default\n- if snapshot used, - only one host passed or nothing\n- if snapshot not used, - all hosts are passed\n\nTo make your current installation work with this change, you should do:\n- apply this change\n- update file manila.egg-info/entry_points.txt as setup.cfg updated\n- restart manila-scheduler\n\nChange-Id: I0743c92b55e123409133ea623e0f8ec03d8e9aaa\nCloses-Bug: #1373882\n""}, {'number': 2, 'created': '2014-09-26 10:49:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/fac44fa28c12288dd678277ca0fad3b4f21db38e', 'message': ""Fix creation of share from snapshot\n\nCurrent behaviour does not take into accout source host of snapshot for\nshare creation from snapshot. It leads to impossibility to create share\nif improper host was scheduled using multibackend setup.\n\nSolution:\nImplement config opt that allows us call required host directly without\nscheduling in case we have host restriction and allow scheduling in case\nwe don't. We do not have such restricion, for example, using multibackend\ninstallation with generic drivers onboard.\n\nChange-Id: I0743c92b55e123409133ea623e0f8ec03d8e9aaa\nCloses-Bug: #1373882\n""}, {'number': 3, 'created': '2014-09-26 10:54:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/f23e37be853d3d093ddd52fe55ff94e9e88ad48e', 'message': ""Fix creation of share from snapshot\n\nCurrent behaviour does not take into accout source host of snapshot for\nshare creation from snapshot. It leads to impossibility to create share\nif improper host was scheduled using multibackend setup.\n\nSolution:\nImplement config opt that allows us call required host directly without\nscheduling in case we have host restriction and allow scheduling in case\nwe don't. We do not have such restricion, for example, using multibackend\ninstallation with generic drivers onboard.\n\nChange-Id: I0743c92b55e123409133ea623e0f8ec03d8e9aaa\nCloses-Bug: #1373882\n""}, {'number': 4, 'created': '2014-09-26 12:42:33.000000000', 'files': ['manila/tests/share/test_api.py', 'manila/share/api.py'], 'web_link': 'https://opendev.org/openstack/manila/commit/0655b58eaf9b186b20391cd8cf68b8f97ea0358a', 'message': ""Fix creation of share from snapshot\n\nCurrent behaviour does not take into accout source host of snapshot for\nshare creation from snapshot. It leads to impossibility to create share\nif improper host was scheduled using multibackend setup.\n\nSolution:\nImplement config opt that allows us call required host directly without\nscheduling in case we have host restriction and allow scheduling in case\nwe don't. We do not have such restricion, for example, using multibackend\ninstallation with generic drivers onboard.\n\nChange-Id: I0743c92b55e123409133ea623e0f8ec03d8e9aaa\nCloses-Bug: #1373882\n""}]",0,124089,0655b58eaf9b186b20391cd8cf68b8f97ea0358a,30,6,4,8851,,,0,"Fix creation of share from snapshot

Current behaviour does not take into accout source host of snapshot for
share creation from snapshot. It leads to impossibility to create share
if improper host was scheduled using multibackend setup.

Solution:
Implement config opt that allows us call required host directly without
scheduling in case we have host restriction and allow scheduling in case
we don't. We do not have such restricion, for example, using multibackend
installation with generic drivers onboard.

Change-Id: I0743c92b55e123409133ea623e0f8ec03d8e9aaa
Closes-Bug: #1373882
",git fetch https://review.opendev.org/openstack/manila refs/changes/89/124089/4 && git format-patch -1 --stdout FETCH_HEAD,"['manila/tests/share/test_api.py', 'manila/share/api.py', 'manila/scheduler/filters/host_filter.py', 'manila/scheduler/host_manager.py', 'setup.cfg']",5,f9243b6048430c412fd34c4937ed5aab160450b6,bug/1373882, HostFilter = manila.scheduler.filters.host_filter:HostFilter,,38,3
openstack%2Fpython-solumclient~master~Ib87ce85950833af9d17d0c76ca924396ecdf6b25,openstack/python-solumclient,master,Ib87ce85950833af9d17d0c76ca924396ecdf6b25,Switch to using oslotest package,MERGED,2014-09-09 21:51:29.000000000,2014-09-26 22:57:56.000000000,2014-09-26 22:57:56.000000000,"[{'_account_id': 3}, {'_account_id': 1375}, {'_account_id': 2506}, {'_account_id': 8334}, {'_account_id': 9548}]","[{'number': 1, 'created': '2014-09-09 21:51:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/2a87a2227045059bdbc8e6ae3c1dde6c381e6ba3', 'message': 'Switch to using oslotest package.\n\nImplements blueprint: oslotest\n\nChange-Id: Ib87ce85950833af9d17d0c76ca924396ecdf6b25\n'}, {'number': 2, 'created': '2014-09-09 21:53:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/5e8944eb0e05768583187178a8757b45395685c3', 'message': 'Switch to using oslotest package.\n\nChange-Id: Ib87ce85950833af9d17d0c76ca924396ecdf6b25\n'}, {'number': 3, 'created': '2014-09-09 21:54:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/4b030a7d38401c563ed3370516f882dad0fd66da', 'message': 'Switch to using oslotest package\n\nChange-Id: Ib87ce85950833af9d17d0c76ca924396ecdf6b25\n'}, {'number': 4, 'created': '2014-09-09 21:55:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/8316c7b5d2bd56da4b0451073d063f06373ccae3', 'message': 'Switch to using oslotest package\n\nChange-Id: Ib87ce85950833af9d17d0c76ca924396ecdf6b25\n'}, {'number': 5, 'created': '2014-09-14 18:24:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/f61e85fc27d4319b071167cd9e19e3f0d65801b7', 'message': 'Switch to using oslotest package\n\nChange-Id: Ib87ce85950833af9d17d0c76ca924396ecdf6b25\n'}, {'number': 6, 'created': '2014-09-14 18:33:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/f3c7f607c4540b8e827a4928c5d91d5693fd6312', 'message': 'Switch to using oslotest package\n\nChange-Id: Ib87ce85950833af9d17d0c76ca924396ecdf6b25\n'}, {'number': 7, 'created': '2014-09-23 17:04:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/d23581bba10a16b1fc450d6c030bb99db95aa916', 'message': 'Switch to using oslotest package\n\nChange-Id: Ib87ce85950833af9d17d0c76ca924396ecdf6b25\n'}, {'number': 8, 'created': '2014-09-24 06:53:29.000000000', 'files': ['test-requirements.txt', 'openstack-common.conf', 'solumclient/openstack/common/test.py', 'solumclient/tests/base.py'], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/76b82b6e4f2f93346b6f41122d33af64e59296b3', 'message': 'Switch to using oslotest package\n\nChange-Id: Ib87ce85950833af9d17d0c76ca924396ecdf6b25\n'}]",0,120255,76b82b6e4f2f93346b6f41122d33af64e59296b3,28,5,8,8334,,,0,"Switch to using oslotest package

Change-Id: Ib87ce85950833af9d17d0c76ca924396ecdf6b25
",git fetch https://review.opendev.org/openstack/python-solumclient refs/changes/55/120255/8 && git format-patch -1 --stdout FETCH_HEAD,"['test-requirements.txt', 'solumclient/openstack/common/test.py', 'solumclient/tests/base.py']",3,2a87a2227045059bdbc8e6ae3c1dde6c381e6ba3,oslotest,from oslotest import base class TestCase(base.BaseTestCase):,from solumclient.openstack.common import test class TestCase(test.BaseTestCase):,3,101
openstack%2Foslo.i18n~master~I64ae9191863564e278a35d42ec9cd743a233028e,openstack/oslo.i18n,master,I64ae9191863564e278a35d42ec9cd743a233028e,warn against sorting requirements,ABANDONED,2014-09-03 18:24:35.000000000,2014-09-26 22:44:11.000000000,,"[{'_account_id': 3}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-09-03 18:24:35.000000000', 'files': ['requirements.txt', 'test-requirements.txt'], 'web_link': 'https://opendev.org/openstack/oslo.i18n/commit/5dcf27f5b971a6087fd59846a48a145c7a343c57', 'message': 'warn against sorting requirements\n\nChange-Id: I64ae9191863564e278a35d42ec9cd743a233028e\nAddresses-Bug: #1365061\n'}]",0,118720,5dcf27f5b971a6087fd59846a48a145c7a343c57,5,2,1,2472,,,0,"warn against sorting requirements

Change-Id: I64ae9191863564e278a35d42ec9cd743a233028e
Addresses-Bug: #1365061
",git fetch https://review.opendev.org/openstack/oslo.i18n refs/changes/20/118720/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'test-requirements.txt']",2,5dcf27f5b971a6087fd59846a48a145c7a343c57,bug/1365061,"# The order of packages is significant, because pip processes them in the order # of appearance. Changing the order has an impact on the overall integration # process, which may cause wedges in the gate later. ",,8,0
openstack%2Fglance~master~Ice4ade659307ee5e635a75d6637b4664ee431c20,openstack/glance,master,Ice4ade659307ee5e635a75d6637b4664ee431c20,Remove db_enforce_mysql_charset option for db_sync of glance-manage,MERGED,2014-09-09 05:45:10.000000000,2014-09-26 22:41:45.000000000,2014-09-26 22:41:44.000000000,"[{'_account_id': 3}, {'_account_id': 100}, {'_account_id': 616}, {'_account_id': 5202}, {'_account_id': 6159}, {'_account_id': 8759}]","[{'number': 1, 'created': '2014-09-09 05:45:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/de9f6a0bcd5fa473815e3ec203f4af1f6a03f268', 'message': 'Remove db_enforce_mysql_charset option for db_sync of glance-manage\n\nThis is a deprecated option only should be used for migration. As we\nscheduled before, now glance removes it out from Juno release.\n\nChange-Id: Ice4ade659307ee5e635a75d6637b4664ee431c20\nSigned-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>\n'}, {'number': 2, 'created': '2014-09-10 06:16:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/0e13635f59bd8a18e40b863ad4ade4c4fe5867bd', 'message': 'Remove db_enforce_mysql_charset option for db_sync of glance-manage\n\nThis is a deprecated option only should be used for migration. As we\nscheduled before, now glance removes it out from Juno release.\n\nChange-Id: Ice4ade659307ee5e635a75d6637b4664ee431c20\nSigned-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>\n'}, {'number': 3, 'created': '2014-09-22 13:52:33.000000000', 'files': ['glance/tests/unit/test_manage.py', 'glance/cmd/manage.py', 'etc/glance-api.conf'], 'web_link': 'https://opendev.org/openstack/glance/commit/ee842bbb6696f1f6f56dfb350d04076b639562d4', 'message': 'Remove db_enforce_mysql_charset option for db_sync of glance-manage\n\nThis is a deprecated option only should be used for migration. As we\nscheduled before, now glance removes it out from Juno release.\n\nChange-Id: Ice4ade659307ee5e635a75d6637b4664ee431c20\nSigned-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>\n'}]",0,120002,ee842bbb6696f1f6f56dfb350d04076b639562d4,14,6,3,6549,,,0,"Remove db_enforce_mysql_charset option for db_sync of glance-manage

This is a deprecated option only should be used for migration. As we
scheduled before, now glance removes it out from Juno release.

Change-Id: Ice4ade659307ee5e635a75d6637b4664ee431c20
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>
",git fetch https://review.opendev.org/openstack/glance refs/changes/02/120002/1 && git format-patch -1 --stdout FETCH_HEAD,"['glance/cmd/manage.py', 'glance/tests/unit/test_manage.py', 'etc/glance-api.conf']",3,de9f6a0bcd5fa473815e3ec203f4af1f6a03f268,,,# =============== Manager Options ================================= # DEPRECATED. TO BE REMOVED IN THE JUNO RELEASE. # Whether or not to enforce that all DB tables have charset utf8. # If your database tables do not have charset utf8 you will # need to convert before this option is removed. This option is # only relevant if your database engine is MySQL. #db_enforce_mysql_charset = True ,14,114
openstack%2Fnova~master~Id03dc089af064969501ee2fc9d965a2028f901e4,openstack/nova,master,Id03dc089af064969501ee2fc9d965a2028f901e4,VMware: Use vm_mode constants,MERGED,2014-09-25 13:53:41.000000000,2014-09-26 22:41:23.000000000,2014-09-26 22:41:20.000000000,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 1063}, {'_account_id': 1653}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 5367}, {'_account_id': 9008}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-25 13:53:41.000000000', 'files': ['nova/virt/vmwareapi/host.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/2d7b252f52be6315ff3d2924977de2f36cec3fc1', 'message': 'VMware: Use vm_mode constants\n\nAddress missing changes for the vm_mode type from\ncommit 8fcd1b2c73dad8e2a7b4d299da270934fd5328cc.\n\nChange-Id: Id03dc089af064969501ee2fc9d965a2028f901e4\nCo-authored-by: Gary Kotton <gkotton@vmware.com>\n'}]",1,124055,2d7b252f52be6315ff3d2924977de2f36cec3fc1,13,9,1,9555,,,0,"VMware: Use vm_mode constants

Address missing changes for the vm_mode type from
commit 8fcd1b2c73dad8e2a7b4d299da270934fd5328cc.

Change-Id: Id03dc089af064969501ee2fc9d965a2028f901e4
Co-authored-by: Gary Kotton <gkotton@vmware.com>
",git fetch https://review.opendev.org/openstack/nova refs/changes/55/124055/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/virt/vmwareapi/host.py'],1,2d7b252f52be6315ff3d2924977de2f36cec3fc1,vm_mode," (arch.I686, hvtype.VMWARE, vm_mode.HVM), (arch.X86_64, hvtype.VMWARE, vm_mode.HVM)]"," (arch.I686, hvtype.VMWARE, 'hvm'), (arch.X86_64, hvtype.VMWARE, 'hvm')]",2,2
openstack%2Fglance_store~master~I58efc8981b38d1e445f9acde669802b97e4b2077,openstack/glance_store,master,I58efc8981b38d1e445f9acde669802b97e4b2077,VMware store: Use the Content-Length if available,MERGED,2014-09-25 21:42:31.000000000,2014-09-26 22:41:17.000000000,2014-09-26 22:41:17.000000000,"[{'_account_id': 3}, {'_account_id': 6549}]","[{'number': 1, 'created': '2014-09-25 21:42:31.000000000', 'files': ['tests/unit/test_vmware_store.py', 'glance_store/_drivers/vmware_datastore.py'], 'web_link': 'https://opendev.org/openstack/glance_store/commit/4917fe64b303383507bc26e5c7c43dfecd904c44', 'message': 'VMware store: Use the Content-Length if available\n\nChange I579084460e7f61ab4042632d17ec0f045fa6f5af changed the VMware\nstore to use chunked encoding to upload data to the underlying backend.\nWe should do that only if the image size is not provided or zero and not\nall the time.\nThis patch addressed the issue by checking the image_size before upload.\n\nMerged in Glance: Change-Id: If348be7fd24fd05146b0c86bef45b79f600cc0f7\nCloses-Bug: #1336970\n\nChange-Id: I58efc8981b38d1e445f9acde669802b97e4b2077\n'}]",1,124193,4917fe64b303383507bc26e5c7c43dfecd904c44,6,2,1,8759,,,0,"VMware store: Use the Content-Length if available

Change I579084460e7f61ab4042632d17ec0f045fa6f5af changed the VMware
store to use chunked encoding to upload data to the underlying backend.
We should do that only if the image size is not provided or zero and not
all the time.
This patch addressed the issue by checking the image_size before upload.

Merged in Glance: Change-Id: If348be7fd24fd05146b0c86bef45b79f600cc0f7
Closes-Bug: #1336970

Change-Id: I58efc8981b38d1e445f9acde669802b97e4b2077
",git fetch https://review.opendev.org/openstack/glance_store refs/changes/93/124193/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/test_vmware_store.py', 'glance_store/_drivers/vmware_datastore.py']",2,4917fe64b303383507bc26e5c7c43dfecd904c44,bug/1336970,"from glance_store.openstack.common import units def __init__(self, data): self.data = data self.checksum = hashlib.md5() def read(self, size=None): result = self.data.read(size) self._size += len(result) self.checksum.update(result) return result def rewind(self): try: self.data.seek(0) self._size = 0 self.checksum = hashlib.md5() except IOError: with excutils.save_and_reraise_exception(): LOG.exception(_LE('Failed to rewind image content')) @property def size(self): return self._size class _ChunkReader(_Reader): def __init__(self, data, blocksize=8192): super(_ChunkReader, self).__init__(data) # def _is_valid_path(self, path): # reason = 'Badly formed VMware datastore URI %(uri)s.' % {'uri': uri} # LOG.debug(reason) # raise exceptions.BadStoreUri(reason) WRITE_CHUNKSIZE = units.Mi if image_size > 0: headers = {'Content-Length': image_size} image_file = _Reader(image_file) else: # NOTE (arnaud): use chunk encoding when the image is still being # generated by the server (ex: stream optimized disks generated by # Nova). headers = {'Transfer-Encoding': 'chunked'} image_file = _ChunkReader(image_file) headers = dict(headers.items() + {'Cookie': cookie}.items()) return (loc.get_uri(), image_file.size, image_file.checksum.hexdigest(), {}) iterator = http_response_iterator(conn, resp, self.READ_CHUNKSIZE)"," def __init__(self, data, checksum, blocksize=8192): self.data = data self.checksum = checksum def rewind(self): try: self.data.seek(0) except IOError: with excutils.save_and_reraise_exception(): LOG.exception(_LE('Failed to rewind image content')) @property def size(self): return self._size #def _is_valid_path(self, path): #reason = 'Badly formed VMware datastore URI %(uri)s.' % {'uri': uri} #LOG.debug(reason) #raise exceptions.BadStoreUri(reason) checksum = hashlib.md5() image_file = _Reader(image_file, checksum) headers = {'Connection': 'Keep-Alive', 'Cookie': cookie, 'Transfer-Encoding': 'chunked'} return (loc.get_uri(), image_file.size, checksum.hexdigest(), {}) iterator = http_response_iterator(conn, resp, self.CHUNKSIZE)",91,37
openstack%2Foslo.log~master~Ie11a51deea65627b45a11d7dfca36d16c1b5949e,openstack/oslo.log,master,Ie11a51deea65627b45a11d7dfca36d16c1b5949e,Test formatting errors with log level being emitted,MERGED,2014-09-07 14:35:44.000000000,2014-09-26 22:41:15.000000000,2014-09-26 22:41:14.000000000,"[{'_account_id': 3}, {'_account_id': 1561}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 6601}]","[{'number': 1, 'created': '2014-09-07 14:35:44.000000000', 'files': ['tests/unit/fixture/test_logging.py'], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/24582fa0e031ad8c964a094912a6fb5a02bc2ace', 'message': 'Test formatting errors with log level being emitted\n\nThe test to ensure that formatting errors are handled properly was being\nrun using info level logging, but the default log configuration for\ntests does not emit messages at that level, so switch to error.\n\nChange-Id: Ie11a51deea65627b45a11d7dfca36d16c1b5949e\n'}]",0,119618,24582fa0e031ad8c964a094912a6fb5a02bc2ace,16,5,1,2472,,,0,"Test formatting errors with log level being emitted

The test to ensure that formatting errors are handled properly was being
run using info level logging, but the default log configuration for
tests does not emit messages at that level, so switch to error.

Change-Id: Ie11a51deea65627b45a11d7dfca36d16c1b5949e
",git fetch https://review.opendev.org/openstack/oslo.log refs/changes/18/119618/1 && git format-patch -1 --stdout FETCH_HEAD,['tests/unit/fixture/test_logging.py'],1,24582fa0e031ad8c964a094912a6fb5a02bc2ace,openstack/requirements," LOG.error,"," LOG.info,",1,1
openstack%2Fglance_store~master~Ie415667a809975948c8cfb71ec63a0905995fa67,openstack/glance_store,master,Ie415667a809975948c8cfb71ec63a0905995fa67,Make rbd store's pool handling more universal,MERGED,2014-09-12 12:05:56.000000000,2014-09-26 22:41:11.000000000,2014-09-26 22:41:10.000000000,"[{'_account_id': 3}, {'_account_id': 1107}, {'_account_id': 6159}, {'_account_id': 6549}, {'_account_id': 8582}]","[{'number': 1, 'created': '2014-09-12 12:05:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/0854a35d290b5157460aab5f46af6010ff88e9c0', 'message': ""Make rbd store's get_size() more universal\n\nCurrently the pool is assumed to be the same pool as configured in\nthe configuration, however in the location parameter we have the\npool, so use that instead.\n\nThis comes handy when later on nova snapshotting will work by just\ncreating the snapshot in ceph itself so glance might need to\nreference an other pool not just it's own.\n\nChange-Id: Ie415667a809975948c8cfb71ec63a0905995fa67\nCloses-Bug: 1368128\n""}, {'number': 2, 'created': '2014-09-17 09:21:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/ae888964d332eea9d8db2e9e18f285032885ba66', 'message': ""Make rbd store's pool handling more universal\n\nCurrently in get_size()/remove() ignores the pool part of the location.\nIf there is a pool specified, use that. Otherwise we can still fall back to the\nconfigured pool.\n\nThis is a required change if we want to support ephemeral disk snapshotting later on\nas in that scenario the ephemeral pool might be a different pool from images pool,\nyet we're going to need to reference the snapshot of a disk in the ephemeral pool.\n\nChange-Id: Ie415667a809975948c8cfb71ec63a0905995fa67\nCloses-Bug: 1368128\n""}, {'number': 3, 'created': '2014-09-17 11:02:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/1183af782b409af8e1228fce3a4a4d31b54a7c8e', 'message': ""Make rbd store's pool handling more universal\n\nCurrently in get_size()/remove() ignores the pool part of the location.\nIf there is a pool specified, use that. Otherwise we can still fall back to the\nconfigured pool.\n\nThis is a required change if we want to support ephemeral disk snapshotting later on\nas in that scenario the ephemeral pool might be a different pool from images pool,\nyet we're going to need to reference the snapshot of a disk in the ephemeral pool.\n\nChange-Id: Ie415667a809975948c8cfb71ec63a0905995fa67\nCloses-Bug: 1368128\n""}, {'number': 4, 'created': '2014-09-18 09:07:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/73852d2a4349cb35112ae0c9839dc94fb9124aa6', 'message': ""Make rbd store's pool handling more universal\n\nCurrently in get_size()/remove() ignores the pool part of the location.\nIf there is a pool specified, use that. Otherwise we can still fall back to the\nconfigured pool.\n\nThis is a required change if we want to support ephemeral disk snapshotting later on\nas in that scenario the ephemeral pool might be a different pool from images pool,\nyet we're going to need to reference the snapshot of a disk in the ephemeral pool.\n\nChange-Id: Ie415667a809975948c8cfb71ec63a0905995fa67\nCloses-Bug: 1368128\n""}, {'number': 5, 'created': '2014-09-18 11:17:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/fc11ef34ce618b0509d691c9f9ee3ad56793ca24', 'message': ""Make rbd store's pool handling more universal\n\nCurrently in get_size()/remove() ignores the pool part of the location.\nIf there is a pool specified, use that. Otherwise we can still fall back to the\nconfigured pool.\n\nThis is a required change if we want to support ephemeral disk snapshotting later on\nas in that scenario the ephemeral pool might be a different pool from images pool,\nyet we're going to need to reference the snapshot of a disk in the ephemeral pool.\n\nChange-Id: Ie415667a809975948c8cfb71ec63a0905995fa67\nCloses-Bug: 1368128\n""}, {'number': 6, 'created': '2014-09-23 15:56:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance_store/commit/fec86a1d427dbfd703f57f5174ecf92b68f28eab', 'message': ""Make rbd store's pool handling more universal\n\nCurrently we ignore the pool part of the location throughout the rbd store code.\nIf there is a pool specified, use that. Otherwise we can still fall back to the\nconfigured pool.\n\nThis is a required change if we want to support ephemeral disk snapshotting later on\nas in that scenario the ephemeral pool might be a different pool from images pool,\nyet we're going to need to reference the snapshot of a disk in the ephemeral pool.\n\nChange-Id: Ie415667a809975948c8cfb71ec63a0905995fa67\nCloses-Bug: 1368128\n""}, {'number': 7, 'created': '2014-09-24 07:18:50.000000000', 'files': ['tests/unit/test_rbd_store.py', 'glance_store/_drivers/rbd.py'], 'web_link': 'https://opendev.org/openstack/glance_store/commit/ff52e926731eb795a53674a60b24d31f3aa496fc', 'message': ""Make rbd store's pool handling more universal\n\nCurrently we ignore the pool part of the location throughout the rbd store code.\nIf there is a pool specified, use that. Otherwise we can still fall back to the\nconfigured pool.\n\nThis is a required change if we want to support ephemeral disk snapshotting later on\nas in that scenario the ephemeral pool might be a different pool from images pool,\nyet we're going to need to reference the snapshot of a disk in the ephemeral pool.\n\nChange-Id: Ie415667a809975948c8cfb71ec63a0905995fa67\nCloses-Bug: 1368128\n""}]",2,121067,ff52e926731eb795a53674a60b24d31f3aa496fc,31,5,7,8582,,,0,"Make rbd store's pool handling more universal

Currently we ignore the pool part of the location throughout the rbd store code.
If there is a pool specified, use that. Otherwise we can still fall back to the
configured pool.

This is a required change if we want to support ephemeral disk snapshotting later on
as in that scenario the ephemeral pool might be a different pool from images pool,
yet we're going to need to reference the snapshot of a disk in the ephemeral pool.

Change-Id: Ie415667a809975948c8cfb71ec63a0905995fa67
Closes-Bug: 1368128
",git fetch https://review.opendev.org/openstack/glance_store refs/changes/67/121067/4 && git format-patch -1 --stdout FETCH_HEAD,['glance_store/_drivers/rbd.py'],1,0854a35d290b5157460aab5f46af6010ff88e9c0,bug/1368128, with conn.open_ioctx(loc.pool) as ioctx:, with conn.open_ioctx(self.pool) as ioctx:,1,1
openstack%2Fcinder~master~Ibcf19ccc81a56a9a3e795022e3e07bc061fa9401,openstack/cinder,master,Ibcf19ccc81a56a9a3e795022e3e07bc061fa9401,Update /etc/cinder/cinder.conf.sample for memcache,MERGED,2014-09-25 21:07:16.000000000,2014-09-26 22:40:38.000000000,2014-09-26 22:40:38.000000000,"[{'_account_id': 3}, {'_account_id': 6491}, {'_account_id': 7173}, {'_account_id': 7198}, {'_account_id': 8247}, {'_account_id': 12369}]","[{'number': 1, 'created': '2014-09-25 21:07:16.000000000', 'files': ['etc/cinder/cinder.conf.sample'], 'web_link': 'https://opendev.org/openstack/cinder/commit/46f90e99004e1ef76a7363165e340941b8c9a638', 'message': 'Update /etc/cinder/cinder.conf.sample for memcache\n\nIt appears that an update to keystone middleware earlier today\nadded options for memcache_secret_key, memcache_pool_dead_retry,\nmemcache_pool_maxsize, memcache_pool_socket_timeout,\nmemcache_pool_unused_timeout, memcache_pool_conn_get_timeout and\nmemcache_use_advanced_pool.  The commit that added these options\nwas: a7beb50b38be5c3dd4c44d68ad79d1bb206dab6b - ""Add an optional\nadvanced pool of memcached clients"".\n\nThis has once again caused the check_uptodate.sh script to fail.\n\nThis patch updates cinder.sample.conf .\n\nChange-Id: Ibcf19ccc81a56a9a3e795022e3e07bc061fa9401\nCloses-bug: 1374154\n'}]",2,124182,46f90e99004e1ef76a7363165e340941b8c9a638,11,6,1,7198,,,0,"Update /etc/cinder/cinder.conf.sample for memcache

It appears that an update to keystone middleware earlier today
added options for memcache_secret_key, memcache_pool_dead_retry,
memcache_pool_maxsize, memcache_pool_socket_timeout,
memcache_pool_unused_timeout, memcache_pool_conn_get_timeout and
memcache_use_advanced_pool.  The commit that added these options
was: a7beb50b38be5c3dd4c44d68ad79d1bb206dab6b - ""Add an optional
advanced pool of memcached clients"".

This has once again caused the check_uptodate.sh script to fail.

This patch updates cinder.sample.conf .

Change-Id: Ibcf19ccc81a56a9a3e795022e3e07bc061fa9401
Closes-bug: 1374154
",git fetch https://review.opendev.org/openstack/cinder refs/changes/82/124182/1 && git format-patch -1 --stdout FETCH_HEAD,['etc/cinder/cinder.conf.sample'],1,46f90e99004e1ef76a7363165e340941b8c9a638,bug/1374154,# (optional) number of seconds memcached server is considered # dead before it is tried again. (integer value) #memcache_pool_dead_retry=300 # (optional) max total number of open connections to every # memcached server. (integer value) #memcache_pool_maxsize=10 # (optional) socket timeout in seconds for communicating with # a memcache server. (integer value) #memcache_pool_socket_timeout=3 # (optional) number of seconds a connection to memcached is # held unused in the pool before it is closed. (integer value) #memcache_pool_unused_timeout=60 # (optional) number of seconds that an operation will wait to # get a memcache client connection from the pool. (integer # value) #memcache_pool_conn_get_timeout=10 # (optional) use the advanced (eventlet safe) memcache client # pool. The advanced pool will only work under python 2.x. # (boolean value) #memcache_use_advanced_pool=false ,,26,0
openstack%2Ffuel-library~master~Ied9f4308b998e9f345449e03a00221de41261e85,openstack/fuel-library,master,Ied9f4308b998e9f345449e03a00221de41261e85,Declare cinder::glance to set glance_api_version,MERGED,2014-09-25 18:05:47.000000000,2014-09-26 22:40:01.000000000,2014-09-26 22:40:01.000000000,"[{'_account_id': 3}, {'_account_id': 8787}, {'_account_id': 8971}, {'_account_id': 9387}]","[{'number': 1, 'created': '2014-09-25 18:05:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/9e05d8286fb231166df94bfd609c26780978f427', 'message': 'Declare cinder::glance to set glance_api_version\n\nPrior to syncing upstream cinder module, glance_api_version was set from\ncinder::backend::rbd. New upstream moved this to cinder::glance.\n\nChange-Id: Ied9f4308b998e9f345449e03a00221de41261e85\nCloses-Bug: #1373096\n'}, {'number': 2, 'created': '2014-09-25 19:18:38.000000000', 'files': ['deployment/puppet/openstack/manifests/cinder.pp'], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/c8f374144f9e8c3f3e7a97dfad3a952a6fd7afba', 'message': 'Declare cinder::glance to set glance_api_version\n\nPrior to syncing upstream cinder module, glance_api_version was set from\ncinder::backend::rbd. New upstream moved this to cinder::glance.\n\nChange-Id: Ied9f4308b998e9f345449e03a00221de41261e85\nCloses-Bug: #1373096\n'}]",0,124123,c8f374144f9e8c3f3e7a97dfad3a952a6fd7afba,18,4,2,8787,,,0,"Declare cinder::glance to set glance_api_version

Prior to syncing upstream cinder module, glance_api_version was set from
cinder::backend::rbd. New upstream moved this to cinder::glance.

Change-Id: Ied9f4308b998e9f345449e03a00221de41261e85
Closes-Bug: #1373096
",git fetch https://review.opendev.org/openstack/fuel-library refs/changes/23/124123/1 && git format-patch -1 --stdout FETCH_HEAD,['deployment/puppet/openstack/manifests/cinder.pp'],1,9e05d8286fb231166df94bfd609c26780978f427,bug/1373096," class {'cinder::glance': glance_api_servers => $glance_api_servers, # Glance API v2 is required for Ceph RBD backend $glance_api_version => '2', }", cinder_config { 'DEFAULT/glance_api_servers': value => $glance_api_servers },5,1
openstack%2Fpython-swiftclient~master~I80eee49eaa74b52e9c36f00669206aa7b2cdec9b,openstack/python-swiftclient,master,I80eee49eaa74b52e9c36f00669206aa7b2cdec9b,Remove a debugging print statement,MERGED,2014-09-25 08:35:40.000000000,2014-09-26 22:35:40.000000000,2014-09-26 22:35:39.000000000,"[{'_account_id': 3}, {'_account_id': 2622}, {'_account_id': 7847}, {'_account_id': 9216}]","[{'number': 1, 'created': '2014-09-25 08:35:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/be8f0134469242c1ce672729136eb7b3419b621c', 'message': 'This patch removes a debugging print statement that slipped into\nservice.py in patch https://review.openstack.org/#/c/85453/\n\nChange-Id: I80eee49eaa74b52e9c36f00669206aa7b2cdec9b\n'}, {'number': 2, 'created': '2014-09-25 11:03:43.000000000', 'files': ['swiftclient/service.py'], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/3496e07ea42bed9e45203a13322f6b77f8feae96', 'message': 'Remove a debugging print statement\n\nThis patch removes a debugging print statement that slipped into\nservice.py in patch https://review.openstack.org/#/c/85453/\n\nChange-Id: I80eee49eaa74b52e9c36f00669206aa7b2cdec9b\n'}]",0,123975,3496e07ea42bed9e45203a13322f6b77f8feae96,15,4,2,9216,,,0,"Remove a debugging print statement

This patch removes a debugging print statement that slipped into
service.py in patch https://review.openstack.org/#/c/85453/

Change-Id: I80eee49eaa74b52e9c36f00669206aa7b2cdec9b
",git fetch https://review.opendev.org/openstack/python-swiftclient refs/changes/75/123975/2 && git format-patch -1 --stdout FETCH_HEAD,['swiftclient/service.py'],1,be8f0134469242c1ce672729136eb7b3419b621c,remove-segment-delete-print-statement,, print(seg),0,1
openstack%2Fceilometer~stable%2Ficehouse~I996a50a7b501698a7586bc04fc0b3cb4830b0214,openstack/ceilometer,stable/icehouse,I996a50a7b501698a7586bc04fc0b3cb4830b0214,Updated from global requirements,MERGED,2014-09-13 13:16:02.000000000,2014-09-26 22:35:18.000000000,2014-09-26 22:35:17.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 3012}, {'_account_id': 6537}, {'_account_id': 7729}, {'_account_id': 9656}]","[{'number': 1, 'created': '2014-09-13 13:16:02.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/8bc43b73faa8008fa91ff5f5f8a1a6937eed708a', 'message': 'Updated from global requirements\n\nChange-Id: I996a50a7b501698a7586bc04fc0b3cb4830b0214\n'}]",0,121345,8bc43b73faa8008fa91ff5f5f8a1a6937eed708a,11,6,1,11131,,,0,"Updated from global requirements

Change-Id: I996a50a7b501698a7586bc04fc0b3cb4830b0214
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/45/121345/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,8bc43b73faa8008fa91ff5f5f8a1a6937eed708a,openstack/requirements,"sqlalchemy-migrate>=0.8.2,!=0.8.4,!=0.9.2","sqlalchemy-migrate>=0.8.2,!=0.8.4",1,1
openstack%2Fhorizon~stable%2Ficehouse~Ifced62b24a9b6e4800b70dae2912ff16d809743d,openstack/horizon,stable/icehouse,Ifced62b24a9b6e4800b70dae2912ff16d809743d,Not able to delete a pseudo-folder via horizon,MERGED,2014-08-29 13:39:59.000000000,2014-09-26 22:35:08.000000000,2014-09-26 22:35:07.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 4978}, {'_account_id': 9656}, {'_account_id': 10442}]","[{'number': 1, 'created': '2014-08-29 13:39:59.000000000', 'files': ['openstack_dashboard/dashboards/project/containers/tests.py', 'openstack_dashboard/dashboards/project/containers/tables.py', 'openstack_dashboard/test/test_data/swift_data.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/faac9e7afb6a3925ed057200a19bec963933fe1b', 'message': ""Not able to delete a pseudo-folder via horizon\n\nThrough horizon the user not able to delete a\npseudo-folder. it giving a error message like\n'you are not allowed to delete object'\n\nproblem-1:\n'subfolders' datatype is not added in the\nallowed datatypes tuple, so the delete operation\nis not allowed for pseudo-folder. now the 'subfolder'\nis added in the tuple.\n\nproblem-2:\nthe object name is starting with container name and\nnot trailing with '/'. now the object name is modified\nbefore sending to swift client.(removing the container\nname and trailing with '/'.\n\nChange-Id: Ifced62b24a9b6e4800b70dae2912ff16d809743d\nCloses-Bug: #1317016\nCo-Authored-By: Mizielski Robert <robert.mizielski@cloudwatt.com>\n(cherry picked from commit a46ce9e3ffaa4ee18618ea18496a76508717c871)\n""}]",0,117779,faac9e7afb6a3925ed057200a19bec963933fe1b,11,5,1,10786,,,0,"Not able to delete a pseudo-folder via horizon

Through horizon the user not able to delete a
pseudo-folder. it giving a error message like
'you are not allowed to delete object'

problem-1:
'subfolders' datatype is not added in the
allowed datatypes tuple, so the delete operation
is not allowed for pseudo-folder. now the 'subfolder'
is added in the tuple.

problem-2:
the object name is starting with container name and
not trailing with '/'. now the object name is modified
before sending to swift client.(removing the container
name and trailing with '/'.

Change-Id: Ifced62b24a9b6e4800b70dae2912ff16d809743d
Closes-Bug: #1317016
Co-Authored-By: Mizielski Robert <robert.mizielski@cloudwatt.com>
(cherry picked from commit a46ce9e3ffaa4ee18618ea18496a76508717c871)
",git fetch https://review.opendev.org/openstack/horizon refs/changes/79/117779/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/containers/tests.py', 'openstack_dashboard/dashboards/project/containers/tables.py', 'openstack_dashboard/test/test_data/swift_data.py']",3,faac9e7afb6a3925ed057200a19bec963933fe1b,," TEST.folder = utils.TestDataContainer() folder_dict = {""name"": u""test folder%\u6346"", ""content_type"": u""text/plain"", ""bytes"": 128, ""timestamp"": timeutils.isotime(), ""_table_data_type"": u""subfolders"", ""last_modified"": None, ""hash"": u""object_hash""} TEST.folder.add(swift.PseudoFolder(folder_dict, container_1.name))",,35,4
openstack%2Ffuel-library~master~I3515c6350c1314cf091480ea109c8ade6195ea17,openstack/fuel-library,master,I3515c6350c1314cf091480ea109c8ade6195ea17,Replace start_guests with resume_guests,MERGED,2014-09-25 22:32:16.000000000,2014-09-26 22:35:00.000000000,2014-09-26 22:34:59.000000000,"[{'_account_id': 3}, {'_account_id': 8787}, {'_account_id': 8829}, {'_account_id': 8971}]","[{'number': 1, 'created': '2014-09-25 22:32:16.000000000', 'files': ['deployment/puppet/osnailyfacter/manifests/cluster_ha.pp', 'deployment/puppet/osnailyfacter/manifests/cluster_simple.pp'], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/8a3c19054d3d75185d3fd112104ee6e6d7c005a0', 'message': 'Replace start_guests with resume_guests\n\nFuel UI offers an option to ""start guests on reboot"" which,\nif set to true, will attempt to restart all guests on a\ngiven host regardless of prior status. This commit changes\nthe nova.conf entry from start_guests_on_boot to\nresume_guets_state_on_host_boot.\n\nChange-Id: I3515c6350c1314cf091480ea109c8ade6195ea17\nPartial-Bug: #1352284\n'}]",0,124207,8a3c19054d3d75185d3fd112104ee6e6d7c005a0,11,4,1,9788,,,0,"Replace start_guests with resume_guests

Fuel UI offers an option to ""start guests on reboot"" which,
if set to true, will attempt to restart all guests on a
given host regardless of prior status. This commit changes
the nova.conf entry from start_guests_on_boot to
resume_guets_state_on_host_boot.

Change-Id: I3515c6350c1314cf091480ea109c8ade6195ea17
Partial-Bug: #1352284
",git fetch https://review.opendev.org/openstack/fuel-library refs/changes/07/124207/1 && git format-patch -1 --stdout FETCH_HEAD,"['deployment/puppet/osnailyfacter/manifests/cluster_ha.pp', 'deployment/puppet/osnailyfacter/manifests/cluster_simple.pp']",2,8a3c19054d3d75185d3fd112104ee6e6d7c005a0,bug/1352284, nova_config { 'DEFAULT/resume_guests_state_on_host_boot': value => $::fuel_settings['resume_guests_state_on_host_boot'] }, nova_config { 'DEFAULT/start_guests_on_host_boot': value => $::fuel_settings['start_guests_on_host_boot'] },2,2
openstack%2Ffuel-web~master~Ib7515b760fff08226de887748e611030ae101a50,openstack/fuel-web,master,Ib7515b760fff08226de887748e611030ae101a50,Replace start_guests with resume_guests,MERGED,2014-09-25 22:31:57.000000000,2014-09-26 22:34:13.000000000,2014-09-26 22:34:13.000000000,"[{'_account_id': 3}, {'_account_id': 8787}, {'_account_id': 8829}, {'_account_id': 8971}]","[{'number': 1, 'created': '2014-09-25 22:31:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/09853af7b7af38602e564179a015ab069cca451e', 'message': 'Replace start_guests with resume_guests\n\nFuel UI offers an option to ""start guests on reboot"" which,\nif set to true, will attempt to restart all guests on a\ngiven host regardless of prior status. This commit changes\nthe nova.conf entry from start_guests_on_boot to\nresume_guets_state_on_host_boot.\n\nChange-Id: Ib7515b760fff08226de887748e611030ae101a50\nPartial-Bug: #1352284\n'}, {'number': 2, 'created': '2014-09-25 22:44:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/11b4bed034b032ee002d4415203c9f08ea8e32a6', 'message': 'Replace start_guests with resume_guests\n\nFuel UI offers an option to ""start guests on reboot"" which,\nif set to true, will attempt to restart all guests on a\ngiven host regardless of prior status. This commit changes\nthe nova.conf entry from start_guests_on_boot to\nresume_guets_state_on_host_boot.\n\nChange-Id: Ib7515b760fff08226de887748e611030ae101a50\nPartial-Bug: #1352284\n'}, {'number': 3, 'created': '2014-09-26 00:27:22.000000000', 'files': ['nailgun/nailgun/fixtures/openstack.yaml'], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/62c1c40b090f604fbdcaf9132741b5ff7a24d9b5', 'message': 'Replace start_guests with resume_guests\n\nFuel UI offers an option to ""start guests on reboot"" which,\nif set to true, will attempt to restart all guests on a\ngiven host regardless of prior status. This commit changes\nthe nova.conf entry from start_guests_on_boot to\nresume_guets_state_on_host_boot.\n\nDepends on https://review.openstack.org/#/c/124207/ in fuel-library\n\nChange-Id: Ib7515b760fff08226de887748e611030ae101a50\nPartial-Bug: #1352284\n'}]",0,124206,62c1c40b090f604fbdcaf9132741b5ff7a24d9b5,20,4,3,9788,,,0,"Replace start_guests with resume_guests

Fuel UI offers an option to ""start guests on reboot"" which,
if set to true, will attempt to restart all guests on a
given host regardless of prior status. This commit changes
the nova.conf entry from start_guests_on_boot to
resume_guets_state_on_host_boot.

Depends on https://review.openstack.org/#/c/124207/ in fuel-library

Change-Id: Ib7515b760fff08226de887748e611030ae101a50
Partial-Bug: #1352284
",git fetch https://review.opendev.org/openstack/fuel-web refs/changes/06/124206/1 && git format-patch -1 --stdout FETCH_HEAD,['nailgun/nailgun/fixtures/openstack.yaml'],1,09853af7b7af38602e564179a015ab069cca451e,bug/1352284," resume_guests_state_on_host_boot: label: ""Resume guests state on host boot"" description: ""Whether to resume previous guests state when the host reboots. If enabled, this option causes guests assigned to the host to resume their previous state. If the guest was running a restart will be attempted when nova-compute states. If the guest was not running previously, a restart will not be attempted."""," start_guests_on_host_boot: label: ""Start guests on host boot"" description: ""Whether to (re-)start guests when the host reboots. If enabled, this option causes guests assigned to the host to be unconditionally restarted when nova-compute starts. If the guest is found to be stopped, it starts. If it is found to be running, it reboots.""",3,3
openstack%2Ffuel-library~stable%2F5.1~I5b4ea431d6907e4d57d0f753d3720a470583c77f,openstack/fuel-library,stable/5.1,I5b4ea431d6907e4d57d0f753d3720a470583c77f,multiple start rescheduling after migration L3/DHCP agent,MERGED,2014-09-22 11:39:35.000000000,2014-09-26 22:32:52.000000000,2014-09-26 22:32:51.000000000,"[{'_account_id': 3}, {'_account_id': 6072}, {'_account_id': 6926}, {'_account_id': 7126}, {'_account_id': 8787}, {'_account_id': 8971}]","[{'number': 1, 'created': '2014-09-22 11:39:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/5fb8a8aec7d0e0e22ff1f0e4c3f02f487f48be08', 'message': 'multiple start rescheduling after migration L3/DHCP agent\n\nThis workaround is safe, because starting rescheduling on alive agent do nothing\n\nChange-Id: I5b4ea431d6907e4d57d0f753d3720a470583c77f\nCloses-bug: #1371561\n'}, {'number': 2, 'created': '2014-09-22 12:20:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/4b2ce7b296f54860b4920b3a579480ec87198951', 'message': 'multiple start rescheduling after migration L3/DHCP agent\n\nThis workaround is safe, because starting rescheduling on alive agent do nothing\n\nChange-Id: I5b4ea431d6907e4d57d0f753d3720a470583c77f\nCloses-bug: #1371561\n'}, {'number': 3, 'created': '2014-09-25 22:06:47.000000000', 'files': ['deployment/puppet/neutron/files/ocf/neutron-agent-dhcp', 'deployment/puppet/neutron/files/ocf/neutron-agent-l3'], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/342d0b800eed8f496355d19b83240f17a74327fb', 'message': 'multiple start rescheduling after migration L3/DHCP agent\n\nThis workaround is safe, because starting rescheduling on alive agent do nothing\n\nChange-Id: I5b4ea431d6907e4d57d0f753d3720a470583c77f\nCloses-bug: #1371561\n'}]",3,123098,342d0b800eed8f496355d19b83240f17a74327fb,30,6,3,7468,,,0,"multiple start rescheduling after migration L3/DHCP agent

This workaround is safe, because starting rescheduling on alive agent do nothing

Change-Id: I5b4ea431d6907e4d57d0f753d3720a470583c77f
Closes-bug: #1371561
",git fetch https://review.opendev.org/openstack/fuel-library refs/changes/98/123098/2 && git format-patch -1 --stdout FETCH_HEAD,"['deployment/puppet/neutron/files/ocf/neutron-agent-dhcp', 'deployment/puppet/neutron/files/ocf/neutron-agent-l3']",2,5fb8a8aec7d0e0e22ff1f0e4c3f02f487f48be08,(detached," RESCHEDULIND_CMD=""q-agent-cleanup.py --agent=l3 --reschedule --remove-dead ${AUTH_TAIL} 2>&1 >> /var/log/neutron/rescheduling.log"" RESCH_CMD='' for ((i=0; i<5; i++)) ; do RESCH_CMD=""$RESCH_CMD sleep 33 ; $RESCHEDULIND_CMD ;"" done bash -c ""$RESCH_CMD"" &"," bash -c ""sleep 33 ; q-agent-cleanup.py --agent=l3 --reschedule --remove-dead ${AUTH_TAIL} 2>&1 >> /var/log/neutron/rescheduling.log "" &",12,2
openstack%2Fhorizon~stable%2Ficehouse~I0809908ec5d15a448691dccf526b66308cf929d4,openstack/horizon,stable/icehouse,I0809908ec5d15a448691dccf526b66308cf929d4,Proper port for LBaaS members,MERGED,2014-08-05 10:20:50.000000000,2014-09-26 22:21:12.000000000,2014-09-26 22:21:11.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 1420}, {'_account_id': 6282}, {'_account_id': 6914}, {'_account_id': 9656}, {'_account_id': 12000}]","[{'number': 1, 'created': '2014-08-05 10:20:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/973519fd0f320cc2313208ed9e6038884a7d74f9', 'message': 'Proper port for LBaaS members\n\nThis patch set makes loadbalancer select a port on pool subnet\nfor new members if possible.\n\nConflicts:\n        openstack_dashboard/dashboards/project/loadbalancers/tests.py\n        openstack_dashboard/dashboards/project/loadbalancers/workflows.py\n\nCloses-Bug: #1288859\n\nChange-Id: I0809908ec5d15a448691dccf526b66308cf929d4\n(cherry picked from commit 98bd3159d65b5d94b10403b6894002ea6de553b2)\n'}, {'number': 2, 'created': '2014-08-05 14:41:55.000000000', 'files': ['openstack_dashboard/dashboards/project/loadbalancers/tests.py', 'openstack_dashboard/dashboards/project/loadbalancers/workflows.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/95dcdaeff699b1642b5e4e43a505b6667eab3d2d', 'message': 'Proper port for LBaaS members\n\nThis patch set makes loadbalancer select a port on pool subnet\nfor new members if possible.\n\nConflicts:\n        openstack_dashboard/dashboards/project/loadbalancers/tests.py\n        openstack_dashboard/dashboards/project/loadbalancers/workflows.py\n\nCloses-Bug: #1288859\n\nChange-Id: I0809908ec5d15a448691dccf526b66308cf929d4\n(cherry picked from commit 98bd3159d65b5d94b10403b6894002ea6de553b2)\n'}]",11,111970,95dcdaeff699b1642b5e4e43a505b6667eab3d2d,18,7,2,6914,,,0,"Proper port for LBaaS members

This patch set makes loadbalancer select a port on pool subnet
for new members if possible.

Conflicts:
        openstack_dashboard/dashboards/project/loadbalancers/tests.py
        openstack_dashboard/dashboards/project/loadbalancers/workflows.py

Closes-Bug: #1288859

Change-Id: I0809908ec5d15a448691dccf526b66308cf929d4
(cherry picked from commit 98bd3159d65b5d94b10403b6894002ea6de553b2)
",git fetch https://review.opendev.org/openstack/horizon refs/changes/70/111970/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/loadbalancers/tests.py', 'openstack_dashboard/dashboards/project/loadbalancers/workflows.py']",2,973519fd0f320cc2313208ed9e6038884a7d74f9,bug/1288859," try: pool = api.lbaas.pool_get(request, context['pool_id']) subnet_id = pool['subnet_id'] except Exception: self.failure_message = _('Unable to retrieve the specified pool.') return False # Sort port list for each member. This is needed to avoid # attachment of random ports in case of creation of several # members attached to several networks. plist = sorted(plist, key=lambda port: port.network_id) psubnet = [p for p in plist for ips in p.fixed_ips if ips['subnet_id'] == subnet_id] # If possible, select a port on pool subnet. if psubnet: selected_port = psubnet[0] elif plist: selected_port = plist[0] else: selected_port = None if selected_port: context['address'] = selected_port.fixed_ips[0]['ip_address'] try: api.lbaas.member_create(request, **context).id except Exception: return False"," if plist: context['address'] = plist[0].fixed_ips[0]['ip_address'] try: context['member_id'] = api.lbaas.member_create( request, **context).id except Exception: return False",52,13
openstack%2Fhorizon~stable%2Ficehouse~If3ea4625ae50ae4561df3e566d1236b86226307b,openstack/horizon,stable/icehouse,If3ea4625ae50ae4561df3e566d1236b86226307b,Use default_project_id as user project for keystone v3,MERGED,2014-07-30 01:24:35.000000000,2014-09-26 22:16:12.000000000,2014-09-26 22:16:11.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 1420}, {'_account_id': 1941}, {'_account_id': 2455}, {'_account_id': 6914}, {'_account_id': 9317}, {'_account_id': 9576}, {'_account_id': 9622}, {'_account_id': 9656}]","[{'number': 1, 'created': '2014-07-30 01:24:35.000000000', 'files': ['openstack_dashboard/api/keystone.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/dc8e46fae02a3ac76bebc51513bc72d1bd29c58f', 'message': 'Use default_project_id as user project for keystone v3\n\nThis fixes the keystone api module so that it looks for the\ndefault_project_id attribute on the user and uses that as the\nprimary project.\n\nCloses-Bug: 1347840\nChange-Id: If3ea4625ae50ae4561df3e566d1236b86226307b\n(cherry picked from commit 03f9caa9d1e43ac4c4db4c10c16e07ca14daaf6e)\n'}]",0,110515,dc8e46fae02a3ac76bebc51513bc72d1bd29c58f,18,10,1,9647,,,0,"Use default_project_id as user project for keystone v3

This fixes the keystone api module so that it looks for the
default_project_id attribute on the user and uses that as the
primary project.

Closes-Bug: 1347840
Change-Id: If3ea4625ae50ae4561df3e566d1236b86226307b
(cherry picked from commit 03f9caa9d1e43ac4c4db4c10c16e07ca14daaf6e)
",git fetch https://review.opendev.org/openstack/horizon refs/changes/15/110515/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack_dashboard/api/keystone.py'],1,dc8e46fae02a3ac76bebc51513bc72d1bd29c58f,bug/1347840," user.project_id = getattr(user, ""default_project_id"", getattr(user, ""tenantId"", None))"," user.project_id = getattr(user, ""tenantId"", None)",2,1
openstack%2Fnova~stable%2Ficehouse~I00427379dc7aa39770f9a16ff026addb6e311735,openstack/nova,stable/icehouse,I00427379dc7aa39770f9a16ff026addb6e311735,Fix _parse_datetime in simple tenant usage extension,MERGED,2014-08-04 19:00:55.000000000,2014-09-26 22:13:49.000000000,2014-09-26 22:13:46.000000000,"[{'_account_id': 3}, {'_account_id': 1955}, {'_account_id': 5170}, {'_account_id': 5292}, {'_account_id': 6873}, {'_account_id': 9008}, {'_account_id': 9656}, {'_account_id': 10118}]","[{'number': 1, 'created': '2014-08-04 19:00:55.000000000', 'files': ['nova/tests/api/openstack/compute/contrib/test_simple_tenant_usage.py', 'nova/api/openstack/compute/contrib/simple_tenant_usage.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/825cfe4ab2696612a832e6a90e7ae69816fb7e28', 'message': 'Fix _parse_datetime in simple tenant usage extension\n\n_parse_datetime in os-simple-tenant-usage incorrectly attempts\nto parse the datetime string even when it is None. As a result\na 400 BadRequest is returned. This has the overall effect of making\nthe start and end datetime parameters compulsory when they are meant\nto be optional.\n\nNote that this restores the behavior where start/end are optional which is\nin contradiction to the API docs, but is the long standing API behaviour.\nThis was an API change which was accidentally applied in commit\nI8e0e870727d687da165c809ffb7a4456bff81122 as part of some nova internal\nchanges but not picked up by the unitests\n\nChange-Id: I00427379dc7aa39770f9a16ff026addb6e311735\nCloses-Bug: 1300972\n(cherry picked from commit 99e6ec9131c2e3eeb5fe3e1a4dc5a26e6302ebd2)\n'}]",4,111803,825cfe4ab2696612a832e6a90e7ae69816fb7e28,21,8,1,6873,,,0,"Fix _parse_datetime in simple tenant usage extension

_parse_datetime in os-simple-tenant-usage incorrectly attempts
to parse the datetime string even when it is None. As a result
a 400 BadRequest is returned. This has the overall effect of making
the start and end datetime parameters compulsory when they are meant
to be optional.

Note that this restores the behavior where start/end are optional which is
in contradiction to the API docs, but is the long standing API behaviour.
This was an API change which was accidentally applied in commit
I8e0e870727d687da165c809ffb7a4456bff81122 as part of some nova internal
changes but not picked up by the unitests

Change-Id: I00427379dc7aa39770f9a16ff026addb6e311735
Closes-Bug: 1300972
(cherry picked from commit 99e6ec9131c2e3eeb5fe3e1a4dc5a26e6302ebd2)
",git fetch https://review.opendev.org/openstack/nova refs/changes/03/111803/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/api/openstack/compute/contrib/test_simple_tenant_usage.py', 'nova/api/openstack/compute/contrib/simple_tenant_usage.py']",2,825cfe4ab2696612a832e6a90e7ae69816fb7e28,bug/1300972-icehouse," else: for fmt in [""%Y-%m-%dT%H:%M:%S"", ""%Y-%m-%dT%H:%M:%S.%f"", ""%Y-%m-%d %H:%M:%S.%f""]: try: value = parse_strtime(dtstr, fmt) break except exception.InvalidStrTime: pass else: msg = _(""Datetime is in invalid format"") raise exception.InvalidStrTime(reason=msg)"," for fmt in [""%Y-%m-%dT%H:%M:%S"", ""%Y-%m-%dT%H:%M:%S.%f"", ""%Y-%m-%d %H:%M:%S.%f""]: try: value = parse_strtime(dtstr, fmt) break except exception.InvalidStrTime: pass else: msg = _(""Datetime is in invalid format"") raise exception.InvalidStrTime(reason=msg)",30,10
openstack%2Fnova~stable%2Ficehouse~Iecd42239357b96450b6530c0486b27c68995c37f,openstack/nova,stable/icehouse,Iecd42239357b96450b6530c0486b27c68995c37f,Avoid traceback logs from simple tenant usage extension,MERGED,2014-08-04 19:00:55.000000000,2014-09-26 22:05:07.000000000,2014-09-26 22:05:04.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 1501}, {'_account_id': 1955}, {'_account_id': 5170}, {'_account_id': 5292}, {'_account_id': 5367}, {'_account_id': 6873}, {'_account_id': 7166}, {'_account_id': 7400}, {'_account_id': 9008}, {'_account_id': 9656}, {'_account_id': 10118}]","[{'number': 1, 'created': '2014-08-04 19:00:55.000000000', 'files': ['nova/tests/api/openstack/compute/contrib/test_simple_tenant_usage.py', 'nova/exception.py', 'nova/api/openstack/compute/contrib/simple_tenant_usage.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/073ee0697f951e12587e9bd05ce610474027fc4d', 'message': 'Avoid traceback logs from simple tenant usage extension\n\nAvoid generating traceback logs when invalid formatted datetime\nparameters are passed to the simple tenant usage\nextension. Exceptions have not been correctly handled by the\nos-simple-tenant usage extension which results in tracebacks\nbeing output to the log files. This patch correctly handles\nexceptions as a result of badly formatted datetime strings.\n\nConflicts:\n        nova/tests/api/openstack/compute/contrib/test_simple_tenant_usage.py\n\nChange-Id: Iecd42239357b96450b6530c0486b27c68995c37f\nPartial-Bug: 1300972\n(cherry picked from commit f60c72fac04a97b321a7f2bd0865c1ae81250882)\n'}]",0,111802,073ee0697f951e12587e9bd05ce610474027fc4d,42,13,1,6873,,,0,"Avoid traceback logs from simple tenant usage extension

Avoid generating traceback logs when invalid formatted datetime
parameters are passed to the simple tenant usage
extension. Exceptions have not been correctly handled by the
os-simple-tenant usage extension which results in tracebacks
being output to the log files. This patch correctly handles
exceptions as a result of badly formatted datetime strings.

Conflicts:
        nova/tests/api/openstack/compute/contrib/test_simple_tenant_usage.py

Change-Id: Iecd42239357b96450b6530c0486b27c68995c37f
Partial-Bug: 1300972
(cherry picked from commit f60c72fac04a97b321a7f2bd0865c1ae81250882)
",git fetch https://review.opendev.org/openstack/nova refs/changes/02/111802/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/api/openstack/compute/contrib/test_simple_tenant_usage.py', 'nova/exception.py', 'nova/api/openstack/compute/contrib/simple_tenant_usage.py']",3,073ee0697f951e12587e9bd05ce610474027fc4d,bug/1300972-icehouse,"def parse_strtime(dstr, fmt): try: return timeutils.parse_strtime(dstr, fmt) except (TypeError, ValueError) as e: raise exception.InvalidStrTime(reason=unicode(e)) for fmt in [""%Y-%m-%dT%H:%M:%S"", ""%Y-%m-%dT%H:%M:%S.%f"", ""%Y-%m-%d %H:%M:%S.%f""]: try: value = parse_strtime(dtstr, fmt) break except exception.InvalidStrTime: pass else: msg = _(""Datetime is in invalid format"") raise exception.InvalidStrTime(reason=msg) try: (period_start, period_stop, detailed) = self._get_datetime_range( req) except exception.InvalidStrTime as e: raise exc.HTTPBadRequest(explanation=e.format_message()) try: (period_start, period_stop, ignore) = self._get_datetime_range( req) except exception.InvalidStrTime as e: raise exc.HTTPBadRequest(explanation=e.format_message()) "," try: value = timeutils.parse_strtime(dtstr, ""%Y-%m-%dT%H:%M:%S"") except Exception: try: value = timeutils.parse_strtime(dtstr, ""%Y-%m-%dT%H:%M:%S.%f"") except Exception: value = timeutils.parse_strtime(dtstr, ""%Y-%m-%d %H:%M:%S.%f"") (period_start, period_stop, detailed) = self._get_datetime_range(req) (period_start, period_stop, ignore) = self._get_datetime_range(req)",63,8
openstack%2Fglance-specs~master~I1dd12c13df22315e8cbe6b253382ca05b434554b,openstack/glance-specs,master,I1dd12c13df22315e8cbe6b253382ca05b434554b,Proposal for Swift Store to use Multiple Containers,ABANDONED,2014-09-26 21:52:04.000000000,2014-09-26 22:03:01.000000000,,[{'_account_id': 11864}],"[{'number': 1, 'created': '2014-09-26 21:52:04.000000000', 'files': ['specs/kilo/swift-store-multiple-containers.rst'], 'web_link': 'https://opendev.org/openstack/glance-specs/commit/a3b803ae4da715ceaf2927e6643df54834f9c5df', 'message': 'Proposal for Swift Store to use Multiple Containers\n\nThis spec focuses on using multiple containers in glance swift store\n\nChange-Id: I1dd12c13df22315e8cbe6b253382ca05b434554b\n'}]",0,124521,a3b803ae4da715ceaf2927e6643df54834f9c5df,3,1,1,11864,,,0,"Proposal for Swift Store to use Multiple Containers

This spec focuses on using multiple containers in glance swift store

Change-Id: I1dd12c13df22315e8cbe6b253382ca05b434554b
",git fetch https://review.opendev.org/openstack/glance-specs refs/changes/21/124521/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/kilo/swift-store-multiple-containers.rst'],1,a3b803ae4da715ceaf2927e6643df54834f9c5df,,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Glance Swift Store to use Multiple Containers for Storing Images ========================================== Glance, when configured to use Swift store in Single Tenant Mode, stores images in one container as indicated by the configuration option, swift_store_container. This approach of storing images in ONE container is subject to performance bottleneck. Storing images in one container is prone to Swift rate-limiting on containers . Swift is equipped with container rate-limiting that can throttle concurrent POST, PUT and DELETE operations in a single container. This becomes a serious issue in a large scale deployments especially when coupled with smaller segment sizes. Problem description =================== Swift is known to be capable of throttling incoming traffic[1]. The very fact that swift can throttle write operations on containers presents a performance bottleneck and hence large scale deployments need an alternative to get around Swift throttling. When container rate-limiting is enabled for a Swift cluster, it throttles concurrent POST, PUT and DELETE requests after a certain configurable rate. This directly translates to a limit on concurrent image creation and deletion operations for Glance before experiencing performance degradation. Proposed change =============== To reduce/overcome the performance bottleneck, we propose the use of multiple containers for storing images in Single Tenant Mode (this change will not affect Multi Tenant Mode because that setup stores each image in its own container). This leads to increased concurrency of image creation and deletion operations. There are four major aspects to this change: - Container Selection - determining what container an image should go into - Container Creation - creating the new containers - Re-distribution of Existing Images - moving images from old to new containers - Database Migration - updating image locations as per new containers **1) Container Selection:** This change proposes to select containers based on image uuid. Images will be stored in multiple containers in order to avoid throttling during multiple simultaneous uploads. The first N characters of the image UUID (where N is a configurable integer between 1 and 32 with the default value of 2) will be used to determine which container the image will be uploaded to. With the default value of the first two characters used, this gives 16*16=256 unique containers. At N=1, the smallest valid value for this configuration, 16 containers will be created and used for storing images. The containers will be named after the value set for swift_store_container with the first N chars of the image UUID as the suffix. The number of containers can be easily increased or decreased by changing N in the configuration. However, a new set of containers will be created with every change to this configuration. Images created after a configuration change will go into new containers while older images remain in their previous containers. The older images do not need to necessarily be moved into new containers as their locations would still point to the existing older container they are stored in. However, if one wishes to move the older images to new containers, they may do so by re-distributing the images, which is described later in this section. **2) Container Creation:** Glance ships with a configuration option to dynamically create the container, if it doesn't exist already at the time of uploading image data to Swift. This is indicated by configuration option, swift_store_create_container_on_put. If dynamic container creation is enabled, Glance would automatically create each container when the appropriate container for that image is not found. However, if the config option for dynamic container creation is disabled, image uploads would fail if the appropriate containers are not created manually by the deployer. This behavior is consistent with how Glance currently handles missing containers if the config option to create them is not enabled. **3) Re-distribution of Existing Images:** This spec will not provide code or scripts migrate existing images since lazy loading is an existing effective method of distributing new images. However, if one wants to migrate images here is the process: Once the use of multiple containers is enabled or the number of containers is changed, all previously created images would remain in the older container(s). If desired, older images can be moved to new containers appropriately. This can be achieved as a separate batch job that can be run as and when desired. Subject to the number of older images, redistributing images may involve significant movement of data in the Swift cluster. Hence, it would be helpful to achieve this in phases and in a non-intrusive fashion. Once the images are re-distributed, their image locations need to be updated as well. **4) Database Migration:** If images are re-distributed by operator choice, image location of each re-distributed image must be updated to reflect the new container name. This requires a db migration to replace the old container name in the location with the new container name as per the image id. This migration can go hand-in-hand with re-distribution. **Scope of this spec:** Of the four aspects discussed above, this specification would only addresses container creation and selection while leaving re-distribution and the required db migrations out, which can be implemented as another concerted effort. Alternatives ------------ 1) Instead of using image id as the basis for container selection, one can use other basis like tenant id, which would keep all images belonging to a certain tenant in the same container. While there are may be many other bases possible, using image id provides an easier way to correlate an image to its container. 2) An alternative to creating containers could be to allow the API to create all the required containers while it boots up. This requires the API to know all possible containers before hand, which may or may not be possible depending upon the container selection basis chosen. This places a certain limitation on the kind of bases one may opt for. Hence, going with dynamic container creation will eliminate this limitation as both container selection and creation could be dynamic. Also, dynamic container creation is in-line with current Glance behavior. Data model impact ----------------- New containers will be created and used for storing images. However, this does not have any impact on the Glance image data model itself. **Database migrations**: No database migrations are required. The code supporting multiple containers would only affect the uploading of new images, determining which container they belong to based on uuid. For existing images (those uploaded before support for multiple containers), the image already contains a valid location in its metadata. Essentially, new containers will be populated by lazy loading: When an image is uploading, it will first check through a HEAD request if the appropriate container exists for that image based on its UUID, and if the container does not exist then the container will be created immediately with a PUT request. This image will then be the first image stored in that particular container. REST API impact --------------- None Security impact --------------- Given the scope of this spec, where image data is not being re-distributed among new containers and no migrations are being run, there is minimal to no security impact introduced. Notifications impact -------------------- This change only impacts the image location property among all the image properties. And, since image location is not included in notifications, there should be no impact to Glance notifications. Other end user impact --------------------- As image location is not accessible to either the end-user or from Glance client, there should be no end-user impact. Performance Impact ------------------ The use of multiple containers will reduce throttling when multiple images are uploaded simultaneously. This leads to increased concurrency of image creation and deletion operations in large scale deployments. Container selection would take place for every image upload request and thus adds an extra operation to the current set of operations to upload image data. However, selecting a container would be a simple substring operation to fetch the first few characters of an image id. The time incurred in determining the container would be significantly smaller than the time incurred to upload image data. Overall, the performance impact of container selection should be very minimal. Container creation is a conditional operation that would take place only when the container is not present already. This would occur once for each combination of N characters as specified in the configuration. For example, the default configuration option is that the first 2 characters of the image UUID are used to select an appropriate container, leading to a total of 256 containers which should be optimal for mid size deployments. We found that in a large scale deployment, 4096 containers would be preferred over 256 containers if smaller segment sizes were chosen. The time incurred in creating a new container is significantly smaller than the time incurred in upload image data. Hence, the overall performance impact in image uploads should be minimal. Other deployer impact --------------------- This change would begin taking effect upon enabling multiple containers in a configuration. When enabled, new images would be uploaded to new containers, while existing images would remain in their previously assigned container. This change is forwards and backwards compatible, such that the deployer can choose to enable or disable multiple containers at any time and images will still upload and download correctly. Deployers should note that if their deployment limits the total number of containers per account, the seed for the total number of containers should be set such that this limit is not hit. New configuration options in *glance-api.conf* **swift_store_use_multiple_containers** - default = False - A boolean value that determines if single-tenant store should use multiple containers for storing images. Used only when swift_store_multi_tenant is disabled. **swift_store_multiple_containers_seed** - default = 2 - An integer value between 1 and 32 representing the number of characters used from the image UUID to determine which container the image will be placed. Used only when swift_store_use_multiple_containers is enabled. The total number of containers that will be used is equal to 16^N, or 16^2=256 by default. Developer impact ---------------- None Implementation ============== Assignee(s) ----------- Primary assignee: hemanth-makkapati Other contributors: ben-roble Work Items ---------- 1) Implement new config options in Swift store driver 2) Implement container selection in Swift store driver 3) Implement unit, functional, and integration tests 4) Change glance-api sample conf in glance repo Points to note: - All code changes would be limited to glance_store module. - Image download code wouldn't require any changes. - Both manifest and segments would go into the same container. Dependencies ============ None Testing ======= No tempest tests needed Documentation Impact ==================== * Document new configuration options References ========== [1] http://docs.openstack.org/developer/swift/ratelimit.html#configuration ",,290,0
openstack%2Fcookbook-openstack-network~master~I04956a15faa3bc72c71bc27908675d2e30239e86,openstack/cookbook-openstack-network,master,I04956a15faa3bc72c71bc27908675d2e30239e86,Add attribute for ML2 enable_ipset,MERGED,2014-09-19 16:41:00.000000000,2014-09-26 21:20:11.000000000,2014-09-26 21:20:11.000000000,"[{'_account_id': 3}, {'_account_id': 1032}, {'_account_id': 2589}, {'_account_id': 7128}, {'_account_id': 9970}, {'_account_id': 12323}]","[{'number': 1, 'created': '2014-09-19 16:41:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-network/commit/e69944ad0025bae43697670c7c1735ca86da504c', 'message': 'Add attribute for ML2 enable_ipset\n\nAdd enable_ipset within securitygroup section for\nml2_conf.ini template.\n\nChange-Id: I04956a15faa3bc72c71bc27908675d2e30239e86\nCloses-Bug: #1371684\n'}, {'number': 2, 'created': '2014-09-22 19:58:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-network/commit/e32a978b5c6914fc0ab8bc149fd2d76f173233c9', 'message': 'Add attribute for ML2 enable_ipset\n\nAdd enable_ipset within securitygroup section for\nml2_conf.ini template.\n\nChange-Id: I04956a15faa3bc72c71bc27908675d2e30239e86\nCloses-Bug: #1371684\n'}, {'number': 3, 'created': '2014-09-24 15:34:05.000000000', 'files': ['templates/default/plugins/ml2/ml2_conf.ini.erb', 'attributes/default.rb', 'spec/server_spec.rb', 'CHANGELOG.md'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-network/commit/2a71aac70cc2ccbb42bb33bd1f8024c1f07bf11f', 'message': 'Add attribute for ML2 enable_ipset\n\nAdd enable_ipset within securitygroup section for\nml2_conf.ini template.\n\nChange-Id: I04956a15faa3bc72c71bc27908675d2e30239e86\nCloses-Bug: #1371684\n'}]",0,122787,2a71aac70cc2ccbb42bb33bd1f8024c1f07bf11f,20,6,3,7128,,,0,"Add attribute for ML2 enable_ipset

Add enable_ipset within securitygroup section for
ml2_conf.ini template.

Change-Id: I04956a15faa3bc72c71bc27908675d2e30239e86
Closes-Bug: #1371684
",git fetch https://review.opendev.org/openstack/cookbook-openstack-network refs/changes/87/122787/3 && git format-patch -1 --stdout FETCH_HEAD,"['templates/default/plugins/ml2/ml2_conf.ini.erb', 'attributes/default.rb', 'spec/server_spec.rb', 'CHANGELOG.md']",4,e69944ad0025bae43697670c7c1735ca86da504c,bug/1371684,* Add attribute for ML2 enable_ipset,,11,1
openstack%2Fcookbook-openstack-identity~master~I1043d8e9137ea2a6c8f84ee3a2da4f98e3c82521,openstack/cookbook-openstack-identity,master,I1043d8e9137ea2a6c8f84ee3a2da4f98e3c82521,Allow admin_bind_host to be settable in the keystone.conf,MERGED,2014-09-11 15:13:58.000000000,2014-09-26 21:18:46.000000000,2014-09-26 21:18:46.000000000,"[{'_account_id': 3}, {'_account_id': 1032}, {'_account_id': 2589}, {'_account_id': 7128}, {'_account_id': 8410}, {'_account_id': 11915}]","[{'number': 1, 'created': '2014-09-11 15:13:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-identity/commit/2e544d895247793eeb177f68e4697c4467f720c6', 'message': 'Allow admin_bind_host to be settable in the keystone.conf template\n\nChange server recipe and template to allow the admin_bind_host\nto be set to the identity_admin endpoint defined in Common.\n\nChange-Id: I1043d8e9137ea2a6c8f84ee3a2da4f98e3c82521\nCloses-Bug: #1368282\n'}, {'number': 2, 'created': '2014-09-12 20:12:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-identity/commit/06688d8af7dc2b147f8793fae8623cff7a96d6c5', 'message': 'Allow admin_bind_host to be settable in the keystone.conf template\n\nChange server recipe and template to allow the admin_bind_host\nto be set to the identity_admin endpoint defined in Common.\n\nChange-Id: I1043d8e9137ea2a6c8f84ee3a2da4f98e3c82521\nCloses-Bug: #1368282\n'}, {'number': 3, 'created': '2014-09-12 20:13:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-identity/commit/16553a6a883eccc9415890822d566f3b4c380f9f', 'message': 'Allow admin_bind_host to be settable in the keystone.conf\n\nChange server recipe and template to allow the admin_bind_host\nto be set to the identity_admin endpoint defined in Common.\n\nChange-Id: I1043d8e9137ea2a6c8f84ee3a2da4f98e3c82521\nCloses-Bug: #1368282\n'}, {'number': 4, 'created': '2014-09-15 14:37:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-identity/commit/a0f54b45588ed4715b22d083472aeb80fe6d2b92', 'message': 'Allow admin_bind_host to be settable in the keystone.conf\n\nChange server recipe and template to allow the admin_bind_host\nto be set to the identity_admin endpoint defined in Common.\n\nChange-Id: I1043d8e9137ea2a6c8f84ee3a2da4f98e3c82521\nCloses-Bug: #1368282\n'}, {'number': 5, 'created': '2014-09-15 16:11:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-identity/commit/b4a08c1d6f0bbd285a54a378b5bb7534334aa0d2', 'message': 'Allow admin_bind_host to be settable in the keystone.conf\n\nChange server recipe and template to allow the admin_bind_host\nto be set to the identity_admin endpoint defined in Common.\n\nChange-Id: I1043d8e9137ea2a6c8f84ee3a2da4f98e3c82521\nCloses-Bug: #1368282\n'}, {'number': 6, 'created': '2014-09-23 16:24:32.000000000', 'files': ['templates/default/keystone.conf.erb', 'spec/server_spec.rb', 'CHANGELOG.md', 'metadata.rb', 'recipes/server.rb'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-identity/commit/0ab0cd3defe65d4040429b3ec9b2dfc622f31def', 'message': 'Allow admin_bind_host to be settable in the keystone.conf\n\nChange server recipe and template to allow the admin_bind_host\nto be set to the identity_admin endpoint defined in Common.\n\nChange-Id: I1043d8e9137ea2a6c8f84ee3a2da4f98e3c82521\nCloses-Bug: #1368282\n'}]",2,120809,0ab0cd3defe65d4040429b3ec9b2dfc622f31def,31,6,6,7128,,,0,"Allow admin_bind_host to be settable in the keystone.conf

Change server recipe and template to allow the admin_bind_host
to be set to the identity_admin endpoint defined in Common.

Change-Id: I1043d8e9137ea2a6c8f84ee3a2da4f98e3c82521
Closes-Bug: #1368282
",git fetch https://review.opendev.org/openstack/cookbook-openstack-identity refs/changes/09/120809/6 && git format-patch -1 --stdout FETCH_HEAD,"['templates/default/keystone.conf.erb', 'spec/server_spec.rb', 'CHANGELOG.md', 'recipes/server.rb']",4,2e544d895247793eeb177f68e4697c4467f720c6,bug/1368282,"admin_address = identity_admin_endpoint.host admin_address: admin_address,",,24,1
openstack%2Fcookbook-openstack-common~master~I89298db2461b59ab8038dfc01d42f0eb69a8b58c,openstack/cookbook-openstack-common,master,I89298db2461b59ab8038dfc01d42f0eb69a8b58c,Separate endpoints for vncserver_listen and vncserver_proxyclient_address,MERGED,2014-09-10 16:32:05.000000000,2014-09-26 21:17:55.000000000,2014-09-26 21:17:54.000000000,"[{'_account_id': 3}, {'_account_id': 1032}, {'_account_id': 2589}, {'_account_id': 7128}, {'_account_id': 8410}, {'_account_id': 12323}]","[{'number': 1, 'created': '2014-09-10 16:32:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-common/commit/e33c61505e0a012e65f3a2c67637d499e399019f', 'message': 'Separate endpoints for vncserver_listen and vncserver_proxyclient_address\n\n* define new endpoint for proxy based on existing vnc endpoint\n* will be used in Compute  nova-common recipe for nova.conf\n\nChange-Id: I89298db2461b59ab8038dfc01d42f0eb69a8b58c\nPartial-Bug: #1367807\n'}, {'number': 2, 'created': '2014-09-10 17:15:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-common/commit/b5c408ae1f368f985adbc46f628fc988cda80c6c', 'message': 'Separate endpoints for vncserver_listen and vncserver_proxyclient_address\n\n* define new endpoint for proxy based on existing vnc endpoint\n* will be used in Compute  nova-common recipe for nova.conf\n\nChange-Id: I89298db2461b59ab8038dfc01d42f0eb69a8b58c\nPartial-Bug: #1367807\n'}, {'number': 3, 'created': '2014-09-15 14:38:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-common/commit/6e281badb0fd6a67802d1cd06ad5a6c40862e610', 'message': 'Separate endpoints for vncserver_listen and vncserver_proxyclient_address\n\n* define new endpoint for proxy based on existing vnc endpoint\n* will be used in Compute  nova-common recipe for nova.conf\n\nChange-Id: I89298db2461b59ab8038dfc01d42f0eb69a8b58c\nPartial-Bug: #1367807\n'}, {'number': 4, 'created': '2014-09-23 16:11:50.000000000', 'files': ['attributes/default.rb', 'CHANGELOG.md', 'metadata.rb'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-common/commit/6a44710656f618c1363f0b9372bccd2a6f4d07ec', 'message': 'Separate endpoints for vncserver_listen and vncserver_proxyclient_address\n\n* define new endpoint for proxy based on existing vnc endpoint\n* will be used in Compute  nova-common recipe for nova.conf\n\nChange-Id: I89298db2461b59ab8038dfc01d42f0eb69a8b58c\nPartial-Bug: #1367807\n'}]",0,120474,6a44710656f618c1363f0b9372bccd2a6f4d07ec,25,6,4,7128,,,0,"Separate endpoints for vncserver_listen and vncserver_proxyclient_address

* define new endpoint for proxy based on existing vnc endpoint
* will be used in Compute  nova-common recipe for nova.conf

Change-Id: I89298db2461b59ab8038dfc01d42f0eb69a8b58c
Partial-Bug: #1367807
",git fetch https://review.opendev.org/openstack/cookbook-openstack-common refs/changes/74/120474/2 && git format-patch -1 --stdout FETCH_HEAD,"['attributes/default.rb', 'CHANGELOG.md']",2,e33c61505e0a012e65f3a2c67637d499e399019f,bug/1367807,* Separate endpoints for vncserver_listen and vncserver_proxyclient_address,,5,0
openstack%2Fneutron~master~If3df9770fa9e2d2eada932ee5f243d3458bf7261,openstack/neutron,master,If3df9770fa9e2d2eada932ee5f243d3458bf7261,"Do not lookup l3-agent for floating IP if host=None, dvr issue",MERGED,2014-09-18 01:55:37.000000000,2014-09-26 21:14:10.000000000,2014-09-24 16:28:41.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 748}, {'_account_id': 1038}, {'_account_id': 1131}, {'_account_id': 5170}, {'_account_id': 6876}, {'_account_id': 7016}, {'_account_id': 7448}, {'_account_id': 7787}, {'_account_id': 8645}, {'_account_id': 9077}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10386}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-09-18 01:55:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/36314146c7dc3129dbfa379664b59b714daad9e1', 'message': 'Do not lookup l3-agent for floating IP if host=None\n\nIf a floating IP has been associated with a port, but the port\nhas not been associated with an instance, attempting to lookup\nthe l3-agent hosting it will cause an AgentNotFoundByTypeHost\nexception.  Just skip it and go onto the next one.\n\nChange-Id: If3df9770fa9e2d2eada932ee5f243d3458bf7261\nCloses-Bug: #1370795\n'}, {'number': 2, 'created': '2014-09-18 13:58:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8814f1b1a161b13d3df1f0d47bcfc24e1a4d3c8b', 'message': 'Do not lookup l3-agent for floating IP if host=None, dvr issue\n\nIf a floating IP has been associated with a port, but the port\nhas not been associated with an instance, attempting to lookup\nthe l3-agent hosting it will cause an AgentNotFoundByTypeHost\nexception.  Just skip it and go onto the next one.\n\nChange-Id: If3df9770fa9e2d2eada932ee5f243d3458bf7261\nCloses-Bug: #1370795\n'}, {'number': 3, 'created': '2014-09-18 13:59:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/23d6cf4ec8e171f005a02ab1936a7de3fd002a68', 'message': 'Do not lookup l3-agent for floating IP if host=None, dvr issue\n\nIf a floating IP has been associated with a port, but the port\nhas not been associated with an instance, attempting to lookup\nthe l3-agent hosting it will cause an AgentNotFoundByTypeHost\nexception.  Just skip it and go onto the next one.\n\nChange-Id: If3df9770fa9e2d2eada932ee5f243d3458bf7261\nCloses-Bug: #1370795\n'}, {'number': 4, 'created': '2014-09-18 20:47:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/10b108f3da4ffcc647d52db682e2fbc1971a46e4', 'message': 'Do not lookup l3-agent for floating IP if host=None, dvr issue\n\nIf a floating IP has been associated with a port, but the port\nhas not been associated with an instance, attempting to lookup\nthe l3-agent hosting it will cause an AgentNotFoundByTypeHost\nexception.  Just skip it and go onto the next one.\n\nChange-Id: If3df9770fa9e2d2eada932ee5f243d3458bf7261\nCloses-Bug: #1370795\n'}, {'number': 5, 'created': '2014-09-23 17:56:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/0e7d933242756ad734007f1946cd5616d8b78534', 'message': 'Do not lookup l3-agent for floating IP if host=None, dvr issue\n\nIf a floating IP has been associated with a port, but the port\nhas not been associated with an instance, attempting to lookup\nthe l3-agent hosting it will cause an AgentNotFoundByTypeHost\nexception.  Just skip it and go onto the next one.\n\nChange-Id: If3df9770fa9e2d2eada932ee5f243d3458bf7261\nCloses-Bug: #1370795\n'}, {'number': 6, 'created': '2014-09-23 19:08:49.000000000', 'files': ['neutron/tests/unit/db/test_l3_dvr_db.py', 'neutron/db/l3_dvr_db.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/4478eee9e55193746b43ad5158be5fee29aef8a3', 'message': 'Do not lookup l3-agent for floating IP if host=None, dvr issue\n\nIf a floating IP has been associated with a port, but the port\nhas not been associated with an instance, attempting to lookup\nthe l3-agent hosting it will cause an AgentNotFoundByTypeHost\nexception.  Just skip it and go onto the next one.\n\nChange-Id: If3df9770fa9e2d2eada932ee5f243d3458bf7261\nCloses-Bug: #1370795\n'}]",17,122298,4478eee9e55193746b43ad5158be5fee29aef8a3,133,30,6,1131,,,0,"Do not lookup l3-agent for floating IP if host=None, dvr issue

If a floating IP has been associated with a port, but the port
has not been associated with an instance, attempting to lookup
the l3-agent hosting it will cause an AgentNotFoundByTypeHost
exception.  Just skip it and go onto the next one.

Change-Id: If3df9770fa9e2d2eada932ee5f243d3458bf7261
Closes-Bug: #1370795
",git fetch https://review.opendev.org/openstack/neutron refs/changes/98/122298/2 && git format-patch -1 --stdout FETCH_HEAD,['neutron/db/l3_dvr_db.py'],1,36314146c7dc3129dbfa379664b59b714daad9e1,bug/1370795," if floating_ip['host']: fip_agent = self._get_agent_by_type_and_host( context, l3_const.AGENT_TYPE_L3, floating_ip['host']) LOG.debug(""FIP Agent: %s"", fip_agent['id']) floatingip_agent_intfs = self.get_fip_sync_interfaces( context, fip_agent['id']) LOG.debug(""FIP Agent ports: %s"", floatingip_agent_intfs)"," fip_agent = self._get_agent_by_type_and_host( context, l3_const.AGENT_TYPE_L3, floating_ip['host']) LOG.debug(""FIP Agent : %s "", fip_agent['id']) floatingip_agent_intfs = self.get_fip_sync_interfaces( context, fip_agent['id']) LOG.debug(""FIP Agent ports: %s"", floatingip_agent_intfs)",9,7
openstack%2Fcookbook-openstack-ops-messaging~master~I6429d8f770b79ab099c9af5110fe49c88e74bfa0,openstack/cookbook-openstack-ops-messaging,master,I6429d8f770b79ab099c9af5110fe49c88e74bfa0,Fix metadata constriant for Common,MERGED,2014-09-24 15:48:48.000000000,2014-09-26 21:12:46.000000000,2014-09-26 21:12:45.000000000,"[{'_account_id': 3}, {'_account_id': 2589}, {'_account_id': 12323}]","[{'number': 1, 'created': '2014-09-24 15:48:48.000000000', 'files': ['CHANGELOG.md', 'metadata.rb'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-ops-messaging/commit/b8de0c99aeb9c1880fe6e93d771efec79416197e', 'message': 'Fix metadata constriant for Common\n\nA patch went in that needed attributes in Common and the ops-messaging\nmetadata was change to ~> 10.0.1 which worked until Common\nwas recently bumped to 10.1.  The metadata for Common should have had\na minor version bump, not a patch bump for the new attributes.\n\nChange-Id: I6429d8f770b79ab099c9af5110fe49c88e74bfa0\nCloses-Bug: #1373483\n'}]",0,123767,b8de0c99aeb9c1880fe6e93d771efec79416197e,7,3,1,7128,,,0,"Fix metadata constriant for Common

A patch went in that needed attributes in Common and the ops-messaging
metadata was change to ~> 10.0.1 which worked until Common
was recently bumped to 10.1.  The metadata for Common should have had
a minor version bump, not a patch bump for the new attributes.

Change-Id: I6429d8f770b79ab099c9af5110fe49c88e74bfa0
Closes-Bug: #1373483
",git fetch https://review.opendev.org/openstack/cookbook-openstack-ops-messaging refs/changes/67/123767/1 && git format-patch -1 --stdout FETCH_HEAD,"['CHANGELOG.md', 'metadata.rb']",2,b8de0c99aeb9c1880fe6e93d771efec79416197e,bug/1373483,"depends 'openstack-common', '~> 10.0'","depends 'openstack-common', '~> 10.0.1'",2,1
openstack%2Fzaqar~master~Ie01cee026e7ebf530cdb2709e2c17d030ad95480,openstack/zaqar,master,Ie01cee026e7ebf530cdb2709e2c17d030ad95480,Replace misleading max_message_size option name,MERGED,2014-09-22 19:12:22.000000000,2014-09-26 21:10:14.000000000,2014-09-26 21:10:13.000000000,"[{'_account_id': 3}, {'_account_id': 6159}, {'_account_id': 6413}, {'_account_id': 6427}, {'_account_id': 9219}]","[{'number': 1, 'created': '2014-09-22 19:12:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zaqar/commit/d3e175ebdbba1d41742c5dfc390b814d1c59e395', 'message': 'Replace misleading max_message_size option name\n\nThe max_message_size option name is misleading. The option\ndetermines max size of message post body. Replacing the\noption name to max_size_message_post_body.\n\nChange-Id: Ie01cee026e7ebf530cdb2709e2c17d030ad95480\nCloses-Bug: #1357397\n'}, {'number': 2, 'created': '2014-09-22 20:23:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zaqar/commit/eb18863e3c379dce8eb71df3e5fb5fd8863720d5', 'message': 'Replace misleading max_message_size option name\n\nThe max_message_size option name is misleading. The option\ndetermines max size of message post body. Replacing the\noption name to max_size_message_post_body.\n\nChange-Id: Ie01cee026e7ebf530cdb2709e2c17d030ad95480\nCloses-Bug: #1357397\n'}, {'number': 3, 'created': '2014-09-22 20:44:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/zaqar/commit/26b20916f0c9b70b781affd2126484f80782c82c', 'message': 'Replace misleading max_message_size option name\n\nThe max_message_size option name is misleading. The option\ndetermines max size of message post body. Replacing the\noption name to max_size_message_post_body.\n\nChange-Id: Ie01cee026e7ebf530cdb2709e2c17d030ad95480\nCloses-Bug: #1357397\n'}, {'number': 4, 'created': '2014-09-22 20:46:23.000000000', 'files': ['tests/functional/wsgi/v1_1/test_messages.py', 'zaqar/tests/queues/transport/wsgi/v1/test_messages.py', 'etc/zaqar.conf.sample', 'doc/user-guide/zaqar-config-ref/common/tables/zaqar-transport.xml', 'zaqar/tests/queues/transport/wsgi/v1_1/test_messages.py', 'doc/user-guide/zaqar-config-ref/autogen/zaqar.flagmappings', 'tests/etc/wsgi_sqlalchemy_validation.conf', 'zaqar/queues/transport/validation.py', 'zaqar/tests/queues/transport/wsgi/v1_1/test_validation.py', 'tests/etc/functional-zaqar.conf', 'tests/functional/wsgi/v1/test_messages.py', 'zaqar/tests/queues/transport/wsgi/v1/test_validation.py'], 'web_link': 'https://opendev.org/openstack/zaqar/commit/6253626829fd94d2cc18332309048c8ef21c671d', 'message': 'Replace misleading max_message_size option name\n\nThe max_message_size option name is misleading. The option\ndetermines max size of message post body. Replacing the\noption name to max_messages_post_size.\n\nChange-Id: Ie01cee026e7ebf530cdb2709e2c17d030ad95480\nCloses-Bug: #1357397\n'}]",3,123223,6253626829fd94d2cc18332309048c8ef21c671d,16,5,4,9219,,,0,"Replace misleading max_message_size option name

The max_message_size option name is misleading. The option
determines max size of message post body. Replacing the
option name to max_messages_post_size.

Change-Id: Ie01cee026e7ebf530cdb2709e2c17d030ad95480
Closes-Bug: #1357397
",git fetch https://review.opendev.org/openstack/zaqar refs/changes/23/123223/2 && git format-patch -1 --stdout FETCH_HEAD,"['tests/functional/wsgi/v1_1/test_messages.py', 'zaqar/tests/queues/transport/wsgi/v1/test_messages.py', 'etc/zaqar.conf.sample', 'doc/user-guide/zaqar-config-ref/common/tables/zaqar-transport.xml', 'zaqar/tests/queues/transport/wsgi/v1_1/test_messages.py', 'doc/user-guide/zaqar-config-ref/autogen/zaqar.flagmappings', 'tests/etc/wsgi_sqlalchemy_validation.conf', 'zaqar/queues/transport/validation.py', 'zaqar/tests/queues/transport/wsgi/v1_1/test_validation.py', 'tests/etc/functional-zaqar.conf', 'tests/functional/wsgi/v1/test_messages.py', 'zaqar/tests/queues/transport/wsgi/v1/test_validation.py']",12,d3e175ebdbba1d41742c5dfc390b814d1c59e395,bug/1357397," max_size_message_post_body = 256 obj['b'] = 'x' * (max_size_message_post_body - envelope_length + 1) for long_body in ('a' * (max_size_message_post_body - 2 + 1), obj):"," max_message_size = 256 obj['b'] = 'x' * (max_message_size - envelope_length + 1) for long_body in ('a' * (max_message_size - 2 + 1), obj):",20,20
openstack%2Fcookbook-openstack-dashboard~master~I72c5555750530b15812ef68d6c731ad18931fda0,openstack/cookbook-openstack-dashboard,master,I72c5555750530b15812ef68d6c731ad18931fda0,Add sensitive flag to key and certificate resources,MERGED,2014-09-19 21:20:10.000000000,2014-09-26 21:09:20.000000000,2014-09-26 21:09:20.000000000,"[{'_account_id': 3}, {'_account_id': 2589}, {'_account_id': 7128}, {'_account_id': 12323}]","[{'number': 1, 'created': '2014-09-19 21:20:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-dashboard/commit/66ca732903d586a25ce95385f07bc9ded1670c9c', 'message': 'Add sensitive flag to key and certificate resources\n\nUse to ensure that sensitive resource data is not logged by the chef-client.\nNeed to update Foodcritic to 4.0 to handle new sensitive attribute.\n\nChange-Id: I72c5555750530b15812ef68d6c731ad18931fda0\nCloses-Bug: #1371794\n'}, {'number': 2, 'created': '2014-09-23 15:14:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-dashboard/commit/c589911d46d78102f8f7f2ea03c199f6654090d8', 'message': 'Add sensitive flag to key and certificate resources\n\nUse to ensure that sensitive resource data is not logged by the chef-client.\nNeed to update Foodcritic to 4.0 to handle new sensitive attribute.\n\nChange-Id: I72c5555750530b15812ef68d6c731ad18931fda0\nCloses-Bug: #1371794\n'}, {'number': 3, 'created': '2014-09-24 15:35:47.000000000', 'files': ['Gemfile', 'spec/server_spec.rb', 'CHANGELOG.md', 'recipes/server.rb'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-dashboard/commit/d3b03107994f0384423a5dc54cf47324a17644fd', 'message': 'Add sensitive flag to key and certificate resources\n\nUse to ensure that sensitive resource data is not logged by the chef-client.\nNeed to update Foodcritic to 4.0 to handle new sensitive attribute.\n\nChange-Id: I72c5555750530b15812ef68d6c731ad18931fda0\nCloses-Bug: #1371794\n'}]",0,122866,d3b03107994f0384423a5dc54cf47324a17644fd,18,4,3,7128,,,0,"Add sensitive flag to key and certificate resources

Use to ensure that sensitive resource data is not logged by the chef-client.
Need to update Foodcritic to 4.0 to handle new sensitive attribute.

Change-Id: I72c5555750530b15812ef68d6c731ad18931fda0
Closes-Bug: #1371794
",git fetch https://review.opendev.org/openstack/cookbook-openstack-dashboard refs/changes/66/122866/2 && git format-patch -1 --stdout FETCH_HEAD,"['Gemfile', 'spec/server_spec.rb', 'CHANGELOG.md', 'recipes/server.rb']",4,66ca732903d586a25ce95385f07bc9ded1670c9c,bug/1371794, sensitive true sensitive true sensitive true sensitive true,,24,4
openstack%2Fcookbook-openstack-compute~master~I57a622f0370845f84c47a1ac145090726c819519,openstack/cookbook-openstack-compute,master,I57a622f0370845f84c47a1ac145090726c819519,Allow vnc_keymap to be configured attribute,MERGED,2014-09-02 19:51:46.000000000,2014-09-26 21:07:31.000000000,2014-09-26 21:07:31.000000000,"[{'_account_id': 3}, {'_account_id': 1032}, {'_account_id': 2589}, {'_account_id': 7128}, {'_account_id': 8410}, {'_account_id': 12323}]","[{'number': 1, 'created': '2014-09-02 19:51:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-compute/commit/9ef88496678996273ff5402d6f6d5b03f3534084', 'message': 'Allow vnc_keymap to be configured attribute\n\n* add new attribute vnc_keymap default en-us\n\nChange-Id: I57a622f0370845f84c47a1ac145090726c819519\nCloses-bug: #1346085\n'}, {'number': 2, 'created': '2014-09-19 15:27:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-compute/commit/4befed71943e8de941bd72ee099e35f45869ff00', 'message': 'Allow vnc_keymap to be configured attribute\n\n* add new attribute vnc_keymap default en-us\n\nChange-Id: I57a622f0370845f84c47a1ac145090726c819519\nCloses-bug: #1346085\n'}, {'number': 3, 'created': '2014-09-22 19:15:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-compute/commit/eacf28854f46838577b511ca7a5084986bb79176', 'message': 'Allow vnc_keymap to be configured attribute\n\n* add new attribute vnc_keymap default en-us\n\nChange-Id: I57a622f0370845f84c47a1ac145090726c819519\nCloses-bug: #1346085\n'}, {'number': 4, 'created': '2014-09-25 22:18:45.000000000', 'files': ['attributes/default.rb', 'spec/nova-common_spec.rb', 'CHANGELOG.md', 'templates/default/nova.conf.erb'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-compute/commit/1462c1f52f4f02cd4ed80c412df30febbc85ba77', 'message': 'Allow vnc_keymap to be configured attribute\n\n* add new attribute vnc_keymap default en-us\n\nChange-Id: I57a622f0370845f84c47a1ac145090726c819519\nCloses-bug: #1346085\n'}]",0,118453,1462c1f52f4f02cd4ed80c412df30febbc85ba77,33,6,4,7128,,,0,"Allow vnc_keymap to be configured attribute

* add new attribute vnc_keymap default en-us

Change-Id: I57a622f0370845f84c47a1ac145090726c819519
Closes-bug: #1346085
",git fetch https://review.opendev.org/openstack/cookbook-openstack-compute refs/changes/53/118453/1 && git format-patch -1 --stdout FETCH_HEAD,"['attributes/default.rb', 'spec/nova-common_spec.rb', 'CHANGELOG.md', 'templates/default/nova.conf.erb']",4,9ef88496678996273ff5402d6f6d5b03f3534084,bug/1346085,vnc_keymap=<%= node['openstack']['compute']['vnc']['keymap'] %> ,,8,1
openstack%2Fcookbook-openstack-ops-database~master~I988fbab91b7ac2251276ffe837e5b918d2f7371a,openstack/cookbook-openstack-ops-database,master,I988fbab91b7ac2251276ffe837e5b918d2f7371a,Fix mysql cookbook version for previous password patch,MERGED,2014-09-24 17:14:55.000000000,2014-09-26 20:57:53.000000000,2014-09-26 20:57:52.000000000,"[{'_account_id': 3}, {'_account_id': 2589}, {'_account_id': 12323}]","[{'number': 1, 'created': '2014-09-24 17:14:55.000000000', 'files': ['CHANGELOG.md', 'metadata.rb'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-ops-database/commit/3f7bcb9421a88f9121e21dc7f425c3c239416b2c', 'message': 'Fix mysql cookbook version for previous password patch\n\nThe previous patch is using a new method only available in\nmysql cookbook version 5.4.  Bump metadata to that.\n\nChange-Id: I988fbab91b7ac2251276ffe837e5b918d2f7371a\nPartial-Bug: #1373523\n'}]",0,123791,3f7bcb9421a88f9121e21dc7f425c3c239416b2c,7,3,1,7128,,,0,"Fix mysql cookbook version for previous password patch

The previous patch is using a new method only available in
mysql cookbook version 5.4.  Bump metadata to that.

Change-Id: I988fbab91b7ac2251276ffe837e5b918d2f7371a
Partial-Bug: #1373523
",git fetch https://review.opendev.org/openstack/cookbook-openstack-ops-database refs/changes/91/123791/1 && git format-patch -1 --stdout FETCH_HEAD,"['CHANGELOG.md', 'metadata.rb']",2,3f7bcb9421a88f9121e21dc7f425c3c239416b2c,bug/1373523,"depends 'mysql', '~> 5.4'","depends 'mysql', '~> 5.3'",3,2
openstack%2Fproject-config~master~Ia7c003e124791a15265c125fd8614d086d154ec3,openstack/project-config,master,Ia7c003e124791a15265c125fd8614d086d154ec3,Disabling check-requirements job from GBP project,MERGED,2014-09-26 04:49:19.000000000,2014-09-26 20:57:47.000000000,2014-09-26 20:57:46.000000000,"[{'_account_id': 1}, {'_account_id': 2}, {'_account_id': 3}, {'_account_id': 490}, {'_account_id': 1849}, {'_account_id': 4146}, {'_account_id': 5263}, {'_account_id': 6482}, {'_account_id': 6547}, {'_account_id': 6786}, {'_account_id': 7069}, {'_account_id': 7987}]","[{'number': 1, 'created': '2014-09-26 04:49:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/d7ce315aa9284d2fdbc7b7ca38fa8a6e8a00ed38', 'message': 'Disabling check-requirements job from GBP project\n\nFixes Bug: 1374263\n\nChange-Id: Ia7c003e124791a15265c125fd8614d086d154ec3\n'}, {'number': 2, 'created': '2014-09-26 06:17:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/61556e93ab1e382e3f5a5c52e48a6f4cf3661f8c', 'message': 'Disabling check-requirements job from GBP project\n\nCloses-Bug: #1374263\n\nChange-Id: Ia7c003e124791a15265c125fd8614d086d154ec3\n'}, {'number': 3, 'created': '2014-09-26 08:09:50.000000000', 'files': ['zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/6a00547260673d2aa8e49bce4cda884011205eb4', 'message': 'Disabling check-requirements job from GBP project\n\nWe will need to use some experimental dependencies which are\nnot part of global requirements. So we need to disable\nthe check-requirements jobs.\n\nCloses-Bug: #1374263\n\nChange-Id: Ia7c003e124791a15265c125fd8614d086d154ec3\n'}]",2,124269,6a00547260673d2aa8e49bce4cda884011205eb4,18,12,3,490,,,0,"Disabling check-requirements job from GBP project

We will need to use some experimental dependencies which are
not part of global requirements. So we need to disable
the check-requirements jobs.

Closes-Bug: #1374263

Change-Id: Ia7c003e124791a15265c125fd8614d086d154ec3
",git fetch https://review.opendev.org/openstack/project-config refs/changes/69/124269/3 && git format-patch -1 --stdout FETCH_HEAD,['zuul/layout.yaml'],1,d7ce315aa9284d2fdbc7b7ca38fa8a6e8a00ed38,gbp,, - name: check-requirements - name: check-requirements - name: check-requirements - name: check-requirements,0,4
openstack%2Fopenstack-planet~master~I1f391e3bf6505a0f7ecdd76065011111b58ef1b0,openstack/openstack-planet,master,I1f391e3bf6505a0f7ecdd76065011111b58ef1b0,Added Cloud Platform at Symantec to Planet OpenStack,MERGED,2014-09-11 18:44:46.000000000,2014-09-26 20:49:53.000000000,2014-09-23 21:18:44.000000000,"[{'_account_id': 3}, {'_account_id': 287}, {'_account_id': 1063}, {'_account_id': 10281}, {'_account_id': 11197}, {'_account_id': 11204}, {'_account_id': 12703}]","[{'number': 1, 'created': '2014-09-11 18:44:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-planet/commit/e65c9658e9e5068768edba4c5c34e76c48c43570', 'message': 'Added Cloud Platform at Symantec to Planet OpenStack\n\nChange-Id: I1f391e3bf6505a0f7ecdd76065011111b58ef1b0\n'}, {'number': 2, 'created': '2014-09-22 18:16:02.000000000', 'files': ['images/cloud-platform-symantec.jpg', 'planet.ini'], 'web_link': 'https://opendev.org/openstack/openstack-planet/commit/56827d4138305c92f883a78178c2cd35f85d217b', 'message': 'Added Cloud Platform at Symantec to Planet OpenStack\n\nChange-Id: I1f391e3bf6505a0f7ecdd76065011111b58ef1b0\n'}]",0,120878,56827d4138305c92f883a78178c2cd35f85d217b,18,7,2,10281,,,0,"Added Cloud Platform at Symantec to Planet OpenStack

Change-Id: I1f391e3bf6505a0f7ecdd76065011111b58ef1b0
",git fetch https://review.opendev.org/openstack/openstack-planet refs/changes/78/120878/2 && git format-patch -1 --stdout FETCH_HEAD,"['images/cloud-platform-symantec.jpg', 'planet.ini']",2,e65c9658e9e5068768edba4c5c34e76c48c43570,, [http://www.symantec.com/connect/cloud-platform-engineering] name = Cloud Platform @ Symantec face = cloud-platform-symantec.png,,4,0
openstack%2Fopenstack-chef-repo~master~I1ab3ec02993fd9ae3f8859815b63dc539a6b1e12,openstack/openstack-chef-repo,master,I1ab3ec02993fd9ae3f8859815b63dc539a6b1e12,Bump mysql and mysql-chef cookbooks to required levels for Juno,MERGED,2014-09-24 17:18:53.000000000,2014-09-26 20:46:16.000000000,2014-09-26 20:46:15.000000000,"[{'_account_id': 3}, {'_account_id': 2589}, {'_account_id': 7128}, {'_account_id': 8410}, {'_account_id': 12323}]","[{'number': 1, 'created': '2014-09-24 17:18:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-chef-repo/commit/30f707ba49c483c1ee79b17e511061247a2da7e3', 'message': 'Bump mysql and mysql-chef to required levels for Juno\n\n* Changes in ops-database for mysql\n* Bugs in mysql-gem\n\nChange-Id: I1ab3ec02993fd9ae3f8859815b63dc539a6b1e12\nPartial-Bug: #1373523\n'}, {'number': 2, 'created': '2014-09-24 18:38:05.000000000', 'files': ['CHANGELOG.md', 'Berksfile'], 'web_link': 'https://opendev.org/openstack/openstack-chef-repo/commit/a83f6bf1c94dbecb9337704598849875aedb138f', 'message': 'Bump mysql and mysql-chef cookbooks to required levels for Juno\n\n* Changes in ops-database for mysql\n* Bugs in mysql-gem\n\nChange-Id: I1ab3ec02993fd9ae3f8859815b63dc539a6b1e12\nPartial-Bug: #1373523\n'}]",0,123793,a83f6bf1c94dbecb9337704598849875aedb138f,12,5,2,7128,,,0,"Bump mysql and mysql-chef cookbooks to required levels for Juno

* Changes in ops-database for mysql
* Bugs in mysql-gem

Change-Id: I1ab3ec02993fd9ae3f8859815b63dc539a6b1e12
Partial-Bug: #1373523
",git fetch https://review.opendev.org/openstack/openstack-chef-repo refs/changes/93/123793/2 && git format-patch -1 --stdout FETCH_HEAD,"['CHANGELOG.md', 'Berksfile']",2,30f707ba49c483c1ee79b17e511061247a2da7e3,bug/1373523,"cookbook 'mysql', '5.4.4' cookbook 'mysql-chef_gem', '0.0.4'","cookbook 'mysql', '5.3.6' cookbook 'mysql-chef_gem', '0.0.2'",3,2
openstack%2Foctavia~master~I2f6fbac790a195457d6b15d8994ede8d5c7d7326,openstack/octavia,master,I2f6fbac790a195457d6b15d8994ede8d5c7d7326,Spec for the amphora driver interface,ABANDONED,2014-09-26 19:44:57.000000000,2014-09-26 20:42:54.000000000,,[{'_account_id': 6951}],"[{'number': 1, 'created': '2014-09-26 19:44:57.000000000', 'files': ['specs/version0.5/amphora-driver-interface.rst'], 'web_link': 'https://opendev.org/openstack/octavia/commit/37affc7c534c88bff61868efa0e8a6d2dda17dcc', 'message': 'Spec for the amphora driver interface\n\nThis is using listeners this time.\n\nChange-Id: I2f6fbac790a195457d6b15d8994ede8d5c7d7326\n'}]",0,124501,37affc7c534c88bff61868efa0e8a6d2dda17dcc,3,1,1,10850,,,0,"Spec for the amphora driver interface

This is using listeners this time.

Change-Id: I2f6fbac790a195457d6b15d8994ede8d5c7d7326
",git fetch https://review.opendev.org/openstack/octavia refs/changes/01/124501/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/version0.5/amphora-driver-interface.rst'],1,37affc7c534c88bff61868efa0e8a6d2dda17dcc,,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Amphora Driver Interface ========================================== https://blueprints.launchpad.net/octavia/+spec/amphora-driver-interface This blueprint describes how a driver will interface with the controller. It will describe the base class and other classes required. It will not describe the REST interface needed to talk to an amphora nor how health information or statistsics are gathered from the amphora. Problem description =================== The controller needs to talk through a driver to the amphora to allow for custom APIs and custom rendering of configuration data for different amphora implementations. Proposed change =============== Establish a base class to model the desire functionality: .. code:: python class AmphoraLoadBalancerDriver(object): def getLogger(self): #return the logger to use - this is a way to inject a custom logger for testing, etc def update(self, listener, vip): #update the amphora with a new configuration for the ;istener on vip raise NotImplementedError def suspend(self, listener, vip): #stop the listener - OPTIONAL raise NotImplementedError def enable(self, listener, vip): #start/enable the listener raise NotImplementedError def delete(self, listener, vip): #delete the listener from the amphora raise NotImplementedError def info(self, amphora): #returns information about the amphora, e.g. {""Rest Interface"": ""1.0"", ""Amphorae"": ""1.0"", # ""packages"":{""ha proxy"":""1.5""}} #some information might come from querying the amphora raise NotImplementedError def get_metrics(self, amphora): #return ceilometer ready metrics - some amphora might choose to send them #straight to ceilometer others might use the mixin #support metrics to be compatible with Neutron LBaaS raise NotImplementedError def get_health(self, amphora): #returns map: {""amphora-status"":HEALTHY, loadbalancers: {""loadbalancer-id"": {""loadbalancer-status"": HEALTHY, # ""listeners"":{""listener-id"":{""listener-status"":HEALTHY, ""nodes"":{""node-id"":HEALTHY, ...}}, ...}, ...}} raise NotImplementedError def get_diagnostics(self, amphora): #run some expensive self tests to determine if the amphora and the lbs are healthy #the idea is that those tests are triggered more infrequent than the health #gathering raise NotImplementedError The referenced listener is a listener object and vip a vip as described in our model. The model is detached from the DB so the driver can't write to the DB. Because our initial goal is to render a whole config no special methods for adding nodes, health monitors, etc. are supported at this juncture. This might be added in later versions. No method for obtaining logs has been added. This will be done in a future blueprint. Exception Model --------------- The driver is expected to raise the following well defined exceptions * NotImplementedError - this functionality is not implemented/not supported * AmphoraDriverError - a super class for all other exceptions and the catch all if no specific exception can be found * NotFoundError - this amphora couldn't be found/ was deleted by nova * InfoException - gathering information about this amphora failed * MetricsException - gathering metrics failed * UnauthorizedException - the driver can't access the amphora * StatisticsException - gathering statistics failed * TimeOutException - contacting the amphora timed out * UnavailableException - the amphora is temporary unavailable * DeleteFailed - this load balancer couldn't be deleted * SuspendFaied - this load balancer couldn't be suspended * EnableFailed - this load balancer couldn't be enabled * ArchiveException - couldn't archive the logs * TargetException - the target is not accessible * QuotaException - the target has no space left * UnauthorizedException - unauthorized to write to the target * ProvisioningErrors - those are errors which happen during provisioning * ListenerProvisioningError - could not provision Listener * LoadBalancerProvisoningError - could not provision LoadBalancer * HealthMonitorProvisioningError - could not provision HealthMonitor * NodeProvisioningError - could not provision Node Health and Stat Mixin --------------------- It has been suggested to gather health and statistic information via UDP packets emitted from the amphora. This requires each driver to spin up a thread to listen on a UDP port and then hand the information to the controller as a mixin to make sense of it. Here is the mixin definition: .. code:: python class HealthMixIn(object): def update_health(health): #map: {""amphora-status"":HEALTHY, loadbalancers: {""loadbalancer-id"": {""loadbalancer-status"": HEALTHY, # ""listeners"":{""listener-id"":{""listener-status"":HEALTHY, ""nodes"":{""node-id"":HEALTHY, ...}}, ...}, ...}} # only items whose health has changed need to be submitted # awesome update code pass class StatsMixIn(object): def update_stats(stats): #uses map {""loadbalancer-id"":{""listener-id"": {""bytes-in"": 123, ""bytes_out"":123, ""active_connections"":123, # ""total_connections"", 123}, ...} # elements are named to keep it extsnsible for future versions #awesome update code and code to send to ceilometer pass Things a good driver should do: ------------------------------- * Non blocking IO - throw an appropriate exception instead to wait forever; use timeouts on sockets * We might employ a circuit breaker to insulate driver problems from controller problems [1] * Use appropriate logging * Use the preferred threading model This will be demonstrated in the Noop-driver code. Alternatives ------------ Require all amphora to implement a common REST interface and use that as the integration point. Data model impact ----------------- None REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None - since initial version Other end user impact --------------------- None Performance Impact ------------------ Minimal Other deployer impact --------------------- Deployers need to make sure to bundle the compatible versions of amphora, driver, controller -- Developer impact ---------------- Need to write towards this clean interface. Implementation ============== Assignee(s) ----------- German Eichberger Work Items ---------- * Write abstract interface * Write Noop driver * Write tests Dependencies ============ None Testing ======= * Unit tests with tox and Noop-Driver * tempest tests with Noop-Driver Documentation Impact ==================== None - we won't document the interface for 0.5. If that changes we need to write an interface documentation so 3rd party drivers know what we expect. References ========== [1] http://martinfowler.com/bliki/CircuitBreaker.html ",,241,0
openstack%2Fcookbook-openstack-common~master~I3f81a80457df7a421cf60ba192fb8d7062f641a6,openstack/cookbook-openstack-common,master,I3f81a80457df7a421cf60ba192fb8d7062f641a6,address_for does not check for nil interface addresses,MERGED,2014-09-11 17:00:53.000000000,2014-09-26 20:40:20.000000000,2014-09-26 20:40:20.000000000,"[{'_account_id': 3}, {'_account_id': 1032}, {'_account_id': 2589}, {'_account_id': 7128}, {'_account_id': 8112}, {'_account_id': 8410}, {'_account_id': 12323}]","[{'number': 1, 'created': '2014-09-11 17:00:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-common/commit/6ee205809bf12719f801a559a18067a265e076cf', 'message': 'address_for does not check for nil interface addresses\n\nAdd checks to address_for to check for no addresses for interface\nor no address for interface family.\n\nChange-Id: I3f81a80457df7a421cf60ba192fb8d7062f641a6\nCloses-Bug: #1368292\n'}, {'number': 2, 'created': '2014-09-12 18:52:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-common/commit/6b45ab6f4ec27231d5f661de823c3daf920c1435', 'message': 'address_for does not check for nil interface addresses\n\nAdd checks to address_for to check for no addresses for interface\nor no address for interface family.\n\nChange-Id: I3f81a80457df7a421cf60ba192fb8d7062f641a6\nCloses-Bug: #1368292\n'}, {'number': 3, 'created': '2014-09-12 18:53:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-common/commit/77e8b11ac13ffdf2b96df744e00719c1f7270fba', 'message': 'address_for does not check for nil interface addresses\n\nAdd checks to address_for to check for no addresses for interface\nor no address for interface family.\n\nChange-Id: I3f81a80457df7a421cf60ba192fb8d7062f641a6\nCloses-Bug: #1368292\n'}, {'number': 4, 'created': '2014-09-23 16:53:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-common/commit/35e1fc6e5812ce90bc48a41df18f92bbc1d98b3b', 'message': 'address_for does not check for nil interface addresses\n\nAdd checks to address_for to check for no addresses for interface\nor no address for interface family.\n\nChange-Id: I3f81a80457df7a421cf60ba192fb8d7062f641a6\nCloses-Bug: #1368292\n'}, {'number': 5, 'created': '2014-09-25 14:45:36.000000000', 'files': ['libraries/endpoints.rb', 'libraries/network.rb', 'CHANGELOG.md', 'spec/network_spec.rb'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-common/commit/ff8bff52c3f2a08859a0a2d9659e88ca1a10ab54', 'message': 'address_for does not check for nil interface addresses\n\nAdd checks to address_for to check for no addresses for interface\nor no address for interface family.\n\nChange-Id: I3f81a80457df7a421cf60ba192fb8d7062f641a6\nCloses-Bug: #1368292\n'}]",0,120841,ff8bff52c3f2a08859a0a2d9659e88ca1a10ab54,28,7,5,7128,,,0,"address_for does not check for nil interface addresses

Add checks to address_for to check for no addresses for interface
or no address for interface family.

Change-Id: I3f81a80457df7a421cf60ba192fb8d7062f641a6
Closes-Bug: #1368292
",git fetch https://review.opendev.org/openstack/cookbook-openstack-common refs/changes/41/120841/5 && git format-patch -1 --stdout FETCH_HEAD,"['libraries/endpoints.rb', 'libraries/network.rb', 'CHANGELOG.md', 'spec/network_spec.rb']",4,6ee205809bf12719f801a559a18067a265e076cf,bug/1368292," describe '#address_for failures' do it 'fails when no addresses are avaiable for interface' do node.set['network'] = { 'interfaces' => { 'lo' => { 'addresses' => {} } } } allow(subject).to receive(:address_for).with('lo') .and_raise('Interface lo has no addresses assigned') expect { subject.address_for('lo') } .to raise_error end it 'fails when no address is available for interface family' do node.set['network'] = { 'interfaces' => { 'lo' => { 'addresses' => { '::1' => { 'family' => 'inet6', } } } } } allow(subject).to receive(:address_for).with('lo') .and_raise('Interface lo for family inet has no address assigned') expect { subject.address_for('lo') } .to raise_error end end",,42,5
openstack%2Fcookbook-openstack-image~master~Ifaeb99dc17784dff5467471fa29ca4bd6cb9997f,openstack/cookbook-openstack-image,master,Ifaeb99dc17784dff5467471fa29ca4bd6cb9997f,Add attribute for glance registry workers,MERGED,2014-09-12 16:57:18.000000000,2014-09-26 20:39:55.000000000,2014-09-26 20:39:54.000000000,"[{'_account_id': 3}, {'_account_id': 1032}, {'_account_id': 2589}, {'_account_id': 7128}, {'_account_id': 9488}, {'_account_id': 11915}, {'_account_id': 12323}]","[{'number': 1, 'created': '2014-09-12 16:57:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-image/commit/d7c2615ab66075d8283cfd853e32de641e24d03f', 'message': 'Add attribute for glance registry workers\n\nChange-Id: Ifaeb99dc17784dff5467471fa29ca4bd6cb9997f\nCloses-Bug: #1368859\n'}, {'number': 2, 'created': '2014-09-22 19:11:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-image/commit/757546cc37cc839e5e532ca1c9d3bccc42ba638a', 'message': 'Add attribute for glance registry workers\n\nChange-Id: Ifaeb99dc17784dff5467471fa29ca4bd6cb9997f\nCloses-Bug: #1368859\n'}, {'number': 3, 'created': '2014-09-23 17:51:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-image/commit/904e80257c25ec3ed54a51ae7fdf2f893d421870', 'message': 'Add attribute for glance registry workers\n\nChange-Id: Ifaeb99dc17784dff5467471fa29ca4bd6cb9997f\nCloses-Bug: #1368859\n'}, {'number': 4, 'created': '2014-09-25 22:22:06.000000000', 'files': ['templates/default/glance-registry.conf.erb', 'attributes/default.rb', 'spec/registry_spec.rb', 'CHANGELOG.md'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-image/commit/b0fcafe606e50883de5fafebaabd6e4052ae7891', 'message': 'Add attribute for glance registry workers\n\nChange-Id: Ifaeb99dc17784dff5467471fa29ca4bd6cb9997f\nCloses-Bug: #1368859\n'}]",0,121173,b0fcafe606e50883de5fafebaabd6e4052ae7891,25,7,4,7128,,,0,"Add attribute for glance registry workers

Change-Id: Ifaeb99dc17784dff5467471fa29ca4bd6cb9997f
Closes-Bug: #1368859
",git fetch https://review.opendev.org/openstack/cookbook-openstack-image refs/changes/73/121173/1 && git format-patch -1 --stdout FETCH_HEAD,"['templates/default/glance-registry.conf.erb', 'attributes/default.rb', 'spec/registry_spec.rb', 'CHANGELOG.md']",4,d7c2615ab66075d8283cfd853e32de641e24d03f,bug/1368859,* Add attribute for glance-registry workers,,12,2
openstack%2Fnova~master~I6803bdf5de59ab3f3c495de2876ea11fffcff6ee,openstack/nova,master,I6803bdf5de59ab3f3c495de2876ea11fffcff6ee,Don't have more workers than cpus by default,ABANDONED,2014-09-08 20:00:27.000000000,2014-09-26 20:17:57.000000000,,"[{'_account_id': 3}, {'_account_id': 1561}, {'_account_id': 1849}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 5196}, {'_account_id': 6873}, {'_account_id': 9008}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-08 20:00:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e97f26133b1fcb6557eba1b2d662d41329ec04ef', 'message': ""Don't have more workers than cpus by default\n\nThis commit changes the default behavior for starting WSGIServices\nto only use the number of cpus on the system. Previously, by default\nit would launch N workers for each enabled api where N is the number\nof CPUs on the system. If there are 3 enabled APIs this would result\nin workers for 3x the number of CPUs being started. This commit\nchanges the default behavior to launch only N / A where N is the\nnumber of CPUs and A is the number of enabled APIs.\n\nPartial-Bug: #1366931\nPartial-Bug: #1362347\nChange-Id: I6803bdf5de59ab3f3c495de2876ea11fffcff6ee\n""}, {'number': 2, 'created': '2014-09-08 20:38:05.000000000', 'files': ['nova/service.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/51d9bd701bb6f826b1f6900bd891d5a68659caa1', 'message': ""Don't have more workers than cpus by default\n\nThis commit changes the default behavior for starting WSGIServices\nto only use the number of cpus on the system. Previously, by default\nit would launch N workers for each enabled api where N is the number\nof CPUs on the system. If there are 3 enabled APIs this would result\nin workers for 3x the number of CPUs being started. This commit\nchanges the default behavior to launch only N / A where N is the\nnumber of CPUs and A is the number of enabled APIs.\n\nThis issue was noticed because occasionally during gate runs we were\nseeing oom-killer being invoked. In those instances nova-api is among\nthe top consumers of memory and with 24 workers we were pushing the\nlimits of our gate slaves with 8 CPUs and 8GB of RAM.\n\nDocImpact\nPartial-Bug: #1366931\nPartial-Bug: #1362347\nChange-Id: I6803bdf5de59ab3f3c495de2876ea11fffcff6ee\n""}]",8,119894,51d9bd701bb6f826b1f6900bd891d5a68659caa1,20,9,2,5196,,,0,"Don't have more workers than cpus by default

This commit changes the default behavior for starting WSGIServices
to only use the number of cpus on the system. Previously, by default
it would launch N workers for each enabled api where N is the number
of CPUs on the system. If there are 3 enabled APIs this would result
in workers for 3x the number of CPUs being started. This commit
changes the default behavior to launch only N / A where N is the
number of CPUs and A is the number of enabled APIs.

This issue was noticed because occasionally during gate runs we were
seeing oom-killer being invoked. In those instances nova-api is among
the top consumers of memory and with 24 workers we were pushing the
limits of our gate slaves with 8 CPUs and 8GB of RAM.

DocImpact
Partial-Bug: #1366931
Partial-Bug: #1362347
Change-Id: I6803bdf5de59ab3f3c495de2876ea11fffcff6ee
",git fetch https://review.opendev.org/openstack/nova refs/changes/94/119894/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/service.py'],1,e97f26133b1fcb6557eba1b2d662d41329ec04ef,bug/1366931, worker_count = processutils.get_worker_count() / len(CONF.enabled_apis) worker_count), processutils.get_worker_count()),2,1
openstack%2Fsecurity-doc~master~I110fd49a010c100c81907b5ebf6ba8b074b0e77e,openstack/security-doc,master,I110fd49a010c100c81907b5ebf6ba8b074b0e77e,Adding OSSN-0030: Bash 'shellshock' bug.,MERGED,2014-09-26 10:53:36.000000000,2014-09-26 20:04:26.000000000,2014-09-26 20:04:26.000000000,"[{'_account_id': 3}, {'_account_id': 2807}, {'_account_id': 6486}, {'_account_id': 6547}, {'_account_id': 7063}, {'_account_id': 9098}, {'_account_id': 11029}, {'_account_id': 11716}]","[{'number': 1, 'created': '2014-09-26 10:53:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/security-doc/commit/39c0848d2580a848b2ef17c35cc6e7c0b0656d13', 'message': ""Adding OSSN-0030: Bash 'shellshock' bug.\n\nCloses-bug: #1374055\nChange-Id: I110fd49a010c100c81907b5ebf6ba8b074b0e77e\n""}, {'number': 2, 'created': '2014-09-26 10:54:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/security-doc/commit/215099f83e3e65b3576757d28b8b9deefcf49d80', 'message': ""Adding OSSN-0030: Bash 'shellshock' bug.\n\nCloses-bug: #1374055\nChange-Id: I110fd49a010c100c81907b5ebf6ba8b074b0e77e\n""}, {'number': 3, 'created': '2014-09-26 11:41:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/security-doc/commit/78751c3bdb95a9d5f55d0c247af4fd8f341abbdb', 'message': ""Adding OSSN-0030: Bash 'shellshock' bug.\n\nCloses-bug: #1374055\nChange-Id: I110fd49a010c100c81907b5ebf6ba8b074b0e77e\n""}, {'number': 4, 'created': '2014-09-26 14:14:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/security-doc/commit/b453af89796a525175820bfb2477258929bd45dd', 'message': ""Adding OSSN-0030: Bash 'shellshock' bug.\n\nCloses-bug: #1374055\nChange-Id: I110fd49a010c100c81907b5ebf6ba8b074b0e77e\n""}, {'number': 5, 'created': '2014-09-26 14:36:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/security-doc/commit/8aca9e95c9e3e22795c7a56f85d91c42287b34a6', 'message': ""Adding OSSN-0030: Bash 'shellshock' bug.\n\nCloses-bug: #1374055\nChange-Id: I110fd49a010c100c81907b5ebf6ba8b074b0e77e\n""}, {'number': 6, 'created': '2014-09-26 15:03:26.000000000', 'files': ['security-notes/OSSN-0030'], 'web_link': 'https://opendev.org/openstack/security-doc/commit/57a9604b7d80abc03a9d121e58bca58115be68bc', 'message': ""Adding OSSN-0030: Bash 'shellshock' bug.\n\nCloses-bug: #1374055\nChange-Id: I110fd49a010c100c81907b5ebf6ba8b074b0e77e\n""}]",12,124357,57a9604b7d80abc03a9d121e58bca58115be68bc,28,8,6,11716,,,0,"Adding OSSN-0030: Bash 'shellshock' bug.

Closes-bug: #1374055
Change-Id: I110fd49a010c100c81907b5ebf6ba8b074b0e77e
",git fetch https://review.opendev.org/openstack/security-doc refs/changes/57/124357/1 && git format-patch -1 --stdout FETCH_HEAD,['security-notes/OSSN-0030'],1,39c0848d2580a848b2ef17c35cc6e7c0b0656d13,bug/1374055,"Bash 'shellshock' bug can lead to code injection vulnerability. --- ### Summary ### A bug in the GNU Bash shell (4.3 and lower) exposes a code injection vulnerability via crafted environment variables (Shellshock, CVE-2014-6271). Through network utilities such as SSH and CGI enabled web servers, this vulnerability can become remotely exploitable. Bash is universal to nearly all Linux distributions as well as Apple OS X. ### Affected Services / Software ### GNU Bash, Grizzly, Havana, Icehouse ### Discussion ### The GNU Bash shell (4.3 and lower) is vulnerable to a code injection attack via the setting of environment variables. This stems from a bug in the way bash processes functions definitions present in the environment, an example might look like the following: env x='() { :;}; echo vulnerable' bash -c 'echo hello' when executed, this command line will print: vulnerable hello This behaviour occurs because bash continues to process the rest of the variable string after the function definition, the name of the variable is also unimportant. Many programs on a Linux installation will 'shell out' to launch helper commands. If a malicious user can set an environment variable in the spawned shell they can execute arbitrary commands with the same user permissions as the legitimate command. If these programs are network connected then this vulnerability becomes remotely exploitable. To illustrait how this might be accomplished, consider the OpenSSH forced command mechanism. This mechanism allows commands run via SSH to be restricted to a specific invocation, however OpenSSH will set an environment variable 'SSH_ORIGINAL_COMMAND' to the command that was requested by the user before executing the forced command. If 'SSH_ORIGINAL_COMMAND' contains a function definition of the form given above, then this will be executed by bash regardless of the forced command specified. Note that there are many remotely accessible programs that may set one or more environment variables before spawning a bash sub-processes, know examples include but are not limited to: - CGI Enabled web servers (Apache mod_cgi, nginex, etc) - SSH (OpenSSH mechanisms as above) - DHCP (dhcpcd) OpenStack software itself is not currently understood to be directly affected, however deployments of OpenStack will very likely be using GNU Bash in many places. While employed mechanisms such as rootwrap filter environment variables, any variable that can be set via user provided import becomes a potential security issue. ### Recommended Actions ### Owing to the ubiquitous nature of the bash shell and its indirect use via other programs it is highly recommended that all systems update to a version of bash greater than 4.3 immediately. Additionally, network filtering and IDS systems should be configured to detect incoming requests containing bash function-like definitions. System logs should also be interrogated for any such strings as an indication of possible attacks. ### Contacts / References ### This OSSN : https://wiki.openstack.org/wiki/OSSN/OSSN-0030 Original LaunchPad Bug : https://bugs.launchpad.net/ossn/+bug/1374055 OpenStack Security ML : openstack-security@lists.openstack.org OpenStack Security Group : https://launchpad.net/~openstack-ossg CVE: CVE-2014-6271 ",,74,0
openstack%2Ftrove-integration~master~I687486b04939f42f4d9a89da3716e7168f278e8a,openstack/trove-integration,master,I687486b04939f42f4d9a89da3716e7168f278e8a,Add GLANCE_SERVICE_PROTOCOL to fix image upload,MERGED,2014-09-26 18:13:33.000000000,2014-09-26 19:38:38.000000000,2014-09-26 19:37:17.000000000,"[{'_account_id': 3}, {'_account_id': 5293}, {'_account_id': 7092}]","[{'number': 1, 'created': '2014-09-26 18:13:33.000000000', 'files': ['scripts/redstack'], 'web_link': 'https://opendev.org/openstack/trove-integration/commit/5680a88c67b16d30168d60df0b86c9515d29902f', 'message': 'Add GLANCE_SERVICE_PROTOCOL to fix image upload\n\nDevStack change I36fe56c063ca921131ad98439bd452cb135916ac introduced\nsupport for https. This requires us to explicitly set the glance\nservice protocol in redstack for image uploads to work correctly.\nSet GLANCE_SERVICE_PROTOCOL to be http (default), which can be\noverridden by setting the environment variable appropriately.\n\nChange-Id: I687486b04939f42f4d9a89da3716e7168f278e8a\nCloses-bug: 1374555\n'}]",0,124482,5680a88c67b16d30168d60df0b86c9515d29902f,10,3,1,5293,,,0,"Add GLANCE_SERVICE_PROTOCOL to fix image upload

DevStack change I36fe56c063ca921131ad98439bd452cb135916ac introduced
support for https. This requires us to explicitly set the glance
service protocol in redstack for image uploads to work correctly.
Set GLANCE_SERVICE_PROTOCOL to be http (default), which can be
overridden by setting the environment variable appropriately.

Change-Id: I687486b04939f42f4d9a89da3716e7168f278e8a
Closes-bug: 1374555
",git fetch https://review.opendev.org/openstack/trove-integration refs/changes/82/124482/1 && git format-patch -1 --stdout FETCH_HEAD,['scripts/redstack'],1,5680a88c67b16d30168d60df0b86c9515d29902f,bug/1374555,GLANCE_SERVICE_PROTOCOL=${GLANCE_SERVICE_PROTOCOL:-http},,1,0
openstack%2Fcookbook-openstack-identity~stable%2Ficehouse~I0d4f5e040027ff7b96e39a47473a16e32c593ccf,openstack/cookbook-openstack-identity,stable/icehouse,I0d4f5e040027ff7b96e39a47473a16e32c593ccf,Updated for UTF8 issue,ABANDONED,2014-09-25 20:10:00.000000000,2014-09-26 19:34:24.000000000,,[{'_account_id': 3}],"[{'number': 1, 'created': '2014-09-25 20:10:00.000000000', 'files': ['metadata.rb'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-identity/commit/4cab6ee4d94bf3ed56f61058c6e7fbd7e1cd99f0', 'message': 'Updated for UTF8 issue\n\nChange-Id: I0d4f5e040027ff7b96e39a47473a16e32c593ccf\n'}]",0,124169,4cab6ee4d94bf3ed56f61058c6e7fbd7e1cd99f0,3,1,1,12323,,,0,"Updated for UTF8 issue

Change-Id: I0d4f5e040027ff7b96e39a47473a16e32c593ccf
",git fetch https://review.opendev.org/openstack/cookbook-openstack-identity refs/changes/69/124169/1 && git format-patch -1 --stdout FETCH_HEAD,['metadata.rb'],1,4cab6ee4d94bf3ed56f61058c6e7fbd7e1cd99f0,common_upgrade_berksfile,version '9.3.3',version '9.3.2',1,1
openstack%2Fhorizon~stable%2Ficehouse~Id8459947498127e47700d9f690d4ed4d5cadbba9,openstack/horizon,stable/icehouse,Id8459947498127e47700d9f690d4ed4d5cadbba9,Fix endpoint error when running keystone on apache,MERGED,2014-09-04 07:56:29.000000000,2014-09-26 19:29:05.000000000,2014-09-26 19:29:04.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 1970}, {'_account_id': 2455}, {'_account_id': 4428}, {'_account_id': 6610}, {'_account_id': 6638}, {'_account_id': 6914}, {'_account_id': 8871}, {'_account_id': 9531}, {'_account_id': 9656}, {'_account_id': 12119}, {'_account_id': 12929}]","[{'number': 1, 'created': '2014-09-04 07:56:29.000000000', 'files': ['openstack_dashboard/api/keystone.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/e32a00dc32b5feaac210439e2e5ecb2b84d9a22d', 'message': 'Fix endpoint error when running keystone on apache\n\nWhen running keystone in httpd, horizon could not generate the\nright keystone endpoint url. Fixes this issue by retrieving the\nwhole path from the service_catalog or OPENSTACK_KEYSTONE_URL\nand generating a new url.\n\nChange-Id: Id8459947498127e47700d9f690d4ed4d5cadbba9\nCloses-bug: #1295128\n(cherry picked from commit 48a0d07e45bac04393978776c63b67cd7c71415d)\n'}]",0,118992,e32a00dc32b5feaac210439e2e5ecb2b84d9a22d,20,13,1,12929,,,0,"Fix endpoint error when running keystone on apache

When running keystone in httpd, horizon could not generate the
right keystone endpoint url. Fixes this issue by retrieving the
whole path from the service_catalog or OPENSTACK_KEYSTONE_URL
and generating a new url.

Change-Id: Id8459947498127e47700d9f690d4ed4d5cadbba9
Closes-bug: #1295128
(cherry picked from commit 48a0d07e45bac04393978776c63b67cd7c71415d)
",git fetch https://review.opendev.org/openstack/horizon refs/changes/92/118992/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack_dashboard/api/keystone.py'],1,e32a00dc32b5feaac210439e2e5ecb2b84d9a22d,bug/1295128," url = url.rstrip('/') url = urlparse.urljoin(url, 'v%s' % VERSIONS.active)"," bits = urlparse.urlparse(url) root = ""://"".join((bits.scheme, bits.netloc)) url = ""%s/v%s"" % (root, VERSIONS.active)",2,3
openstack%2Fdiskimage-builder~master~Iad2907c8be491c88727d87ed5e5a720e5beb66c3,openstack/diskimage-builder,master,Iad2907c8be491c88727d87ed5e5a720e5beb66c3,Refactor ramdisk element to allow alternate implementations,MERGED,2014-09-26 00:34:52.000000000,2014-09-26 19:23:54.000000000,2014-09-26 19:23:53.000000000,"[{'_account_id': 3}, {'_account_id': 215}, {'_account_id': 6928}, {'_account_id': 7144}, {'_account_id': 12459}]","[{'number': 1, 'created': '2014-09-26 00:34:52.000000000', 'files': ['elements/ramdisk/element-deps', 'elements/ramdisk-base/init.d/40-check-network-ready', 'elements/ramdisk-base/extra-data.d/scripts/init', 'elements/ramdisk/init.d/10-start-base-system', 'elements/ramdisk-base/README.md', 'elements/ramdisk/extra-data.d/scripts/init', 'elements/ramdisk-base/extra-data.d/scripts/d/init-func', 'elements/ramdisk-base/extra-data.d/scripts/init-end', 'elements/ramdisk/install.d/52-ramdisk-install-busybox', 'elements/ramdisk-base/cleanup.d/99-extract-ramdisk-files', 'elements/ramdisk/init.d/30-start-network', 'elements/ramdisk-base/init.d/20-init-variables', 'elements/ramdisk-base/extra-data.d/01-inject-ramdisk-build-files'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/5aca301693462f2682e27407e301681871f329b1', 'message': 'Refactor ramdisk element to allow alternate implementations\n\nIn preparation for enabling Dracut-based ramdisks, this change\nfactors out functionality that is common to both busybox and Dracut\nramdisks.  Said functionality is moved to a ramdisk-base element\nwhich is added as a dependency of the ramdisk element.  ramdisk now\nonly contains the functionality specific to building busybox-based\nramdisks.\n\nbp tripleo-juno-dracut-ramdisks\nChange-Id: Iad2907c8be491c88727d87ed5e5a720e5beb66c3\n'}]",0,124235,5aca301693462f2682e27407e301681871f329b1,11,5,1,6928,,,0,"Refactor ramdisk element to allow alternate implementations

In preparation for enabling Dracut-based ramdisks, this change
factors out functionality that is common to both busybox and Dracut
ramdisks.  Said functionality is moved to a ramdisk-base element
which is added as a dependency of the ramdisk element.  ramdisk now
only contains the functionality specific to building busybox-based
ramdisks.

bp tripleo-juno-dracut-ramdisks
Change-Id: Iad2907c8be491c88727d87ed5e5a720e5beb66c3
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/35/124235/1 && git format-patch -1 --stdout FETCH_HEAD,"['elements/ramdisk/element-deps', 'elements/ramdisk-base/init.d/40-check-network-ready', 'elements/ramdisk-base/extra-data.d/scripts/init', 'elements/ramdisk/init.d/10-start-base-system', 'elements/ramdisk-base/README.md', 'elements/ramdisk/extra-data.d/scripts/init', 'elements/ramdisk-base/extra-data.d/scripts/d/init-func', 'elements/ramdisk-base/extra-data.d/scripts/init-end', 'elements/ramdisk/install.d/52-ramdisk-install-busybox', 'elements/ramdisk-base/cleanup.d/99-extract-ramdisk-files', 'elements/ramdisk/init.d/30-start-network', 'elements/ramdisk-base/init.d/20-init-variables', 'elements/ramdisk-base/extra-data.d/01-inject-ramdisk-build-files']",13,5aca301693462f2682e27407e301681871f329b1,bp/tripleo-juno-dracut-ramdisks,,,134,132
openstack%2Fcookbook-openstack-block-storage~stable%2Ficehouse~Ifa4f1cb1415edf24fb739d15b305305c7ddc695e,openstack/cookbook-openstack-block-storage,stable/icehouse,Ifa4f1cb1415edf24fb739d15b305305c7ddc695e,Updated for UTF8 issue,ABANDONED,2014-09-25 18:55:14.000000000,2014-09-26 19:22:15.000000000,,[{'_account_id': 3}],"[{'number': 1, 'created': '2014-09-25 18:55:14.000000000', 'files': ['Berksfile.lock', 'metadata.rb'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-block-storage/commit/9113755827ce5d263a029023d9862a80a7f5a60d', 'message': 'Updated for UTF8 issue\n\nChange-Id: Ifa4f1cb1415edf24fb739d15b305305c7ddc695e\n'}]",0,124131,9113755827ce5d263a029023d9862a80a7f5a60d,3,1,1,12323,,,0,"Updated for UTF8 issue

Change-Id: Ifa4f1cb1415edf24fb739d15b305305c7ddc695e
",git fetch https://review.opendev.org/openstack/cookbook-openstack-block-storage refs/changes/31/124131/1 && git format-patch -1 --stdout FETCH_HEAD,"['Berksfile.lock', 'metadata.rb']",2,9113755827ce5d263a029023d9862a80a7f5a60d,,version '9.4.2',version '9.4.1',2,2
openstack%2Ffuel-web~master~Ie25221a6125654bac1810aca4c1745a6c1716bab,openstack/fuel-web,master,Ie25221a6125654bac1810aca4c1745a6c1716bab,Serialization of neutron attrs depends on environment version,MERGED,2014-08-28 16:39:53.000000000,2014-09-26 18:48:44.000000000,2014-08-29 14:58:49.000000000,"[{'_account_id': 3}, {'_account_id': 406}, {'_account_id': 7468}, {'_account_id': 8392}, {'_account_id': 8749}, {'_account_id': 8907}, {'_account_id': 8935}, {'_account_id': 8971}, {'_account_id': 10391}, {'_account_id': 10959}, {'_account_id': 12200}]","[{'number': 1, 'created': '2014-08-28 16:39:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/0a38e187e54647ed840a705365fe00f08f6e972b', 'message': ""Serialization of neutron attrs depends on environment version\n\nIt is a quick 'dirty' fix for the issue.\nVersioning of serializer must be introduced to avoid such bugs in future.\n\nCloses-Bug: #1362659\n\nChange-Id: Ie25221a6125654bac1810aca4c1745a6c1716bab\n""}, {'number': 2, 'created': '2014-08-29 07:57:43.000000000', 'files': ['nailgun/nailgun/network/neutron.py', 'nailgun/nailgun/test/integration/test_cluster_changes_handler.py', 'nailgun/nailgun/test/base.py', 'nailgun/nailgun/test/integration/test_orchestrator_serializer.py'], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/0f642fca4372bfb1dad50d3c7a4ab89330ebdf23', 'message': ""Serialization of neutron attrs depends on environment version\n\nIt is a quick 'dirty' fix for the issue.\nVersioning of serializer must be introduced to avoid such bugs in future.\n\nCloses-Bug: #1362659\n\nChange-Id: Ie25221a6125654bac1810aca4c1745a6c1716bab\n""}]",1,117557,0f642fca4372bfb1dad50d3c7a4ab89330ebdf23,25,11,2,8392,,,0,"Serialization of neutron attrs depends on environment version

It is a quick 'dirty' fix for the issue.
Versioning of serializer must be introduced to avoid such bugs in future.

Closes-Bug: #1362659

Change-Id: Ie25221a6125654bac1810aca4c1745a6c1716bab
",git fetch https://review.opendev.org/openstack/fuel-web refs/changes/57/117557/1 && git format-patch -1 --stdout FETCH_HEAD,"['nailgun/nailgun/network/neutron.py', 'nailgun/nailgun/test/integration/test_cluster_changes_handler.py', 'nailgun/nailgun/test/base.py', 'nailgun/nailgun/test/integration/test_orchestrator_serializer.py']",4,0a38e187e54647ed840a705365fe00f08f6e972b,bug/1362659," self.new_env_release_version = None release_kwargs = {} if self.new_env_release_version: release_kwargs['version'] = self.new_env_release_version release_kwargs=release_kwargs, def test_serialize_neutron_attrs_on_different_env_versions(self): # 5.1 environment self.new_env_release_version = ""2014.1.1-5.1"" cluster = self.create_env(mode='ha_compact') serialized_nodes = self.serializer.serialize(cluster, cluster.nodes) for node in serialized_nodes: self.assertEqual( { ""network_type"": ""local"", ""segment_id"": None, ""router_ext"": True, ""physnet"": None }, node['quantum_settings']['predefined_networks'][ 'net04_ext']['L2'] ) self.assertFalse( 'physnet1' in node['quantum_settings']['L2']['phys_nets'] ) # non-5.1 environment self.new_env_release_version = ""2014.1.1-5.0.2"" cluster = self.create_env(mode='ha_compact') serialized_nodes = self.serializer.serialize(cluster, cluster.nodes) for node in serialized_nodes: self.assertEqual( { ""network_type"": ""flat"", ""segment_id"": None, ""router_ext"": True, ""physnet"": ""physnet1"" }, node['quantum_settings']['predefined_networks'][ 'net04_ext']['L2'] ) self.assertEqual( { ""bridge"": ""br-ex"", ""vlan_range"": None }, node['quantum_settings']['L2']['phys_nets']['physnet1'] ) ",,61,3
openstack%2Fneutron~master~Ia6fe2bd0317c241bf7eb55915df7650dfdc68210,openstack/neutron,master,Ia6fe2bd0317c241bf7eb55915df7650dfdc68210,HA routers master state now distributed amongst agents,MERGED,2014-09-15 16:48:18.000000000,2014-09-26 18:47:54.000000000,2014-09-20 05:48:39.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 748}, {'_account_id': 2035}, {'_account_id': 5170}, {'_account_id': 5948}, {'_account_id': 7293}, {'_account_id': 7448}, {'_account_id': 7787}, {'_account_id': 8645}, {'_account_id': 8873}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10386}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 10971}, {'_account_id': 12040}, {'_account_id': 12444}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-09-15 16:48:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/01dea41467813c00037ba1327d1ca35b8c4803b0', 'message': ""HA routers master state now distributed amongst agents\n\nWe're currently running with no pre-emption, meaning that\nthe first router in a cluster to go up will be the master,\nregardless of priority. Since the order in which we sent\nnotifications was constant, the same agent hosted the\nmaster instances of all HA routers, defeating the idea\nof load sharing.\n\nCloses-Bug: #1365429\nChange-Id: Ia6fe2bd0317c241bf7eb55915df7650dfdc68210\n""}, {'number': 2, 'created': '2014-09-17 08:28:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/214fe67da8596f0bc9aad0e71773b4595e897ff5', 'message': ""HA routers master state now distributed amongst agents\n\nWe're currently running with no pre-emption, meaning that\nthe first router in a cluster to go up will be the master,\nregardless of priority. Since the order in which we sent\nnotifications was constant, the same agent hosted the\nmaster instances of all HA routers, defeating the idea\nof load sharing.\n\nCloses-Bug: #1365429\nChange-Id: Ia6fe2bd0317c241bf7eb55915df7650dfdc68210\n""}, {'number': 3, 'created': '2014-09-17 14:06:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/776456ddaba79ecf00a1400842e966797595e87a', 'message': ""HA routers master state now distributed amongst agents\n\nWe're currently running with no pre-emption, meaning that\nthe first router in a cluster to go up will be the master,\nregardless of priority. Since the order in which we sent\nnotifications was constant, the same agent hosted the\nmaster instances of all HA routers, defeating the idea\nof load sharing.\n\nCloses-Bug: #1365429\nChange-Id: Ia6fe2bd0317c241bf7eb55915df7650dfdc68210\n""}, {'number': 4, 'created': '2014-09-18 12:31:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c2fbbcc371e86fdef0bc1295ca2e0090db85e2f9', 'message': ""HA routers master state now distributed amongst agents\n\nWe're currently running with no pre-emption, meaning that\nthe first router in a cluster to go up will be the master,\nregardless of priority. Since the order in which we sent\nnotifications was constant, the same agent hosted the\nmaster instances of all HA routers, defeating the idea\nof load sharing.\n\nCloses-Bug: #1365429\nChange-Id: Ia6fe2bd0317c241bf7eb55915df7650dfdc68210\n""}, {'number': 5, 'created': '2014-09-18 14:08:25.000000000', 'files': ['neutron/plugins/nec/nec_router.py', 'neutron/db/l3_hamode_db.py', 'neutron/api/rpc/agentnotifiers/l3_rpc_agent_api.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/0bd4472ef7bdb9d94f988669f34f7eaa53ca0a89', 'message': ""HA routers master state now distributed amongst agents\n\nWe're currently running with no pre-emption, meaning that\nthe first router in a cluster to go up will be the master,\nregardless of priority. Since the order in which we sent\nnotifications was constant, the same agent hosted the\nmaster instances of all HA routers, defeating the idea\nof load sharing.\n\nCloses-Bug: #1365429\nChange-Id: Ia6fe2bd0317c241bf7eb55915df7650dfdc68210\n""}]",10,121620,0bd4472ef7bdb9d94f988669f34f7eaa53ca0a89,137,32,5,8873,,,0,"HA routers master state now distributed amongst agents

We're currently running with no pre-emption, meaning that
the first router in a cluster to go up will be the master,
regardless of priority. Since the order in which we sent
notifications was constant, the same agent hosted the
master instances of all HA routers, defeating the idea
of load sharing.

Closes-Bug: #1365429
Change-Id: Ia6fe2bd0317c241bf7eb55915df7650dfdc68210
",git fetch https://review.opendev.org/openstack/neutron refs/changes/20/121620/5 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/common/constants.py', 'neutron/db/l3_hamode_db.py', 'neutron/api/rpc/agentnotifiers/l3_rpc_agent_api.py']",3,01dea41467813c00037ba1327d1ca35b8c4803b0,bug/1365429,import random if operation == constants.HA_ROUTERS_UPDATED: random.shuffle(l3_agents),,7,1
openstack%2Fgovernance~master~I0b34fe8cb5e242440e2051b523a4b3444e2ef579,openstack/governance,master,I0b34fe8cb5e242440e2051b523a4b3444e2ef579,Add translation support requirement,ABANDONED,2014-06-04 18:21:07.000000000,2014-09-26 18:41:36.000000000,,"[{'_account_id': 2}, {'_account_id': 3}, {'_account_id': 7}, {'_account_id': 67}, {'_account_id': 308}, {'_account_id': 964}, {'_account_id': 1247}, {'_account_id': 2271}, {'_account_id': 2284}, {'_account_id': 2472}, {'_account_id': 2592}, {'_account_id': 2750}, {'_account_id': 2889}, {'_account_id': 3108}, {'_account_id': 6547}, {'_account_id': 11564}]","[{'number': 1, 'created': '2014-06-04 18:21:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/governance/commit/d3c2b8cd0d22a3a22ae362f1daa8310408274ed1', 'message': 'Add translation support requirement\n\nChange-Id: I0b34fe8cb5e242440e2051b523a4b3444e2ef579\n'}, {'number': 2, 'created': '2014-06-05 18:52:49.000000000', 'files': ['reference/incubation-integration-requirements.rst'], 'web_link': 'https://opendev.org/openstack/governance/commit/5095975117b3e46f2ecd14840cff7313a54045ed', 'message': 'Add translation support requirement\n\nChange-Id: I0b34fe8cb5e242440e2051b523a4b3444e2ef579\n'}]",13,97872,5095975117b3e46f2ecd14840cff7313a54045ed,68,16,2,2472,,,0,"Add translation support requirement

Change-Id: I0b34fe8cb5e242440e2051b523a4b3444e2ef579
",git fetch https://review.opendev.org/openstack/governance refs/changes/72/97872/1 && git format-patch -1 --stdout FETCH_HEAD,['reference/incubation-integration-requirements.rst'],1,d3c2b8cd0d22a3a22ae362f1daa8310408274ed1,i18n-requirements,"* Project must have translation support enabled for user-facing error messages and deployer-facing log messages, following the guidelines of the I18nTeam_ and `oslo.i18n`_. The translation export/import jobs must be running properly, and patches with translation updates must be reviewed and accepted by the reviewers when submitted by those jobs. If translators have not started processing the strings for a project, that should not block graduation if all of the other translation requirements are met. .. _I18nTeam: https://wiki.openstack.org/wiki/I18n .. _oslo.i18n: http://docs.openstack.org/developer/oslo.i18n/",,11,0
openstack%2Fbarbican~master~Id1aecd08640fcca324bdb2028d6b3b6867a8b892,openstack/barbican,master,Id1aecd08640fcca324bdb2028d6b3b6867a8b892,Deduplicate HACKING.rst with docs.openstack.org/developer/hacking/,MERGED,2014-09-24 21:35:47.000000000,2014-09-26 18:39:25.000000000,2014-09-26 18:39:24.000000000,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 7262}, {'_account_id': 7789}, {'_account_id': 8004}]","[{'number': 1, 'created': '2014-09-24 21:35:47.000000000', 'files': ['HACKING.rst'], 'web_link': 'https://opendev.org/openstack/barbican/commit/1262c9b204ad994c15d7719f741ee9db0f1a363a', 'message': 'Deduplicate HACKING.rst with docs.openstack.org/developer/hacking/\n\nReference the OpenStack hacking guide in HACKING.rst and remove\nduplicate entries. Add empty section as a place holder for barbican\nspecific rules.\n\nChange-Id: Id1aecd08640fcca324bdb2028d6b3b6867a8b892\n'}]",0,123859,1262c9b204ad994c15d7719f741ee9db0f1a363a,12,5,1,1849,,,0,"Deduplicate HACKING.rst with docs.openstack.org/developer/hacking/

Reference the OpenStack hacking guide in HACKING.rst and remove
duplicate entries. Add empty section as a place holder for barbican
specific rules.

Change-Id: Id1aecd08640fcca324bdb2028d6b3b6867a8b892
",git fetch https://review.opendev.org/openstack/barbican refs/changes/59/123859/1 && git format-patch -1 --stdout FETCH_HEAD,['HACKING.rst'],1,1262c9b204ad994c15d7719f741ee9db0f1a363a,HACKING,============================ - Step 1: Read the OpenStack Style Commandments http://docs.openstack.org/developer/hacking/ - Step 2: Read on Barbican Specific Commandments ------------------------------- ,"======================= TBD: Translate the content below to the Barbican project. ...Adding cosmetic change to trigger build process... - Step 1: Read http://www.python.org/dev/peps/pep-0008/ - Step 2: Read http://www.python.org/dev/peps/pep-0008/ again - Step 3: Read on General ------- - Put two newlines between top-level code (funcs, classes, etc) - Put one newline between methods in classes and anywhere else - Do not write ""except:"", use ""except Exception:"" at the very least - Include your name with TODOs as in ""#TODO(termie)"" - Do not name anything the same name as a built-in or reserved word Imports ------- - Do not make relative imports - Order your imports by the full module path - Organize your imports according to the following template Example:: # vim: tabstop=4 shiftwidth=4 softtabstop=4 {{stdlib imports in human alphabetical order}} \n {{third-party lib imports in human alphabetical order}} \n {{barbican imports in human alphabetical order}} \n \n {{begin your code}} Human Alphabetical Order Examples --------------------------------- Example:: import httplib import logging import random import StringIO import time import unittest import eventlet import webob.exc import barbican.api.middleware import barbican.common from barbican.crypto import plugin.py from barbican.model import repositories Docstrings ---------- Docstrings are required for all functions and methods. Docstrings should ONLY use triple-double-quotes (``""""""``) Single-line docstrings should NEVER have extraneous whitespace between enclosing triple-double-quotes. **INCORRECT** :: """""" There is some whitespace between the enclosing quotes :( """""" **CORRECT** :: """"""There is no whitespace between the enclosing quotes :)"""""" Docstrings that span more than one line should look like this: Example:: """""" Start the docstring on the line following the opening triple-double-quote If you are going to describe parameters and return values, use Sphinx, the appropriate syntax is as follows. :param foo: the foo parameter :param bar: the bar parameter :returns: return_type -- description of the return value :returns: description of the return value :raises: AttributeError, KeyError """""" **DO NOT** leave an extra newline before the closing triple-double-quote. Dictionaries/Lists ------------------ If a dictionary (dict) or list object is longer than 80 characters, its items should be split with newlines. Embedded iterables should have their items indented. Additionally, the last item in the dictionary should have a trailing comma. This increases readability and simplifies future diffs. Example:: my_dictionary = { ""image"": { ""name"": ""Just a Snapshot"", ""size"": 2749573, ""properties"": { ""user_id"": 12, ""arch"": ""x86_64"", }, ""things"": [ ""thing_one"", ""thing_two"", ], ""status"": ""ACTIVE"", }, } Calling Methods --------------- Calls to methods 80 characters or longer should format each argument with newlines. This is not a requirement, but a guideline:: unnecessarily_long_function_name('string one', 'string two', kwarg1=constants.ACTIVE, kwarg2=['a', 'b', 'c']) Rather than constructing parameters inline, it is better to break things up:: list_of_strings = [ 'what_a_long_string', 'not as long', ] dict_of_numbers = { 'one': 1, 'two': 2, 'twenty four': 24, } object_one.call_a_method('string three', 'string four', kwarg1=list_of_strings, kwarg2=dict_of_numbers) Internationalization (i18n) Strings ----------------------------------- In order to support multiple languages, we have a mechanism to support automatic translations of exception and log strings. Example:: msg = _(""An error occurred"") raise HTTPBadRequest(explanation=msg) If you have a variable to place within the string, first internationalize the template string then do the replacement. Example:: msg = _(""Missing parameter: %s"") % (""flavor"",) LOG.error(msg) If you have multiple variables to place in the string, use keyword parameters. This helps our translators reorder parameters when needed. Example:: msg = _(""The server with id %(s_id)s has no key %(m_key)s"") LOG.error(msg % {""s_id"": ""1234"", ""m_key"": ""imageId""}) Creating Unit Tests ------------------- For every new feature, unit tests should be created that both test and (implicitly) document the usage of said feature. If submitting a patch for a bug that had no unit test, a new passing unit test should be added. If a submitted bug fix does have a unit test, be sure to add a new one that fails without the patch and passes with the patch.",7,184
openstack%2Fopenstack-manuals~master~Ia95f88b1805491fe12d886223c26fad100aa8cc8,openstack/openstack-manuals,master,Ia95f88b1805491fe12d886223c26fad100aa8cc8,Fix sync of entities file,MERGED,2014-09-25 15:41:38.000000000,2014-09-26 18:38:48.000000000,2014-09-26 18:38:47.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 964}, {'_account_id': 6547}, {'_account_id': 7923}]","[{'number': 1, 'created': '2014-09-25 15:41:38.000000000', 'files': ['tools/sync-projects.sh'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/cdd2704bf892a8e77cb4704cc7eb7b0d32a656ea', 'message': 'Fix sync of entities file\n\nFix reference to entitites file.\n\nChange-Id: Ia95f88b1805491fe12d886223c26fad100aa8cc8\n'}]",2,124086,cdd2704bf892a8e77cb4704cc7eb7b0d32a656ea,9,5,1,6547,,,0,"Fix sync of entities file

Fix reference to entitites file.

Change-Id: Ia95f88b1805491fe12d886223c26fad100aa8cc8
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/86/124086/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/sync-projects.sh'],1,cdd2704bf892a8e77cb4704cc7eb7b0d32a656ea,fix2-entities," CHECK_MARK_DIR=""figures"" CHECK_MARK_DIR=""figures"" CHECK_MARK_DIR=""static"" CHECK_MARK_DIR=""figures""sed -i -e ""s|imagedata fileref=\""../common/figures|imagedata fileref=\""$CHECK_MARK_DIR|"" \ $GLOSSARY_DIR/../$ENT_DIR/openstack.ent",,6,0
openstack%2Fdiskimage-builder~master~Id11433ea342aace71a358936a7ca3151ec11d506,openstack/diskimage-builder,master,Id11433ea342aace71a358936a7ca3151ec11d506,Add svc-map element.,MERGED,2014-05-30 13:55:15.000000000,2014-09-26 18:36:16.000000000,2014-09-26 18:36:15.000000000,"[{'_account_id': 3}, {'_account_id': 360}, {'_account_id': 1726}, {'_account_id': 4190}, {'_account_id': 4330}, {'_account_id': 6449}, {'_account_id': 6796}, {'_account_id': 7144}, {'_account_id': 7582}, {'_account_id': 8449}, {'_account_id': 8532}]","[{'number': 1, 'created': '2014-05-30 13:55:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/f49b797035bf2b79d6673ca4912e4635d2c60482', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' JSON config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 2, 'created': '2014-06-03 13:18:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/8d7eb92cdbe0d2c3024ff9e02ef715043b59e13b', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' JSON config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 3, 'created': '2014-06-04 14:12:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/1285fece4cd8dc98924171054e6cd53a9198ba0d', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' JSON config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 4, 'created': '2014-06-04 18:46:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/ec65bd1e0e9f0cb56d2ddaf49d3fc03d72e43bba', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' JSON config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 5, 'created': '2014-07-02 14:39:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/e863b545d3095f75c250db1303c275eee32cdff2', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' JSON config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 6, 'created': '2014-07-23 20:15:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/bfa9715b8520bc4b00c698852a2826f5aa912e75', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' JSON config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 7, 'created': '2014-07-24 14:58:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/e12704e6a9c0ca13b3bac1e1a595775e3f644b15', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' JSON config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 8, 'created': '2014-09-08 15:24:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/52db4618b8a4ae696e20eb623c47df11e01220b7', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' JSON config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 9, 'created': '2014-09-11 02:33:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/35a47b36b057217bc88c373369ffbdbf43cc1e7e', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' JSON config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nThis patch ensures all of the json files are compiled into\na single json file without namespaces.  The json compilation\nprocess occurs during image creation time and errors if\nconflicting mappings are found.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 10, 'created': '2014-09-15 20:04:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/64e4201c276578319bb791ede80e7690b969bb6a', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' YAML config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nThis patch ensures all of the YAML files are compiled into\na single file without namespaces.  The YAML compilation\nprocess occurs during image creation time and errors if\nconflicting mappings are found.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 11, 'created': '2014-09-15 21:54:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/18286ae10fd0ccfe71604913b147a735dfd9e7ad', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' YAML config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nThis patch ensures all of the YAML files are compiled into\na single file without namespaces.  The YAML compilation\nprocess occurs during image creation time and errors if\nconflicting mappings are found.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 12, 'created': '2014-09-15 22:57:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/0c01ef35712ca03c8407032170e4eede4aa36209', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' YAML config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nThis patch ensures all of the YAML files are compiled into\na single file without namespaces.  The YAML compilation\nprocess occurs during image creation time and errors if\nconflicting mappings are found.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 13, 'created': '2014-09-16 00:58:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/5476ff23ed444bcba0114b80c01ba25742e4334c', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' YAML config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nThis patch ensures all of the YAML files are compiled into\na single file without namespaces.  The YAML compilation\nprocess occurs during image creation time and errors if\nconflicting mappings are found.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 14, 'created': '2014-09-16 03:28:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/3526057d93a1c3db82dd0ff49bb8c9d01ad032ab', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' YAML config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nThis patch ensures all of the YAML files are compiled into\na single file without namespaces.  The YAML compilation\nprocess occurs during image creation time and errors if\nconflicting mappings are found.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 15, 'created': '2014-09-17 13:24:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/cc23fd110c01e98329dbefc86d9dbe034abd5f1b', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' YAML config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nThis patch ensures all of the YAML files are compiled into\na single file without namespaces.  The YAML compilation\nprocess occurs during image creation time and errors if\nconflicting mappings are found.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 16, 'created': '2014-09-17 13:25:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/d22d0fc7d51176c015da6b8b75c81fa5dfd06554', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' YAML config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nThis patch ensures all of the YAML files are compiled into\na single file without namespaces.  The YAML compilation\nprocess occurs during image creation time and errors if\nconflicting mappings are found.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 17, 'created': '2014-09-18 11:03:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/8168793d05914ff2116caccd6507de6a34eee249', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' YAML config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nThis patch ensures all of the YAML files are compiled into\na single file without namespaces.  The YAML compilation\nprocess occurs during image creation time and errors if\nconflicting mappings are found.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 18, 'created': '2014-09-18 11:20:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/4c838fdc7c906dcf5e0b86f11920d9fdd236aaec', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' YAML config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nThis patch ensures all of the YAML files are compiled into\na single file without namespaces.  The YAML compilation\nprocess occurs during image creation time and errors if\nconflicting mappings are found.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}, {'number': 19, 'created': '2014-09-26 16:00:47.000000000', 'files': ['elements/svc-map/bin/svc-map', 'test-requirements.txt', 'elements/svc-map/README.md', 'elements/svc-map/extra-data.d/11-copy-svc-map-file', 'elements/svc-map/tests/test_data_merge.py', 'elements/svc-map/extra-data.d/10-merge-svc-map-files'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/e1853a7d5b2ef60d81560e34fbe0b1a705f2d495', 'message': ""Add svc-map element.\n\nAdds a new svc-map element which can be used to install\nservices based on an in element 'svc-map' YAML config\nfile format.\n\nThis change is intended to decouple elements from DIB\nand allow new elements to support multiple distributions\n(with various service naming schemes) without having to\nconstantly maintain DIB's various bin/map-services files.\n\nThis patch ensures all of the YAML files are compiled into\na single file without namespaces.  The YAML compilation\nprocess occurs during image creation time and errors if\nconflicting mappings are found.\n\nChange-Id: Id11433ea342aace71a358936a7ca3151ec11d506\n""}]",27,96770,e1853a7d5b2ef60d81560e34fbe0b1a705f2d495,165,11,19,8532,,,0,"Add svc-map element.

Adds a new svc-map element which can be used to install
services based on an in element 'svc-map' YAML config
file format.

This change is intended to decouple elements from DIB
and allow new elements to support multiple distributions
(with various service naming schemes) without having to
constantly maintain DIB's various bin/map-services files.

This patch ensures all of the YAML files are compiled into
a single file without namespaces.  The YAML compilation
process occurs during image creation time and errors if
conflicting mappings are found.

Change-Id: Id11433ea342aace71a358936a7ca3151ec11d506
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/70/96770/19 && git format-patch -1 --stdout FETCH_HEAD,"['elements/svc-map/extra-data.d/10-create-svc-map-dir', 'elements/svc-map/bin/svc-map', 'elements/svc-map/README.md']",3,f49b797035bf2b79d6673ca4912e4635d2c60482,svc-map,"Map service names to distro specific services. Provides the following: * bin/svc-map usage: svc-map [-h] [--element ELEMENT] [--distro DISTRO] Translate service name to distro specific name. optional arguments: -h, --help show this help message and exit --element ELEMENT The element (namespace) to use for translation. --distro DISTRO The distro name to use for translation. Defaults to DISTRO_NAME * Any element may create its own svc-map JSON config file using the one of 3 sections for the distro/family/ and or default. The family is set automatically within svc-map based on the supplied distro name. Families include: + redhat: includes centos, fedora, and rhel distros + debian: includes debian and ubuntu distros + suse: includes the opensuse distro The most specific section takes priority. Example for Nova and Glance (NOTE: using fictitious service names for Fedora to provide a good example!) Example format: { ""distro"": { ""fedora"": { ""nova_service"": ""openstack-compute"", ""glance_service"": ""openstack-image"" } }, ""family"": { ""redhat"": { ""nova_service"": ""openstack-nova"", ""glance_service"": ""openstack-glance"" } }, ""default"": { ""nova_service"": ""nova"", ""glance_service"": ""glance"" } } Example commands using this format: svc-map --element nova-compute --distro fedora nova_service Returns: openstack-compute svc-map --element nova-compute --distro rhel nova_service Returns: openstack-nova svc-map --element nova-compute --distro ubuntu nova_service Returns: nova * This output can be used to filter what other tools actually install (install-services can be modified to use this for example) * Individual svc-map files live within each element. For example if you are created an Apache element your svc-map JSON file should be created at elements/apache/svc-map. ",,186,0
openstack%2Fnova~stable%2Ficehouse~Ifc76f2b1cce834b3c9927359ac9b957bc9f9c65f,openstack/nova,stable/icehouse,Ifc76f2b1cce834b3c9927359ac9b957bc9f9c65f,Neutron: Atomic update of instance info cache,MERGED,2014-09-25 02:34:38.000000000,2014-09-26 18:11:29.000000000,2014-09-26 18:11:27.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 4393}, {'_account_id': 5170}, {'_account_id': 6873}, {'_account_id': 9656}, {'_account_id': 10583}]","[{'number': 1, 'created': '2014-09-25 02:34:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a53ed5d48acfa26cfaca6f4c4c01c45e296b6455', 'message': ""Neutron: Atomic update of instance info cache\n\nIn the Neutron network API implementation, there is a race condition\nbetween a thread performing periodic task to read and heal instance\nnetwork info cache and another thread servicing interface-attach or\ninterface-detach calls. This patch ensures that instance info cache is\nread and then updated in a synchronized block to ensure atomicity.\n\nPlease see the bug report for more details.\n\nConflicts:\n        nova/network/base_api.py\n        nova/network/neutronv2/api.py\n        nova/tests/network/test_neutronv2.py\n\nNOTE(mriedem): Notes on conflicts:\n\n1. base_api didn't exist until Juno so that's where refresh_cache\n   lives now.\n2. nova.network.neutronv2.api is using oslo.i18n in Juno rather\n   than the old gettextutils import.\n3. test_neutronv2 didn't start using mock until Juno and the change\n   that added the test class is probably something that we won't\n   be backporting, so the test class definition is brought back\n   with this change for the new test case.\n\nChange-Id: Ifc76f2b1cce834b3c9927359ac9b957bc9f9c65f\nCloses-Bug: #1326183\nCo-Authored-By: Dan Smith <dansmith@redhat.com>\n(cherry picked from commit 232cbfe67ffb7696f115830c711a960af5fa0828)\n""}, {'number': 2, 'created': '2014-09-25 14:08:40.000000000', 'files': ['nova/tests/network/test_neutronv2.py', 'nova/network/neutronv2/api.py', 'nova/network/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/dfb0e0fc1604c0673fb9af8316dbf0e5d2c93cd1', 'message': ""Neutron: Atomic update of instance info cache\n\nIn the Neutron network API implementation, there is a race condition\nbetween a thread performing periodic task to read and heal instance\nnetwork info cache and another thread servicing interface-attach or\ninterface-detach calls. This patch ensures that instance info cache is\nread and then updated in a synchronized block to ensure atomicity.\n\nPlease see the bug report for more details.\n\nConflicts:\n        nova/network/base_api.py\n        nova/network/neutronv2/api.py\n        nova/tests/network/test_neutronv2.py\n\nNOTE(mriedem): Notes on conflicts:\n\n1. base_api didn't exist until Juno so that's where refresh_cache\n   lives now.\n2. nova.network.neutronv2.api is using oslo.i18n in Juno rather\n   than the old gettextutils import.\n3. test_neutronv2 didn't start using mock until Juno and the change\n   that added the test class is probably something that we won't\n   be backporting, so the test class definition is brought back\n   with this change for the new test case.\n\nChange-Id: Ifc76f2b1cce834b3c9927359ac9b957bc9f9c65f\nCloses-Bug: #1326183\nCo-Authored-By: Dan Smith <dansmith@redhat.com>\n(cherry picked from commit 232cbfe67ffb7696f115830c711a960af5fa0828)\n""}]",0,123917,dfb0e0fc1604c0673fb9af8316dbf0e5d2c93cd1,24,7,2,6873,,,0,"Neutron: Atomic update of instance info cache

In the Neutron network API implementation, there is a race condition
between a thread performing periodic task to read and heal instance
network info cache and another thread servicing interface-attach or
interface-detach calls. This patch ensures that instance info cache is
read and then updated in a synchronized block to ensure atomicity.

Please see the bug report for more details.

Conflicts:
        nova/network/base_api.py
        nova/network/neutronv2/api.py
        nova/tests/network/test_neutronv2.py

NOTE(mriedem): Notes on conflicts:

1. base_api didn't exist until Juno so that's where refresh_cache
   lives now.
2. nova.network.neutronv2.api is using oslo.i18n in Juno rather
   than the old gettextutils import.
3. test_neutronv2 didn't start using mock until Juno and the change
   that added the test class is probably something that we won't
   be backporting, so the test class definition is brought back
   with this change for the new test case.

Change-Id: Ifc76f2b1cce834b3c9927359ac9b957bc9f9c65f
Closes-Bug: #1326183
Co-Authored-By: Dan Smith <dansmith@redhat.com>
(cherry picked from commit 232cbfe67ffb7696f115830c711a960af5fa0828)
",git fetch https://review.opendev.org/openstack/nova refs/changes/17/123917/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/network/test_neutronv2.py', 'nova/network/neutronv2/api.py', 'nova/network/api.py']",3,a53ed5d48acfa26cfaca6f4c4c01c45e296b6455,bug/1326183-icehouse,"from nova.openstack.common import lockutils with lockutils.lock('refresh_cache-%s' % instance['uuid']): update_instance_cache_with_nw_info(self, context, instance, nw_info=res)"," update_instance_cache_with_nw_info(self, context, instance, nw_info=res)",37,8
openstack%2Fnova~stable%2Ficehouse~I58f681f3674d826e3dce4fa956e2eb0e4c60f82b,openstack/nova,stable/icehouse,I58f681f3674d826e3dce4fa956e2eb0e4c60f82b,Ensure info cache updates don't overwhelm cells,MERGED,2014-09-25 02:34:38.000000000,2014-09-26 18:10:16.000000000,2014-09-26 18:10:13.000000000,"[{'_account_id': 3}, {'_account_id': 191}, {'_account_id': 1420}, {'_account_id': 5170}, {'_account_id': 6873}, {'_account_id': 9656}]","[{'number': 1, 'created': '2014-09-25 02:34:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/bd8730e498d0b0af670b2817bc51d2b41af1ca6c', 'message': ""Ensure info cache updates don't overwhelm cells\n\nCells does its own healing of the info cache outside\nof get_instance_nw_info, so we have double healing\nhappening if info_cache_cells_update is called for every\nget_instance_nw_info. This also overwhelms cells'\nability to keep the global db updated at scale. Similar\nfunctionality exists in nova/network/api.py.\n\nConflicts:\n        nova/network/neutronv2/api.py\n\nNOTE(mriedem): This is due to base_api not existing\nuntil Juno, where the refresh_cache decorator now\nlives.\n\nChange-Id: I58f681f3674d826e3dce4fa956e2eb0e4c60f82b\n(cherry picked from commit 446d433f6fea7624acae371189239a82c94ad455)\n""}, {'number': 2, 'created': '2014-09-25 14:08:40.000000000', 'files': ['nova/network/neutronv2/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/bce481c515b0c96db0b471eed70c3591e5a847d5', 'message': ""Ensure info cache updates don't overwhelm cells\n\nCells does its own healing of the info cache outside\nof get_instance_nw_info, so we have double healing\nhappening if info_cache_cells_update is called for every\nget_instance_nw_info. This also overwhelms cells'\nability to keep the global db updated at scale. Similar\nfunctionality exists in nova/network/api.py.\n\nConflicts:\n        nova/network/neutronv2/api.py\n\nNOTE(mriedem): This is due to base_api not existing\nuntil Juno, where the refresh_cache decorator now\nlives.\n\nChange-Id: I58f681f3674d826e3dce4fa956e2eb0e4c60f82b\n(cherry picked from commit 446d433f6fea7624acae371189239a82c94ad455)\n""}]",1,123916,bce481c515b0c96db0b471eed70c3591e5a847d5,23,6,2,6873,,,0,"Ensure info cache updates don't overwhelm cells

Cells does its own healing of the info cache outside
of get_instance_nw_info, so we have double healing
happening if info_cache_cells_update is called for every
get_instance_nw_info. This also overwhelms cells'
ability to keep the global db updated at scale. Similar
functionality exists in nova/network/api.py.

Conflicts:
        nova/network/neutronv2/api.py

NOTE(mriedem): This is due to base_api not existing
until Juno, where the refresh_cache decorator now
lives.

Change-Id: I58f681f3674d826e3dce4fa956e2eb0e4c60f82b
(cherry picked from commit 446d433f6fea7624acae371189239a82c94ad455)
",git fetch https://review.opendev.org/openstack/nova refs/changes/16/123916/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/network/neutronv2/api.py'],1,bd8730e498d0b0af670b2817bc51d2b41af1ca6c,bug/1326183-icehouse," base_api.update_instance_cache_with_nw_info(self, context, instance, result, update_cells=False)", @refresh_cache,2,1
openstack%2Fneutron~master~Ifdfb84e7a7dbb56e8a64baa9af1f5e11070da426,openstack/neutron,master,Ifdfb84e7a7dbb56e8a64baa9af1f5e11070da426,Perform combination check only when both ipv6 subnet modes are set,ABANDONED,2014-09-24 15:09:42.000000000,2014-09-26 18:09:38.000000000,,"[{'_account_id': 3}, {'_account_id': 748}, {'_account_id': 5170}, {'_account_id': 6524}, {'_account_id': 6685}, {'_account_id': 8645}, {'_account_id': 9008}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10386}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-09-24 15:09:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/92fb33dc7c3504adaebf549ffde5e090c0d472f9', 'message': 'Perform combination check only when both ipv6 subnet modes are set\n\nChange-Id: Ifdfb84e7a7dbb56e8a64baa9af1f5e11070da426\nCloses-Bug: #1373410\n'}, {'number': 2, 'created': '2014-09-24 22:34:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/77d333f7374168c1e76e7ac6feb908f57b08ad47', 'message': 'Perform combination check only when both ipv6 subnet modes are set\n\nChange-Id: Ifdfb84e7a7dbb56e8a64baa9af1f5e11070da426\nPartial-Bug: #1373417\n'}, {'number': 3, 'created': '2014-09-25 14:50:43.000000000', 'files': ['neutron/tests/unit/test_db_plugin.py', 'neutron/db/db_base_plugin_v2.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/78737335d8370d660777b4f30ad3a05129bafeda', 'message': 'Perform combination check only when both ipv6 subnet modes are set\n\nChange-Id: Ifdfb84e7a7dbb56e8a64baa9af1f5e11070da426\nPartial-Bug: #1373417\n'}]",3,123756,78737335d8370d660777b4f30ad3a05129bafeda,86,26,3,6685,,,0,"Perform combination check only when both ipv6 subnet modes are set

Change-Id: Ifdfb84e7a7dbb56e8a64baa9af1f5e11070da426
Partial-Bug: #1373417
",git fetch https://review.opendev.org/openstack/neutron refs/changes/56/123756/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/db/db_base_plugin_v2.py'],1,92fb33dc7c3504adaebf549ffde5e090c0d472f9,bug/1373417, if (ra_mode_set or address_mode_set) and ra_mode and addr_mode:, if ra_mode_set or address_mode_set:,1,1
openstack%2Fmagnetodb~master~I253e9d4287fb8ab5e5604df7ba00b99b37920409,openstack/magnetodb,master,I253e9d4287fb8ab5e5604df7ba00b99b37920409,Fix error handling,MERGED,2014-09-23 14:54:28.000000000,2014-09-26 17:50:29.000000000,2014-09-26 17:50:29.000000000,"[{'_account_id': 3}, {'_account_id': 5538}, {'_account_id': 8188}, {'_account_id': 8491}, {'_account_id': 8601}]","[{'number': 1, 'created': '2014-09-23 14:54:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/3569b54a2e3db52f3984d99693bafb42437ca425', 'message': 'Fix error handling\n\nChange-Id: I253e9d4287fb8ab5e5604df7ba00b99b37920409\n'}, {'number': 2, 'created': '2014-09-24 09:52:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/2e14f80d2a5d0a784eba40d1ea67b0b1d3e82ee0', 'message': 'Fix error handling\n\nChange-Id: I253e9d4287fb8ab5e5604df7ba00b99b37920409\n'}, {'number': 3, 'created': '2014-09-24 14:15:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/2f5a99e4d26b02d5079cd357c9ef0179bbeb2331', 'message': 'Fix error handling\n\nChange-Id: I253e9d4287fb8ab5e5604df7ba00b99b37920409\n'}, {'number': 4, 'created': '2014-09-25 14:03:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/14374497aa15915dc9691a5f852e506afd77f640', 'message': 'Fix error handling\n\nIn some cases our code throws Amazon DynamoDB specific\nexceptions even we work with REST API.\n\nCloses-bug: #1373955\n\nChange-Id: I253e9d4287fb8ab5e5604df7ba00b99b37920409\n'}, {'number': 5, 'created': '2014-09-26 13:50:34.000000000', 'files': ['magnetodb/storage/manager/async_simple_impl.py', 'magnetodb/tests/unittests/api/openstack/v1/test_delete_table.py', 'magnetodb/common/middleware/fault.py', 'magnetodb/common/middleware/ec2token.py', 'magnetodb/api/amz/dynamodb/action/create_table.py', 'magnetodb/storage/manager/queued_impl.py', 'magnetodb/api/amz/dynamodb/exception.py', 'magnetodb/api/amz/dynamodb/action/describe_table.py', 'magnetodb/api/amz/dynamodb/parser.py', 'tempest/api/keyvalue/in_progress/rest/test_list_table.py', 'magnetodb/api/amz/dynamodb/action/query.py', 'magnetodb/api/amz/dynamodb/action/__init__.py', 'magnetodb/common/exception.py', 'magnetodb/common/wsgi.py', 'magnetodb/api/amz/dynamodb/action/get_item.py', 'magnetodb/api/amz/dynamodb/action/put_item.py', 'magnetodb/api/amz/dynamodb/action/update_item.py', 'magnetodb/storage/manager/simple_impl.py', 'doc/source/update_item.rst', 'magnetodb/api/amz/controller.py', 'magnetodb/api/amz/dynamodb/action/delete_item.py', 'magnetodb/storage/manager/base_async_storage_manager.py'], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/c04e846720923f4661c945119be70036f5bf3847', 'message': 'Fix error handling\n\nIn some cases our code throws Amazon DynamoDB specific\nexceptions even we work with REST API.\n\nCloses-bug: #1373955\n\nChange-Id: I253e9d4287fb8ab5e5604df7ba00b99b37920409\n'}]",3,123458,c04e846720923f4661c945119be70036f5bf3847,22,5,5,8601,,,0,"Fix error handling

In some cases our code throws Amazon DynamoDB specific
exceptions even we work with REST API.

Closes-bug: #1373955

Change-Id: I253e9d4287fb8ab5e5604df7ba00b99b37920409
",git fetch https://review.opendev.org/openstack/magnetodb refs/changes/58/123458/5 && git format-patch -1 --stdout FETCH_HEAD,"['magnetodb/common/middleware/fault.py', 'magnetodb/common/middleware/ec2token.py', 'magnetodb/api/amz/dynamodb/action/create_table.py', 'magnetodb/api/amz/dynamodb/action/describe_table.py', 'magnetodb/api/amz/dynamodb/parser.py', 'magnetodb/api/amz/dynamodb/action/query.py', 'magnetodb/api/amz/dynamodb/action/__init__.py', 'magnetodb/common/exception.py', 'magnetodb/common/wsgi.py', 'magnetodb/api/amz/dynamodb/action/get_item.py', 'magnetodb/api/amz/dynamodb/action/put_item.py', 'magnetodb/api/amz/dynamodb/action/update_item.py', 'doc/source/update_item.rst', 'magnetodb/api/amz/controller.py', 'magnetodb/api/amz/dynamodb/action/delete_item.py']",15,3569b54a2e3db52f3984d99693bafb42437ca425,error-handling-fix,from magnetodb.api.amz.dynamodb.exception import AWSValidationException from magnetodb.api.amz.dynamodb.exception import AWSErrorResponseException raise AWSValidationException() except AWSErrorResponseException as e: raise AWSErrorResponseException() raise AWSErrorResponseException(), raise exception.ValidationException() except exception.AWSErrorResponseException as e: raise exception.AWSErrorResponseException() raise exception.AWSErrorResponseException(),103,412
openstack%2Ftrove-specs~master~I7cc337c4f290eec68baad73b730bc05af48c7423,openstack/trove-specs,master,I7cc337c4f290eec68baad73b730bc05af48c7423,Fix Table of Contents and add Kilo spec location,MERGED,2014-09-24 14:12:09.000000000,2014-09-26 17:45:19.000000000,2014-09-26 17:45:19.000000000,"[{'_account_id': 3}, {'_account_id': 4240}, {'_account_id': 5293}]","[{'number': 1, 'created': '2014-09-24 14:12:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove-specs/commit/b0472e2a7dc177e8f721d3ad75be96cf1c49173c', 'message': 'Fix toctree indexing depth\n\nChange-Id: I7cc337c4f290eec68baad73b730bc05af48c7423\n'}, {'number': 2, 'created': '2014-09-24 14:39:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove-specs/commit/5ed4808e9912bb0dc629f10e4a6c1c5e73336491', 'message': 'Fix toctree indexing depth\n\nChanges:\n - added kilo milestone directory with spec template\n - fixed index.rst\n\nBehaviour: starting this patch all checkins might be green(SUCCESS),\nnot taking into account Sphinx warnings/errors related to proposed docs.\n\nChange-Id: I7cc337c4f290eec68baad73b730bc05af48c7423\n'}, {'number': 3, 'created': '2014-09-26 10:55:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove-specs/commit/5ba90b4289d1b7d8af6f6cb4d2aff9b4d538ab05', 'message': 'Fix Table of Contents and add Kilo spec location\n\nAdded a kilo directory for specs for Kilo. Also fixed the\nTable of contents.\n\nCo-Authored-By: Nikhil Manchanda <SlickNik@gmail.com>\nChange-Id: I7cc337c4f290eec68baad73b730bc05af48c7423\n'}, {'number': 4, 'created': '2014-09-26 11:13:22.000000000', 'files': ['doc/source/index.rst', 'README.rst', 'specs/kilo/.gitignore', 'doc/source/template.rst', 'template.rst', 'specs/kilo/placeholder.rst'], 'web_link': 'https://opendev.org/openstack/trove-specs/commit/0ce8d353a420620ac9544ae5e36dcca55212819b', 'message': 'Fix Table of Contents and add Kilo spec location\n\nAdded a kilo directory for specs for Kilo. Also fixed the\nTable of contents.\n\nCo-Authored-By: Nikhil Manchanda <SlickNik@gmail.com>\nChange-Id: I7cc337c4f290eec68baad73b730bc05af48c7423\n'}]",0,123740,0ce8d353a420620ac9544ae5e36dcca55212819b,12,3,4,8415,,,0,"Fix Table of Contents and add Kilo spec location

Added a kilo directory for specs for Kilo. Also fixed the
Table of contents.

Co-Authored-By: Nikhil Manchanda <SlickNik@gmail.com>
Change-Id: I7cc337c4f290eec68baad73b730bc05af48c7423
",git fetch https://review.opendev.org/openstack/trove-specs refs/changes/40/123740/4 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/index.rst'],1,b0472e2a7dc177e8f721d3ad75be96cf1c49173c,, :maxdepth: 3 specs/*/*, :maxdepth: 2 specs/*,2,2
openstack%2Foslo.concurrency~master~I528228c5753cf2a4dbdfecb665ae05bc98131851,openstack/oslo.concurrency,master,I528228c5753cf2a4dbdfecb665ae05bc98131851,Add docs for oslo.config opts,ABANDONED,2014-09-26 16:11:23.000000000,2014-09-26 17:29:34.000000000,,[],"[{'number': 1, 'created': '2014-09-26 16:11:23.000000000', 'files': ['doc/source/index.rst', 'doc/source/opts.rst'], 'web_link': 'https://opendev.org/openstack/oslo.concurrency/commit/b4ab53d3a2e24c17d5503f3771e8aadbccf2acf2', 'message': 'Add docs for oslo.config opts\n\nChange-Id: I528228c5753cf2a4dbdfecb665ae05bc98131851\n'}]",0,124454,b4ab53d3a2e24c17d5503f3771e8aadbccf2acf2,2,0,1,6928,,,0,"Add docs for oslo.config opts

Change-Id: I528228c5753cf2a4dbdfecb665ae05bc98131851
",git fetch https://review.opendev.org/openstack/oslo.concurrency refs/changes/54/124454/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/index.rst', 'doc/source/opts.rst']",2,b4ab53d3a2e24c17d5503f3771e8aadbccf2acf2,concurrency-cleanup,---------------------- Configuration Options ---------------------- .. currentmodule:: oslo.concurrency.opts .. autofunction:: list_opts ,,8,0
openstack%2Fneutron~master~I01753a472200a961cdcecee703616fd3239abd3c,openstack/neutron,master,I01753a472200a961cdcecee703616fd3239abd3c,Fix to delete user and group association in Nuage Plugin,MERGED,2014-09-10 22:54:20.000000000,2014-09-26 17:25:34.000000000,2014-09-26 17:25:32.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 490}, {'_account_id': 748}, {'_account_id': 841}, {'_account_id': 1923}, {'_account_id': 5170}, {'_account_id': 6659}, {'_account_id': 7962}, {'_account_id': 8645}, {'_account_id': 9380}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 11784}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-09-10 22:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ab422c41f66ec4c555ac440645b20ae5bbe03683', 'message': 'Fix for user and group association\n\nAfter a router delete operation, the attached zone to that\nrouter is also deleted. Got rid of code that tried to get\nthe nuage_zone after router delete operation.\nCloses-Bug: 1367864\n\nChange-Id: I01753a472200a961cdcecee703616fd3239abd3c\n'}, {'number': 2, 'created': '2014-09-11 07:01:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/1c6d2f1a9c086b06d82fea69c3443d6bd6b7faf2', 'message': 'Fix for user and group association\n\nAfter a router delete operation, the attached zone to that\nrouter is also deleted. Got rid of code that tried to do\na get operation on the nuage_zone after router delete \noperation.\nCloses-Bug: 1367864\n\nChange-Id: I01753a472200a961cdcecee703616fd3239abd3c\n'}, {'number': 3, 'created': '2014-09-12 19:07:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/53485e03a85e06edcf658c7fba6e01ebfa3038b8', 'message': 'Fix for user and group association\n\nAfter a router delete operation, the attached zone to that\nrouter is also deleted. Got rid of code that tried to do\na get operation on the nuage_zone after router delete\noperation.\nCloses-Bug: #1367864\n\nChange-Id: I01753a472200a961cdcecee703616fd3239abd3c\n'}, {'number': 4, 'created': '2014-09-17 18:43:40.000000000', 'files': ['neutron/plugins/nuage/plugin.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/1d4e13574574472e299296892d7c9e8f706aea67', 'message': 'Fix to delete user and group association in Nuage Plugin\n\nAfter a router delete operation, the attached zone to that\nrouter is also deleted. Got rid of code that tried to do\na get operation on the nuage_zone after router delete\noperation.\nCloses-Bug: #1367864\n\nChange-Id: I01753a472200a961cdcecee703616fd3239abd3c\n'}]",3,120600,1d4e13574574472e299296892d7c9e8f706aea67,110,29,4,11784,,,0,"Fix to delete user and group association in Nuage Plugin

After a router delete operation, the attached zone to that
router is also deleted. Got rid of code that tried to do
a get operation on the nuage_zone after router delete
operation.
Closes-Bug: #1367864

Change-Id: I01753a472200a961cdcecee703616fd3239abd3c
",git fetch https://review.opendev.org/openstack/neutron refs/changes/00/120600/4 && git format-patch -1 --stdout FETCH_HEAD,['neutron/plugins/nuage/plugin.py'],1,ab422c41f66ec4c555ac440645b20ae5bbe03683,bug/1367864, if not self._check_router_subnet_for_tenant(, nuage_zone = self.nuageclient.get_zone_by_routerid(id) if nuage_zone and not self._check_router_subnet_for_tenant(,1,2
openstack%2Fkeystone~master~I8eac8b83110aef2fc9cb638eca6784c097bcb22c,openstack/keystone,master,I8eac8b83110aef2fc9cb638eca6784c097bcb22c,Mark k2k as experimental,MERGED,2014-09-25 16:35:50.000000000,2014-09-26 17:25:28.000000000,2014-09-26 17:25:28.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 792}, {'_account_id': 1916}, {'_account_id': 2218}, {'_account_id': 2903}, {'_account_id': 5046}, {'_account_id': 5707}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 7191}, {'_account_id': 7725}, {'_account_id': 8978}]","[{'number': 1, 'created': '2014-09-25 16:35:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/327cae1e56640ab39f1e0ee79ccba744c20264df', 'message': 'Mark k2k as experimental\n\nClearly label that running keystone as an IdP is experimental.\n\nChange-Id: I8eac8b83110aef2fc9cb638eca6784c097bcb22c\n'}, {'number': 2, 'created': '2014-09-25 16:47:38.000000000', 'files': ['doc/source/configure_federation.rst'], 'web_link': 'https://opendev.org/openstack/keystone/commit/dec20865d1febe40f3743c211e04a9c2fa1106c3', 'message': 'Mark k2k as experimental\n\nClearly label that running keystone as an IdP is experimental.\n\nChange-Id: I8eac8b83110aef2fc9cb638eca6784c097bcb22c\n'}]",0,124107,dec20865d1febe40f3743c211e04a9c2fa1106c3,13,13,2,6482,,,0,"Mark k2k as experimental

Clearly label that running keystone as an IdP is experimental.

Change-Id: I8eac8b83110aef2fc9cb638eca6784c097bcb22c
",git fetch https://review.opendev.org/openstack/keystone refs/changes/07/124107/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/configure_federation.rst'],1,327cae1e56640ab39f1e0ee79ccba744c20264df,mark_k2k_experimental,.. WARNING:: Running Keystone as an IdP is considered experimental for the Juno release and will continue to see improvement over the next development cycle. ,,5,0
openstack%2Fkeystone~stable%2Ficehouse~I1babb065065cb5b06899f59568020a1c38f1156c,openstack/keystone,stable/icehouse,I1babb065065cb5b06899f59568020a1c38f1156c,Remove extraenous instantiations of managers,MERGED,2014-09-11 22:12:15.000000000,2014-09-26 17:25:19.000000000,2014-09-26 17:25:18.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 1420}, {'_account_id': 2903}, {'_account_id': 9656}]","[{'number': 1, 'created': '2014-09-11 22:12:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/830e05c740d54d3d26bda5e5d8ef2140ca7c7d23', 'message': 'Remove extraenous instantiations of managers\n\nThere were cases where a number of the API managers were being\ninstantiated more than one time. This could cause a number of\nodd edge cases where the managers would have different\nconfigurations and/or different dependency injection results.\n\nThe managers should now be properly instantiated only once\nunless explicitly required (e.g. testing the token provider\nmanager raises an exception in badly configured states).\n\nCloses-Bug: #1294994\nChange-Id: I1babb065065cb5b06899f59568020a1c38f1156c\n'}, {'number': 2, 'created': '2014-09-11 22:28:38.000000000', 'files': ['keystone/tests/test_auth.py', 'keystone/token/providers/common.py', 'keystone/tests/test_auth_plugin.py', 'keystone/auth/plugins/token.py', 'keystone/tests/test_sql_upgrade.py', 'keystone/tests/test_token_provider.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/a391713b6216ad13a1aa6e1e9fbf947923fed433', 'message': 'Remove extraenous instantiations of managers\n\nThere were cases where a number of the API managers were being\ninstantiated more than one time. This could cause a number of\nodd edge cases where the managers would have different\nconfigurations and/or different dependency injection results.\n\nThe managers should now be properly instantiated only once\nunless explicitly required (e.g. testing the token provider\nmanager raises an exception in badly configured states).\n\nConflicts:\n\tkeystone/auth/plugins/token.py\n\nCloses-Bug: #1294994\nChange-Id: I1babb065065cb5b06899f59568020a1c38f1156c\n(cherry-picked from commit 0a1cb0e20247a3c7856b409452b01ad6db8069f0)\n'}]",0,120924,a391713b6216ad13a1aa6e1e9fbf947923fed433,17,5,2,4,,,0,"Remove extraenous instantiations of managers

There were cases where a number of the API managers were being
instantiated more than one time. This could cause a number of
odd edge cases where the managers would have different
configurations and/or different dependency injection results.

The managers should now be properly instantiated only once
unless explicitly required (e.g. testing the token provider
manager raises an exception in badly configured states).

Conflicts:
	keystone/auth/plugins/token.py

Closes-Bug: #1294994
Change-Id: I1babb065065cb5b06899f59568020a1c38f1156c
(cherry-picked from commit 0a1cb0e20247a3c7856b409452b01ad6db8069f0)
",git fetch https://review.opendev.org/openstack/keystone refs/changes/24/120924/2 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/tests/test_auth.py', 'keystone/token/providers/common.py', 'keystone/tests/test_auth_plugin.py', 'keystone/auth/plugins/token.py', 'keystone/tests/test_sql_upgrade.py', 'keystone/tests/test_token_provider.py']",6,830e05c740d54d3d26bda5e5d8ef2140ca7c7d23,bug/1294994," self.assertRaises(exception.UnexpectedError, token.provider.Manager) self.assertRaises(exception.UnexpectedError, token.provider.Manager) class TestTokenProviderOAuth1(tests.TestCase): def setUp(self): super(TestTokenProviderOAuth1, self).setUp() self.load_backends() def config_overrides(self): super(TestTokenProviderOAuth1, self).config_overrides() def test_uuid_provider_no_oauth_fails_oauth(self): self.load_fixtures(default_fixtures) self.token_provider_api.driver.oauth_api = None self.token_provider_api.driver.issue_v3_token,"," try: token.provider.Manager() raise Exception( 'expecting ValueError on token provider misconfiguration') except exception.UnexpectedError: pass try: token.provider.Manager() raise Exception( 'expecting ValueError on token provider misconfiguration') except exception.UnexpectedError: pass def test_uuid_provider_no_oauth_fails_oauth(self): self.load_fixtures(default_fixtures) driver = token.provider.Manager().driver driver.oauth_api = None driver.issue_v3_token,",25,44
openstack%2Fnova~master~I8b1a36163984fbef6be66964456072d32da6a9d0,openstack/nova,master,I8b1a36163984fbef6be66964456072d32da6a9d0,postgresql: use postgres db instead of template1,MERGED,2014-09-16 22:36:04.000000000,2014-09-26 17:24:54.000000000,2014-09-26 17:24:51.000000000,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 5441}, {'_account_id': 6873}, {'_account_id': 8412}, {'_account_id': 9008}, {'_account_id': 9545}, {'_account_id': 9578}, {'_account_id': 10385}, {'_account_id': 10618}]","[{'number': 1, 'created': '2014-09-16 22:36:04.000000000', 'files': ['nova/tests/db/test_migrations.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/02fa15b4caaa414930b6ddf6a3a9fe8019751c26', 'message': 'postgresql: use postgres db instead of template1\n\nIn PostgreSQL template1 database is kind of special, as all new databases are\ncopied from it, when you do CREATE DATABASE. This implies a few restrictions:\ne.g. you can\'t have more than one active session to template1, if one of them\ntries to create a database. It will fail with:\n\n    ERROR: source database ""template1"" is being accessed by other users\n\nUsing of postgres database instead of template1 resolves this issue.\n\nCloses-Bug: #1355929\n\nChange-Id: I8b1a36163984fbef6be66964456072d32da6a9d0\n'}]",0,122011,02fa15b4caaa414930b6ddf6a3a9fe8019751c26,17,12,1,6849,,,0,"postgresql: use postgres db instead of template1

In PostgreSQL template1 database is kind of special, as all new databases are
copied from it, when you do CREATE DATABASE. This implies a few restrictions:
e.g. you can't have more than one active session to template1, if one of them
tries to create a database. It will fail with:

    ERROR: source database ""template1"" is being accessed by other users

Using of postgres database instead of template1 resolves this issue.

Closes-Bug: #1355929

Change-Id: I8b1a36163984fbef6be66964456072d32da6a9d0
",git fetch https://review.opendev.org/openstack/nova refs/changes/11/122011/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/db/test_migrations.py'],1,02fa15b4caaa414930b6ddf6a3a9fe8019751c26,bug/1355929," # operations there is a special database postgres. "" '%(sql)s' -d postgres"") "" '%(sql)s' -d postgres"")"," # operations there is a special database template1. "" '%(sql)s' -d template1"") "" '%(sql)s' -d template1"")",3,3
openstack%2Fnova~master~Ida5591f9e4d1eb61e229096d541a8285803efeb5,openstack/nova,master,Ida5591f9e4d1eb61e229096d541a8285803efeb5,Increase sleeps in baremetal driver,MERGED,2014-09-12 15:54:25.000000000,2014-09-26 17:24:30.000000000,2014-09-26 17:24:27.000000000,"[{'_account_id': 3}, {'_account_id': 360}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 1926}, {'_account_id': 2889}, {'_account_id': 4190}, {'_account_id': 5170}, {'_account_id': 6488}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-12 15:54:25.000000000', 'files': ['nova/cmd/baremetal_deploy_helper.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/b8e4c5fe94c9054a7f6906d980b00f0aeef23864', 'message': 'Increase sleeps in baremetal driver\n\nThe current sleeps are not long enough in some cases, increasing them\nvery simply makes the driver far more reliable.\n\nChange-Id: Ida5591f9e4d1eb61e229096d541a8285803efeb5\nCloses-Bug: #1316350\n'}]",3,121155,b8e4c5fe94c9054a7f6906d980b00f0aeef23864,25,12,1,1926,,,0,"Increase sleeps in baremetal driver

The current sleeps are not long enough in some cases, increasing them
very simply makes the driver far more reliable.

Change-Id: Ida5591f9e4d1eb61e229096d541a8285803efeb5
Closes-Bug: #1316350
",git fetch https://review.opendev.org/openstack/nova refs/changes/55/121155/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/cmd/baremetal_deploy_helper.py'],1,b8e4c5fe94c9054a7f6906d980b00f0aeef23864,more-sleep, time.sleep(10) time.sleep(10), time.sleep(3) time.sleep(3),2,2
openstack%2Fnova~master~I1a5db4e9817390ba3c2423d33387f780c890de64,openstack/nova,master,I1a5db4e9817390ba3c2423d33387f780c890de64,Undo changes to obj_make_compatible,MERGED,2014-09-15 20:03:26.000000000,2014-09-26 17:24:05.000000000,2014-09-26 17:24:02.000000000,"[{'_account_id': 3}, {'_account_id': 67}, {'_account_id': 1030}, {'_account_id': 1653}, {'_account_id': 2271}, {'_account_id': 4393}, {'_account_id': 5170}, {'_account_id': 5367}, {'_account_id': 6802}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-15 20:03:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/7bf5e29f07a24a2f01a5ba046c263068bbc384a8', 'message': 'Undo changes to obj_make_compatible\n\nCommit 7cdfdccf1bb936d559bd3e247094a817bb3c03f4 attempted to make\nthe obj_make_compatible calls consistent, but it actually changed\nthem the wrong way. This fixes them to unwrap nova_object.data\nand updates the test that was testing the reverse.\n\nChange-Id: I1a5db4e9817390ba3c2423d33387f780c890de64\n'}, {'number': 2, 'created': '2014-09-22 17:59:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9f41d1eaf22904469a4dbc710a9ea2a595e4a3d6', 'message': 'Undo changes to obj_make_compatible\n\nCommit 7cdfdccf1bb936d559bd3e247094a817bb3c03f4 attempted to make\nthe obj_make_compatible calls consistent, but it actually changed\nthem the wrong way. This fixes them to unwrap nova_object.data\nand updates the test that was testing the reverse.\n\nChange-Id: I1a5db4e9817390ba3c2423d33387f780c890de64\n'}, {'number': 3, 'created': '2014-09-22 20:14:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/28a15ab0496a10aa5f415c8435be904385c20e35', 'message': 'Undo changes to obj_make_compatible\n\nCommit 7cdfdccf1bb936d559bd3e247094a817bb3c03f4 attempted to make\nthe obj_make_compatible calls consistent, but it actually changed\nthem the wrong way. This fixes them to unwrap nova_object.data\nand updates the test that was testing the reverse.\n\nChange-Id: I1a5db4e9817390ba3c2423d33387f780c890de64\n'}, {'number': 4, 'created': '2014-09-25 18:09:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/342a10f941e833c71024b6870ceafca7037d324c', 'message': 'Undo changes to obj_make_compatible\n\nCommit 7cdfdccf1bb936d559bd3e247094a817bb3c03f4 attempted to make\nthe obj_make_compatible calls consistent, but it actually changed\nthem the wrong way. This fixes them to unwrap nova_object.data\nand updates the test that was testing the reverse.\n\nChange-Id: I1a5db4e9817390ba3c2423d33387f780c890de64\n'}, {'number': 5, 'created': '2014-09-25 18:19:41.000000000', 'files': ['nova/objects/floating_ip.py', 'nova/objects/service.py', 'nova/tests/objects/test_floating_ip.py', 'nova/objects/block_device.py', 'nova/objects/instance.py', 'nova/objects/fixed_ip.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/279b6e98bc72c906ac2c3f8665a1acdc99b30833', 'message': 'Undo changes to obj_make_compatible\n\nCommit 7cdfdccf1bb936d559bd3e247094a817bb3c03f4 attempted to make\nthe obj_make_compatible calls consistent, but it actually changed\nthem the wrong way. This fixes them to unwrap nova_object.data\nand updates the test that was testing the reverse.\n\nChange-Id: I1a5db4e9817390ba3c2423d33387f780c890de64\nCloses-Bug: #1373535\n'}]",0,121663,279b6e98bc72c906ac2c3f8665a1acdc99b30833,39,12,5,67,,,0,"Undo changes to obj_make_compatible

Commit 7cdfdccf1bb936d559bd3e247094a817bb3c03f4 attempted to make
the obj_make_compatible calls consistent, but it actually changed
them the wrong way. This fixes them to unwrap nova_object.data
and updates the test that was testing the reverse.

Change-Id: I1a5db4e9817390ba3c2423d33387f780c890de64
Closes-Bug: #1373535
",git fetch https://review.opendev.org/openstack/nova refs/changes/63/121663/4 && git format-patch -1 --stdout FETCH_HEAD,"['nova/objects/floating_ip.py', 'nova/objects/service.py', 'nova/tests/objects/test_floating_ip.py', 'nova/objects/block_device.py', 'nova/objects/instance.py', 'nova/objects/fixed_ip.py']",6,7bf5e29f07a24a2f01a5ba046c263068bbc384a8,bug/1365606," self.instance.obj_make_compatible( primitive['instance']['nova_object.data'], '1.14') self.instance.obj_make_compatible( primitive['instance']['nova_object.data'], '1.13')"," self.instance.obj_make_compatible(primitive['instance'], '1.14') self.instance.obj_make_compatible(primitive['instance'], '1.13')",25,19
openstack%2Fcinder~master~If1cff4924585a11e5545cd8f3b206d574f5a4558,openstack/cinder,master,If1cff4924585a11e5545cd8f3b206d574f5a4558,Imported Translations from Transifex,MERGED,2014-09-11 06:09:26.000000000,2014-09-26 17:06:49.000000000,2014-09-25 05:12:02.000000000,"[{'_account_id': 3}, {'_account_id': 1207}, {'_account_id': 6547}, {'_account_id': 9008}, {'_account_id': 10503}, {'_account_id': 10622}, {'_account_id': 10628}, {'_account_id': 11811}, {'_account_id': 12017}, {'_account_id': 12369}, {'_account_id': 12778}, {'_account_id': 12780}, {'_account_id': 13049}]","[{'number': 1, 'created': '2014-09-11 06:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/e6ac0425637ade39a2d60de8ab42be11c91742bd', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 2, 'created': '2014-09-12 06:03:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/da0f0e41f18d6a48f5dd2a4deca2266e126f63b3', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 3, 'created': '2014-09-13 06:08:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9493dceed8d533925a6e87fbfae79c81cd0e743c', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 4, 'created': '2014-09-14 06:07:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c144847d851c9659dd52c84d016d605fc296f8f2', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 5, 'created': '2014-09-15 06:05:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/cc2eca04124ed4e3f140ad5f922708ecdeb067b6', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 6, 'created': '2014-09-16 06:05:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/7945714a364e4fe66b94e11d74966a9662c47971', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 7, 'created': '2014-09-17 06:13:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/f7100a91df6ded3cbac7469c7d344bc19c2f48eb', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 8, 'created': '2014-09-18 06:11:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/aa91ea8eace01384640200d67b9236de801b1129', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 9, 'created': '2014-09-19 06:38:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/258d6bc21ce37022161d8e7c3de395e3e64cdb54', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 10, 'created': '2014-09-20 06:09:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/dbc5556bca13599dca9fdd206595bfe5f0e298bd', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 11, 'created': '2014-09-21 06:06:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/80b3776b238bc414a967ecb3d2442018937c2b73', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 12, 'created': '2014-09-22 06:05:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/3e4df0993184d773c0964c697422f6320b45af5e', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 13, 'created': '2014-09-23 06:04:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/fd96b515e70d3c7d0392662180010214d54f0621', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}, {'number': 14, 'created': '2014-09-24 06:04:11.000000000', 'files': ['cinder/locale/tr_TR/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/ko_KR/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/pt_BR/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/zh_CN/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/te_IN/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/en_GB/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/zh_CN/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/it/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/ko_KR/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/es/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/en_US/LC_MESSAGES/cinder.po', 'cinder/locale/te_IN/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/te_IN/LC_MESSAGES/cinder-log-warning.po', 'cinder/locale/cinder.pot', 'cinder/locale/en_GB/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/en_GB/LC_MESSAGES/cinder-log-warning.po', 'cinder/locale/fr/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/pt_BR/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/cinder-log-warning.pot', 'cinder/locale/de/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/fr/LC_MESSAGES/cinder-log-warning.po', 'cinder/locale/ja/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/ru/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/cinder-log-info.pot', 'cinder/locale/it/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/cs/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/zh_TW/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/es/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/ja/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/vi_VN/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/en_AU/LC_MESSAGES/cinder-log-error.po', 'cinder/locale/en_AU/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/cinder-log-error.pot', 'cinder/locale/de/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/zh_TW/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/fr/LC_MESSAGES/cinder-log-info.po'], 'web_link': 'https://opendev.org/openstack/cinder/commit/731c48100222da5500b7c567713a2c7454986314', 'message': 'Imported Translations from Transifex\n\nChange-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558\n'}]",0,120696,731c48100222da5500b7c567713a2c7454986314,85,13,14,11131,,,0,"Imported Translations from Transifex

Change-Id: If1cff4924585a11e5545cd8f3b206d574f5a4558
",git fetch https://review.opendev.org/openstack/cinder refs/changes/96/120696/8 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/locale/cinder.pot', 'cinder/locale/en_US/LC_MESSAGES/cinder.po']",2,e6ac0425637ade39a2d60de8ab42be11c91742bd,transifex/translations,"""POT-Creation-Date: 2014-09-11 06:08+0000\n""#: cinder/exception.py:723 cinder/exception.py:736 cinder/exception.py:762msgid ""Unknown SMBFS exception."" msgstr """" #: cinder/exception.py:745 msgid ""No mounted SMBFS shares found."" msgstr """" #: cinder/exception.py:749 #, python-format msgid ""There is no share which can host %(volume_size)sG."" msgstr """" #: cinder/exception.py:754#: cinder/exception.py:758#: cinder/exception.py:767msgid ""VDisk doesn't exist (%(vdisk)s)""msgid ""Unable to connect to MSA array""msgid ""Not enough space on VDisk (%(vdisk)s)"" msgstr """" #: cinder/exception.py:784 #, python-format msgid ""Fibre Channel connection control failure: %(reason)s"" msgstr """" #: cinder/exception.py:788 #, python-format msgid ""Fibre Channel Zone operation failed: %(reason)s"" msgstr """" #: cinder/exception.py:792 #, python-format#: cinder/exception.py:796 cinder/exception.py:800#: cinder/exception.py:804#: cinder/exception.py:813#: cinder/exception.py:829#: cinder/exception.py:833#: cinder/exception.py:838#: cinder/exception.py:842#: cinder/exception.py:847#: cinder/exception.py:864#: cinder/exception.py:869msgid ""smbfs_mount_point_base required"" msgstr """" #: cinder/brick/remotefs/remotefs.py:55#: cinder/brick/remotefs/remotefs.py:96#: cinder/brick/remotefs/remotefs.py:136#: cinder/consistencygroup/api.py:364 cinder/volume/manager.py:1810#: cinder/volume/api.py:1001 cinder/volume/manager.py:1450#: cinder/volume/manager.py:543 cinder/volume/manager.py:1830#: cinder/volume/manager.py:618 cinder/volume/manager.py:1930#: cinder/volume/manager.py:1018#: cinder/volume/manager.py:1021#: cinder/volume/manager.py:1044#: cinder/volume/manager.py:1073#: cinder/volume/manager.py:1085#: cinder/volume/manager.py:1106#: cinder/volume/manager.py:1192#: cinder/volume/manager.py:1200#: cinder/volume/manager.py:1236#: cinder/volume/manager.py:1304#: cinder/volume/manager.py:1306#: cinder/volume/manager.py:1308#: cinder/volume/manager.py:1313#: cinder/volume/manager.py:1387#: cinder/volume/manager.py:1388#: cinder/volume/manager.py:1419#: cinder/volume/manager.py:1422#: cinder/volume/manager.py:1433#: cinder/volume/manager.py:1441#: cinder/volume/manager.py:1489#: cinder/volume/manager.py:1491#: cinder/volume/manager.py:1521#: cinder/volume/manager.py:1530#: cinder/volume/manager.py:1540 cinder/volume/manager.py:1571#: cinder/volume/manager.py:1552#: cinder/volume/manager.py:1561#: cinder/volume/manager.py:1579#: cinder/volume/manager.py:1587#: cinder/volume/manager.py:1605#: cinder/volume/manager.py:1623#: cinder/volume/manager.py:1637#: cinder/volume/manager.py:1645#: cinder/volume/manager.py:1664#: cinder/volume/manager.py:1674#: cinder/volume/manager.py:1702#: cinder/volume/manager.py:1725#: cinder/volume/manager.py:1742#: cinder/volume/manager.py:1760#: cinder/volume/manager.py:1773#: cinder/volume/manager.py:1848#: cinder/volume/manager.py:1861#: cinder/volume/manager.py:1896#: cinder/volume/manager.py:1941#: cinder/volume/drivers/glusterfs.py:179 cinder/volume/drivers/remotefs.py:115#: cinder/volume/drivers/glusterfs.py:233 cinder/volume/drivers/remotefs.py:165#: cinder/volume/drivers/glusterfs.py:312#: cinder/volume/drivers/glusterfs.py:336#: cinder/volume/drivers/glusterfs.py:349#: cinder/volume/drivers/glusterfs.py:398#: cinder/volume/drivers/glusterfs.py:424 cinder/volume/drivers/smbfs.py:442#: cinder/volume/drivers/glusterfs.py:432#: cinder/volume/drivers/glusterfs.py:450#: cinder/volume/drivers/glusterfs.py:476 cinder/volume/drivers/remotefs.py:152#: cinder/volume/drivers/glusterfs.py:548#: cinder/volume/drivers/glusterfs.py:563#: cinder/volume/drivers/glusterfs.py:571#: cinder/volume/drivers/glusterfs.py:600 msgid ""Call to Nova to create snapshot failed"" msgstr """" #: cinder/volume/drivers/glusterfs.py:622 msgid ""Nova returned \""error\"" status while creating snapshot."" msgstr """" #: cinder/volume/drivers/glusterfs.py:639 #, python-format msgid ""Timed out while waiting for Nova update for creation of snapshot %s."" msgstr """" #: cinder/volume/drivers/nfs.py:249 cinder/volume/drivers/smbfs.py:403 #: cinder/volume/drivers/netapp/nfs.py:612#: cinder/volume/drivers/nfs.py:257 cinder/volume/drivers/smbfs.py:410#: cinder/volume/drivers/remotefs.py:261 cinder/volume/drivers/smbfs.py:522#: cinder/volume/drivers/remotefs.py:262#: cinder/volume/drivers/remotefs.py:298#: cinder/volume/drivers/remotefs.py:359#: cinder/volume/drivers/remotefs.py:416#: cinder/volume/drivers/remotefs.py:542#: cinder/volume/drivers/remotefs.py:595 #, python-format msgid ""Cloning volume %(src)s to volume %(dst)s"" msgstr """" #: cinder/volume/drivers/remotefs.py:600 msgid ""Volume status must be 'available'."" msgstr """" #: cinder/volume/drivers/remotefs.py:639 #, python-format msgid ""Deleting stale snapshot: %s"" msgstr """" #: cinder/volume/drivers/remotefs.py:663 msgid ""Volume status must be \""available\"" or \""in-use\""."" msgstr """" #: cinder/volume/drivers/remotefs.py:679 #, python-format msgid """" ""Snapshot record for %s is not present, allowing snapshot_delete to "" ""proceed."" msgstr """" #: cinder/volume/drivers/remotefs.py:705 #, python-format msgid ""No backing file found for %s, allowing snapshot to be deleted."" msgstr """" #: cinder/volume/drivers/remotefs.py:776 #, python-format msgid ""No file found with %s as backing file."" msgstr """" #: cinder/volume/drivers/remotefs.py:785 #, python-format msgid ""No snap found with %s as backing file."" msgstr """" #: cinder/volume/drivers/remotefs.py:814 #, python-format msgid ""Check condition failed: %s expected to be None."" msgstr """" #: cinder/volume/drivers/remotefs.py:829 msgid ""Snapshot status must be \""available\"" to clone."" msgstr """" #: cinder/volume/drivers/remotefs.py:958 #, python-format msgid ""Volume status must be \""available\"" or \""in-use\"" for snapshot. (is %s)"" msgstr """" #: cinder/volume/drivers/smbfs.py:131 msgid ""SMBFS config file not set (smbfs_shares_config)."" msgstr """" #: cinder/volume/drivers/smbfs.py:135 #, python-format msgid ""SMBFS config file at %(config)s doesn't exist."" msgstr """" #: cinder/volume/drivers/smbfs.py:140 #, python-format msgid ""Invalid mount point base: %s"" msgstr """" #: cinder/volume/drivers/smbfs.py:144 #, python-format msgid ""SMBFS config 'smbfs_oversub_ratio' invalid. Must be > 0: %s"" msgstr """" #: cinder/volume/drivers/smbfs.py:153 #, python-format msgid ""SMBFS config 'smbfs_used_ratio' invalid. Must be > 0 and <= 1.0: %s"" msgstr """" #: cinder/volume/drivers/smbfs.py:204 cinder/volume/drivers/ibm/ibmnas.py:287 #, python-format msgid ""Volume %s does not have provider_location specified, skipping."" msgstr """" #: cinder/volume/drivers/smbfs.py:226 msgid ""qemu-img is not installed."" msgstr """" #: cinder/volume/drivers/smbfs.py:238 msgid """" ""This version of qemu-img does not support vhdx images. Please upgrade to "" ""1.7 or greater."" msgstr """" #: cinder/volume/drivers/smbfs.py:258 #, python-format msgid ""File already exists at %s."" msgstr """" #: cinder/volume/drivers/smbfs.py:375 msgid ""This driver does not support snapshotting in-use volumes."" msgstr """" #: cinder/volume/drivers/smbfs.py:379 msgid ""This driver does not support deleting in-use snapshots."" msgstr """" #: cinder/volume/drivers/smbfs.py:391 #, python-format msgid ""Snapshots are not supported for this volume format: %s"" msgstr """" #: cinder/volume/drivers/smbfs.py:505 msgid """" ""Unsupported volume format: vhdx. qemu-img 1.7 or higher is required in "" ""order to properly support this format."" msgstr """" #: cinder/volume/drivers/smbfs.py:523 #, python-format msgid "" but size is now %d."" msgstr """" #: cinder/volume/drivers/netapp/iscsi.py:595#: cinder/volume/drivers/netapp/iscsi.py:601#: cinder/volume/drivers/netapp/iscsi.py:628#: cinder/volume/drivers/netapp/iscsi.py:672#: cinder/volume/drivers/netapp/iscsi.py:679#: cinder/volume/drivers/netapp/iscsi.py:685#: cinder/volume/drivers/netapp/iscsi.py:702#: cinder/volume/drivers/netapp/iscsi.py:713#: cinder/volume/drivers/netapp/iscsi.py:718#: cinder/volume/drivers/netapp/iscsi.py:722#: cinder/volume/drivers/netapp/iscsi.py:725#: cinder/volume/drivers/netapp/iscsi.py:727#: cinder/volume/drivers/netapp/iscsi.py:736#: cinder/volume/drivers/netapp/iscsi.py:783#: cinder/volume/drivers/netapp/iscsi.py:795#: cinder/volume/drivers/netapp/iscsi.py:1014#: cinder/volume/drivers/netapp/iscsi.py:1115#: cinder/volume/drivers/netapp/iscsi.py:1180#: cinder/volume/drivers/netapp/iscsi.py:1184#: cinder/volume/drivers/netapp/iscsi.py:1195#: cinder/volume/drivers/netapp/iscsi.py:1304#: cinder/volume/drivers/netapp/iscsi.py:1514#: cinder/volume/drivers/netapp/iscsi.py:1520#: cinder/volume/drivers/netapp/iscsi.py:1528#: cinder/volume/drivers/windows/remotefs.py:52 msgid ""Link path already exists and its not a symlink"" msgstr """" #: cinder/volume/drivers/windows/remotefs.py:80 #, python-format msgid ""Could not create symbolic link. Link: %(link)s Target %(target)s"" msgstr """" #: cinder/volume/drivers/windows/remotefs.py:115 #, python-format msgid ""Mounting share: %s"" msgstr """" #: cinder/volume/drivers/windows/remotefs.py:118 #, python-format msgid """" ""Unable to mount SMBFS share: %(smbfs_share)s WMI exception: "" ""%(wmi_exc)sOptions: %(options)s"" msgstr """" #: cinder/volume/drivers/windows/remotefs.py:137 #, python-format msgid ""Could not get share %s capacity info."" msgstr """" #: cinder/volume/drivers/windows/smbfs.py:66 #, python-format msgid """" ""This system platform (%s) is not supported. This driver supports only "" ""Win32 platforms."" msgstr """" #: cinder/volume/drivers/windows/smbfs.py:76 #, python-format msgid ""File already exists at: %s"" msgstr """" #: cinder/volume/drivers/windows/smbfs.py:81 #, python-format msgid ""Unsupported volume format: %s "" msgstr """" #: cinder/volume/drivers/windows/vhdutils.py:220#: cinder/volume/drivers/windows/vhdutils.py:231#: cinder/volume/drivers/windows/vhdutils.py:249#: cinder/volume/drivers/windows/vhdutils.py:271msgid ""Virtual disk merge failed with error: %s"" msgstr """" #: cinder/volume/drivers/windows/vhdutils.py:318 #, python-format msgid ""Virtual disk creation failed with error: %s"" msgstr """" #: cinder/volume/drivers/windows/vhdutils.py:423 #, python-format msgid ""Virtual disk reconnect failed with error: %s""","""POT-Creation-Date: 2014-09-08 06:08+0000\n""#: cinder/exception.py:723 cinder/exception.py:736 cinder/exception.py:749#: cinder/exception.py:745#: cinder/exception.py:754#: cinder/exception.py:758 #, python-format msgid ""VDisk doesn't exist (%(vdisk)s)"" msgstr """" #: cinder/exception.py:762 msgid ""Unable to connect to MSA array"" msgstr """" #: cinder/exception.py:766 #, python-format msgid ""Not enough space on VDisk (%(vdisk)s)"" msgstr """" msgid ""Fibre Channel connection control failure: %(reason)s""#, python-format msgid ""Fibre Channel Zone operation failed: %(reason)s""#: cinder/exception.py:783 cinder/exception.py:787#: cinder/exception.py:791#: cinder/exception.py:800#: cinder/exception.py:816#: cinder/exception.py:820#: cinder/exception.py:825#: cinder/exception.py:829#: cinder/exception.py:834#: cinder/exception.py:851#: cinder/exception.py:856#: cinder/brick/remotefs/remotefs.py:90#: cinder/brick/remotefs/remotefs.py:130#: cinder/consistencygroup/api.py:364 cinder/volume/manager.py:1808#: cinder/volume/api.py:1001 cinder/volume/manager.py:1448#: cinder/volume/manager.py:543 cinder/volume/manager.py:1828#: cinder/volume/manager.py:618 cinder/volume/manager.py:1928#: cinder/volume/manager.py:1016#: cinder/volume/manager.py:1019#: cinder/volume/manager.py:1042#: cinder/volume/manager.py:1071#: cinder/volume/manager.py:1083#: cinder/volume/manager.py:1104#: cinder/volume/manager.py:1190#: cinder/volume/manager.py:1198#: cinder/volume/manager.py:1234#: cinder/volume/manager.py:1302#: cinder/volume/manager.py:1304#: cinder/volume/manager.py:1306#: cinder/volume/manager.py:1311#: cinder/volume/manager.py:1385#: cinder/volume/manager.py:1386#: cinder/volume/manager.py:1417#: cinder/volume/manager.py:1420#: cinder/volume/manager.py:1431#: cinder/volume/manager.py:1439#: cinder/volume/manager.py:1487#: cinder/volume/manager.py:1489#: cinder/volume/manager.py:1519#: cinder/volume/manager.py:1528#: cinder/volume/manager.py:1538 cinder/volume/manager.py:1569#: cinder/volume/manager.py:1550#: cinder/volume/manager.py:1559#: cinder/volume/manager.py:1577#: cinder/volume/manager.py:1585#: cinder/volume/manager.py:1603#: cinder/volume/manager.py:1621#: cinder/volume/manager.py:1635#: cinder/volume/manager.py:1643#: cinder/volume/manager.py:1662#: cinder/volume/manager.py:1672#: cinder/volume/manager.py:1700#: cinder/volume/manager.py:1723#: cinder/volume/manager.py:1740#: cinder/volume/manager.py:1758#: cinder/volume/manager.py:1771#: cinder/volume/manager.py:1846#: cinder/volume/manager.py:1859#: cinder/volume/manager.py:1894#: cinder/volume/manager.py:1939#: cinder/volume/drivers/glusterfs.py:184 #, python-format msgid ""Cloning volume %(src)s to volume %(dst)s"" msgstr """" #: cinder/volume/drivers/glusterfs.py:189 msgid ""Volume status must be 'available'."" msgstr """" #: cinder/volume/drivers/glusterfs.py:225 cinder/volume/drivers/remotefs.py:115#: cinder/volume/drivers/glusterfs.py:239 msgid ""Snapshot status must be \""available\"" to clone."" msgstr """" #: cinder/volume/drivers/glusterfs.py:298 cinder/volume/drivers/remotefs.py:165#: cinder/volume/drivers/glusterfs.py:406 #, python-format msgid ""Volume status must be \""available\"" or \""in-use\"" for snapshot. (is %s)"" msgstr """" #: cinder/volume/drivers/glusterfs.py:438 msgid ""Call to Nova to create snapshot failed"" msgstr """" #: cinder/volume/drivers/glusterfs.py:460 msgid ""Nova returned \""error\"" status while creating snapshot."" msgstr """" #: cinder/volume/drivers/glusterfs.py:477 #, python-format msgid ""Timed out while waiting for Nova update for creation of snapshot %s."" msgstr """" #: cinder/volume/drivers/glusterfs.py:575 msgid ""Volume status must be \""available\"" or \""in-use\""."" msgstr """" #: cinder/volume/drivers/glusterfs.py:591 #, python-format msgid """" ""Snapshot record for %s is not present, allowing snapshot_delete to "" ""proceed."" msgstr """" #: cinder/volume/drivers/glusterfs.py:617 #, python-format msgid ""No backing file found for %s, allowing snapshot to be deleted."" msgstr """" #: cinder/volume/drivers/glusterfs.py:694 #, python-format msgid ""No file found with %s as backing file."" msgstr """" #: cinder/volume/drivers/glusterfs.py:704 #, python-format msgid ""No snap found with %s as backing file."" msgstr """" #: cinder/volume/drivers/glusterfs.py:741 #, python-format msgid ""Check condition failed: %s expected to be None."" msgstr """" #: cinder/volume/drivers/glusterfs.py:792#: cinder/volume/drivers/glusterfs.py:816#: cinder/volume/drivers/glusterfs.py:829#: cinder/volume/drivers/glusterfs.py:854 #, python-format msgid ""Deleting stale snapshot: %s"" msgstr """" #: cinder/volume/drivers/glusterfs.py:895#: cinder/volume/drivers/glusterfs.py:953#: cinder/volume/drivers/glusterfs.py:961#: cinder/volume/drivers/glusterfs.py:979#: cinder/volume/drivers/glusterfs.py:1005 #: cinder/volume/drivers/remotefs.py:152#: cinder/volume/drivers/glusterfs.py:1077#: cinder/volume/drivers/glusterfs.py:1092#: cinder/volume/drivers/glusterfs.py:1100#: cinder/volume/drivers/nfs.py:249 cinder/volume/drivers/netapp/nfs.py:612#: cinder/volume/drivers/nfs.py:257#: cinder/volume/drivers/remotefs.py:256#: cinder/volume/drivers/remotefs.py:257#: cinder/volume/drivers/remotefs.py:293#: cinder/volume/drivers/remotefs.py:354#: cinder/volume/drivers/remotefs.py:404#: cinder/volume/drivers/remotefs.py:525#: cinder/volume/drivers/ibm/ibmnas.py:287 #, python-format msgid ""Volume %s does not have provider_location specified, skipping."" msgstr """" #: cinder/volume/drivers/netapp/iscsi.py:594#: cinder/volume/drivers/netapp/iscsi.py:600#: cinder/volume/drivers/netapp/iscsi.py:627#: cinder/volume/drivers/netapp/iscsi.py:671#: cinder/volume/drivers/netapp/iscsi.py:678#: cinder/volume/drivers/netapp/iscsi.py:684#: cinder/volume/drivers/netapp/iscsi.py:701#: cinder/volume/drivers/netapp/iscsi.py:712#: cinder/volume/drivers/netapp/iscsi.py:717#: cinder/volume/drivers/netapp/iscsi.py:721#: cinder/volume/drivers/netapp/iscsi.py:724#: cinder/volume/drivers/netapp/iscsi.py:726#: cinder/volume/drivers/netapp/iscsi.py:735#: cinder/volume/drivers/netapp/iscsi.py:782#: cinder/volume/drivers/netapp/iscsi.py:794#: cinder/volume/drivers/netapp/iscsi.py:1013#: cinder/volume/drivers/netapp/iscsi.py:1114#: cinder/volume/drivers/netapp/iscsi.py:1176#: cinder/volume/drivers/netapp/iscsi.py:1180#: cinder/volume/drivers/netapp/iscsi.py:1191#: cinder/volume/drivers/netapp/iscsi.py:1300#: cinder/volume/drivers/netapp/iscsi.py:1510#: cinder/volume/drivers/netapp/iscsi.py:1516#: cinder/volume/drivers/netapp/iscsi.py:1524#: cinder/volume/drivers/windows/vhdutils.py:132#: cinder/volume/drivers/windows/vhdutils.py:143#: cinder/volume/drivers/windows/vhdutils.py:163#: cinder/volume/drivers/windows/vhdutils.py:206msgid ""Virtual disk conversion failed with error: %s""",669,403
openstack%2Ffuel-library~master~I6cc86a30de840108cff91d02bc9002794150f17f,openstack/fuel-library,master,I6cc86a30de840108cff91d02bc9002794150f17f,Disable fwaas in neutron server config file,MERGED,2014-09-25 10:12:26.000000000,2014-09-26 16:44:26.000000000,2014-09-26 16:44:26.000000000,"[{'_account_id': 3}, {'_account_id': 7468}, {'_account_id': 7745}, {'_account_id': 8787}, {'_account_id': 8971}]","[{'number': 1, 'created': '2014-09-25 10:12:26.000000000', 'files': ['deployment/puppet/neutron/lib/puppet/parser/functions/sanitize_neutron_config.rb'], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/a3011c83c61cb5ff2f423d46ee1e8c344c19f477', 'message': ""Disable fwaas in neutron server config file\n\nCurrently Firewall-as-a-Service isn't used but it is enabled.\nIt prevents neutron-l3-agent from starting.\n\nChange-Id: I6cc86a30de840108cff91d02bc9002794150f17f\nCloses-bug: #1372787\n""}]",0,123996,a3011c83c61cb5ff2f423d46ee1e8c344c19f477,12,5,1,7604,,,0,"Disable fwaas in neutron server config file

Currently Firewall-as-a-Service isn't used but it is enabled.
It prevents neutron-l3-agent from starting.

Change-Id: I6cc86a30de840108cff91d02bc9002794150f17f
Closes-bug: #1372787
",git fetch https://review.opendev.org/openstack/fuel-library refs/changes/96/123996/1 && git format-patch -1 --stdout FETCH_HEAD,['deployment/puppet/neutron/lib/puppet/parser/functions/sanitize_neutron_config.rb'],1,a3011c83c61cb5ff2f423d46ee1e8c344c19f477,bug/1372787," :service_plugins => 'neutron.services.l3_router.l3_router_plugin.L3RouterPlugin,neutron.services.metering.metering_plugin.MeteringPlugin',"," :service_plugins => 'neutron.services.l3_router.l3_router_plugin.L3RouterPlugin,neutron.services.firewall.fwaas_plugin.FirewallPlugin,neutron.services.metering.metering_plugin.MeteringPlugin',",1,1
openstack%2Ftripleo-ci~master~I7b6bf0c93c88888231d260036c765228e57b848f,openstack/tripleo-ci,master,I7b6bf0c93c88888231d260036c765228e57b848f,Update virsh ssh commands to support setting the locale,MERGED,2014-09-25 14:43:53.000000000,2014-09-26 16:37:11.000000000,2014-09-26 16:37:11.000000000,"[{'_account_id': 3}, {'_account_id': 1926}, {'_account_id': 4190}, {'_account_id': 5805}, {'_account_id': 7882}, {'_account_id': 9369}, {'_account_id': 12459}]","[{'number': 1, 'created': '2014-09-25 14:43:53.000000000', 'files': ['elements/testenv-worker/bin/ci_commands'], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/53b005500aa5e90f5803c311a930207c0ca8bdfa', 'message': ""Update virsh ssh commands to support setting the locale\n\nThe patch https://review.openstack.org/#/c/124061 in Ironic is changing\nthe SSH commands to force the commands to use their default language\n(whatever language the strings in the application was written in, for\nvirsh that's english) for output by passing LC_ALL=C as part of the\ncommand line. This is needed because as we do some pattern matchings\nwith the output of some commands and if the system is configured with\na non-english locale setting the matching is going to fail.\n\nThis patch makes the locale variable optional when checking the virsh\ncommands so it would work with the old and new commands.\n\nChange-Id: I7b6bf0c93c88888231d260036c765228e57b848f\n""}]",0,124072,53b005500aa5e90f5803c311a930207c0ca8bdfa,12,7,1,6773,,,0,"Update virsh ssh commands to support setting the locale

The patch https://review.openstack.org/#/c/124061 in Ironic is changing
the SSH commands to force the commands to use their default language
(whatever language the strings in the application was written in, for
virsh that's english) for output by passing LC_ALL=C as part of the
command line. This is needed because as we do some pattern matchings
with the output of some commands and if the system is configured with
a non-english locale setting the matching is going to fail.

This patch makes the locale variable optional when checking the virsh
commands so it would work with the old and new commands.

Change-Id: I7b6bf0c93c88888231d260036c765228e57b848f
",git fetch https://review.opendev.org/openstack/tripleo-ci refs/changes/72/124072/1 && git format-patch -1 --stdout FETCH_HEAD,['elements/testenv-worker/bin/ci_commands'],1,53b005500aa5e90f5803c311a930207c0ca8bdfa,locale," r""""""^(LC_ALL=C )?/usr/bin/virsh( --connect qemu:///system)? list --all \| tail -n \+2 \| awk -F"" "" '{print \$2}'$"""""": run_command, r""""""^(LC_ALL=C )?/usr/bin/virsh( --connect qemu:///system)? dumpxml [a-z0-9_]+ \| grep ""mac address"" \| awk -F""'"" '{print \$2}' \| tr -d ':'$"""""": run_command, r""""""^(LC_ALL=C )?/usr/bin/virsh( --connect qemu:///system)? dumpxml [a-z0-9_]+ \| awk -F ""'"" '/mac address/{print \$2}'\| tr -d ':'$"""""": run_command, r""""""^(LC_ALL=C )?/usr/bin/virsh( --connect qemu:///system)? list --all\|grep running( )?\|( )?awk -v qc='""' -F"" "" '{print qc\$2qc}'$"""""": run_command, r""""""^(LC_ALL=C )?/usr/bin/virsh( --connect qemu:///system)? (start|destroy|reset) ""?[a-z0-9_]+""?$"""""": run_command, r""""""^(LC_ALL=C )?/usr/bin/virsh( --connect qemu:///system)? dumpxml [a-z0-9_]+ \| awk '[\w\\\/<>\(\){},.|=*;"" ]+' Q=""'"" RS=""\[<>\]"" \| head -1$"""""": run_command, r""""""^EDITOR=""sed -i '[\w\\\/<>\(\)|=*;"" ]+'""( LC_ALL=C)? /usr/bin/virsh( --connect qemu:///system)? edit ""?[a-z0-9_]+""?$"""""": run_command,"," r""""""^/usr/bin/virsh( --connect qemu:///system)? list --all \| tail -n \+2 \| awk -F"" "" '{print \$2}'$"""""": run_command, r""""""^/usr/bin/virsh( --connect qemu:///system)? dumpxml [a-z0-9_]+ \| grep ""mac address"" \| awk -F""'"" '{print \$2}' \| tr -d ':'$"""""": run_command, r""""""^/usr/bin/virsh( --connect qemu:///system)? dumpxml [a-z0-9_]+ \| awk -F ""'"" '/mac address/{print \$2}'\| tr -d ':'$"""""": run_command, r""""""^/usr/bin/virsh( --connect qemu:///system)? list --all\|grep running( )?\|( )?awk -v qc='""' -F"" "" '{print qc\$2qc}'$"""""": run_command, r""""""^/usr/bin/virsh( --connect qemu:///system)? (start|destroy|reset) ""?[a-z0-9_]+""?$"""""": run_command, r""""""^/usr/bin/virsh( --connect qemu:///system)? dumpxml [a-z0-9_]+ \| awk '[\w\\\/<>\(\){},.|=*;"" ]+' Q=""'"" RS=""\[<>\]"" \| head -1$"""""": run_command, r""""""^EDITOR=""sed -i '[\w\\\/<>\(\)|=*;"" ]+'"" /usr/bin/virsh( --connect qemu:///system)? edit ""?[a-z0-9_]+""?$"""""": run_command,",7,7
openstack%2Fopenstack-manuals~master~I407451b757781cc4d9dfe019fe4464558c522609,openstack/openstack-manuals,master,I407451b757781cc4d9dfe019fe4464558c522609,Updating cinder policy.json file,MERGED,2014-09-26 15:11:32.000000000,2014-09-26 16:30:23.000000000,2014-09-26 16:30:23.000000000,"[{'_account_id': 3}, {'_account_id': 9162}, {'_account_id': 9515}]","[{'number': 1, 'created': '2014-09-26 15:11:32.000000000', 'files': ['doc/common/samples/policy.json'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/50b5b1790c7305111fe1664984af5e7598f380bd', 'message': 'Updating cinder policy.json file\n\nAdding policy.json file to the samples folder.\n\nChange-Id: I407451b757781cc4d9dfe019fe4464558c522609\n'}]",0,124439,50b5b1790c7305111fe1664984af5e7598f380bd,8,3,1,10607,,,0,"Updating cinder policy.json file

Adding policy.json file to the samples folder.

Change-Id: I407451b757781cc4d9dfe019fe4464558c522609
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/39/124439/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/common/samples/policy.json'],1,50b5b1790c7305111fe1664984af5e7598f380bd,asettle/policy,"{ ""context_is_admin"": ""role:admin"", ""admin_or_owner"": ""is_admin:True or project_id:%(project_id)s"", ""default"": ""rule:admin_or_owner"", ""admin_api"": ""is_admin:True"", ""volume:create"": """", ""volume:get_all"": """", ""volume:get_volume_metadata"": """", ""volume:get_volume_admin_metadata"": ""rule:admin_api"", ""volume:delete_volume_admin_metadata"": ""rule:admin_api"", ""volume:update_volume_admin_metadata"": ""rule:admin_api"", ""volume:get_snapshot"": """", ""volume:get_all_snapshots"": """", ""volume:extend"": """", ""volume:update_readonly_flag"": """", ""volume:retype"": """", ""volume_extension:types_manage"": ""rule:admin_api"", ""volume_extension:types_extra_specs"": ""rule:admin_api"", ""volume_extension:volume_type_encryption"": ""rule:admin_api"", ""volume_extension:volume_encryption_metadata"": ""rule:admin_or_owner"", ""volume_extension:extended_snapshot_attributes"": """", ""volume_extension:volume_image_metadata"": """", ""volume_extension:quotas:show"": """", ""volume_extension:quotas:update"": ""rule:admin_api"", ""volume_extension:quota_classes"": """", ""volume_extension:volume_admin_actions:reset_status"": ""rule:admin_api"", ""volume_extension:snapshot_admin_actions:reset_status"": ""rule:admin_api"", ""volume_extension:backup_admin_actions:reset_status"": ""rule:admin_api"", ""volume_extension:volume_admin_actions:force_delete"": ""rule:admin_api"", ""volume_extension:volume_admin_actions:force_detach"": ""rule:admin_api"", ""volume_extension:snapshot_admin_actions:force_delete"": ""rule:admin_api"", ""volume_extension:volume_admin_actions:migrate_volume"": ""rule:admin_api"", ""volume_extension:volume_admin_actions:migrate_volume_completion"": ""rule:admin_api"", ""volume_extension:volume_host_attribute"": ""rule:admin_api"", ""volume_extension:volume_tenant_attribute"": ""rule:admin_or_owner"", ""volume_extension:volume_mig_status_attribute"": ""rule:admin_api"", ""volume_extension:hosts"": ""rule:admin_api"", ""volume_extension:services"": ""rule:admin_api"", ""volume_extension:volume_manage"": ""rule:admin_api"", ""volume_extension:volume_unmanage"": ""rule:admin_api"", ""volume:services"": ""rule:admin_api"", ""volume:create_transfer"": """", ""volume:accept_transfer"": """", ""volume:delete_transfer"": """", ""volume:get_all_transfers"": """", ""volume_extension:replication:promote"": ""rule:admin_api"", ""volume_extension:replication:reenable"": ""rule:admin_api"", ""backup:create"" : """", ""backup:delete"": """", ""backup:get"": """", ""backup:get_all"": """", ""backup:restore"": """", ""backup:backup-import"": ""rule:admin_api"", ""backup:backup-export"": ""rule:admin_api"", ""snapshot_extension:snapshot_actions:update_snapshot_status"": """", ""consistencygroup:create"" : ""group:nobody"", ""consistencygroup:delete"": ""group:nobody"", ""consistencygroup:get"": ""group:nobody"", ""consistencygroup:get_all"": ""group:nobody"", ""consistencygroup:create_cgsnapshot"" : """", ""consistencygroup:delete_cgsnapshot"": """", ""consistencygroup:get_cgsnapshot"": """", ""consistencygroup:get_all_cgsnapshots"": """", ""scheduler_extension:scheduler_stats:get_pools"" : ""rule:admin_api"" } ",,80,0
openstack%2Fcongress~master~I8b77576a72e953971bd99bd7cb0a947bf5dc5aa9,openstack/congress,master,I8b77576a72e953971bd99bd7cb0a947bf5dc5aa9,Add a Congress driver for Keystone,MERGED,2014-09-18 00:02:21.000000000,2014-09-26 16:18:37.000000000,2014-09-26 16:18:36.000000000,"[{'_account_id': 3}, {'_account_id': 1923}, {'_account_id': 4395}, {'_account_id': 8215}, {'_account_id': 9008}, {'_account_id': 9253}, {'_account_id': 12875}]","[{'number': 1, 'created': '2014-09-18 00:02:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/congress/commit/823cd892ba5c0570c538486cbbd7b13e8c28402d', 'message': 'Add a Congress driver for Keystone\n\nIncludes a test.\n\nChange-Id: I8b77576a72e953971bd99bd7cb0a947bf5dc5aa9\n'}, {'number': 2, 'created': '2014-09-18 00:08:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/congress/commit/2c92a55b1e2d16b3183e5de900f36bd090b89165', 'message': 'Add a Congress driver for Keystone\n\nIncludes a test.\n\nChange-Id: I8b77576a72e953971bd99bd7cb0a947bf5dc5aa9\n'}, {'number': 3, 'created': '2014-09-22 20:37:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/congress/commit/5b478e85a10e7c0a1fad56fc90be65db1076deda', 'message': ""Add a Congress driver for Keystone\n\nThis driver uses three keystone API calls to populate Congress tables users,\nroles, and tenants.  These are the columns for these three tables:\n\ncolumns ('username', 'name', enabled', 'tenantId', 'id', 'email').\nroles ('id', 'name')\ntenants ('enabled', 'description', 'name', 'id')\n\nThe keystone driver includes a test.\n\nChange-Id: I8b77576a72e953971bd99bd7cb0a947bf5dc5aa9\n""}, {'number': 4, 'created': '2014-09-23 18:10:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/congress/commit/b82148edad9f092b6789d6838b9c5cac34c644ba', 'message': ""Add a Congress driver for Keystone\n\nThis driver uses three keystone API calls to populate Congress tables users,\nroles, and tenants.  These are the columns for these three tables:\n\ncolumns ('username', 'name', enabled', 'tenantId', 'id', 'email').\nroles ('id', 'name')\ntenants ('enabled', 'description', 'name', 'id')\n\nThe keystone driver includes a test.\n\nblueprint: refactor-drivers\n\nChange-Id: I8b77576a72e953971bd99bd7cb0a947bf5dc5aa9\n""}, {'number': 5, 'created': '2014-09-23 20:34:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/congress/commit/4c75b856a66cb7c1485b25e079d3152be8b22dc2', 'message': ""Add a Congress driver for Keystone\n\nThis driver uses three keystone API calls to populate Congress tables users,\nroles, and tenants.  These are the columns for these three tables:\n\ncolumns ('username', 'name', enabled', 'tenantId', 'id', 'email').\nroles ('id', 'name')\ntenants ('enabled', 'description', 'name', 'id')\n\nThe keystone driver includes a test.\n\nblueprint: refactor-drivers\n\nChange-Id: I8b77576a72e953971bd99bd7cb0a947bf5dc5aa9\n""}, {'number': 6, 'created': '2014-09-24 16:58:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/congress/commit/98fead8c39dd96f0e11510901bb54694c53d4f31', 'message': ""Add a Congress driver for Keystone\n\nThis driver uses three keystone API calls to populate Congress tables users,\nroles, and tenants.  These are the columns for these three tables:\n\ncolumns ('username', 'name', enabled', 'tenantId', 'id', 'email').\nroles ('id', 'name')\ntenants ('enabled', 'description', 'name', 'id')\n\nThe keystone driver includes a test.\n\nblueprint: refactor-drivers\n\nChange-Id: I8b77576a72e953971bd99bd7cb0a947bf5dc5aa9\n""}, {'number': 7, 'created': '2014-09-25 17:28:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/congress/commit/3818a2023f7d91eef9849f14b595b34c79f7b191', 'message': ""Add a Congress driver for Keystone\n\nThis driver uses three keystone API calls to populate Congress tables users,\nroles, and tenants.  These are the columns for these three tables:\n\ncolumns ('username', 'name', enabled', 'tenantId', 'id', 'email').\nroles ('id', 'name')\ntenants ('enabled', 'description', 'name', 'id')\n\nThe keystone driver includes a test.\n\nblueprint: refactor-drivers\n\nChange-Id: I8b77576a72e953971bd99bd7cb0a947bf5dc5aa9\n""}, {'number': 8, 'created': '2014-09-25 22:09:55.000000000', 'files': ['congress/datasources/keystone_driver.py', 'requirements.txt', 'congress/datasources/tests/unit/util.py', 'etc/datasources.conf.sample', 'congress/datasources/tests/unit/test_keystone_driver.py'], 'web_link': 'https://opendev.org/openstack/congress/commit/4c5c3d71b83041709c9087d9daba417fba8bfbac', 'message': ""Add a Congress driver for Keystone\n\nThis driver uses three keystone API calls to populate Congress tables users,\nroles, and tenants.  These are the columns for these three tables:\n\ncolumns ('username', 'name', enabled', 'tenantId', 'id', 'email').\nroles ('id', 'name')\ntenants ('enabled', 'description', 'name', 'id')\n\nThe keystone driver includes a test.\n\nblueprint: refactor-drivers\n\nChange-Id: I8b77576a72e953971bd99bd7cb0a947bf5dc5aa9\n""}]",32,122271,4c5c3d71b83041709c9087d9daba417fba8bfbac,41,7,8,12875,,,0,"Add a Congress driver for Keystone

This driver uses three keystone API calls to populate Congress tables users,
roles, and tenants.  These are the columns for these three tables:

columns ('username', 'name', enabled', 'tenantId', 'id', 'email').
roles ('id', 'name')
tenants ('enabled', 'description', 'name', 'id')

The keystone driver includes a test.

blueprint: refactor-drivers

Change-Id: I8b77576a72e953971bd99bd7cb0a947bf5dc5aa9
",git fetch https://review.opendev.org/openstack/congress refs/changes/71/122271/8 && git format-patch -1 --stdout FETCH_HEAD,"['congress/datasources/keystone_driver.py', 'etc/datasources.conf.sample', 'congress/datasources/tests/unit/test_keystone_driver.py']",3,823cd892ba5c0570c538486cbbd7b13e8c28402d,bp/refactor-drivers,"#!/usr/bin/env python # Copyright (c) 2014 VMware, Inc. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # import mock import mox from congress.datasources.datasource_driver import DataSourceConfigException from congress.datasources.keystone_driver import KeystoneDriver from congress.dse import d6cage from congress.policy import compile from congress.tests import base from congress.tests import helper import keystoneclient.v2_0.client class TestKeystoneDriver(base.TestCase): def setUp(self): super(TestKeystoneDriver, self).setUp() self.keystone_client = mock.MagicMock() self.keystone_client.list_users.return_value = users_data #self.keystone_client.list_roles.return_value = roles_data #self.keystone_client.list_tenants.return_value = tenants_data args = helper.datasource_openstack_args() args['poll_time'] = 0 args['client'] = self.keystone_client self.driver = KeystoneDriver(args=args) def test_list_users(self): """"""Test conversion of complex user objects to tables."""""" user_list = self.driver._get_tuple_list(users_data, KeystoneDriver.USERS) self.assertIsNotNone(user_list) self.assertEquals(2, len(user_list)) # Check an individual user entry self.assertEquals(('alice', 'alice foo', True, '019b18a15f2a44c1880d57704b2c4009', '00f2c34a156c40058004ee8eb3320e04', 'alice@foo.com'), user_list[0]) self.assertEquals(('bob', 'bob bar', False, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', 'bob@bar.edu'), user_list[1]) def test_list_roles(self): """"""Test conversion of complex role objects to tables."""""" roles_list = self.driver._get_tuple_list(roles_data, KeystoneDriver.ROLES) self.assertIsNotNone(roles_list) self.assertEquals(2, len(roles_list)) # Check an individual role entry self.assertEquals(('cccccccccccccccccccccccccccccccc', 'admin'), roles_list[0]) self.assertEquals(('dddddddddddddddddddddddddddddddd', 'viewer'), roles_list[1]) def test_list_tenants(self): """"""Test conversion of complex tenant objects to tables."""""" tenants_list = self.driver._get_tuple_list(tenants_data, KeystoneDriver.TENANTS) self.assertIsNotNone(tenants_list) self.assertEquals(2, len(tenants_list)) # Check an individual role entry self.assertEquals((True, 'accounting team', 'accounting', '00000000000000000000000000000001'), tenants_list[0]) self.assertEquals((False, 'eng team', 'eng', '00000000000000000000000000000002'), tenants_list[1]) class ResponseObj(object): """"""Allows callers to use dot notation to access a dictionary."""""" def __init__(self, values): self.values = values def __getattr__(self, name): return self.values[name] users_data = [ ResponseObj({'username': 'alice', 'name': 'alice foo', 'enabled': True, 'tenantId': '019b18a15f2a44c1880d57704b2c4009', 'id': '00f2c34a156c40058004ee8eb3320e04', 'email': 'alice@foo.com'}), ResponseObj({'username': 'bob', 'name': 'bob bar', 'enabled': False, 'tenantId': 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'id': 'bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', 'email': 'bob@bar.edu'}), ] roles_data = [ ResponseObj({'id': 'cccccccccccccccccccccccccccccccc', 'name': 'admin'}), ResponseObj({'id': 'dddddddddddddddddddddddddddddddd', 'name': 'viewer'}), ] tenants_data = [ ResponseObj({'enabled': True, 'description': 'accounting team', 'name': 'accounting', 'id': '00000000000000000000000000000001'}), ResponseObj({'enabled': False, 'description': 'eng team', 'name': 'eng', 'id': '00000000000000000000000000000002'}), ] ",,295,0
openstack%2Ffuel-devops~master~Icd5be3d3992a92386973b5a94c591797fc8b7d04,openstack/fuel-devops,master,Icd5be3d3992a92386973b5a94c591797fc8b7d04,Temporary fix for Django version,MERGED,2014-09-24 18:26:37.000000000,2014-09-26 16:10:18.000000000,2014-09-26 16:10:18.000000000,"[{'_account_id': 3}, {'_account_id': 7468}, {'_account_id': 8882}, {'_account_id': 8971}, {'_account_id': 9977}, {'_account_id': 10136}, {'_account_id': 11110}]","[{'number': 1, 'created': '2014-09-24 18:26:37.000000000', 'files': ['setup.py'], 'web_link': 'https://opendev.org/openstack/fuel-devops/commit/57a579465ccf1f9e88fee727a525366f115c1d60', 'message': 'Temporary fix for Django version\n\nChange-Id: Icd5be3d3992a92386973b5a94c591797fc8b7d04\nRelated-Bug: #1373381\n'}]",0,123810,57a579465ccf1f9e88fee727a525366f115c1d60,10,7,1,8965,,,0,"Temporary fix for Django version

Change-Id: Icd5be3d3992a92386973b5a94c591797fc8b7d04
Related-Bug: #1373381
",git fetch https://review.opendev.org/openstack/fuel-devops refs/changes/10/123810/1 && git format-patch -1 --stdout FETCH_HEAD,['setup.py'],1,57a579465ccf1f9e88fee727a525366f115c1d60,bug/1373381," 'django<1.7',"," 'django>=1.4.3',",1,1
openstack%2Fmagnetodb~master~Id3c4965537b6d958db5cad3fee72d2db1af8cdab,openstack/magnetodb,master,Id3c4965537b6d958db5cad3fee72d2db1af8cdab,Added documentation for installation magnetodb behind a proxy,MERGED,2014-09-12 14:39:07.000000000,2014-09-26 16:01:35.000000000,2014-09-26 16:01:34.000000000,"[{'_account_id': 3}, {'_account_id': 5538}, {'_account_id': 8188}, {'_account_id': 8491}, {'_account_id': 8601}, {'_account_id': 10829}, {'_account_id': 11006}, {'_account_id': 11428}]","[{'number': 1, 'created': '2014-09-12 14:39:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/8c68ca0d0efd2edbb2931f13ad57dbe44d25184b', 'message': 'Added documentation for installation devstack and magnetodb behind a proxy\n\nChange-Id: Id3c4965537b6d958db5cad3fee72d2db1af8cdab\nRelated-Bug: #1366788\n'}, {'number': 2, 'created': '2014-09-12 14:41:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/2104521edb14d91b4e0d32fe77f56a702415a83d', 'message': 'Added documentation for installation devstack and magnetodb behind a proxy\n\nRelated-Bug: #1366788\nChange-Id: Id3c4965537b6d958db5cad3fee72d2db1af8cdab\n'}, {'number': 3, 'created': '2014-09-12 15:15:23.000000000', 'files': ['contrib/devstack/README.rst', 'doc/source/developer_guide.rst'], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/8c4b3d10ea4a6a5a9a9b5fe5de1fd54beef782ba', 'message': 'Added documentation for installation magnetodb behind a proxy\n\nRelated-Bug: #1366788\nChange-Id: Id3c4965537b6d958db5cad3fee72d2db1af8cdab\n'}]",12,121136,8c4b3d10ea4a6a5a9a9b5fe5de1fd54beef782ba,15,8,3,10829,,,0,"Added documentation for installation magnetodb behind a proxy

Related-Bug: #1366788
Change-Id: Id3c4965537b6d958db5cad3fee72d2db1af8cdab
",git fetch https://review.opendev.org/openstack/magnetodb refs/changes/36/121136/2 && git format-patch -1 --stdout FETCH_HEAD,"['contrib/devstack/README.rst', 'doc/source/developer_guide.rst']",2,8c68ca0d0efd2edbb2931f13ad57dbe44d25184b,master,Vagrant script for DevStack here_. You can also use this documentation_... _documentation: https://github.com/stackforge/magnetodb/blob/master/contrib/devstack/README.rst ,Vagrant script for DevStack here_.,44,1
openstack%2Ftempest~master~I63321e8b9f277be040d0f75d5ed9f4461da0defc,openstack/tempest,master,I63321e8b9f277be040d0f75d5ed9f4461da0defc,Updates Tempest to match Zaqar's API name change,MERGED,2014-09-22 21:44:15.000000000,2014-09-26 15:59:26.000000000,2014-09-26 15:59:25.000000000,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 5196}, {'_account_id': 5292}, {'_account_id': 5689}, {'_account_id': 5803}, {'_account_id': 6159}, {'_account_id': 6167}, {'_account_id': 6413}, {'_account_id': 6427}, {'_account_id': 7488}, {'_account_id': 7498}, {'_account_id': 7872}]","[{'number': 1, 'created': '2014-09-22 21:44:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ac0c9660dbdd5ccf56d0d28b735afd8d813d4ec3', 'message': ""Updates Tempest to match Zaqar's API name change\n\nZaqar is a messaging service which provides support for different\nmessaging patterns and messaging related semantics.\nThis fix changes the old API name -queuing- to a more accurate name, -messaging-.\n\nChange-Id: I63321e8b9f277be040d0f75d5ed9f4461da0defc\n""}, {'number': 2, 'created': '2014-09-22 21:45:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5986c0d8acea4b23215e989e0f137dfd99fd675c', 'message': ""Updates Tempest to match Zaqar's API name change\n\nZaqar is a messaging service which provides support for different\nmessaging patterns and messaging related semantics.\nThis fix changes the old API name -queuing- to a more accurate name, -messaging-.\n\nChange-Id: I63321e8b9f277be040d0f75d5ed9f4461da0defc\n""}, {'number': 3, 'created': '2014-09-23 02:28:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/20f0312e0c6ead952587e4ab99dc044d172a1dc1', 'message': ""Updates Tempest to match Zaqar's API name change\n\nZaqar is a messaging service which provides support for different\nmessaging patterns and messaging related semantics.\nThis fix changes the old API name -queuing- to a more accurate name, -messaging-.\n\nChange-Id: I63321e8b9f277be040d0f75d5ed9f4461da0defc\n""}, {'number': 4, 'created': '2014-09-23 03:05:28.000000000', 'files': ['tempest/api_schema/response/messaging/v1/__init__.py', 'tempest/api_schema/response/messaging/v1/queues.py', 'tempest/cmd/verify_tempest_config.py', 'etc/tempest.conf.sample', 'tempest/services/messaging/__init__.py', 'tempest/services/messaging/json/messaging_client.py', 'tempest/services/messaging/json/__init__.py', 'tempest/api/messaging/test_messages.py', 'tempest/api/messaging/test_queues.py', 'tempest/api/messaging/test_claims.py', 'tempest/clients.py', 'tempest/api_schema/response/messaging/__init__.py', 'tempest/config.py', 'tempest/api/messaging/base.py', 'tempest/api/messaging/__init__.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/1173b6e1ca36ff30b7ba8f5f64e1e067d91b74fb', 'message': ""Updates Tempest to match Zaqar's API name change\n\nZaqar is a messaging service which provides support for different\nmessaging patterns and messaging related semantics.\nThis fix changes the old API name -queuing- to a more accurate name, -messaging-.\n\nChange-Id: I63321e8b9f277be040d0f75d5ed9f4461da0defc\n""}]",0,123252,1173b6e1ca36ff30b7ba8f5f64e1e067d91b74fb,43,15,4,6413,,,0,"Updates Tempest to match Zaqar's API name change

Zaqar is a messaging service which provides support for different
messaging patterns and messaging related semantics.
This fix changes the old API name -queuing- to a more accurate name, -messaging-.

Change-Id: I63321e8b9f277be040d0f75d5ed9f4461da0defc
",git fetch https://review.opendev.org/openstack/tempest refs/changes/52/123252/4 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/services/queuing/json/queuing_client.py', 'tempest/api/queuing/test_claims.py', 'tempest/api/queuing/test_queues.py', 'tempest/cmd/verify_tempest_config.py', 'etc/tempest.conf.sample', 'tempest/api/queuing/__init__.py', 'tempest/api/queuing/test_messages.py', 'tempest/clients.py', 'tempest/config.py', 'tempest/api/queuing/base.py']",10,ac0c9660dbdd5ccf56d0d28b735afd8d813d4ec3,api-rename,,"# Copyright (c) 2014 Rackspace, Inc. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. from tempest.common.utils import data_utils from tempest import config from tempest.openstack.common import log as logging from tempest import test CONF = config.CONF LOG = logging.getLogger(__name__) class BaseQueuingTest(test.BaseTestCase): """""" Base class for the Queuing tests that use the Tempest Zaqar REST client It is assumed that the following option is defined in the [service_available] section of etc/tempest.conf queuing as True """""" @classmethod def setUpClass(cls): super(BaseQueuingTest, cls).setUpClass() if not CONF.service_available.zaqar: raise cls.skipException(""Zaqar support is required"") os = cls.get_client_manager() cls.queuing_cfg = CONF.queuing cls.client = os.queuing_client @classmethod def create_queue(cls, queue_name): """"""Wrapper utility that returns a test queue."""""" resp, body = cls.client.create_queue(queue_name) return resp, body @classmethod def delete_queue(cls, queue_name): """"""Wrapper utility that deletes a test queue."""""" resp, body = cls.client.delete_queue(queue_name) return resp, body @classmethod def check_queue_exists(cls, queue_name): """"""Wrapper utility that checks the existence of a test queue."""""" resp, body = cls.client.get_queue(queue_name) return resp, body @classmethod def check_queue_exists_head(cls, queue_name): """"""Wrapper utility checks the head of a queue via http HEAD."""""" resp, body = cls.client.head_queue(queue_name) return resp, body @classmethod def list_queues(cls): """"""Wrapper utility that lists queues."""""" resp, body = cls.client.list_queues() return resp, body @classmethod def get_queue_stats(cls, queue_name): """"""Wrapper utility that returns the queue stats."""""" resp, body = cls.client.get_queue_stats(queue_name) return resp, body @classmethod def get_queue_metadata(cls, queue_name): """"""Wrapper utility that gets a queue metadata."""""" resp, body = cls.client.get_queue_metadata(queue_name) return resp, body @classmethod def set_queue_metadata(cls, queue_name, rbody): """"""Wrapper utility that sets the metadata of a queue."""""" resp, body = cls.client.set_queue_metadata(queue_name, rbody) return resp, body @classmethod def post_messages(cls, queue_name, rbody): '''Wrapper utility that posts messages to a queue.''' resp, body = cls.client.post_messages(queue_name, rbody) return resp, body @classmethod def list_messages(cls, queue_name): '''Wrapper utility that lists the messages in a queue.''' resp, body = cls.client.list_messages(queue_name) return resp, body @classmethod def get_single_message(cls, message_uri): '''Wrapper utility that gets a single message.''' resp, body = cls.client.get_single_message(message_uri) return resp, body @classmethod def get_multiple_messages(cls, message_uri): '''Wrapper utility that gets multiple messages.''' resp, body = cls.client.get_multiple_messages(message_uri) return resp, body @classmethod def delete_messages(cls, message_uri): '''Wrapper utility that deletes messages.''' resp, body = cls.client.delete_messages(message_uri) return resp, body @classmethod def post_claims(cls, queue_name, rbody, url_params=False): '''Wrapper utility that claims messages.''' resp, body = cls.client.post_claims( queue_name, rbody, url_params=False) return resp, body @classmethod def query_claim(cls, claim_uri): '''Wrapper utility that gets a claim.''' resp, body = cls.client.query_claim(claim_uri) return resp, body @classmethod def update_claim(cls, claim_uri, rbody): '''Wrapper utility that updates a claim.''' resp, body = cls.client.update_claim(claim_uri, rbody) return resp, body @classmethod def release_claim(cls, claim_uri): '''Wrapper utility that deletes a claim.''' resp, body = cls.client.release_claim(claim_uri) return resp, body @classmethod def generate_message_body(cls, repeat=1): '''Wrapper utility that sets the metadata of a queue.''' message_ttl = data_utils.rand_int_id(start=60, end=CONF.queuing.max_message_ttl) key = data_utils.arbitrary_string(size=20, base_text='QueuingKey') value = data_utils.arbitrary_string(size=20, base_text='QueuingValue') message_body = {key: value} rbody = ([{'body': message_body, 'ttl': message_ttl}] * repeat) return rbody ",17,562
openstack%2Fneutron~stable%2Ficehouse~I6537bb1da5ef0d6899bc71e4e949f2c760c103c2,openstack/neutron,stable/icehouse,I6537bb1da5ef0d6899bc71e4e949f2c760c103c2,Forbid regular users to reset admin-only attrs to default values,MERGED,2014-09-24 20:54:50.000000000,2014-09-26 15:59:12.000000000,2014-09-26 15:59:11.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 5170}, {'_account_id': 6854}, {'_account_id': 7293}, {'_account_id': 9311}, {'_account_id': 9656}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9846}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12040}, {'_account_id': 13308}]","[{'number': 1, 'created': '2014-09-24 20:54:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/50bf9bd162f4769413ab624c0fdbb427f50b0649', 'message': 'Forbid regular users to reset admin-only attrs to default values\n\nA regular user can reset an admin-only attribute to its default\nvalue due to the fact that a corresponding policy rule is\nenforced only in the case when an attribute is present in the\ntarget AND has a non-default value.\n\nAdded a new attribute ""attributes_to_update"" which contains a list\nof all to-be updated attributes to the body of the target that is\npassed to policy.enforce.\n\nChanged a check for whether an attribute is explicitly set.\nNow, in the case of update, the function should not pay attention\nto a default value of an attribute, but check whether it was\nexplicitly marked as being updated.\n\nAdded unit-tests.\n\nCloses-Bug: #1357379\nRelated-Bug: #1338880\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n(Cherry-picked from https://review.openstack.org/114531)\n'}, {'number': 2, 'created': '2014-09-25 15:15:03.000000000', 'files': ['neutron/common/constants.py', 'neutron/api/v2/base.py', 'neutron/tests/unit/test_policy.py', 'neutron/tests/unit/test_api_v2.py', 'neutron/policy.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/dd4b77ff53479389a5af8b45fd95ee2987562f29', 'message': 'Forbid regular users to reset admin-only attrs to default values\n\nA regular user can reset an admin-only attribute to its default\nvalue due to the fact that a corresponding policy rule is\nenforced only in the case when an attribute is present in the\ntarget AND has a non-default value.\n\nAdded a new attribute ""attributes_to_update"" which contains a list\nof all to-be updated attributes to the body of the target that is\npassed to policy.enforce.\n\nChanged a check for whether an attribute is explicitly set.\nNow, in the case of update, the function should not pay attention\nto a default value of an attribute, but check whether it was\nexplicitly marked as being updated.\n\nAdded unit-tests.\n\nConflicts:\n\tneutron/common/constants.py\n\nCloses-Bug: #1357379\nRelated-Bug: #1338880\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n(cherry picked from commit 74d10939903984d5f06c1749a8707fa3257e44ff)\n'}]",1,123849,dd4b77ff53479389a5af8b45fd95ee2987562f29,47,20,2,9311,,,0,"Forbid regular users to reset admin-only attrs to default values

A regular user can reset an admin-only attribute to its default
value due to the fact that a corresponding policy rule is
enforced only in the case when an attribute is present in the
target AND has a non-default value.

Added a new attribute ""attributes_to_update"" which contains a list
of all to-be updated attributes to the body of the target that is
passed to policy.enforce.

Changed a check for whether an attribute is explicitly set.
Now, in the case of update, the function should not pay attention
to a default value of an attribute, but check whether it was
explicitly marked as being updated.

Added unit-tests.

Conflicts:
	neutron/common/constants.py

Closes-Bug: #1357379
Related-Bug: #1338880
Change-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2
(cherry picked from commit 74d10939903984d5f06c1749a8707fa3257e44ff)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/49/123849/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/common/constants.py', 'neutron/api/v2/base.py', 'neutron/tests/unit/test_policy.py', 'neutron/tests/unit/test_api_v2.py', 'neutron/policy.py']",5,50bf9bd162f4769413ab624c0fdbb427f50b0649,bug/1357379," import collectionsfrom neutron.common import constants as constdef _is_attribute_explicitly_set(attribute_name, resource, target, action): """"""Verify that an attribute is present and is explicitly set."""""" if 'update' in action: # In the case of update, the function should not pay attention to a # default value of an attribute, but check whether it was explicitly # marked as being updated instead. return (attribute_name in target[const.ATTRIBUTES_TO_UPDATE] and target[attribute_name] is not attributes.ATTR_NOT_SPECIFIED)def _should_validate_sub_attributes(attribute, sub_attr): """"""Verify that sub-attributes are iterable and should be validated."""""" validate = attribute.get('validate') return (validate and isinstance(sub_attr, collections.Iterable) and any([k.startswith('type:dict') and v for (k, v) in validate.iteritems()])) target, action): # Build match entries for sub-attributes if _should_validate_sub_attributes( attribute, target[attribute_name]):","def _is_attribute_explicitly_set(attribute_name, resource, target): """"""Verify that an attribute is present and has a non-default value."""""" target): # Build match entries for sub-attributes, if present validate = attribute.get('validate') if (validate and any([k.startswith('type:dict') and v for (k, v) in validate.iteritems()])):",58,13
openstack%2Fkeystone~stable%2Ficehouse~I59350ae2e7bc421fa736b8419f39c740af2ce80c,openstack/keystone,stable/icehouse,I59350ae2e7bc421fa736b8419f39c740af2ce80c,Updated from global requirements,MERGED,2014-09-13 13:17:26.000000000,2014-09-26 15:58:41.000000000,2014-09-26 15:58:40.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 9656}]","[{'number': 1, 'created': '2014-09-13 13:17:26.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/keystone/commit/18a10e9b6c4fee250e56838e7ec4dc4c8624c1b0', 'message': 'Updated from global requirements\n\nChange-Id: I59350ae2e7bc421fa736b8419f39c740af2ce80c\n'}]",0,121349,18a10e9b6c4fee250e56838e7ec4dc4c8624c1b0,9,3,1,11131,,,0,"Updated from global requirements

Change-Id: I59350ae2e7bc421fa736b8419f39c740af2ce80c
",git fetch https://review.opendev.org/openstack/keystone refs/changes/49/121349/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,18a10e9b6c4fee250e56838e7ec4dc4c8624c1b0,openstack/requirements,"sqlalchemy-migrate>=0.8.2,!=0.8.4,!=0.9.2","sqlalchemy-migrate>=0.8.2,!=0.8.4",1,1
openstack%2Fdevstack~master~I36fe56c063ca921131ad98439bd452cb135916ac,openstack/devstack,master,I36fe56c063ca921131ad98439bd452cb135916ac,Configure endpoints to use SSL natively or via proxy,MERGED,2014-06-09 18:51:07.000000000,2014-09-26 15:58:20.000000000,2014-09-26 15:58:20.000000000,"[{'_account_id': 1}, {'_account_id': 3}, {'_account_id': 866}, {'_account_id': 970}, {'_account_id': 1313}, {'_account_id': 1653}, {'_account_id': 2750}, {'_account_id': 5196}, {'_account_id': 6486}, {'_account_id': 7118}, {'_account_id': 7175}, {'_account_id': 7191}, {'_account_id': 7662}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9009}, {'_account_id': 9098}, {'_account_id': 10385}, {'_account_id': 12754}]","[{'number': 1, 'created': '2014-06-09 18:51:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/e7ddbeb679e840038baab9f91d31f7a2cd7c2bf3', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder and glance to use SSL on the endpoints using\neither SSL natively or via a TLS proxy using stud.\n\nThis adds some limited support for neutron using native SSL.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nSSL_ENABLED_SERVICES=""<services>""\n\nWhere services is one or more of key, nova, cinder and glance.\n\nNative SSL requires the user to provide their own SSL certificate,\nprivate key and CA for the service. The same set can be used for\nmultiple services. Each service requires these lines in local.conf:\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 2, 'created': '2014-06-10 14:22:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/8f61acc364beb23ac8423b79bc80026b9217bcbd', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder and glance to use SSL on the endpoints using\neither SSL natively or via a TLS proxy using stud.\n\nThis adds some limited support for neutron using native SSL.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nSSL_ENABLED_SERVICES=""<services>""\n\nWhere services is one or more of key, nova, cinder and glance.\n\nNative SSL requires the user to provide their own SSL certificate,\nprivate key and CA for the service. The same set can be used for\nmultiple services. Each service requires these lines in local.conf:\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 3, 'created': '2014-06-19 14:57:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/1cabcba73d4c95a3cb309354641122b541a49c63', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder and glance to use SSL on the endpoints using\neither SSL natively or via a TLS proxy using stud.\n\nThis adds some limited support for neutron using native SSL.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nThis will by default enable SSL for keystone, nova, cinder and\nglance.\n\nWith native SSL you can either provide your own SSL certificate,\nprivate key and CA for the service or have devstack generate them\nfor you (the default).\n\nTo provide your own, set these in local.conf for each service:\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nFor any method of enabling SSL you should also set SERVICE_HOST to the\nFQDN of the host. This value defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 4, 'created': '2014-07-29 20:26:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/d7319ca9e07d9fc91a070f29e7a6743dfd6c00bb', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 5, 'created': '2014-08-04 15:48:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/dbe060f1185063b94d1aa875aad14dde08e2cea7', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 6, 'created': '2014-08-05 18:56:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/db212b9ab810b4ef58eaaf838a1ff8a55145229e', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 7, 'created': '2014-08-20 14:48:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/b3f456503491c2104436dffada9491f8099aaba2', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 8, 'created': '2014-08-22 19:20:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/328ffc5a6fad6edb643a34d982877663f00946ec', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 9, 'created': '2014-08-26 15:29:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/d894d8d3eabd16a106f6657e2bcaa92ba1239f26', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 10, 'created': '2014-09-02 18:19:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/d6c13bb58a40d57e713e13f322f23628ce2d0b6c', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 11, 'created': '2014-09-09 17:29:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/6404fb43b4d9f13fb2afb949b35509aded3c83bb', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 12, 'created': '2014-09-10 14:19:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/384d28a81a712ed0f22879e31ef5d2d348392643', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 13, 'created': '2014-09-13 22:43:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/fed11e69603023a2d54688307bd34512593f7977', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 14, 'created': '2014-09-22 14:59:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/6ad29d73080776bf2dd3005a539f4658a05a900a', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 15, 'created': '2014-09-23 16:34:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/65407ea82664680b27d068f3da349f496d197f14', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}, {'number': 16, 'created': '2014-09-25 14:14:34.000000000', 'files': ['lib/cinder', 'lib/neutron', 'functions', 'lib/nova', 'lib/tempest', 'lib/heat', 'lib/keystone', 'lib/swift', 'files/rpms/keystone', 'lib/glance', 'lib/tls', 'files/apache-keystone.template', 'stack.sh'], 'web_link': 'https://opendev.org/openstack/devstack/commit/18d4778cf7bffa60eb2e996a13c129c64f83575f', 'message': 'Configure endpoints to use SSL natively or via proxy\n\nConfigure nova, cinder, glance, swift and neutron to use SSL\non the endpoints using either SSL natively or via a TLS proxy\nusing stud.\n\nTo enable SSL via proxy, in local.conf add\n\nENABLED_SERVICES+=,tls-proxy\n\nThis will create a new test root CA, a subordinate CA and an SSL\nserver cert. It uses the value of hostname -f for the certificate\nsubject. The CA certicates are also added to the system CA bundle.\n\nTo enable SSL natively, in local.conf add:\n\nUSE_SSL=True\n\nNative SSL by default will also use the devstack-generate root and\nsubordinate CA.\n\nYou can override this on a per-service basis by setting\n\n<SERVICE>_SSL_CERT=/path/to/cert\n<SERVICE>_SSL_KEY=/path/to/key\n<SERVICE>_SSL_PATH=/path/to/ca\n\nYou should also set SERVICE_HOST to the FQDN of the host. This\nvalue defaults to the host IP address.\n\nChange-Id: I36fe56c063ca921131ad98439bd452cb135916ac\nCloses-Bug: 1328226\n'}]",22,98854,18d4778cf7bffa60eb2e996a13c129c64f83575f,135,19,16,7662,,,0,"Configure endpoints to use SSL natively or via proxy

Configure nova, cinder, glance, swift and neutron to use SSL
on the endpoints using either SSL natively or via a TLS proxy
using stud.

To enable SSL via proxy, in local.conf add

ENABLED_SERVICES+=,tls-proxy

This will create a new test root CA, a subordinate CA and an SSL
server cert. It uses the value of hostname -f for the certificate
subject. The CA certicates are also added to the system CA bundle.

To enable SSL natively, in local.conf add:

USE_SSL=True

Native SSL by default will also use the devstack-generate root and
subordinate CA.

You can override this on a per-service basis by setting

<SERVICE>_SSL_CERT=/path/to/cert
<SERVICE>_SSL_KEY=/path/to/key
<SERVICE>_SSL_PATH=/path/to/ca

You should also set SERVICE_HOST to the FQDN of the host. This
value defaults to the host IP address.

Change-Id: I36fe56c063ca921131ad98439bd452cb135916ac
Closes-Bug: 1328226
",git fetch https://review.opendev.org/openstack/devstack refs/changes/54/98854/16 && git format-patch -1 --stdout FETCH_HEAD,"['lib/ironic', 'lib/cinder', 'lib/neutron', 'functions', 'lib/nova', 'lib/sahara', 'lib/tempest', 'lib/heat', 'lib/keystone', 'lib/swift', 'lib/glance', 'lib/tls', 'lib/trove']",13,e7ddbeb679e840038baab9f91d31f7a2cd7c2bf3,bug/1328226, iniset $TROVE_API_PASTE_INI filter:authtoken cafile $SSL_BUNDLE_FILE, iniset $TROVE_API_PASTE_INI filter:authtoken cafile $KEYSTONE_SSL_CA,198,44
openstack%2Fnova~master~I7b0263cb367cbda530ed830021128ba400948fab,openstack/nova,master,I7b0263cb367cbda530ed830021128ba400948fab,fix indentation,ABANDONED,2014-09-26 12:35:49.000000000,2014-09-26 15:56:43.000000000,,"[{'_account_id': 3}, {'_account_id': 91}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-26 12:35:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/45d56a971dff2f1456a4f759b727e169740a2a5d', 'message': 'fix indentation\n\nJust fix some ugly indentation\n\nChange-Id: I7b0263cb367cbda530ed830021128ba400948fab\n'}, {'number': 2, 'created': '2014-09-26 15:19:44.000000000', 'files': ['nova/cells/weights/mute_child.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/c03c9baecb1c8f0fa9692e1929a699d1a563b2d5', 'message': 'fix indentation\n\nJust fix some ugly indentation\n\nChange-Id: I7b0263cb367cbda530ed830021128ba400948fab\n'}]",1,124388,c03c9baecb1c8f0fa9692e1929a699d1a563b2d5,9,5,2,91,,,0,"fix indentation

Just fix some ugly indentation

Change-Id: I7b0263cb367cbda530ed830021128ba400948fab
",git fetch https://review.opendev.org/openstack/nova refs/changes/88/124388/2 && git format-patch -1 --stdout FETCH_HEAD,['nova/cells/weights/mute_child.py'],1,45d56a971dff2f1456a4f759b727e169740a2a5d,cells_weighers," default=-10000.0, help='Multiplier used to weigh mute children. (The value' ' should be negative.)'),"," default=-10000.0, help='Multiplier used to weigh mute children. (The value ' 'should be negative.)'),",3,3
openstack%2Ffuel-main~stable%2F5.1~I21ae585c1ce50300834ed29517bb7f1ba72ef4e3,openstack/fuel-main,stable/5.1,I21ae585c1ce50300834ed29517bb7f1ba72ef4e3,Updating mirror URIs,MERGED,2014-09-26 15:48:10.000000000,2014-09-26 15:50:53.000000000,2014-09-26 15:50:52.000000000,"[{'_account_id': 3}, {'_account_id': 8777}, {'_account_id': 8971}]","[{'number': 1, 'created': '2014-09-26 15:48:10.000000000', 'files': ['config.mk'], 'web_link': 'https://opendev.org/openstack/fuel-main/commit/06563429faab3286290d800d705d30fd3dd689aa', 'message': 'Updating mirror URIs\n\nChange-Id: I21ae585c1ce50300834ed29517bb7f1ba72ef4e3\n(cherry picked from commit 5579585121f8445e061bde9a8bd48fb7e53714cd)\n'}]",0,124447,06563429faab3286290d800d705d30fd3dd689aa,7,3,1,8965,,,0,"Updating mirror URIs

Change-Id: I21ae585c1ce50300834ed29517bb7f1ba72ef4e3
(cherry picked from commit 5579585121f8445e061bde9a8bd48fb7e53714cd)
",git fetch https://review.opendev.org/openstack/fuel-main refs/changes/47/124447/1 && git format-patch -1 --stdout FETCH_HEAD,['config.mk'],1,06563429faab3286290d800d705d30fd3dd689aa,,"ifeq ($(USE_MIRROR),us)ifeq ($(USE_MIRROR),ext) YUM_REPOS?=proprietary MIRROR_BASE?=http://mirror.fuel-infra.org/fwm/$(PRODUCT_VERSION) MIRROR_CENTOS?=$(MIRROR_BASE)/centos MIRROR_UBUNTU?=$(MIRROR_BASE)/ubuntu MIRROR_DOCKER?=$(MIRROR_BASE)/docker endifMIRROR_BASE?=http://osci-mirror-srt.srt.mirantis.net/fwm/$(PRODUCT_VERSION)MIRROR_BASE?=http://osci-mirror-msk.msk.mirantis.net/fwm/$(PRODUCT_VERSION)MIRROR_BASE?=http://osci-mirror-kha.kha.mirantis.net/fwm/$(PRODUCT_VERSION)","ifeq ($(USE_MIRROR),ext)MIRROR_BASE?=http://fuel-mirror.srt.mirantis.net/fwm/$(PRODUCT_VERSION)MIRROR_BASE?=http://fuel-mirror.msk.mirantis.net/fwm/$(PRODUCT_VERSION)MIRROR_BASE?=http://fuel-mirror.kha.mirantis.net/fwm/$(PRODUCT_VERSION)",11,4
openstack%2Fpython-swiftclient~master~I5d100f81115e74878d510326acb5777e6a3626c8,openstack/python-swiftclient,master,I5d100f81115e74878d510326acb5777e6a3626c8,Fix unit tests failing when OS_ env vars are set,MERGED,2014-09-25 11:00:01.000000000,2014-09-26 15:48:49.000000000,2014-09-26 15:48:49.000000000,"[{'_account_id': 3}, {'_account_id': 2622}, {'_account_id': 7479}]","[{'number': 1, 'created': '2014-09-25 11:00:01.000000000', 'files': ['tests/unit/test_shell.py'], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/b13c84b2ef8a7fee31de2a77b66b50370e6968e5', 'message': 'Fix unit tests failing when OS_ env vars are set\n\ntests/unit/test_shell.py:TestParsing tests can fail if\nthere are OS_* variables set in the environment. There\nis already code in the test setUp to remove ST_* variables,\nso we should do the same for OS_* variables.\n\nThis patch also changes the mechanism used to remove and\nthen restore any unwanted variables found in os.environ.\n\nThe existing setUp() takes a copy of os.environ and then\ndeletes any ST_* variables in the original. In tearDown() it\nsets os.environ as the copy. However, the environ imported\ninto shell.py remains pointing to the original os.environ\nobject. So after the first call to tearDown, subsequent\nmocking of os.environ has no effect on shell.environ.\nThis renders some of the tests ineffective e.g.\ntest_insufficient_env_vars_v3 is not actually setting any\nvars in shell.environ.\n\nThe issue can be provoked by repeating a test:\nnosetests -w tests/unit/ test_shell.py:TestParsing.test_args_v3 \\\n test_shell.py:TestParsing.test_args_v3\n\nThe test will pass first time and fail second time.\n\nChange-Id: I5d100f81115e74878d510326acb5777e6a3626c8\n'}]",0,124009,b13c84b2ef8a7fee31de2a77b66b50370e6968e5,8,3,1,7847,,,0,"Fix unit tests failing when OS_ env vars are set

tests/unit/test_shell.py:TestParsing tests can fail if
there are OS_* variables set in the environment. There
is already code in the test setUp to remove ST_* variables,
so we should do the same for OS_* variables.

This patch also changes the mechanism used to remove and
then restore any unwanted variables found in os.environ.

The existing setUp() takes a copy of os.environ and then
deletes any ST_* variables in the original. In tearDown() it
sets os.environ as the copy. However, the environ imported
into shell.py remains pointing to the original os.environ
object. So after the first call to tearDown, subsequent
mocking of os.environ has no effect on shell.environ.
This renders some of the tests ineffective e.g.
test_insufficient_env_vars_v3 is not actually setting any
vars in shell.environ.

The issue can be provoked by repeating a test:
nosetests -w tests/unit/ test_shell.py:TestParsing.test_args_v3 \
 test_shell.py:TestParsing.test_args_v3

The test will pass first time and fail second time.

Change-Id: I5d100f81115e74878d510326acb5777e6a3626c8
",git fetch https://review.opendev.org/openstack/python-swiftclient refs/changes/09/124009/1 && git format-patch -1 --stdout FETCH_HEAD,['tests/unit/test_shell.py'],1,b13c84b2ef8a7fee31de2a77b66b50370e6968e5,p-os-env-test-setup," self._environ_vars = {} if (k in ('ST_KEY', 'ST_USER', 'ST_AUTH') or k.startswith('OS_')): self._environ_vars[k] = os.environ.pop(k) os.environ.update(self._environ_vars)"," self._orig_environ = os.environ.copy() if k in ('ST_KEY', 'ST_USER', 'ST_AUTH'): del os.environ[k] os.environ = self._orig_environ",5,4
openstack%2Fswift~master~Ic6d6b833a5b04ca2019be94b1b90d941929d21c8,openstack/swift,master,Ic6d6b833a5b04ca2019be94b1b90d941929d21c8,Zero-copy object-server GET responses with splice(),MERGED,2014-06-25 19:03:25.000000000,2014-09-26 15:48:25.000000000,2014-09-26 15:48:25.000000000,"[{'_account_id': 3}, {'_account_id': 330}, {'_account_id': 2622}, {'_account_id': 2696}, {'_account_id': 2828}, {'_account_id': 7233}, {'_account_id': 7479}, {'_account_id': 7680}, {'_account_id': 7847}, {'_account_id': 8871}, {'_account_id': 9205}, {'_account_id': 13052}]","[{'number': 1, 'created': '2014-06-25 19:03:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1fe7ae0182bed8691c5ed1ad5a7fc096b864dd79', 'message': 'Zero-copy object-server GET responses with splice()\n\nThis commit lets the object server use splice() and tee() to move data\nfrom disk to the network without ever copying it into user space.\n\nRequires the ""splicetee"" library and Linux. Sorry, FreeBSD folks. You\nstill have the old mechanism, as does anyone who doesn\'t want to\ninstall splicetee. It\'s optional. (While I normally dislike optional\ndependencies, this one requires a particular OS, so it can\'t be\nmandatory).\n\nNote that this only applies to GET responses without Range headers. It\ncan easily be extended to single-range GET requests, but this commit\nleaves that for future work. Same goes for PUT requests, or at least\nnon-chunked ones.\n\nThis requires a relatively recent kernel (2.6.38+) to work, which\nincludes the two most recent Ubuntu LTS releases (Precise and Trusty)\nas well as RHEL 7. However, it excludes Lucid and RHEL 6. On those\nsystems, setting ""splice = on"" will result in warnings in the logs but\nno actual use of splice.\n\nAlso fixed a long-standing annoyance in FakeLogger:\n\n    >>> fake_logger.warn(\'stuff\')\n    >>> fake_logger.get_lines_for_level(\'warn\')\n    []\n    >>>\n\nThis, of course, is because the correct log level is \'warning\'. Now\nyou get a KeyError if you call get_lines_for_level with a bogus log\nlevel.\n\nChange-Id: Ic6d6b833a5b04ca2019be94b1b90d941929d21c8\n'}, {'number': 2, 'created': '2014-06-25 20:22:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/3f86e731a1ae535f473eaa5cc5e24adaab1cef74', 'message': 'Zero-copy object-server GET responses with splice()\n\nThis commit lets the object server use splice() and tee() to move data\nfrom disk to the network without ever copying it into user space.\n\nRequires the ""splicetee"" library and Linux. Sorry, FreeBSD folks. You\nstill have the old mechanism, as does anyone who doesn\'t want to\ninstall splicetee. It\'s optional. (While I normally dislike optional\ndependencies, this one requires a particular OS, so it can\'t be\nmandatory).\n\nNote that this only applies to GET responses without Range headers. It\ncan easily be extended to single-range GET requests, but this commit\nleaves that for future work. Same goes for PUT requests, or at least\nnon-chunked ones.\n\nOn some real hardware I had laying around (not a VM), this produced a\n37% reduction in CPU usage for GETs made directly to the object\nserver. Measurements were done by looking at /proc/<pid>/stat,\nspecifically the utime and stime fields (user and kernel CPU jiffies,\nrespectively).\n\nThis requires a relatively recent kernel (2.6.38+) to work, which\nincludes the two most recent Ubuntu LTS releases (Precise and Trusty)\nas well as RHEL 7. However, it excludes Lucid and RHEL 6. On those\nsystems, setting ""splice = on"" will result in warnings in the logs but\nno actual use of splice.\n\nAlso fixed a long-standing annoyance in FakeLogger:\n\n    >>> fake_logger.warn(\'stuff\')\n    >>> fake_logger.get_lines_for_level(\'warn\')\n    []\n    >>>\n\nThis, of course, is because the correct log level is \'warning\'. Now\nyou get a KeyError if you call get_lines_for_level with a bogus log\nlevel.\n\nChange-Id: Ic6d6b833a5b04ca2019be94b1b90d941929d21c8\n'}, {'number': 3, 'created': '2014-06-26 00:53:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/440d3da8bd37686d2674e79640eeeba4dc8c2430', 'message': 'Zero-copy object-server GET responses with splice()\n\nThis commit lets the object server use splice() and tee() to move data\nfrom disk to the network without ever copying it into user space.\n\nRequires the ""splicetee"" library and Linux. Sorry, FreeBSD folks. You\nstill have the old mechanism, as does anyone who doesn\'t want to\ninstall splicetee. It\'s optional. (While I normally dislike optional\ndependencies, this one requires a particular OS, so it can\'t be\nmandatory).\n\nNote that this only applies to GET responses without Range headers. It\ncan easily be extended to single-range GET requests, but this commit\nleaves that for future work. Same goes for PUT requests, or at least\nnon-chunked ones.\n\nOn some real hardware I had laying around (not a VM), this produced a\n37% reduction in CPU usage for GETs made directly to the object\nserver. Measurements were done by looking at /proc/<pid>/stat,\nspecifically the utime and stime fields (user and kernel CPU jiffies,\nrespectively).\n\nThis requires a relatively recent kernel (2.6.38+) to work, which\nincludes the two most recent Ubuntu LTS releases (Precise and Trusty)\nas well as RHEL 7. However, it excludes Lucid and RHEL 6. On those\nsystems, setting ""splice = on"" will result in warnings in the logs but\nno actual use of splice.\n\nAlso fixed a long-standing annoyance in FakeLogger:\n\n    >>> fake_logger.warn(\'stuff\')\n    >>> fake_logger.get_lines_for_level(\'warn\')\n    []\n    >>>\n\nThis, of course, is because the correct log level is \'warning\'. Now\nyou get a KeyError if you call get_lines_for_level with a bogus log\nlevel.\n\nChange-Id: Ic6d6b833a5b04ca2019be94b1b90d941929d21c8\n'}, {'number': 4, 'created': '2014-06-26 23:09:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/57fe9510cab9cdee95208c7fbc75ce9887094786', 'message': 'Zero-copy object-server GET responses with splice()\n\nThis commit lets the object server use splice() and tee() to move data\nfrom disk to the network without ever copying it into user space.\n\nRequires the ""splicetee"" library and Linux. Sorry, FreeBSD folks. You\nstill have the old mechanism, as does anyone who doesn\'t want to\ninstall splicetee. It\'s optional. (While I normally dislike optional\ndependencies, this one requires a particular OS, so it can\'t be\nmandatory).\n\nNote that this only applies to GET responses without Range headers. It\ncan easily be extended to single-range GET requests, but this commit\nleaves that for future work. Same goes for PUT requests, or at least\nnon-chunked ones.\n\nOn some real hardware I had laying around (not a VM), this produced a\n37% reduction in CPU usage for GETs made directly to the object\nserver. Measurements were done by looking at /proc/<pid>/stat,\nspecifically the utime and stime fields (user and kernel CPU jiffies,\nrespectively).\n\nThis requires a relatively recent kernel (2.6.38+) to work, which\nincludes the two most recent Ubuntu LTS releases (Precise and Trusty)\nas well as RHEL 7. However, it excludes Lucid and RHEL 6. On those\nsystems, setting ""splice = on"" will result in warnings in the logs but\nno actual use of splice.\n\nAlso fixed a long-standing annoyance in FakeLogger:\n\n    >>> fake_logger.warn(\'stuff\')\n    >>> fake_logger.get_lines_for_level(\'warn\')\n    []\n    >>>\n\nThis, of course, is because the correct log level is \'warning\'. Now\nyou get a KeyError if you call get_lines_for_level with a bogus log\nlevel.\n\nChange-Id: Ic6d6b833a5b04ca2019be94b1b90d941929d21c8\n'}, {'number': 5, 'created': '2014-06-27 20:54:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7ba5467492c2a76a418be9918d5dea3d89bef46e', 'message': 'Zero-copy object-server GET responses with splice()\n\nThis commit lets the object server use splice() and tee() to move data\nfrom disk to the network without ever copying it into user space.\n\nRequires the ""splicetee"" library and Linux. Sorry, FreeBSD folks. You\nstill have the old mechanism, as does anyone who doesn\'t want to\ninstall splicetee. It\'s optional. (While I normally dislike optional\ndependencies, this one requires a particular OS, so it can\'t be\nmandatory).\n\nNote that this only applies to GET responses without Range headers. It\ncan easily be extended to single-range GET requests, but this commit\nleaves that for future work. Same goes for PUT requests, or at least\nnon-chunked ones.\n\nOn some real hardware I had laying around (not a VM), this produced a\n37% reduction in CPU usage for GETs made directly to the object\nserver. Measurements were done by looking at /proc/<pid>/stat,\nspecifically the utime and stime fields (user and kernel CPU jiffies,\nrespectively).\n\nThis requires a relatively recent kernel (2.6.38+) to work, which\nincludes the two most recent Ubuntu LTS releases (Precise and Trusty)\nas well as RHEL 7. However, it excludes Lucid and RHEL 6. On those\nsystems, setting ""splice = on"" will result in warnings in the logs but\nno actual use of splice.\n\nAlso fixed a long-standing annoyance in FakeLogger:\n\n    >>> fake_logger.warn(\'stuff\')\n    >>> fake_logger.get_lines_for_level(\'warn\')\n    []\n    >>>\n\nThis, of course, is because the correct log level is \'warning\'. Now\nyou get a KeyError if you call get_lines_for_level with a bogus log\nlevel.\n\nChange-Id: Ic6d6b833a5b04ca2019be94b1b90d941929d21c8\n'}, {'number': 6, 'created': '2014-07-03 22:21:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1505e49c19a77147a3b5a7e42163fa66bc3efb26', 'message': 'Zero-copy object-server GET responses with splice()\n\nThis commit lets the object server use splice() and tee() to move data\nfrom disk to the network without ever copying it into user space.\n\nRequires the ""splicetee"" library and Linux. Sorry, FreeBSD folks. You\nstill have the old mechanism, as does anyone who doesn\'t want to\ninstall splicetee. It\'s optional. (While I normally dislike optional\ndependencies, this one requires a particular OS, so it can\'t be\nmandatory).\n\nNote that this only applies to GET responses without Range headers. It\ncan easily be extended to single-range GET requests, but this commit\nleaves that for future work. Same goes for PUT requests, or at least\nnon-chunked ones.\n\nOn some real hardware I had laying around (not a VM), this produced a\n37% reduction in CPU usage for GETs made directly to the object\nserver. Measurements were done by looking at /proc/<pid>/stat,\nspecifically the utime and stime fields (user and kernel CPU jiffies,\nrespectively).\n\nThis requires a relatively recent kernel (2.6.38+) to work, which\nincludes the two most recent Ubuntu LTS releases (Precise and Trusty)\nas well as RHEL 7. However, it excludes Lucid and RHEL 6. On those\nsystems, setting ""splice = on"" will result in warnings in the logs but\nno actual use of splice.\n\nAlso fixed a long-standing annoyance in FakeLogger:\n\n    >>> fake_logger.warn(\'stuff\')\n    >>> fake_logger.get_lines_for_level(\'warn\')\n    []\n    >>>\n\nThis, of course, is because the correct log level is \'warning\'. Now\nyou get a KeyError if you call get_lines_for_level with a bogus log\nlevel.\n\nChange-Id: Ic6d6b833a5b04ca2019be94b1b90d941929d21c8\n'}, {'number': 7, 'created': '2014-07-04 01:03:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/af57eec113a9b71cae436a7bcf3a77c9613d4643', 'message': 'Zero-copy object-server GET responses with splice()\n\nThis commit lets the object server use splice() and tee() to move data\nfrom disk to the network without ever copying it into user space.\n\nRequires Linux. Sorry, FreeBSD folks. You still have the old\nmechanism, as does anyone who doesn\'t want to use splice. This\nrequires a relatively recent kernel (2.6.38+) to work, which includes\nthe two most recent Ubuntu LTS releases (Precise and Trusty) as well\nas RHEL 7. However, it excludes Lucid and RHEL 6. On those systems,\nsetting ""splice = on"" will result in warnings in the logs but no\nactual use of splice.\n\nNote that this only applies to GET responses without Range headers. It\ncan easily be extended to single-range GET requests, but this commit\nleaves that for future work. Same goes for PUT requests, or at least\nnon-chunked ones.\n\nOn some real hardware I had laying around (not a VM), this produced a\n37% reduction in CPU usage for GETs made directly to the object\nserver. Measurements were done by looking at /proc/<pid>/stat,\nspecifically the utime and stime fields (user and kernel CPU jiffies,\nrespectively).\n\nNote: There is a Python module called ""splicetee"" available on PyPi,\nbut it\'s licensed under the GPL, so it cannot easily be added to\nOpenStack\'s requirements. That\'s why this patch uses ctypes instead.\n\nAlso fixed a long-standing annoyance in FakeLogger:\n\n    >>> fake_logger.warn(\'stuff\')\n    >>> fake_logger.get_lines_for_level(\'warn\')\n    []\n    >>>\n\nThis, of course, is because the correct log level is \'warning\'. Now\nyou get a KeyError if you call get_lines_for_level with a bogus log\nlevel.\n\nChange-Id: Ic6d6b833a5b04ca2019be94b1b90d941929d21c8\n'}, {'number': 8, 'created': '2014-07-07 21:13:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ba2e2c9754b8fc6c49293fac24a27f4f7510b5c2', 'message': 'Zero-copy object-server GET responses with splice()\n\nThis commit lets the object server use splice() and tee() to move data\nfrom disk to the network without ever copying it into user space.\n\nRequires Linux. Sorry, FreeBSD folks. You still have the old\nmechanism, as does anyone who doesn\'t want to use splice. This\nrequires a relatively recent kernel (2.6.38+) to work, which includes\nthe two most recent Ubuntu LTS releases (Precise and Trusty) as well\nas RHEL 7. However, it excludes Lucid and RHEL 6. On those systems,\nsetting ""splice = on"" will result in warnings in the logs but no\nactual use of splice.\n\nNote that this only applies to GET responses without Range headers. It\ncan easily be extended to single-range GET requests, but this commit\nleaves that for future work. Same goes for PUT requests, or at least\nnon-chunked ones.\n\nOn some real hardware I had laying around (not a VM), this produced a\n37% reduction in CPU usage for GETs made directly to the object\nserver. Measurements were done by looking at /proc/<pid>/stat,\nspecifically the utime and stime fields (user and kernel CPU jiffies,\nrespectively).\n\nNote: There is a Python module called ""splicetee"" available on PyPi,\nbut it\'s licensed under the GPL, so it cannot easily be added to\nOpenStack\'s requirements. That\'s why this patch uses ctypes instead.\n\nAlso fixed a long-standing annoyance in FakeLogger:\n\n    >>> fake_logger.warn(\'stuff\')\n    >>> fake_logger.get_lines_for_level(\'warn\')\n    []\n    >>>\n\nThis, of course, is because the correct log level is \'warning\'. Now\nyou get a KeyError if you call get_lines_for_level with a bogus log\nlevel.\n\nChange-Id: Ic6d6b833a5b04ca2019be94b1b90d941929d21c8\n'}, {'number': 9, 'created': '2014-08-20 18:28:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/3b863cbf14225b04cf70a17a50abbf515f16f632', 'message': 'Zero-copy object-server GET responses with splice()\n\nThis commit lets the object server use splice() and tee() to move data\nfrom disk to the network without ever copying it into user space.\n\nRequires Linux. Sorry, FreeBSD folks. You still have the old\nmechanism, as does anyone who doesn\'t want to use splice. This\nrequires a relatively recent kernel (2.6.38+) to work, which includes\nthe two most recent Ubuntu LTS releases (Precise and Trusty) as well\nas RHEL 7. However, it excludes Lucid and RHEL 6. On those systems,\nsetting ""splice = on"" will result in warnings in the logs but no\nactual use of splice.\n\nNote that this only applies to GET responses without Range headers. It\ncan easily be extended to single-range GET requests, but this commit\nleaves that for future work. Same goes for PUT requests, or at least\nnon-chunked ones.\n\nOn some real hardware I had laying around (not a VM), this produced a\n37% reduction in CPU usage for GETs made directly to the object\nserver. Measurements were done by looking at /proc/<pid>/stat,\nspecifically the utime and stime fields (user and kernel CPU jiffies,\nrespectively).\n\nNote: There is a Python module called ""splicetee"" available on PyPi,\nbut it\'s licensed under the GPL, so it cannot easily be added to\nOpenStack\'s requirements. That\'s why this patch uses ctypes instead.\n\nAlso fixed a long-standing annoyance in FakeLogger:\n\n    >>> fake_logger.warn(\'stuff\')\n    >>> fake_logger.get_lines_for_level(\'warn\')\n    []\n    >>>\n\nThis, of course, is because the correct log level is \'warning\'. Now\nyou get a KeyError if you call get_lines_for_level with a bogus log\nlevel.\n\nChange-Id: Ic6d6b833a5b04ca2019be94b1b90d941929d21c8\n'}, {'number': 10, 'created': '2014-09-03 00:56:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8e35e670a5727322c9af544c37d216c007b4d43c', 'message': 'Zero-copy object-server GET responses with splice()\n\nThis commit lets the object server use splice() and tee() to move data\nfrom disk to the network without ever copying it into user space.\n\nRequires Linux. Sorry, FreeBSD folks. You still have the old\nmechanism, as does anyone who doesn\'t want to use splice. This\nrequires a relatively recent kernel (2.6.38+) to work, which includes\nthe two most recent Ubuntu LTS releases (Precise and Trusty) as well\nas RHEL 7. However, it excludes Lucid and RHEL 6. On those systems,\nsetting ""splice = on"" will result in warnings in the logs but no\nactual use of splice.\n\nNote that this only applies to GET responses without Range headers. It\ncan easily be extended to single-range GET requests, but this commit\nleaves that for future work. Same goes for PUT requests, or at least\nnon-chunked ones.\n\nOn some real hardware I had laying around (not a VM), this produced a\n37% reduction in CPU usage for GETs made directly to the object\nserver. Measurements were done by looking at /proc/<pid>/stat,\nspecifically the utime and stime fields (user and kernel CPU jiffies,\nrespectively).\n\nNote: There is a Python module called ""splicetee"" available on PyPi,\nbut it\'s licensed under the GPL, so it cannot easily be added to\nOpenStack\'s requirements. That\'s why this patch uses ctypes instead.\n\nAlso fixed a long-standing annoyance in FakeLogger:\n\n    >>> fake_logger.warn(\'stuff\')\n    >>> fake_logger.get_lines_for_level(\'warn\')\n    []\n    >>>\n\nThis, of course, is because the correct log level is \'warning\'. Now\nyou get a KeyError if you call get_lines_for_level with a bogus log\nlevel.\n\nChange-Id: Ic6d6b833a5b04ca2019be94b1b90d941929d21c8\n'}, {'number': 11, 'created': '2014-09-18 23:02:58.000000000', 'files': ['swift/obj/server.py', 'swift/common/utils.py', 'test/unit/__init__.py', 'test/unit/obj/test_server.py', 'test/unit/obj/test_diskfile.py', 'test/unit/common/test_utils.py', 'etc/object-server.conf-sample', 'swift/obj/diskfile.py', 'test/unit/common/test_container_sync_realms.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/7d0e5ebe690bf3cf41ccd970281d532a23284e58', 'message': 'Zero-copy object-server GET responses with splice()\n\nThis commit lets the object server use splice() and tee() to move data\nfrom disk to the network without ever copying it into user space.\n\nRequires Linux. Sorry, FreeBSD folks. You still have the old\nmechanism, as does anyone who doesn\'t want to use splice. This\nrequires a relatively recent kernel (2.6.38+) to work, which includes\nthe two most recent Ubuntu LTS releases (Precise and Trusty) as well\nas RHEL 7. However, it excludes Lucid and RHEL 6. On those systems,\nsetting ""splice = on"" will result in warnings in the logs but no\nactual use of splice.\n\nNote that this only applies to GET responses without Range headers. It\ncan easily be extended to single-range GET requests, but this commit\nleaves that for future work. Same goes for PUT requests, or at least\nnon-chunked ones.\n\nOn some real hardware I had laying around (not a VM), this produced a\n37% reduction in CPU usage for GETs made directly to the object\nserver. Measurements were done by looking at /proc/<pid>/stat,\nspecifically the utime and stime fields (user and kernel CPU jiffies,\nrespectively).\n\nNote: There is a Python module called ""splicetee"" available on PyPi,\nbut it\'s licensed under the GPL, so it cannot easily be added to\nOpenStack\'s requirements. That\'s why this patch uses ctypes instead.\n\nAlso fixed a long-standing annoyance in FakeLogger:\n\n    >>> fake_logger.warn(\'stuff\')\n    >>> fake_logger.get_lines_for_level(\'warn\')\n    []\n    >>>\n\nThis, of course, is because the correct log level is \'warning\'. Now\nyou get a KeyError if you call get_lines_for_level with a bogus log\nlevel.\n\nChange-Id: Ic6d6b833a5b04ca2019be94b1b90d941929d21c8\n'}]",50,102609,7d0e5ebe690bf3cf41ccd970281d532a23284e58,85,12,11,2622,,,0,"Zero-copy object-server GET responses with splice()

This commit lets the object server use splice() and tee() to move data
from disk to the network without ever copying it into user space.

Requires Linux. Sorry, FreeBSD folks. You still have the old
mechanism, as does anyone who doesn't want to use splice. This
requires a relatively recent kernel (2.6.38+) to work, which includes
the two most recent Ubuntu LTS releases (Precise and Trusty) as well
as RHEL 7. However, it excludes Lucid and RHEL 6. On those systems,
setting ""splice = on"" will result in warnings in the logs but no
actual use of splice.

Note that this only applies to GET responses without Range headers. It
can easily be extended to single-range GET requests, but this commit
leaves that for future work. Same goes for PUT requests, or at least
non-chunked ones.

On some real hardware I had laying around (not a VM), this produced a
37% reduction in CPU usage for GETs made directly to the object
server. Measurements were done by looking at /proc/<pid>/stat,
specifically the utime and stime fields (user and kernel CPU jiffies,
respectively).

Note: There is a Python module called ""splicetee"" available on PyPi,
but it's licensed under the GPL, so it cannot easily be added to
OpenStack's requirements. That's why this patch uses ctypes instead.

Also fixed a long-standing annoyance in FakeLogger:

    >>> fake_logger.warn('stuff')
    >>> fake_logger.get_lines_for_level('warn')
    []
    >>>

This, of course, is because the correct log level is 'warning'. Now
you get a KeyError if you call get_lines_for_level with a bogus log
level.

Change-Id: Ic6d6b833a5b04ca2019be94b1b90d941929d21c8
",git fetch https://review.opendev.org/openstack/swift refs/changes/09/102609/10 && git format-patch -1 --stdout FETCH_HEAD,"['swift/obj/server.py', 'swift/common/utils.py', 'test/unit/__init__.py', 'test/unit/obj/test_server.py', 'test/unit/obj/test_diskfile.py', 'etc/object-server.conf-sample', 'swift/obj/diskfile.py', 'test/unit/common/test_container_sync_realms.py']",8,1fe7ae0182bed8691c5ed1ad5a7fc096b864dd79,ksplice-c," logger.all_log_lines(), logger.all_log_lines(), self.assertEqual(logger.all_log_lines(), {}) logger.all_log_lines(), self.assertEqual(logger.all_log_lines(), {}) self.assertEqual(logger.all_log_lines(), {}) self.assertEqual(logger.all_log_lines(), {}) logger.all_log_lines(),"," logger.lines_dict, logger.lines_dict, self.assertEqual(logger.lines_dict, {}) logger.lines_dict, self.assertEqual(logger.lines_dict, {}) self.assertEqual(logger.lines_dict, {}) self.assertEqual(logger.lines_dict, {}) logger.lines_dict,",556,19
openstack%2Fkeystone~master~I99db0ae7a6dea8d6bc32f6ed3af758ae0dcc5e18,openstack/keystone,master,I99db0ae7a6dea8d6bc32f6ed3af758ae0dcc5e18,Adds RBAC to Keystone V2 API get_endpoints method.,ABANDONED,2014-07-31 21:19:14.000000000,2014-09-26 15:40:50.000000000,,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 220}, {'_account_id': 1916}, {'_account_id': 9098}, {'_account_id': 11022}, {'_account_id': 12715}]","[{'number': 1, 'created': '2014-07-31 21:19:14.000000000', 'files': ['keystone/catalog/controllers.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/c6833f973fac9f06f5c9df0d2ba7bf7fbb493296', 'message': 'Adds RBAC to Keystone V2 API get_endpoints method.\n\nAdded the @controller.protected() decorator to the get_endpoints\nmethod so that the permissions for accessing this method are taken\nfrom the policy.json file.\nRemoved the hardcoded admin-only permissions.\n\nChange-Id: I99db0ae7a6dea8d6bc32f6ed3af758ae0dcc5e18\nCloses-Bug: 1350879\n'}]",3,111088,c6833f973fac9f06f5c9df0d2ba7bf7fbb493296,20,7,1,12715,,,0,"Adds RBAC to Keystone V2 API get_endpoints method.

Added the @controller.protected() decorator to the get_endpoints
method so that the permissions for accessing this method are taken
from the policy.json file.
Removed the hardcoded admin-only permissions.

Change-Id: I99db0ae7a6dea8d6bc32f6ed3af758ae0dcc5e18
Closes-Bug: 1350879
",git fetch https://review.opendev.org/openstack/keystone refs/changes/88/111088/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/catalog/controllers.py'],1,c6833f973fac9f06f5c9df0d2ba7bf7fbb493296,bug/1350879, @controller.protected(), self.assert_admin(context),1,1
openstack%2Ffuel-web~master~Ic00bed869b2c8a157a012dad4a6b42d76c496418,openstack/fuel-web,master,Ic00bed869b2c8a157a012dad4a6b42d76c496418,input.jsx,ABANDONED,2014-09-26 10:13:11.000000000,2014-09-26 15:34:28.000000000,,"[{'_account_id': 3}, {'_account_id': 8735}, {'_account_id': 8766}, {'_account_id': 8971}, {'_account_id': 9091}, {'_account_id': 9730}]","[{'number': 1, 'created': '2014-09-26 10:13:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/d6088e7cc99b53934c5c8162cf510a587c1a35d4', 'message': 'input.jsx\n\nChange-Id: Ic00bed869b2c8a157a012dad4a6b42d76c496418\n'}, {'number': 2, 'created': '2014-09-26 11:32:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/a6abb9eabd3368e208ea4d4153c1624e673bccee', 'message': 'input.jsx\n\nChange-Id: Ic00bed869b2c8a157a012dad4a6b42d76c496418\n'}, {'number': 3, 'created': '2014-09-26 14:08:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/473149e96a108aa734a09aa86ece2a7936ff7e6c', 'message': 'input.jsx\n\nChange-Id: Ic00bed869b2c8a157a012dad4a6b42d76c496418\n'}, {'number': 4, 'created': '2014-09-26 14:11:45.000000000', 'files': ['nailgun/static/i18n/translation.json', 'nailgun/static/templates/cluster/node_list.html', 'nailgun/static/templates/cluster/node_group.html', 'nailgun/static/js/views/releases_page.jsx', 'nailgun/static/js/views/capacity_page.jsx', 'nailgun/static/js/views/controls.jsx', 'nailgun/static/js/views/input.jsx', 'nailgun/static/templates/cluster/healthcheck_tab.html'], 'web_link': 'https://opendev.org/openstack/fuel-web/commit/d7a279e37ceb2e8dd76598c487c3237701392c80', 'message': 'input.jsx\n\nChange-Id: Ic00bed869b2c8a157a012dad4a6b42d76c496418\n'}]",0,124352,d7a279e37ceb2e8dd76598c487c3237701392c80,25,6,4,8766,,,0,"input.jsx

Change-Id: Ic00bed869b2c8a157a012dad4a6b42d76c496418
",git fetch https://review.opendev.org/openstack/fuel-web refs/changes/52/124352/2 && git format-patch -1 --stdout FETCH_HEAD,"['nailgun/static/js/views/controls.jsx', 'nailgun/static/js/views/input.jsx']",2,d6088e7cc99b53934c5c8162cf510a587c1a35d4,bp/backbone-to-react,"/* * Copyright 2014 Mirantis, Inc. * * Licensed under the Apache License, Version 2.0 (the ""License""); you may * not use this file except in compliance with the License. You may obtain * a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the * License for the specific language governing permissions and limitations * under the License. **/ define( [ 'jquery', 'underscore', 'react' ], function($, _, React) { 'use strict'; var cx = React.addons.classSet; var Input = React.createClass({ propTypes: { // props used by <input /> type: React.PropTypes.string.isRequired, name: React.PropTypes.string, defaultValue: React.PropTypes.oneOfType([ React.PropTypes.string, React.PropTypes.number, React.PropTypes.bool ]), defaultChecked: React.PropTypes.bool, disabled: React.PropTypes.bool, onChange: React.PropTypes.func, onKeyDown: React.PropTypes.func, maxLength: React.PropTypes.renderable, // another props label: React.PropTypes.renderable, description: React.PropTypes.renderable, error: React.PropTypes.renderable, tooltipText: React.PropTypes.renderable }, getInputDOMNode: function() { return this.refs.input.getDOMNode(); }, isCheckboxOrRadio: function() { return this.props.type === 'radio' || this.props.type === 'checkbox'; }, getValue: function() { var input = this.getInputDOMNode(); return this.isCheckboxOrRadio() ? input.checked : input.value; }, componentDidMount: function() { if (this.props.tooltipText) $(this.getInputDOMNode()).tooltip(); }, componentWillUnmount: function() { if (this.props.tooltipText) $(this.getInputDOMNode()).tooltip('destroy'); }, renderInput: function() { var input = null; switch (this.props.type) { case 'select': input = (<select ref='input' key='input'>{this.props.children}</select>); break; case 'textarea': input = <textarea ref='input' key='input' />; break; default: input = <input ref='input' key='input' />; } return this.isCheckboxOrRadio() ? [ this.transferPropsTo(input), <span>&nbsp;</span> ] : this.transferPropsTo(input); }, renderTooltipIcon: function() { return this.props.tooltipText ? ( <i key='tooltip' className='icon-attention text-warning' data-toggle='tooltip' title={this.props.tooltipText} /> ) : null; }, renderLabel: function(children) { return this.props.label ? ( <label key='label' className='enable-selection' htmlFor={this.props.id}> {this.props.label} {children} </label> ) : children; }, renderDescription: function() { var error = !_.isNull(this.props.error); return error || this.props.description ? ( <div key='description' className='description enable-selection'> {error ? this.props.error : this.props.description} </div> ) : null; }, renderWrapper: function(children) { return ( <div className={cx({'control-box': true, 'validation-error': !_.isNull(this.props.error)})}> {children} </div> ); }, render: function() { return this.renderWrapper([ this.renderLabel([ this.renderInput(), this.renderTooltipIcon() ]), this.renderDescriprion() ]); } }); return Input; }); ",,134,210
openstack%2Fkeystone~master~I3544894482b30a47fcd4fac8948d03136fd83f14,openstack/keystone,master,I3544894482b30a47fcd4fac8948d03136fd83f14,Add a pool of memcached clients,MERGED,2014-09-05 19:36:26.000000000,2014-09-26 15:33:49.000000000,2014-09-23 19:37:18.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 708}, {'_account_id': 2218}, {'_account_id': 2903}, {'_account_id': 5046}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 7725}, {'_account_id': 8978}, {'_account_id': 13055}]","[{'number': 1, 'created': '2014-09-05 19:36:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/ca1ee7c65d6691d5f83f0c4f0476139c6c09af02', 'message': ""Add a pool of memcached clients\n\nThis would avoid creation of too many connections and keep 'dead' flags\non memcached servers\n\nTo use it, set 'keystone.cache.memcache_pool' as active dogpile.cache\nbackend:\n\n[cache]\nbackend=keystone.cache.memcache_pool\nbackend_argument=url:127.0.0.1:11211\nbackend_argument=memcache_dead_retry:300\nbackend_argument=memcache_socket_timeout:3\nbackend_argument=pool_maxsize:10\nbackend_argument=pool_unused_timeout:60\nbackend_argument=pool_debug:\n\nWhere:\n- url - comma-separated list of host:port pairs (was already there);\n- memcache_dead_retry - number of seconds memcached server is considered\n  dead before its tried again;\n- memcache_socket_timeout - timeout in seconds for every call to a\n  server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before its closed;\n- pool_debug - add non-empty string if you want debug log of this pool\n  (there are a lot of logs there).\n\nNOTE: there is no way to specify these parameters for token backend\nwithout this patch: https://gerrit.mirantis.com/20587\n\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 2, 'created': '2014-09-06 22:32:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/7a5367ccbd34a05bc8b63266e14952e926b9731b', 'message': ""Add a pool of memcached clients\n\nThis would avoid creation of too many connections and keep 'dead' flags\non memcached servers\n\nTo use it, set 'keystone.cache.memcache_pool' as active dogpile.cache\nbackend:\n\n[cache]\nbackend=keystone.cache.memcache_pool\nbackend_argument=url:127.0.0.1:11211\nbackend_argument=memcache_dead_retry:300\nbackend_argument=memcache_socket_timeout:3\nbackend_argument=pool_maxsize:10\nbackend_argument=pool_unused_timeout:60\nbackend_argument=pool_debug:\n\nWhere:\n- url - comma-separated list of host:port pairs (was already there);\n- memcache_dead_retry - number of seconds memcached server is considered\n  dead before its tried again;\n- memcache_socket_timeout - timeout in seconds for every call to a\n  server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before its closed;\n- pool_debug - add non-empty string if you want debug log of this pool\n  (there are a lot of logs there).\n\nNOTE: there is no way to specify these parameters for token backend\nwithout this patch: https://gerrit.mirantis.com/20587\n\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 3, 'created': '2014-09-09 11:01:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/ad7210144c9682e799a5a5422f6e095b4bc9e599', 'message': ""Add a pool of memcached clients\n\nThis would avoid creation of too many connections and keep 'dead' flags\non memcached servers.\n\nThe core module that provides pool functionality is copied between\nkeystone and keystonemiddleware (see _memcache_pool.py in this change\nhttps://review.openstack.org/119774 in keystonemiddleware).\n\nTo use it, set 'keystone.cache.memcache_pool' as active dogpile.cache\nbackend:\n\n[cache]\nbackend=keystone.cache.memcache_pool\nbackend_argument=url:127.0.0.1:11211\nbackend_argument=memcache_dead_retry:300\nbackend_argument=memcache_socket_timeout:3\nbackend_argument=pool_maxsize:10\nbackend_argument=pool_unused_timeout:60\nbackend_argument=pool_debug:\n\nWhere:\n- url - comma-separated list of host:port pairs (was already there);\n- memcache_dead_retry - number of seconds memcached server is considered\n  dead before its tried again;\n- memcache_socket_timeout - timeout in seconds for every call to a\n  server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before its closed;\n- pool_debug - add non-empty string if you want debug log of this pool\n  (there are a lot of logs there).\n\nNOTE: there is no way to specify these parameters for token backend\nwithout this patch: https://gerrit.mirantis.com/20587\n\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 4, 'created': '2014-09-10 08:21:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/f43122ea441329d37d56a4a6d1cd6957de58651b', 'message': ""Add a pool of memcached clients\n\nThis would avoid creation of too many connections and keep 'dead' flags\non memcached servers.\n\nThe core module that provides pool functionality is copied between\nkeystone and keystonemiddleware (see _memcache_pool.py in this change\nhttps://review.openstack.org/119774 in keystonemiddleware).\n\nTo use it, set 'keystone.cache.memcache_pool' as active dogpile.cache\nbackend:\n\n[cache]\nbackend=keystone.cache.memcache_pool\nbackend_argument=url:127.0.0.1:11211\nbackend_argument=memcache_dead_retry:300\nbackend_argument=memcache_socket_timeout:3\nbackend_argument=pool_maxsize:10\nbackend_argument=pool_unused_timeout:60\nbackend_argument=pool_debug:\n\nWhere:\n- url - comma-separated list of host:port pairs (was already there);\n- memcache_dead_retry - number of seconds memcached server is considered\n  dead before its tried again;\n- memcache_socket_timeout - timeout in seconds for every call to a\n  server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before its closed;\n- pool_debug - add non-empty string if you want debug log of this pool\n  (there are a lot of logs there).\n\nNOTE: there is no way to specify these parameters for token backend\nwithout this patch: https://review.openstack.org/120340\n\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 5, 'created': '2014-09-11 11:19:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/a143c7174e357c7066fbbcc9f674a9f4852ea837', 'message': ""Add a pool of memcached clients\n\nThis would avoid creation of too many connections and keep 'dead' flags\non memcached servers.\n\nThe core module that provides pool functionality is copied between\nkeystone and keystonemiddleware (see _memcache_pool.py in this change\nhttps://review.openstack.org/119774 in keystonemiddleware).\n\nThis backend is used instead of dogpile's backend in memcache token\npersistence backend, to configure it one should use following config\narguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\npool_debug = False\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- memcache_dead_retry - number of seconds memcached server is considered\n  dead before its tried again;\n- memcache_socket_timeout - timeout in seconds for every call to a\n  server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before its closed;\n- pool_debug - add non-empty string if you want debug log of this pool\n  (there are a lot of logs there).\n\nTo use it as cache backend, set 'keystone.cache.memcache_pool' as active\ndogpile.cache backend, other options are the same, but with 'memcache_'\nprefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\nmemcache_pool_debug = False\n\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 6, 'created': '2014-09-15 19:53:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/bb056e15cc4e098149f4fb0fab7412e8e6606e8c', 'message': ""Add a pool of memcached clients\n\nThis would avoid creation of too many connections and keep 'dead' flags\non memcached servers.\n\nThe core module that provides pool functionality is copied between\nkeystone and keystonemiddleware (see _memcache_pool.py in this change\nhttps://review.openstack.org/119774 in keystonemiddleware).\n\nThis backend is used instead of dogpile's backend in memcache token\npersistence backend, to configure it one should use following config\narguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\npool_debug = False\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- memcache_dead_retry - number of seconds memcached server is considered\n  dead before its tried again;\n- memcache_socket_timeout - timeout in seconds for every call to a\n  server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before its closed;\n- pool_debug - add non-empty string if you want debug log of this pool\n  (there are a lot of logs there).\n\nTo use it as cache backend, set 'keystone.cache.memcache_pool' as active\ndogpile.cache backend, other options are the same, but with 'memcache_'\nprefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\nmemcache_pool_debug = False\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 7, 'created': '2014-09-15 21:25:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/830e3d9249acfef18223945352a88f3f909b5727', 'message': ""Add a pool of memcached clients\n\nThis would avoid creation of too many connections and keep 'dead' flags\non memcached servers.\n\nThe core module that provides pool functionality is copied between\nkeystone and keystonemiddleware (see _memcache_pool.py in this change\nhttps://review.openstack.org/119774 in keystonemiddleware).\n\nThis backend is used instead of dogpile's backend in memcache token\npersistence backend, to configure it one should use following config\narguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\npool_debug = False\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- memcache_dead_retry - number of seconds memcached server is considered\n  dead before its tried again;\n- memcache_socket_timeout - timeout in seconds for every call to a\n  server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before its closed;\n- pool_debug - add non-empty string if you want debug log of this pool\n  (there are a lot of logs there).\n\nTo use it as cache backend, set 'keystone.cache.memcache_pool' as active\ndogpile.cache backend, other options are the same, but with 'memcache_'\nprefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\nmemcache_pool_debug = False\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 8, 'created': '2014-09-15 21:52:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/7ff30079053c579c4f52ea69cdafc3f26baf99b7', 'message': ""Add a pool of memcached clients\n\nThis would avoid creation of too many connections and keep 'dead' flags\non memcached servers.\n\nThe core module that provides pool functionality is copied between\nkeystone and keystonemiddleware (see _memcache_pool.py in this change\nhttps://review.openstack.org/119774 in keystonemiddleware).\n\nThis backend is used instead of dogpile's backend in memcache token\npersistence backend, to configure it one should use following config\narguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\npool_debug = False\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- memcache_dead_retry - number of seconds memcached server is considered\n  dead before its tried again;\n- memcache_socket_timeout - timeout in seconds for every call to a\n  server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before its closed;\n- pool_debug - add non-empty string if you want debug log of this pool\n  (there are a lot of logs there).\n\nTo use it as cache backend, set 'keystone.cache.memcache_pool' as active\ndogpile.cache backend, other options are the same, but with 'memcache_'\nprefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\nmemcache_pool_debug = False\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 9, 'created': '2014-09-15 22:03:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/f9597a724d7bf25cc1034b3efbcd348af588029c', 'message': ""Add a pool of memcached clients\n\nThis would avoid creation of too many connections and keep 'dead' flags\non memcached servers.\n\nThe core module that provides pool functionality is copied between\nkeystone and keystonemiddleware (see _memcache_pool.py in this change\nhttps://review.openstack.org/119774 in keystonemiddleware).\n\nThis backend is used instead of dogpile's backend in memcache token\npersistence backend, to configure it one should use following config\narguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\npool_debug = False\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- memcache_dead_retry - number of seconds memcached server is considered\n  dead before its tried again;\n- memcache_socket_timeout - timeout in seconds for every call to a\n  server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before its closed;\n- pool_debug - add non-empty string if you want debug log of this pool\n  (there are a lot of logs there).\n\nTo use it as cache backend, set 'keystone.cache.memcache_pool' as active\ndogpile.cache backend, other options are the same, but with 'memcache_'\nprefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\nmemcache_pool_debug = False\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 10, 'created': '2014-09-15 22:06:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/64bd889c41099da0d62e9d05db32d0178d9e6def', 'message': ""Add a pool of memcached clients\n\nThis would avoid creation of too many connections and keep 'dead' flags\non memcached servers.\n\nThe core module that provides pool functionality is copied between\nkeystone and keystonemiddleware (see _memcache_pool.py in this change\nhttps://review.openstack.org/119774 in keystonemiddleware).\n\nThis backend is used instead of dogpile's backend in memcache token\npersistence backend, to configure it one should use following config\narguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\npool_debug = False\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- memcache_dead_retry - number of seconds memcached server is considered\n  dead before its tried again;\n- memcache_socket_timeout - timeout in seconds for every call to a\n  server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before its closed;\n- pool_debug - add non-empty string if you want debug log of this pool\n  (there are a lot of logs there).\n\nTo use it as cache backend, set 'keystone.cache.memcache_pool' as active\ndogpile.cache backend, other options are the same, but with 'memcache_'\nprefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\nmemcache_pool_debug = False\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 11, 'created': '2014-09-16 02:37:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/8c1b2057ed08fe9b05c298e5d661676683bd8369', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 12, 'created': '2014-09-16 02:46:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/76973825772e64d630bb747bccc03c6d18e5793b', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 13, 'created': '2014-09-16 02:48:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/a67197d79ea305093acfbff7699654b190ea2909', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 14, 'created': '2014-09-16 02:51:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/0b92cfc8ea10531a05ba7fb4d63c3bc3ab4c5579', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 15, 'created': '2014-09-16 03:00:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/746f6d94d12b02bc7e1c14cada5a120cf9e016a3', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 16, 'created': '2014-09-16 17:55:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/27f31a9463760c82ce9d873e8bb84091b07db0f9', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 17, 'created': '2014-09-16 18:00:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/21dc32f128105452b6b11f9b98f0b18cd6cefe31', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 18, 'created': '2014-09-16 19:10:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/6fa78e0c5a2af6fcff847cf90055301e7da5e46f', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 19, 'created': '2014-09-16 19:21:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/db0de93809561fc1f647d4f5c9d53ddd16d18478', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 20, 'created': '2014-09-17 01:39:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/58a92c9a68c3ce774a7a7ed275f2047b1726974e', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 21, 'created': '2014-09-17 01:40:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/eaba34515b70cebe0cb71adb66b9754682e146ff', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 22, 'created': '2014-09-17 01:41:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/71a2941b994355dd1cd1026d4bd18423509b9dbf', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 23, 'created': '2014-09-17 02:15:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/1187ec9b97f4390ce699688145ac40132212d13d', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 24, 'created': '2014-09-17 02:18:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/fb3fd5bb575b8f86981883ca920eec3a6d96ffe3', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 25, 'created': '2014-09-17 02:24:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/b61cb75bb247891991fef6195e024e4a81a5a7e9', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 26, 'created': '2014-09-17 03:18:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/c3ec76972c2dca436ca08ba3916fea84c5f02163', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 27, 'created': '2014-09-17 20:08:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/35e8d1a9628347afc618fbf9e41e514a9bb9d1b7', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 28, 'created': '2014-09-17 20:25:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/30dedada763c2760c40137eba939161ac4bb6d2c', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 29, 'created': '2014-09-18 00:03:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/429b8c73e0ff618eb2cf22cd230aafcca087b843', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is used instead of dogpile's default memcache\nbackend in the memcache token persistence driver. To configure the new backend\none should use following config arguments:\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 30, 'created': '2014-09-20 00:02:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/d3604b40858eac8ee9964f2c842e19dad3175388', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is available either by being set as the memcache\nbackend or by using keystone.token.persistence.backends.memcache_pool.Token for\nthe Token memcache persistence driver.\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}, {'number': 31, 'created': '2014-09-22 17:09:38.000000000', 'files': ['etc/keystone.conf.sample', 'keystone/tests/unit/common/test_connection_pool.py', 'keystone/tests/core.py', 'keystone/common/config.py', 'keystone/common/cache/backends/memcache_pool.py', 'doc/source/configuration.rst', 'keystone/token/persistence/backends/memcache_pool.py', 'keystone/common/kvs/backends/memcached.py', 'keystone/common/cache/_memcache_pool.py', 'keystone/common/cache/core.py', 'keystone/token/persistence/backends/memcache.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/0010803288748fcd3ce7dba212a54bffe7a61a0c', 'message': ""Add a pool of memcached clients\n\nThis patchset adds a pool of memcache clients. This pool allows for reuse of\na client object, prevents too many client object from being instantiated, and\nmaintains proper tracking of dead servers so as to limit delays\nwhen a server (or all servers) become unavailable.\n\nThe new memcache pool backend is available either by being set as the memcache\nbackend or by using keystone.token.persistence.backends.memcache_pool.Token for\nthe Token memcache persistence driver.\n\n[memcache]\nservers = 127.0.0.1:11211\ndead_retry = 300\nsocket_timeout = 3\npool_maxsize = 10\npool_unused_timeout = 60\n\nWhere:\n- servers - comma-separated list of host:port pairs (was already there);\n- dead_retry - number of seconds memcached server is considered dead\n  before it is tried again;\n- socket_timeout - timeout in seconds for every call to a server;\n- pool_maxsize - max total number of open connections in the pool;\n- pool_unused_timeout - number of seconds a connection is held unused in\n  the pool before it is closed;\n\nThe new memcache pool backend can be used as the driver for the Keystone\ncaching layer. To use it as caching driver, set\n'keystone.cache.memcache_pool' as the value of the [cache]\\backend option,\nthe other options are the same as above, but with 'memcache_' prefix:\n\n[cache]\nbackend = keystone.cache.memcache_pool\nmemcache_servers = 127.0.0.1:11211\nmemcache_dead_retry = 300\nmemcache_socket_timeout = 3\nmemcache_pool_maxsize = 10\nmemcache_pool_unused_timeout = 60\n\nCo-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>\nCloses-bug: #1332058\nCloses-bug: #1360446\nChange-Id: I3544894482b30a47fcd4fac8948d03136fd83f14\n""}]",161,119452,0010803288748fcd3ce7dba212a54bffe7a61a0c,135,11,31,708,,,0,"Add a pool of memcached clients

This patchset adds a pool of memcache clients. This pool allows for reuse of
a client object, prevents too many client object from being instantiated, and
maintains proper tracking of dead servers so as to limit delays
when a server (or all servers) become unavailable.

The new memcache pool backend is available either by being set as the memcache
backend or by using keystone.token.persistence.backends.memcache_pool.Token for
the Token memcache persistence driver.

[memcache]
servers = 127.0.0.1:11211
dead_retry = 300
socket_timeout = 3
pool_maxsize = 10
pool_unused_timeout = 60

Where:
- servers - comma-separated list of host:port pairs (was already there);
- dead_retry - number of seconds memcached server is considered dead
  before it is tried again;
- socket_timeout - timeout in seconds for every call to a server;
- pool_maxsize - max total number of open connections in the pool;
- pool_unused_timeout - number of seconds a connection is held unused in
  the pool before it is closed;

The new memcache pool backend can be used as the driver for the Keystone
caching layer. To use it as caching driver, set
'keystone.cache.memcache_pool' as the value of the [cache]\backend option,
the other options are the same as above, but with 'memcache_' prefix:

[cache]
backend = keystone.cache.memcache_pool
memcache_servers = 127.0.0.1:11211
memcache_dead_retry = 300
memcache_socket_timeout = 3
memcache_pool_maxsize = 10
memcache_pool_unused_timeout = 60

Co-Authored-By: Morgan Fainberg <morgan.fainberg@gmail.com>
Closes-bug: #1332058
Closes-bug: #1360446
Change-Id: I3544894482b30a47fcd4fac8948d03136fd83f14
",git fetch https://review.opendev.org/openstack/keystone refs/changes/52/119452/30 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/common/cache/backends/memcache_pool.py', 'keystone/common/kvs/backends/memcached.py', 'keystone/common/cache/core.py']",3,ca1ee7c65d6691d5f83f0c4f0476139c6c09af02,bug/1332058,"dogpile.cache.register_backend( 'keystone.cache.memcache_pool', 'keystone.common.cache.backends.memcache_pool', 'PooledMemcachedBackend') ",,249,1
openstack%2Fhorizon~master~I47688c2342b8ce6c1e7728f53111d8f4ce6bb98e,openstack/horizon,master,I47688c2342b8ce6c1e7728f53111d8f4ce6bb98e,Add validation to Resource Usage daily report,ABANDONED,2014-06-23 20:33:11.000000000,2014-09-26 15:31:27.000000000,,"[{'_account_id': 3}, {'_account_id': 4264}, {'_account_id': 8984}, {'_account_id': 9981}]","[{'number': 1, 'created': '2014-06-23 20:33:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/970f4920b80f798d049db4249b01c1747d5f639e', 'message': ""Add validation to Resource Usage daily report\n\nThe limit project count field doesn't have any validation and\ncurrently fails if the value supplied can't be cast by\npython to an int.\n\nSolution is to add some jQuery based validation to make sure\nthat field contains nothing but digits.  Also change the field\nto numeric type so modern browsers will handle it better.\n\nChange-Id: I47688c2342b8ce6c1e7728f53111d8f4ce6bb98e\nCloses-bug: #1288512\n""}, {'number': 2, 'created': '2014-06-26 23:46:38.000000000', 'files': ['openstack_dashboard/dashboards/admin/metering/templates/metering/daily.html'], 'web_link': 'https://opendev.org/openstack/horizon/commit/f7103b7daa5944bddf0cd81c66e2cef0f5bb0ba1', 'message': ""Add validation to Resource Usage daily report\n\nThe limit project count field doesn't have any validation and\ncurrently fails if the value supplied can't be cast by\npython to an int, or if the value is 0.\n\nSolution is to add some jQuery based validation to make sure\nthat field contains nothing but digits, and is non-zero.  Also\nchange the field to numeric type so modern browsers will handle\nit better.\n\nChange-Id: I47688c2342b8ce6c1e7728f53111d8f4ce6bb98e\nCloses-bug: #1288512\n""}]",2,102003,f7103b7daa5944bddf0cd81c66e2cef0f5bb0ba1,20,4,2,9981,,,0,"Add validation to Resource Usage daily report

The limit project count field doesn't have any validation and
currently fails if the value supplied can't be cast by
python to an int, or if the value is 0.

Solution is to add some jQuery based validation to make sure
that field contains nothing but digits, and is non-zero.  Also
change the field to numeric type so modern browsers will handle
it better.

Change-Id: I47688c2342b8ce6c1e7728f53111d8f4ce6bb98e
Closes-bug: #1288512
",git fetch https://review.opendev.org/openstack/horizon refs/changes/03/102003/2 && git format-patch -1 --stdout FETCH_HEAD,['openstack_dashboard/dashboards/admin/metering/templates/metering/daily.html'],1,970f4920b80f798d049db4249b01c1747d5f639e,bug/1288512," <form id=""ceilometer-report-form"" class=""form-horizontal"" action=""{% url 'horizon:admin:metering:report' %}"" method=""POST""> <div class=""control-group"" id=""limit_project_count""> <input type=""number"" name=""limit"" class=""span2 example"" value=""20""/> <span class=""help-inline"" style=""display: none;"">{% trans ""Enter a whole number"" %}</span> check_limit_numeric(); check_limit_numeric(); function check_limit_numeric() { $(""#ceilometer-report-form"").submit(function(evt) { var limit_project_count = $(""#limit_project_count input"").val(); if (/\D/.test(limit_project_count)) { $(""#limit_project_count"").addClass(""error""); $(""#limit_project_count .help-inline"").css(""display"", ""inline-block"") evt.preventDefault(); return; } $(""#limit_project_count"").removeClass(""error""); $(""#limit_project_count .help-inline"").hide(); }) } "," <form class=""form-horizontal"" action=""{% url 'horizon:admin:metering:report' %}"" method=""POST""> <div class=""control-group""> <input type=""text"" name=""limit"" class=""span2 example"" value=""20""/>",22,3
openstack%2Fneutron-specs~master~Ic6bd4eda996bd6b145d1b9cfbee020e0e2a8cec0,openstack/neutron-specs,master,Ic6bd4eda996bd6b145d1b9cfbee020e0e2a8cec0,Setup the neutron-specs repository for Kilo specs.,MERGED,2014-09-25 19:55:13.000000000,2014-09-26 15:30:42.000000000,2014-09-26 15:30:41.000000000,"[{'_account_id': 3}, {'_account_id': 2592}, {'_account_id': 5948}]","[{'number': 1, 'created': '2014-09-25 19:55:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/558e9704b4e79f489eb483b2fa53edde0ff419e4', 'message': 'Setup the neutron-specs repository for Kilo specs.\n\nThis commit adds the kilo directory for Neutron specs.\n\nChange-Id: Ic6bd4eda996bd6b145d1b9cfbee020e0e2a8cec0\n'}, {'number': 2, 'created': '2014-09-25 20:55:31.000000000', 'files': ['specs/kilo/example.rst', 'doc/source/index.rst', 'specs/kilo/dummy.rst'], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/31945d8d1d4410421268330bc41f0a70bbb0e47b', 'message': 'Setup the neutron-specs repository for Kilo specs.\n\nThis commit adds the kilo directory for Neutron specs.\n\nChange-Id: Ic6bd4eda996bd6b145d1b9cfbee020e0e2a8cec0\n'}]",0,124158,31945d8d1d4410421268330bc41f0a70bbb0e47b,11,3,2,105,,,0,"Setup the neutron-specs repository for Kilo specs.

This commit adds the kilo directory for Neutron specs.

Change-Id: Ic6bd4eda996bd6b145d1b9cfbee020e0e2a8cec0
",git fetch https://review.opendev.org/openstack/neutron-specs refs/changes/58/124158/1 && git format-patch -1 --stdout FETCH_HEAD,"['specs/kilo/example.rst', 'doc/source/index.rst', 'specs/kilo/dummy.rst']",3,558e9704b4e79f489eb483b2fa53edde0ff419e4,specs/kilo,.. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Title of your blueprint ========================================== Problem description =================== Proposed change =============== Alternatives ------------ Data model impact ----------------- REST API impact --------------- Security impact --------------- Notifications impact -------------------- Other end user impact --------------------- Performance Impact ------------------ Other deployer impact --------------------- Developer impact ---------------- Implementation ============== Assignee(s) ----------- Work Items ---------- Dependencies ============ Testing ======= Documentation Impact ==================== References ========== ,,84,0
openstack%2Fneutron-specs~master~Ie5e0c2779e69585c1800d6ecaf9062f3d2d81cbb,openstack/neutron-specs,master,Ie5e0c2779e69585c1800d6ecaf9062f3d2d81cbb,Reformat template.rst.,MERGED,2014-09-25 19:55:13.000000000,2014-09-26 15:27:52.000000000,2014-09-26 15:27:51.000000000,"[{'_account_id': 3}, {'_account_id': 2592}, {'_account_id': 5948}]","[{'number': 1, 'created': '2014-09-25 19:55:13.000000000', 'files': ['tests/test_titles.py', 'specs/template.rst', 'specs/skeleton.rst'], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/8a524f54c7a73b35076b2dd6d003e897f14c7635', 'message': 'Reformat template.rst.\n\nChange the layout of this file to move the formatting examples to the bottom\nto move clearly highlight the operator requirements section at the top. Also\nupdate the test infrastructure to reflect the changes, and also only run\nunit tests on the kilo specs.\n\nChange-Id: Ie5e0c2779e69585c1800d6ecaf9062f3d2d81cbb\n'}]",0,124157,8a524f54c7a73b35076b2dd6d003e897f14c7635,8,3,1,105,,,0,"Reformat template.rst.

Change the layout of this file to move the formatting examples to the bottom
to move clearly highlight the operator requirements section at the top. Also
update the test infrastructure to reflect the changes, and also only run
unit tests on the kilo specs.

Change-Id: Ie5e0c2779e69585c1800d6ecaf9062f3d2d81cbb
",git fetch https://review.opendev.org/openstack/neutron-specs refs/changes/57/124157/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/test_titles.py', 'specs/template.rst', 'specs/skeleton.rst']",3,8a524f54c7a73b35076b2dd6d003e897f14c7635,specs/kilo,Problem DescriptionProposed ChangeData Model ImpactREST API ImpactSecurity ImpactNotifications ImpactOther End User ImpactOther Deployer ImpactDeveloper ImpactCommunity Impact ---------------- Alternatives ------------ Tempest Tests ------------- Functional Tests ---------------- API Tests --------- User Documentation ------------------ Developer Documentation ----------------------- ,Problem descriptionProposed changeAlternatives ------------ Data model impactREST API impactSecurity impactNotifications impactOther end user impactOther deployer impactDeveloper impact,409,335
openstack%2Fcookbook-openstack-common~stable%2Ficehouse~I4eff0fa8d38a5689b184b929f7d447ef8a307ebd,openstack/cookbook-openstack-common,stable/icehouse,I4eff0fa8d38a5689b184b929f7d447ef8a307ebd,Updated for UTF8 issue,ABANDONED,2014-09-26 15:21:05.000000000,2014-09-26 15:26:40.000000000,,[],"[{'number': 1, 'created': '2014-09-26 15:21:05.000000000', 'files': ['CHANGELOG.md', 'metadata.rb'], 'web_link': 'https://opendev.org/openstack/cookbook-openstack-common/commit/9a23085118be6de6d6cae5b85032a0fb170b973d', 'message': 'Updated for UTF8 issue\n\n- This bumps the cookbook for common so the new Berks.lock file can\npull down the UTF8 glance fix.\n- Updated CHANGELOG.md to reflect this\n\nChange-Id: I4eff0fa8d38a5689b184b929f7d447ef8a307ebd\nRelated-Bug: #1356887\n'}]",0,124441,9a23085118be6de6d6cae5b85032a0fb170b973d,2,0,1,12323,,,0,"Updated for UTF8 issue

- This bumps the cookbook for common so the new Berks.lock file can
pull down the UTF8 glance fix.
- Updated CHANGELOG.md to reflect this

Change-Id: I4eff0fa8d38a5689b184b929f7d447ef8a307ebd
Related-Bug: #1356887
",git fetch https://review.opendev.org/openstack/cookbook-openstack-common refs/changes/41/124441/1 && git format-patch -1 --stdout FETCH_HEAD,"['CHANGELOG.md', 'metadata.rb']",2,9a23085118be6de6d6cae5b85032a0fb170b973d,bug/1356887,version '9.7.2',version '9.7.1',3,1
openstack%2Ftripleo-image-elements~master~I7e85fd88b3ef5c5c1f3bc7813cf4112c1af20ebf,openstack/tripleo-image-elements,master,I7e85fd88b3ef5c5c1f3bc7813cf4112c1af20ebf,Passthrough enable heat.conf,MERGED,2014-09-26 12:56:14.000000000,2014-09-26 15:22:51.000000000,2014-09-26 15:22:50.000000000,"[{'_account_id': 3}, {'_account_id': 4330}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-26 12:56:14.000000000', 'files': ['elements/heat/os-apply-config/etc/heat/heat.conf'], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/011005cfda511c88dc82e952ebcb0b00fac98e8c', 'message': 'Passthrough enable heat.conf\n\nAdds ability to apply passthrough configuration to the Heat\nconfiguration file, like our other services.\n\nChange-Id: I7e85fd88b3ef5c5c1f3bc7813cf4112c1af20ebf\n'}]",0,124393,011005cfda511c88dc82e952ebcb0b00fac98e8c,8,3,1,7144,,,0,"Passthrough enable heat.conf

Adds ability to apply passthrough configuration to the Heat
configuration file, like our other services.

Change-Id: I7e85fd88b3ef5c5c1f3bc7813cf4112c1af20ebf
",git fetch https://review.opendev.org/openstack/tripleo-image-elements refs/changes/93/124393/1 && git format-patch -1 --stdout FETCH_HEAD,['elements/heat/os-apply-config/etc/heat/heat.conf'],1,011005cfda511c88dc82e952ebcb0b00fac98e8c,heat-conf,{{#heat}} {{#config}} [{{{section}}}] {{#values}} {{#comment}} # {{{.}}} {{/comment}} {{#option}} {{{option}}} = {{{value}}} {{/option}} {{/values}} {{/config}} {{/heat}},,13,0
openstack%2Ffuel-library~master~Ie48379042121b8e2fc0de81414b6a6b9e0fa2491,openstack/fuel-library,master,Ie48379042121b8e2fc0de81414b6a6b9e0fa2491,Change require to subscribe for zabbix service,MERGED,2014-09-24 13:46:24.000000000,2014-09-26 14:58:36.000000000,2014-09-26 14:58:35.000000000,"[{'_account_id': 3}, {'_account_id': 8786}, {'_account_id': 8971}, {'_account_id': 9037}, {'_account_id': 9546}, {'_account_id': 10392}]","[{'number': 1, 'created': '2014-09-24 13:46:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/c20919008d82504287cb07419e7128d479504b6e', 'message': 'Change require to subscribe for zabbix service\n\nZabbix service now subscribed to changes in zabbix_server.conf.\n\nChange-Id: Ie48379042121b8e2fc0de81414b6a6b9e0fa2491\nCloses-Bug: #1373387\n'}, {'number': 2, 'created': '2014-09-24 15:35:49.000000000', 'files': ['deployment/puppet/zabbix/manifests/server.pp'], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/b73c59c04d043efd02b3a530778925150946bb76', 'message': 'Change require to subscribe for zabbix service\n\nZabbix service now subscribed to changes in zabbix_server.conf.\n\nChange-Id: Ie48379042121b8e2fc0de81414b6a6b9e0fa2491\nCloses-Bug: #1373387\n'}]",0,123732,b73c59c04d043efd02b3a530778925150946bb76,18,6,2,11827,,,0,"Change require to subscribe for zabbix service

Zabbix service now subscribed to changes in zabbix_server.conf.

Change-Id: Ie48379042121b8e2fc0de81414b6a6b9e0fa2491
Closes-Bug: #1373387
",git fetch https://review.opendev.org/openstack/fuel-library refs/changes/32/123732/2 && git format-patch -1 --stdout FETCH_HEAD,['deployment/puppet/zabbix/manifests/server.pp'],1,c20919008d82504287cb07419e7128d479504b6e,bug/1373387," subscribe => File[$zabbix::params::server_config],"," require => File[$zabbix::params::server_config],",1,1
openstack%2Fkeystone~master~Ifa66500249e9d2426cde19667d092b36a8855337,openstack/keystone,master,Ifa66500249e9d2426cde19667d092b36a8855337,Add expected status for all HEAD requests in tests,ABANDONED,2014-09-26 01:44:00.000000000,2014-09-26 14:56:55.000000000,,"[{'_account_id': 3}, {'_account_id': 2903}, {'_account_id': 6482}]","[{'number': 1, 'created': '2014-09-26 01:44:00.000000000', 'files': ['keystone/tests/test_v3_identity.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/5de6935e07aa7bdb45c482ad813032d73a2df84a', 'message': 'Add expected status for all HEAD requests in tests\n\nWe recently had some inconsistencies between responses for GET and HEAD\nmethods for calls that support both methods. This inconsistency was made\nobvious when deploying Keystone in httpd since HEAD requests are converted\nto GET requests. In order to ensure that our reponse status is correct for\nHEAD calls, we should check the status in our tests.\n\nThis patch adds expected status checks for all HEAD requests that were\nnot previously checked.\nRelated-bug: #1334368\n\nChange-Id: Ifa66500249e9d2426cde19667d092b36a8855337\n'}]",0,124244,5de6935e07aa7bdb45c482ad813032d73a2df84a,4,3,1,9098,,,0,"Add expected status for all HEAD requests in tests

We recently had some inconsistencies between responses for GET and HEAD
methods for calls that support both methods. This inconsistency was made
obvious when deploying Keystone in httpd since HEAD requests are converted
to GET requests. In order to ensure that our reponse status is correct for
HEAD calls, we should check the status in our tests.

This patch adds expected status checks for all HEAD requests that were
not previously checked.
Related-bug: #1334368

Change-Id: Ifa66500249e9d2426cde19667d092b36a8855337
",git fetch https://review.opendev.org/openstack/keystone refs/changes/44/124244/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/tests/test_v3_identity.py'],1,5de6935e07aa7bdb45c482ad813032d73a2df84a,head_response," 'group_id': self.group_id, 'user_id': self.user['id']}, expected_status=204) self.head(member_url, expected_status=204) self.head(member_url, expected_status=204) self.head(member_url, expected_status=204) self.head(member_url, expected_status=204) self.head(member_url, expected_status=204) self.head(member_url, expected_status=204) self.head(member_url, expected_status=204) self.head(member_url, expected_status=204) self.head(member_url, expected_status=204) self.head(member_url, expected_status=204)"," 'group_id': self.group_id, 'user_id': self.user['id']}) self.head(member_url) self.head(member_url) self.head(member_url) self.head(member_url) self.head(member_url) self.head(member_url) self.head(member_url) self.head(member_url) self.head(member_url) self.head(member_url)",12,11
openstack%2Fmurano-agent~master~Ifedfbbc339b1eb00fd2584aef141cce7c87e1616,openstack/murano-agent,master,Ifedfbbc339b1eb00fd2584aef141cce7c87e1616,Windows agent was moved to another repo,MERGED,2014-09-25 19:31:01.000000000,2014-09-26 14:55:00.000000000,2014-09-26 14:55:00.000000000,"[{'_account_id': 3}, {'_account_id': 7225}, {'_account_id': 7600}]","[{'number': 1, 'created': '2014-09-25 19:31:01.000000000', 'files': ['contrib/windows-agent/ExecutionPlanGenerator/Program.cs', 'contrib/windows-agent/ExecutionPlanGenerator/packages.config', 'contrib/windows-agent/ExecutionPlanGenerator/App.config', 'contrib/windows-agent/WindowsAgent/RabbitMqClient.cs', 'contrib/windows-agent/WindowsAgent/ServiceManager.cs', 'contrib/windows-agent/ExecutionPlanGenerator/Properties/AssemblyInfo.cs', 'contrib/windows-agent/WindowsAgent/ExecutionPlan.cs', 'contrib/windows-agent/WindowsAgent/Properties/AssemblyInfo.cs', 'contrib/windows-agent/WindowsAgent.sln', 'contrib/windows-agent/Tools/NuGet.exe', 'contrib/windows-agent/WindowsAgent/Program.cs', 'contrib/windows-agent/WindowsAgent/PlanExecutor.cs', 'contrib/windows-agent/WindowsAgent/WindowsAgent.csproj', 'contrib/windows-agent/WindowsAgent/MqMessage.cs', 'contrib/windows-agent/WindowsAgent/WindowsServiceInstaller.cs', 'contrib/windows-agent/ExecutionPlanGenerator/ExecutionPlanGenerator.csproj', 'contrib/windows-agent/WindowsAgent/SampleExecutionPlan.json', 'contrib/windows-agent/packages/repositories.config', 'contrib/windows-agent/WindowsAgent/App.config', 'contrib/windows-agent/WindowsAgent/WindowsService.cs', 'contrib/windows-agent/WindowsAgent/packages.config'], 'web_link': 'https://opendev.org/openstack/murano-agent/commit/87367a40205f1df0ac730c1e32b1376ee8c82226', 'message': 'Windows agent was moved to another repo\n\nNow it is at https://github.com/murano-project/murano-windows-agent\n\nChange-Id: Ifedfbbc339b1eb00fd2584aef141cce7c87e1616\n'}]",0,124146,87367a40205f1df0ac730c1e32b1376ee8c82226,8,3,1,7226,,,0,"Windows agent was moved to another repo

Now it is at https://github.com/murano-project/murano-windows-agent

Change-Id: Ifedfbbc339b1eb00fd2584aef141cce7c87e1616
",git fetch https://review.opendev.org/openstack/murano-agent refs/changes/46/124146/1 && git format-patch -1 --stdout FETCH_HEAD,"['contrib/windows-agent/ExecutionPlanGenerator/Program.cs', 'contrib/windows-agent/ExecutionPlanGenerator/packages.config', 'contrib/windows-agent/ExecutionPlanGenerator/App.config', 'contrib/windows-agent/WindowsAgent/RabbitMqClient.cs', 'contrib/windows-agent/WindowsAgent/ServiceManager.cs', 'contrib/windows-agent/ExecutionPlanGenerator/Properties/AssemblyInfo.cs', 'contrib/windows-agent/WindowsAgent/ExecutionPlan.cs', 'contrib/windows-agent/WindowsAgent/Properties/AssemblyInfo.cs', 'contrib/windows-agent/WindowsAgent.sln', 'contrib/windows-agent/Tools/NuGet.exe', 'contrib/windows-agent/WindowsAgent/Program.cs', 'contrib/windows-agent/WindowsAgent/PlanExecutor.cs', 'contrib/windows-agent/WindowsAgent/WindowsAgent.csproj', 'contrib/windows-agent/WindowsAgent/MqMessage.cs', 'contrib/windows-agent/WindowsAgent/WindowsServiceInstaller.cs', 'contrib/windows-agent/ExecutionPlanGenerator/ExecutionPlanGenerator.csproj', 'contrib/windows-agent/WindowsAgent/SampleExecutionPlan.json', 'contrib/windows-agent/packages/repositories.config', 'contrib/windows-agent/WindowsAgent/App.config', 'contrib/windows-agent/WindowsAgent/WindowsService.cs', 'contrib/windows-agent/WindowsAgent/packages.config']",21,87367a40205f1df0ac730c1e32b1376ee8c82226,,,"<?xml version=""1.0"" encoding=""utf-8""?> <packages> <package id=""Newtonsoft.Json"" version=""4.5.11"" targetFramework=""net45"" /> <package id=""NLog"" version=""2.0.0.2000"" targetFramework=""net45"" /> <package id=""RabbitMQ.Client"" version=""3.0.2"" targetFramework=""net45"" /> </packages>",0,1325
openstack%2Fopenstacksdk~master~Id8b09ea7863d402ccda5b33ad947cd54344cf85b,openstack/openstacksdk,master,Id8b09ea7863d402ccda5b33ad947cd54344cf85b,Add database users for instances.,MERGED,2014-09-16 00:58:50.000000000,2014-09-26 14:54:46.000000000,2014-09-26 14:54:45.000000000,"[{'_account_id': 3}, {'_account_id': 8257}, {'_account_id': 8736}, {'_account_id': 12807}]","[{'number': 1, 'created': '2014-09-16 00:58:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/fdeb4d704ee2fb7cc3b7afb61c7c5dccf98158a0', 'message': 'Add database users for instances.\n\nCreate, List, and Delete operations are supported. Because the password is\nwrite-on-create only and otherwise omitted, this property is fully implemented.\nCreate is reimplemented to format as a list, expected by the service.\n\nChange-Id: Id8b09ea7863d402ccda5b33ad947cd54344cf85b\n'}, {'number': 2, 'created': '2014-09-16 01:00:01.000000000', 'files': ['openstack/tests/database/v1/test_user.py', 'openstack/database/v1/user.py'], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/5b70cec03b40b0fe1e87ab6f2a93f18562a9df2f', 'message': 'Add database users for instances.\n\nCreate, List, and Delete operations are supported. Because the\npassword is write-on-create only and otherwise omitted, this property\nis fully implemented. Create is reimplemented to format as a list,\nexpected by the service.\n\nChange-Id: Id8b09ea7863d402ccda5b33ad947cd54344cf85b\n'}]",2,121722,5b70cec03b40b0fe1e87ab6f2a93f18562a9df2f,9,4,2,12807,,,0,"Add database users for instances.

Create, List, and Delete operations are supported. Because the
password is write-on-create only and otherwise omitted, this property
is fully implemented. Create is reimplemented to format as a list,
expected by the service.

Change-Id: Id8b09ea7863d402ccda5b33ad947cd54344cf85b
",git fetch https://review.opendev.org/openstack/openstacksdk refs/changes/22/121722/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack/tests/database/v1/test_user.py', 'openstack/database/v1/user.py']",2,fdeb4d704ee2fb7cc3b7afb61c7c5dccf98158a0,add-database-users,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from openstack.database import database_service from openstack import resource class User(resource.Resource): id_attribute = 'name' resource_key = 'user' resources_key = 'users' base_path = '/instances/%(instance_id)s/users' service = database_service.DatabaseService() # capabilities allow_create = True allow_delete = True allow_list = True # path args instance_id = resource.prop('instance_id') # Properties databases = resource.prop('databases') name = resource.prop('name') _password = resource.prop('password') @property def password(self): try: val = self._password except AttributeError: val = None return val @password.setter def password(self, val): self._password = val @classmethod def create_by_id(cls, session, attrs, r_id=None, path_args=None): url = cls.base_path % path_args # Create expects an array of users body = {'users': [attrs]} resp = session.post(url, service=cls.service, json=body).body return resp ",,124,0
openstack%2Foslo-specs~master~I69b372b3e4b584bc6fc4da48e2830130834d0206,openstack/oslo-specs,master,I69b372b3e4b584bc6fc4da48e2830130834d0206,Replace default bp URLs with portable examples,MERGED,2014-09-09 20:13:59.000000000,2014-09-26 14:52:01.000000000,2014-09-26 14:52:01.000000000,"[{'_account_id': 3}, {'_account_id': 2472}]","[{'number': 1, 'created': '2014-09-09 20:13:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/2769240093f1dd31362bf7b8e86fae34e6c14e75', 'message': 'Replace default bp URLs with portable examples\n\nUse the search feature of launchpad to find a blueprint that might move\naround within the Oslo project.\n\nChange-Id: I69b372b3e4b584bc6fc4da48e2830130834d0206\n'}, {'number': 2, 'created': '2014-09-26 13:51:22.000000000', 'files': ['specs/graduation-template.rst', 'specs/template.rst'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/663a18421eb7bdb04479bf92e6452775597b2fb7', 'message': 'Replace default bp URLs with portable examples\n\nUse the search feature of launchpad to find a blueprint that might move\naround within the Oslo project.\n\nChange-Id: I69b372b3e4b584bc6fc4da48e2830130834d0206\n'}]",0,120205,663a18421eb7bdb04479bf92e6452775597b2fb7,9,2,2,2472,,,0,"Replace default bp URLs with portable examples

Use the search feature of launchpad to find a blueprint that might move
around within the Oslo project.

Change-Id: I69b372b3e4b584bc6fc4da48e2830130834d0206
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/05/120205/2 && git format-patch -1 --stdout FETCH_HEAD,"['specs/graduation-template.rst', 'specs/template.rst']",2,2769240093f1dd31362bf7b8e86fae34e6c14e75,bp/URLs, https://blueprints.launchpad.net/oslo?searchtext=awesome-thing should behttps://blueprints.launchpad.net/oslo?searchtext=awesome-thing, https://blueprints.launchpad.net/oslo/+spec/awesome-thing should behttps://blueprints.launchpad.net/oslo/+spec/example,4,4
openstack%2Fmurano~master~Ic4cedd323ab7b55690d095ed8addcb0dc3e335a7,openstack/murano,master,Ic4cedd323ab7b55690d095ed8addcb0dc3e335a7,Fixes stealing of agent responses in some cases,MERGED,2014-09-15 14:35:16.000000000,2014-09-26 14:48:44.000000000,2014-09-26 14:48:43.000000000,"[{'_account_id': 3}, {'_account_id': 7225}, {'_account_id': 7226}, {'_account_id': 7227}, {'_account_id': 7562}, {'_account_id': 7600}, {'_account_id': 7821}, {'_account_id': 8443}, {'_account_id': 13149}]","[{'number': 1, 'created': '2014-09-15 14:35:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano/commit/2b9befef372f6842ad55db93f2026d86b83cf2d0', 'message': 'Fixes stealing of agent responses in some cases\n\nWhen deployment of environment fails with exception AgentListener.stop() is not\ncalled and thus there remains a listener for RabbitMQ response queue.\nBesides being a resource leak it introduces another problems: when that environment\nget redeployed it becomes 2 listeners on the same queue and responses from agents\nmay be stolen by zombie listener making workflow wait forever or response\n(hang deployment)\n\nChange-Id: Ic4cedd323ab7b55690d095ed8addcb0dc3e335a7\nCloses-Bug: #1369589\n'}, {'number': 2, 'created': '2014-09-18 13:58:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano/commit/7f3a3be2ba0826e19f961b4b02dda0c91ea6643f', 'message': 'Fixes stealing of agent responses in some cases\n\nWhen deployment of environment fails with exception AgentListener.stop() is not\ncalled and thus there remains a listener for RabbitMQ response queue.\nBesides being a resource leak it introduces another problems: when that environment\nget redeployed it becomes 2 listeners on the same queue and responses from agents\nmay be stolen by zombie listener making workflow wait forever or response\n(hang deployment)\n\nChange-Id: Ic4cedd323ab7b55690d095ed8addcb0dc3e335a7\nCloses-Bug: #1369589\n'}, {'number': 3, 'created': '2014-09-23 10:51:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano/commit/c1fd010432174367a5d3d5bbfa01f71a25382913', 'message': 'Fixes stealing of agent responses in some cases\n\nWhen deployment of environment fails with exception AgentListener.stop() is not\ncalled and thus there remains a listener for RabbitMQ response queue.\nBesides being a resource leak it introduces another problems: when that environment\nget redeployed it becomes 2 listeners on the same queue and responses from agents\nmay be stolen by zombie listener making workflow wait forever or response\n(hang deployment)\n\nChange-Id: Ic4cedd323ab7b55690d095ed8addcb0dc3e335a7\nCloses-Bug: #1369589\n'}, {'number': 4, 'created': '2014-09-23 12:00:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano/commit/6316bf0465a593ef76d2655b2f6b75ac5bcb4169', 'message': 'Fixes stealing of agent responses in some cases\n\nWhen deployment of environment fails with exception AgentListener.stop() is not\ncalled and thus there remains a listener for RabbitMQ response queue.\nBesides being a resource leak it introduces another problems: when that environment\nget redeployed it becomes 2 listeners on the same queue and responses from agents\nmay be stolen by zombie listener making workflow wait forever or response\n(hang deployment)\n\nChange-Id: Ic4cedd323ab7b55690d095ed8addcb0dc3e335a7\nCloses-Bug: #1369589\n'}, {'number': 5, 'created': '2014-09-25 12:50:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano/commit/49be50cbce446320d68d7b6cd207aa8fab46f46c', 'message': 'Fixes stealing of agent responses in some cases\n\nWhen deployment of environment fails with exception AgentListener.stop() is not\ncalled and thus there remains a listener for RabbitMQ response queue.\nBesides being a resource leak it introduces another problems: when that environment\nget redeployed it becomes 2 listeners on the same queue and responses from agents\nmay be stolen by zombie listener making workflow wait forever or response\n(hang deployment)\n\nChange-Id: Ic4cedd323ab7b55690d095ed8addcb0dc3e335a7\nCloses-Bug: #1369589\n'}, {'number': 6, 'created': '2014-09-25 13:13:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano/commit/6c0521a24c514777ed09cc637bfa28fc3ceabd66', 'message': 'Fixes stealing of agent responses in some cases\n\nWhen deployment of environment fails with exception AgentListener.stop() is not\ncalled and thus there remains a listener for RabbitMQ response queue.\nBesides being a resource leak it introduces another problems: when that environment\nget redeployed it becomes 2 listeners on the same queue and responses from agents\nmay be stolen by zombie listener making workflow wait forever or response\n(hang deployment)\n\nChange-Id: Ic4cedd323ab7b55690d095ed8addcb0dc3e335a7\nCloses-Bug: #1369589\n'}, {'number': 7, 'created': '2014-09-26 12:22:10.000000000', 'files': ['murano/dsl/executor.py', 'meta/io.murano/Classes/Environment.yaml'], 'web_link': 'https://opendev.org/openstack/murano/commit/6252069be09a32959eb1e9b313e4ba17e4131db4', 'message': 'Fixes stealing of agent responses in some cases\n\nWhen deployment of environment fails with exception AgentListener.stop() is not\ncalled and thus there remains a listener for RabbitMQ response queue.\nBesides being a resource leak it introduces another problems: when that environment\nget redeployed it becomes 2 listeners on the same queue and responses from agents\nmay be stolen by zombie listener making workflow wait forever or response\n(hang deployment)\n\nChange-Id: Ic4cedd323ab7b55690d095ed8addcb0dc3e335a7\nCloses-Bug: #1369589\n'}]",0,121590,6252069be09a32959eb1e9b313e4ba17e4131db4,52,9,7,7226,,,0,"Fixes stealing of agent responses in some cases

When deployment of environment fails with exception AgentListener.stop() is not
called and thus there remains a listener for RabbitMQ response queue.
Besides being a resource leak it introduces another problems: when that environment
get redeployed it becomes 2 listeners on the same queue and responses from agents
may be stolen by zombie listener making workflow wait forever or response
(hang deployment)

Change-Id: Ic4cedd323ab7b55690d095ed8addcb0dc3e335a7
Closes-Bug: #1369589
",git fetch https://review.opendev.org/openstack/murano refs/changes/90/121590/3 && git format-patch -1 --stdout FETCH_HEAD,['meta/io.murano/Classes/Environment.yaml'],1,2b9befef372f6842ad55db93f2026d86b83cf2d0,bug/1369589, Try: - $.agentListener.start() - If: len($.applications) = 0 Then: - $.stack.delete() Else: - $.applications.pselect($.deploy()) Finally: - $.agentListener.stop(), - $.agentListener.start() - If: len($.applications) = 0 Then: - $.stack.delete() Else: - $.applications.pselect($.deploy()) - $.agentListener.stop(),9,7
openstack%2Foslo-specs~master~Ib483d8fef9893129fb51f0113fe2a85bbb34dce2,openstack/oslo-specs,master,Ib483d8fef9893129fb51f0113fe2a85bbb34dce2,Fix the blueprint URLs in existing specs,MERGED,2014-09-09 20:09:59.000000000,2014-09-26 14:44:37.000000000,2014-09-26 14:44:36.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-09 20:09:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/710dcfe96b1f6865bdc6d119daa706b429d28548', 'message': 'Fix the blueprint URLs in existing specs\n\nAfter we shuffled launchpad stuff around, the blueprint pages moved so\ntheir URLs are different.\n\nChange-Id: Ib483d8fef9893129fb51f0113fe2a85bbb34dce2\n'}, {'number': 2, 'created': '2014-09-26 13:51:22.000000000', 'files': ['specs/juno/oslo-config-generator.rst', 'specs/juno/graduate-oslo-serialization.rst', 'specs/juno/add-tpool-proxy-wrapper.rst', 'specs/juno/use-events-for-error-wrapping.rst', 'specs/juno/graduate-oslo-middleware.rst', 'specs/juno/policy-configuration-directories.rst', 'specs/juno/graduate-oslo-concurrency.rst', 'specs/juno/graduate-oslo-log.rst', 'specs/juno/graduate-oslo-utils.rst', 'specs/juno/oslo-config-cfgfilter.rst', 'specs/juno/chaining-regexp-filter.rst', 'specs/juno/graduate-oslo-i18n.rst', 'specs/juno/rootwrap-daemon-mode.rst', 'specs/juno/pylockfile-adoption.rst'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/fe028d78eecca3f313a5ec7c7e54b25dbd3d40a1', 'message': 'Fix the blueprint URLs in existing specs\n\nAfter we shuffled launchpad stuff around, the blueprint pages moved so\ntheir URLs are different.\n\nChange-Id: Ib483d8fef9893129fb51f0113fe2a85bbb34dce2\n'}]",0,120203,fe028d78eecca3f313a5ec7c7e54b25dbd3d40a1,10,3,2,2472,,,0,"Fix the blueprint URLs in existing specs

After we shuffled launchpad stuff around, the blueprint pages moved so
their URLs are different.

Change-Id: Ib483d8fef9893129fb51f0113fe2a85bbb34dce2
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/03/120203/1 && git format-patch -1 --stdout FETCH_HEAD,"['specs/juno/oslo-config-generator.rst', 'specs/juno/graduate-oslo-serialization.rst', 'specs/juno/add-tpool-proxy-wrapper.rst', 'specs/juno/use-events-for-error-wrapping.rst', 'specs/juno/graduate-oslo-middleware.rst', 'specs/juno/policy-configuration-directories.rst', 'specs/juno/graduate-oslo-concurrency.rst', 'specs/juno/graduate-oslo-log.rst', 'specs/juno/enable-mysql-connector.rst', 'specs/juno/graduate-oslo-utils.rst', 'specs/juno/oslo-config-cfgfilter.rst', 'specs/juno/chaining-regexp-filter.rst', 'specs/juno/graduate-oslo-i18n.rst', 'specs/juno/rootwrap-daemon-mode.rst', 'specs/juno/pylockfile-adoption.rst']",15,710dcfe96b1f6865bdc6d119daa706b429d28548,bp/URLs,https://blueprints.launchpad.net/pylockfile/+spec/pylockfile-adoption,https://blueprints.launchpad.net/oslo/+spec/pylockfile-adoption,15,15
openstack%2Ffuel-library~master~I5b4ea431d6907e4d57d0f753d3720a470583c77f,openstack/fuel-library,master,I5b4ea431d6907e4d57d0f753d3720a470583c77f,multiple start rescheduling after migration L3/DHCP agent,MERGED,2014-09-22 18:50:46.000000000,2014-09-26 14:41:23.000000000,2014-09-26 14:41:23.000000000,"[{'_account_id': 3}, {'_account_id': 6072}, {'_account_id': 6719}, {'_account_id': 6926}, {'_account_id': 7468}, {'_account_id': 7604}, {'_account_id': 8392}, {'_account_id': 8786}, {'_account_id': 8971}, {'_account_id': 9037}]","[{'number': 1, 'created': '2014-09-22 18:50:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/9253c27a4b61ace1deffe9aec82f6af80e2a48ac', 'message': 'multiple start rescheduling after migration L3/DHCP agent\n\nThis workaround is safe, because starting rescheduling on alive agent do nothing\n\nChange-Id: I5b4ea431d6907e4d57d0f753d3720a470583c77f\nCloses-bug: #1371561\n'}, {'number': 2, 'created': '2014-09-25 22:08:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/9e37005696f4a2bd7eee3fdb3a5aee34b30fb1ae', 'message': 'multiple start rescheduling after migration L3/DHCP agent\n\nThis workaround is safe, because starting rescheduling on alive agent do nothing\n\nChange-Id: I5b4ea431d6907e4d57d0f753d3720a470583c77f\nCloses-bug: #1371561\n'}, {'number': 3, 'created': '2014-09-26 12:31:30.000000000', 'files': ['deployment/puppet/neutron/files/ocf/neutron-agent-dhcp', 'deployment/puppet/neutron/files/ocf/neutron-agent-l3'], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/cbe6de69f2e0d55ed4ebb47a1691e642e8c8787e', 'message': 'multiple start rescheduling after migration L3/DHCP agent\n\nThis workaround is safe, because starting rescheduling on alive agent do nothing\n\nChange-Id: I5b4ea431d6907e4d57d0f753d3720a470583c77f\nCloses-bug: #1371561\n'}]",2,123217,cbe6de69f2e0d55ed4ebb47a1691e642e8c8787e,30,10,3,8786,,,0,"multiple start rescheduling after migration L3/DHCP agent

This workaround is safe, because starting rescheduling on alive agent do nothing

Change-Id: I5b4ea431d6907e4d57d0f753d3720a470583c77f
Closes-bug: #1371561
",git fetch https://review.opendev.org/openstack/fuel-library refs/changes/17/123217/1 && git format-patch -1 --stdout FETCH_HEAD,"['deployment/puppet/neutron/files/ocf/neutron-agent-dhcp', 'deployment/puppet/neutron/files/ocf/neutron-agent-l3']",2,9253c27a4b61ace1deffe9aec82f6af80e2a48ac,bp/merge-openstack-puppet-modules,"OCF_RESKEY_rescheduling_tries_default=3 OCF_RESKEY_rescheduling_interspace_default=33: ${OCF_RESKEY_rescheduling_tries=${OCF_RESKEY_rescheduling_tries_default}} : ${OCF_RESKEY_rescheduling_interspace=${OCF_RESKEY_rescheduling_interspace_default}}<parameter name=""rescheduling_tries"" unique=""0"" required=""0""> <longdesc lang=""en""> Tries to start rescheduling script after start of agent. </longdesc> <shortdesc lang=""en"">Tries to start rescheduling script after start of agent.</shortdesc> <content type=""boolean"" default=""${OCF_RESKEY_rescheduling_tries_default}""/> </parameter> <parameter name=""rescheduling_interspace"" unique=""0"" required=""0""> <longdesc lang=""en""> Interspace between starts of rescheduling script. </longdesc> <shortdesc lang=""en"">Interspace between starts of rescheduling script.</shortdesc> <content type=""boolean"" default=""${OCF_RESKEY_rescheduling_interspace_default}""/> </parameter> RESCHEDULING_CMD=""q-agent-cleanup.py --agent=l3 --reschedule --remove-dead ${AUTH_TAIL} 2>&1 >> /var/log/neutron/rescheduling.log"" RESCH_CMD='' for ((i=0; i<$OCF_RESKEY_rescheduling_tries; i++)) ; do RESCH_CMD=""$RESCH_CMD sleep $OCF_RESKEY_rescheduling_interspace ; $RESCHEDULING_CMD ;"" done bash -c ""$RESCH_CMD"" &"," bash -c ""sleep 33 ; q-agent-cleanup.py --agent=l3 --reschedule --remove-dead ${AUTH_TAIL} 2>&1 >> /var/log/neutron/rescheduling.log "" &",51,2
openstack%2Ftraining-guides~master~Ifa5e23b3d9b0af321b907ab75bd62b171555750b,openstack/training-guides,master,Ifa5e23b3d9b0af321b907ab75bd62b171555750b,labs: reorder setup_neutron_network.sh,MERGED,2014-09-23 10:20:04.000000000,2014-09-26 14:34:33.000000000,2014-09-26 14:34:32.000000000,"[{'_account_id': 3}, {'_account_id': 7007}, {'_account_id': 11109}, {'_account_id': 11889}]","[{'number': 1, 'created': '2014-09-23 10:20:04.000000000', 'files': ['labs/scripts/setup_neutron_network.sh'], 'web_link': 'https://opendev.org/openstack/training-guides/commit/64c46a96cb4f278921627142801f8acda799b68e', 'message': 'labs: reorder setup_neutron_network.sh\n\nThis patch reorders the content of setup_neutron_network.sh to more\nclosely match the order of instructions in the install-guide, making it\neasier to compare script and instructions.\n\nChange-Id: Ifa5e23b3d9b0af321b907ab75bd62b171555750b\nCo-Authored-By: Pranav Salunke <dguitarbite@gmail.com>\n'}]",0,123407,64c46a96cb4f278921627142801f8acda799b68e,10,4,1,11109,,,0,"labs: reorder setup_neutron_network.sh

This patch reorders the content of setup_neutron_network.sh to more
closely match the order of instructions in the install-guide, making it
easier to compare script and instructions.

Change-Id: Ifa5e23b3d9b0af321b907ab75bd62b171555750b
Co-Authored-By: Pranav Salunke <dguitarbite@gmail.com>
",git fetch https://review.opendev.org/openstack/training-guides refs/changes/07/123407/1 && git format-patch -1 --stdout FETCH_HEAD,['labs/scripts/setup_neutron_network.sh'],1,64c46a96cb4f278921627142801f8acda799b68e,reorder_neutron_network," # Configuring [keystone_authtoken] section iniset_sudo $conf keystone_authtoken auth_uri ""http://controller-mgmt:5000"" iniset_sudo $conf keystone_authtoken auth_host controller-mgmt iniset_sudo $conf keystone_authtoken auth_protocol http iniset_sudo $conf keystone_authtoken auth_port 35357 iniset_sudo $conf keystone_authtoken admin_tenant_name ""$SERVICE_TENANT_NAME"" iniset_sudo $conf keystone_authtoken admin_user ""$neutron_admin_user"" iniset_sudo $conf keystone_authtoken admin_password ""$neutron_admin_password""iniset_sudo $conf DEFAULT verbose True echo ""Configuring Layer-3 agent."" conf=/etc/neutron/l3_agent.ini iniset_sudo $conf DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriver iniset_sudo $conf DEFAULT use_namespaces True iniset_sudo $conf DEFAULT verbose True echo ""Configuring the DHCP agent"" conf=/etc/neutron/dhcp_agent.ini iniset_sudo $conf DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriver iniset_sudo $conf DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasq iniset_sudo $conf DEFAULT use_namespaces True iniset_sudo $conf DEFAULT verbose True iniset_sudo $conf DEFAULT dnsmasq_config_file /etc/neutron/dnsmasq-neutron.conf if [ -n ""${TENANT_VM_DNS_SERVER:-''}"" ]; then iniset_sudo $conf DEFAULT dnsmasq_dns_servers ""$TENANT_VM_DNS_SERVER"" fi cat << DNSMASQ | sudo tee /etc/neutron/dnsmasq-neutron.conf # Set interface MTU to 1454 (for instance, ssh authentication may fail # otherwise due to GRE overhead) dhcp-option-force=26,1454 # Override --no-hosts dnsmasq option supplied by neutron addn-hosts=/etc/hosts # Log dnsmasq queries to syslog log-queries # Verbose logging for DHCP log-dhcp DNSMASQ killall dnsmasq echo ""Configuring the metadata agent"" conf=/etc/neutron/metadata_agent.ini iniset_sudo $conf DEFAULT auth_url http://controller-mgmt:5000/v2.0 iniset_sudo $conf DEFAULT auth_region regionOne iniset_sudo $conf DEFAULT admin_tenant_name ""$SERVICE_TENANT_NAME"" iniset_sudo $conf DEFAULT admin_user ""$neutron_admin_user"" iniset_sudo $conf DEFAULT admin_password ""$neutron_admin_password"" iniset_sudo $conf DEFAULT nova_metadata_ip ""$(hostname_to_ip controller-mgmt)"" iniset_sudo $conf DEFAULT metadata_proxy_shared_secret ""$METADATA_SECRET""# Under the securitygroup section iniset_sudo $conf securitygroup firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver iniset_sudo $conf securitygroup enable_security_group True ","iniset_sudo $conf DEFAULT verbose True# Configuring [keystone_authtoken] section iniset_sudo $conf keystone_authtoken auth_uri ""http://controller-mgmt:5000"" iniset_sudo $conf keystone_authtoken auth_host controller-mgmt iniset_sudo $conf keystone_authtoken auth_port 35357 iniset_sudo $conf keystone_authtoken auth_protocol http iniset_sudo $conf keystone_authtoken admin_tenant_name ""$SERVICE_TENANT_NAME"" iniset_sudo $conf keystone_authtoken admin_user ""$neutron_admin_user"" iniset_sudo $conf keystone_authtoken admin_password ""$neutron_admin_password""# Under the securitygroup section iniset_sudo $conf securitygroup firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver iniset_sudo $conf securitygroup enable_security_group True echo ""Configuring Layer-3 agent."" conf=/etc/neutron/l3_agent.ini iniset_sudo $conf DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriver iniset_sudo $conf DEFAULT use_namespaces True iniset_sudo $conf DEFAULT verbose True echo ""Configuring the metadata agent"" conf=/etc/neutron/metadata_agent.ini iniset_sudo $conf DEFAULT auth_url http://controller-mgmt:5000/v2.0 iniset_sudo $conf DEFAULT auth_region regionOne iniset_sudo $conf DEFAULT admin_tenant_name ""$SERVICE_TENANT_NAME"" iniset_sudo $conf DEFAULT admin_user ""$neutron_admin_user"" iniset_sudo $conf DEFAULT admin_password ""$neutron_admin_password"" iniset_sudo $conf DEFAULT nova_metadata_ip ""$(hostname_to_ip controller-mgmt)"" iniset_sudo $conf DEFAULT metadata_proxy_shared_secret ""$METADATA_SECRET"" echo ""Configuring the DHCP agent"" conf=/etc/neutron/dhcp_agent.ini iniset_sudo $conf DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriver iniset_sudo $conf DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasq iniset_sudo $conf DEFAULT use_namespaces True iniset_sudo $conf DEFAULT verbose True iniset_sudo $conf DEFAULT dnsmasq_config_file /etc/neutron/dnsmasq-neutron.conf if [ -n ""${TENANT_VM_DNS_SERVER:-''}"" ]; then iniset_sudo $conf DEFAULT dnsmasq_dns_servers ""$TENANT_VM_DNS_SERVER"" fi cat << DNSMASQ | sudo tee /etc/neutron/dnsmasq-neutron.conf # Set interface MTU to 1454 (for instance, ssh authentication may fail # otherwise due to GRE overhead) dhcp-option-force=26,1454 # Override --no-hosts dnsmasq option supplied by neutron addn-hosts=/etc/hosts # Log dnsmasq queries to syslog log-queries # Verbose logging for DHCP log-dhcp DNSMASQ killall dnsmasq ",57,57
openstack%2Fnova~stable%2Ficehouse~I9c90a410a7ecb91f5a4de28acee21fe7da49242c,openstack/nova,stable/icehouse,I9c90a410a7ecb91f5a4de28acee21fe7da49242c,libvirt: Use VIR_DOMAIN_AFFECT_LIVE for paused instances,MERGED,2014-06-25 07:47:13.000000000,2014-09-26 14:30:16.000000000,2014-09-26 14:30:13.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 1779}, {'_account_id': 5170}, {'_account_id': 8247}, {'_account_id': 9008}, {'_account_id': 9656}, {'_account_id': 9796}, {'_account_id': 10118}, {'_account_id': 11531}]","[{'number': 1, 'created': '2014-06-25 07:47:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/0ff862ef3bf55b21fbdea58fe94b1a5e6c67e8ff', 'message': ""libvirt: Use VIR_DOMAIN_AFFECT_LIVE for paused instances\n\nWhen a volume is attached to a paused instance, it does not\nappear in the instance's block devices list (i.e. lsblk)\nafter the instance has resumed. A similar situation\nhappens when detaching a volume from a paused instance; the\nblock device continues to exists in the block devices list,\nwhere it should not.  It was found that the volume is only\npersisted in the domain's config settings and not its active\nsettings, since only the VIR_DOMAIN_AFFECT_CONFIG flag was used\non attach and detach volume.  In order to affect the active\nsettings, the VIR_DOMAIN_AFFECT_LIVE flag has to be added when\nattaching and detaching volumes.\n\nChange-Id: I9c90a410a7ecb91f5a4de28acee21fe7da49242c\nCloses-Bug: #1242366\nRelated-Bug: #1299331\n(cherry picked from commit ead9cfca93c9b5ca55e3ba269213c98d5e6e1d38)\n""}, {'number': 2, 'created': '2014-06-25 08:48:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3437d7edefd03cf5184a0a116a1c4c1d88823556', 'message': ""libvirt: Use VIR_DOMAIN_AFFECT_LIVE for paused instances\n\nWhen a volume is attached to a paused instance, it does not\nappear in the instance's block devices list (i.e. lsblk)\nafter the instance has resumed. A similar situation\nhappens when detaching a volume from a paused instance; the\nblock device continues to exists in the block devices list,\nwhere it should not.  It was found that the volume is only\npersisted in the domain's config settings and not its active\nsettings, since only the VIR_DOMAIN_AFFECT_CONFIG flag was used\non attach and detach volume.  In order to affect the active\nsettings, the VIR_DOMAIN_AFFECT_LIVE flag has to be added when\nattaching and detaching volumes.\n\nThis change adjusts little about the unit test:\nadd two imports.\n\nChange-Id: I9c90a410a7ecb91f5a4de28acee21fe7da49242c\nCloses-Bug: #1242366\nRelated-Bug: #1299331\n(cherry picked from commit ead9cfca93c9b5ca55e3ba269213c98d5e6e1d38)\n""}, {'number': 3, 'created': '2014-07-10 02:07:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b908b508c5ccb125f14e96cb674683deaa191a8e', 'message': ""libvirt: Use VIR_DOMAIN_AFFECT_LIVE for paused instances\n\nWhen a volume is attached to a paused instance, it does not\nappear in the instance's block devices list (i.e. lsblk)\nafter the instance has resumed. A similar situation\nhappens when detaching a volume from a paused instance; the\nblock device continues to exists in the block devices list,\nwhere it should not.  It was found that the volume is only\npersisted in the domain's config settings and not its active\nsettings, since only the VIR_DOMAIN_AFFECT_CONFIG flag was used\non attach and detach volume.  In order to affect the active\nsettings, the VIR_DOMAIN_AFFECT_LIVE flag has to be added when\nattaching and detaching volumes.\n\nThis change adjusts little about the unit test:\nadd two imports.\n\nChange-Id: I9c90a410a7ecb91f5a4de28acee21fe7da49242c\nCloses-Bug: #1242366\nRelated-Bug: #1299331\n(cherry picked from commit ead9cfca93c9b5ca55e3ba269213c98d5e6e1d38)\n""}, {'number': 4, 'created': '2014-07-26 07:18:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/7e838d2eae3662d87a0de81a68f8c91161b4ddb6', 'message': ""libvirt: Use VIR_DOMAIN_AFFECT_LIVE for paused instances\n\nWhen a volume is attached to a paused instance, it does not\nappear in the instance's block devices list (i.e. lsblk)\nafter the instance has resumed. A similar situation\nhappens when detaching a volume from a paused instance; the\nblock device continues to exists in the block devices list,\nwhere it should not.  It was found that the volume is only\npersisted in the domain's config settings and not its active\nsettings, since only the VIR_DOMAIN_AFFECT_CONFIG flag was used\non attach and detach volume.  In order to affect the active\nsettings, the VIR_DOMAIN_AFFECT_LIVE flag has to be added when\nattaching and detaching volumes.\n\nThis change adjusts little about the unit test:\nadd two imports.\n\nChange-Id: I9c90a410a7ecb91f5a4de28acee21fe7da49242c\nCloses-Bug: #1242366\nRelated-Bug: #1299331\n(cherry picked from commit ead9cfca93c9b5ca55e3ba269213c98d5e6e1d38)\n""}, {'number': 5, 'created': '2014-08-21 02:52:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/cd0ca478d968192b4fa7bc4f7c78c51230b94cc7', 'message': ""libvirt: Use VIR_DOMAIN_AFFECT_LIVE for paused instances\n\nWhen a volume is attached to a paused instance, it does not\nappear in the instance's block devices list (i.e. lsblk)\nafter the instance has resumed. A similar situation\nhappens when detaching a volume from a paused instance; the\nblock device continues to exists in the block devices list,\nwhere it should not.  It was found that the volume is only\npersisted in the domain's config settings and not its active\nsettings, since only the VIR_DOMAIN_AFFECT_CONFIG flag was used\non attach and detach volume.  In order to affect the active\nsettings, the VIR_DOMAIN_AFFECT_LIVE flag has to be added when\nattaching and detaching volumes.\n\nThis change adjusts little about the unit test:\nadd two imports.\n\nChange-Id: I9c90a410a7ecb91f5a4de28acee21fe7da49242c\nCloses-Bug: #1242366\nRelated-Bug: #1299331\n(cherry picked from commit ead9cfca93c9b5ca55e3ba269213c98d5e6e1d38)\n""}, {'number': 6, 'created': '2014-08-28 08:18:52.000000000', 'files': ['nova/tests/virt/libvirt/test_libvirt.py', 'nova/virt/libvirt/driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/520aa4c0829da9ba4b2beb8f90995f4bdec9b1f2', 'message': ""libvirt: Use VIR_DOMAIN_AFFECT_LIVE for paused instances\n\nWhen a volume is attached to a paused instance, it does not\nappear in the instance's block devices list (i.e. lsblk)\nafter the instance has resumed. A similar situation\nhappens when detaching a volume from a paused instance; the\nblock device continues to exists in the block devices list,\nwhere it should not.  It was found that the volume is only\npersisted in the domain's config settings and not its active\nsettings, since only the VIR_DOMAIN_AFFECT_CONFIG flag was used\non attach and detach volume.  In order to affect the active\nsettings, the VIR_DOMAIN_AFFECT_LIVE flag has to be added when\nattaching and detaching volumes.\n\nThis change adjusts little about the unit test:\nadd two imports.\n\nChange-Id: I9c90a410a7ecb91f5a4de28acee21fe7da49242c\nCloses-Bug: #1242366\nRelated-Bug: #1299331\n(cherry picked from commit ead9cfca93c9b5ca55e3ba269213c98d5e6e1d38)\n""}]",0,102456,520aa4c0829da9ba4b2beb8f90995f4bdec9b1f2,43,10,6,9796,,,0,"libvirt: Use VIR_DOMAIN_AFFECT_LIVE for paused instances

When a volume is attached to a paused instance, it does not
appear in the instance's block devices list (i.e. lsblk)
after the instance has resumed. A similar situation
happens when detaching a volume from a paused instance; the
block device continues to exists in the block devices list,
where it should not.  It was found that the volume is only
persisted in the domain's config settings and not its active
settings, since only the VIR_DOMAIN_AFFECT_CONFIG flag was used
on attach and detach volume.  In order to affect the active
settings, the VIR_DOMAIN_AFFECT_LIVE flag has to be added when
attaching and detaching volumes.

This change adjusts little about the unit test:
add two imports.

Change-Id: I9c90a410a7ecb91f5a4de28acee21fe7da49242c
Closes-Bug: #1242366
Related-Bug: #1299331
(cherry picked from commit ead9cfca93c9b5ca55e3ba269213c98d5e6e1d38)
",git fetch https://review.opendev.org/openstack/nova refs/changes/56/102456/6 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/libvirt/test_libvirt.py', 'nova/virt/libvirt/driver.py']",2,0ff862ef3bf55b21fbdea58fe94b1a5e6c67e8ff,bug/1242366," if state in (power_state.RUNNING, power_state.PAUSED): if state in (power_state.RUNNING, power_state.PAUSED):", if state == power_state.RUNNING: if state == power_state.RUNNING:,81,2
openstack%2Fopenstack-manuals~master~I1c50f13409deddb430d3578bb56c8795bbafe4b2,openstack/openstack-manuals,master,I1c50f13409deddb430d3578bb56c8795bbafe4b2,Fix comment typo,MERGED,2014-09-26 09:28:51.000000000,2014-09-26 14:28:53.000000000,2014-09-26 14:28:52.000000000,"[{'_account_id': 3}, {'_account_id': 964}, {'_account_id': 7369}]","[{'number': 1, 'created': '2014-09-26 09:28:51.000000000', 'files': ['doc/common/entities/openstack.ent'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/ead38aac50f97561005a0af5cec964ce2c31f47c', 'message': 'Fix comment typo\n\nChange-Id: I1c50f13409deddb430d3578bb56c8795bbafe4b2\n'}]",0,124341,ead38aac50f97561005a0af5cec964ce2c31f47c,7,3,1,6547,,,0,"Fix comment typo

Change-Id: I1c50f13409deddb430d3578bb56c8795bbafe4b2
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/41/124341/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/common/entities/openstack.ent'],1,ead38aac50f97561005a0af5cec964ce2c31f47c,fix-typo, Any changes to the master file will override changes in other, Any changes to the master file will override changs in other,1,1
openstack%2Fnova~master~I7f3afe0e56da97e841c96c143ff6dececa77f7b7,openstack/nova,master,I7f3afe0e56da97e841c96c143ff6dececa77f7b7,Consider allowed cpus in numa placement,ABANDONED,2014-09-24 17:14:26.000000000,2014-09-26 14:25:25.000000000,,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 5170}, {'_account_id': 5511}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-24 17:14:26.000000000', 'files': ['nova/virt/libvirt/driver.py', 'nova/tests/virt/libvirt/test_driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/247bfe42306d6b9d3358e18b0f99103fd11dd599', 'message': ""Consider allowed cpus in numa placement\n\nThe current implementation of the libvirt driver's NUMA cpuset\nplacement does not take into account conf defined vcpu\npin set. As a result, the conf may define a subset of vcpus\napplicable to NUMA, but the cell (node) selection logic\nmay not properly pin an instance to a cell as defined by\nthe vcpu pin set.\n\nThis patch updates the libvirt NUMA node selection logic\nto incorporate the vcpu pin set such that a cell is properly\nselected based on the conf value (if provided). Additionally\nthis patch adds a unit test to verify the pinning placement.\n\nChange-Id: I7f3afe0e56da97e841c96c143ff6dececa77f7b7\nCloses-Bug: 1372845\n""}]",2,123790,247bfe42306d6b9d3358e18b0f99103fd11dd599,7,5,1,5367,,,0,"Consider allowed cpus in numa placement

The current implementation of the libvirt driver's NUMA cpuset
placement does not take into account conf defined vcpu
pin set. As a result, the conf may define a subset of vcpus
applicable to NUMA, but the cell (node) selection logic
may not properly pin an instance to a cell as defined by
the vcpu pin set.

This patch updates the libvirt NUMA node selection logic
to incorporate the vcpu pin set such that a cell is properly
selected based on the conf value (if provided). Additionally
this patch adds a unit test to verify the pinning placement.

Change-Id: I7f3afe0e56da97e841c96c143ff6dececa77f7b7
Closes-Bug: 1372845
",git fetch https://review.opendev.org/openstack/nova refs/changes/90/123790/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/libvirt/driver.py', 'nova/tests/virt/libvirt/test_driver.py']",2,247bfe42306d6b9d3358e18b0f99103fd11dd599,bug/1372845," def test_get_guest_config_numa_host_instance_allowed_cpus(self): instance_ref = db.instance_create(self.context, self.test_instance) flavor = objects.Flavor(memory_mb=1, vcpus=2, root_gb=496, ephemeral_gb=8128, swap=33550336, name='fake', extra_specs={}) caps = vconfig.LibvirtConfigCaps() caps.host = vconfig.LibvirtConfigCapsHost() caps.host.cpu = vconfig.LibvirtConfigCPU() caps.host.cpu.arch = ""x86_64"" caps.host.topology = self._fake_caps_numa_topology() conn = libvirt_driver.LibvirtDriver(fake.FakeVirtAPI(), True) disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type, instance_ref) with contextlib.nested( mock.patch.object( objects.Flavor, ""get_by_id"", return_value=flavor), mock.patch.object( conn, ""_get_host_capabilities"", return_value=caps), mock.patch.object( hardware, 'get_vcpu_pin_set', return_value=set([2, 3]))): cfg = conn._get_guest_config(instance_ref, [], {}, disk_info) self.assertEqual(set([2, 3]), cfg.cpuset) self.assertIsNone(cfg.cputune) self.assertIsNone(cfg.cpu.numa) with contextlib.nested( mock.patch.object( objects.Flavor, ""get_by_id"", return_value=flavor), mock.patch.object( conn, ""_get_host_capabilities"", return_value=caps), mock.patch.object( hardware, 'get_vcpu_pin_set', return_value=set([0, 1, 2]))): cfg = conn._get_guest_config(instance_ref, [], {}, disk_info) self.assertEqual(set([0, 1]), cfg.cpuset) self.assertIsNone(cfg.cputune) self.assertIsNone(cfg.cpu.numa) ",,48,4
openstack%2Foslo-incubator~stable%2Ficehouse~Ia7ff187df1685f0dd5ef32c3f6de474218874b04,openstack/oslo-incubator,stable/icehouse,Ia7ff187df1685f0dd5ef32c3f6de474218874b04,Don't call provision.py directly,MERGED,2014-09-13 16:23:10.000000000,2014-09-26 14:24:15.000000000,2014-09-26 14:24:14.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 6601}, {'_account_id': 7118}, {'_account_id': 8213}, {'_account_id': 9311}, {'_account_id': 9656}, {'_account_id': 9796}]","[{'number': 1, 'created': '2014-09-13 16:23:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/e4530159b897549bd6c92c2433bc0e443ef0a769', 'message': 'Don\'t call provision.py directly\n\nWhen testr.conf calls the sqlalchemy/provision.py script directly,\nsys.path[0] is the directory that invoked the script.  So when it\ntries to import openstack.common.db you end up with an import failure.\n\n---\nTraceback (most recent call last):\n  File ""./openstack/common/db/sqlalchemy/provision.py"", line 27, in <module>\n    from openstack.common.db import exception as exc\nImportError: No module named openstack.common.db\n---\n\nThe solution is to run it with -m\n\nCloses-Bug: #1303579\nChange-Id: Ia7ff187df1685f0dd5ef32c3f6de474218874b04\n'}, {'number': 2, 'created': '2014-09-15 15:34:58.000000000', 'files': ['.testr.conf'], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/983ceb4a04c57e1c409d9865a0be755276eaa20e', 'message': 'Don\'t call provision.py directly\n\nWhen testr.conf calls the sqlalchemy/provision.py script directly,\nsys.path[0] is the directory that invoked the script.  So when it\ntries to import openstack.common.db you end up with an import failure.\n\n---\nTraceback (most recent call last):\n  File ""./openstack/common/db/sqlalchemy/provision.py"", line 27, in <module>\n    from openstack.common.db import exception as exc\nImportError: No module named openstack.common.db\n---\n\nThe solution is to run it with -m\n\nCloses-Bug: #1303579\nChange-Id: Ia7ff187df1685f0dd5ef32c3f6de474218874b04\n(cherry picked from commit 2ae2377b)\n'}]",0,121362,983ceb4a04c57e1c409d9865a0be755276eaa20e,19,8,2,9311,,,0,"Don't call provision.py directly

When testr.conf calls the sqlalchemy/provision.py script directly,
sys.path[0] is the directory that invoked the script.  So when it
tries to import openstack.common.db you end up with an import failure.

---
Traceback (most recent call last):
  File ""./openstack/common/db/sqlalchemy/provision.py"", line 27, in <module>
    from openstack.common.db import exception as exc
ImportError: No module named openstack.common.db
---

The solution is to run it with -m

Closes-Bug: #1303579
Change-Id: Ia7ff187df1685f0dd5ef32c3f6de474218874b04
(cherry picked from commit 2ae2377b)
",git fetch https://review.opendev.org/openstack/oslo-incubator refs/changes/62/121362/1 && git format-patch -1 --stdout FETCH_HEAD,['.testr.conf'],1,e4530159b897549bd6c92c2433bc0e443ef0a769,bug/1303579,instance_provision=${PYTHON:-python} -m openstack.common.db.sqlalchemy.provision create $INSTANCE_COUNT instance_dispose=${PYTHON:-python} -m openstack.common.db.sqlalchemy.provision drop $INSTANCE_IDS,instance_provision=${PYTHON:-python} ./openstack/common/db/sqlalchemy/provision.py create $INSTANCE_COUNT instance_dispose=${PYTHON:-python} ./openstack/common/db/sqlalchemy/provision.py drop $INSTANCE_IDS,2,2
openstack%2Fcloudkitty~master~Ia62f5deea280f1b65d36ddcb3c0b01b8444093b2,openstack/cloudkitty,master,Ia62f5deea280f1b65d36ddcb3c0b01b8444093b2,Move the tests in the cloudkitty package,MERGED,2014-09-26 07:38:52.000000000,2014-09-26 14:17:53.000000000,2014-09-26 14:17:52.000000000,"[{'_account_id': 3}, {'_account_id': 7042}, {'_account_id': 7923}]","[{'number': 1, 'created': '2014-09-26 07:38:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cloudkitty/commit/4aba135f01af4d97161ecf0c7e433b2b14b33364', 'message': 'Move the tests in the cloudkitty package\n\nChange-Id: Ia62f5deea280f1b65d36ddcb3c0b01b8444093b2\n'}, {'number': 2, 'created': '2014-09-26 13:34:54.000000000', 'files': ['cloudkitty/tests/__init__.py', '.testr.conf', 'cloudkitty/tests/test_fake.py'], 'web_link': 'https://opendev.org/openstack/cloudkitty/commit/c3eecc835db90d5eaf541cbb342149afae526159', 'message': 'Move the tests in the cloudkitty package\n\nChange-Id: Ia62f5deea280f1b65d36ddcb3c0b01b8444093b2\n'}]",0,124315,c3eecc835db90d5eaf541cbb342149afae526159,10,3,2,7923,,,0,"Move the tests in the cloudkitty package

Change-Id: Ia62f5deea280f1b65d36ddcb3c0b01b8444093b2
",git fetch https://review.opendev.org/openstack/cloudkitty refs/changes/15/124315/2 && git format-patch -1 --stdout FETCH_HEAD,"['cloudkitty/tests/__init__.py', '.testr.conf', 'cloudkitty/tests/test_fake.py']",3,4aba135f01af4d97161ecf0c7e433b2b14b33364,state-tests,,,1,1
openstack%2Fpython-ironicclient~master~I9045a8913770e83b891062a5939b928094909498,openstack/python-ironicclient,master,I9045a8913770e83b891062a5939b928094909498,"Add unit tests for ""ironic node-create"" shell cmd",MERGED,2014-09-10 04:01:29.000000000,2014-09-26 14:14:39.000000000,2014-09-26 14:14:38.000000000,"[{'_account_id': 3}, {'_account_id': 3099}, {'_account_id': 7882}, {'_account_id': 8106}, {'_account_id': 12385}]","[{'number': 1, 'created': '2014-09-10 04:01:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/13c887250ffd8b769f7133ac3abf4af61debac14', 'message': 'Add unit tests for ""ironic node-create"" shell cmd\n\nThis patch set add unit tests for ""ironic node-create"" command.\n""ironic node-create"" command calls below API:\nPOST /v1/nodes\n\nChange-Id: I9045a8913770e83b891062a5939b928094909498\n'}, {'number': 2, 'created': '2014-09-12 01:14:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/f60825a48f7f907bb703543746d44221caeac176', 'message': 'Add unit tests for ""ironic node-create"" shell cmd\n\nThis patch set add unit tests for ""ironic node-create"" command.\n\nChange-Id: I9045a8913770e83b891062a5939b928094909498\n'}, {'number': 3, 'created': '2014-09-24 04:15:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/8695ca3de926248033db58f9ad00611383b17da1', 'message': 'Add unit tests for ""ironic node-create"" shell cmd\n\nThis patch set add unit tests for ""ironic node-create"" command.\n\nChange-Id: I9045a8913770e83b891062a5939b928094909498\n'}, {'number': 4, 'created': '2014-09-26 00:24:11.000000000', 'files': ['ironicclient/tests/v1/test_node_shell.py'], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/722db7af21c98d322423025ae477e82101962582', 'message': 'Add unit tests for ""ironic node-create"" shell cmd\n\nThis patch set add unit tests for ""ironic node-create"" command.\n\nChange-Id: I9045a8913770e83b891062a5939b928094909498\n'}]",2,120312,722db7af21c98d322423025ae477e82101962582,23,5,4,7882,,,0,"Add unit tests for ""ironic node-create"" shell cmd

This patch set add unit tests for ""ironic node-create"" command.

Change-Id: I9045a8913770e83b891062a5939b928094909498
",git fetch https://review.opendev.org/openstack/python-ironicclient refs/changes/12/120312/4 && git format-patch -1 --stdout FETCH_HEAD,['ironicclient/tests/v1/test_node_shell.py'],1,13c887250ffd8b769f7133ac3abf4af61debac14,test_node_shell_create," def test_do_node_create(self): client_mock = mock.MagicMock() args = mock.MagicMock() n_shell.do_node_create(client_mock, args) client_mock.node.create.assert_called_once_with() def test_do_node_create_with_driver(self): client_mock = mock.MagicMock() args = mock.MagicMock() args.driver = 'driver' n_shell.do_node_create(client_mock, args) client_mock.node.create.assert_called_once_with( driver='driver') def test_do_node_create_with_chassis_uuid(self): client_mock = mock.MagicMock() args = mock.MagicMock() args.chassis_uuid = 'chassis_uuid' n_shell.do_node_create(client_mock, args) client_mock.node.create.assert_called_once_with( chassis_uuid='chassis_uuid') def test_do_node_create_with_driver_info(self): client_mock = mock.MagicMock() args = mock.MagicMock() args.driver_info = ['arg1=val1', 'arg2=val2'] n_shell.do_node_create(client_mock, args) kwargs = { 'driver_info': {'arg1': 'val1', 'arg2': 'val2'} } client_mock.node.create.assert_called_once_with(**kwargs) def test_do_node_create_with_properties(self): client_mock = mock.MagicMock() args = mock.MagicMock() args.properties = ['arg1=val1', 'arg2=val2'] n_shell.do_node_create(client_mock, args) kwargs = { 'properties': {'arg1': 'val1', 'arg2': 'val2'} } client_mock.node.create.assert_called_once_with(**kwargs) def test_do_node_create_with_extra(self): client_mock = mock.MagicMock() args = mock.MagicMock() args.driver = 'driver_name' args.extra = ['arg1=val1', 'arg2=val2'] n_shell.do_node_create(client_mock, args) kwargs = { 'driver': 'driver_name', 'extra': {'arg1': 'val1', 'arg2': 'val2'} } client_mock.node.create.assert_called_once_with(**kwargs) ",,60,0
openstack%2Fpython-ironicclient~master~I415f305f7eb98e444b4f6b340409bc5d32d4e9bb,openstack/python-ironicclient,master,I415f305f7eb98e444b4f6b340409bc5d32d4e9bb,Remove unused command in tox.ini,MERGED,2014-09-26 06:26:02.000000000,2014-09-26 14:14:32.000000000,2014-09-26 14:14:32.000000000,"[{'_account_id': 3}, {'_account_id': 3099}, {'_account_id': 8106}]","[{'number': 1, 'created': '2014-09-26 06:26:02.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/855940028c9da28c8f9cd42134d25931c11392ee', 'message': ""Remove unused command in tox.ini\n\npatch_tox_venv.py is not found in this project,\nso 'tox -ecover' command report error. This fixes it.\n\nChange-Id: I415f305f7eb98e444b4f6b340409bc5d32d4e9bb\n""}]",0,124303,855940028c9da28c8f9cd42134d25931c11392ee,8,3,1,12385,,,0,"Remove unused command in tox.ini

patch_tox_venv.py is not found in this project,
so 'tox -ecover' command report error. This fixes it.

Change-Id: I415f305f7eb98e444b4f6b340409bc5d32d4e9bb
",git fetch https://review.opendev.org/openstack/python-ironicclient refs/changes/03/124303/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,855940028c9da28c8f9cd42134d25931c11392ee,remove-obsolute-patch,, python tools/patch_tox_venv.py,0,1
openstack%2Frally~master~I88f1f1f30c141fa88a0b336645e35c6023aca461,openstack/rally,master,I88f1f1f30c141fa88a0b336645e35c6023aca461,Adds Neutron update Network and subnets benchmarks:,MERGED,2014-09-03 06:26:36.000000000,2014-09-26 14:12:17.000000000,2014-09-26 14:12:16.000000000,"[{'_account_id': 3}, {'_account_id': 4428}, {'_account_id': 6124}, {'_account_id': 6172}, {'_account_id': 6732}, {'_account_id': 7369}, {'_account_id': 9180}, {'_account_id': 9601}, {'_account_id': 10475}]","[{'number': 1, 'created': '2014-09-03 06:26:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/bb05517a166f228e9aace62daa53e79481e427b1', 'message': 'Adds updating scenarios for Neutron:\n\n- NeutronNetwork.create_and_update_networks\n\n- NeutronNetwork.create_and_update_subnets\n\nblueprint benchmark-scenarios-for-neutron-update\nhttps://blueprints.launchpad.net/rally/+spec/tcs-neutron-update-scenorios\n\nChange-Id: I88f1f1f30c141fa88a0b336645e35c6023aca461\n'}, {'number': 2, 'created': '2014-09-04 06:09:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/212bb4c86155ad9a649b235b4d98713f0d146eb6', 'message': 'Adds Neutron update Network and subnets benchmarks:\n\n- NeutronNetwork.create_and_update_networks\n\n- NeutronNetwork.create_and_update_subnets\n\nblueprint benchmark-scenarios-for-neutron-update\n\nChange-Id: I88f1f1f30c141fa88a0b336645e35c6023aca461\n'}, {'number': 3, 'created': '2014-09-04 15:00:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/25444ab765c6f3ed5358c7740906a4a3da2eae30', 'message': 'Adds Neutron update Network and subnets benchmarks:\n\n- NeutronNetwork.create_and_update_networks\n\n- NeutronNetwork.create_and_update_subnets\n\nblueprint benchmark-scenarios-for-neutron-update\n\nChange-Id: I88f1f1f30c141fa88a0b336645e35c6023aca461\n'}, {'number': 4, 'created': '2014-09-12 12:19:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d2e4604f739663d7261fddb343879102f335f91d', 'message': 'Adds Neutron update Network and subnets benchmarks:\n\n- NeutronNetwork.create_and_update_networks\n\n- NeutronNetwork.create_and_update_subnets\n\nblueprint benchmark-scenarios-for-neutron-update\n\nChange-Id: I88f1f1f30c141fa88a0b336645e35c6023aca461\n'}, {'number': 5, 'created': '2014-09-19 05:58:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/f9860a67d4436616166357fca8699d7cb9438923', 'message': 'Adds Neutron update Network and subnets benchmarks:\n\n- NeutronNetwork.create_and_update_networks\n\n- NeutronNetwork.create_and_update_subnets\n\nPartial implementation of blueprint benchmark-scenarios-for-neutron-update\n\nChange-Id: I88f1f1f30c141fa88a0b336645e35c6023aca461\n'}, {'number': 6, 'created': '2014-09-22 07:27:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/66042ac2387887f5c26e85f77166932f3bb5d75a', 'message': 'Adds Neutron update Network and subnets benchmarks:\n\n- NeutronNetwork.create_and_update_networks\n\n- NeutronNetwork.create_and_update_subnets\n\nPartial implementation of blueprint benchmark-scenarios-for-neutron-update\n\nChange-Id: I88f1f1f30c141fa88a0b336645e35c6023aca461\n'}, {'number': 7, 'created': '2014-09-22 07:41:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1cc859e5fdf08628601587242cb82b73933b98c4', 'message': 'Adds Neutron update Network and subnets benchmarks:\n\n- NeutronNetwork.create_and_update_networks\n\n- NeutronNetwork.create_and_update_subnets\n\nPartial implementation of blueprint benchmark-scenarios-for-neutron-update\n\nChange-Id: I88f1f1f30c141fa88a0b336645e35c6023aca461\n'}, {'number': 8, 'created': '2014-09-23 15:32:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/15bc5ca72da21cac470153be9ab918b425c63381', 'message': 'Adds Neutron update Network and subnets benchmarks:\n\n- NeutronNetwork.create_and_update_networks\n\n- NeutronNetwork.create_and_update_subnets\n\nPartial implementation of blueprint benchmark-scenarios-for-neutron-update\n\nChange-Id: I88f1f1f30c141fa88a0b336645e35c6023aca461\n'}, {'number': 9, 'created': '2014-09-24 11:59:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/1ce99d191c9c4c28dbf63073816ccc4805f6827c', 'message': 'Adds Neutron update Network and subnets benchmarks:\n\n- NeutronNetwork.create_and_update_networks\n\n- NeutronNetwork.create_and_update_subnets\n\nPartial implementation of blueprint benchmark-scenarios-for-neutron-update\n\nChange-Id: I88f1f1f30c141fa88a0b336645e35c6023aca461\n'}, {'number': 10, 'created': '2014-09-25 12:38:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/31ddf01efe359a1e7dcc0e3ce7d81ef7be9d5c25', 'message': 'Adds Neutron update Network and subnets benchmarks:\n\n- NeutronNetwork.create_and_update_networks\n\n- NeutronNetwork.create_and_update_subnets\n\nPartial implementation of blueprint benchmark-scenarios-for-neutron-update\n\nChange-Id: I88f1f1f30c141fa88a0b336645e35c6023aca461\n'}, {'number': 11, 'created': '2014-09-25 14:11:16.000000000', 'files': ['doc/samples/tasks/scenarios/neutron/create_and_update_subnets.json', 'tests/benchmark/scenarios/neutron/test_utils.py', 'rally-scenarios/rally-neutron.yaml', 'doc/samples/tasks/scenarios/neutron/create_and_update_networks.yaml', 'rally/benchmark/scenarios/neutron/network.py', 'doc/samples/tasks/scenarios/neutron/create_and_update_subnets.yaml', 'rally/benchmark/scenarios/neutron/utils.py', 'tests/fakes.py', 'doc/samples/tasks/scenarios/neutron/create_and_update_networks.json', 'tests/benchmark/scenarios/neutron/test_network.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/219f26bbfe1027b08d67d11ae35323e64a376c7b', 'message': 'Adds Neutron update Network and subnets benchmarks:\n\n- NeutronNetwork.create_and_update_networks\n\n- NeutronNetwork.create_and_update_subnets\n\nPartial implementation of blueprint benchmark-scenarios-for-neutron-update\n\nChange-Id: I88f1f1f30c141fa88a0b336645e35c6023aca461\n'}]",71,118552,219f26bbfe1027b08d67d11ae35323e64a376c7b,88,9,11,13046,,,0,"Adds Neutron update Network and subnets benchmarks:

- NeutronNetwork.create_and_update_networks

- NeutronNetwork.create_and_update_subnets

Partial implementation of blueprint benchmark-scenarios-for-neutron-update

Change-Id: I88f1f1f30c141fa88a0b336645e35c6023aca461
",git fetch https://review.opendev.org/openstack/rally refs/changes/52/118552/9 && git format-patch -1 --stdout FETCH_HEAD,"['doc/samples/tasks/scenarios/neutron/create_and_update_subnets.json', 'tests/benchmark/scenarios/neutron/test_utils.py', 'rally-scenarios/rally-neutron.yaml', 'doc/samples/tasks/scenarios/neutron/create_and_update_networks.yaml', 'rally/benchmark/scenarios/neutron/network.py', 'doc/samples/tasks/scenarios/neutron/create_and_update_subnets.yaml', 'rally/benchmark/scenarios/neutron/utils.py', 'doc/samples/tasks/scenarios/neutron/create_and_update_networks.json', 'tests/benchmark/scenarios/neutron/test_network.py']",9,bb05517a166f228e9aace62daa53e79481e427b1,bp/benchmark-scenarios-for-neutron-update," @mock.patch(NEUTRON_NETWORKS + ""._update_network"") @mock.patch(NEUTRON_NETWORKS + ""._create_network"") def test_create_and_update_networks(self, mock_create_network, mock_update_network): scenario = network.NeutronNetworks() net = {""network"": {""id"": ""network-id"", ""name"": ""network-name""}} mock_create_network.return_value = net network_update_args = {'name': '_updated'} updated_net = { ""network"": { ""id"": ""network-id"", ""name"": ""network-name_updated"" } } mock_update_network.return_value = updated_net # Default options scenario.create_and_update_networks( network_update_args=network_update_args) mock_create_network.assert_called_once_with({}) self.assertEqual( mock_update_network.mock_calls, [mock.call(net, network_update_args)]) mock_create_network.reset_mock() mock_update_network.reset_mock() # Explicit network name is specified network_create_args = {""name"": ""given-name""} updated_net = { ""network"": { ""id"": ""network-id"", ""name"": ""given-name_updated"" } } mock_update_network.return_value = updated_net scenario.create_and_update_networks( network_create_args=network_create_args, network_update_args=network_update_args) mock_create_network.assert_called_once_with(network_create_args) self.assertEqual( mock_update_network.mock_calls, [mock.call(net, network_update_args)]) @mock.patch(NEUTRON_NETWORKS + ""._update_subnet"") @mock.patch(NEUTRON_NETWORKS + ""._create_subnet"") @mock.patch(NEUTRON_NETWORKS + ""._create_network"") @mock.patch(NEUTRON_NETWORKS + "".SUBNET_CIDR_START"", new_callable=mock.PropertyMock(return_value=""default_cidr"")) def test_create_and_update_subnets(self, mock_cidr_start, mock_create_network, mock_create_subnet, mock_update_subnet): scenario = network.NeutronNetworks() mock_create_network.return_value = {""network"": {""id"": ""fake-id""}} subnets_per_network = 4 subnet = {""subnet"": {""name"": ""subnet-name"", ""id"": ""subnet-id""}} mock_create_subnet.return_value = subnet subnet_update_args = {'name': '_updated'} updated_subnet = { ""subnet"": { ""name"": ""subnet-name_updated"", ""id"": ""subnet-id"" } } mock_update_subnet.return_value = updated_subnet mock_create_network.reset_mock() mock_create_subnet.reset_mock() mock_update_subnet.reset_mock() # Default options scenario.create_and_update_subnets( subnet_update_args=subnet_update_args, subnets_per_network=subnets_per_network) mock_create_network.assert_called_once_with({}) self.assertEqual(mock_create_subnet.mock_calls, [mock.call({""network"": {""id"": ""fake-id""}}, {})] * subnets_per_network) self.assertEqual(mock_update_subnet.mock_calls, [mock.call(subnet, subnet_update_args) ] * subnets_per_network) self.assertEqual(scenario.SUBNET_CIDR_START, ""default_cidr"") mock_create_network.reset_mock() mock_create_subnet.reset_mock() mock_update_subnet.reset_mock() # Custom options scenario.create_and_update_subnets( subnet_update_args=subnet_update_args, subnet_create_args={""allocation_pools"": []}, subnet_cidr_start=""custom_cidr"", subnets_per_network=subnets_per_network) self.assertEqual(scenario.SUBNET_CIDR_START, ""custom_cidr"") mock_create_network.assert_called_once_with({}) self.assertEqual( mock_create_subnet.mock_calls, [mock.call({""network"": {""id"": ""fake-id""}}, {""allocation_pools"": []})] * subnets_per_network) self.assertEqual(mock_update_subnet.mock_calls, [mock.call(subnet, subnet_update_args) ] * subnets_per_network) ",,370,0
openstack%2Fmagnetodb~master~Ic540be67e2bd0745031d50da499231674036e1eb,openstack/magnetodb,master,Ic540be67e2bd0745031d50da499231674036e1eb,Cleanup MagnetoDB specific data from Cassandra,MERGED,2014-09-25 15:12:27.000000000,2014-09-26 14:08:27.000000000,2014-09-26 14:08:26.000000000,"[{'_account_id': 3}, {'_account_id': 8188}, {'_account_id': 8491}, {'_account_id': 8601}, {'_account_id': 8863}]","[{'number': 1, 'created': '2014-09-25 15:12:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/0e5d0970c07aa29569d052268ff1a463fc4e1f26', 'message': 'fixed: Cleanup MagnetoDB specific data from Cassandra\n\nChange-Id: Ic540be67e2bd0745031d50da499231674036e1eb\n'}, {'number': 2, 'created': '2014-09-25 18:03:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/86c4ad5476c87ce4f8be6efc3a42e5da0ccdfa3d', 'message': 'fixed: Cleanup MagnetoDB specific data from Cassandra\n\nWhen executed stack.sh after unstack.sh had errors with re-creation key-spaces\n\nRelated-Bug: #1374065\nChange-Id: Ic540be67e2bd0745031d50da499231674036e1eb\n'}, {'number': 3, 'created': '2014-09-26 09:37:39.000000000', 'files': ['contrib/devstack/lib/magnetodb'], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/d325113ed1ccc1f579c707214acd5d8a24f01806', 'message': 'Cleanup MagnetoDB specific data from Cassandra\n\nWhen executed stack.sh after unstack.sh had errors with re-creation key-spaces\n\nRelated-Bug: #1374065\nChange-Id: Ic540be67e2bd0745031d50da499231674036e1eb\n'}]",0,124078,d325113ed1ccc1f579c707214acd5d8a24f01806,15,5,3,10829,,,0,"Cleanup MagnetoDB specific data from Cassandra

When executed stack.sh after unstack.sh had errors with re-creation key-spaces

Related-Bug: #1374065
Change-Id: Ic540be67e2bd0745031d50da499231674036e1eb
",git fetch https://review.opendev.org/openstack/magnetodb refs/changes/78/124078/3 && git format-patch -1 --stdout FETCH_HEAD,['contrib/devstack/lib/magnetodb'],1,0e5d0970c07aa29569d052268ff1a463fc4e1f26,master, test -e ~/.ccm/cql.state || ccm node1 cqlsh -f ~/.ccm/cql.txt rm -f ~/.ccm/cql.txt echo true > ~/.ccm/cql.state rm -f ~/.ccm/cql.state, ccm node1 cqlsh -f ~/.ccm/cql.txt,4,1
openstack%2Foslo-specs~master~Ic77d2c842b26e3e7c6a73d7bf6d07dcc237fc6d5,openstack/oslo-specs,master,Ic77d2c842b26e3e7c6a73d7bf6d07dcc237fc6d5,Convert oslo.cache to use dogpile.cache,ABANDONED,2014-06-02 06:32:55.000000000,2014-09-26 14:07:44.000000000,,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 1297}, {'_account_id': 2472}, {'_account_id': 2903}, {'_account_id': 5638}, {'_account_id': 6159}, {'_account_id': 6928}, {'_account_id': 11816}]","[{'number': 1, 'created': '2014-06-02 06:32:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/6a438ec0e97697d36aedbaddcb20486cfd0b0804', 'message': 'Convert oslo.cache to use dogpile.cache\n\nSpecification for converting the currently unused openstack.common.cache\nmodule to leverage dogpile.cache as the underlying caching library.\n\nChange-Id: Ic77d2c842b26e3e7c6a73d7bf6d07dcc237fc6d5\n'}, {'number': 2, 'created': '2014-06-06 16:01:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/4745de326dc6b0c1bbdf489e47ae5c06ed620db5', 'message': 'Convert oslo.cache to use dogpile.cache\n\nSpecification for converting the currently unused openstack.common.cache\nmodule to leverage dogpile.cache as the underlying caching library.\n\nChange-Id: Ic77d2c842b26e3e7c6a73d7bf6d07dcc237fc6d5\n'}, {'number': 3, 'created': '2014-07-12 23:38:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/94e86f3763f0bd493e58652774548ccc5e90b437', 'message': 'Convert oslo.cache to use dogpile.cache\n\nSpecification for converting the currently unused openstack.common.cache\nmodule to leverage dogpile.cache as the underlying caching library.\n\nChange-Id: Ic77d2c842b26e3e7c6a73d7bf6d07dcc237fc6d5\n'}, {'number': 4, 'created': '2014-07-16 01:43:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/5030081a105cb9c15f04de34b5ad876434cd58a9', 'message': 'Convert oslo.cache to use dogpile.cache\n\nSpecification for converting the currently unused openstack.common.cache\nmodule to leverage dogpile.cache as the underlying caching library.\n\nChange-Id: Ic77d2c842b26e3e7c6a73d7bf6d07dcc237fc6d5\n'}, {'number': 5, 'created': '2014-07-16 14:47:26.000000000', 'files': ['specs/juno/oslo-cache-using-dogpile.rst'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/6c0aa61279fb493209291f567d9e09b3c3865e23', 'message': 'Convert oslo.cache to use dogpile.cache\n\nSpecification for converting the currently unused openstack.common.cache\nmodule to leverage dogpile.cache as the underlying caching library.\n\nChange-Id: Ic77d2c842b26e3e7c6a73d7bf6d07dcc237fc6d5\n'}]",55,97155,6c0aa61279fb493209291f567d9e09b3c3865e23,61,9,5,2903,,,0,"Convert oslo.cache to use dogpile.cache

Specification for converting the currently unused openstack.common.cache
module to leverage dogpile.cache as the underlying caching library.

Change-Id: Ic77d2c842b26e3e7c6a73d7bf6d07dcc237fc6d5
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/55/97155/1 && git format-patch -1 --stdout FETCH_HEAD,"['specs/juno/index.rst', 'specs/juno/oslo-cache-using-dogpile.rst']",2,6a438ec0e97697d36aedbaddcb20486cfd0b0804,caching,".. This template should be in ReSTructured text. For help with syntax, see http://sphinx-doc.org/rest.html To test out your formatting, build the docs using tox, or see: http://rst.ninjs.org The filename in the git repository should match the launchpad URL, for example a URL of https://blueprints.launchpad.net/oslo/+spec/awesome-thing should be named awesome-thing.rst. Please wrap text at 79 columns. Please do not delete any of the sections in this template. If you have nothing to say for a whole section, just write: None If you would like to provide a diagram with your spec, ascii diagrams are required. http://asciiflow.com/ is a very nice tool to assist with making ascii diagrams. The reason for this is that the tool used to review specs is based purely on plain text. Plain text will allow review to proceed without having to look at additional files which can not be viewed in gerrit. It will also allow inline feedback on the diagram itself. ======================================= Oslo Cache Updated to use dogpile.cache ======================================= `proposed bp oslo-cache-using-dogpile <https://blueprints.launchpad.net/oslo/+spec/oslo-cache-using-dogpile>`_ Currently the various OpenStack projects implement caching (memoization, key-value-store, etc) in a number of various ways. All of the mechanisms for caching should be unified under a single Oslo library ``oslocache``. Problem description =================== With the many implementations (and varying degree of flexibility) OpenStack projects should standardize on a single library for caching (memoization, key-value-store, etc) within the code base. Currently, Keystone has used `dogpile.cache <http://dogpilecache.readthedocs.org/en/latest/>`_ very successfully. With the flexible backend implementation, ``dogpile.cache`` is a natural fit to replace the many different implementations of caching. The new ``oslocache`` library would replace the following: * Keystone Caching Layer * Keystone Key-Value-Store Implementation * Oslo MemoryCache Module * Caching within the Keystone Middleware * (potentially) Swift Ring Memcache This new module would also open the door for other projects to more easily adopt memoization or other forms of caching. The ``oslocache`` module is mostly targeted to be an OpenStack friendly wrapper for ``dogpile.cache``. This will provide a consistent way to cache / memoize data within the OpenStack ecosystem. Proposed change =============== The current ``oslo-incubator`` `oslo openstack.common.cache module <https://git.openstack.org/cgit/openstack/oslo-incubator/openstack/common/ cache>`_ would be replaced with an implementation that leverages ``dogpile.cache`` library instead of a custom-built system. This new implementation will provide a mechianism to handle key-value-store (e.g. traditional Memcached) and memoization to begin with. The system will also include (optionally) cryptographic signing and/or full encryption of data stored in external services (e.g. Memcached). The Base backends that will be supported are: * Memcached * BMemcached * Standard Memcached * Pylibmc * In-Memory (Python dict-based) * Redis * MongoDB Each project that implements either it's own version of caching using ``dogpile.cache`` or other implementations of caching will be converted to use the new ``oslocache`` module. Alternatives ------------ This could be left to each project to implement their own caching or develop an OpenStack specific set of modules. Neither of these options are optimal as it requires specific knowledge of the various (or OpenStack specific) caching system to implement a custom backend. With ``dogpile.cache`` it is possible to implement a very simple backend and configure it for use with all OpenStack services that make use of the ``oslocache`` module. Impact on Existing APIs ----------------------- Current APIs should remain unchanged (both in Oslo and in other projects) until caching is implemented. Caching implementation may impact the APIs in some regard (e.g. memoization requires proper invalidation of data). Security impact --------------- The Security Team will be asked to review the cryptographic signing and encryption specific code paths prior to acceptance into the new library. Performance Impact ------------------ Ideally the performance impact of using caching should be only positive, however maintenance of the cache coherency can have overhead when caching is implemented. It is likely ``dogpile.cache`` will have a better performance profile than the current ``MemoryCache`` module. Configuration Impact -------------------- New options for caching will be added. To leverage caching the configuration for a service using ``oslocache`` will need to have the values set. By default caching will be disabled (can be overrided by a project) to ensure that memory leaking / improper caching / negative performance impact waiting on non-existent external services will not impact an OpenStack project. Developer Impact ---------------- Developers will need to become familiar with ``dogpile.cache`` and how to implement usage of memoization and/or key-value-store regions. Implementation ============== Assignee(s) ----------- Who is leading the writing of the code? Or is this a blueprint where you're throwing it out there to see who picks it up? If more than one person is working on the implementation, please designate the primary author and contact. Primary assignee: Morgan Fainberg <mdrnstm> Other contributors: TBD Milestones ---------- Target Milestone for completion: Juno-2 Work Items ---------- * Convert ``oslo.cache`` module over to use ``dogpile.cache`` * Provide a clean / easy way to configure the cache region (in-memory cache object that provides access to the key-value-store and memoization decorators) * Implement cryptographic signing and encryption mechanisms for validating and securing the data stored in the cache. * Provide Stevedore loading of any/all ``dogpile.cache`` backends and entry point standardization for 3rd-party (out-of-tree) backends. Incubation ========== Lifecycle will be incubation, and adoption in projects that leverage either ``dogpile.cache`` directly (Keystone) or the oslo.MemoryCache module. Once the interfaces are stable (and clearly documented) it is expected this module can quickly move to graduation. Adoption -------- Keystone will be the primary (first target) to adopt the new module (replacing the custom ``dogpile.cache`` implementation. The oslo.MemoryCache module will be updated to leverage the new ``oslocache`` module. The direct use of the oslo.MemoryCache module will be deprecated in favor of directly using ``oslocache``. Library ------- The Library will graduate into a top-level ``oslocache`` library. Anticipated API Stabilization ----------------------------- I expect that this library should be able to stabilize within a single development cycle. Adoption via incubator for Juno and K, release as a library in either K or L. Documentation Impact ==================== * Documentation on configuring the cache region will be required. * Developer documentation on implementing key-value-store and memoization within an OpenStack project will be needed. Dependencies ============ All dependencies should already be in the global requirements. No external blueprints should be needed. References ========== .. note:: This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ",,251,4
openstack%2Foslo-specs~master~Ib956d846e31864f528953c8aa0fac2a8f2d072b1,openstack/oslo-specs,master,Ib956d846e31864f528953c8aa0fac2a8f2d072b1,oslo.concurrency update,ABANDONED,2014-08-22 16:19:50.000000000,2014-09-26 14:07:25.000000000,,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-08-22 16:19:50.000000000', 'files': ['specs/juno/graduate-oslo-concurrency.rst'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/5c75791b3e64f934c3dc77ccb71200d0c7ee1e97', 'message': 'oslo.concurrency update\n\nBefore release we need to move the config opts to a separate group,\nand this is definitely not going to be done for J-2.\n\nChange-Id: Ib956d846e31864f528953c8aa0fac2a8f2d072b1\n'}]",0,116325,5c75791b3e64f934c3dc77ccb71200d0c7ee1e97,8,4,1,6928,,,0,"oslo.concurrency update

Before release we need to move the config opts to a separate group,
and this is definitely not going to be done for J-2.

Change-Id: Ib956d846e31864f528953c8aa0fac2a8f2d072b1
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/25/116325/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/graduate-oslo-concurrency.rst'],1,5c75791b3e64f934c3dc77ccb71200d0c7ee1e97,graduate-concurrency," Juno-3* Move lockutils config opts to a separate group, with the appropriate deprecated names for backwards compatibility. ", Juno-2,4,1
openstack%2Foslo-specs~master~If611aeb3d1fd4418eba261a45e9d9a74414a602c,openstack/oslo-specs,master,If611aeb3d1fd4418eba261a45e9d9a74414a602c,Add a config validator libary to oslo.config,ABANDONED,2014-05-09 23:37:57.000000000,2014-09-26 14:07:02.000000000,,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 6699}]","[{'number': 1, 'created': '2014-05-09 23:37:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/2bffbebd4e81a3acf8cdaef548222de929321d05', 'message': 'Add a config validator libary to oslo.config\n\nChange-Id: If611aeb3d1fd4418eba261a45e9d9a74414a602c\n'}, {'number': 2, 'created': '2014-05-13 13:51:48.000000000', 'files': ['specs/juno/index.rst', 'specs/juno/config-validator-simple.rst'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/a511fd702f8a70d3bb6ab5c4dee201c62dba3f42', 'message': 'Add a config validator libary to oslo.config\n\nChange-Id: If611aeb3d1fd4418eba261a45e9d9a74414a602c\n'}]",8,93149,a511fd702f8a70d3bb6ab5c4dee201c62dba3f42,16,3,2,6699,,,0,"Add a config validator libary to oslo.config

Change-Id: If611aeb3d1fd4418eba261a45e9d9a74414a602c
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/49/93149/2 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/config-validator-simple.rst'],1,2bffbebd4e81a3acf8cdaef548222de929321d05,config-validator-simple,".. This template should be in ReSTructured text. The filename in the git repository should match the launchpad URL, for example a URL of https://blueprints.launchpad.net/oslo/+spec/awesome-thing should be named awesome-thing.rst. Please do not delete any of the sections in this template. If you have nothing to say for a whole section, just write: None For help with syntax, see http://sphinx-doc.org/rest.html To test out your formatting, see http://www.tele3.cz/jbar/rest/rest.html ============================= A simple config validator ============================= Leverage oslo.config to create a simple config validator library. https://blueprints.launchpad.net//+spec/config-validator-simple Problem description =================== There are a **lot** of configuration options in OpenStack. In most mature projects the automatically generated config files are over 2000 lines long! Plus, in an effort to keep things simple and up-to-date, we depreciate many config entries--usually just to move them to a more appropriate section. All of this opens up a bunch of places where a deployer could mess something up: a typo here, or putting the right config key under the wrong section. Wouldn't it be nice to be able to run a simple command to validate the entire configuration file? E.g. :: nova-manage config validate /etc/nova/nova.conf Proposed change =============== oslo.config does the hard work here by centralizing all config option registration. This blueprint would add an additional library to oslo that implemented basic validation functions. This library could then be used by projects to construct CLI tools that could easily be used by operators. At a cursory level this would behave like the oslo-incubator config.generator: modules would be loaded (registering config options), the validator would then scan the config file and perform checks. Based on the information available with the current config registration system, there are a few checks that the validator could perform: - Raise an error if an option is of the wrong type. - Raise an error if an option in the file (section, key) does not exist for the service. E.g. a misspelled option. - Raise a warning if an option is depreciated. - Raise a warning if an option in the file (section, key, value), is set to the default value. Note that this kind of behavior would be configurable in the library. This level of validation is achievable with no changes to the config registration syntax. And it would catch many deployer mistakes that now cost hours in debugging strange service behavior. While it is not in the scope of this proposal, further validation could be done to check that a config option that is correctly set is *actually used* by the service. E.g. a driver must be activated before any of it's config options take effect. So if the options are set but the driver is not loaded, a warning should be produced. To do this, new syntax could be added to config registration declaring a type of dependency on another config option. Alternatives ------------ There is a proposal to allow services to register their own validators, which would process the configuration and fail to start the service if there were problems in the config file. [1]_. This is a promising idea, but it requires significant buy-in on the part of service authors to define new validators. This blueprint would provide a lot of validation from the very start. Implementation ============== Assignee(s) ----------- I will work to get a basic version of the validator working. However, help is always welcome! Primary assignee: scott-devoid Milestones ---------- Target Milestone for completion: Juno-1 Work Items ---------- - Implement basic validator interface with unit tests. - Implement a simple script that calls the validator for a given project. Hopefully projects using oslo.config will then integrate the library into project-manage commands, as this is probably the best place for them. Dependencies ============ None .. [1] https://blueprints.launchpad.net/oslo/+spec/service-validation .. note:: This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ",,127,0
openstack%2Foslo-specs~master~I7b984b908ed1a43387d8b1b4cc373984b407c0e4,openstack/oslo-specs,master,I7b984b908ed1a43387d8b1b4cc373984b407c0e4,Modify graduate-oslo-concurrency blueprint,ABANDONED,2014-07-28 17:25:26.000000000,2014-09-26 14:06:52.000000000,,"[{'_account_id': 3}, {'_account_id': 708}, {'_account_id': 1247}, {'_account_id': 1669}, {'_account_id': 2472}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-07-28 17:25:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/a12c6c46bd0ab892d3bf4749fb0a2dbb3e841c58', 'message': 'Modify graduate-oslo-concurrency blueprint\n\n* added local.py to the list of modules;\n* moved target to Juno-3;\n* removed dependency on oslo.log (https://review.openstack.org/109561\n  took care of that);\n* moved processutils cleanup blueprint from dependency to references as\n  it will be done in the new library.\n\nblueprint graduate-oslo-concurrency\n\nChange-Id: I7b984b908ed1a43387d8b1b4cc373984b407c0e4\n'}, {'number': 2, 'created': '2014-07-28 18:24:46.000000000', 'files': ['specs/juno/graduate-oslo-concurrency.rst'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/aa0f281b68a0100fc80e3b0970077326096da847', 'message': 'Modify graduate-oslo-concurrency blueprint\n\n* added local.py to the list of modules;\n* moved target to Juno-3;\n* removed dependency on oslo.log (https://review.openstack.org/109561\n  took care of that);\n* moved processutils cleanup blueprint from dependency to references as\n  it will be done in the new library.\n\nblueprint graduate-oslo-concurrency\n\nChange-Id: I7b984b908ed1a43387d8b1b4cc373984b407c0e4\n'}]",2,110070,aa0f281b68a0100fc80e3b0970077326096da847,26,6,2,708,,,0,"Modify graduate-oslo-concurrency blueprint

* added local.py to the list of modules;
* moved target to Juno-3;
* removed dependency on oslo.log (https://review.openstack.org/109561
  took care of that);
* moved processutils cleanup blueprint from dependency to references as
  it will be done in the new library.

blueprint graduate-oslo-concurrency

Change-Id: I7b984b908ed1a43387d8b1b4cc373984b407c0e4
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/70/110070/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/graduate-oslo-concurrency.rst'],1,a12c6c46bd0ab892d3bf4749fb0a2dbb3e841c58,bp/from,local.py Juno-3* https://etherpad.openstack.org/p/juno-oslo-release-plan * https://blueprints.launchpad.net/oslo/+spec/cleanup-processutils-for-graduation, Juno-2* https://blueprints.launchpad.net/oslo/+spec/graduate-oslo-log * https://blueprints.launchpad.net/oslo/+spec/cleanup-processutils-for-graduation https://etherpad.openstack.org/p/juno-oslo-release-plan,6,6
openstack%2Foslo-specs~master~Id9d05be2717b09b19dee27b4c5da76a017414caf,openstack/oslo-specs,master,Id9d05be2717b09b19dee27b4c5da76a017414caf,oslo.messaging main loop refactoring,ABANDONED,2014-07-09 17:05:55.000000000,2014-09-26 14:06:35.000000000,,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 2472}, {'_account_id': 2813}, {'_account_id': 6159}, {'_account_id': 7763}, {'_account_id': 9107}]","[{'number': 1, 'created': '2014-07-09 17:05:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/f3f93a16b896b68d9b167ac7a0895a07ecf28b4d', 'message': 'oslo.messaging main loop refactoring\n\nChange-Id: Id9d05be2717b09b19dee27b4c5da76a017414caf\n'}, {'number': 2, 'created': '2014-07-09 18:26:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/36f3bb0d3cba06dd663300b52efc406498412170', 'message': 'oslo.messaging main loop refactoring\n\nChange-Id: Id9d05be2717b09b19dee27b4c5da76a017414caf\n'}, {'number': 3, 'created': '2014-07-11 17:37:13.000000000', 'files': ['specs/juno/oslo-messaging-server-main-loop/main_loop_updated.uml', 'specs/juno/oslo-messaging-server-main-loop.rst', 'specs/juno/oslo-messaging-server-main-loop/main_loop_current.uml'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/6f47b9d19ef1c7e11f60db70c1e6f7637797abd3', 'message': 'oslo.messaging main loop refactoring\n\nChange-Id: Id9d05be2717b09b19dee27b4c5da76a017414caf\n'}]",30,105796,6f47b9d19ef1c7e11f60db70c1e6f7637797abd3,24,7,3,7763,,,0,"oslo.messaging main loop refactoring

Change-Id: Id9d05be2717b09b19dee27b4c5da76a017414caf
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/96/105796/3 && git format-patch -1 --stdout FETCH_HEAD,"['specs/juno/oslo-messaging-server-main-loop/main_loop_updated.uml', 'specs/juno/oslo-messaging-server-main-loop.rst', 'specs/juno/oslo-messaging-server-main-loop/main_loop_current.uml']",3,f3f93a16b896b68d9b167ac7a0895a07ecf28b4d,messaging,"@startuml hide footbox actor User boundary messaging.rpc entity Transport control MessageHandlingServer as Server entity EventletExecutor as Executor entity RPCDispatcher as Dispatcher entity Listener title Current flow activate User User -> messaging.rpc : get_trasport create Transport messaging.rpc -> Transport : __init__ messaging.rpc <- Transport : transport User <- messaging.rpc : transport User -> messaging.rpc : get_rpc_server(transport, target) activate messaging.rpc create Dispatcher messaging.rpc -> Dispatcher : __init__(target) messaging.rpc <- Dispatcher : dispatcher create Server messaging.rpc -> Server: __init__(dispatcher, executor) User <- messaging.rpc : server deactivate messaging.rpc User -> Server : start activate Server Server -> Dispatcher : listen(transport) Dispatcher -> Transport : listen(target) create Listener Transport -> Listener : __init__(connection) Transport <- Listener : listener Dispatcher <- Transport : listener Server <- Dispatcher : listener create Executor Server -> Executor : __init__(listener, dispatcher) Server <- Executor : executor Server -> Executor : start activate Executor Executor -> eventlet : spawn_executor_thread loop while True Executor -> Listener : poll activate Listener Executor <- Listener : message deactivate Listener Executor -> Dispatcher : dispatch(message) activate Dispatcher Executor <- Dispatcher : callback deactivate Dispatcher Executor --> eventlet : spawn(callback) end deactivate Executor deactivate Server deactivate User @enduml ",,266,0
openstack%2Foslo-specs~master~I8e32a26ea0ec201f447bde4797e5d4621c20cc7a,openstack/oslo-specs,master,I8e32a26ea0ec201f447bde4797e5d4621c20cc7a,Add a Service Bus for Windows Server RPC backend,ABANDONED,2014-07-27 17:16:00.000000000,2014-09-26 14:04:51.000000000,,"[{'_account_id': 3}, {'_account_id': 1247}, {'_account_id': 2472}, {'_account_id': 2813}, {'_account_id': 3185}, {'_account_id': 6159}]","[{'number': 1, 'created': '2014-07-27 17:16:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/bfcdc530a0e14c93175ea98ad450dd2394f8506b', 'message': 'Add a Service Bus for Windows Server RPC backend\n\nMicrosoft provides a messaging service named Service Bus for Windows\nServer available on premise and compatible with the Windows Azure Service Bus.\n\nA platform independent Service Bus Python client implementation is also\navailable and can be used to implement an OpenStack RPC backend to be used\nby any OpenStack component as an alternative to RabbitMQ, Qpid or ZMQ on any\nsupported platform, including Windows and Linux.\n\nChange-Id: I8e32a26ea0ec201f447bde4797e5d4621c20cc7a\nblueprint: windows-service-bus-rpc-backend\n'}, {'number': 2, 'created': '2014-08-05 18:58:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/f333e6409d837e1883b427f17e9443951d0d486d', 'message': 'Add a Service Bus for Windows Server RPC backend\n\nMicrosoft provides a messaging service named Service Bus for Windows\nServer available on premise and compatible with the Windows Azure Service Bus.\n\nA platform independent Service Bus Python client implementation is also\navailable and can be used to implement an OpenStack RPC backend to be used\nby any OpenStack component as an alternative to RabbitMQ, Qpid or ZMQ on any\nsupported platform, including Windows and Linux.\n\nChange-Id: I8e32a26ea0ec201f447bde4797e5d4621c20cc7a\nblueprint: windows-service-bus-rpc-backend\n'}, {'number': 3, 'created': '2014-08-05 19:21:14.000000000', 'files': ['specs/juno/windows-service-bus-rpc-backend.rst'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/ede95ec537bf14eaaed6d72a594b4110544aba57', 'message': 'Add a Service Bus for Windows Server RPC backend\n\nMicrosoft provides a messaging service named Service Bus for Windows\nServer available on premise and compatible with the Windows Azure Service Bus.\n\nA platform independent Service Bus Python client implementation is also\navailable and can be used to implement an OpenStack RPC backend to be used\nby any OpenStack component as an alternative to RabbitMQ, Qpid or ZMQ on any\nsupported platform, including Windows and Linux.\n\nChange-Id: I8e32a26ea0ec201f447bde4797e5d4621c20cc7a\nblueprint: windows-service-bus-rpc-backend\n'}]",0,109863,ede95ec537bf14eaaed6d72a594b4110544aba57,14,6,3,3185,,,0,"Add a Service Bus for Windows Server RPC backend

Microsoft provides a messaging service named Service Bus for Windows
Server available on premise and compatible with the Windows Azure Service Bus.

A platform independent Service Bus Python client implementation is also
available and can be used to implement an OpenStack RPC backend to be used
by any OpenStack component as an alternative to RabbitMQ, Qpid or ZMQ on any
supported platform, including Windows and Linux.

Change-Id: I8e32a26ea0ec201f447bde4797e5d4621c20cc7a
blueprint: windows-service-bus-rpc-backend
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/63/109863/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/windows-service-bus-rpc-backend.rst'],1,bfcdc530a0e14c93175ea98ad450dd2394f8506b,bp/windows-service-bus-rpc-backend,"============================================ RPC backend: Service Bus for Windows Server ============================================ https://blueprints.launchpad.net/oslo/+spec/windows-service-bus-rpc-backend Microsoft Service Bus for Windows Server RPC backend. Problem description =================== Microsoft provides a messaging service named Service Bus for Windows Server available on premise and compatible with the Windows Azure Service Bus. A platform independent Service Bus Python client implementation is also available and can be used to implement an OpenStack RPC backend to be used by any OpenStack component as an alternative to RabbitMQ, Qpid or ZMQ on any supported platform, including Windows and Linux. Proposed change =============== The implementation consists in implementing the required modules and classes for the RPC backend as part of the oslo.messaging project. Alternatives ------------ Other backend options are availbale, including RabbitMQ, Qpid and ZMQ. Impact on Existing APIs ----------------------- None Security impact --------------- None Performance Impact ------------------ None Configuration Impact -------------------- Configuration options to provide the Service Bus host, port and authentication information are necessary. Developer Impact ---------------- None Implementation ============== Primary assignee: alexpilotti Other contributors: cbelu Milestones ---------- Target Milestone for completion: Juno-3 Work Items ---------- * Backend implementation * Unit tests Incubation ========== None Adoption -------- Any OpenStack project that uses RPC messaging. Library ------- oslo.messaging Anticipated API Stabilization ----------------------------- None Documentation Impact ==================== Information on the Service Bus RPC backend and for the related configuration options. Dependencies ============ None References ========== * Service Bus for Windows Server: http://msdn.microsoft.com/en-us/library/dn282144.aspx * Service Bus Python client: http://azure.microsoft.com/en-us/documentation/articles/service-bus-python-how-to-use-queues/ .. note:: This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ",,120,0
openstack%2Fmanila~master~I8ec94c522f65cd66f5e53841ac73d078973c6b85,openstack/manila,master,I8ec94c522f65cd66f5e53841ac73d078973c6b85,Specify the correct Samba share path,MERGED,2014-09-25 14:13:57.000000000,2014-09-26 14:04:31.000000000,2014-09-26 14:04:30.000000000,"[{'_account_id': 3}, {'_account_id': 2417}, {'_account_id': 6491}, {'_account_id': 6529}, {'_account_id': 8851}, {'_account_id': 10068}, {'_account_id': 13211}]","[{'number': 1, 'created': '2014-09-25 14:13:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/db7718d3ecfff70c5edc49de3c92b977dfd2935a', 'message': 'Specify the correct Samba share path\n\nWhen configuring a new Samba share, the generic driver doesn\'t specify\nthe correct share path. It specifies ""path = /shares"" instead of ""path =\n/shares/share-GUID"". As a result, a host with access to one share can\naccess any other shares deployed by the tenant.\n\nChange-Id: I8ec94c522f65cd66f5e53841ac73d078973c6b85\nCloses-Bug: 1370223\n'}, {'number': 2, 'created': '2014-09-26 08:53:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/9812061e9d3649b784fc63ea031fad4d356b17e4', 'message': 'Specify the correct Samba share path\n\nWhen configuring a new Samba share, the generic driver doesn\'t specify\nthe correct share path. It specifies ""path = /shares"" instead of ""path =\n/shares/share-GUID"". As a result, a host with access to one share can\naccess any other shares deployed by the tenant.\n\nChange-Id: I8ec94c522f65cd66f5e53841ac73d078973c6b85\nCloses-Bug: 1370223\n'}, {'number': 3, 'created': '2014-09-26 11:47:37.000000000', 'files': ['manila/tests/share/drivers/test_generic.py', 'manila/share/drivers/generic.py'], 'web_link': 'https://opendev.org/openstack/manila/commit/4d01d3a137a7411f6fb2e06f6eafa0d06a2320f9', 'message': 'Specify the correct Samba share path\n\nWhen configuring a new Samba share, the generic driver doesn\'t specify\nthe correct share path. It specifies ""path = /shares"" instead of ""path =\n/shares/share-GUID"". As a result, a host with access to one share can\naccess any other shares deployed by the tenant.\n\nChange-Id: I8ec94c522f65cd66f5e53841ac73d078973c6b85\nCloses-Bug: #1370223\n'}]",7,124065,4d01d3a137a7411f6fb2e06f6eafa0d06a2320f9,20,7,3,13211,,,0,"Specify the correct Samba share path

When configuring a new Samba share, the generic driver doesn't specify
the correct share path. It specifies ""path = /shares"" instead of ""path =
/shares/share-GUID"". As a result, a host with access to one share can
access any other shares deployed by the tenant.

Change-Id: I8ec94c522f65cd66f5e53841ac73d078973c6b85
Closes-Bug: #1370223
",git fetch https://review.opendev.org/openstack/manila refs/changes/65/124065/1 && git format-patch -1 --stdout FETCH_HEAD,['manila/share/drivers/generic.py'],1,db7718d3ecfff70c5edc49de3c92b977dfd2935a,(detached," share_name, os.path.join( self.configuration.share_mount_path, share_name),"," share_name, self.configuration.share_mount_path,",2,1
openstack%2Ffuel-main~stable%2F5.1~Ic85e7ad73a9c828e1bdc4621c9e1cb99ac255df0,openstack/fuel-main,stable/5.1,Ic85e7ad73a9c828e1bdc4621c9e1cb99ac255df0,Move to next fuel release 5.1.1,MERGED,2014-09-25 21:27:49.000000000,2014-09-26 14:01:24.000000000,2014-09-26 14:01:24.000000000,"[{'_account_id': 3}, {'_account_id': 406}, {'_account_id': 8777}, {'_account_id': 8965}, {'_account_id': 8971}, {'_account_id': 10474}]","[{'number': 1, 'created': '2014-09-25 21:27:49.000000000', 'files': ['config.mk'], 'web_link': 'https://opendev.org/openstack/fuel-main/commit/a964d25cf35f782c915342a711871ead3ba9f564', 'message': 'Move to next fuel release 5.1.1\n\nChange-Id: Ic85e7ad73a9c828e1bdc4621c9e1cb99ac255df0\n'}]",0,124188,a964d25cf35f782c915342a711871ead3ba9f564,12,6,1,8777,,,0,"Move to next fuel release 5.1.1

Change-Id: Ic85e7ad73a9c828e1bdc4621c9e1cb99ac255df0
",git fetch https://review.opendev.org/openstack/fuel-main refs/changes/88/124188/1 && git format-patch -1 --stdout FETCH_HEAD,['config.mk'],1,a964d25cf35f782c915342a711871ead3ba9f564,,PRODUCT_VERSION:=5.1.1,PRODUCT_VERSION:=5.1,1,1
openstack%2Foslo-specs~master~Ib4f84efab8b3af3ddd47d38d63a91362612bd655,openstack/oslo-specs,master,Ib4f84efab8b3af3ddd47d38d63a91362612bd655,use yasfb for rss feeds,MERGED,2014-09-09 19:37:06.000000000,2014-09-26 13:59:13.000000000,2014-09-26 13:59:12.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-09 19:37:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/12465a29292ebb3b96a198a018bf6bb2675affa5', 'message': 'use yasfb for rss feeds\n\nPublish an RSS feed of the changes to the specs repository to make the\nspecs more discoverable.\n\nChange-Id: Ib4f84efab8b3af3ddd47d38d63a91362612bd655\n'}, {'number': 2, 'created': '2014-09-10 20:05:08.000000000', 'files': ['requirements.txt', 'doc/source/conf.py'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/89438538b3eca110b0ce6f8ce5aaa3955c778cba', 'message': 'use yasfb for rss feeds\n\nPublish an RSS feed of the changes to the specs repository to make the\nspecs more discoverable.\n\nChange-Id: Ib4f84efab8b3af3ddd47d38d63a91362612bd655\n'}]",0,120194,89438538b3eca110b0ce6f8ce5aaa3955c778cba,13,4,2,2472,,,0,"use yasfb for rss feeds

Publish an RSS feed of the changes to the specs repository to make the
specs more discoverable.

Change-Id: Ib4f84efab8b3af3ddd47d38d63a91362612bd655
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/94/120194/2 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'doc/source/conf.py']",2,12465a29292ebb3b96a198a018bf6bb2675affa5,rss-feed," 'oslosphinx', 'yasfb',# Feed configuration for yasfb feed_base_url = 'http://specs.openstack.org/openstack/oslo-specs/' feed_author = 'OpenStack Oslo Team' #intersphinx_mapping = {'http://docs.python.org/': None} ", 'oslosphinx'#intersphinx_mapping = {'http://docs.python.org/': None},9,2
openstack%2Foslo-specs~master~Ia01a3f23105ca97f327de2d8249ea9ae8c67189b,openstack/oslo-specs,master,Ia01a3f23105ca97f327de2d8249ea9ae8c67189b,use the current date to get the copyright,MERGED,2014-09-09 19:37:06.000000000,2014-09-26 13:57:46.000000000,2014-09-26 13:57:45.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-09 19:37:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/1d7097f6bd845644dce505ed963aef70f80fd374', 'message': 'use the current date to get the copyright\n\nChange-Id: Ia01a3f23105ca97f327de2d8249ea9ae8c67189b\n'}, {'number': 2, 'created': '2014-09-10 20:05:08.000000000', 'files': ['doc/source/conf.py'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/54d8e92549646704d58f9384b533095fa64d0efe', 'message': 'use the current date to get the copyright\n\nChange-Id: Ia01a3f23105ca97f327de2d8249ea9ae8c67189b\n'}]",0,120193,54d8e92549646704d58f9384b533095fa64d0efe,9,3,2,2472,,,0,"use the current date to get the copyright

Change-Id: Ia01a3f23105ca97f327de2d8249ea9ae8c67189b
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/93/120193/2 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/conf.py'],1,1d7097f6bd845644dce505ed963aef70f80fd374,rss-feed,"import datetimecopyright = u'%s, OpenStack Foundation' % datetime.date.today().year","copyright = u'2013, OpenStack Foundation'",2,1
openstack%2Foslo-specs~master~Ic93ad16b7d919069f6dc1e6be4eb29fe6b4a7bef,openstack/oslo-specs,master,Ic93ad16b7d919069f6dc1e6be4eb29fe6b4a7bef,remove templates from toctree,MERGED,2014-09-09 19:37:06.000000000,2014-09-26 13:57:24.000000000,2014-09-26 13:57:24.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-09 19:37:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/4ba3590a4713160630188eb4ba74a3dc41434ae4', 'message': ""remove templates from toctree\n\nWe don't want the templates to appear in the table of contents.\n\nChange-Id: Ic93ad16b7d919069f6dc1e6be4eb29fe6b4a7bef\n""}, {'number': 2, 'created': '2014-09-10 20:05:08.000000000', 'files': ['doc/source/index.rst', 'doc/source/conf.py'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/435a3e6fc75b135d445d7ace2f3c75afa97e62b3', 'message': ""remove templates from toctree\n\nWe don't want the templates to appear in the table of contents.\n\nChange-Id: Ic93ad16b7d919069f6dc1e6be4eb29fe6b4a7bef\n""}]",0,120192,435a3e6fc75b135d445d7ace2f3c75afa97e62b3,9,3,2,2472,,,0,"remove templates from toctree

We don't want the templates to appear in the table of contents.

Change-Id: Ic93ad16b7d919069f6dc1e6be4eb29fe6b4a7bef
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/92/120192/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/index.rst', 'doc/source/conf.py']",2,4ba3590a4713160630188eb4ba74a3dc41434ae4,rss-feed,"exclude_patterns = [ '**/*template.rst', ] ",,4,2
openstack%2Fbarbican~master~Iadfd19b24496f52afe0e0ca1b5be9a12a4cd2468,openstack/barbican,master,Iadfd19b24496f52afe0e0ca1b5be9a12a4cd2468,Switch barbican.sh to use testr,MERGED,2014-09-19 20:20:01.000000000,2014-09-26 13:51:00.000000000,2014-09-26 13:50:59.000000000,"[{'_account_id': 3}, {'_account_id': 7789}, {'_account_id': 7973}, {'_account_id': 8004}]","[{'number': 1, 'created': '2014-09-19 20:20:01.000000000', 'files': ['bin/barbican.sh'], 'web_link': 'https://opendev.org/openstack/barbican/commit/6eb56cb62f66132ab8d1ad042000cc4eb38f4952', 'message': 'Switch barbican.sh to use testr\n\nCurrently, the script attempts to run unittests through\nnosetests instead of testr. Switching this to testr.\n\nChange-Id: Iadfd19b24496f52afe0e0ca1b5be9a12a4cd2468\nCloses-Bug: 1362714\n'}]",0,122855,6eb56cb62f66132ab8d1ad042000cc4eb38f4952,8,4,1,7262,,,0,"Switch barbican.sh to use testr

Currently, the script attempts to run unittests through
nosetests instead of testr. Switching this to testr.

Change-Id: Iadfd19b24496f52afe0e0ca1b5be9a12a4cd2468
Closes-Bug: 1362714
",git fetch https://review.opendev.org/openstack/barbican refs/changes/55/122855/1 && git format-patch -1 --stdout FETCH_HEAD,['bin/barbican.sh'],1,6eb56cb62f66132ab8d1ad042000cc4eb38f4952,bug/1362714, python setup.py testr, nosetests,1,1
openstack%2Factivity-board~master~I63d78b66348a5ddc4321c513808e9530ae0566c9,openstack/activity-board,master,I63d78b66348a5ddc4321c513808e9530ae0566c9,Update Grimoire JavaScript library,MERGED,2014-09-26 13:48:38.000000000,2014-09-26 13:49:22.000000000,2014-09-26 13:49:22.000000000,"[{'_account_id': 3}, {'_account_id': 8000}]","[{'number': 1, 'created': '2014-09-26 13:48:38.000000000', 'files': ['browser/lib/vizgrimoire.min.js'], 'web_link': 'https://opendev.org/openstack/activity-board/commit/2d9ebc7009096caf449898a9f4fd91df1266c7bc', 'message': 'Update Grimoire JavaScript library\n\nChange-Id: I63d78b66348a5ddc4321c513808e9530ae0566c9\n'}]",0,124401,2d9ebc7009096caf449898a9f4fd91df1266c7bc,6,2,1,8000,,,0,"Update Grimoire JavaScript library

Change-Id: I63d78b66348a5ddc4321c513808e9530ae0566c9
",git fetch https://review.opendev.org/openstack/activity-board refs/changes/01/124401/1 && git format-patch -1 --stdout FETCH_HEAD,['browser/lib/vizgrimoire.min.js'],1,2d9ebc7009096caf449898a9f4fd91df1266c7bc,update-jslib,"(""for_each"",""below"",e,t,n),this},o.get_highest_occupied_cell=function(){var e,t=this.gridmap,n=[],r=[];for(var i=t.length-1;i>=1;i--)for(e=t[i].length-1;e>=1;e--)if(this.is_widget(i,e)){n.push(e),r[e]=i;break}var s=Math.max.apply(Math,n);return this.highest_occupied_cell={col:r[s],row:s},this.highest_occupied_cell},o.get_widgets_from=function(t,n){var r=this.gridmap,i=e();return t&&(i=i.add(this.$widgets.filter(function(){var n=e(this).attr(""data-col"");return n===t||n>t}))),n&&(i=i.add(this.$widgets.filter(function(){var t=e(this).attr(""data-row"");return t===n||t>n}))),i},o.set_dom_grid_height=function(){var e=this.get_highest_occupied_cell().row;return this.$el.css(""height"",e*this.min_widget_height),this},o.generate_stylesheet=function(t){var n="""",r=this.options.max_size_x,i=0,o=0,u,a;t||(t={}),t.cols||(t.cols=this.cols),t.rows||(t.rows=this.rows),t.namespace||(t.namespace=this.options.namespace),t.widget_base_dimensions||(t.widget_base_dimensions=this.options.widget_base_dimensions),t.widget_margins||(t.widget_margins=this.options.widget_margins),t.min_widget_width=t.widget_margins[0]*2+t.widget_base_dimensions[0],t.min_widget_height=t.widget_margins[1]*2+t.widget_base_dimensions[1];var f=e.param(t);if(e.inArray(f,s.generated_stylesheets)>=0)return!1;s.generated_stylesheets.push(f);for(u=t.cols;u>=0;u--)n+=t.namespace+' [data-col=""'+(u+1)+'""] { left:'+(u*t.widget_base_dimensions[0]+u*t.widget_margins[0]+(u+1)*t.widget_margins[0])+""px;} "";for(u=t.rows;u>=0;u--)n+=t.namespace+' [data-row=""'+(u+1)+'""] { top:'+(u*t.widget_base_dimensions[1]+u*t.widget_margins[1]+(u+1)*t.widget_margins[1])+""px;} "";for(var l=1;l<=t.rows;l++)n+=t.namespace+' [data-sizey=""'+l+'""] { height:'+(l*t.widget_base_dimensions[1]+(l-1)*t.widget_margins[1]*2)+""px;}"";for(var c=1;c<=r;c++)n+=t.namespace+' [data-sizex=""'+c+'""] { width:'+(c*t.widget_base_dimensions[0]+(c-1)*t.widget_margins[0]*2)+""px;}"";return this.add_style_tag(n)},o.add_style_tag=function(e){var t=n,r=t.createElement(""style"");return t.getElementsByTagName(""head"")[0].appendChild(r),r.setAttribute(""type"",""text/css""),r.styleSheet?r.styleSheet.cssText=e:r.appendChild(n.createTextNode(e)),this},o.generate_faux_grid=function(e,t){this.faux_grid=[],this.gridmap=[];var n,r;for(n=t;n>0;n--){this.gridmap[n]=[];for(r=e;r>0;r--)this.add_faux_cell(r,n)}return this},o.add_faux_cell=function(t,n){var r=e({left:this.baseX+(n-1)*this.min_widget_width,top:this.baseY+(t-1)*this.min_widget_height,width:this.min_widget_width,height:this.min_widget_height,col:n,row:t,original_col:n,original_row:t}).coords();return e.isArray(this.gridmap[n])||(this.gridmap[n]=[]),this.gridmap[n][t]=!1,this.faux_grid.push(r),this},o.add_faux_rows=function(e){var t=this.rows,n=t+(e||1);for(var r=n;r>t;r--)for(var i=this.cols;i>=1;i--)this.add_faux_cell(r,i);return this.rows=n,this.options.autogenerate_stylesheet&&this.generate_stylesheet(),this},o.add_faux_cols=function(e){var t=this.cols,n=t+(e||1);for(var r=t;r<n;r++)for(var i=this.rows;i>=1;i--)this.add_faux_cell(i,r);return this.cols=n,this.options.autogenerate_stylesheet&&this.generate_stylesheet(),this},o.recalculate_faux_grid=function(){var n=this.$wrapper.width();return this.baseX=(e(t).width()-n)/2,this.baseY=this.$wrapper.offset().top,e.each(this.faux_grid,e.proxy(function(e,t){this.faux_grid[e]=t.update({left:this.baseX+(t.data.col-1)*this.min_widget_width,top:this.baseY+(t.data.row-1)*this.min_widget_height})},this)),this},o.get_widgets_from_DOM=function(){return this.$widgets.each(e.proxy(function(t,n){this.register_widget(e(n))},this)),this},o.generate_grid_and_stylesheet=function(){var n=this.$wrapper.width(),r=this.$wrapper.height(),i=Math.floor(n/this.min_widget_width)+this.options.extra_cols,s=this.$widgets.map(function(){return e(this).attr(""data-col"")});s=Array.prototype.slice.call(s,0),s.length||(s=[0]);var o=Math.max.apply(Math,s),u=this.options.extra_rows;return this.$widgets.each(function(t,n){u+=+e(n).attr(""data-sizey"")}),this.cols=Math.max(o,i,this.options.min_cols),this.rows=Math.max(u,this.options.min_rows),this.baseX=(e(t).width()-n)/2,this.baseY=this.$wrapper.offset().top,this.options.autogenerate_stylesheet&&this.generate_stylesheet(),this.generate_faux_grid(this.rows,this.cols)},e.fn.gridster=function(t){return this.each(function(){e(this).data(""gridster"")||e(this).data(""gridster"",new s(this,t))})},e.Gridster=o}(jQuery,window,document),vizjslib_git_revision=""36be874ff592b7bf6d95587e933a0a481aae510a"",vizjslib_git_tag=""2.1.3-108-g36be874"",function(){function n(e,t,n){var r="""";if(!t||t.length===0)return r;for(var i=0;i<t.date.length;i++)t.date[i]==n[e.index]&&(r=t.marks[i]);return r}function r(e,t){var r={name:e,config:{colors:t.colors,grid:{verticalLines:!1,horizontalLines:!1},mouse:{track:!0,trackY:!1,position:""ne""},yaxis:{min:0,autoscale:!0},legend:{show:!1,backgroundColor:""#FFFFFF"",backgroundOpacity:0}}};return t.gtype===""whiskers""?r.config.whiskers={show:!0,lineWidth:2}:r.config[""lite-lines""]={lineWidth:2,show:!0,fill:!1,fillOpacity:.5},t.y_labels&&(r.config.yaxis={showLabels:!0,min:0}),t.show_markers&&(r.config.markers={show:!0,position:""ct"",labelFormatter:function(e){return n(e,t.markers,t.dates)}}),r}function i(e,n,i,s){var o=Report.getAllMetrics(),u=null;$.each(i,function(i,a){config=s,a.envision&&(config=DataProcess.mergeConfig(s,a.envision)),$.inArray(i,t.envision_hide)===-1&&(n[i]=r(""report-""+e.getName()+""-""+i,config),u=i,o[i]&&(u=o[i].name),n[i].config.subtitle=u,e.getMainMetric()==i&&(n[i+""_relative""]=r(""report-""+e.getName()+""-""+i+""_relative"",config),n[i].config[""lite-lines""]={show:!1},n[i].config.lines={lineWidth:1,show:!0,stacked:!0,fill:!0,fillOpacity:1}))})}function s(n){var s=[""#ffa500"",""#00A8F0"",""#C0D800"",""#ffff00"",""#00ff00"",""#4DA74D"",""#9440ED""],o={colors:s,dates:t.dates,g_type:"""",markers:t.markers,y_labels:!1},u=Report.getDataSources(),a={},f={};return n?$.each(u,function(e,t){$.inArray(t.getName(),n)>-1&&(f=t.getMetrics(),i(t,a,f,o))}):$.each(u,function(e,t){f=t.getMetrics(),i(t,a,f,o)}),config=o,a.summary=r(""report-summary"",config),a.summary.config.xaxis={noTickets:10,showLabels:!0},a.summary.config.handles={show:!0},a.summary.config.selection={mode:""x""},a.summary.config.mouse={},a.connection={name:""report-connection"",adapterConstructor:e.components.QuadraticDrawing},a}function o(e,t){var n=[],r=null;return $.each(e,function(e,n){if(n.getMetrics()[t])return r=n,!1}),n.push(r),$.each(e,function(e,t){if(t===r)return;n.push(t)}),n}function u(n,r){var i=n.data.main_metric;t=n.data,r||(r=Report.getDataSources()),r=o(r,i);var u=[];for(var a=0;a<r.length;a++){if(r[a].getData().length===0)continue;u.push(r[a].getName())}var f=n.data,l=s(u),c=new e.Visualization({name:""report-""+u.join("","")}),h=new e.Interaction,p=new e.Interaction,d={};$.each(r,function(e,t){if(t.getData().length===0)return;d=$.extend(d,t.getMetrics())}),$.each(d,function(e,t){if($.inArray(e,f.envision_hide)!==-1)return;if(f[e]===undefined)return;l[e].data=f[e],l[e].data.length<Report.getProjectsList().length&&(l[e].config.legend.show=!0),f[e+""_relative""]&&(l[e].data=f[e+""_relative""])}),l.summary.data=f.summary,l[i].config.legend.show=!0,n.legend_show===!1&&(l[i].config.legend.show=!1),l[i].config.mouse.trackFormatter=n.trackFormatter,n.xTickFormatter&&(l.summary.config.xaxis.tickFormatter=n.xTickFormatter),l[i].config.yaxis.tickFormatter=n.yTickFormatter||function(e){return""$""+e};var v={};$.each(d,function(t,n){if(f[t]===undefined)return;$.inArray(t,f.envision_hide)===-1&&(v[t]=new e.Component(l[t]))}),connection=new e.Component(l.connection),summary=new e.Component(l.summary),$.each(v,function(e,t){c.add(t)}),c.add(connection).add(summary).render(n.container),$.each(v,function(e,t){h.follower(t)}),h.follower(connection).leader(summary).add(e.actions.selection,n.selectionCallback?{callback:n.selectionCallback}:null);var m=[];$.each(v,function(e,t){m.push(t)}),p.group(m).add(e.actions.hit),n.selection&&summary.trigger(""select"",n.selection)}var e=envision,t={};e.templates.Envision_Report=u}();if(Loader===undefined)var Loader={};(function(){function a(e,t){e.project_name===undefined&&(e.project_name=t.replace(""data/json"","""").replace(/\.\.\//g,""""));var n=Report.getProjectsData();n[e.project_name]={dir:t,url:e.project_url}}function f(e,t,n){$.when($.getJSON(e)).done(function(e){t(e,n),D()}).fail(function(){t([],n),D()})}function l(){var e=[""irc"",""mediawiki""],t=Report.getDataSources();$.each(t,function(t,n){$.inArray(n.getName(),e)>-1?n.setCompaniesData([]):f(n.getCompaniesDataFile(),n.setCompaniesData,n)})}function c(){var e=[""mediawiki""],t=Report.getDataSources();$.each(t,function(t,n){$.inArray(n.getName(),e)>-1?n.setReposData([]):f(n.getReposDataFile(),n.setReposData,n)}),f(Report.getReposMapFile(),Report.setReposMap)}function h(){var e=[""irc"",""mediawiki""],t=Report.getDataSources();$.each(t,function(t,n){$.inArray(n.getName(),e)>-1?n.setCountriesData([]):f(n.getCountriesDataFile(),n.setCountriesData,n)})}function p(){var e=[""irc"",""mediawiki""],t=Report.getDataSources();$.each(t,function(t,n){$.inArray(n.getName(),e)>-1?n.setDomainsData([]):f(n.getDomainsDataFile(),n.setDomainsData,n)})}function d(){var e=[""irc"",""mediawiki""],t=Report.getDataSources();$.each(t,function(t,n){$.inArray(n.getName(),e)>-1?n.setProjectsData([]):f(n.getProjectsDataFile(),n.setProjectsData,n)})}function v(){var e=Report.getDataSources();$.each(e,function(e,t){t.getName()===""its""&&f(t.getTimeToFixDataFile(),t.setTimeToFixData,t)})}function m(){var e=Report.getDataSources();$.each(e,function(e,t){t.getName()===""its""&&f(t.getMarkovTableDataFile(),t.setMarkovTableData,t)})}function g(){var e=Report.getDataSources();$.each(e,function(e,t){t.getName()===""mls""&&f(t.getTimeToAttentionDataFile(),t.setTimeToAttentionData,t)})}function y(){var e=Report.getDataSources();$.each(e,function(e,t){f(t.getDemographicsAgingFile(),t.setDemographicsAgingData,t),f(t.getDemographicsBirthFile(),t.setDemographicsBirthData,t)})}function b(e){var t=Report.getDataSources();$.each(t,function(e,t){var n=t.getTopDataFile();$.when($.getJSON(n)).done(function(e){t.setGlobalTopData(e),D()}).fail(function(){t.setGlobalTopData([],t),D()})})}function w(e,t){var n=null;return $.each(Report.getDataSources(),function(r,i){if(t==""repos""&&$.inArray(e,i.getReposData())>-1)return n=i,!1;if(t==""companies""&&$.inArray(e,i.getCompaniesData())>-1)return n=i,!1;if(t==""countries""&&$.inArray(e,i.getCountriesData())>-1)return n=i,!1;if(t==""domains""&&$.inArray(e,i.getDomainsData())>-1)return n=i,!1;if(t==""projects""&&$.inArray(e,i.getProjectsData())>-1)return n=i,!1}),n}function E(e){var t="""";return e===""repos""?t=""rep"":e===""companies""?t=""com"":e===""countries""?t=""cou"":e===""domains""?t=""dom"":e===""projects""&&(t=""prj""),t}function S(){var e=Report.getDataSources();$.each(e,function(e,t){f(t.getDataFile(),t.setData,t),f(t.getGlobalDataFile(),t.setGlobalData,t),t instanceof MLS&&f(t.getListsFile(),t.setListsData,t)})}function x(){f(""VizGrimoireJS/data/metrics.json"",Report.setMetricsDefinition)}function T(){var e=Report.getDataSources();$.each(e,function(e,t){f(t.getPeopleDataFile(),t.setPeopleData,t)})}function N(){f(Report.getDataDir()+""/people.json"",Report.setPeopleIdentities)}function C(e){return e.getCompaniesData()===null?!1:!0}function k(e){return e.getReposData()===null?!1:!0}function L(e){return e.getCountriesData()===null?!1:!0}function A(e){return e.getDomainsData()===null?!1:!0}function O(e){return e.getProjectsData()===null?!1:!0}function M(){var e=0,t=Report.getProjectsData(),n=Report.getProjectsDirs();for(var r in t)e++;return e<n.length?!1:!0}function _(){var e=!0;if(Report.getProjectData()===null||Report.getVizConfig()===null)return!1;if(Report.getConfig()===null&&Report.getMarkers()===null)return!1;if(Report.getReposMap()===null)return!1;if(Report.getConfig()===null&&!M())return!1;var t=Report.getDataSources();return $.each(t,function(t,n){if(n.getData()===null)return e=!1,!1;if(n.getGlobalData()===null)return e=!1,!1;if(n.getGlobalTopData()===null)return e=!1,!1;if(n.getDemographicsData().aging===undefined||n.getDemographicsData().birth===undefined)return e=!1,!1;if(n.getName()===""its""&&n.getTimeToFixData()===null)return e=!1,!1;if(n.getName()===""mls""&&n.getTimeToAttentionData()===null)return e=!1,!1}),e}function D(){if(_()){for(var n=0;n<t.length;n++)t[n]();t=[]}if(Loader.check_data_loaded())for(var r=0;r<e.length;r++)e[r].called!==!0&&e[r](),e[r].called=!0}var e=[],t=[],n=[],r=!1,i=!1,s=!1,o=[""scr"",""irc"",""mediawiki""],u=[""scm"",""mls"",""its""];Loader.data_ready=function(t){e.push(t)},Loader.data_ready_global=function(e){t.push(e)},Loader.data_load=function(){Report.getConfig()!==null&&Report.getConfig().project_info!==undefined?(Report.setProjectData(Report.getConfig().project_info),Report.getConfig().markers&&f(Report.getMarkersFile(),function(e,t){Report.setMarkers(e)})):(f(Report.getProjectFile(),function(e,t){Report.setProjectData(e)}),f(Report.getMarkersFile(),function(e,t){Report.setMarkers(e)}));var e=Report.getProjectsDirs();for(var t=0;t<e.length;t++){var n=e[t],r=Report.getDataDir()+""/project-info.json"";f(r,a,n)}f(Report.getProjectsHierarchyFile(),Report.setProjectsHierarchy),f(Report.getMenuElementsFile(),Report.setMenuElements),f(Report.getVizConfigFile(),function(e,t){Report.setVizConfig(e)}),x(),S(),b(""authors""),v(),g(),y(),m();if(Report.getConfig()!==null&&Report.getConfig().reports!==undefined){var i=Report.getConfig().reports;$.inArray(""companies"",i)>-1&&l(),$.inArray(""repositories"",i)>-1&&c(),$.inArray(""countries"",i)>-1&&h(),$.inArray(""domains"",i)>-1&&p(),$.inArray(""projects"",i)>-1&&d(),$.inArray(""people"",i)>-1&&(T(),N())}else l(),c(),h(),p(),d(),T(),N()},Loader.get_file_data_div=function(e,t,n){$.when($.getJSON(e)).done(function(r){t(n,e,r)}).fail(function(){t(e,null)})},Loader.check_filters_page=function(e){var t=!0,n=[""repos"",""companies"",""countries""];return $.each(n,function(n,r){if(!Loader.check_filter_page(e,r))return t=!1,!1}),t},Loader.check_filter_page=function(e,t){var n=!0;e===undefined&&(e=1);var r=Report.getPageSize()*(e-1),i=r+Report.getPageSize();return $.each(Report.getDataSources(),function(e,s){var o=0;t===""repos""&&(o=s.getReposData().length),t===""companies""&&(o=s.getCompaniesData().length),t===""countries""&&(o=s.getCountriesData().length),t===""domains""&&(o=s.getDomainsData().length),t===""projects""&&(o=s.getProjectsData().length),i>o&&(i=o);for(var u=r;u<i;u++){var a;if(t===""repos""){a=s.getReposData()[u];if(s.getReposGlobalData()[a]===undefined||s.getReposMetricsData()[a]===undefined)return n=!1,!1}if(t===""companies""){a=s.getCompaniesData()[u];if(s.getCompaniesGlobalData()[a]===undefined||s.getCompaniesMetricsData()[a]===undefined)return n=!1,!1}if(t===""countries""){a=s.getCountriesData()[u];if(s.getCountriesGlobalData()[a]===undefined||s.getCountriesMetricsData()[a]===undefined)return n=!1,!1}if(t===""domains""){a=s.getDomainsData()[u];if(s.getDomainsGlobalData()[a]===undefined||s.getDomainsMetricsData()[a]===undefined)return n=!1,!1}if(t===""projects""){a=s.getProjectsData()[u];if(s.getProjectsGlobalData()[a]===undefined||s.getProjectsMetricsData()[a]===undefined)return n=!1,!1}}i=r+Report.getPageSize()}),n},Loader.filterTopCheck=function(e,t){var n=!0;return t===""repos""&&Loader.check_item(e,t,""top"")===!1?(ds=w(e,t),ds===null?(Report.log(""Can't find data source for ""+e),!0):($.inArray(ds.getName(),u)>=0&&Loader.data_load_item_top(e,ds,null,Convert.convertFilterTop,t,""top""),!1)):n},Loader.FilterItemCheck=function(e,t){var n=!0,r,i=Report.getReposMap();if(t===""repos""){if(Loader.check_item(e,t)===!1)return r=w(e,t),r===null?(Report.log(""Can't find data source for ""+e),!0):(Loader.data_load_item(e,r,null,Convert.convertFilterStudyItem,t,null),$.inArray(r.getName(),u)>=0&&Loader.data_load_item_top(e,r,null,Convert.convertFilterStudyItem,t),!1);if(i!==undefined&&i.length!==0){var s=[];$.each(Report.getDataSources(),function(n,r){var i=Convert.getRealItem(r,t,e);i!==undefined&&i!==null&&s.push(i)});if(Loader.check_items(s,t)===!1){for(var a=0;a<s.length;a++)if(Loader.check_item(s[a],t)===!1){r=w(s[a],t);if(r===null){Report.log(""Can't find ""+s[a]),Report.log(""Check repos-map.json"");continue}Loader.data_load_item(s[a],r,null,Convert.convertFilterStudyItem,t,s)}n=!1}}}else $.each(Report.getDataSources(),function(r,i){Loader.check_item(e,t)===!1&&(n=!1,Loader.data_load_item(e,i,null,Convert.convertFilterStudyItem,t,null),t===""companies""&&$.inArray(i.getName(),o)===-1&&Loader.data_load_item_top(e,i,null,Convert.convertFilterStudyItem,t))});return n},Loader.check_item=function(e,t,n){var r=!1;return $.each(Report.getDataSources(),function(i,s){if(t===""repos""){if(n===""top""){if($.inArray(s.getName(),u)>=0&&$.inArray(e,s.getReposData())>=0&&s.getRepositoriesTopData()[e]!==undefined)return r=!0,!1}else if(s.getReposGlobalData()[e]!==undefined&&s.getReposMetricsData()[e]!==undefined)return r=!0,!1}else if(t===""companies""){var a=s.getCompaniesData();if(a.length===0)r=!0;else if($.inArray(e,a)===-1)r=!0;else{if(s.getCompaniesGlobalData()[e]===undefined||s.getCompaniesMetricsData()[e]===undefined)return r=!1,!1;if($.inArray(s.getName(),o)===-1&&s.getCompaniesTopData()[e]===undefined)return r=!1,!1;r=!0}}else if(t===""countries""){var f=s.getCountriesData();if(f.length===0)r=!0;else if($.inArray(e,f)===-1)r=!0;else{if(s.getCountriesGlobalData()[e]===undefined||s.getCountriesMetricsData()[e]===undefined)return r=!1,!1;r=!0}}else if(t===""domains""){var l=s.getDomainsData();if(l.length===0)r=!0;else if($.inArray(e,l)===-1)r=!0;else{if(s.getDomainsGlobalData()[e]===undefined||s.getDomainsMetricsData()[e]===undefined)return r=!1,!1;r=!0}}else if(t===""projects""){var c=s.getProjectsData();if(c.length===0)r=!0;else if($.inArray(e,c)===-1)r=!0;else{if(s.getProjectsGlobalData()[e]===undefined||s.getProjectsMetricsData()[e]===undefined)return r=!1,!1;r=!0}}}),r},Loader.check_items=function(e,t){var n=!0;return $.each(e,function(e,r){if(Loader.check_item(r,t)===!1)return n=!1,!1}),n},Loader.data_load_items_page=function(e,t,n,r){t===undefined&&(t=1);if(r===""repos""&&e.getReposData()===null)return!1;if(r===""companies""&&e.getCompaniesData()===null)return!1;if(r===""countries""&&e.getCountriesData()===null)return!1;if(r===""domains""&&e.getDomainsData()===null)return!1;if(r===""projects""&&e.getProjectsData()===null)return!1;var i=0;r===""repos""&&(i=e.getReposData().length),r===""companies""&&(i=e.getCompaniesData().length),r===""countries""&&(i=e.getCountriesData().length),r===""domains""&&(i=e.getDomainsData().length),r===""projects""&&(i=e.getProjectsData().length);if(i===0)return!0;var s=Report.getPageSize()*(t-1),o=s+Report.getPageSize();o>i&&(o=i);for(var u=s;u<o;u++)if(r===""repos""){var a=e.getReposData()[u];Loader.data_load_item(a,e,t,n,""repos"")}else if(r===""companies""){var f=e.getCompaniesData()[u];Loader.data_load_item(f,e,t,n,""companies"")}else if(r===""countries""){var l=e.getCountriesData()[u];Loader.data_load_item(l,e,t,n,""countries"")}else if(r===""domains""){var c=e.getDomainsData()[u];Loader.data_load_item(c,e,t,n,""domains"")}else if(r===""projects""){var h=e.getProjectsData()[u];Loader.data_load_item(h,e,t,n,""projects"")}},Loader.check_people_item=function(e){var t=!0;return $.each(Report.getDataSources(),function(n,r){if(r.getPeopleGlobalData()[e]===undefined||r.getPeopleMetricsData()[e]===undefined)return t=!1,!1}),t},Loader.data_load_people_item=function(e,t,n){var r=t.getDataDir()+""/people-""+e+""-""+t.getName(),i=r+""-evolutionary.json"",s=r+""-static.json"";$.when($.getJSON(i),$.getJSON(s)).done(function(r,i){t.addPeopleMetricsData(e,r[0],t),t.addPeopleGlobalData(e,i[0],t),Loader.check_people_item(e)&&n(e)}).fail(function(){t.addPeopleMetricsData(e,[],t),t.addPeopleGlobalData(e,[],t),Loader.check_people_item(e)&&n(e)})},Loader.data_load_item_top=function(e,t,n,r,i,s){var o=t.getDataDir()+""/""+e+""-""+t.getName();o+=""-""+E(i)+""-top-"";if(t.getName()===""scm"")o+=""authors"";else if(t.getName()===""its"")o+=""closers"";else{if(t.getName()!==""mls"")return;o+=""senders""}o+="".json"",$.when($.getJSON(o)).done(function(n){i===""companies""?t.addCompanyTopData(e,n):i===""repos""&&t.addRepositoryTopData(e,n)}).fail(function(){i===""companies""?t.addCompanyTopData(e,[]):i===""repos""&&t.addRepositoryTopData(e,[])}).always(function(){Loader.check_item(e,i,s)&&(r.called_item||r(i),r.called_item=!0)})},Loader.data_load_item=function(e,t,n,r,i,s){var o=[""irc"",""mediawiki""],u=[""irc"",""mediawiki""],a=[""irc"",""mediawiki""],f=[""mediawiki""],l=[""irc"",""mediawiki""];if(i===""repos""){if($.inArray(t.getName(),f)>-1){t.addRepoMetricsData(e,[],t),t.addRepoGlobalData(e,[],t);return}}else if(i===""companies""){if($.inArray(t.getName(),u)>-1){t.addCompanyMetricsData(e,[],t),t.addCompanyGlobalData(e,[],t);return}}else if(i===""countries""){if($.inArray(t.getName(),o)>-1){t.addCountryMetricsData(e,[],t),t.addCountryGlobalData(e,[],t);return}}else if(i===""domains""){if($.inArray(t.getName(),a)>-1){t.addDomainMetricsData(e,[],t),t.addDomainGlobalData(e,[],t);return}}else{if(i!==""projects"")return;if($.inArray(t.getName(),l)>-1){t.addDomainMetricsData(e,[],t),t.addDomainGlobalData(e,[],t);return}}var c=encodeURIComponent(e),h=t.getDataDir()+""/""+c+""-"";h+=t.getName()+""-""+E(i);var p=h+""-evolutionary.json"",d=h+""-static.json"";$.when($.getJSON(p),$.getJSON(d)).done(function(n,r){i===""repos""?(t.addRepoMetricsData(e,n[0],t),t.addRepoGlobalData(e,r[0],t)):i===""companies""?(t.addCompanyMetricsData(e,n[0],t),t.addCompanyGlobalData(e,r[0],t)):i===""countries""?(t.addCountryMetricsData(e,n[0],t),t.addCountryGlobalData(e,r[0],t)):i===""domains""?(t.addDomainMetricsData(e,n[0],t),t.addDomainGlobalData(e,r[0],t)):i===""projects""&&(t.addProjectMetricsData(e,n[0],t),t.addProjectGlobalData(e,r[0],t))}).always(function(){n!==null?Loader.check_filter_page(n,i)&&(r.called_page===undefined?(r.called_page={},r.called_page[i]=!0,r(i)):r.called_page[i]||(r(i),r.called_page[i]=!0)):s!==null?Loader.check_items(s,i)&&(r.called_map===undefined?(r.called_map={},r.called_map[i]=!0,r(i)):r.called_map[i]||(r(i),r.called_map[i]=!0)):Loader.check_item(e,i)&&(r.called_item===undefined?(r.called_item={},r.called_item[i]=!0,r(i,e)):r.called_item[i]||(r(i,e),r.called_item[i]=!0))})},Loader.check_data_loaded=function(){var e=!0;if(!_())return!1;var t=Report.getDataSources(),n=[""companies"",""repositories"",""countries"",""domains"",""projects""];return Report.getConfig()!==null&&Report.getConfig().reports!==undefined&&(n=Report.getConfig().reports),$.each(t,function(t,r){if(r.getPeopleData()===null)return e=!1,!1;if($.inArray(""companies"",n)>-1&&!C(r))return e=!1,!1;if($.inArray(""repositories"",n)>-1&&!k(r))return e=!1,!1;if($.inArray(""countries"",n)>-1&&!L(r))return e=!1,!1;if($.inArray(""domains"",n)>-1&&!A(r))return e=!1,!1;if($.inArray(""projects"",n)>-1&&!O(r))return e=!1,!1;if(r instanceof MLS&&r.getListsData()===null)return e=!1,!1}),e}})();var DataProcess={};(function(){DataProcess.info=function(){},DataProcess.paginate=function(e,t){if(t===undefined||t===0||isNaN(t))return e;var n=[],r=Report.getPageSize(),i=(t-1)*r;for(var s=i;s<r*t;s++)e[s]&&n.push(e[s]);return n},DataProcess.convert=function(e,t,n){return t===""aggregate""?e=DataProcess.aggregate(e,n):t===""substract""?(e=DataProcess.substract(e,n[0],n[1]),n=[""substract""]):t===""substract-aggregate""?(e=DataProcess.substract(e,n[0],n[1]),n=[""substract""],e=DataProcess.aggregate(e,n)):t===""divide""&&(e=DataProcess.divide(e,n[0],n[1]),n=[""divide""]),e},DataProcess.sortGlobal=function(e,t,n){t===undefined&&(t=""scm_commits"");var r=[],i=[],s={};s.name=[],s[t]=[];var o=null;n===""companies""?(i=e.getCompaniesData(),o=e.getCompaniesDataFull()):n===""repos""?(i=e.getReposData(),o=e.getReposDataFull()):n===""countries""?i=e.getCountriesData():n===""domains""?(i=e.getDomainsData(),o=e.getDomainsDataFull()):n===""projects""&&(i=e.getProjectsData());if(i===null)return[];if(o===null)return i;if(o instanceof Array||t in o==0)return i;for(var u=0;u<o[t].length;u++){var a=o[t][u];a===""NA""&&(a=0),r.push([o.name[u],a])}return r.sort(function(e,t){return t[1]-e[1]}),$.each(r,function(e,n){s.name.push(n[0]),s[t].push(n[1])}),s.name},DataProcess.orderItems=function(e){$.each($(""[class^='FilterItems']""),function(t,n){order_by=$(this).data(""order-by"");if(order_by!==undefined){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;var r=$(this).data(""filter"");if(r===undefined)return;if(r!==e)return;Report.log(""Ordering with ""+order_by+"" ""+ds+"" for ""+r);var i=DataProcess.sortGlobal(DS,order_by,r);return r===""companies""&&DS.setCompaniesData(i),r===""repos""&&DS.setReposData(i),r===""countries""&&DS.setCountriesData(i),r===""domains""&&DS.setDomainsData(i),!1}})},DataProcess.mergeConfig=function(e,t){var n={};return $.each(e,function(e,t){n[e]=t}),$.each(t,function(e,t){n[e]=t}),n},DataProcess.hideEmail=function(e){var t=e;return typeof e==""string""&&e.indexOf(""@"")>-1&&(t=e.split(""@"")[0]),t},DataProcess.selectPersonName=function(e){var t="""",n,r;for(var i=0;i<e.identity.length;i++)n=e.identity[i],r=e.type[i],r===""name""&&n.length>t.length&&(t=n);return t},DataProcess.selectPersonEmail=function(e){var t="""",n,r;for(var i=0;i<e.identity.length;i++)n=e.identity[i],r=e.type[i],r===""email""&&(t=n);return t},DataProcess.frameTime=function(e,t){var n={},r=-1,i=-1,s=0;if(t.length===0)return e;var o=e[t[0]].length,u=0;$.each(t,function(t,n){s=0;for(u=0;u<e[n].length;u++){if(e[n][u]!==0){r===-1&&(r=s),s<r&&(r=s);break}s++}}),$.each(t,function(t,n){s=0;for(u=e[n].length-1;u>=0;u--){if(e[n][u]!==0){i===-1&&(i=s),s<i&&(i=s);break}s++}});for(var a in e){n[a]=[];for(u=0;u<e[a].length;u++){if(u<r)continue;if(u>=o-i)continue;n[a].push(e[a][u])}}return n},DataProcess.filterDates=function(e,t,n){var r={};return $.each(n,function(i,s){r[i]=[],$.each(s,function(s,o){var u=n.unixtime[s];u>e&&(!t||t&&u<=t)&&r[i].push(o)})}),r},DataProcess.filterYear=function(e,t){e=parseInt(e,null);var n=(new Date(e.toString())).getTime(),r=(new Date((e+1).toString())).getTime(),i=filterDates(n,r,t);return i},DataProcess.fillDates=function(e,t){if(e[0].length===0)return t;var n=[[],[]],r=0;if(e[0][0]>t[0][0])for(r=0;r<t[0].length;r++)n[0][r]=t[0][r],n[1][r]=t[1][r];for(r=0;r<e[0].length;r++)pos=n[0].indexOf(e[0][r]),pos===-1&&(n[0].push(e[0][r]),n[1].push(e[1][r]));if(e[0][e[0].length-1]<t[0][t[0].length-1])for(r=0;r<t[0].length;r++)pos=n[0].indexOf(t[0][r]),pos===-1&&(n[0].push(t[0][r]),n[1].push(t[1][r]));return n},DataProcess.fillHistory=function(e,t){var n=[[],[]];for(var r=0;r<e.length;r++)pos=t[0].indexOf(e[r]),n[0][r]=e[r],pos!=-1?n[1][r]=t[1][pos]:n[1][r]=0;return n},DataProcess.fillHistoryLines=function(e,t){var n=[[],[]],r=[[],[]],i=[];for(var s=0;s<t.length;s++)n[0].push(t[s][0]),n[1].push(t[s][1]);r=DataProcess.fillHistory(e,n);for(s=0;s<e.length;s++)i.push([r[0][s],r[1][s]]);return i},DataProcess.addRelativeValues=function(e,t){if(e[t]===undefined)return;e[t+""_relative""]=[];var n=[];$.each(e[t],function(e,t){var r=t.data[1];for(var i=0;i<r.length;i++)n[i]===undefined&&(n[i]=0),n[i]+=r[i]}),$.each(e[t],function(r,i){var s=[];for(var o=0;o<i.data[0].length;o++)if(n[o]===0)s[o]=0;else{var u=i.data[1][o]/n[o]*100;s[o]=u}e[t+""_relative""].push({label:i.label,data:[i.data[0],s]})})},DataProcess.aggregate=function(e,t){var n={};return t instanceof Array||(t=[t]),$.each(e,function(r,i){if($.inArray(r,t)>-1){var s=[];s[0]=e[r][0];for(var o=1;o<e[r].length;o++)s[o]=s[o-1]+e[r][o];n[r]=s}else n[r]=e[r]}),n},DataProcess.substract=function(e,t,n){var r={},i=[];for(var s=0;s<e[t].length;s++)i[s]=e[t][s]-e[n][s];return $.each(e,function(t,n){r[t]=e[t]}),r.substract=i,r},DataProcess.divide=function(e,t,n){var r={},i=[];for(var s=0;s<e[t].length;s++)e[t][s]===0||e[n][s]===0?i[s]=0:i[s]=parseInt(e[t][s]/e[n][s],null);return $.each(e,function(t,n){r[t]=e[t]}),r.divide=i,r},DataProcess.revomeLastPoint=function(e){var t={};return $.each(e,function(n,r){t[n]=[];for(var i=0;i<e[n].length-1;i++)t[n].push(e[n][i])}),t}})();var Utils={};(function(){function e(){return $.urlParam(""release"")===null?!1:!0}function t(){return params="""",document.URL.split(""?"").length>1&&(params=document.URL.split(""?"")[1]),params}function n(){return aux=document.URL.split(""?"")[0].split(""/""),res=aux[aux.length-1],res}function r(e){return url=e,t().length>0&&(url+=""?""+t()),url}function i(t){return url=t,e()&&(url+=""?release=""+$.urlParam(""release"")),url}Utils.paramsInURL=t,Utils.isReleasePage=e,Utils.filenameInURL=n,Utils.createLink=r,Utils.createReleaseLink=i,$.urlParam=function(e){var t=(new RegExp(""[?&]""+e+""=([^&#]*)"")).exec(window.location.href);return t===null?null:t[1]||0}})();var HTMLComposer={};(function(){function e(e,t){var n='<div class=""col-md-12"">';return n+='<div class=""well well-small"">',n+='<div class=""row"">',n+='<div class=""col-md-12"">',n+=""<p>""+u(e)+""</p>"",n+=""</div>"",n+='<div class=""col-md-3"">',n+='<div class=""PersonSummary"" data-data-source=""'+e+'""></div>',n+=""</div>"",n+='<div class=""col-md-9"">',n+='<div class=""PersonMetrics"" data-data-source=""'+e+'""',n+='data-metrics=""'+t+'"" data-min=""true""',n+='data-frame-time=""true""></div>',n+=""</div>"",n+=""</div>"",n+=""</div>"",n+=""</div>"",n}function t(e,t,n){var r='<div class=""col-md-12"">';return r+='<div class=""row"">',r+='<div class=""col-md-3"">',r+='<div class=""well"">',r+='<div class=""FilterItemSummary"" data-data-source=""'+e+'"" data-filter=""'+t+'""></div>',r+=""</div></div>"",r+='<div class=""col-md-9"">',r+='<div class=""well"">',$.each(n,function(n,i){r+='<div class=""row""><div class=""col-md-12""></br></br></div></div>',r+='<div class=""row"">',r+='<div class=""col-md-12"">',r+='<div class=""FilterItemMetricsEvol"" data-data-source=""'+e+'""',r+='data-metrics=""'+i+'"" data-min=""true""',r+='data-filter=""'+t+'"" data-frame-time=""true""></div>',r+=""</div></div>""}),r+=""</div></div></div></div>"",r}function n(e,t,n){var r=""<table class='table-condensed table-hover'>"";r+='<tr><td colspan=""2""><p class=""subsection-title"">'+u(e.getName())+""</p></td></tr>"";var i=""<tr><td>"",s=""</td></tr>"";return $.each(t,function(t,o){e.getMetrics()[t]?(r+=i+e.getMetrics()[t].name,t===""first_date""||t===""last_date""?r+='</td><td class=""numberInTD"">'+o+s:r+='</td><td class=""numberInTD"">'+Report.formatValue(o)+s):n[t]&&(r+=i+n[t],t===""first_date""||t===""last_date""?r+='</td><td class=""numberInTD"">'+o+s:r+='</td><td class=""numberInTD"">'+Report.formatValue(o)+s)}),r+=""</table>"",r}function r(e,t){var n=""<table class='table-condensed table-hover'>"";return n+=""<tr><td>"",n+=""First contribution: </br>"",n+=""&nbsp;&nbsp;""+t.first_date,n+=""</td></tr><tr><td>"",n+=""Last contribution: </br>"",n+=""&nbsp;&nbsp;""+t.last_date,n+=""</td></tr><tr><td>"",e==""scm""?n+=""Commits:</br>&nbsp;&nbsp;""+t.scm_commits:e==""its""?n+=""Closed:</br>&nbsp;&nbsp;""+t.its_closed:e==""mls""?n+=""Sent:</br>&nbsp;&nbsp;""+t.mls_sent:e==""irc""?n+=""Sent:</br>&nbsp;&nbsp;""+t.irc_sent:e==""scr""&&(n+=""Closed:</br>&nbsp;&nbsp;""+t.scr_closed),n+=""</td></tr>"",n+=""</table>"",n}function s(e,t){var n='<p class=""section-title"" style=""margin-bottom:0px;""><i class=""fa fa-user fa-lg""></i> &nbsp;&nbsp;';return e.length>0?n+=e:t.length>0&&(t.indexOf(""@"")>0&&(t=t.split(""@"")[0]),n+=t),n+=""</p>"",n}function o(e,t){var n='<p class=""section-title"" style=""margin-bottom:0px;"">';return t===""companies""&&(n+='<i class=""fa fa-building-o""></i> &nbsp;&nbsp;'),n+=e,n+=""</p>"",n}function u(e){var t="""";return e===""scm""?t='<i class=""fa fa-code""></i> Source Code Management':e===""scr""?t='<i class=""fa fa-check""></i> Source Code Review':e===""its""?t='<i class=""fa fa-ticket""></i> Issue tracking system':e===""mls""?t='<i class=""fa fa-envelope-o""></i> Mailing Lists':e===""irc""?t='<i class=""fa fa-comment-o""></i> IRC Channels':e===""mediawiki""?t='<i class=""fa fa-pencil-square-o""></i> Wiki':e===""releases""&&(t='<i class=""fa fa-umbrella""></i> Forge Releases'),t}function a(){return html="""",params=""?data_dir=""+$.urlParam(""data_dir"")+""&release=""+$.urlParam(""release""),html+='<li><a href=""./""><i class=""fa fa-home""></i> Home</a></li>',html+='<li><a href=""./scm-companies.html'+params+'""><i class=""fa fa-code""></i> Source code repositories by companies</a></li>',html+='<li><a href=""./mls-companies.html'+params+'""><i class=""fa fa-envelope-o""></i> Mailing Lists by companies</a></li>',html+='<li><a href=""./its-companies.html'+params+'""><i class=""fa fa-ticket""></i> Tickets by companies</a></li>',html}function f(e,t){return t.length===0?"""":(unsupported=[""irc.html"",""qaforums.html"",""project.html""],ah_label=""&nbsp;All history&nbsp;"",label=e,label===null?label=ah_label:(label=""&nbsp; ""+label[0].toUpperCase()+label.substring(1)+"" release &nbsp;"",t.reverse().push(ah_label),t.reverse()),html='<div class=""input-group-btn"">',html+='<button type=""button"" class=""btn btn-default btn-lg btn-releaseselector dropdown-toggle""',html+='data-toggle=""dropdown"">',html+=label,html+='<span class=""caret""></span>',html+=""</button>"",html+='<ul class=""dropdown-menu pull-left"">',page_name=Utils.filenameInURL(),unsupported.indexOf(page_name)<0?$.each(t,function(e,t){var n=[];params=Utils.paramsInURL ().split(""&"");for(i=0;i<params.length;i++){sub_value=params[i];if(sub_value.length===0)continue;sub_value.indexOf(""release"")===0?t!=ah_label&&n.push(""release=""+t):n.push(sub_value)}$.urlParam(""release"")===null&&n.push(""release=""+t),t===ah_label?html+='<li><a href=""'+page_name+""?""+n.join(""&"")+'"" data-value=""'+t+'""> '+t+""</a></li>"":html+='<li><a href=""'+page_name+""?""+n.join(""&"")+'"" data-value=""'+t+'""> '+t[0].toUpperCase()+t.substring(1)+"" release</a></li>""}):html+=""<li><i>No releases for this section</i></li>"",html+=""</ul>"",html+=""</div>"",html)}function l(e,t,n,r){return html="""",html+=""<!-- irc -->"",html+='<div class=""row invisible-box"">',blabels=t.split("",""),bmetrics=n.split("",""),html+=h(e,blabels,bmetrics),html+='<div class=""col-md-5"">',tsm=r.split("",""),html+=p(e,tsm[0]),html+=""</div>"",html+='<div class=""col-md-5"">',html+=p(e,tsm[1]),html+=""</div>"",html+=""</div>"",html+=""<!-- end irc -->"",html}function c(e,t,n,r){return html="""",html+='<div class=""col-md-'+e+'"">',html+='<div class=""row thin-border"">',html+='<div class=""col-md-12"">'+t+""</div>"",html+=""</div>"",html+='<div class=""row"">',html+='<div class=""col-md-12 medium-fp-number"">',target_page=Utils.createLink(n+"".html""),html+='<a href=""'+target_page+'""> <span class=""GlobalData""',html+='data-data-source=""'+n+'"" data-field=""'+r+'""></span>',html+=""</a>"",html+=""</div>"",html+=""</div>"",html+=""</div>"",html}function h(e,t,n){return html="""",html+=""<!-- summary box-->"",html+='<div class=""col-md-2"">',html+='<div class=""well well-small"">',html+='<div class=""row thin-border"">',html+='<div class=""col-md-12"">'+t[0]+""</div>"",html+=""</div>"",html+='<div class=""row grey-border"">',html+='<div class=""col-md-12 big-fp-number"">',target_page=Utils.createLink(e+"".html""),e===""releases""&&(target_page=Utils.createLink(""forge.html"")),html+='<a href=""'+target_page+'""> <span class=""GlobalData""',html+='data-data-source=""'+e+'"" data-field=""'+n[0]+'""></span>',html+=""</a>"",html+=""</div>"",html+=""</div>"",html+='<div class=""row"" style=""padding: 5px 0px 0px 0px;"">',t.length===2&&n.length===2?html+=c(""12"",t[1],e,n[1]):t.length===3&&n.length===3?(html+=c(""6"",t[1],e,n[1]),html+=c(""6"",t[2],e,n[2])):t.length===4&&n.length===4&&(html+=c(""4"",t[1],e,n[1]),html+=c(""4"",t[2],e,n[2]),html+=c(""4"",t[3],e,n[3])),html+=""</div>"",html+=""</div>"",html+=""</div>"",html+=""<!-- end summary box -->"",html}function p(e,t){return html="""",html+='<div class=""well well-small"">',html+='<div class=""MetricsEvol"" data-data-source=""'+e+'""',html+='data-metrics=""'+t+'"" data-min=""true"" style=""height: 100px;""',html+='data-light-style=""true""></div>',html+='<a href=""irc.html"" style=""color: black;"">',html+=' <span class=""MicrodashText"" data-metric=""'+t+'""></span>',html+=""</a>"",html+=""</div>"",html}function d(e,t,n,r){text={companies:""Companies"",""companies-summary"":""Companies summary"",contributors:""Contributors"",countries:""Countries"",domains:""Domains"",projects:""Projects"",repos:""Repositories"",tags:""Tags"",states:""States""},html="""",html+='<li class=""dropdown"">',html+='<a href=""#"" class=""dropdown-toggle"" data-toggle=""dropdown"">',html+='<i class=""fa '+e+'""></i>&nbsp;'+t+' <b class=""caret""></b></a>',html+='<ul class=""dropdown-menu navmenu-nav"">';var i=Utils.createLink(n+"".html"");return html+='<li><a href=""'+i+'"">&nbsp;Overview</a></li>',$.each(r,function(e,t){i=Utils.createLink(n+""-""+t+"".html"");if(text.hasOwnProperty(t)){var r=text[t];if(t===""repos""){var s=Report.getDataSourceByName(n);r=s.getLabelForRepositories(),r=r.charAt(0).toUpperCase()+r.slice(1)}html+='<li><a href=""'+i+'"">&nbsp;'+r+""</a></li>""}else html+='<li><a href=""'+i+'"">&nbsp;'+t+""</a></li>""}),html+=""</ul></li>"",html}function v(){return html="""",html+=""<!-- summary bar -->"",html+='<div class=""capped-box overall-summary "">',html+='<div class=""stats-switcher-viewport js-stats-switcher-viewport"">',html+='<ul class=""numbers-summary"">',html+='<li><a href=""'+Utils.createReleaseLink(""scm.html"")+'""><span class=""GlobalData"" ',html+='data-data-source=""scm"" data-field=""scm_commits""></span></a> commits</li>',html+='<li><a href=""'+Utils.createReleaseLink(""scm.html"")+'""><span class=""GlobalData"" ',html+='data-data-source=""scm"" data-field=""scm_authors""></span></a> developers ',html+=""</li>"",html+='<li><a href=""'+Utils.createReleaseLink(""its.html"")+'""><span class=""GlobalData"" ',html+='data-data-source=""its"" data-field=""its_opened""></span></a> tickets</li>',html+='<li><a href=""'+Utils.createReleaseLink(""mls.html"")+'""><span class=""GlobalData"" ',html+='data-data-source=""mls"" data-field=""mls_sent""></span></a> mail messages ',html+=""</li>"",html+=""</ul>"",html+=""</div>"",html+=""</div>"",html+=""<!-- end of summary bar -->"",html}function m(e,t){html="""",link_exists=!1;try{fname=e.split(""."")[0],section=fname.split(""-"")[0],subsection=fname.split(""-"")[1];var n=Report.getMenuElements();n[section].indexOf(subsection)>=0&&(link_exists=!0),Utils.isReleasePage()&&link_exists?(link_to=Utils.createReleaseLink(e),html='<a href=""'+link_to+'"">'+t+""</a>""):link_exists?html='<a href=""'+e+'"">'+t+""</a>"":html=t}catch(r){html=t}return html}HTMLComposer.personDSBlock=e,HTMLComposer.filterDSBlock=t,HTMLComposer.DSBlock=l,HTMLComposer.repositorySummaryTable=n,HTMLComposer.personSummaryTable=r,HTMLComposer.personName=s,HTMLComposer.itemName=o,HTMLComposer.sideMenu4Release=a,HTMLComposer.releaseSelector=f,HTMLComposer.sideBarLinks=d,HTMLComposer.overallSummaryBlock=v,HTMLComposer.smartLinks=m})();var Convert={};(function(){function e(e,t){return t.hasOwnProperty(e)&&t[e].title?t[e].title:undefined}function t(e,t){return e.project_id<t.project_id?-1:e.project_id>t.project_id?1:0}function n(e,t){var n=[],r=e,i="""",s={};while(t[r].hasOwnProperty(""parent_project""))i=t[r].parent_project,s=t[i],s.project_id=i,n.push(s),r=i;return n.reverse()}function r(e,n){var r=[],i={};return $.each(n,function(t,s){n[t].parent_project===e&&(i=n[t],i.project_id=t,r.push(i))}),r.sort(t),r}function i(t,n,i){var s="""",o=n.length;return o>0?(s+='<li class=""dropdown"">',s+='<span data-toggle=""tooltip"" title=""Project name""> '+e(t,i)+""</span>"",s+='&nbsp;<a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"">',s+='<span data-toggle=""tooltip"" title=""Select subproject"" class=""badge""> '+o+"" Subprojects </span></a>"",s+='<ul class=""dropdown-menu scrollable-menu"">',$.each(n,function(e,t){gchildren=r(t.project_id,i),gchildren.length>0?s+='<li><a href=""project.html?project='+t.project_id+'"">'+t.title+'&nbsp;&nbsp;<span data-toggle=""tooltip"" title=""Number of suprojects"" class=""badge"">'+gchildren.length+'&nbsp;<i class=""fa fa-rocket""></i></span></a></li>':s+='<li><a href=""project.html?project='+t.project_id+'"">'+t.title+""</a></li>""}),s+='<li class=""divider""></li>',s+='<li><a href=""./project_map.html""><i class=""fa fa-icon fa-sitemap""></i> Projects treemap</a></li>',s+=""</ul></li>""):s+=""<li>""+e(t,i)+""</li>"",s}function s(e){var t='<ol class=""breadcrumbtitle"">',s=Report.getProjectsHierarchy();if(s.length===0)return"""";e===undefined&&(e=""root"");var o=r(e,s),u=n(e,s);return u.length>0&&$.each(u,function(e,n){n.parent_project?t+='<li><a href=""project.html?project='+n.project_id+'"">'+n.title+""</a></li>"":t+='<li><a href=""./"">'+n.title+""</a></li>""}),t+=i(e,o,s),t+=""</ol>"",t}function o(e){var t="""";return t=e.replace("" "",""_""),t=t.toLowerCase(),t}function u(t,n,i){var s="""",a=n.length,f=o(t);return a>0?(s+=""<li>"",s+='<a href=""project.html?project='+f+'"">'+e(t,i)+""</a>"",s+='&nbsp;<a data-toggle=""collapse"" data-parent=""#accordion"" href=""#collapse'+f+'""><span class=""badge"">'+a+'&nbsp;subprojects</span></a><div id=""collapse'+f+'"" class=""panel-collapse collapse""><ul>',$.each(n,function(e,t){gchildren=r(t.project_id,i),s+=u(t.project_id,gchildren,i)}),s+=""</ul></li>""):s+='<li><a href=""project.html?project='+t+'"">'+e(t,i)+""</a></li>"",s}function a(){var e=""<ul>"",t=Report.getProjectsHierarchy();if(t.length===0)return"""";project_id=""root"";var i=r(project_id,t),s=n(project_id,t);return $.each(i,function(n,i){grandchildren=r(i.project_id,t),e+=u(i.project_id,grandchildren,t)}),e+=""</ul>"",e}function f(){var e=[],t={data_sources:""Data sources"",project_map:""Project map"",people:""Contributor"",company:""Company"",country:""Country"",domain:""Domain"",""scm-companies"":""Activity on code repositories by companies"",""mls-companies"":""Activity on mailing lists by companies"",""its-companies"":""Activity on issue trackers by companies""};url_no_params=document.URL.split(""?"")[0],url_tokens=url_no_params.split(""/"");var n=url_tokens[url_tokens.length-1].split(""."")[0];return n===""project""||n===""index""||n===""release""||n===""""?[]:t.hasOwnProperty(n)?(e.push([n,t[n]]),e):[[""#"",""Unavailable section name""]]}function l(){var e=[],t={mls:""MLS overview"",irc:""IRC overview"",its:""ITS overview"",qaforums:""QA Forums overview"",scr:""Code Review overview"",scm:""SCM overview"",wiki:""Wiki overview"",downloads:""Downloads"",forge:""Forge releases"",data_sources:""Data sources"",project_map:""Project map"",people:""Contributor"",company:""Company"",country:""Country"",domain:""Domain"",release:""Companies analysis by release""},n={companies:""Activity by companies"",contributors:""Activity by contributors"",countries:""Activity by companies"",domains:""Activity by domains"",projects:""Activity by project"",repos:""Activity by repositories"",states:""Activity by states"",tags:""Activity by tags""},r={repository:""Repository""};url_no_params=document.URL.split(""?"")[0],url_tokens=url_no_params.split(""/"");var i=url_tokens[url_tokens.length-1].split(""."")[0];if(i===""project""||i===""index""||i==="""")return[];var s=i.split(""-"");return s[0]===""repository""&&(ds_name=$.urlParam(""ds""),s=[ds_name,""repos"",""repository""]),t.hasOwnProperty(s[0])?(e.push([s[0],t[s[0]]]),s.length>0&&n.hasOwnProperty(s[1])&&(e.push([s[0]+""-""+s[1],n[s[1]]]),s.length>2&&r.hasOwnProperty(s[2])&&e.push([s[0],r[s[2]]])),e):[[""#"",""Unavailable section name""]]}function c(){return $.urlParam(""release"")!==null&&$.urlParam(""release"").length>0?!0:!1}function h(e){e===undefined&&(e=""root"");var t="""",n="""";t+='<ul class=""nav navmenu-nav"">';var r=Report.getMenuElements();return e===""root""&&(r.hasOwnProperty(""scm"")&&(aux=r.scm,aux_html=HTMLComposer.sideBarLinks(""fa-code"",""Source code management"",""scm"",aux),t+=aux_html),r.hasOwnProperty(""scr"")&&(aux=r.scr,aux_html=HTMLComposer.sideBarLinks(""fa-check"",""Code review"",""scr"",aux),t+=aux_html),r.hasOwnProperty(""its"")&&(aux=r.its,aux_html=HTMLComposer.sideBarLinks(""fa-ticket"",""Tickets"",""its"",aux),t+=aux_html),r.hasOwnProperty(""mls"")&&(aux=r.mls,aux_html=HTMLComposer.sideBarLinks(""fa-envelope-o"",""Mailing lists"",""mls"",aux),t+=aux_html),r.hasOwnProperty(""qaforums"")&&Utils.isReleasePage()===!1&&(aux=r.qaforums,aux_html=HTMLComposer.sideBarLinks(""fa-question"",""Q&A Forums"",""qaforums"",aux),t+=aux_html),r.hasOwnProperty(""irc"")&&Utils.isReleasePage()===!1&&(aux=r.irc,aux_html=HTMLComposer.sideBarLinks(""fa-comment-o"",""IRC"",""irc"",aux),t+=aux_html),r.hasOwnProperty(""downloads"")&&Utils.isReleasePage()===!1&&(aux=r.downloads,aux_html=HTMLComposer.sideBarLinks(""fa-download"",""Downloads"",""downloads"",aux),t+=aux_html),r.hasOwnProperty(""forge"")&&Utils.isReleasePage()===!1&&(aux=r.forge,aux_html=HTMLComposer.sideBarLinks(""fa-umbrella"",""Forge releases"",""forge"",aux),t+=aux_html),r.hasOwnProperty(""wiki"")&&Utils.isReleasePage()===!1&&(aux=r.wiki,aux_html=HTMLComposer.sideBarLinks(""fa-pencil-square-o"",""Wiki"",""wiki"",aux),t+=aux_html),r.hasOwnProperty(""studies"")&&Utils.isReleasePage()===!1&&(aux=r.studies,t+='<li class=""dropdown"">',t+='<a href=""#"" class=""dropdown-toggle"" data-toggle=""dropdown"">',t+='<i class=""fa fa-lightbulb-o""></i>&nbsp;Studies <b class=""caret""></b></a>',t+='<ul class=""dropdown-menu navmenu-nav"">',aux.indexOf(""demographics"")>=0&&(t+='<li><a href=""demographics.html"">&nbsp;Demographics</a></li>'),aux.indexOf(""release"")>=0&&(aux=Report.getReleaseNames().reverse(),latest_release=aux[0],t+='<li><a href=""release.html?release='+latest_release+'"">&nbsp;Companies by release</a></li>'),t+=""</ul></li>""),t+='<li><a href=""data_sources.html""><i class=""fa fa-database""></i> Data sources</a></li>',t+='<li><a href=""project_map.html""><i class=""fa fa-icon fa-sitemap""></i> Project map</a></li>',r.hasOwnProperty(""extra"")&&(aux=r.extra,n+='<li class=""sidemenu-divider""></li>',n+='<li class=""sidemenu-smallheader"">More links:</li>',$.each(aux,function(e,t){n+='<li><a href=""'+t[1]+'"">&nbsp;'+t[0]+""</a></li>""})),t+=n),t+=""</ul>"",t}function p(e){var t='<ol class=""breadcrumb"">';if(e===undefined){var n=l(),r=Utils.paramsInURL();if(n.length>0){t+='<li><a href=""./',Utils.isReleasePage()&&(t+=""?release=""+$.urlParam(""release"")),t+='"">Project Overview</a></li>';var i=1;$.each(n,function(e,r){n.length===i?t+='<li class=""active"">'+r[1]+""</li>"":Utils.isReleasePage()?(t+='<li><a href=""'+r[0]+"".html"",t+=""?release=""+$.urlParam(""release"")+'"">',t+=r[1]+""</a></li>""):t+='<li><a href=""'+r[0]+'.html"">'+r[1]+""</a></li>"",i+=1})}else t+='<li class=""active"">Project Overview</li>'}else t+=""<li> ""+l()+""</li>"";return t+=""</ol>"",t}function d(e){var t=Report.getParameterByName(""repository"");if(t&&$.inArray(t,e.getReposData())<0)return"""";var n=e.getName(),r="""",i=e.getLabelForRepository(),s=e.getLabelForRepositories();return t!==undefined?r=t:r=""All ""+s,html='<div class=""row""><span class=""col-md-12"">',html='<ol class=""filterbar""><li>Filtered by '+i+"":&nbsp;&nbsp;</li>"",html+='<li><div class=""dropdown""><button class=""btn btn-default dropdown-toggle"" type=""button"" id=""dropdownMenu1"" data-toggle=""dropdown""> '+r+' <span class=""caret""></span></button>',html+='<ul class=""dropdown-menu"" role=""menu"" aria-labelledby=""dropdownMenu1"">',t&&(html+='<li role=""presentation""><a role=""menuitem"" tabindex=""-1"" href=""'+n+'-contributors.html"">',html+=""All ""+s,html+=""</a></li>""),$.each(e.getReposData(),function(e,n){if(n===t)return;html+='<li role=""presentation""><a role=""menuitem"" tabindex=""-1"" href=""?repository=',html+=n,html+='"">',html+=n,html+=""</a></li>""}),html+=""</ul></div></li></ol>"",html+=""</span></div>"",html}function v(){data=Report.getProjectData(),document.title=data.project_name+"" Report by Bitergia"",data.title&&(document.title=data.title),$("".report_date"").text(data.date),$("".report_name"").text(data.project_name),str=data.blog_url,str&&str.length>0?($(""#blogEntry"").html(""<br><a href='""+str+""'>Blog post with some more details</a>""),$("".blog_url"").attr(""href"",data.blog_url)):$(""#more_info"").hide(),str=data.producer,str&&str.length>0?$(""#producer"").html(str):$(""#producer"").html(""<a href='http://bitergia.com'>Bitergia</a>""),$("".project_name"").text(data.project_name),$(""#project_url"").attr(""href"",data.project_url)}function m(e,t){t.help=!0;var n=$(e).data(""help"");n!==undefined&&(t.help=n),t.show_legend=!1,$(e).data(""frame-time"")&&(t.frame_time=!0),t.graph=$(e).data(""graph""),$(e).data(""min"")&&(t.show_legend=!1,t.show_labels=!0,t.show_grid=!0,t.help=!1),$(e).data(""legend"")&&(t.show_legend=!0),t.ligth_style=!1,$(e).data(""light-style"")&&(t.light_style=!0),$(e).data(""custom-title"")&&(t.custom_title=$(e).data(""custom-title"")),t.help&&$(e).data(""custom-help"")?t.custom_help=$(e).data(""custom-help""):t.custom_help="""",$(e).data(""repo-filter"")&&(t.repo_filter=$(e).data(""repo-filter""));var r=$(e).data(""start"");r&&(t.start_time=r);var i=$(e).data(""end"");i&&(t.end_time=i);var s=$(e).data(""remove-last-point"");return s&&(t.remove_last_point=!0),t}function g(){return Math.floor(Math.random()*1e3+1)}function y(e,t){if(e===""repos""){if(DS.getReposGlobalData()[t]===undefined||DS.getReposGlobalData()[t].length===0)return!1}else if(e===""companies""){if(DS.getCompaniesGlobalData()[t]===undefined||DS.getCompaniesGlobalData()[t].length===0)return!1}else if(e===""countries""){if(DS.getCountriesGlobalData()[t]===undefined||DS.getCountriesGlobalData()[t].length===0)return!1}else if(e===""companies"")if(DS.getDomainsGlobalData()[t]===undefined||DS.getDomainsGlobalData()[t].length===0)return!1;return!0}function b(){var e={};return e.show_desc=!1,e.show_title=!1,e.show_labels=!0,e.show_legend=!1,e}Convert.convertMicrodashText=function(){var e=$("".MicrodashText"");e.length>0&&$.each(e,function(e,t){$(this).empty();var n=$(this).data(""metric""),r=$(this).data(""name""),i=Report.getMetricDS(n)[0];if(i===undefined)return;var s=i.getGlobalData()[n],o='<div class=""row"">';r&&(o+='<div class=""col-md-3"">',o+='<span class=""dayschange"">'+i.basic_metrics[n].name+""</span>"",o+=""</div>""),$.each([365,30,7],function(e,t){var s=i.getMetrics()[n].column,u=i.getGlobalData()[n+""_""+t],a=i.getGlobalData()[""diff_net""+s+""_""+t],f=i.getGlobalData()[""percentage_""+s+""_""+t];f=Math.round(f*10)/10;if(u===undefined)return;var l="""";f===0?l=Math.abs(f):a>0?l=""+""+f:a<0&&(l=""-""+Math.abs(f)),r?o+='<div class=""col-md-3"">':o+='<div class=""col-md-4"">',o+='<span class=""dayschange"">Last '+t+"" days:</span>"",o+="" ""+Report.formatValue(u)+""<br>"",f===0?o+='<i class=""fa fa-arrow-circle-right""></i> <span class=""zeropercent"">&nbsp;'+l+""%</span>&nbsp;"":a>0?o+='<i class=""fa fa-arrow-circle-up""></i> <span class=""pospercent"">&nbsp;'+l+""%</span>&nbsp;"":a<0&&(o+='<i class=""fa fa-arrow-circle-down""></i> <span class=""negpercent"">&nbsp;'+l+""%</span>&nbsp;""),o+=""</div><!--col-md-4-->""}),o+=""</div><!--row-->"",$(t).append(o)})},Convert.convertMicrodash=function(){var e=$("".Microdash"");e.length>0&&$.each(e,function(e,t){$(this).empty();var n=$(this).data(""metric""),r=$(this).data(""text""),i=Report.getMetricDS(n)[0],s=i.getGlobalData()[n],o=""<div>"";o+='<div style=""float:left"">',o+='<span class=""medium-fp-number"">'+Report.formatValue(s),o+=""</span> ""+i.getMetrics()[n].name,o+=""</div>"",o+='<div id=""Microdash"" class=""MetricsEvol"" data-data-source=""'+i.getName()+'"" data-metrics=""'+n+'"" data-min=true style=""margin-left:10px; float:left;width:100px; height:25px;""></div>',o+='<div style=""clear:both""></div><div>',$.each([365,30,7],function(e,t){var r=i.getMetrics()[n].column,s=i.getGlobalData()[""diff_net""+r+""_""+t],u=i.getGlobalData()[""percentage_""+r+""_""+t],a=i.getGlobalData()[n+""_""+t];if(a===undefined)return;o+=""<span class='dayschange'>""+t+"" Days Change</span>:""+Report.formatValue(a)+""&nbsp;"",s===0?o+="""":s>0?(o+='<i class=""icon-circle-arrow-up""></i>',o+=""<small>(+""+u+""%)</small>&nbsp;""):s<0&&(o+='<i class=""icon-circle-arrow-down""></i>',o+=""<small>(-""+Math.abs(u)+""%)</small>&nbsp;"")}),o+=""</div>"",o+=""<div>"",$(t).append(o)})},Convert.convertSideBar=function(e){var t=$("".SideNavBar"");t.length>0&&$.each(t,function(t,n){$(this).empty(),n.id||(n.id=""SideNavBar"");var r;e&&(r=Report.cleanLabel(e));var i=h(r);$(""#""+n.id).append(i),data=Report.getProjectData(),$("".report_name"").text(data.project_name),Utils.isReleasePage()&&$("".report_name"").attr(""href"",""./?release=""+$.urlParam(""release""))})},Convert.convertProjectNavBar=function(e){var t=$("".ProjectNavBar"");t.length>0&&$.each(t,function(t,n){$(this).empty(),n.id||(n.id=""ProjectNavBar"");var r;e&&(r=Report.cleanLabel(e));var i=s(r);$(""#""+n.id).append(i)})},Convert.convertNavbar=function(){$.get(Report.getHtmlDir()+""navbar.html"",function(e){$(""#Navbar"").html(e);var t=Report.getParameterByName(""project"");Convert.convertProjectNavBar(t),Convert.convertReleaseSelector(),Convert.convertSideBar(t)})},Convert.convertReleaseSelector=function(){var e=Report.getReleaseNames();if(e.length>0){var t=$("".ReleaseSelector"");t.length>0&&$.each(t,function(t,n){$(this).empty(),n.id||(n.id=""ReleaseSelector""+g());var r=HTMLComposer.releaseSelector($.urlParam(""release""),e);$(""#""+n.id).append(r)})}},Convert.convertSectionBreadcrumb=function(e){var t=$("".SectionBreadcrumb"");t.length>0&&$.each(t,function(t,n){$(this).empty(),n.id||(n.id=""SectionBreadcrumb"");var r;e&&(r=Report.cleanLabel(e));var i=p(r);$(""#""+n.id).append(i)})},Convert.convertProjectMap=function(){var e=$("".ProjectMap"");e.length>0&&$.each(e,function(e,t){$(this).empty(),t.id||(t.id=""ProjectMap"");var n,r=a();$(""#""+t.id).append(r)})},Convert.convertFooter=function(){$.get(Report.getHtmlDir()+""footer.html"",function(e){$(""#Footer"").html(e),$(""#vizjs-lib-version"").append(vizjslib_git_tag)})},Convert.convertSummary=function(){div_param=""Summary"";var e=$("".""+div_param);e.length>0&&$.each(e,function(e,t){$(this).empty();var n=$(this).data(""data-source""),r=Report.getDataSourceByName(n);if(r===null)return;t.id=n+""-Summary"",r.displayGlobalSummary(t.id)})},Convert.convertRepositorySelector=function(){var e=$("".repository-selector"");e.length>0&&$.each(e,function(e,t){$(this).empty();var n=$(this).data(""data-source""),r=Report.getDataSourceByName(n);if(r===null)return;t.id=n+""-repository-selector"";var i=d(r);$(""#""+t.id).append(i)})},Convert.convertRefcard=function(){$.when($.get(Report.getHtmlDir()+""refcard.html""),$.get(Report.getHtmlDir()+""project-card.html"")).done(function(e,t){refcard=e[0],projcard=t[0],$(""#Refcard"").html(refcard),v(),$.each(Report.getProjectsData(),function(e,t){var n=""card-""+e.replace(""."","""").replace("" "","""");$(""#Refcard #projects_info"").append(projcard),$(""#Refcard #projects_info #new_card"").attr(""id"",n),$.each(Report.getDataSources(),function(t,r){if(r.getProject()!==e){$(""#""+n+"" .""+r.getName()+""-info"").hide();return}r.displayData(n)}),$(""#""+n+"" #project_name"").text(e),Report.getProjectsDirs.length>1&&$(""#""+n+"" .project_info"").append(' <a href=""VizGrimoireJS/browser/index.html?data_dir=../../'+t.dir+'"">Report</a>'),$(""#""+n+"" #project_url"").attr(""href"",t.url)})})},Convert.convertGlobalData=function(){var e=$("".GlobalData"");e.length>0&&$.each(e,function(e,t){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;var n=DS.getGlobalData(),r=$(this).data(""field"");$(this).text(Report.formatValue(n[r],r))})},Convert.convertProjectData=function(){var e=$("".ProjectData""),t=Report.getParameterByName(""project"");e.length>0&&$.each(e,function(e,n){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;var r=DS.getProjectsGlobalData()[t],i=$(this).data(""field"");$(this).text(Report.formatValue(r[i],i))})},Convert.convertRadarActivity=function(){var e=""RadarActivity"",t=$(""#""+e);t.length>0&&($.each(t,function(e,t){$(this).empty()}),Viz.displayRadarActivity(e))},Convert.convertRadarCommunity=function(){var e=""RadarCommunity"",t=$(""#""+e);t.length>0&&($.each(t,function(e,t){$(this).empty()}),Viz.displayRadarCommunity(""RadarCommunity""))},Convert.convertTreemap=function(){var e=""Treemap"",t=$(""#""+e);if(t.length>0){$.each(t,function(e,t){$(this).empty()});var n=$(""#Treemap"").data(""file"");$(""#Treemap"").empty(),Viz.displayTreeMap(""Treemap"",n)}},Convert.convertBubbles=function(){div_param=""Bubbles"";var e=$("".""+div_param);e.length>0&&$.each(e,function(e,t){$(this).empty();var n=$(this).data(""data-source""),r=Report.getDataSourceByName(n);if(r===null)return;if(r.getData().length===0)return;var i=$(this).data(""radius"");t.id=n+""-Bubbles"",r.displayBubbles(t.id,i)})},Convert.convertMetricsEvol=function(){var e={};e.show_desc=!1,e.show_title=!0,e.show_labels=!0;var t=Report.getVizConfig();t&&$.each(t,function(t,n){e[t]=n});var n=""MetricsEvol"",r=$("".""+n);r.length>0&&$.each(r,function(t,n){var r={};$.each(e,function(e,t){r[e]=t}),$(this).empty();var i=$(this).data(""metrics""),s=$(this).data(""data-source"");r.title=$(this).data(""title"");var o=Report.getDataSourceByName(s);if(o===null)return;r=m(n,r),n.id=i.replace(/,/g,""-"")+""-""+s+""-metrics-evol-""+this.id,n.id=n.id.replace(/\n|\s/g,""""),o.displayMetricsEvol(i.split("",""),n.id,r,$(this).data(""convert""))})},Convert.convertMetricsEvolCustomized=function(e){var t={};t.show_desc=!1,t.show_title=!0,t.show_labels=!0;var n=Report.getVizConfig();n&&$.each(n,function(e,n){t[e]=n});var r=""MetricsEvolCustomized"",i=$("".""+r);i.length>0&&$.each(i,function(n,r){if(e!==$(this).data(""filter""))return;var i={};$.each(t,function(e,t){i[e]=t}),$(this).empty();var s=$(this).data(""metrics""),o=$(this).data(""data-source"");i.title=$(this).data(""title"");var u=Report.getDataSourceByName(o);if(u===null)return;i=m(r,i),r.id=s.replace(/,/g,""-"")+""-""+o+""-metrics-evol-""+this.id,r.id=r.id.replace(/\n|\s/g,""""),u.displayMetricsEvol(s.split("",""),r.id,i,$(this).data(""convert""))})},Convert.convertMetricsEvolSelector=function(){var e={};e.show_desc=!1,e.show_title=!0,e.show_labels=!0;var t=Report.getVizConfig();t&&$.each(t,function(t,n){e[t]=n});var n=""MetricsEvol"",r=$("".""+n);r.length>0&&$.each(r,function(t,n){var r={};$.each(e,function(e,t){r[e]=t}),$(this).empty();var i=$(this).data(""metrics""),s=$(this).data(""data-source""),o=Report.getDataSourceByName(s);if(o===null)return;var u=Report.getParameterByName(""repository"");r=m(n,r),n.id=i.replace(/,/g,""-"")+""-""+s+""-metrics-evol-""+this.id,n.id=n.id.replace(/\n|\s/g,""""),o.displayMetricsEvol(i.split("",""),n.id,r,$(this).data(""convert""),u)})},Convert.convertMetricsEvolSet=function(){div_param=""MetricsEvolSet"";var e=$("".""+div_param);e.length>0&&$.each(e,function(e,t){$(this).empty();var n=$(this).data(""all""),r=$(this).data(""relative""),i=$(this).data(""summary-graph""),s=$(this).data(""legend-show"");t.id=o+""-MetricsEvolSet-""+this.id;if(n===!0)return t.id=o+""-All"",Viz.displayEnvisionAll(t.id,r,s,i),!1;var o=$(this).data(""data-source""),u=Report.getDataSourceByName(o);if(u===null)return;u.displayEnvision(t.id,r,s,i)})},Convert.convertTimeTo=function(){var e=""TimeTo"";divs=$("".""+e),divs.length>0&&$.each(divs,function(e,t){$(this).empty();var n=$(this).data(""data-source""),r=Report.getDataSourceByName(n);if(r===null)return;var i=$(this).data(""quantil""),s=$(this).data(""type"");t.id=n+""-time-to-""+s+""-""+i,s===""fix""&&r.displayTimeToFix(t.id,i),s===""attention""&&r.displayTimeToAttention(t.id,i)})},Convert.convertMarkovTable=function(){var e=""MarkovTable"",t=$("".""+e),n,r;t.length>0&&$.each(t,function(e,t){$(this).empty(),r=$(this).data(""data-source""),n=Report.getDataSourceByName(r);if(n===null)return;if(n.getData().length===0)return;var i=$(this).data(""title"");t.id=r+""-markov-table"",n.displayMarkovTable(t.id,i)})},Convert.convertLastActivity=function(){function t(t,n,r){var i=""<h4>Last ""+r+""</h4>"";$.each(Report.getDataSources(),function(t,s){var o=s.getGlobalData();$.each(o,function(t,s){var u=""_""+n;if(t.indexOf(u,t.length-u.length)!==-1){var a=t.substring(0,t.length-u.length);r=a,e[a]&&(r=e[a].name),i+=r+"":""+o[t]+""<br>""}})}),$(t).append(i)}var e=Report.getAllMetrics(),n=$("".LastActivity""),r=null,i={Week:7,Month:30,Quarter:90,Year:365};n.length>0&&$.each(n,function(e,n){r=$(n).data(""period""),t(n,i[r],r)})},Convert.convertTop=function(){var e=""Top"",t=$("".""+e),n,r;if(t.length>0){var i=0;$.each(t,function(t,s){$(this).empty(),r=$(this).data(""data-source""),n=Report.getDataSourceByName(r);if(n===null)return;if(n.getData().length===0)return;var o=!1;$(this).data(""show_all"")&&(o=!0);var u=$(this).data(""metric""),a=$(this).data(""limit""),f=$(this).data(""graph""),l=$(this).data(""people_links""),c=$(this).data(""threads_links""),h=$(this).data(""period""),p=$(this).data(""period_all""),d=Report.getParameterByName(""repository"");s.id=r+""-""+e+i++,f&&(s.id+=""-""+f),h===undefined&&p===undefined&&(p=!0),a===undefined&&(a=10),n.displayTop(s.id,o,u,h,p,f,a,l,c,d)})}},Convert.convertPersonMetrics=function(e,t){var n={};n.show_desc=!1,n.show_title=!1,n.show_labels=!0,divs=$("".PersonMetrics""),divs.length&&$.each(divs,function(r,i){$(this).empty(),ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;var s=$(this).data(""metrics"");n.show_legend=!1,n.help=!1,$(this).data(""frame-time"")&&(n.frame_time=!0),$(this).data(""legend"")&&(n.show_legend=!0),$(this).data(""person_id"")&&(e=$(this).data(""person_id"")),$(this).data(""person_name"")&&(t=$(this).data(""person_name"")),i.id=s.replace(/,/g,""-"")+""-people-metrics"",DS.displayMetricsPeople(e,t,s.split("",""),i.id,n)})},Convert.convertPersonData=function(e,t){var n=$("".PersonData""),r,i;n.length>0&&$.each(n,function(n,s){$(this).empty(),$(this).data(""person_id"")&&(e=$(this).data(""person_id"")),s.id||(s.id=""PersonData-""+e+""-""+g());var o=Report.getPeopleIdentities()[e];o?(r=DataProcess.selectPersonName(o),i=DataProcess.selectPersonEmail(o),i=""(""+DataProcess.hideEmail(i)+"")""):(t!==undefined?r=t:r=e,i=""""),html=HTMLComposer.personName(r,i),$(""#""+s.id).append(html)})},Convert.personSummaryBlock=function(e){var t=$("".PersonSummaryBlock"");t.length>0&&$.each(t,function(t,n){if(n.id.indexOf(""Parsed"")>=0)return;ds_name=$(this).data(""data-source""),metric_name=$(this).data(""metrics""),DS=Report.getDataSourceByName(ds_name);if(DS===null)return;if(DS.getData().length===0)return;if(DS.getPeopleMetricsData()[e].length===0)return;var r=HTMLComposer.personDSBlock(ds_name,metric_name);n.id||(n.id=""Parsed""+g()),$(""#""+n.id).append(r)})},Convert.convertPersonSummary=function(e,t){var n=$("".PersonSummary"");n.length>0&&$.each(n,function(n,r){$(this).empty(),ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;$(this).data(""person_id"")&&(e=$(this).data(""person_id"")),$(this).data(""person_name"")&&(t=$(this).data(""person_name"")),r.id=ds+""-refcard-people"",DS.displayPeopleSummary(r.id,e,t,DS)})},Convert.convertPeople=function(e,t){e===undefined&&(e=Report.getParameterByName(""id"")),t===undefined&&(t=Report.getParameterByName(""name""));if(e===undefined)return;if(Loader.check_people_item(e)===!1){$.each(Report.getDataSources(),function(t,n){Loader.data_load_people_item(e,n,Convert.convertPeople)});return}Convert.personSummaryBlock(e),Convert.convertPersonData(e,t),Convert.convertPersonSummary(e,t),Convert.convertPersonMetrics(e,t),Convert.activateHelp()},Convert.repositoryDSBlock=function(e){var t=$("".FilterDSBlock"");t.length>0&&$.each(t,function(t,n){if(n.id.indexOf(""Parsed"")>=0)return;ds_name=$(this).data(""data-source""),filter_name=$(this).data(""filter""),aux=$(this).data(""metrics""),metric_names=aux.split("",""),$.each(metric_names,function(e,t){metric_names[e]=metric_names[e].replace(/:/g,"","")}),DS=Report.getDataSourceByName(ds_name);if(DS===null)return;if(DS.getData().length===0)return;if(y(filter_name,e)){var r=HTMLComposer.filterDSBlock(ds_name,filter_name,metric_names);n.id||(n.id=""Parsed""+g()),$(""#""+n.id).append(r)}})},Convert.convertDSSummaryBlock=function(e){var t=$("".DSSummaryBlock"");t.length>0&&$.each(t,function(e,t){if(t.id.indexOf(""Parsed"")>=0)return;ds_name=$(this).data(""data-source""),box_labels=$(this).data(""box-labels""),box_metrics=$(this).data(""box-metrics""),ts_metrics=$(this).data(""ts-metrics""),DS=Report.getDataSourceByName(ds_name);if(DS===null)return;if(DS.getData().length===0)return;var n=HTMLComposer.DSBlock(ds_name,box_labels,box_metrics,ts_metrics);t.id||(t.id=""Parsed""+g()),$(""#""+t.id).append(n)})},Convert.convertOverallSummaryBlock=function(){var e=$("".OverallSummaryBlock"");e.length>0&&$.each(e,function(e,t){if(t.id.indexOf(""Parsed"")>=0)return;var n=HTMLComposer.overallSummaryBlock();t.id||(t.id=""Parsed""+g()),$(""#""+t.id).append(n)})},Convert.convertDemographics=function(){var e=$("".Demographics"");e.length>0&&$.each(e,function(e,t){$(this).empty(),ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;var n=$(this).data(""period"");t.id=""Demographics-""+ds+""-""+""-""+n,DS.displayDemographics(t.id,n)})},Convert.getRealItem=function(e,t,n){var r=Report.getReposMap();if(r===undefined||r.length===0)return $.inArray(n,e.getReposData())>-1?n:null;var i=null;if(t===""repos""){var s=e.getReposMetricsData()[n];s===undefined?$.each(r,function(t,r){$.each(Report.getDataSources(),function(t,s){if(r[s.getName()]===n)return i=r[e.getName()],!1});if(i!==null)return!1}):i=n}else i=n;return i},Convert.convertFilterItemsSummary=function(e){var t=""FilterItemsSummary"";divs=$("".""+t),divs.length>0&&$.each(divs,function(n,r){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;r.id=ds+""-""+t,$(this).empty(),e===""repos""&&DS.displayReposSummary(r.id,DS),e===""countries""&&DS.displayCountriesSummary(r.id,DS),e===""companies""&&DS.displayCompaniesSummary(r.id,DS),e===""domains""&&DS.displayDomainsSummary(r.id,DS),e===""projects""&&DS.displayProjectsSummary(r.id,DS)})},Convert.convertFilterItemsGlobal=function(e){var t=b(),n=""FilterItemsGlobal"";divs=$("".""+n),divs.length>0&&$.each(divs,function(r,i){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;var s=$(this).data(""metric""),o=$(this).data(""show-others""),u=$(this).data(""order-by"");t.show_legend=$(this).data(""legend""),$(""#""+$(this).data(""legend-div"")).length>0?t.legend={container:$(this).data(""legend-div"")}:t.legend={container:null},t.graph=$(this).data(""graph""),t.title=$(this).data(""title""),t.show_title=1,i.id=s+""-""+n,$(this).empty(),e===""repos""&&DS.displayMetricReposStatic(s,i.id,t,u,o),e===""countries""&&DS.displayMetricCountriesStatic(s,i.id,t,u,o),e===""companies""&&DS.displayMetricCompaniesStatic(s,i.id,t,u,o),e===""domains""&&DS.displayMetricDomainsStatic(s,i.id,t,u,o),e===""projects""&&DS.displayMetricProjectsStatic(s,i.id,t,u,o)})},Convert.convertFilterItemsNav=function(e,t){var n=""FilterItemsNav"";divs=$("".""+n);if(divs.length>0){var r=0;$.each(divs,function(i,s){ds=$ (this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;$(this).data(""page"")&&(t=$(this).data(""page"")),order_by=$(this).data(""order-by""),s.id=ds+""-""+n+""-""+r,r+=1,$(this).empty(),e===""repos""?DS.displayItemsNav(s.id,e,t,order_by):e===""countries""?DS.displayItemsNav(s.id,e,t):e===""companies""?DS.displayItemsNav(s.id,e,t):e===""domains""?DS.displayItemsNav(s.id,e,t):e===""projects""&&DS.displayItemsNav(s.id,e,t)})}},Convert.convertFilterItemsMetricsEvol=function(e){var t=b(),n=""FilterItemsMetricsEvol"";divs=$("".""+n),divs.length>0&&$.each(divs,function(r,i){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;var s=$(this).data(""metric""),o=!1;$(this).data(""stacked"")&&(o=!0),$(this).data(""min"")&&(config_viz.show_legend=!1,config_viz.show_labels=!0,config_viz.show_grid=!0,config_viz.help=!1);var u=$(this).data(""start""),a=$(this).data(""end"");t.lines={stacked:o},$(""#""+$(this).data(""legend-div"")).length>0?t.legend={container:$(this).data(""legend-div"")}:t.legend={container:null},t.show_legend=$(this).data(""legend""),t.mouse_tracker=$(this).data(""mouse_tracker"");var f=$(this).data(""remove-last-point"");f&&(t.remove_last_point=!0),i.id=s+""-""+n,$(this).empty(),e===""companies""?DS.displayMetricCompanies(s,i.id,t,u,a):e===""repos""?DS.displayMetricRepos(s,i.id,t,u,a):e===""domains""?DS.displayMetricDomains(s,i.id,t,u,a):e===""projects""&&DS.displayMetricProjects(s,i.id,t,u,a)})},Convert.convertFilterItemsMiniCharts=function(e,t){var n=b(),r=""FilterItemsMiniCharts"";divs=$("".""+r),divs.length>0&&$.each(divs,function(i,s){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;$(this).data(""page"")&&(t=$(this).data(""page""));var o=$(this).data(""metrics""),u=$(this).data(""order-by""),a=!0;$(this).data(""show_links"")!==undefined&&(a=$(this).data(""show_links""));var f=$(this).data(""start""),l=$(this).data(""end""),c=$(this).data(""convert"");$(this).data(""frame-time"")&&(n.frame_time=!0);var h=$(this).data(""remove-last-point"");h&&(n.remove_last_point=!0),s.id=o.replace(/,/g,""-"")+""-""+e+""-""+r,$(this).empty(),e===""repos""?DS.displayReposList(o.split("",""),s.id,n,u,t,a,f,l,c):e===""countries""?DS.displayCountriesList(o.split("",""),s.id,n,u,t,a,f,l,c):e===""companies""?DS.displayCompaniesList(o.split("",""),s.id,n,u,t,a,f,l,c):e===""domains""?DS.displayDomainsList(o.split("",""),s.id,n,u,t,a,f,l,c):e===""projects""&&DS.displayProjectsList(o.split("",""),s.id,n,u,t,a,f,l,c)})},Convert.convertFilterItemData=function(e,t){var n=$("".FilterItemData"");n.length>0&&$.each(n,function(n,r){$(this).empty();var i=Report.cleanLabel(t);r.id||(r.id=""FilterItemData""+g()),html=HTMLComposer.itemName(i,e),$(""#""+r.id).append(html)})},Convert.convertFilterItemSummary=function(e,t){var n=""FilterItemSummary"";divs=$("".""+n),t!==null&&divs.length>0&&$.each(divs,function(r,i){var s=t;ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;$(this).data(""item"")&&(s=$(this).data(""item"")),i.id=ds+""-""+e+""-""+n,$(this).empty(),e===""repos""?DS.displayRepoSummary(i.id,s,DS):e===""countries""?DS.displayCountrySummary(i.id,s,DS):e===""companies""?DS.displayCompanySummary(i.id,s,DS):e===""domains""?DS.displayDomainSummary(i.id,s,DS):e===""projects""&&DS.displayProjectSummary(i.id,s,DS)})},Convert.convertFilterItemMicrodashText=function(e,t){var n=$("".FilterItemMicrodashText"");n.length>0&&$.each(n,function(n,r){$(this).empty();var i=t,s=$(this).data(""metric""),o=$(this).data(""name""),u=Report.getMetricDS(s)[0];if(u===undefined)return;if(e!==""projects"")return;var a=u.getProjectsGlobalData()[t],f='<div class=""row"">';o&&(f+='<div class=""col-md-3"">',f+='<span class=""dayschange"">'+u.basic_metrics[s].name+""</span>"",f+=""</div>""),$.each([365,30,7],function(e,t){var n=u.getMetrics()[s].column,r=a[s+""_""+t],i=a[""diff_net""+n+""_""+t],l=a[""percentage_""+n+""_""+t];l=Math.round(l*10)/10;if(r===undefined)return;var c="""";i>0&&(c=""+""+l),i<0&&(c=""-""+Math.abs(l)),o?f+='<div class=""col-md-3"">':f+='<div class=""col-md-4"">',f+='<span class=""dayschange"">Last '+t+"" days:</span>"",f+="" ""+Report.formatValue(r)+""<br>"",i===0?f+='<i class=""fa fa-arrow-circle-right""></i> <span class=""zeropercent"">&nbsp;'+c+""%</span>&nbsp;"":i>0?f+='<i class=""fa fa-arrow-circle-up""></i> <span class=""pospercent"">&nbsp;'+c+""%</span>&nbsp;"":i<0&&(f+='<i class=""fa fa-arrow-circle-down""></i> <span class=""negpercent"">&nbsp;'+c+""%</span>&nbsp;""),f+=""</div><!--col-md-4-->""}),f+=""</div><!--row-->"",$(r).append(f)})},Convert.convertFilterItemMetricsEvol=function(e,t){var n=b(),r=""FilterItemMetricsEvol"";divs=$("".""+r),t!==null&&divs.length>0&&$.each(divs,function(i,s){var o=t,u=$(this).data(""metrics"");ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;$(this).data(""item"")&&(o=$(this).data(""item"")),n=m(s,n),s.id=Report.cleanLabel(t).replace(/ /g,""_"")+""-"",s.id+=u.replace(/,/g,""-"")+""-""+ds+""-""+e+""-""+r,$(this).empty(),e===""repos""?DS.displayMetricsRepo(o,u.split("",""),s.id,n):e===""countries""?DS.displayMetricsCountry(o,u.split("",""),s.id,n):e===""companies""?DS.displayMetricsCompany(o,u.split("",""),s.id,n):e===""domains""?DS.displayMetricsDomain(o,u.split("",""),s.id,n):e===""projects""&&DS.displayMetricsProject(o,u.split("",""),s.id,n)})},Convert.convertFilterItemTop=function(e,t){var n=""FilterItemTop"";divs=$("".""+n),divs.length>0&&$.each(divs,function(r,i){var s=t;$(this).empty(),ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;$(this).data(""item"")&&(s=$(this).data(""item""));var o=$(this).data(""metric""),u=$(this).data(""period""),a=$(this).data(""titles"");i.id=o+""-""+ds+""-""+e+""-""+n+""-""+g(),$(this).empty(),i.className="""",e===""companies""&&DS.displayTopCompany(s,i.id,o,u,a)})},Convert.convertSmartLinks=function(){var e=$("".SmartLinks"");e.length>0&&$.each(e,function(e,t){if(t.id.indexOf(""Parsed"")>=0)return;target_page=$(this).data(""target""),label=$(this).data(""label"");var n=HTMLComposer.smartLinks(target_page,label);t.id||(t.id=""Parsed""+g()),$(""#""+t.id).append(n)})},Convert.convertFilterStudyItem=function(e,t){var n=Convert.convertFilterStudyItem;if(n.done===undefined)n.done={};else if(n.done[e]===!0)return;e===""repositories""&&(e=""repos""),t===undefined&&(e===""repos""&&(t=Report.getParameterByName(""repository"")),e===""countries""&&(t=Report.getParameterByName(""country"")),e===""companies""&&(t=Report.getParameterByName(""company"")),e===""domains""&&(t=Report.getParameterByName(""domain"")),e===""projects""&&(t=Report.getParameterByName(""project"")));if(!t)return;if(Loader.FilterItemCheck(t,e)===!1)return;Convert.repositoryDSBlock(t),Convert.convertFilterItemData(e,t),Convert.convertFilterItemSummary(e,t),Convert.convertFilterItemMetricsEvol(e,t),Convert.convertFilterItemTop(e,t),Convert.convertFilterItemMicrodashText(e,t),Convert.convertProjectData(),Convert.activateHelp(),Convert.convertMetricsEvolSelector(),n.done[e]=!0},Convert.activateHelp=function(){$("".help"").popover({html:!0,trigger:""manual""}).click(function(e){$(this).popover(""toggle""),e.stopPropagation()})},Convert.convertFilterStudy=function(e){var t=Report.getCurrentPage();t===null&&(t=Report.getParameterByName(""page""),t!==undefined&&Report.setCurrentPage(t));if(t===undefined){if(!($(""[class^='FilterItems']"").length>0))return;t=1,Report.setCurrentPage(t)}e===""repositories""&&(e=""repos"");if(Loader.check_filter_page(t,e)===!1){$.each(Report.getDataSources(),function(n,r){Loader.data_load_items_page(r,t,Convert.convertFilterStudy,e)});return}Convert.convertFilterItemsSummary(e),Convert.convertFilterItemsGlobal(e),Convert.convertFilterItemsNav(e,t),Convert.convertFilterItemsMetricsEvol(e),Convert.convertFilterItemsMiniCharts(e,t)},Convert.convertDSTable=function(){var e=""DataSourcesTable"",t=$("".""+e),n,r;if(t.length>0){var i=0;$.each(t,function(t,n){$(this).empty(),n.id=e+i++,Viz.displayDataSourcesTable(n)})}},Convert.convertBasicDivs=function(){Convert.convertNavbar(),Convert.convertSmartLinks(),Convert.convertSectionBreadcrumb(),Convert.convertProjectMap(),Convert.convertFooter(),Convert.convertOverallSummaryBlock(),Convert.convertDSSummaryBlock(),Convert.convertDSTable(),Convert.convertGlobalData(),Convert.convertSummary()},Convert.convertBasicDivsMisc=function(){Convert.convertRadarActivity(),Convert.convertRadarCommunity(),Convert.convertTreemap(),Convert.convertBubbles()},Convert.convertBasicMetrics=function(e){var t=Report.getParameterByName(""repository"");t===undefined&&Convert.convertMetricsEvol(),Convert.convertTimeTo(),Convert.convertMarkovTable()},Convert.convertModifiedBasicMetrics=function(e){var t=1;if(Loader.check_filter_page(t,e)===!1){$.each(Report.getDataSources(),function(n,r){if(e!==""repos"")return;e===""repos""&&(total=r.getReposData().length);for(var i=0;i<total;i++){var s=r.getReposData()[i];Loader.data_load_item(s,r,t,Convert.convertModifiedBasicMetrics,e)}});return}Convert.convertMetricsEvolCustomized(e)},Convert.convertFilterTop=function(e){var t=Report.getParameterByName(""repository"");if(Loader.filterTopCheck(t,e)===!1)return;Convert.convertTop(),Convert.convertRepositorySelector()}})();if(Report===undefined)var Report={};(function(){function x(){return t}function T(){return n}function N(){return projects_hierarchy}function C(){return menu_elements.menu}function k(){return menu_elements.releases}function L(){return r}function A(e){r=e}function O(){return e}function M(){return h}function _(e){var t=e.toString().split(""."");return t[0]=t[0].replace(/\B(?=(\d{3})+(?!\d))/g,"",""),t.join(""."")}function D(e){var t=[];return $.each(Report.getDataSources(),function(n,r){r.getMetrics()[e]&&t.push(r)}),t}function P(){var e={};return $.each(Report.getDataSources(),function(t,n){e=$.extend({},e,n.getMetrics())}),e}function H(){var e=[],t=$.urlParam(""release"");t!==null&&t.length>0&&(e.push(""data/json/""+t),Report.setDataDir(""data/json/""+t),e.length>0&&Report.setProjectsDirs(e))}function B(){H();var e=Report.getProjectsDirs(),t,n,r,i,s,o,u,a,f,l;return $.each(e,function(e,c){if(Report.getConfig()===null||Report.getConfig()[""data-sources""]===undefined)n=new ITS,Report.registerDataSource(n),r=new MLS,Report.registerDataSource(r),t=new SCM,Report.registerDataSource(t),i=new SCR,Report.registerDataSource(i),s=new IRC,Report.registerDataSource(s),o=new MediaWiki,Report.registerDataSource(o),u=new People,Report.registerDataSource(u),a=new Downloads,Report.registerDataSource(a),f=new QAForums,Report.registerDataSource(f),l=new Releases,Report.registerDataSource(l);else{var h=Report.getConfig()[""data-sources""];$.each(h,function(e,c){c===""its""?(n=new ITS,Report.registerDataSource(n)):c===""mls""?(r=new MLS,Report.registerDataSource(r)):c===""scm""?(t=new SCM,Report.registerDataSource(t)):c===""scr""?(i=new SCR,Report.registerDataSource(i)):c===""irc""?(s=new IRC,Report.registerDataSource(s)):c===""mediawiki""?(o=new MediaWiki,Report.registerDataSource(o)):c===""people""?(u=new People,Report.registerDataSource(u)):c===""downloads""?(a=new Downloads,Report.registerDataSource(a)):c===""qaforums""?(f=new QAForums,Report.registerDataSource(f)):c===""releases""?(l=new Releases,Report.registerDataSource(l)):Report.log(""Not support data source ""+c)})}n&&n.setDataDir(c),r&&r.setDataDir(c),t&&t.setDataDir(c),i&&i.setDataDir(c),s&&s.setDataDir(c),o&&o.setDataDir(c),u&&u.setDataDir(c),a&&a.setDataDir(c),f&&f.setDataDir(c),l&&l.setDataDir(c),t&&n&&t.setITS(n)}),!0}function j(){$.each(Report.getActiveStudies(),function(e,t){var n=t;t===""repositories""&&(n=""repos""),DataProcess.orderItems(n),Convert.convertFilterStudy(t),Convert.convertFilterStudyItem(t)})}var e=null,t=null,n=null,r={},i=[],s=null,o="""",u=""data/json"",a=""config"",f=""data/json"",l="""",c=[f],h={},p={},d,v=a+""/project-info.json"",m=u+""/viz_cfg.json"",g=u+""/markers.json"",y=u+""/repos-map.json"",b=u+""/projects_hierarchy.json"";menu_elements_file=a+""/menu-elements.json"";var w=10,E=null,S={};Report.createDataSources=B,Report.getAllMetrics=P,Report.getMarkers=x,Report.getVizConfig=T,Report.getProjectsHierarchy=N,Report.getMenuElements=C,Report.getReleaseNames=k,Report.getMetricDS=D,Report.getGridster=L,Report.setGridster=A,Report.getCurrentPage=function(){return E},Report.setCurrentPage=function(e){E=e},Report.getPageSize=function(){return w},Report.setPageSize=function(e){w=e},Report.getProjectData=O,Report.getProjectsData=M,Report.convertStudies=j,Report.getDataSources=function(){return i},Report.registerDataSource=function(e){i.push(e)},Report.setHtmlDir=function(e){o=e},Report.getHtmlDir=function(){return o},Report.getDataDir=function(){return u},Report.setDataDir=function(e){u=e,v=e+""/project-info.json"",config_file=e+""/viz_cfg.json"",g=e+""/markers.json"",repos_mapping_file=u+""/repos-mapping.json"",b=u+""/projects_hierarchy.json""},Report.setMarkers=function(e){t=e},Report.getMarkersFile=function(){return g},Report.getReposMap=function(){return d},Report.setReposMap=function(e){d=e},Report.getReposMapFile=function(){return y},Report.setVizConfig=function(e){n=e},Report.getVizConfigFile=function(){return m},Report.setProjectsHierarchy=function(e){projects_hierarchy=e},Report.getProjectsHierarchyFile=function(){return b},Report.setMenuElements=function(e){menu_elements=e},Report.getMenuElementsFile=function(){return menu_elements_file},Report.setProjectData=function(t){e=t},Report.getProjectFile=function(){return v},Report.getProjectsDirs=function(){return c},Report.setProjectsDirs=function(e){c=e},Report.getProjectsList=function(){var e=[];return $.each(M(),function(t,n){e.push(t)}),e},Report.getProjectsDataSources=function(){return p},Report.setMetricsDefinition=function(e){$.each(Report.getDataSources(),function(t,n){n.setMetricsDefinition(e[n.getName()])})},Report.getPeopleIdentities=function(){return S},Report.setPeopleIdentities=function(e){S=e},Report.cleanLabel=function(e){var t=e,n=null;return e.split(""___"").length===2?(n=e.split("" ""),t=n[0]):e.lastIndexOf(""http"")===0||e.split(""_"").length>3?(n=e.split(""_""),t=n.pop(),t===""""&&(t=n.pop()),t=t.replace(""buglist.cgi?product="",""""),t=t.replace(""gmane.comp.sysutils."","""")):e.lastIndexOf(""<"")===0&&(t=MLS.displayMLSListName(e)),t},Report.formatValue=function(e,t){if(e===undefined)return""-"";var n=[""last_date"",""first_date""],r=e;try{r=parseFloat(e).toFixed(1).toString().replace(/\.0$/,""""),r=_(r);if(navigator.language===""es""){var i=r.split(""."");i[0]=i[0].replace(/,/g,"".""),r=i.join("","")}}catch(s){}return typeof r==""number""&&isNaN(r)&&(r=e.toString()),t!==undefined&&$.inArray(t,n)>-1&&(r=e.toString()),r},Report.escapeHtml=function(e){return e.replace(/&/g,""&amp;"").replace(/</g,""&lt;"").replace(/>/g,""&gt;"").replace(/""/g,""&quot;"").replace(/'/g,""&#039;"")},Report.getParameterByName=function(e){e=e.replace(/[\[]/,""\\["").replace(/[\]]/,""\\]"");var t=new RegExp(""[\\?&]""+e+""=([^&#]*)""),n=t.exec(location.search);return n===null?undefined:Report.escapeHtml(decodeURIComponent(n[1].replace(/\+/g,"" "")))},Report.getDataSourceByName=function(e){var t=null;return $.each(Report.getDataSources(),function(n,r){if(r.getName()===e)return t=r,!1}),t},Report.displayActiveMenu=function(){var e=window.location.href,t=e.substr(e.lastIndexOf(""/"")+1,e.length);t=t.split("".html"")[0];if(t.indexOf(""scm"")===0)$("".scm-menu"")[0].className=$("".scm-menu"")[0].className+"" active"";else if(t.indexOf(""its"")===0)$("".its-menu"")[0].className=$("".its-menu"")[0].className+"" active"";else if(t.indexOf(""mls"")===0)$("".mls-menu"")[0].className=$("".mls-menu"")[0].className+"" active"";else if(t.indexOf(""scr"")===0)$("".scr-menu"")[0].className=$("".scr-menu"")[0].className+"" active"";else if(t.indexOf(""irc"")===0)$("".irc-menu"")[0].className=$("".irc-menu"")[0].className+"" active"";else if(t.indexOf(""qaforum"")===0)$("".qaforum-menu"")[0].className=$("".qaforum-menu"")[0].className+"" active"";else if(t.indexOf(""studies"")===0)$("".studies-menu"")[0].className=$("".studies-menu"")[0].className+"" active"";else if(t.indexOf(""wiki"")===0)$("".wiki-menu"")[0].className=$("".wiki-menu"")[0].className+"" active"";else if(t.indexOf(""downloads"")===0)$("".downloads-menu"")[0].className=$("".downloads-menu"")[0].className+"" active"";else if(t.indexOf(""projects"")===0)$("".listprojects-menu"")[0].className=$("".listprojects-menu"")[0].className+"" active"";else if(t.indexOf(""index"")===0||t===""""){if($("".summary-menu"").length===0)return;$("".summary-menu"")[0].className=$("".summary-menu"")[0].className+"" active""}else $("".experimental-menu"")[0]&&($("".experimental-menu"")[0].className=$("".experimental-menu"")[0].className+"" active"")},Report.addDataDir=function(){var e,t=window.location.search.substr(1);return t&&t.indexOf(""data_dir"")!==-1&&(e=window.location.search.substr(1)),e},Report.configDataSources=function(){var e=Report.getProjectsDataSources();$.each(Report.getDataSources(),function(t,n){if(n.getData()instanceof Array)return;$.each(h,function(t,r){if(r.dir===n.getDataDir())return e[t]===undefined&&(e[t]=[]),$.each(e[t],function(e,t){if(n.getName()===t.getName())return!1}),n.setProject(t),e[t].push(n),!1})})},Report.getConfig=function(){return s},Report.setConfig=function(e){s=e,e&&(Report.log(""Global config file found""),e[""global-html-dir""]&&Report.setHtmlDir(e[""global-html-dir""]),e[""global-data-dir""]&&(Report.setDataDir(e[""global-data-dir""]),Report.setProjectsDirs([e[""global-data-dir""]])),e[""projects-data-dirs""]&&Report.setProjectsDirs(e[""projects-data-dirs""]))},Report.convertGlobal=function(){Convert.convertBasicDivs(),Convert.convertBasicDivsMisc(),Convert.convertBasicMetrics(),Convert.convertDemographics(),Convert.convertMetricsEvolSet(),Convert.convertLastActivity(),Convert.convertMicrodash(),Convert.convertMicrodashText()},Report.getActiveStudies=function(){var e=[],t,n=[""repositories"",""countries"",""companies"",""domains"",""projects""];return Report.getConfig()!==null?t=Report.getConfig().reports:t=n,$.each(n,function(n,r){$.inArray(r,t)>-1&&e.push(r)}),e},Report.convertStudiesGlobal=function(){Convert.convertPeople()};var F=!0;Report.getLog=function(){return F},Report.setLog=function(e){F=e},Report.log=function(e){Report.getLog()===!0&&window.console&&console.log(e)}})(),Loader.data_ready_global(function(){Report.configDataSources(),Report.convertGlobal(),Report.convertStudiesGlobal()}),Loader.data_ready(function(){study=""repos"",Convert.convertFilterTop(study)}),Loader.data_ready(function(){Report.convertStudies(),$(""body"").css(""cursor"",""auto""),$(""html"").click(function(e){$("".help"").popover(""hide"")}),Convert.activateHelp()}),$(document).ready(function(){var e=""./config.json"";$.getJSON(e,function(e){Report.setConfig(e)}).fail(function(){window.console&&Report.log(""Can't read global config file ""+e)}).always(function(e){Report.createDataSources(),Loader.data_load(),$(""body"").css(""cursor"",""progress"")})});var resized;$(window).resize(function(){clearTimeout(resized),resized=setTimeout(resizedw,100)});if(Viz===undefined)var Viz={};(function(){function t(e,t){var n="""";return $.each(Report.getAllMetrics(),function(e,r){if(r.action===t)return n=r.column,!1}),n}function n(e,t,n){var r=""<h4>""+n+""</h4>"",i='<table id=""itsmarkovtable"" class=""table table-striped"">';i+=""<thead><tr><th>Transition</th><th>Number</th><th>Percent</th></tr></thead><tbody>"",$.each(t,function(e,n){subdata=t[e],old_value=""old_value"",new_value=""new_value"",percent=""f"",number=""issue"";for(var r=0;r<subdata[old_value].length;r++){var s=subdata[new_value][r],o=subdata[percent][r];o=Math.round(o*100)/100;var u=subdata[number][r];i+=""<tr><td>""+e+"" -> ""+s+""</td>"",i+=""<td>""+u+""</td>"",i+=""<td>""+o+""</td></tr>""}}),i+=""</tbody></table>"",r+=i,div=$(""#""+e),div.append(r);return}function r(e,t){return e.hasOwnProperty(t)?e[t]:t}function i(e,t){var n={};return n.id=""id"",e===""senders""&&(t===""mls""||t===""irc"")&&(n.name=""senders"",n.action=""sent""),e===""authors""&&t===""scm""&&(n.name=""authors"",n.action=""commits""),e===""closers""&&t===""its""&&(n.name=""closers"",n.action=""closed""),t===""scr""&&(e===""mergers""&&(n.name=""mergers"",n.action=""merged""),e===""openers""&&(n.name=""openers"",n.action=""opened""),e===""reviewers""&&(n.name=""reviewers"",n.action=""reviews"")),t===""downloads""&&(e===""ips""&&(n.name=""ips"",n.action=""downloads""),e===""packages""&&(n.name=""packages"",n.action=""downloads"")),t===""mediawiki""&&e===""authors""&&(n.name=""authors"",n.action=""reviews""),t===""qaforums""&&(e===""senders""||e===""asenders""||e===""qsenders""?(n.name=""senders"",n.action=""sent""):e===""participants""&&(n.name=""name"",n.action=""messages_sent"")),t===""releases""&&e===""authors""&&(n.name=""username"",n.action=""releases""),n}function s(){return[""last month"",""last year"",""""]}function o(e,t,n){var r="""";for(var i=0;i<e[n.name].length;i++){if(t&&t<=i)break;var s=e[n.action][i];r+=""<tr><td> ""+(i+1)+""</td>"",r+=""<td>"",r+=e[n.name][i],r+=""</td>"",r+=""<td>""+s+""</td></tr>""}return r}function u(e,t,n){var r="""";for(var i=0;i<e.subject.length;i++){if(t&&t<=i)break;r+=""<tr><td>#""+(i+1)+""</td>"",r+=""<td>"";if(n===!0){var s=""http://www.google.com/search?output=search&q=X&btnI=1"";e.hasOwnProperty(""url"")&&e.url[i].length>0&&(s=""http://www.google.com/search?output=search&q=X%20site%3AY&btnI=1"",s=s.replace(/Y/g,e.url[i])),s=s.replace(/X/g,e.subject[i]),r+=""<td>"",r+='<a target=""_blank"" href=""'+s+'"">',r+=e.subject[i]+""</a>"",r+='&nbsp;<i class=""fa fa-external-link""></i></td>'}else r+=""<td>""+e.subject[i]+""</td>"";r+=""<td>""+e.initiator_name[i]+""</td>"",r+=""<td>""+e.length[i]+""</td>"",r+=""</tr>""}return r}function a(e,t,n,r){var i="""";for(var s=0;s<e[r.id].length;s++){if(t&&t<=s)break;var o=e[r.action][s];i+=""<tr><td>""+(s+1)+""</td>"",i+=""<td>"",n?(i+='<a href=""people.html?id='+e[r.id][s],get_params=Utils.paramsInURL(),get_params.length>0&&(i+=""&""+get_params),i+='"">',i+=e[r.name][s]+""</a>""):i+=e[r.name][s],i+=""</td>"",i+=""<td>""+o+""</td></tr>""}return i}function f(e,t,n,r){var i="""",s=!0;i+='<ul id=""myTab"" class=""nav nav-tabs"">';for(var o=0;o<e.length;o++){var u=t+"".""+e[o];if(n[u]){var a=e[o],f=a;a===""""?(a=""all"",f=""Complete history""):a===""last month""?f=""Last 30 days"":a===""last year""&&(f=""Last 365 days"");var l=a.replace(/\ /g,""""),c="""";s===!0&&(c=' class=""active""',s=!1),i+=""<li""+c+'><a href=""#'+r+t+l+'""data-toogle=""tab"">',i+=f+""</a></li>""}}return i+=""</ul>"",i}function l(e,t,n,r,i){var s=t+""_""+e,o="""",u="""";return s in r&&(o=r[s].desc,o=o.toLowerCase()),i===""""?data_period_formatted=""Complete history"":i===""last month""?data_period_formatted=""Last 30 days"":i===""last year""&&(data_period_formatted=""Last 365 days""),Utils.isReleasePage()&&(data_period_formatted=""Release history""),n===!0?u+=""<h6>Top ""+o+""</h6>"":u+='<div class=""toptable-title"">'+data_period_formatted+""</div>"",u}function c(e,t,n,r,c,h,p,d){var v="""",m="""",g="""",y=!0,b=$(""#""+e),w=b.attr(""data-data-source"");periods=s(),d!==undefined&&(y=!1),g+=l(n,w,y,c,d),y===!0&&(v+=f(periods,n,t,w)),m+='<div class=""tab-content"">';var E=i(n,w);if(y===!0){var S=!0,x="""";for(var T=0;T<periods.length;T++){x="""";var N=n+"".""+periods[T];if(t[N]){var C=periods[T];C===""""&&(C=""all"");var k=C.replace(/\ /g,"""");S===!0&&(x="" active in"",S=!1),m+='<div class=""tab-pane fade'+x+'"" id=""'+w+n+k+'"">',m+='<table class=""table table-striped"">',n===""threads""?m+=u(t[N],r,p):n===""packages""||n===""ips""?(unit=c[w+""_""+n].action,metric_name=c[w+""_""+n].name,m+=""<thead><th>#</th><th>""+metric_name.capitalize()+""</th><th>""+unit.capitalize()+""</th></thead><tbody>"",m+=o(t[N],r,E)):(unit=c[w+""_""+n].action,metric_name=c[w+""_""+n].name,m+=""<thead><th>#</th><th>""+metric_name.capitalize()+""</th><th>""+unit.capitalize()+""</th></thead><tbody>"",m+=a(t[N],r,h,E),m+=""</tbody>""),m+=""</table>"",m+=""</div>""}}}else m+='<table class=""table table-striped""><tbody>',n===""threads""?m+=u(t,r,p):n===""packages""||n===""ips""?(unit=c[w+""_""+n].action,m+=""<thead><th>#</th><th>""+n.capitalize()+""</th><th>""+unit.capitalize()+""</th></thead><tbody>"",m+=o(t,r,E)):(unit=c[w+""_""+n].action,m+=""<thead><th>#</th><th>""+n.capitalize()+""</th><th>""+unit.capitalize()+""</th></thead><tbody>"",m+=a(t,r,h,E),m+=""</tbody>""),m+=""</tbody></table>"";m+=""</div>"",y===!1&&b.append(g),b.append(v),b.append(m),y===!0&&(script=""<script>$('#myTab a').click(function (e) {e.preventDefault();$(this).tab('show');});</script>"",b.append(script))}function h(e,n,r,i,s,o,u,a){var f=n.name;if(!i||$.isEmptyObject(i))return;var l=n.action;u&&i[l].length<u&&(u=i[l].length,s=!1);var c=n.column;c===undefined&&(c=t(i,l));var h=""Top ""+f+"" ""+r,p=displayTopMetricTable(i,l,c,u,a,h),d=null;if(p===undefined)return;if(o===!1){d=$(""#""+e),d.append(p);return}var v="""",m="""";s&&(v=""top-""+s+""-""+c+""-"",v+=l+""-""+r,m+=""<div id='""+v+""' class='graph' style='float:right'></div>""),m+=p,d=$(""#""+e),d.append(m);if(s){var g=i[c],y=i[l];if(u){g=[],y=[];for(var b=0;b<u;b++)g.push(i[c][b]),y.push(i[l][b])}O(v,g,y,s)}}function p(e){dsources=Report.getDataSources(),html='<table class=""table table-striped"">',html+=""<thead><th>Data Source</th><th>From</th>"",html+=""<th>To <small>(Updated on)</small></th></thead><tbody>"",$.each(dsources,function(e,t){if(t.getName()===""people"")return;var n=t.getGlobalData(),r=t.getTitle();r===undefined&&(r=""-"");var i=n.last_date;if(i===undefined)return;var s=n.first_date;s===undefined&&(s=""-"");var o=n.type;html+=""<tr><td>""+r,o!==undefined&&(o=o.toLowerCase(),o=o.charAt(0).toUpperCase()+o.slice(1),html+="" (""+o+"")""),html+=""</td>"",html+=""<td>""+s+""</td>"",html+=""<td>""+i+""</td></tr>""}),html+=""</tbody></table>"",$(e).append(html)}function d(e,t,n){var r=Report.getAllMetrics(),i='<a href=""#"" class=""help""',s="""";if(n===""""){var o=function(e,n){if(t[u]===e)return s+=""<strong>""+n.name+""</strong>: ""+n.desc+""<br>"",!1};for(var u=0;u<t.length;u++)$.each(r,o)}else s=""<strong>Description</strong>: ""+n;i+='data-content=""'+s+'"" data-html=""true"">',i+='<img src=""qm_15.png""></a>';var a=$(""#""+e).prev()[0];a&&a.className===""help""&&$(""#""+e).prev().empty(),$(""#""+e).before(i)}function v(e,t,n,r,i){(!i||i.help!==!1)&&d(e,t,i.custom_help);var s=[];i.remove_last_point&&(n=DataProcess.revomeLastPoint(n)),i.frame_time&&(n=DataProcess.frameTime(n,t)),i.start_time&&(n=DataProcess.filterDates(i.start_time,i.end_time,n)),$.each(t,function(e,t){if(!n[t])return;var r=[[],[]];$.each(n[t],function(e,i){r[e]=[n.id[e],n[t][e]]});var i=t;Report.getAllMetrics()[t]&&(i=Report.getAllMetrics()[t].name),s.push({label:i,data:r})}),N(e,n,s,r,i)}function m(e,t,n,r,i,s){(!i||i.help!==!1)&&d(e,t,i.custom_help);var o=[],u=t[0],a={};$.each(n,function(e,t){if(t===undefined)return!1;if(t[u]===undefined)return!1;i.remove_last_point&&(t=DataProcess.revomeLastPoint(t)),i.frame_time&&(t=DataProcess.frameTime(t,[u])),i.start_time&&(t=DataProcess.filterDates(i.start_time,i.end_time,t));var n=[[],[]];$.each(t[u],function(e,r){n[e]=[t.id[e],t[u][e]]}),o.push({label:e,data:n}),a=t}),N(e,a,o,r,i)}function g(e,t,n,r,i,s,o,u,a){var f=[],l={};$.each(n,function(e,n){if(n===undefined)return!1;if(n[t]===undefined)return!1;u&&(n=DataProcess.convert(n,u,t)),s&&(n=DataProcess.filterDates(s,o,n)),i.frame_time&&(n=DataProcess.frameTime(n,[t]));var r=[[],[]];for(var a=0;a<n.id.length;a++)r[a]=[n.id[a],n[t][a]];e=Report.cleanLabel(e),f.push({label:e,data:r}),l=n});if(f.length===0)return;if(a){var c=[];$.each(a,function(e,t){$.each(f,function(e,n){if(t===n.label)return c.push(n),!1})}),f=c}N(e,l,f,r,i)}function y(e,t,n,r,i){var s={subtitle:e,legend:{show:!0,container:t},xaxis:{minorTickFreq:4,mode:""time"",timeUnit:""second"",timeFormat:""%b %y"",margin:!0},yaxis:{min:null,noTicks:2,autoscale:!0},grid:{verticalLines:!1,color:""#000000"",outlineWidth:1,outline:""s""},mouse:{container:t,track:!0,trackY:!1,relative:!0,position:""ne"",trackFormatter:function(e){var t=n.date[parseInt(e.index,10)];t===undefined?t="""":t+=""<br>"";for(var i=0;i<r.length;i++){var s=r[i].data[e.index][1];if(s===undefined)continue;r.length>1&&r[i].label!==undefined&&(value_name=r[i].label,t+=value_name+"":""),t+=""<strong>""+Report.formatValue(s)+""</strong><br>""}return t}},selection:{mode:""x"",fps:10},shadowSize:4};return i&&(Viz._history=n,Viz._lines_data=r,s.mouse.trackFormatter=Viz[i]),s}function b(e,t){if(t.length===0)return t;if(t.length>1)for(var n=0;n<t.length;n++){var r=t[n].data.length-1;t[n].data[r][1]=undefined}}function w(e,t){if(t.length!==1)return t;var n=t[0].data.length,r=[],i=0;for(var s=0;s<n-1;s++)i=parseInt(e.unixtime[s],10),r.push([i,undefined]);i=parseInt(e.unixtime[n-1],10),r.push([i,t[0].data[n-1][1]]);var o={data:r};return o.points={show:!0,radius:3,lineWidth:1,fillColor:null,shadowSize:0},t.push(o),t[0].data[n-1][1]=undefined,t[1].label=t[0].label,t}function E(e,t,n){var r=[""Jan"",""Feb"",""Mar"",""Apr"",""May"",""Jun"",""Jul"",""Aug"",""Sep"",""Oct"",""Nov"",""Dec""],i=new Date(parseInt(t,10)*1e3),s=r[i.getMonth()]+"" ""+i.getFullYear();i=new Date(parseInt(n,10)*1e3);var o=r[i.getMonth()]+"" ""+i.getFullYear();return e+"" ( ""+s+"" - ""+o+"" )""}function S(e){return e.sort(function(e,t){return e[1]>t[1]||t[1]===undefined?1:-1}),e}function x(e,t,n){t=Math.round(t),n=Math.round(n);var r=e.length,i=[];for(var s=0;s<r;s++)for(var o=e[s].data.length-1;o>0;o--){var u=e[s].data[o][0],a=u<t||u>n;a&&e[s].data.splice(o,1)}var f=[];for(s=0;s<r;s++)i=e[s].data,i=S(i),f.push(i[i.length-1][1]);return f.sort(function(e,t){return e-t}),f[f.length-1]}function T(e){var t=e[0].data[1][0]-e[0].data[0][0],n=e.length,r=0;for(var i=0;i<n;i++){var s=e[i].data.length;r=e[i].data[s-1][0],e[i].data.push([r+t,undefined])}return e}function N(e,t,n,r,i){var s=!1;i&&i.lines&&i.lines.stacked&&(s=!0),s?C(e,t,n,r,i):t.unixtime===undefined?C(e,t,n,r,i):A(e,t,n,r,i)}function C(e,t,n,r,i){var s=document.getElementById(e),o=null;i&&i.legend&&i.legend.container&&(o=$(""#""+i.legend.container));var u={subtitle:r,legend:{show:!0,container:o},xaxis:{minorTickFreq:4,tickFormatter:function(e){var n=null;for(var r=0;r<t.id.length;r++)if(parseInt(e,10)===t.id[r]){n=r;break}return t.date[n]}},yaxis:{min:0,noTicks:2,autoscale:!1},grid:{verticalLines:!1,color:""#000000"",outlineWidth:1,outline:""s""},mouse:{container:o,track:!0,trackY:!1,trackFormatter:function(e){var r=t.date[parseInt(e.index,10)];r===undefined?r="""":r+=""<br>"";for(var i=0;i<n.length;i++){var s=n[i].data[e.index][1];if(s===undefined)continue;n.length>1&&n[i].label!==undefined&&(r+=n[i].label+"":""),r+=Report.formatValue(s)+""<br>""}return r}}};i&&(i.show_title||(u.title=""""),""show_legend""in i&&(i.show_legend===!0?u.legend.show=!0:u.legend.show=!1),i.lines&&i.lines.stacked&&(u.lines={stacked:!0,fill:!0,fillOpacity:1,fillBorder:!0,lineWidth:.01}),i.show_labels||(u.xaxis.showLabels=!1,u.yaxis.showLabels=!1),i.show_grid===!1&&(u.grid.verticalLines=!1,u.grid.horizontalLines=!1,u.grid.outlineWidth=0),i.show_mouse===!1&&(u.mouse.track=!1),i.graph===""bars""&&(u.bars={show:!0}),i.light_style===!0&&(u.grid.color=""#ccc"",u.legend.show=!1),i.custom_title&&(u.subtitle=i.custom_title));var a=!1;i.graph!==""bars""&&n.length===1&&n[0].data[0][0]===0&&(a=!0);if(a){n=w(t,n);var f=t.id[t.id.length-1]+1;n[0].data.push([f,undefined]),n[1].data.push([f,undefined]),t.date.push(""""),t.id.push(f)}graph=Flotr.draw(s,n,u),a&&(t.date&&t.date.pop(),t.id&&t.id.pop())}function k(e,t){var n,r=e.length;return n=parseInt(t.unixtime[1],10)-parseInt(t.unixtime[0],10),n/(r+1)}function L(e,t,n,r){var i=e.length,s=e[0].data.length;for(var o=0;o<i;o++)for(var u=0;u<s;u++)n?e[o].data[u][0]=parseInt(t.unixtime[u],10)+o*r:e[o].data[u][0]=parseInt(t.unixtime[u],10);return e}function A(e,t,n,r,i){function c(e){var t=Flotr._.extend(Flotr._.clone(f),e||{});return Flotr.draw(u,n,t)}var s=!1,o;if(n.length===0)return;var u=document.getElementById(e),a=null;i&&i.legend&&i.legend.container&&(a=$(""#""+i.legend.container));var f=y(r,a,t,n,i.mouse_tracker);i&&(i.show_title||(f.title=""""),""show_legend""in i&&(i.show_legend===!0?f.legend.show=!0:f.legend.show=!1),i.lines&&i.lines.stacked&&(f.lines={stacked:!0,fill:!0,fillOpacity:1,fillBorder:!0,lineWidth:.01}),i.show_labels||(f.xaxis.showLabels=!1,f.yaxis.showLabels=!1),i.show_grid===!1&&(f.grid.verticalLines=!1,f.grid.horizontalLines=!1,f.grid.outlineWidth=0),i.show_mouse===!1&&(f.mouse.track=!1),i.graph===""bars""&&(f.bars={show:!0,stacked:!1,horizontal:!1,barWidth:728e3,lineWidth:1},f.bars.barWidth=k(n,t),s=!0,o=f.bars.barWidth),i.light_style===!0&&(f.grid.color=""#ccc"",f.legend.show=!1),i.custom_title&&(f.subtitle=i.custom_title),f.mouse.position=""n"",f.mouse.margin=20),n.length>1&&(f.legend.show=!0),n=L(n,t,s,o);var l=!1;Utils.isReleasePage()===!1&&(i.graph!==""bars""&&n.length===1&&(l=!0),l?(n=w(t,n),T(n)):!l&&n.length>1&&b(t,n)),console.log(f),graph=c(),Flotr.EventAdapter.observe(u,""flotr:select"",function(e){var t={xaxis:{minorTickFreq:4,mode:""time"",timeUnit:""second"",timeFormat:""%b %y"",min:e.x1,max:e.x2},yaxis:{min:e.y1,autoscale:!0},grid:{verticalLines :!0,color:""#000000"",outlineWidth:1,outline:""s""}};t.subtitle=E(f.subtitle,e.xfirst,e.xsecond);var r=JSON.parse(JSON.stringify(n)),i=x(r,e.x1,e.x2);t.yaxis.max=i+i*.2,graph=c(t)}),Flotr.EventAdapter.observe(u,""flotr:click"",function(){c()})}function O(e,t,n,r,i,s,o,u,a){var f=!1;o&&(f=!0);var l=document.getElementById(e),c=null;s&&s.legend&&s.legend.container&&(c=$(""#""+s.legend.container));var h=[],p,d="""";if(!f)for(p=0;p<n.length;p++)t&&(d=DataProcess.hideEmail(t[p])),h.push({data:[[p,n[p]]],label:d});else for(p=0;p<n.length;p++)t&&(d=DataProcess.hideEmail(t[p])),h.push({data:[[n[p],p]],label:d});var v={subtitle:i,grid:{verticalLines:!1,horizontalLines:!1,outlineWidth:0},xaxis:{showLabels:!1,min:0},yaxis:{showLabels:!1,min:0},mouse:{container:c,track:!0,trackFormatter:function(e){var r=""x"";f&&(r=""y"");var i="""";return t&&(i=DataProcess.hideEmail(t[parseInt(e[r],10)])+"": ""),i+n[parseInt(e[r],10)]}},legend:{show:!1,position:""se"",backgroundColor:""#D2E8FF"",container:c}};s&&(s.show_title||(v.title=""""),s.show_legend&&(v.legend.show=!0)),r===""bars""&&(v.bars={show:!0,horizontal:f},u&&(v.bars.color=u,v.bars.fillColor=u),s&&s.show_legend!==!1&&(v.legend={show:!0,position:""ne"",container:c}),v.grid.horizontalLines=!0,v.yaxis={showLabels:!0,min:0},s&&s.xaxis&&(v.xaxis={showLabels:s.xaxis,min:0}),a&&(v.yaxis={showLabels:!0,min:0,tickFormatter:a})),r===""pie""&&(v.pie={show:!0},v.mouse.position=""ne""),r=Flotr.draw(l,h,v)}function M(e,t,n,r,i,s,o,u){var a=.4,f=n[0].length;n[1].length>f&&(f=n[1].length);var l=!1;s&&(l=!0);var c=document.getElementById(e),h=null;i&&i.legend&&i.legend.container&&(h=$(""#""+i.legend.container));var p=[],d,v=[],m=[];for(d=0;d<f;d++){var g,y;n[0].length>d?g=n[0][d]:g=undefined,n[1].length>d?y=n[1][d]:y=undefined,l?(p.push([g,d-a/2]),v.push([y,d+a/2])):(p.push([d-a/2,g]),v.push([d+a/2,y]))}m=[{data:p,label:t[0]},{data:v,label:t[1]}];var b={title:r,bars:{show:!0,horizontal:l,barWidth:a},grid:{verticalLines:!1,horizontalLines:!1,outlineWidth:0},xaxis:{showLabels:!1,min:0},yaxis:{showLabels:!0,min:0},mouse:{container:h,track:!0,trackFormatter:function(e){var n,r=""x"";l&&(r=""y"");var i=parseFloat(e[r],1),s=Math.round((i-.2)*10)/10,o=Math.round((i+.2)*10)/10;s===parseInt(i,10)?n=s:n=o;var a=n;u&&(a=n*u);var f=a+"" years: "",c,h;return p[n]===undefined?c=0:c=parseInt(p[n][0],10),isNaN(c)&&(c=0),v[n]===undefined?h=0:h=parseInt(v[n][0],10),isNaN(h)&&(h=0),f+=c+"" ""+t[0],f+="" , "",f+=h+"" ""+t[1],f+="" (""+parseInt(c/h*100,10)+""% )"",f}},legend:{show:!0,position:""ne"",backgroundColor:""#D2E8FF"",container:h}};i&&(i.show_title||(b.title=""""),i.show_legend&&(b.legend.show=!0)),i&&i.show_legend!==!1&&(b.legend={show:!0,position:""ne"",container:h}),b.grid.horizontalLines=!0,b.yaxis={showLabels:!0,min:0},o&&(b.yaxis={showLabels:!0,min:0,tickFormatter:o}),i&&i.xaxis&&(b.xaxis={showLabels:i.xaxis,min:0}),graph=Flotr.draw(c,m,b)}function _(e,t,n,r){var i=document.getElementById(e),s=Report.getMetricDS(t)[0],o=Report.getMetricDS(n)[0],u=[];if(s!=o){Report.log(""Metrics for bubbles have different data sources"");return}var a=[],f=[];$.each(Report.getDataSources(),function(e,t){t.getName()===s.getName()&&(a.push(t.getData()),f.push(t.getProject()))});var l=[[],[]];l=[a[0].id,a[0].date];for(var c=0;c<a.length;c++){if(a[c]instanceof Array)return;l=DataProcess.fillDates(l,[a[c].id,a[c].date])}for(var h=0;h<a.length;h++){var p=[],d=a[h],v=DataProcess.fillHistory(l[0],[d.id,d[t]]),m=DataProcess.fillHistory(l[0],[d.id,d[n]]);for(c=0;c<l[0].length;c++)p.push([l[0][c],v[1][c],m[1][c]]);u.push({label:f[h],data:p})}var g={bubbles:{show:!0,baseRadius:5},mouse:{track:!0,trackFormatter:function(e){var r=a[0].date[e.index]+"": "";return r+=e.series.label+"" "",r+=e.series.data[e.index][1]+"" ""+t+"","",r+=e.series.data[e.index][2]+"" ""+n,r}},xaxis:{tickFormatter:function(e){return a[0].date[parseInt(e,10)-a[0].id[0]]}}};s.getName()===""its""&&$.extend(g.bubbles,{baseRadius:1}),r&&$.extend(g.bubbles,{baseRadius:r}),Flotr.draw(i,u,g)}function D(e,t,n,r){if(!n)return;r?period=365*r:r=.25;var i=[],s=[],o=[],u,a={show_legend:!1,xaxis:!0},f,l;for(u=0;u<n.aging.persons.age.length;u++)f=n.aging.persons.age[u],l=parseInt(f/period,10),i[l]||(i[l]=0),i[l]+=1;for(u=0;u<n.birth.persons.age.length;u++)f=n.birth.persons.age[u],l=parseInt(f/period,10),s[l]||(s[l]=0),s[l]+=1;o=[""Retained"",""Attracted""],yticks=function(e,t){var n=r,i=""years"";return e*=r,e+"" ""+i};var c=[i,s];n&&M(e,o,c,"""",a,!0,yticks,r)}function P(e,t,n){var r=document.getElementById(e),i=$(""#""+e).data(""max""),s=.2;i||(i=0);for(var o=0;o<n.length;o++)for(var u=0;u<n[o].data.length;u++){var a=n[o].data[u][1];a>i&&(i=a,i=parseInt(i*(1+s),10))}(function(){var e=[n,t]})(),graph=Flotr.draw(r,n,{radar:{show:!0},mouse:{track:!0,trackFormatter:function(e){var r="""";for(var i=0;i<n.length;i++)r+=n[i].label+"" "",r+=n[i].data[e.index][1]+"" "",r+=t[e.index][1]+""<br>"";return r}},grid:{circular:!0,minorHorizontalLines:!0},yaxis:{min:0,max:i,minorTickFreq:1},xaxis:{ticks:t}})}function H(e,t){var n=[],r=[],i=[],s=[],o=0,u=0;for(o=0;o<t.length;o++){var a=Report.getMetricDS(t[o]);for(u=0;u<a.length;u++)n[u]||(n[u]=[],s[u]=a[u].getProject()),n[u].push([o,parseInt(a[u].getGlobalData()[t[o]],10)]);r.push([o,a[0].getMetrics()[t[o]].name])}for(u=0;u<n.length;u++)i.push({label:s[u],data:n[u]});P(e,r,i)}function B(e){var t=[""scm_committers"",""scm_authors"",""its_openers"",""its_closers"",""its_changers"",""mls_senders""];H(e,t)}function j(e){var t=[""scm_commits"",""scm_files"",""its_opened"",""its_closed"",""its_changed"",""mls_sent""];H(e,t)}function F(e,t,n,r,i){q(e,t,n,r,i)}function I(e,t,n,r,i){q(e,t,n,r,i)}function q(e,t,n,r,i){var s=n.split("",""),o=t.data;if(!o[s[0]])return;var u={};u.date=o.date,$.each(o,function(e,t){if($.inArray(e,s)===-1)return;u[e]=[];for(var n=0;n<t.length;n++){var r=parseFloat((parseInt(t[n],null)/24).toFixed(2),10);u[e].push(r)}}),u.id=[];for(var a=0;a<o[s[0]].length;a++)u.id.push(a);var f={show_legend:!0,show_labels:!0};v(e,s,u,n,f)}function R(e,t,n,r,i,s,o,u,a,f,l,h){var p=t.getMetrics();n===undefined&&(n=!0);var d;h===undefined?d=t.getGlobalTopData():d=t.getRepositoriesTopData()[h],Utils.isReleasePage()&&(s=!1,i="""");if(s===!0){var v={};$.each(d,function(e,t){var n=e.split("".""),i=n[0],s=n[1];if(r&&r!==i)return!0;r&&r===i&&(v[e]=d[e])}),c(e,v,r,a,p,f,l)}else $.each(d,function(t,n){var s=t.split("".""),o=s[0],u=s[1];if(r&&r!==o)return!0;if(i!==undefined&&i!==u)return!0;c(e,d[t],r,a,p,f,l,i)})}function U(e,t,n,r,i,s){var o=null;h(n,r,i,t,o,s)}function z(e,t,n,r,i){var s=t.getProject(),o=t.getMetrics()[n],u=null;if(!t.getGlobalTopData()[n])return;data=t.getGlobalTopData()[n][r],h(e,s,o,r,data,u,i)}function W(e,t,n){if(n===undefined){if(t===undefined)return;Loader.get_file_data_div(t,Viz.displayTreeMap,e);return}if(n===null)return;var r=d3.scale.category20c(),i=d3.select(""#""+e),s=$(""#""+e).width(),o=$(""#""+e).height(),u=d3.layout.treemap().size([s,o]).sticky(!0).value(function(e){return e.size}),a=function(){this.style(""left"",function(e){return e.x+""px""}).style(""top"",function(e){return e.y+""px""}).style(""width"",function(e){return Math.max(0,e.dx-1)+""px""}).style(""height"",function(e){return Math.max(0,e.dy-1)+""px""})},f=i.datum(n).selectAll("".node"").data(u.nodes).enter().append(""div"").attr(""class"",""treemap-node"").call(a).style(""background"",function(e){return e.children?r(e.name):null}).text(function(e){return e.children?null:e.name});d3.selectAll(""input"").on(""change"",function(){var t=this.value===""count""?function(){return 1}:function(e){return e.size};f.data(u.value(t).nodes).transition().duration(1500).call(a)})}function X(e,t,n,r,i){var s=null,o="""",u=[[],[]];n?$.each(Report.getDataSources(),function(e,t){if(t.getName()===n)return s=t.getMetrics(),!1}):s=Report.getAllMetrics(),$.each(Report.getDataSources(),function(e,t){o=t.getMainMetric();if(n===null&&t.getName()===""scm""||n&&t.getName()==n)return u=[t.getData().id,t.getData()[o]],i===!1&&(u=[t.getData().id,[]]),!1});var a=[[],[]];$.each(t,function(e,t){$.each(t,function(e,t){if(n&&n!==t.getName())return;a=DataProcess.fillDates(a,[t.getData().id,t.getData().date])})});var f=a[0][0],l=document.getElementById(e),c,h=Report.getMarkers();c={container:l,xTickFormatter:function(e){var t=a[1][e-f];return t===""0""&&(t=""""),t},yTickFormatter:function(e){return e+""""},selection:{data:{x:{min:a[0][0],max:a[0][a[0].length-1]}}}},c.data={summary:DataProcess.fillHistory(a[0],u),markers:h,dates:a[1],envision_hide:r,main_metric:o};var p=null,d=function(e,t){var n=t.getData();if(n[m]===undefined)return;c.data[m]===undefined&&(c.data[m]=[]);var r=DataProcess.fillHistory(a[0],[n.id,n[m]]);if(m===o){c.data[m].push({label:p,data:r});if(n[m+""_relative""]===undefined)return;c.data[m+""_relative""]===undefined&&(c.data[m+""_relative""]=[]),r=DataProcess.fillHistory(a[0],[n.id,n[m+""_relative""]]),c.data[m+""_relative""].push({label:p,data:r})}else c.data[m].push({label:p,data:r})},v=function(e,t){p=e,$.each(t,d)};for(var m in s)$.each(t,v);return c.trackFormatter=function(e){var t=e.series.data,n=t[e.index][0]-f,r={},i=Report.getProjectsList();for(var o=0;o<i.length;o++)r[i[o]]={};var u=a[1][n]+"":<br>"";for(var l in s){if(c.data[l]===undefined)continue;if($.inArray(l,c.data.envision_hide)>-1)continue;for(o=0;o<i.length;o++){if(c.data[l][o]===undefined)continue;var h=c.data[l][o].label,p=c.data[l][o].data;u=p[1][n],r[h][l]=u}}u=""<table><tr><td align='right'>""+a[1][n]+""</td></tr>"",u+=""<tr>"",i.length>1&&(u+=""<td></td>"");for(l in s){if(c.data[l]===undefined)continue;if($.inArray(l,c.data.envision_hide)>-1)continue;u+=""<td>""+s[l].name+""</td>""}return u+=""</tr>"",$.each(r,function(e,t){var n=""<tr>"";for(var o in s){if(c.data[o]===undefined)continue;if($.inArray(o,c.data.envision_hide)>-1)continue;mvalue=r[e][o],mvalue===undefined&&(mvalue=""n/a""),n+=""<td>""+mvalue+""</td>""}i.length>1&&(n=""<td>""+e+""</td>""+n),n+=""</tr>"",u+=n}),u+=""</table>"",u},c}function V(e){return e===undefined&&(e={}),e.show_desc===undefined&&(e.show_desc=!0),e.show_title===undefined&&(e.show_title=!0),e.show_labels===undefined&&(e.show_labels=!0),e}function J(e,t){var n=e.getMetrics(),r="""";for(var i=0;i<t.length;i++)i!==0&&(r+="" vs. ""),t[i]in n?r+=n[t[i]].name:r+=t[i];return r}function K(e,t,n,r,i,s){i=V(i);var o="""";i.show_title&&(i.title===undefined?o=J(e,t):o=i.title),s!==undefined?m(r,t,n,o,i):v(r,t,n,o,i)}function Q(e,t,n,r,i,s){s=V(s);var o=J(e,n);v(i,n,r,o,s)}function G(e,t,n,r,i,s){s=V(s);var o=J(e,n);v(i,n,r,o,s)}function Y(e,t,n,r,i,s){s=V(s);var o=J(e,n);v(i,n,r,o,s)}function Z(e,t,n,r,i,s){s=V(s);var o=J(e,n);v(i,n,r,o,s)}function et(e,t,n,r,i,s){s=V(s);var o=J(e,n);v(i,n,r,o,s)}function tt(e,t,n,r,i,s){r=V(r),r.show_legend!==!1&&(r.show_legend=!0);var o=e;g(n,e,t,o,r,i,s)}function nt(e,t,n,r,i,s){s=V(s);var o=J(e,n);v(i,n,r,o,s)}function rt(e,t,n,r,i,s,o){r=V(r),r.show_legend!==!1&&(r.show_legend=!0);var u=e;g(n,e,t,u,r,i,s,null,o)}function it(e,t,n,r,i,s){r=V(r),r.show_legend!==!1&&(r.show_legend=!0);var o=e;g(n,e,t,o,r,i,s)}function st(e,t,n,r,i,s){r=V(r),r.show_legend!==!1&&(r.show_legend=!0);var o=e;g(n,e,t,o,r,i,s)}function ot(e,t,n,r,i){i=V(i);var s="""";i.title===undefined?s=e:s=i.title;var o=[],u=[],a=""bars"";i.graph&&(a=i.graph),$.each(n,function(n,r){var i=Report.cleanLabel(r);u.push(i),o.push(t[r][e])}),O(r,u,o,a,s,i)}function ut(e,t,n,r){var i=Report.getProjectsDataSources(),s=Report.getVizConfig(),o=Viz.getEnvisionOptions(e,i,null,s.summary_hide,r);o.legend_show=n,t&&($.each(i,function(e,t){$.each(t,function(e,t){main_metric=t.getMainMetric()})}),DataProcess.addRelativeValues(o.data,main_metric)),new envision.templates.Envision_Report(o)}var e=""#ffa500"";Viz.displayTop=R,Viz.displayTopCompany=U,Viz.displayTopGlobal=z,Viz.displayBasicChart=O,Viz.displayMetricCompanies=rt,Viz.displayMetricSubReportStatic=ot,Viz.displayMetricsCompany=Q,Viz.displayMetricsDomain=Y,Viz.displayMetricsProject=Z,Viz.displayMetricsPeople=et,Viz.displayMetricsRepo=G,Viz.displayMetricRepos=tt,Viz.displayMetricsCountry=nt,Viz.displayMetricDomains=it,Viz.displayMetricProjects=st,Viz.displayMetricsEvol=K,Viz.displayBubbles=_,Viz.displayDemographicsChart=D,Viz.displayEnvisionAll=ut,Viz.displayTimeToFix=I,Viz.displayTimeToAttention=F,Viz.displayMetricSubReportLines=g,Viz.displayRadarActivity=j,Viz.displayRadarCommunity=B,Viz.displayTreeMap=W,Viz.displayMarkovTable=n,Viz.displayDataSourcesTable=p,Viz.getEnvisionOptions=X,Viz.checkBasicConfig=V,String.prototype.capitalize=function(){return this.replace(/(?:^|\s)\S/g,function(e){return e.toUpperCase()})},Viz.track_formatter_com_pending=function(e){scr=Report.getDataSourceByName(""scr""),companies=scr.getCompaniesMetricsData(),dhistory=Viz._history,lines_data=Viz._lines_data;var t=dhistory.date[parseInt(e.index,10)];t===undefined?t="""":t+=""<br>"";for(var n=0;n<lines_data.length;n++){var r=lines_data[n].data[e.index][1];if(r===undefined)continue;lines_data.length>1&&(lines_data[n].label!==undefined&&(company_name=lines_data[n].label),t+=lines_data[n].label+"":""),t+=""<strong>""+Report.formatValue(r)+""</strong>"",company_name&&(t+=""(""+companies[company_name].pending[e.index]+"")""),t+=""<br>""}return t},Viz.getEnvisionOptionsMin=function(e,t,n){var r=t.id[0],i=document.getElementById(e),s,o=Report.getMarkers(),u=Report.getAllMetrics();s={container:i,xTickFormatter:function(e){var n=t.date[e-r];return n===""0""&&(n=""""),n},yTickFormatter:function(e){return e+""""},selection:{data:{x:{min:t.id[0],max:t.id[t.id.length-1]}}}},s.data={summary:[t.id,t.sent],markers:o,dates:t.date,envision_hide:n,main_metric:""sent""};var a=Report.getAllMetrics(),f=null;for(var l in t)f=l,a[l]&&(f=a[l].name),s.data[l]=[{label:f,data:[t.id,t[l]]}];return s.trackFormatter=function(e){var n=e.series.data,i=n[e.index][0]-r,s=t.date[i]+"":<br>"";for(var o in u){if(t[o]===undefined)continue;s+=t[o][i]+"" ""+o+"" , ""}return s},s}})(),IRC.prototype=new DataSource(""irc""),ITS.prototype=new DataSource(""its""),MediaWiki.prototype=new DataSource(""mediawiki""),MLS.prototype=new DataSource(""mls""),SCM.prototype=new DataSource(""scm""),SCR.prototype=new DataSource(""scr""),People.prototype=new DataSource(""people""),Downloads.prototype=new DataSource(""downloads""),QAForums.prototype=new DataSource(""qaforums""),Releases.prototype=new DataSource(""releases"");var Identity={};(function(){function t(t,n,r){var i="""";t===e?i="""":i=e,$(""#""+r).sortable({handle:"".handle"",connectWith:""#""+i,start:function(e,t){t.item.siblings("".ui-selected"").appendTo(t.item)},stop:function(t,n){n.item.parent()[0].id===e&&n.item.find("".handle"").remove(),n.item.parent().append(n.item.find(""li"")),n.item.parent().find(""li"").addClass(""mjs-nestedSortable-leaf"")}}).selectable().find(""li"").prepend(""<div class='handle'></div>"")}function n(e,t){$(""#""+e.getName()+""filter"").autocomplete({source:t,select:function(t,n){return $(""#""+e.getName()+""filter"").val(""""),$(""#""+e.getName()+""_people_""+n.item.value).addClass(""ui-selected""),!1}})}var e=""unique-sortable"";Identity.showListNested=function(t,n){list=""<ol id=""+e+' class=""nested_sortable"" ',list+='style=""padding: 5px; background: #eee;""></ol>',$(""#""+t).append(list),$(""#""+e).nestedSortable({forcePlaceholderSize:!0,handle:""div"",helper:""clone"",items:""li"",tolerance:""pointer"",toleranceElement:""> div"",maxLevels:2,isTree:!0,expandOnHover:700,startCollapsed:!0}),$("".disclose"").on(""click"",function(){$(this).closest(""li"").toggleClass(""mjs-nestedSortable-collapsed"").toggleClass(""mjs-nestedSortable-expanded"")})},Identity.showList=function(e,r){var i="""",s=r.getPeopleData(),o=[];i='<ol id=""'+r.getName()+'-sortable"" class=""sortable"">';for(var u=0;u<s.id.length;u++){var a=s.id[u];typeof a==""string""&&(a=a.replace(""@"",""_at_"").replace(""."",""_"")),o.push({value:a,label:s.name[u]}),i+='<li id=""'+r.getName()+""_people_""+a+'"" ',i+='class=""ui-widget-content ui-selectee"">',i+='<div><span class=""disclose""><span></span></span>',i+=s.id[u]+"" ""+s.name[u],i+=""</div></li>""}i+=""</ol>"",$(""#""+e).append(""<input id='""+r.getName()+""filter'>""),n(r,o),$(""#""+e).append(i),t(e,i,r.getName()+""-sortable"")}})(),vizjslib_git_revision=""36be874ff592b7bf6d95587e933a0a481aae510a"",vizjslib_git_tag=""2.1.3-108-g36be874"";","(""for_each"",""below"",e,t,n),this},o.get_highest_occupied_cell=function(){var e,t=this.gridmap,n=[],r=[];for(var i=t.length-1;i>=1;i--)for(e=t[i].length-1;e>=1;e--)if(this.is_widget(i,e)){n.push(e),r[e]=i;break}var s=Math.max.apply(Math,n);return this.highest_occupied_cell={col:r[s],row:s},this.highest_occupied_cell},o.get_widgets_from=function(t,n){var r=this.gridmap,i=e();return t&&(i=i.add(this.$widgets.filter(function(){var n=e(this).attr(""data-col"");return n===t||n>t}))),n&&(i=i.add(this.$widgets.filter(function(){var t=e(this).attr(""data-row"");return t===n||t>n}))),i},o.set_dom_grid_height=function(){var e=this.get_highest_occupied_cell().row;return this.$el.css(""height"",e*this.min_widget_height),this},o.generate_stylesheet=function(t){var n="""",r=this.options.max_size_x,i=0,o=0,u,a;t||(t={}),t.cols||(t.cols=this.cols),t.rows||(t.rows=this.rows),t.namespace||(t.namespace=this.options.namespace),t.widget_base_dimensions||(t.widget_base_dimensions=this.options.widget_base_dimensions),t.widget_margins||(t.widget_margins=this.options.widget_margins),t.min_widget_width=t.widget_margins[0]*2+t.widget_base_dimensions[0],t.min_widget_height=t.widget_margins[1]*2+t.widget_base_dimensions[1];var f=e.param(t);if(e.inArray(f,s.generated_stylesheets)>=0)return!1;s.generated_stylesheets.push(f);for(u=t.cols;u>=0;u--)n+=t.namespace+' [data-col=""'+(u+1)+'""] { left:'+(u*t.widget_base_dimensions[0]+u*t.widget_margins[0]+(u+1)*t.widget_margins[0])+""px;} "";for(u=t.rows;u>=0;u--)n+=t.namespace+' [data-row=""'+(u+1)+'""] { top:'+(u*t.widget_base_dimensions[1]+u*t.widget_margins[1]+(u+1)*t.widget_margins[1])+""px;} "";for(var l=1;l<=t.rows;l++)n+=t.namespace+' [data-sizey=""'+l+'""] { height:'+(l*t.widget_base_dimensions[1]+(l-1)*t.widget_margins[1]*2)+""px;}"";for(var c=1;c<=r;c++)n+=t.namespace+' [data-sizex=""'+c+'""] { width:'+(c*t.widget_base_dimensions[0]+(c-1)*t.widget_margins[0]*2)+""px;}"";return this.add_style_tag(n)},o.add_style_tag=function(e){var t=n,r=t.createElement(""style"");return t.getElementsByTagName(""head"")[0].appendChild(r),r.setAttribute(""type"",""text/css""),r.styleSheet?r.styleSheet.cssText=e:r.appendChild(n.createTextNode(e)),this},o.generate_faux_grid=function(e,t){this.faux_grid=[],this.gridmap=[];var n,r;for(n=t;n>0;n--){this.gridmap[n]=[];for(r=e;r>0;r--)this.add_faux_cell(r,n)}return this},o.add_faux_cell=function(t,n){var r=e({left:this.baseX+(n-1)*this.min_widget_width,top:this.baseY+(t-1)*this.min_widget_height,width:this.min_widget_width,height:this.min_widget_height,col:n,row:t,original_col:n,original_row:t}).coords();return e.isArray(this.gridmap[n])||(this.gridmap[n]=[]),this.gridmap[n][t]=!1,this.faux_grid.push(r),this},o.add_faux_rows=function(e){var t=this.rows,n=t+(e||1);for(var r=n;r>t;r--)for(var i=this.cols;i>=1;i--)this.add_faux_cell(r,i);return this.rows=n,this.options.autogenerate_stylesheet&&this.generate_stylesheet(),this},o.add_faux_cols=function(e){var t=this.cols,n=t+(e||1);for(var r=t;r<n;r++)for(var i=this.rows;i>=1;i--)this.add_faux_cell(i,r);return this.cols=n,this.options.autogenerate_stylesheet&&this.generate_stylesheet(),this},o.recalculate_faux_grid=function(){var n=this.$wrapper.width();return this.baseX=(e(t).width()-n)/2,this.baseY=this.$wrapper.offset().top,e.each(this.faux_grid,e.proxy(function(e,t){this.faux_grid[e]=t.update({left:this.baseX+(t.data.col-1)*this.min_widget_width,top:this.baseY+(t.data.row-1)*this.min_widget_height})},this)),this},o.get_widgets_from_DOM=function(){return this.$widgets.each(e.proxy(function(t,n){this.register_widget(e(n))},this)),this},o.generate_grid_and_stylesheet=function(){var n=this.$wrapper.width(),r=this.$wrapper.height(),i=Math.floor(n/this.min_widget_width)+this.options.extra_cols,s=this.$widgets.map(function(){return e(this).attr(""data-col"")});s=Array.prototype.slice.call(s,0),s.length||(s=[0]);var o=Math.max.apply(Math,s),u=this.options.extra_rows;return this.$widgets.each(function(t,n){u+=+e(n).attr(""data-sizey"")}),this.cols=Math.max(o,i,this.options.min_cols),this.rows=Math.max(u,this.options.min_rows),this.baseX=(e(t).width()-n)/2,this.baseY=this.$wrapper.offset().top,this.options.autogenerate_stylesheet&&this.generate_stylesheet(),this.generate_faux_grid(this.rows,this.cols)},e.fn.gridster=function(t){return this.each(function(){e(this).data(""gridster"")||e(this).data(""gridster"",new s(this,t))})},e.Gridster=o}(jQuery,window,document),vizjslib_git_revision=""6c6bf1471f91c28dc83253ad74ed2a6848c1c538"",vizjslib_git_tag=""2.1.5-46-g6c6bf14"",function(){function n(e,t,n){var r="""";if(!t||t.length===0)return r;for(var i=0;i<t.date.length;i++)t.date[i]==n[e.index]&&(r=t.marks[i]);return r}function r(e,t){var r={name:e,config:{colors:t.colors,grid:{verticalLines:!1,horizontalLines:!1},mouse:{track:!0,trackY:!1,position:""ne""},yaxis:{min:0,autoscale:!0},legend:{show:!1,backgroundColor:""#FFFFFF"",backgroundOpacity:0}}};return t.gtype===""whiskers""?r.config.whiskers={show:!0,lineWidth:2}:r.config[""lite-lines""]={lineWidth:2,show:!0,fill:!1,fillOpacity:.5},t.y_labels&&(r.config.yaxis={showLabels:!0,min:0}),t.show_markers&&(r.config.markers={show:!0,position:""ct"",labelFormatter:function(e){return n(e,t.markers,t.dates)}}),r}function i(e,n,i,s){var o=Report.getAllMetrics(),u=null;$.each(i,function(i,a){config=s,a.envision&&(config=DataProcess.mergeConfig(s,a.envision)),$.inArray(i,t.envision_hide)===-1&&(n[i]=r(""report-""+e.getName()+""-""+i,config),u=i,o[i]&&(u=o[i].name),n[i].config.subtitle=u,e.getMainMetric()==i&&(n[i+""_relative""]=r(""report-""+e.getName()+""-""+i+""_relative"",config),n[i].config[""lite-lines""]={show:!1},n[i].config.lines={lineWidth:1,show:!0,stacked:!0,fill:!0,fillOpacity:1}))})}function s(n){var s=[""#ffa500"",""#00A8F0"",""#C0D800"",""#ffff00"",""#00ff00"",""#4DA74D"",""#9440ED""],o={colors:s,dates:t.dates,g_type:"""",markers:t.markers,y_labels:!1},u=Report.getDataSources(),a={},f={};return n?$.each(u,function(e,t){$.inArray(t.getName(),n)>-1&&(f=t.getMetrics(),i(t,a,f,o))}):$.each(u,function(e,t){f=t.getMetrics(),i(t,a,f,o)}),config=o,a.summary=r(""report-summary"",config),a.summary.config.xaxis={noTickets:10,showLabels:!0},a.summary.config.handles={show:!0},a.summary.config.selection={mode:""x""},a.summary.config.mouse={},a.connection={name:""report-connection"",adapterConstructor:e.components.QuadraticDrawing},a}function o(e,t){var n=[],r=null;return $.each(e,function(e,n){if(n.getMetrics()[t])return r=n,!1}),n.push(r),$.each(e,function(e,t){if(t===r)return;n.push(t)}),n}function u(n,r){var i=n.data.main_metric;t=n.data,r||(r=Report.getDataSources()),r=o(r,i);var u=[];for(var a=0;a<r.length;a++){if(r[a].getData().length===0)continue;u.push(r[a].getName())}var f=n.data,l=s(u),c=new e.Visualization({name:""report-""+u.join("","")}),h=new e.Interaction,p=new e.Interaction,d={};$.each(r,function(e,t){if(t.getData().length===0)return;d=$.extend(d,t.getMetrics())}),$.each(d,function(e,t){if($.inArray(e,f.envision_hide)!==-1)return;if(f[e]===undefined)return;l[e].data=f[e],l[e].data.length<Report.getProjectsList().length&&(l[e].config.legend.show=!0),f[e+""_relative""]&&(l[e].data=f[e+""_relative""])}),l.summary.data=f.summary,l[i].config.legend.show=!0,n.legend_show===!1&&(l[i].config.legend.show=!1),l[i].config.mouse.trackFormatter=n.trackFormatter,n.xTickFormatter&&(l.summary.config.xaxis.tickFormatter=n.xTickFormatter),l[i].config.yaxis.tickFormatter=n.yTickFormatter||function(e){return""$""+e};var v={};$.each(d,function(t,n){if(f[t]===undefined)return;$.inArray(t,f.envision_hide)===-1&&(v[t]=new e.Component(l[t]))}),connection=new e.Component(l.connection),summary=new e.Component(l.summary),$.each(v,function(e,t){c.add(t)}),c.add(connection).add(summary).render(n.container),$.each(v,function(e,t){h.follower(t)}),h.follower(connection).leader(summary).add(e.actions.selection,n.selectionCallback?{callback:n.selectionCallback}:null);var m=[];$.each(v,function(e,t){m.push(t)}),p.group(m).add(e.actions.hit),n.selection&&summary.trigger(""select"",n.selection)}var e=envision,t={};e.templates.Envision_Report=u}();if(Loader===undefined)var Loader={};(function(){function a(e,t){e.project_name===undefined&&(e.project_name=t.replace(""data/json"","""").replace(/\.\.\//g,""""));var n=Report.getProjectsData();n[e.project_name]={dir:t,url:e.project_url}}function f(e,t,n){$.when($.getJSON(e)).done(function(e){t(e,n),D()}).fail(function(){t([],n),D()})}function l(){var e=[""irc"",""mediawiki""],t=Report.getDataSources();$.each(t,function(t,n){$.inArray(n.getName(),e)>-1?n.setCompaniesData([]):f(n.getCompaniesDataFile(),n.setCompaniesData,n)})}function c(){var e=[""mediawiki""],t=Report.getDataSources();$.each(t,function(t,n){$.inArray(n.getName(),e)>-1?n.setReposData([]):f(n.getReposDataFile(),n.setReposData,n)}),f(Report.getReposMapFile(),Report.setReposMap)}function h(){var e=[""irc"",""mediawiki""],t=Report.getDataSources();$.each(t,function(t,n){$.inArray(n.getName(),e)>-1?n.setCountriesData([]):f(n.getCountriesDataFile(),n.setCountriesData,n)})}function p(){var e=[""irc"",""mediawiki""],t=Report.getDataSources();$.each(t,function(t,n){$.inArray(n.getName(),e)>-1?n.setDomainsData([]):f(n.getDomainsDataFile(),n.setDomainsData,n)})}function d(){var e=[""irc"",""mediawiki""],t=Report.getDataSources();$.each(t,function(t,n){$.inArray(n.getName(),e)>-1?n.setProjectsData([]):f(n.getProjectsDataFile(),n.setProjectsData,n)})}function v(){var e=Report.getDataSources();$.each(e,function(e,t){t.getName()===""its""&&f(t.getTimeToFixDataFile(),t.setTimeToFixData,t)})}function m(){var e=Report.getDataSources();$.each(e,function(e,t){t.getName()===""its""&&f(t.getMarkovTableDataFile(),t.setMarkovTableData,t)})}function g(){var e=Report.getDataSources();$.each(e,function(e,t){t.getName()===""mls""&&f(t.getTimeToAttentionDataFile(),t.setTimeToAttentionData,t)})}function y(){var e=Report.getDataSources();$.each(e,function(e,t){f(t.getDemographicsAgingFile(),t.setDemographicsAgingData,t),f(t.getDemographicsBirthFile(),t.setDemographicsBirthData,t)})}function b(e){var t=Report.getDataSources();$.each(t,function(e,t){var n=t.getTopDataFile();$.when($.getJSON(n)).done(function(e){t.setGlobalTopData(e),D()}).fail(function(){t.setGlobalTopData([],t),D()})})}function w(e,t){var n=null;return $.each(Report.getDataSources(),function(r,i){if(t==""repos""&&$.inArray(e,i.getReposData())>-1)return n=i,!1;if(t==""companies""&&$.inArray(e,i.getCompaniesData())>-1)return n=i,!1;if(t==""countries""&&$.inArray(e,i.getCountriesData())>-1)return n=i,!1;if(t==""domains""&&$.inArray(e,i.getDomainsData())>-1)return n=i,!1;if(t==""projects""&&$.inArray(e,i.getProjectsData())>-1)return n=i,!1}),n}function E(e){var t="""";return e===""repos""?t=""rep"":e===""companies""?t=""com"":e===""countries""?t=""cou"":e===""domains""?t=""dom"":e===""projects""&&(t=""prj""),t}function S(){var e=Report.getDataSources();$.each(e,function(e,t){f(t.getDataFile(),t.setData,t),f(t.getGlobalDataFile(),t.setGlobalData,t),t instanceof MLS&&f(t.getListsFile(),t.setListsData,t)})}function x(){f(""VizGrimoireJS/data/metrics.json"",Report.setMetricsDefinition)}function T(){var e=Report.getDataSources();$.each(e,function(e,t){f(t.getPeopleDataFile(),t.setPeopleData,t)})}function N(){f(Report.getDataDir()+""/people.json"",Report.setPeopleIdentities)}function C(e){return e.getCompaniesData()===null?!1:!0}function k(e){return e.getReposData()===null?!1:!0}function L(e){return e.getCountriesData()===null?!1:!0}function A(e){return e.getDomainsData()===null?!1:!0}function O(e){return e.getProjectsData()===null?!1:!0}function M(){var e=0,t=Report.getProjectsData(),n=Report.getProjectsDirs();for(var r in t)e++;return e<n.length?!1:!0}function _(){var e=!0;if(Report.getProjectData()===null||Report.getVizConfig()===null)return!1;if(Report.getConfig()===null&&Report.getMarkers()===null)return!1;if(Report.getReposMap()===null)return!1;if(Report.getConfig()===null&&!M())return!1;var t=Report.getDataSources();return $.each(t,function(t,n){if(n.getData()===null)return e=!1,!1;if(n.getGlobalData()===null)return e=!1,!1;if(n.getGlobalTopData()===null)return e=!1,!1;if(n.getDemographicsData().aging===undefined||n.getDemographicsData().birth===undefined)return e=!1,!1;if(n.getName()===""its""&&n.getTimeToFixData()===null)return e=!1,!1;if(n.getName()===""mls""&&n.getTimeToAttentionData()===null)return e=!1,!1}),e}function D(){if(_()){for(var n=0;n<t.length;n++)t[n]();t=[]}if(Loader.check_data_loaded())for(var r=0;r<e.length;r++)e[r].called!==!0&&e[r](),e[r].called=!0}var e=[],t=[],n=[],r=!1,i=!1,s=!1,o=[""scr"",""irc"",""mediawiki""],u=[""scm"",""mls"",""its""];Loader.data_ready=function(t){e.push(t)},Loader.data_ready_global=function(e){t.push(e)},Loader.data_load=function(){Report.getConfig()!==null&&Report.getConfig().project_info!==undefined?(Report.setProjectData(Report.getConfig().project_info),Report.getConfig().markers&&f(Report.getMarkersFile(),function(e,t){Report.setMarkers(e)})):(f(Report.getProjectFile(),function(e,t){Report.setProjectData(e)}),f(Report.getMarkersFile(),function(e,t){Report.setMarkers(e)}));var e=Report.getProjectsDirs();for(var t=0;t<e.length;t++){var n=e[t],r=Report.getDataDir()+""/project-info.json"";f(r,a,n)}f(Report.getProjectsHierarchyFile(),Report.setProjectsHierarchy),f(Report.getMenuElementsFile(),Report.setMenuElements),f(Report.getVizConfigFile(),function(e,t){Report.setVizConfig(e)}),x(),S(),b(""authors""),v(),g(),y(),m();if(Report.getConfig()!==null&&Report.getConfig().reports!==undefined){var i=Report.getConfig().reports;$.inArray(""companies"",i)>-1&&l(),$.inArray(""repositories"",i)>-1&&c(),$.inArray(""countries"",i)>-1&&h(),$.inArray(""domains"",i)>-1&&p(),$.inArray(""projects"",i)>-1&&d(),$.inArray(""people"",i)>-1&&(T(),N())}else l(),c(),h(),p(),d(),T(),N()},Loader.get_file_data_div=function(e,t,n){$.when($.getJSON(e)).done(function(r){t(n,e,r)}).fail(function(){t(e,null)})},Loader.check_filters_page=function(e){var t=!0,n=[""repos"",""companies"",""countries""];return $.each(n,function(n,r){if(!Loader.check_filter_page(e,r))return t=!1,!1}),t},Loader.check_filter_page=function(e,t){var n=!0;e===undefined&&(e=1);var r=Report.getPageSize()*(e-1),i=r+Report.getPageSize();return $.each(Report.getDataSources(),function(e,s){var o=0;t===""repos""&&(o=s.getReposData().length),t===""companies""&&(o=s.getCompaniesData().length),t===""countries""&&(o=s.getCountriesData().length),t===""domains""&&(o=s.getDomainsData().length),t===""projects""&&(o=s.getProjectsData().length),i>o&&(i=o);for(var u=r;u<i;u++){var a;if(t===""repos""){a=s.getReposData()[u];if(s.getReposGlobalData()[a]===undefined||s.getReposMetricsData()[a]===undefined)return n=!1,!1}if(t===""companies""){a=s.getCompaniesData()[u];if(s.getCompaniesGlobalData()[a]===undefined||s.getCompaniesMetricsData()[a]===undefined)return n=!1,!1}if(t===""countries""){a=s.getCountriesData()[u];if(s.getCountriesGlobalData()[a]===undefined||s.getCountriesMetricsData()[a]===undefined)return n=!1,!1}if(t===""domains""){a=s.getDomainsData()[u];if(s.getDomainsGlobalData()[a]===undefined||s.getDomainsMetricsData()[a]===undefined)return n=!1,!1}if(t===""projects""){a=s.getProjectsData()[u];if(s.getProjectsGlobalData()[a]===undefined||s.getProjectsMetricsData()[a]===undefined)return n=!1,!1}}i=r+Report.getPageSize()}),n},Loader.filterTopCheck=function(e,t){var n=!0;return t===""repos""&&Loader.check_item(e,t,""top"")===!1?(ds=w(e,t),ds===null?(Report.log(""Can't find data source for ""+e),!0):($.inArray(ds.getName(),u)>=0&&Loader.data_load_item_top(e,ds,null,Convert.convertFilterTop,t,""top""),!1)):n},Loader.FilterItemCheck=function(e,t){var n=!0,r,i=Report.getReposMap();if(t===""repos""){if(Loader.check_item(e,t)===!1)return r=w(e,t),r===null?(Report.log(""Can't find data source for ""+e),!0):(Loader.data_load_item(e,r,null,Convert.convertFilterStudyItem,t,null),$.inArray(r.getName(),u)>=0&&Loader.data_load_item_top(e,r,null,Convert.convertFilterStudyItem,t),!1);if(i!==undefined&&i.length!==0){var s=[];$.each(Report.getDataSources(),function(n,r){var i=Convert.getRealItem(r,t,e);i!==undefined&&i!==null&&s.push(i)});if(Loader.check_items(s,t)===!1){for(var a=0;a<s.length;a++)if(Loader.check_item(s[a],t)===!1){r=w(s[a],t);if(r===null){Report.log(""Can't find ""+s[a]),Report.log(""Check repos-map.json"");continue}Loader.data_load_item(s[a],r,null,Convert.convertFilterStudyItem,t,s)}n=!1}}}else $.each(Report.getDataSources(),function(r,i){Loader.check_item(e,t)===!1&&(n=!1,Loader.data_load_item(e,i,null,Convert.convertFilterStudyItem,t,null),t===""companies""&&$.inArray(i.getName(),o)===-1&&Loader.data_load_item_top(e,i,null,Convert.convertFilterStudyItem,t))});return n},Loader.check_item=function(e,t,n){var r=!1;return $.each(Report.getDataSources(),function(i,s){if(t===""repos""){if(n===""top""){if($.inArray(s.getName(),u)>=0&&$.inArray(e,s.getReposData())>=0&&s.getRepositoriesTopData()[e]!==undefined)return r=!0,!1}else if(s.getReposGlobalData()[e]!==undefined&&s.getReposMetricsData()[e]!==undefined)return r=!0,!1}else if(t===""companies""){var a=s.getCompaniesData();if(a.length===0)r=!0;else if($.inArray(e,a)===-1)r=!0;else{if(s.getCompaniesGlobalData()[e]===undefined||s.getCompaniesMetricsData()[e]===undefined)return r=!1,!1;if($.inArray(s.getName(),o)===-1&&s.getCompaniesTopData()[e]===undefined)return r=!1,!1;r=!0}}else if(t===""countries""){var f=s.getCountriesData();if(f.length===0)r=!0;else if($.inArray(e,f)===-1)r=!0;else{if(s.getCountriesGlobalData()[e]===undefined||s.getCountriesMetricsData()[e]===undefined)return r=!1,!1;r=!0}}else if(t===""domains""){var l=s.getDomainsData();if(l.length===0)r=!0;else if($.inArray(e,l)===-1)r=!0;else{if(s.getDomainsGlobalData()[e]===undefined||s.getDomainsMetricsData()[e]===undefined)return r=!1,!1;r=!0}}else if(t===""projects""){var c=s.getProjectsData();if(c.length===0)r=!0;else if($.inArray(e,c)===-1)r=!0;else{if(s.getProjectsGlobalData()[e]===undefined||s.getProjectsMetricsData()[e]===undefined)return r=!1,!1;r=!0}}}),r},Loader.check_items=function(e,t){var n=!0;return $.each(e,function(e,r){if(Loader.check_item(r,t)===!1)return n=!1,!1}),n},Loader.data_load_items_page=function(e,t,n,r){t===undefined&&(t=1);if(r===""repos""&&e.getReposData()===null)return!1;if(r===""companies""&&e.getCompaniesData()===null)return!1;if(r===""countries""&&e.getCountriesData()===null)return!1;if(r===""domains""&&e.getDomainsData()===null)return!1;if(r===""projects""&&e.getProjectsData()===null)return!1;var i=0;r===""repos""&&(i=e.getReposData().length),r===""companies""&&(i=e.getCompaniesData().length),r===""countries""&&(i=e.getCountriesData().length),r===""domains""&&(i=e.getDomainsData().length),r===""projects""&&(i=e.getProjectsData().length);if(i===0)return!0;var s=Report.getPageSize()*(t-1),o=s+Report.getPageSize();o>i&&(o=i);for(var u=s;u<o;u++)if(r===""repos""){var a=e.getReposData()[u];Loader.data_load_item(a,e,t,n,""repos"")}else if(r===""companies""){var f=e.getCompaniesData()[u];Loader.data_load_item(f,e,t,n,""companies"")}else if(r===""countries""){var l=e.getCountriesData()[u];Loader.data_load_item(l,e,t,n,""countries"")}else if(r===""domains""){var c=e.getDomainsData()[u];Loader.data_load_item(c,e,t,n,""domains"")}else if(r===""projects""){var h=e.getProjectsData()[u];Loader.data_load_item(h,e,t,n,""projects"")}},Loader.check_people_item=function(e){var t=!0;return $.each(Report.getDataSources(),function(n,r){if(r.getPeopleGlobalData()[e]===undefined||r.getPeopleMetricsData()[e]===undefined)return t=!1,!1}),t},Loader.data_load_people_item=function(e,t,n){var r=t.getDataDir()+""/people-""+e+""-""+t.getName(),i=r+""-evolutionary.json"",s=r+""-static.json"";$.when($.getJSON(i),$.getJSON(s)).done(function(r,i){t.addPeopleMetricsData(e,r[0],t),t.addPeopleGlobalData(e,i[0],t),Loader.check_people_item(e)&&n(e)}).fail(function(){t.addPeopleMetricsData(e,[],t),t.addPeopleGlobalData(e,[],t),Loader.check_people_item(e)&&n(e)})},Loader.data_load_item_top=function(e,t,n,r,i,s){var o=t.getDataDir()+""/""+e+""-""+t.getName();o+=""-""+E(i)+""-top-"";if(t.getName()===""scm"")o+=""authors"";else if(t.getName()===""its"")o+=""closers"";else{if(t.getName()!==""mls"")return;o+=""senders""}o+="".json"",$.when($.getJSON(o)).done(function(n){i===""companies""?t.addCompanyTopData(e,n):i===""repos""&&t.addRepositoryTopData(e,n)}).fail(function(){i===""companies""?t.addCompanyTopData(e,[]):i===""repos""&&t.addRepositoryTopData(e,[])}).always(function(){Loader.check_item(e,i,s)&&(r.called_item||r(i),r.called_item=!0)})},Loader.data_load_item=function(e,t,n,r,i,s){var o=[""irc"",""mediawiki""],u=[""irc"",""mediawiki""],a=[""irc"",""mediawiki""],f=[""mediawiki""],l=[""irc"",""mediawiki""];if(i===""repos""){if($.inArray(t.getName(),f)>-1){t.addRepoMetricsData(e,[],t),t.addRepoGlobalData(e,[],t);return}}else if(i===""companies""){if($.inArray(t.getName(),u)>-1){t.addCompanyMetricsData(e,[],t),t.addCompanyGlobalData(e,[],t);return}}else if(i===""countries""){if($.inArray(t.getName(),o)>-1){t.addCountryMetricsData(e,[],t),t.addCountryGlobalData(e,[],t);return}}else if(i===""domains""){if($.inArray(t.getName(),a)>-1){t.addDomainMetricsData(e,[],t),t.addDomainGlobalData(e,[],t);return}}else{if(i!==""projects"")return;if($.inArray(t.getName(),l)>-1){t.addDomainMetricsData(e,[],t),t.addDomainGlobalData(e,[],t);return}}var c=encodeURIComponent(e),h=t.getDataDir()+""/""+c+""-"";h+=t.getName()+""-""+E(i);var p=h+""-evolutionary.json"",d=h+""-static.json"";$.when($.getJSON(p),$.getJSON(d)).done(function(n,r){i===""repos""?(t.addRepoMetricsData(e,n[0],t),t.addRepoGlobalData(e,r[0],t)):i===""companies""?(t.addCompanyMetricsData(e,n[0],t),t.addCompanyGlobalData(e,r[0],t)):i===""countries""?(t.addCountryMetricsData(e,n[0],t),t.addCountryGlobalData(e,r[0],t)):i===""domains""?(t.addDomainMetricsData(e,n[0],t),t.addDomainGlobalData(e,r[0],t)):i===""projects""&&(t.addProjectMetricsData(e,n[0],t),t.addProjectGlobalData(e,r[0],t))}).always(function(){n!==null?Loader.check_filter_page(n,i)&&(r.called_page===undefined?(r.called_page={},r.called_page[i]=!0,r(i)):r.called_page[i]||(r(i),r.called_page[i]=!0)):s!==null?Loader.check_items(s,i)&&(r.called_map===undefined?(r.called_map={},r.called_map[i]=!0,r(i)):r.called_map[i]||(r(i),r.called_map[i]=!0)):Loader.check_item(e,i)&&(r.called_item===undefined?(r.called_item={},r.called_item[i]=!0,r(i,e)):r.called_item[i]||(r(i,e),r.called_item[i]=!0))})},Loader.check_data_loaded=function(){var e=!0;if(!_())return!1;var t=Report.getDataSources(),n=[""companies"",""repositories"",""countries"",""domains"",""projects""];return Report.getConfig()!==null&&Report.getConfig().reports!==undefined&&(n=Report.getConfig().reports),$.each(t,function(t,r){if(r.getPeopleData()===null)return e=!1,!1;if($.inArray(""companies"",n)>-1&&!C(r))return e=!1,!1;if($.inArray(""repositories"",n)>-1&&!k(r))return e=!1,!1;if($.inArray(""countries"",n)>-1&&!L(r))return e=!1,!1;if($.inArray(""domains"",n)>-1&&!A(r))return e=!1,!1;if($.inArray(""projects"",n)>-1&&!O(r))return e=!1,!1;if(r instanceof MLS&&r.getListsData()===null)return e=!1,!1}),e}})();var DataProcess={};(function(){DataProcess.info=function(){},DataProcess.paginate=function(e,t){if(t===undefined||t===0||isNaN(t))return e;var n=[],r=Report.getPageSize(),i=(t-1)*r;for(var s=i;s<r*t;s++)e[s]&&n.push(e[s]);return n},DataProcess.convert=function(e,t,n){return t===""aggregate""?e=DataProcess.aggregate(e,n):t===""substract""?(e=DataProcess.substract(e,n[0],n[1]),n=[""substract""]):t===""substract-aggregate""?(e=DataProcess.substract(e,n[0],n[1]),n=[""substract""],e=DataProcess.aggregate(e,n)):t===""divide""&&(e=DataProcess.divide(e,n[0],n[1]),n=[""divide""]),e},DataProcess.sortGlobal=function(e,t,n){t===undefined&&(t=""scm_commits"");var r=[],i=[],s={};s.name=[],s[t]=[];var o=null;n===""companies""?(i=e.getCompaniesData(),o=e.getCompaniesDataFull()):n===""repos""?(i=e.getReposData(),o=e.getReposDataFull()):n===""countries""?i=e.getCountriesData():n===""domains""?(i=e.getDomainsData(),o=e.getDomainsDataFull()):n===""projects""&&(i=e.getProjectsData());if(i===null)return[];if(o===null)return i;if(o instanceof Array||t in o==0)return i;for(var u=0;u<o[t].length;u++){var a=o[t][u];a===""NA""&&(a=0),r.push([o.name[u],a])}return r.sort(function(e,t){return t[1]-e[1]}),$.each(r,function(e,n){s.name.push(n[0]),s[t].push(n[1])}),s.name},DataProcess.orderItems=function(e){$.each($(""[class^='FilterItems']""),function(t,n){order_by=$(this).data(""order-by"");if(order_by!==undefined){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;var r=$(this).data(""filter"");if(r===undefined)return;if(r!==e)return;Report.log(""Ordering with ""+order_by+"" ""+ds+"" for ""+r);var i=DataProcess.sortGlobal(DS,order_by,r);return r===""companies""&&DS.setCompaniesData(i),r===""repos""&&DS.setReposData(i),r===""countries""&&DS.setCountriesData(i),r===""domains""&&DS.setDomainsData(i),!1}})},DataProcess.mergeConfig=function(e,t){var n={};return $.each(e,function(e,t){n[e]=t}),$.each(t,function(e,t){n[e]=t}),n},DataProcess.hideEmail=function(e){var t=e;return typeof e==""string""&&e.indexOf(""@"")>-1&&(t=e.split(""@"")[0]),t},DataProcess.selectPersonName=function(e){var t="""",n,r;for(var i=0;i<e.identity.length;i++)n=e.identity[i],r=e.type[i],r===""name""&&n.length>t.length&&(t=n);return t},DataProcess.selectPersonEmail=function(e){var t="""",n,r;for(var i=0;i<e.identity.length;i++)n=e.identity[i],r=e.type[i],r===""email""&&(t=n);return t},DataProcess.frameTime=function(e,t){var n={},r=-1,i=-1,s=0;if(t.length===0)return e;var o=e[t[0]].length,u=0;$.each(t,function(t,n){s=0;for(u=0;u<e[n].length;u++){if(e[n][u]!==0){r===-1&&(r=s),s<r&&(r=s);break}s++}}),$.each(t,function(t,n){s=0;for(u=e[n].length-1;u>=0;u--){if(e[n][u]!==0){i===-1&&(i=s),s<i&&(i=s);break}s++}});for(var a in e){n[a]=[];for(u=0;u<e[a].length;u++){if(u<r)continue;if(u>=o-i)continue;n[a].push(e[a][u])}}return n},DataProcess.filterDates=function(e,t,n){var r={};return $.each(n,function(i,s){r[i]=[],$.each(s,function(s,o){var u=n.unixtime[s];u>e&&(!t||t&&u<=t)&&r[i].push(o)})}),r},DataProcess.filterYear=function(e,t){e=parseInt(e,null);var n=(new Date(e.toString())).getTime(),r=(new Date((e+1).toString())).getTime(),i=filterDates(n,r,t);return i},DataProcess.fillDates=function(e,t){if(e[0].length===0)return t;var n=[[],[]],r=0;if(e[0][0]>t[0][0])for(r=0;r<t[0].length;r++)n[0][r]=t[0][r],n[1][r]=t[1][r];for(r=0;r<e[0].length;r++)pos=n[0].indexOf(e[0][r]),pos===-1&&(n[0].push(e[0][r]),n[1].push(e[1][r]));if(e[0][e[0].length-1]<t[0][t[0].length-1])for(r=0;r<t[0].length;r++)pos=n[0].indexOf(t[0][r]),pos===-1&&(n[0].push(t[0][r]),n[1].push(t[1][r]));return n},DataProcess.fillHistory=function(e,t){var n=[[],[]];for(var r=0;r<e.length;r++)pos=t[0].indexOf(e[r]),n[0][r]=e[r],pos!=-1?n[1][r]=t[1][pos]:n[1][r]=0;return n},DataProcess.fillHistoryLines=function(e,t){var n=[[],[]],r=[[],[]],i=[];for(var s=0;s<t.length;s++)n[0].push(t[s][0]),n[1].push(t[s][1]);r=DataProcess.fillHistory(e,n);for(s=0;s<e.length;s++)i.push([r[0][s],r[1][s]]);return i},DataProcess.addRelativeValues=function(e,t){if(e[t]===undefined)return;e[t+""_relative""]=[];var n=[];$.each(e[t],function(e,t){var r=t.data[1];for(var i=0;i<r.length;i++)n[i]===undefined&&(n[i]=0),n[i]+=r[i]}),$.each(e[t],function(r,i){var s=[];for(var o=0;o<i.data[0].length;o++)if(n[o]===0)s[o]=0;else{var u=i.data[1][o]/n[o]*100;s[o]=u}e[t+""_relative""].push({label:i.label,data:[i.data[0],s]})})},DataProcess.aggregate=function(e,t){var n={};return t instanceof Array||(t=[t]),$.each(e,function(r,i){if($.inArray(r,t)>-1){var s=[];s[0]=e[r][0];for(var o=1;o<e[r].length;o++)s[o]=s[o-1]+e[r][o];n[r]=s}else n[r]=e[r]}),n},DataProcess.substract=function(e,t,n){var r={},i=[];for(var s=0;s<e[t].length;s++)i[s]=e[t][s]-e[n][s];return $.each(e,function(t,n){r[t]=e[t]}),r.substract=i,r},DataProcess.divide=function(e,t,n){var r={},i=[];for(var s=0;s<e[t].length;s++)e[t][s]===0||e[n][s]===0?i[s]=0:i[s]=parseInt(e[t][s]/e[n][s],null);return $.each(e,function(t,n){r[t]=e[t]}),r.divide=i,r},DataProcess.revomeLastPoint=function(e){var t={};return $.each(e,function(n,r){t[n]=[];for(var i=0;i<e[n].length-1;i++)t[n].push(e[n][i])}),t}})();var Utils={};(function(){function e(){return $.urlParam(""release"")===null?!1:!0}function t(){return params="""",document.URL.split(""?"").length>1&&(params=document.URL.split(""?"")[1]),params}function n(){return aux=document.URL.split(""?"")[0].split(""/""),res=aux[aux.length-1],res}function r(e){return url=e,t().length>0&&(url+=""?""+t()),url}function i(t){return url=t,e()&&(url+=""?release=""+$.urlParam(""release"")),url}Utils.paramsInURL=t,Utils.isReleasePage=e,Utils.filenameInURL=n,Utils.createLink=r,Utils.createReleaseLink=i,$.urlParam=function(e){var t=(new RegExp(""[?&]""+e+""=([^&#]*)"")).exec(window.location.href);return t===null?null:t[1]||0}})();var HTMLComposer={};(function(){function e(e,t){var n='<div class=""col-md-12"">';return n+='<div class=""well well-small"">',n+='<div class=""row"">',n+='<div class=""col-md-12"">',n+=""<p>""+u(e)+""</p>"",n+=""</div>"",n+='<div class=""col-md-3"">',n+='<div class=""PersonSummary"" data-data-source=""'+e+'""></div>',n+=""</div>"",n+='<div class=""col-md-9"">',n+='<div class=""PersonMetrics"" data-data-source=""'+e+'""',n+='data-metrics=""'+t+'"" data-min=""true""',n+='data-frame-time=""true""></div>',n+=""</div>"",n+=""</div>"",n+=""</div>"",n+=""</div>"",n}function t(e,t,n){var r='<div class=""col-md-12"">';return r+='<div class=""row"">',r+='<div class=""col-md-3"">',r+='<div class=""well"">',r+='<div class=""FilterItemSummary"" data-data-source=""'+e+'"" data-filter=""'+t+'""></div>',r+=""</div></div>"",r+='<div class=""col-md-9"">',r+='<div class=""well"">',$.each(n,function(n,i){r+='<div class=""row""><div class=""col-md-12""></br></br></div></div>',r+='<div class=""row"">',r+='<div class=""col-md-12"">',r+='<div class=""FilterItemMetricsEvol"" data-data-source=""'+e+'""',r+='data-metrics=""'+i+'"" data-min=""true""',r+='data-filter=""'+t+'"" data-frame-time=""true""></div>',r+=""</div></div>""}),r+=""</div></div></div></div>"",r}function n(e,t,n){var r=""<table class='table-condensed table-hover'>"";r+='<tr><td colspan=""2""><p class=""subsection-title"">'+u(e.getName())+""</p></td></tr>"";var i=""<tr><td>"",s=""</td></tr>"";return $.each(t,function(t,o){e.getMetrics()[t]?(r+=i+e.getMetrics()[t].name,t===""first_date""||t===""last_date""?r+='</td><td class=""numberInTD"">'+o+s:r+='</td><td class=""numberInTD"">'+Report.formatValue(o)+s):n[t]&&(r+=i+n[t],t===""first_date""||t===""last_date""?r+='</td><td class=""numberInTD"">'+o+s:r+='</td><td class=""numberInTD"">'+Report.formatValue(o)+s)}),r+=""</table>"",r}function r(e,t){var n=""<table class='table-condensed table-hover'>"";return n+=""<tr><td>"",n+=""First contribution: </br>"",n+=""&nbsp;&nbsp;""+t.first_date,n+=""</td></tr><tr><td>"",n+=""Last contribution: </br>"",n+=""&nbsp;&nbsp;""+t.last_date,n+=""</td></tr><tr><td>"",e==""scm""?n+=""Commits:</br>&nbsp;&nbsp;""+t.scm_commits:e==""its""?n+=""Closed:</br>&nbsp;&nbsp;""+t.its_closed:e==""mls""?n+=""Sent:</br>&nbsp;&nbsp;""+t.mls_sent:e==""irc""?n+=""Sent:</br>&nbsp;&nbsp;""+t.irc_sent:e==""scr""&&(n+=""Closed:</br>&nbsp;&nbsp;""+t.scr_closed),n+=""</td></tr>"",n+=""</table>"",n}function s(e,t){var n='<p class=""section-title"" style=""margin-bottom:0px;""><i class=""fa fa-user fa-lg""></i> &nbsp;&nbsp;';return e.length>0?n+=e:t.length>0&&(t.indexOf(""@"")>0&&(t=t.split(""@"")[0]),n+=t),n+=""</p>"",n}function o(e,t){var n='<p class=""section-title"" style=""margin-bottom:0px;"">';return t===""companies""&&(n+='<i class=""fa fa-building-o""></i> &nbsp;&nbsp;'),n+=e,n+=""</p>"",n}function u(e){var t="""";return e===""scm""?t='<i class=""fa fa-code""></i> Source Code Management':e===""scr""?t='<i class=""fa fa-check""></i> Source Code Review':e===""its""?t='<i class=""fa fa-ticket""></i> Issue tracking system':e===""mls""?t='<i class=""fa fa-envelope-o""></i> Mailing Lists':e===""irc""?t='<i class=""fa fa-comment-o""></i> IRC Channels':e===""mediawiki""?t='<i class=""fa fa-pencil-square-o""></i> Wiki':e===""releases""&&(t='<i class=""fa fa-umbrella""></i> Releases'),t}function a(){return html="""",params=""?data_dir=""+$.urlParam(""data_dir"")+""&release=""+$.urlParam(""release""),html+='<li><a href=""./""><i class=""fa fa-home""></i> Home</a></li>',html+='<li><a href=""./scm-companies.html'+params+'""><i class=""fa fa-code""></i> Source code repositories by companies</a></li>',html+='<li><a href=""./mls-companies.html'+params+'""><i class=""fa fa-envelope-o""></i> Mailing Lists by companies</a></li>',html+='<li><a href=""./its-companies.html'+params+'""><i class=""fa fa-ticket""></i> Tickets by companies</a></li>',html}function f(e,t){return t.length===0?"""":(unsupported=[""irc.html"",""qaforums.html"",""project.html""],ah_label=""&nbsp;All history&nbsp;"",label=e,label===null?label=ah_label:(label=""&nbsp;Release ""+label[0].toUpperCase()+label.substring(1)+""&nbsp;"",t.reverse().push(ah_label),t.reverse()),html='<div class=""input-group-btn"">',html+='<button type=""button"" class=""btn btn-default btn-lg btn-releaseselector dropdown-toggle""',html+='data-toggle=""dropdown"">',html+=label,html+='<span class=""caret""></span>',html+=""</button>"",html+='<ul class=""dropdown-menu pull-left"">',page_name=Utils.filenameInURL(),unsupported.indexOf(page_name)<0?$.each(t,function(e,t){var n=[];params=Utils.paramsInURL().split (""&"");for(i=0;i<params.length;i++){sub_value=params[i];if(sub_value.length===0)continue;sub_value.indexOf(""release"")===0?t!=ah_label&&n.push(""release=""+t):n.push(sub_value)}$.urlParam(""release"")===null&&n.push(""release=""+t),t===ah_label?html+='<li><a href=""'+page_name+""?""+n.join(""&"")+'"" data-value=""'+t+'""> '+t+""</a></li>"":html+='<li><a href=""'+page_name+""?""+n.join(""&"")+'"" data-value=""'+t+'""> Release '+t+""</a></li>""}):html+=""<li><i>No releases for this section</i></li>"",html+=""</ul>"",html+=""</div>"",html)}function l(e,t,n,r){return html="""",html+=""<!-- irc -->"",html+='<div class=""row invisible-box"">',blabels=t.split("",""),bmetrics=n.split("",""),html+=h(e,blabels,bmetrics),html+='<div class=""col-md-5"">',tsm=r.split("",""),html+=p(e,tsm[0]),html+=""</div>"",html+='<div class=""col-md-5"">',html+=p(e,tsm[1]),html+=""</div>"",html+=""</div>"",html+=""<!-- end irc -->"",html}function c(e,t,n,r){return html="""",html+='<div class=""col-md-'+e+'"">',html+='<div class=""row thin-border"">',html+='<div class=""col-md-12"">'+t+""</div>"",html+=""</div>"",html+='<div class=""row"">',html+='<div class=""col-md-12 medium-fp-number"">',target_page=Utils.createLink(n+"".html""),html+='<a href=""'+target_page+'""> <span class=""GlobalData""',html+='data-data-source=""'+n+'"" data-field=""'+r+'""></span>',html+=""</a>"",html+=""</div>"",html+=""</div>"",html+=""</div>"",html}function h(e,t,n){return html="""",html+=""<!-- summary box-->"",html+='<div class=""col-md-2"">',html+='<div class=""well well-small"">',html+='<div class=""row thin-border"">',html+='<div class=""col-md-12"">'+t[0]+""</div>"",html+=""</div>"",html+='<div class=""row grey-border"">',html+='<div class=""col-md-12 big-fp-number"">',target_page=Utils.createLink(e+"".html""),html+='<a href=""'+target_page+'""> <span class=""GlobalData""',html+='data-data-source=""'+e+'"" data-field=""'+n[0]+'""></span>',html+=""</a>"",html+=""</div>"",html+=""</div>"",html+='<div class=""row"" style=""padding: 5px 0px 0px 0px;"">',t.length===2&&n.length===2?html+=c(""12"",t[1],e,n[1]):t.length===3&&n.length===3?(html+=c(""6"",t[1],e,n[1]),html+=c(""6"",t[2],e,n[2])):t.length===4&&n.length===4&&(html+=c(""4"",t[1],e,n[1]),html+=c(""4"",t[2],e,n[2]),html+=c(""4"",t[3],e,n[3])),html+=""</div>"",html+=""</div>"",html+=""</div>"",html+=""<!-- end summary box -->"",html}function p(e,t){return html="""",html+='<div class=""well well-small"">',html+='<div class=""MetricsEvol"" data-data-source=""'+e+'""',html+='data-metrics=""'+t+'"" data-min=""true"" style=""height: 100px;""',html+='data-light-style=""true""></div>',html+='<a href=""irc.html"" style=""color: black;"">',html+=' <span class=""MicrodashText"" data-metric=""'+t+'""></span>',html+=""</a>"",html+=""</div>"",html}function d(e,t,n,r){text={companies:""Companies"",""companies-summary"":""Companies summary"",contributors:""Contributors"",countries:""Countries"",domains:""Domains"",projects:""Projects"",repos:""Repositories"",tags:""Tags"",states:""States""},html="""",html+='<li class=""dropdown"">',html+='<a href=""#"" class=""dropdown-toggle"" data-toggle=""dropdown"">',html+='<i class=""fa '+e+'""></i>&nbsp;'+t+' <b class=""caret""></b></a>',html+='<ul class=""dropdown-menu navmenu-nav"">';var i=Utils.createLink(n+"".html"");return html+='<li><a href=""'+i+'"">&nbsp;Overview</a></li>',$.each(r,function(e,t){i=Utils.createLink(n+""-""+t+"".html"");if(text.hasOwnProperty(t)){var r=text[t];if(t===""repos""){var s=Report.getDataSourceByName(n);r=s.getLabelForRepositories(),r=r.charAt(0).toUpperCase()+r.slice(1)}html+='<li><a href=""'+i+'"">&nbsp;'+r+""</a></li>""}else html+='<li><a href=""'+i+'"">&nbsp;'+t+""</a></li>""}),html+=""</ul></li>"",html}function v(){return html="""",html+=""<!-- summary bar -->"",html+='<div class=""capped-box overall-summary "">',html+='<div class=""stats-switcher-viewport js-stats-switcher-viewport"">',html+='<ul class=""numbers-summary"">',html+='<li><a href=""'+Utils.createReleaseLink(""scm.html"")+'""><span class=""GlobalData"" ',html+='data-data-source=""scm"" data-field=""scm_commits""></span></a> commits</li>',html+='<li><a href=""'+Utils.createReleaseLink(""scm.html"")+'""><span class=""GlobalData"" ',html+='data-data-source=""scm"" data-field=""scm_authors""></span></a> developers ',html+=""</li>"",html+='<li><a href=""'+Utils.createReleaseLink(""its.html"")+'""><span class=""GlobalData"" ',html+='data-data-source=""its"" data-field=""its_opened""></span></a> tickets</li>',html+='<li><a href=""'+Utils.createReleaseLink(""mls.html"")+'""><span class=""GlobalData"" ',html+='data-data-source=""mls"" data-field=""mls_sent""></span></a> mail messages ',html+=""</li>"",html+=""</ul>"",html+=""</div>"",html+=""</div>"",html+=""<!-- end of summary bar -->"",html}function m(e,t){html="""",link_exists=!1;try{fname=e.split(""."")[0],section=fname.split(""-"")[0],subsection=fname.split(""-"")[1];var n=Report.getMenuElements();n[section].indexOf(subsection)>=0&&(link_exists=!0),Utils.isReleasePage()&&link_exists?(link_to=Utils.createReleaseLink(e),html='<a href=""'+link_to+'"">'+t+""</a>""):link_exists?html='<a href=""'+e+'"">'+t+""</a>"":html=t}catch(r){html=t}return html}HTMLComposer.personDSBlock=e,HTMLComposer.filterDSBlock=t,HTMLComposer.DSBlock=l,HTMLComposer.repositorySummaryTable=n,HTMLComposer.personSummaryTable=r,HTMLComposer.personName=s,HTMLComposer.itemName=o,HTMLComposer.sideMenu4Release=a,HTMLComposer.releaseSelector=f,HTMLComposer.sideBarLinks=d,HTMLComposer.overallSummaryBlock=v,HTMLComposer.smartLinks=m})();var Convert={};(function(){function e(e,t){return t.hasOwnProperty(e)&&t[e].title?t[e].title:undefined}function t(e,t){return e.project_id<t.project_id?-1:e.project_id>t.project_id?1:0}function n(e,t){var n=[],r=e,i="""",s={};while(t[r].hasOwnProperty(""parent_project""))i=t[r].parent_project,s=t[i],s.project_id=i,n.push(s),r=i;return n.reverse()}function r(e,n){var r=[],i={};return $.each(n,function(t,s){n[t].parent_project===e&&(i=n[t],i.project_id=t,r.push(i))}),r.sort(t),r}function i(t,n,i){var s="""",o=n.length;return o>0?(s+='<li class=""dropdown"">',s+='<span data-toggle=""tooltip"" title=""Project name""> '+e(t,i)+""</span>"",s+='&nbsp;<a class=""dropdown-toggle"" data-toggle=""dropdown"" href=""#"">',s+='<span data-toggle=""tooltip"" title=""Select subproject"" class=""badge""> '+o+"" Subprojects </span></a>"",s+='<ul class=""dropdown-menu scrollable-menu"">',$.each(n,function(e,t){gchildren=r(t.project_id,i),gchildren.length>0?s+='<li><a href=""project.html?project='+t.project_id+'"">'+t.title+'&nbsp;&nbsp;<span data-toggle=""tooltip"" title=""Number of suprojects"" class=""badge"">'+gchildren.length+'&nbsp;<i class=""fa fa-rocket""></i></span></a></li>':s+='<li><a href=""project.html?project='+t.project_id+'"">'+t.title+""</a></li>""}),s+='<li class=""divider""></li>',s+='<li><a href=""./project_map.html""><i class=""fa fa-icon fa-sitemap""></i> Projects treemap</a></li>',s+=""</ul></li>""):s+=""<li>""+e(t,i)+""</li>"",s}function s(e){var t='<ol class=""breadcrumbtitle"">',s=Report.getProjectsHierarchy();if(s.length===0)return"""";e===undefined&&(e=""root"");var o=r(e,s),u=n(e,s);return u.length>0&&$.each(u,function(e,n){n.parent_project?t+='<li><a href=""project.html?project='+n.project_id+'"">'+n.title+""</a></li>"":t+='<li><a href=""./"">'+n.title+""</a></li>""}),t+=i(e,o,s),t+=""</ol>"",t}function o(e){var t="""";return t=e.replace("" "",""_""),t=t.toLowerCase(),t}function u(t,n,i){var s="""",a=n.length,f=o(t);return a>0?(s+=""<li>"",s+='<a href=""project.html?project='+f+'"">'+e(t,i)+""</a>"",s+='&nbsp;<a data-toggle=""collapse"" data-parent=""#accordion"" href=""#collapse'+f+'""><span class=""badge"">'+a+'&nbsp;subprojects</span></a><div id=""collapse'+f+'"" class=""panel-collapse collapse""><ul>',$.each(n,function(e,t){gchildren=r(t.project_id,i),s+=u(t.project_id,gchildren,i)}),s+=""</ul></li>""):s+='<li><a href=""project.html?project='+t+'"">'+e(t,i)+""</a></li>"",s}function a(){var e=""<ul>"",t=Report.getProjectsHierarchy();if(t.length===0)return"""";project_id=""root"";var i=r(project_id,t),s=n(project_id,t);return $.each(i,function(n,i){grandchildren=r(i.project_id,t),e+=u(i.project_id,grandchildren,t)}),e+=""</ul>"",e}function f(){var e=[],t={data_sources:""Data sources"",project_map:""Project map"",people:""Contributor"",company:""Company"",country:""Country"",domain:""Domain"",""scm-companies"":""Activity on code repositories by companies"",""mls-companies"":""Activity on mailing lists by companies"",""its-companies"":""Activity on issue trackers by companies""};url_no_params=document.URL.split(""?"")[0],url_tokens=url_no_params.split(""/"");var n=url_tokens[url_tokens.length-1].split(""."")[0];return n===""project""||n===""index""||n===""release""||n===""""?[]:t.hasOwnProperty(n)?(e.push([n,t[n]]),e):[[""#"",""Unavailable section name""]]}function l(){var e=[],t={mls:""MLS overview"",irc:""IRC overview"",its:""ITS overview"",qaforums:""QA Forums overview"",scr:""Code Review overview"",scm:""SCM overview"",wiki:""Wiki overview"",data_sources:""Data sources"",project_map:""Project map"",people:""Contributor"",company:""Company"",country:""Country"",domain:""Domain"",release:""Companies analysis by release""},n={companies:""Activity by companies"",contributors:""Activity by contributors"",countries:""Activity by companies"",domains:""Activity by domains"",projects:""Activity by project"",repos:""Activity by repositories"",states:""Activity by states"",tags:""Activity by tags""},r={repository:""Repository""};url_no_params=document.URL.split(""?"")[0],url_tokens=url_no_params.split(""/"");var i=url_tokens[url_tokens.length-1].split(""."")[0];if(i===""project""||i===""index""||i==="""")return[];var s=i.split(""-"");return s[0]===""repository""&&(ds_name=$.urlParam(""ds""),s=[ds_name,""repos"",""repository""]),t.hasOwnProperty(s[0])?(e.push([s[0],t[s[0]]]),s.length>0&&n.hasOwnProperty(s[1])&&(e.push([s[0]+""-""+s[1],n[s[1]]]),s.length>2&&r.hasOwnProperty(s[2])&&e.push([s[0],r[s[2]]])),e):[[""#"",""Unavailable section name""]]}function c(){return $.urlParam(""release"")!==null&&$.urlParam(""release"").length>0?!0:!1}function h(e){e===undefined&&(e=""root"");var t="""",n="""";t+='<ul class=""nav navmenu-nav"">';var r=Report.getMenuElements();return e===""root""&&(r.hasOwnProperty(""scm"")&&(aux=r.scm,aux_html=HTMLComposer.sideBarLinks(""fa-code"",""Source code management"",""scm"",aux),t+=aux_html),r.hasOwnProperty(""scr"")&&(aux=r.scr,aux_html=HTMLComposer.sideBarLinks(""fa-check"",""Code review"",""scr"",aux),t+=aux_html),r.hasOwnProperty(""its"")&&(aux=r.its,aux_html=HTMLComposer.sideBarLinks(""fa-ticket"",""Tickets"",""its"",aux),t+=aux_html),r.hasOwnProperty(""mls"")&&(aux=r.mls,aux_html=HTMLComposer.sideBarLinks(""fa-envelope-o"",""Mailing lists"",""mls"",aux),t+=aux_html),r.hasOwnProperty(""qaforums"")&&Utils.isReleasePage()===!1&&(aux=r.qaforums,aux_html=HTMLComposer.sideBarLinks(""fa-question"",""Q&A Forums"",""qaforums"",aux),t+=aux_html),r.hasOwnProperty(""irc"")&&Utils.isReleasePage()===!1&&(aux=r.irc,aux_html=HTMLComposer.sideBarLinks(""fa-comment-o"",""IRC"",""irc"",aux),t+=aux_html),r.hasOwnProperty(""downloads"")&&Utils.isReleasePage()===!1&&(aux=r.downloads,aux_html=HTMLComposer.sideBarLinks(""fa-download"",""Downloads"",""downloads"",aux),t+=aux_html),r.hasOwnProperty(""wiki"")&&Utils.isReleasePage()===!1&&(aux=r.wiki,aux_html=HTMLComposer.sideBarLinks(""fa-pencil-square-o"",""Wiki"",""wiki"",aux),t+=aux_html),r.hasOwnProperty(""studies"")&&Utils.isReleasePage()===!1&&(aux=r.studies,t+='<li class=""dropdown"">',t+='<a href=""#"" class=""dropdown-toggle"" data-toggle=""dropdown"">',t+='<i class=""fa fa-lightbulb-o""></i>&nbsp;Studies <b class=""caret""></b></a>',t+='<ul class=""dropdown-menu navmenu-nav"">',aux.indexOf(""demographics"")>=0&&(t+='<li><a href=""demographics.html"">&nbsp;Demographics</a></li>'),aux.indexOf(""release"")>=0&&(aux=Report.getReleaseNames().reverse(),latest_release=aux[0],t+='<li><a href=""release.html?release='+latest_release+'"">&nbsp;Companies by release</a></li>'),t+=""</ul></li>""),t+='<li><a href=""data_sources.html""><i class=""fa fa-database""></i> Data sources</a></li>',t+='<li><a href=""project_map.html""><i class=""fa fa-icon fa-sitemap""></i> Project map</a></li>',r.hasOwnProperty(""extra"")&&(aux=r.extra,n+='<li class=""sidemenu-divider""></li>',n+='<li class=""sidemenu-smallheader"">More links:</li>',$.each(aux,function(e,t){n+='<li><a href=""'+t[1]+'"">&nbsp;'+t[0]+""</a></li>""})),t+=n),t+=""</ul>"",t}function p(e){var t='<ol class=""breadcrumb"">';if(e===undefined){var n=l(),r=Utils.paramsInURL();if(n.length>0){t+='<li><a href=""./',Utils.isReleasePage()&&(t+=""?release=""+$.urlParam(""release"")),t+='"">Project Overview</a></li>';var i=1;$.each(n,function(e,r){n.length===i?t+='<li class=""active"">'+r[1]+""</li>"":Utils.isReleasePage()?(t+='<li><a href=""'+r[0]+"".html"",t+=""?release=""+$.urlParam(""release"")+'"">',t+=r[1]+""</a></li>""):t+='<li><a href=""'+r[0]+'.html"">'+r[1]+""</a></li>"",i+=1})}else t+='<li class=""active"">Project Overview</li>'}else t+=""<li> ""+l()+""</li>"";return t+=""</ol>"",t}function d(e){var t=Report.getParameterByName(""repository"");if(t&&$.inArray(t,e.getReposData())<0)return"""";var n=e.getName(),r="""",i=e.getLabelForRepository(),s=e.getLabelForRepositories();return t!==undefined?r=t:r=""All ""+s,html='<div class=""row""><span class=""col-md-12"">',html='<ol class=""filterbar""><li>Filtered by '+i+"":&nbsp;&nbsp;</li>"",html+='<li><div class=""dropdown""><button class=""btn btn-default dropdown-toggle"" type=""button"" id=""dropdownMenu1"" data-toggle=""dropdown""> '+r+' <span class=""caret""></span></button>',html+='<ul class=""dropdown-menu"" role=""menu"" aria-labelledby=""dropdownMenu1"">',t&&(html+='<li role=""presentation""><a role=""menuitem"" tabindex=""-1"" href=""'+n+'-contributors.html"">',html+=""All ""+s,html+=""</a></li>""),$.each(e.getReposData(),function(e,n){if(n===t)return;html+='<li role=""presentation""><a role=""menuitem"" tabindex=""-1"" href=""?repository=',html+=n,html+='"">',html+=n,html+=""</a></li>""}),html+=""</ul></div></li></ol>"",html+=""</span></div>"",html}function v(){data=Report.getProjectData(),document.title=data.project_name+"" Report by Bitergia"",data.title&&(document.title=data.title),$("".report_date"").text(data.date),$("".report_name"").text(data.project_name),str=data.blog_url,str&&str.length>0?($(""#blogEntry"").html(""<br><a href='""+str+""'>Blog post with some more details</a>""),$("".blog_url"").attr(""href"",data.blog_url)):$(""#more_info"").hide(),str=data.producer,str&&str.length>0?$(""#producer"").html(str):$(""#producer"").html(""<a href='http://bitergia.com'>Bitergia</a>""),$("".project_name"").text(data.project_name),$(""#project_url"").attr(""href"",data.project_url)}function m(e,t){t.help=!0;var n=$(e).data(""help"");n!==undefined&&(t.help=n),t.show_legend=!1,$(e).data(""frame-time"")&&(t.frame_time=!0),t.graph=$(e).data(""graph""),$(e).data(""min"")&&(t.show_legend=!1,t.show_labels=!0,t.show_grid=!0,t.help=!1),$(e).data(""legend"")&&(t.show_legend=!0),t.ligth_style=!1,$(e).data(""light-style"")&&(t.light_style=!0),$(e).data(""custom-title"")&&(t.custom_title=$(e).data(""custom-title"")),t.help&&$(e).data(""custom-help"")?t.custom_help=$(e).data(""custom-help""):t.custom_help="""",$(e).data(""repo-filter"")&&(t.repo_filter=$(e).data(""repo-filter""));var r=$(e).data(""start"");r&&(t.start_time=r);var i=$(e).data(""end"");i&&(t.end_time=i);var s=$(e).data(""remove-last-point"");return s&&(t.remove_last_point=!0),t}function g(){return Math.floor(Math.random()*1e3+1)}function y(e,t){if(e===""repos""){if(DS.getReposGlobalData()[t]===undefined||DS.getReposGlobalData()[t].length===0)return!1}else if(e===""companies""){if(DS.getCompaniesGlobalData()[t]===undefined||DS.getCompaniesGlobalData()[t].length===0)return!1}else if(e===""countries""){if(DS.getCountriesGlobalData()[t]===undefined||DS.getCountriesGlobalData()[t].length===0)return!1}else if(e===""companies"")if(DS.getDomainsGlobalData()[t]===undefined||DS.getDomainsGlobalData()[t].length===0)return!1;return!0}function b(){var e={};return e.show_desc=!1,e.show_title=!1,e.show_labels=!0,e.show_legend=!1,e}Convert.convertMicrodashText=function(){var e=$("".MicrodashText"");e.length>0&&$.each(e,function(e,t){$(this).empty();var n=$(this).data(""metric""),r=$(this).data(""name""),i=Report.getMetricDS(n)[0];if(i===undefined)return;var s=i.getGlobalData()[n],o='<div class=""row"">';r&&(o+='<div class=""col-md-3"">',o+='<span class=""dayschange"">'+i.basic_metrics[n].name+""</span>"",o+=""</div>""),$.each([365,30,7],function(e,t){var s=i.getMetrics()[n].column,u=i.getGlobalData()[n+""_""+t],a=i.getGlobalData()[""diff_net""+s+""_""+t],f=i.getGlobalData()[""percentage_""+s+""_""+t];f=Math.round(f*10)/10;if(u===undefined)return;var l="""";f===0?l=Math.abs(f):a>0?l=""+""+f:a<0&&(l=""-""+Math.abs(f)),r?o+='<div class=""col-md-3"">':o+='<div class=""col-md-4"">',o+='<span class=""dayschange"">Last '+t+"" days:</span>"",o+="" ""+Report.formatValue(u)+""<br>"",f===0?o+='<i class=""fa fa-arrow-circle-right""></i> <span class=""zeropercent"">&nbsp;'+l+""%</span>&nbsp;"":a>0?o+='<i class=""fa fa-arrow-circle-up""></i> <span class=""pospercent"">&nbsp;'+l+""%</span>&nbsp;"":a<0&&(o+='<i class=""fa fa-arrow-circle-down""></i> <span class=""negpercent"">&nbsp;'+l+""%</span>&nbsp;""),o+=""</div><!--col-md-4-->""}),o+=""</div><!--row-->"",$(t).append(o)})},Convert.convertMicrodash=function(){var e=$("".Microdash"");e.length>0&&$.each(e,function(e,t){$(this).empty();var n=$(this).data(""metric""),r=$(this).data(""text""),i=Report.getMetricDS(n)[0],s=i.getGlobalData()[n],o=""<div>"";o+='<div style=""float:left"">',o+='<span class=""medium-fp-number"">'+Report.formatValue(s),o+=""</span> ""+i.getMetrics()[n].name,o+=""</div>"",o+='<div id=""Microdash"" class=""MetricsEvol"" data-data-source=""'+i.getName()+'"" data-metrics=""'+n+'"" data-min=true style=""margin-left:10px; float:left;width:100px; height:25px;""></div>',o+='<div style=""clear:both""></div><div>',$.each([365,30,7],function(e,t){var r=i.getMetrics()[n].column,s=i.getGlobalData()[""diff_net""+r+""_""+t],u=i.getGlobalData()[""percentage_""+r+""_""+t],a=i.getGlobalData()[n+""_""+t];if(a===undefined)return;o+=""<span class='dayschange'>""+t+"" Days Change</span>:""+Report.formatValue(a)+""&nbsp;"",s===0?o+="""":s>0?(o+='<i class=""icon-circle-arrow-up""></i>',o+=""<small>(+""+u+""%)</small>&nbsp;""):s<0&&(o+='<i class=""icon-circle-arrow-down""></i>',o+=""<small>(-""+Math.abs(u)+""%)</small>&nbsp;"")}),o+=""</div>"",o+=""<div>"",$(t).append(o)})},Convert.convertSideBar=function(e){var t=$("".SideNavBar"");t.length>0&&$.each(t,function(t,n){$(this).empty(),n.id||(n.id=""SideNavBar"");var r;e&&(r=Report.cleanLabel(e));var i=h(r);$(""#""+n.id).append(i),data=Report.getProjectData(),$("".report_name"").text(data.project_name)})},Convert.convertProjectNavBar=function(e){var t=$("".ProjectNavBar"");t.length>0&&$.each(t,function(t,n){$(this).empty(),n.id||(n.id=""ProjectNavBar"");var r;e&&(r=Report.cleanLabel(e));var i=s(r);$(""#""+n.id).append(i)})},Convert.convertNavbar=function(){$.get(Report.getHtmlDir()+""navbar.html"",function(e){$(""#Navbar"").html(e);var t=Report.getParameterByName(""project"");Convert.convertProjectNavBar(t),Convert.convertReleaseSelector(),Convert.convertSideBar(t)})},Convert.convertReleaseSelector=function(){var e=Report.getReleaseNames();if(e.length>0){var t=$("".ReleaseSelector"");t.length>0&&$.each(t,function(t,n){$(this).empty(),n.id||(n.id=""ReleaseSelector""+g());var r=HTMLComposer.releaseSelector($.urlParam(""release""),e);$(""#""+n.id).append(r)})}},Convert.convertSectionBreadcrumb=function(e){var t=$("".SectionBreadcrumb"");t.length>0&&$.each(t,function(t,n){$(this).empty(),n.id||(n.id=""SectionBreadcrumb"");var r;e&&(r=Report.cleanLabel(e));var i=p(r);$(""#""+n.id).append(i)})},Convert.convertProjectMap=function(){var e=$("".ProjectMap"");e.length>0&&$.each(e,function(e,t){$(this).empty(),t.id||(t.id=""ProjectMap"");var n,r=a();$(""#""+t.id).append(r)})},Convert.convertFooter=function(){$.get(Report.getHtmlDir()+""footer.html"",function(e){$(""#Footer"").html(e),$(""#vizjs-lib-version"").append(vizjslib_git_tag)})},Convert.convertSummary=function(){div_param=""Summary"";var e=$("".""+div_param);e.length>0&&$.each(e,function(e,t){$(this).empty();var n=$(this).data(""data-source""),r=Report.getDataSourceByName(n);if(r===null)return;t.id=n+""-Summary"",r.displayGlobalSummary(t.id)})},Convert.convertRepositorySelector=function(){var e=$("".repository-selector"");e.length>0&&$.each(e,function(e,t){$(this).empty();var n=$(this).data(""data-source""),r=Report.getDataSourceByName(n);if(r===null)return;t.id=n+""-repository-selector"";var i=d(r);$(""#""+t.id).append(i)})},Convert.convertRefcard=function(){$.when($.get(Report.getHtmlDir()+""refcard.html""),$.get(Report.getHtmlDir()+""project-card.html"")).done(function(e,t){refcard=e[0],projcard=t[0],$(""#Refcard"").html(refcard),v(),$.each(Report.getProjectsData(),function(e,t){var n=""card-""+e.replace(""."","""").replace("" "","""");$(""#Refcard #projects_info"").append(projcard),$(""#Refcard #projects_info #new_card"").attr(""id"",n),$.each(Report.getDataSources(),function(t,r){if(r.getProject()!==e){$(""#""+n+"" .""+r.getName()+""-info"").hide();return}r.displayData(n)}),$(""#""+n+"" #project_name"").text(e),Report.getProjectsDirs.length>1&&$(""#""+n+"" .project_info"").append(' <a href=""VizGrimoireJS/browser/index.html?data_dir=../../'+t.dir+'"">Report</a>'),$(""#""+n+"" #project_url"").attr(""href"",t.url)})})},Convert.convertGlobalData=function(){var e=$("".GlobalData"");e.length>0&&$.each(e,function(e,t){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;var n=DS.getGlobalData(),r=$(this).data(""field"");$(this).text(Report.formatValue(n[r],r))})},Convert.convertProjectData=function(){var e=$("".ProjectData""),t=Report.getParameterByName(""project"");e.length>0&&$.each(e,function(e,n){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;var r=DS.getProjectsGlobalData()[t],i=$(this).data(""field"");$(this).text(Report.formatValue(r[i],i))})},Convert.convertRadarActivity=function(){var e=""RadarActivity"",t=$(""#""+e);t.length>0&&($.each(t,function(e,t){$(this).empty()}),Viz.displayRadarActivity(e))},Convert.convertRadarCommunity=function(){var e=""RadarCommunity"",t=$(""#""+e);t.length>0&&($.each(t,function(e,t){$(this).empty()}),Viz.displayRadarCommunity(""RadarCommunity""))},Convert.convertTreemap=function(){var e=""Treemap"",t=$(""#""+e);if(t.length>0){$.each(t,function(e,t){$(this).empty()});var n=$(""#Treemap"").data(""file"");$(""#Treemap"").empty(),Viz.displayTreeMap(""Treemap"",n)}},Convert.convertBubbles=function(){div_param=""Bubbles"";var e=$("".""+div_param);e.length>0&&$.each(e,function(e,t){$(this).empty();var n=$(this).data(""data-source""),r=Report.getDataSourceByName(n);if(r===null)return;if(r.getData().length===0)return;var i=$(this).data(""radius"");t.id=n+""-Bubbles"",r.displayBubbles(t.id,i)})},Convert.convertMetricsEvol=function(){var e={};e.show_desc=!1,e.show_title=!0,e.show_labels=!0;var t=Report.getVizConfig();t&&$.each(t,function(t,n){e[t]=n});var n=""MetricsEvol"",r=$("".""+n);r.length>0&&$.each(r,function(t,n){var r={};$.each(e,function(e,t){r[e]=t}),$(this).empty();var i=$(this).data(""metrics""),s=$(this).data(""data-source"");r.title=$(this).data(""title"");var o=Report.getDataSourceByName(s);if(o===null)return;r=m(n,r),n.id=i.replace(/,/g,""-"")+""-""+s+""-metrics-evol-""+this.id,n.id=n.id.replace(/\n|\s/g,""""),o.displayMetricsEvol(i.split("",""),n.id,r,$(this).data(""convert""))})},Convert.convertMetricsEvolCustomized=function(e){var t={};t.show_desc=!1,t.show_title=!0,t.show_labels=!0;var n=Report.getVizConfig();n&&$.each(n,function(e,n){t[e]=n});var r=""MetricsEvolCustomized"",i=$("".""+r);i.length>0&&$.each(i,function(n,r){if(e!==$(this).data(""filter""))return;var i={};$.each(t,function(e,t){i[e]=t}),$(this).empty();var s=$(this).data(""metrics""),o=$(this).data(""data-source"");i.title=$(this).data(""title"");var u=Report.getDataSourceByName(o);if(u===null)return;i=m(r,i),r.id=s.replace(/,/g,""-"")+""-""+o+""-metrics-evol-""+this.id,r.id=r.id.replace(/\n|\s/g,""""),u.displayMetricsEvol(s.split("",""),r.id,i,$(this).data(""convert""))})},Convert.convertMetricsEvolSelector=function(){var e={};e.show_desc=!1,e.show_title=!0,e.show_labels=!0;var t=Report.getVizConfig();t&&$.each(t,function(t,n){e[t]=n});var n=""MetricsEvol"",r=$("".""+n);r.length>0&&$.each(r,function(t,n){var r={};$.each(e,function(e,t){r[e]=t}),$(this).empty();var i=$(this).data(""metrics""),s=$(this).data(""data-source""),o=Report.getDataSourceByName(s);if(o===null)return;var u=Report.getParameterByName(""repository"");r=m(n,r),n.id=i.replace(/,/g,""-"")+""-""+s+""-metrics-evol-""+this.id,n.id=n.id.replace(/\n|\s/g,""""),o.displayMetricsEvol(i.split("",""),n.id,r,$(this).data(""convert""),u)})},Convert.convertMetricsEvolSet=function(){div_param=""MetricsEvolSet"";var e=$("".""+div_param);e.length>0&&$.each(e,function(e,t){$(this).empty();var n=$(this).data(""all""),r=$(this).data(""relative""),i=$(this).data(""summary-graph""),s=$(this).data(""legend-show"");t.id=o+""-MetricsEvolSet-""+this.id;if(n===!0)return t.id=o+""-All"",Viz.displayEnvisionAll(t.id,r,s,i),!1;var o=$(this).data(""data-source""),u=Report.getDataSourceByName(o);if(u===null)return;u.displayEnvision(t.id,r,s,i)})},Convert.convertTimeTo=function(){var e=""TimeTo"";divs=$("".""+e),divs.length>0&&$.each(divs,function(e,t){$(this).empty();var n=$(this).data(""data-source""),r=Report.getDataSourceByName(n);if(r===null)return;var i=$(this).data(""quantil""),s=$(this).data(""type"");t.id=n+""-time-to-""+s+""-""+i,s===""fix""&&r.displayTimeToFix(t.id,i),s===""attention""&&r.displayTimeToAttention(t.id,i)})},Convert.convertMarkovTable=function(){var e=""MarkovTable"",t=$("".""+e),n,r;t.length>0&&$.each(t,function(e,t){$(this).empty(),r=$(this).data(""data-source""),n=Report.getDataSourceByName(r);if(n===null)return;if(n.getData().length===0)return;var i=$(this).data(""title"");t.id=r+""-markov-table"",n.displayMarkovTable(t.id,i)})},Convert.convertLastActivity=function(){function t(t,n,r){var i=""<h4>Last ""+r+""</h4>"";$.each(Report.getDataSources(),function(t,s){var o=s.getGlobalData();$.each(o,function(t,s){var u=""_""+n;if(t.indexOf(u,t.length-u.length)!==-1){var a=t.substring(0,t.length-u.length);r=a,e[a]&&(r=e[a].name),i+=r+"":""+o[t]+""<br>""}})}),$(t).append(i)}var e=Report.getAllMetrics(),n=$("".LastActivity""),r=null,i={Week:7,Month:30,Quarter:90,Year:365};n.length>0&&$.each(n,function(e,n){r=$(n).data(""period""),t(n,i[r],r)})},Convert.convertTop=function(){var e=""Top"",t=$("".""+e),n,r;if(t.length>0){var i=0;$.each(t,function(t,s){$(this).empty(),r=$(this).data(""data-source""),n=Report.getDataSourceByName(r);if(n===null)return;if(n.getData().length===0)return;var o=!1;$(this).data(""show_all"")&&(o=!0);var u=$(this).data(""metric""),a=$(this).data(""limit""),f=$(this).data(""graph""),l=$(this).data(""people_links""),c=$(this).data(""threads_links""),h=$(this).data(""period""),p=$(this).data(""period_all""),d=Report.getParameterByName(""repository"");s.id=r+""-""+e+i++,f&&(s.id+=""-""+f),h===undefined&&p===undefined&&(p=!0),a===undefined&&(a=10),n.displayTop(s.id,o,u,h,p,f,a,l,c,d)})}},Convert.convertPersonMetrics=function(e,t){var n={};n.show_desc=!1,n.show_title=!1,n.show_labels=!0,divs=$("".PersonMetrics""),divs.length&&$.each(divs,function(r,i){$(this).empty(),ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;var s=$(this).data(""metrics"");n.show_legend=!1,n.help=!1,$(this).data(""frame-time"")&&(n.frame_time=!0),$(this).data(""legend"")&&(n.show_legend=!0),$(this).data(""person_id"")&&(e=$(this).data(""person_id"")),$(this).data(""person_name"")&&(t=$(this).data(""person_name"")),i.id=s.replace(/,/g,""-"")+""-people-metrics"",DS.displayMetricsPeople(e,t,s.split("",""),i.id,n)})},Convert.convertPersonData=function(e,t){var n=$("".PersonData""),r,i;n.length>0&&$.each(n,function(n,s){$(this).empty(),$(this).data(""person_id"")&&(e=$(this).data(""person_id"")),s.id||(s.id=""PersonData-""+e+""-""+g());var o=Report.getPeopleIdentities()[e];o?(r=DataProcess.selectPersonName(o),i=DataProcess.selectPersonEmail(o),i=""(""+DataProcess.hideEmail(i)+"")""):(t!==undefined?r=t:r=e,i=""""),html=HTMLComposer.personName(r,i),$(""#""+s.id).append(html)})},Convert.personSummaryBlock=function(e){var t=$("".PersonSummaryBlock"");t.length>0&&$.each(t,function(t,n){if(n.id.indexOf(""Parsed"")>=0)return;ds_name=$(this).data(""data-source""),metric_name=$(this).data(""metrics""),DS=Report.getDataSourceByName(ds_name);if(DS===null)return;if(DS.getData().length===0)return;if(DS.getPeopleMetricsData()[e].length===0)return;var r=HTMLComposer.personDSBlock(ds_name,metric_name);n.id||(n.id=""Parsed""+g()),$(""#""+n.id).append(r)})},Convert.convertPersonSummary=function(e,t){var n=$("".PersonSummary"");n.length>0&&$.each(n,function(n,r){$(this).empty(),ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;$(this).data(""person_id"")&&(e=$(this).data(""person_id"")),$(this).data(""person_name"")&&(t=$(this).data(""person_name"")),r.id=ds+""-refcard-people"",DS.displayPeopleSummary(r.id,e,t,DS)})},Convert.convertPeople=function(e,t){e===undefined&&(e=Report.getParameterByName(""id"")),t===undefined&&(t=Report.getParameterByName(""name""));if(e===undefined)return;if(Loader.check_people_item(e)===!1){$.each(Report.getDataSources(),function(t,n){Loader.data_load_people_item(e,n,Convert.convertPeople)});return}Convert.personSummaryBlock(e),Convert.convertPersonData(e,t),Convert.convertPersonSummary(e,t),Convert.convertPersonMetrics(e,t),Convert.activateHelp()},Convert.repositoryDSBlock=function(e){var t=$("".FilterDSBlock"");t.length>0&&$.each(t,function(t,n){if(n.id.indexOf(""Parsed"")>=0)return;ds_name=$(this).data(""data-source""),filter_name=$(this).data(""filter""),aux=$(this).data(""metrics""),metric_names=aux.split("",""),$.each(metric_names,function(e,t){metric_names[e]=metric_names[e].replace(/:/g,"","")}),DS=Report.getDataSourceByName(ds_name);if(DS===null)return;if(DS.getData().length===0)return;if(y(filter_name,e)){var r=HTMLComposer.filterDSBlock(ds_name,filter_name,metric_names);n.id||(n.id=""Parsed""+g()),$(""#""+n.id).append(r)}})},Convert.convertDSSummaryBlock=function(e){var t=$("".DSSummaryBlock"");t.length>0&&$.each(t,function(e,t){if(t.id.indexOf(""Parsed"")>=0)return;ds_name=$(this).data(""data-source""),box_labels=$(this).data(""box-labels""),box_metrics=$(this).data(""box-metrics""),ts_metrics=$(this).data(""ts-metrics""),DS=Report.getDataSourceByName(ds_name);if(DS===null)return;if(DS.getData().length===0)return;var n=HTMLComposer.DSBlock(ds_name,box_labels,box_metrics,ts_metrics);t.id||(t.id=""Parsed""+g()),$(""#""+t.id).append(n)})},Convert.convertOverallSummaryBlock=function(){var e=$("".OverallSummaryBlock"");e.length>0&&$.each(e,function(e,t){if(t.id.indexOf(""Parsed"")>=0)return;var n=HTMLComposer.overallSummaryBlock();t.id||(t.id=""Parsed""+g()),$(""#""+t.id).append(n)})},Convert.convertDemographics=function(){var e=$("".Demographics"");e.length>0&&$.each(e,function(e,t){$(this).empty(),ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;var n=$(this).data(""period"");t.id=""Demographics-""+ds+""-""+""-""+n,DS.displayDemographics(t.id,n)})},Convert.getRealItem=function(e,t,n){var r=Report.getReposMap();if(r===undefined||r.length===0)return $.inArray(n,e.getReposData())>-1?n:null;var i=null;if(t===""repos""){var s=e.getReposMetricsData()[n];s===undefined?$.each(r,function(t,r){$.each(Report.getDataSources(),function(t,s){if(r[s.getName()]===n)return i=r[e.getName()],!1});if(i!==null)return!1}):i=n}else i=n;return i},Convert.convertFilterItemsSummary=function(e){var t=""FilterItemsSummary"";divs=$("".""+t),divs.length>0&&$.each(divs,function(n,r){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;r.id=ds+""-""+t,$(this).empty(),e===""repos""&&DS.displayReposSummary(r.id,DS),e===""countries""&&DS.displayCountriesSummary(r.id,DS),e===""companies""&&DS.displayCompaniesSummary(r.id,DS),e===""domains""&&DS.displayDomainsSummary(r.id,DS),e===""projects""&&DS.displayProjectsSummary(r.id,DS)})},Convert.convertFilterItemsGlobal=function(e){var t=b(),n=""FilterItemsGlobal"";divs=$("".""+n),divs.length>0&&$.each(divs,function(r,i){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;var s=$(this).data(""metric""),o=$(this).data(""show-others""),u=$(this).data(""order-by"");t.show_legend=$(this).data(""legend""),$(""#""+$(this).data(""legend-div"")).length>0?t.legend={container:$(this).data(""legend-div"")}:t.legend={container:null},t.graph=$(this).data(""graph""),t.title=$(this).data(""title""),t.show_title=1,i.id=s+""-""+n,$(this).empty(),e===""repos""&&DS.displayMetricReposStatic(s,i.id,t,u,o),e===""countries""&&DS.displayMetricCountriesStatic(s,i.id,t,u,o),e===""companies""&&DS.displayMetricCompaniesStatic(s,i.id,t,u,o),e===""domains""&&DS.displayMetricDomainsStatic(s,i.id,t,u,o),e===""projects""&&DS.displayMetricProjectsStatic(s,i.id,t,u,o)})},Convert.convertFilterItemsNav=function(e,t){var n=""FilterItemsNav"";divs=$("".""+n);if(divs.length>0){var r=0;$.each(divs,function(i,s){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;$(this).data(""page"")&&(t=$(this).data(""page"")),order_by=$(this).data(""order-by""),s.id=ds+""-""+n+""-""+r,r+=1,$(this).empty(),e===""repos""?DS.displayItemsNav(s.id,e,t,order_by):e===""countries""?DS.displayItemsNav(s.id,e,t):e===""companies""? DS.displayItemsNav(s.id,e,t):e===""domains""?DS.displayItemsNav(s.id,e,t):e===""projects""&&DS.displayItemsNav(s.id,e,t)})}},Convert.convertFilterItemsMetricsEvol=function(e){var t=b(),n=""FilterItemsMetricsEvol"";divs=$("".""+n),divs.length>0&&$.each(divs,function(r,i){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;var s=$(this).data(""metric""),o=!1;$(this).data(""stacked"")&&(o=!0),$(this).data(""min"")&&(config_viz.show_legend=!1,config_viz.show_labels=!0,config_viz.show_grid=!0,config_viz.help=!1);var u=$(this).data(""start""),a=$(this).data(""end"");t.lines={stacked:o},$(""#""+$(this).data(""legend-div"")).length>0?t.legend={container:$(this).data(""legend-div"")}:t.legend={container:null},t.show_legend=$(this).data(""legend""),t.mouse_tracker=$(this).data(""mouse_tracker"");var f=$(this).data(""remove-last-point"");f&&(t.remove_last_point=!0),i.id=s+""-""+n,$(this).empty(),e===""companies""?DS.displayMetricCompanies(s,i.id,t,u,a):e===""repos""?DS.displayMetricRepos(s,i.id,t,u,a):e===""domains""?DS.displayMetricDomains(s,i.id,t,u,a):e===""projects""&&DS.displayMetricProjects(s,i.id,t,u,a)})},Convert.convertFilterItemsMiniCharts=function(e,t){var n=b(),r=""FilterItemsMiniCharts"";divs=$("".""+r),divs.length>0&&$.each(divs,function(i,s){ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;$(this).data(""page"")&&(t=$(this).data(""page""));var o=$(this).data(""metrics""),u=$(this).data(""order-by""),a=!0;$(this).data(""show_links"")!==undefined&&(a=$(this).data(""show_links""));var f=$(this).data(""start""),l=$(this).data(""end""),c=$(this).data(""convert"");$(this).data(""frame-time"")&&(n.frame_time=!0);var h=$(this).data(""remove-last-point"");h&&(n.remove_last_point=!0),s.id=o.replace(/,/g,""-"")+""-""+e+""-""+r,$(this).empty(),e===""repos""?DS.displayReposList(o.split("",""),s.id,n,u,t,a,f,l,c):e===""countries""?DS.displayCountriesList(o.split("",""),s.id,n,u,t,a,f,l,c):e===""companies""?DS.displayCompaniesList(o.split("",""),s.id,n,u,t,a,f,l,c):e===""domains""?DS.displayDomainsList(o.split("",""),s.id,n,u,t,a,f,l,c):e===""projects""&&DS.displayProjectsList(o.split("",""),s.id,n,u,t,a,f,l,c)})},Convert.convertFilterItemData=function(e,t){var n=$("".FilterItemData"");n.length>0&&$.each(n,function(n,r){$(this).empty();var i=Report.cleanLabel(t);r.id||(r.id=""FilterItemData""+g()),html=HTMLComposer.itemName(i,e),$(""#""+r.id).append(html)})},Convert.convertFilterItemSummary=function(e,t){var n=""FilterItemSummary"";divs=$("".""+n),t!==null&&divs.length>0&&$.each(divs,function(r,i){var s=t;ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;$(this).data(""item"")&&(s=$(this).data(""item"")),i.id=ds+""-""+e+""-""+n,$(this).empty(),e===""repos""?DS.displayRepoSummary(i.id,s,DS):e===""countries""?DS.displayCountrySummary(i.id,s,DS):e===""companies""?DS.displayCompanySummary(i.id,s,DS):e===""domains""?DS.displayDomainSummary(i.id,s,DS):e===""projects""&&DS.displayProjectSummary(i.id,s,DS)})},Convert.convertFilterItemMicrodashText=function(e,t){var n=$("".FilterItemMicrodashText"");n.length>0&&$.each(n,function(n,r){$(this).empty();var i=t,s=$(this).data(""metric""),o=$(this).data(""name""),u=Report.getMetricDS(s)[0];if(u===undefined)return;if(e!==""projects"")return;var a=u.getProjectsGlobalData()[t],f='<div class=""row"">';o&&(f+='<div class=""col-md-3"">',f+='<span class=""dayschange"">'+u.basic_metrics[s].name+""</span>"",f+=""</div>""),$.each([365,30,7],function(e,t){var n=u.getMetrics()[s].column,r=a[s+""_""+t],i=a[""diff_net""+n+""_""+t],l=a[""percentage_""+n+""_""+t];l=Math.round(l*10)/10;if(r===undefined)return;var c="""";i>0&&(c=""+""+l),i<0&&(c=""-""+Math.abs(l)),o?f+='<div class=""col-md-3"">':f+='<div class=""col-md-4"">',f+='<span class=""dayschange"">Last '+t+"" days:</span>"",f+="" ""+Report.formatValue(r)+""<br>"",i===0?f+='<i class=""fa fa-arrow-circle-right""></i> <span class=""zeropercent"">&nbsp;'+c+""%</span>&nbsp;"":i>0?f+='<i class=""fa fa-arrow-circle-up""></i> <span class=""pospercent"">&nbsp;'+c+""%</span>&nbsp;"":i<0&&(f+='<i class=""fa fa-arrow-circle-down""></i> <span class=""negpercent"">&nbsp;'+c+""%</span>&nbsp;""),f+=""</div><!--col-md-4-->""}),f+=""</div><!--row-->"",$(r).append(f)})},Convert.convertFilterItemMetricsEvol=function(e,t){var n=b(),r=""FilterItemMetricsEvol"";divs=$("".""+r),t!==null&&divs.length>0&&$.each(divs,function(i,s){var o=t,u=$(this).data(""metrics"");ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;$(this).data(""item"")&&(o=$(this).data(""item"")),n=m(s,n),s.id=Report.cleanLabel(t).replace(/ /g,""_"")+""-"",s.id+=u.replace(/,/g,""-"")+""-""+ds+""-""+e+""-""+r,$(this).empty(),e===""repos""?DS.displayMetricsRepo(o,u.split("",""),s.id,n):e===""countries""?DS.displayMetricsCountry(o,u.split("",""),s.id,n):e===""companies""?DS.displayMetricsCompany(o,u.split("",""),s.id,n):e===""domains""?DS.displayMetricsDomain(o,u.split("",""),s.id,n):e===""projects""&&DS.displayMetricsProject(o,u.split("",""),s.id,n)})},Convert.convertFilterItemTop=function(e,t){var n=""FilterItemTop"";divs=$("".""+n),divs.length>0&&$.each(divs,function(r,i){var s=t;$(this).empty(),ds=$(this).data(""data-source""),DS=Report.getDataSourceByName(ds);if(DS===null)return;e===undefined&&(e=$(this).data(""filter""));if(e!==$(this).data(""filter""))return;if(!e)return;$(this).data(""item"")&&(s=$(this).data(""item""));var o=$(this).data(""metric""),u=$(this).data(""period""),a=$(this).data(""titles"");i.id=o+""-""+ds+""-""+e+""-""+n+""-""+g(),$(this).empty(),i.className="""",e===""companies""&&DS.displayTopCompany(s,i.id,o,u,a)})},Convert.convertSmartLinks=function(){var e=$("".SmartLinks"");e.length>0&&$.each(e,function(e,t){if(t.id.indexOf(""Parsed"")>=0)return;target_page=$(this).data(""target""),label=$(this).data(""label"");var n=HTMLComposer.smartLinks(target_page,label);t.id||(t.id=""Parsed""+g()),$(""#""+t.id).append(n)})},Convert.convertFilterStudyItem=function(e,t){var n=Convert.convertFilterStudyItem;if(n.done===undefined)n.done={};else if(n.done[e]===!0)return;e===""repositories""&&(e=""repos""),t===undefined&&(e===""repos""&&(t=Report.getParameterByName(""repository"")),e===""countries""&&(t=Report.getParameterByName(""country"")),e===""companies""&&(t=Report.getParameterByName(""company"")),e===""domains""&&(t=Report.getParameterByName(""domain"")),e===""projects""&&(t=Report.getParameterByName(""project"")));if(!t)return;if(Loader.FilterItemCheck(t,e)===!1)return;Convert.repositoryDSBlock(t),Convert.convertFilterItemData(e,t),Convert.convertFilterItemSummary(e,t),Convert.convertFilterItemMetricsEvol(e,t),Convert.convertFilterItemTop(e,t),Convert.convertFilterItemMicrodashText(e,t),Convert.convertProjectData(),Convert.activateHelp(),Convert.convertMetricsEvolSelector(),n.done[e]=!0},Convert.activateHelp=function(){$("".help"").popover({html:!0,trigger:""manual""}).click(function(e){$(this).popover(""toggle""),e.stopPropagation()})},Convert.convertFilterStudy=function(e){var t=Report.getCurrentPage();t===null&&(t=Report.getParameterByName(""page""),t!==undefined&&Report.setCurrentPage(t));if(t===undefined){if(!($(""[class^='FilterItems']"").length>0))return;t=1,Report.setCurrentPage(t)}e===""repositories""&&(e=""repos"");if(Loader.check_filter_page(t,e)===!1){$.each(Report.getDataSources(),function(n,r){Loader.data_load_items_page(r,t,Convert.convertFilterStudy,e)});return}Convert.convertFilterItemsSummary(e),Convert.convertFilterItemsGlobal(e),Convert.convertFilterItemsNav(e,t),Convert.convertFilterItemsMetricsEvol(e),Convert.convertFilterItemsMiniCharts(e,t)},Convert.convertDSTable=function(){var e=""DataSourcesTable"",t=$("".""+e),n,r;if(t.length>0){var i=0;$.each(t,function(t,n){$(this).empty(),n.id=e+i++,Viz.displayDataSourcesTable(n)})}},Convert.convertBasicDivs=function(){Convert.convertNavbar(),Convert.convertSmartLinks(),Convert.convertSectionBreadcrumb(),Convert.convertProjectMap(),Convert.convertFooter(),Convert.convertOverallSummaryBlock(),Convert.convertDSSummaryBlock(),Convert.convertDSTable(),Convert.convertGlobalData(),Convert.convertSummary()},Convert.convertBasicDivsMisc=function(){Convert.convertRadarActivity(),Convert.convertRadarCommunity(),Convert.convertTreemap(),Convert.convertBubbles()},Convert.convertBasicMetrics=function(e){var t=Report.getParameterByName(""repository"");t===undefined&&Convert.convertMetricsEvol(),Convert.convertTimeTo(),Convert.convertMarkovTable()},Convert.convertModifiedBasicMetrics=function(e){var t=1;if(Loader.check_filter_page(t,e)===!1){$.each(Report.getDataSources(),function(n,r){if(e!==""repos"")return;e===""repos""&&(total=r.getReposData().length);for(var i=0;i<total;i++){var s=r.getReposData()[i];Loader.data_load_item(s,r,t,Convert.convertModifiedBasicMetrics,e)}});return}Convert.convertMetricsEvolCustomized(e)},Convert.convertFilterTop=function(e){var t=Report.getParameterByName(""repository"");if(Loader.filterTopCheck(t,e)===!1)return;Convert.convertTop(),Convert.convertRepositorySelector()}})();if(Report===undefined)var Report={};(function(){function x(){return t}function T(){return n}function N(){return projects_hierarchy}function C(){return menu_elements.menu}function k(){return menu_elements.releases}function L(){return r}function A(e){r=e}function O(){return e}function M(){return h}function _(e){var t=e.toString().split(""."");return t[0]=t[0].replace(/\B(?=(\d{3})+(?!\d))/g,"",""),t.join(""."")}function D(e){var t=[];return $.each(Report.getDataSources(),function(n,r){r.getMetrics()[e]&&t.push(r)}),t}function P(){var e={};return $.each(Report.getDataSources(),function(t,n){e=$.extend({},e,n.getMetrics())}),e}function H(){var e=[],t=$.urlParam(""release"");t!==null&&t.length>0&&(e.push(""data/json/""+t),Report.setDataDir(""data/json/""+t),e.length>0&&Report.setProjectsDirs(e))}function B(){H();var e=Report.getProjectsDirs(),t,n,r,i,s,o,u,a,f,l;return $.each(e,function(e,c){if(Report.getConfig()===null||Report.getConfig()[""data-sources""]===undefined)n=new ITS,Report.registerDataSource(n),r=new MLS,Report.registerDataSource(r),t=new SCM,Report.registerDataSource(t),i=new SCR,Report.registerDataSource(i),s=new IRC,Report.registerDataSource(s),o=new MediaWiki,Report.registerDataSource(o),u=new People,Report.registerDataSource(u),a=new Downloads,Report.registerDataSource(a),f=new QAForums,Report.registerDataSource(f),l=new Releases,Report.registerDataSource(l);else{var h=Report.getConfig()[""data-sources""];$.each(h,function(e,c){c===""its""?(n=new ITS,Report.registerDataSource(n)):c===""mls""?(r=new MLS,Report.registerDataSource(r)):c===""scm""?(t=new SCM,Report.registerDataSource(t)):c===""scr""?(i=new SCR,Report.registerDataSource(i)):c===""irc""?(s=new IRC,Report.registerDataSource(s)):c===""mediawiki""?(o=new MediaWiki,Report.registerDataSource(o)):c===""people""?(u=new People,Report.registerDataSource(u)):c===""downloads""?(a=new Downloads,Report.registerDataSource(a)):c===""qaforums""?(f=new QAForums,Report.registerDataSource(f)):c===""releases""?(l=new Releases,Report.registerDataSource(l)):Report.log(""Not support data source ""+c)})}n&&n.setDataDir(c),r&&r.setDataDir(c),t&&t.setDataDir(c),i&&i.setDataDir(c),s&&s.setDataDir(c),o&&o.setDataDir(c),u&&u.setDataDir(c),a&&a.setDataDir(c),f&&f.setDataDir(c),l&&l.setDataDir(c),t&&n&&t.setITS(n)}),!0}function j(){$.each(Report.getActiveStudies(),function(e,t){var n=t;t===""repositories""&&(n=""repos""),DataProcess.orderItems(n),Convert.convertFilterStudy(t),Convert.convertFilterStudyItem(t)})}var e=null,t=null,n=null,r={},i=[],s=null,o="""",u=""data/json"",a=""config"",f=""data/json"",l="""",c=[f],h={},p={},d,v=a+""/project-info.json"",m=u+""/viz_cfg.json"",g=u+""/markers.json"",y=u+""/repos-map.json"",b=u+""/projects_hierarchy.json"";menu_elements_file=a+""/menu-elements.json"";var w=10,E=null,S={};Report.createDataSources=B,Report.getAllMetrics=P,Report.getMarkers=x,Report.getVizConfig=T,Report.getProjectsHierarchy=N,Report.getMenuElements=C,Report.getReleaseNames=k,Report.getMetricDS=D,Report.getGridster=L,Report.setGridster=A,Report.getCurrentPage=function(){return E},Report.setCurrentPage=function(e){E=e},Report.getPageSize=function(){return w},Report.setPageSize=function(e){w=e},Report.getProjectData=O,Report.getProjectsData=M,Report.convertStudies=j,Report.getDataSources=function(){return i},Report.registerDataSource=function(e){i.push(e)},Report.setHtmlDir=function(e){o=e},Report.getHtmlDir=function(){return o},Report.getDataDir=function(){return u},Report.setDataDir=function(e){u=e,v=e+""/project-info.json"",config_file=e+""/viz_cfg.json"",g=e+""/markers.json"",repos_mapping_file=u+""/repos-mapping.json"",b=u+""/projects_hierarchy.json""},Report.setMarkers=function(e){t=e},Report.getMarkersFile=function(){return g},Report.getReposMap=function(){return d},Report.setReposMap=function(e){d=e},Report.getReposMapFile=function(){return y},Report.setVizConfig=function(e){n=e},Report.getVizConfigFile=function(){return m},Report.setProjectsHierarchy=function(e){projects_hierarchy=e},Report.getProjectsHierarchyFile=function(){return b},Report.setMenuElements=function(e){menu_elements=e},Report.getMenuElementsFile=function(){return menu_elements_file},Report.setProjectData=function(t){e=t},Report.getProjectFile=function(){return v},Report.getProjectsDirs=function(){return c},Report.setProjectsDirs=function(e){c=e},Report.getProjectsList=function(){var e=[];return $.each(M(),function(t,n){e.push(t)}),e},Report.getProjectsDataSources=function(){return p},Report.setMetricsDefinition=function(e){$.each(Report.getDataSources(),function(t,n){n.setMetricsDefinition(e[n.getName()])})},Report.getPeopleIdentities=function(){return S},Report.setPeopleIdentities=function(e){S=e},Report.cleanLabel=function(e){var t=e,n=null;return e.split(""___"").length===2?(n=e.split("" ""),t=n[0]):e.lastIndexOf(""http"")===0||e.split(""_"").length>3?(n=e.split(""_""),t=n.pop(),t===""""&&(t=n.pop()),t=t.replace(""buglist.cgi?product="",""""),t=t.replace(""gmane.comp.sysutils."","""")):e.lastIndexOf(""<"")===0&&(t=MLS.displayMLSListName(e)),t},Report.formatValue=function(e,t){if(e===undefined)return""-"";var n=[""last_date"",""first_date""],r=e;try{r=parseFloat(e).toFixed(1).toString().replace(/\.0$/,""""),r=_(r);if(navigator.language===""es""){var i=r.split(""."");i[0]=i[0].replace(/,/g,"".""),r=i.join("","")}}catch(s){}return typeof r==""number""&&isNaN(r)&&(r=e.toString()),t!==undefined&&$.inArray(t,n)>-1&&(r=e.toString()),r},Report.escapeHtml=function(e){return e.replace(/&/g,""&amp;"").replace(/</g,""&lt;"").replace(/>/g,""&gt;"").replace(/""/g,""&quot;"").replace(/'/g,""&#039;"")},Report.getParameterByName=function(e){e=e.replace(/[\[]/,""\\["").replace(/[\]]/,""\\]"");var t=new RegExp(""[\\?&]""+e+""=([^&#]*)""),n=t.exec(location.search);return n===null?undefined:Report.escapeHtml(decodeURIComponent(n[1].replace(/\+/g,"" "")))},Report.getDataSourceByName=function(e){var t=null;return $.each(Report.getDataSources(),function(n,r){if(r.getName()===e)return t=r,!1}),t},Report.displayActiveMenu=function(){var e=window.location.href,t=e.substr(e.lastIndexOf(""/"")+1,e.length);t=t.split("".html"")[0];if(t.indexOf(""scm"")===0)$("".scm-menu"")[0].className=$("".scm-menu"")[0].className+"" active"";else if(t.indexOf(""its"")===0)$("".its-menu"")[0].className=$("".its-menu"")[0].className+"" active"";else if(t.indexOf(""mls"")===0)$("".mls-menu"")[0].className=$("".mls-menu"")[0].className+"" active"";else if(t.indexOf(""scr"")===0)$("".scr-menu"")[0].className=$("".scr-menu"")[0].className+"" active"";else if(t.indexOf(""irc"")===0)$("".irc-menu"")[0].className=$("".irc-menu"")[0].className+"" active"";else if(t.indexOf(""qaforum"")===0)$("".qaforum-menu"")[0].className=$("".qaforum-menu"")[0].className+"" active"";else if(t.indexOf(""studies"")===0)$("".studies-menu"")[0].className=$("".studies-menu"")[0].className+"" active"";else if(t.indexOf(""wiki"")===0)$("".wiki-menu"")[0].className=$("".wiki-menu"")[0].className+"" active"";else if(t.indexOf(""downloads"")===0)$("".downloads-menu"")[0].className=$("".downloads-menu"")[0].className+"" active"";else if(t.indexOf(""projects"")===0)$("".listprojects-menu"")[0].className=$("".listprojects-menu"")[0].className+"" active"";else if(t.indexOf(""index"")===0||t===""""){if($("".summary-menu"").length===0)return;$("".summary-menu"")[0].className=$("".summary-menu"")[0].className+"" active""}else $("".experimental-menu"")[0]&&($("".experimental-menu"")[0].className=$("".experimental-menu"")[0].className+"" active"")},Report.addDataDir=function(){var e,t=window.location.search.substr(1);return t&&t.indexOf(""data_dir"")!==-1&&(e=window.location.search.substr(1)),e},Report.configDataSources=function(){var e=Report.getProjectsDataSources();$.each(Report.getDataSources(),function(t,n){if(n.getData()instanceof Array)return;$.each(h,function(t,r){if(r.dir===n.getDataDir())return e[t]===undefined&&(e[t]=[]),$.each(e[t],function(e,t){if(n.getName()===t.getName())return!1}),n.setProject(t),e[t].push(n),!1})})},Report.getConfig=function(){return s},Report.setConfig=function(e){s=e,e&&(Report.log(""Global config file found""),e[""global-html-dir""]&&Report.setHtmlDir(e[""global-html-dir""]),e[""global-data-dir""]&&(Report.setDataDir(e[""global-data-dir""]),Report.setProjectsDirs([e[""global-data-dir""]])),e[""projects-data-dirs""]&&Report.setProjectsDirs(e[""projects-data-dirs""]))},Report.convertGlobal=function(){Convert.convertBasicDivs(),Convert.convertBasicDivsMisc(),Convert.convertBasicMetrics(),Convert.convertDemographics(),Convert.convertMetricsEvolSet(),Convert.convertLastActivity(),Convert.convertMicrodash(),Convert.convertMicrodashText()},Report.getActiveStudies=function(){var e=[],t,n=[""repositories"",""countries"",""companies"",""domains"",""projects""];return Report.getConfig()!==null?t=Report.getConfig().reports:t=n,$.each(n,function(n,r){$.inArray(r,t)>-1&&e.push(r)}),e},Report.convertStudiesGlobal=function(){Convert.convertPeople()};var F=!0;Report.getLog=function(){return F},Report.setLog=function(e){F=e},Report.log=function(e){Report.getLog()===!0&&window.console&&console.log(e)}})(),Loader.data_ready_global(function(){Report.configDataSources(),Report.convertGlobal(),Report.convertStudiesGlobal()}),Loader.data_ready(function(){study=""repos"",Convert.convertFilterTop(study),Convert.convertModifiedBasicMetrics(study)}),Loader.data_ready(function(){Report.convertStudies(),$(""body"").css(""cursor"",""auto""),$(""html"").click(function(e){$("".help"").popover(""hide"")}),Convert.activateHelp()}),$(document).ready(function(){var e=""./config.json"";$.getJSON(e,function(e){Report.setConfig(e)}).fail(function(){window.console&&Report.log(""Can't read global config file ""+e)}).always(function(e){Report.createDataSources(),Loader.data_load(),$(""body"").css(""cursor"",""progress"")})});var resized;$(window).resize(function(){clearTimeout(resized),resized=setTimeout(resizedw,100)});if(Viz===undefined)var Viz={};(function(){function t(e,t){var n="""";return $.each(Report.getAllMetrics(),function(e,r){if(r.action===t)return n=r.column,!1}),n}function n(e,t,n){var r=""<h4>""+n+""</h4>"",i='<table id=""itsmarkovtable"" class=""table table-striped"">';i+=""<thead><tr><th>Transition</th><th>Number</th><th>Percent</th></tr></thead><tbody>"",$.each(t,function(e,n){subdata=t[e],old_value=""old_value"",new_value=""new_value"",percent=""f"",number=""issue"";for(var r=0;r<subdata[old_value].length;r++){var s=subdata[new_value][r],o=subdata[percent][r];o=Math.round(o*100)/100;var u=subdata[number][r];i+=""<tr><td>""+e+"" -> ""+s+""</td>"",i+=""<td>""+u+""</td>"",i+=""<td>""+o+""</td></tr>""}}),i+=""</tbody></table>"",r+=i,div=$(""#""+e),div.append(r);return}function r(e,t){return e.hasOwnProperty(t)?e[t]:t}function i(e,t){var n={};return n.id=""id"",e===""senders""&&(t===""mls""||t===""irc"")&&(n.name=""senders"",n.action=""sent""),e===""authors""&&t===""scm""&&(n.name=""authors"",n.action=""commits""),e===""closers""&&t===""its""&&(n.name=""closers"",n.action=""closed""),t===""scr""&&(e===""mergers""&&(n.name=""mergers"",n.action=""merged""),e===""openers""&&(n.name=""openers"",n.action=""opened""),e===""reviewers""&&(n.name=""reviewers"",n.action=""reviews"")),t===""downloads""&&(e===""ips""&&(n.name=""ips"",n.action=""downloads""),e===""packages""&&(n.name=""packages"",n.action=""downloads"")),t===""mediawiki""&&e===""authors""&&(n.name=""authors"",n.action=""reviews""),t===""qaforums""&&(e===""senders""||e===""asenders""||e===""qsenders"")&&(n.name=""senders"",n.action=""sent""),t===""releases""&&e===""authors""&&(n.name=""username"",n.action=""releases""),n}function s(){return[""last month"",""last year"",""""]}function o(e,t,n){var r="""";for(var i=0;i<e[n.name].length;i++){if(t&&t<=i)break;var s=e[n.action][i];r+=""<tr><td>#""+(i+1)+""</td>"",r+=""<td>"",r+=e[n.name][i],r+=""</td>"",r+=""<td>""+s+""</td></tr>""}return r}function u(e,t,n){var r="""";for(var i=0;i<e.subject.length;i++){if(t&&t<=i)break;r+=""<tr><td>#""+(i+1)+""</td>"",r+=""<td>"";if(n===!0){var s=""http://www.google.com/search?output=search&q=X&btnI=1"";e.hasOwnProperty(""url"")&&e.url[i].length>0&&(s=""http://www.google.com/search?output=search&q=X%20site%3AY&btnI=1"",s=s.replace(/Y/g,e.url[i])),s=s.replace(/X/g,e.subject[i]),r+=""<td>"",r+='<a target=""_blank"" href=""'+s+'"">',r+=e.subject[i]+""</a>"",r+='&nbsp;<i class=""fa fa-external-link""></i></td>'}else r+=""<td>""+e.subject[i]+""</td>"";r+=""<td>""+e.initiator_name[i]+""</td>"",r+=""<td>""+e.length[i]+""</td>"",r+=""</tr>""}return r}function a(e,t,n,r){var i="""";for(var s=0;s<e[r.id].length;s++){if(t&&t<=s)break;var o=e[r.action][s];i+=""<tr><td>""+(s+1)+""</td>"",i+=""<td>"",n?(i+='<a href=""people.html?id='+e[r.id][s],get_params=Utils.paramsInURL(),get_params.length>0&&(i+=""&""+get_params),i+='"">',i+=e[r.name][s]+""</a>""):i+=e[r.name][s],i+=""</td>"",i+=""<td>""+o+""</td></tr>""}return i}function f(e,t,n,r){var i="""",s=!0;i+='<ul id=""myTab"" class=""nav nav-tabs"">';for(var o=0;o<e.length;o++){var u=t+"".""+e[o];if(n[u]){var a=e[o],f=a;a===""""?(a=""all"",f=""Complete history""):a===""last month""?f=""Last 30 days"":a===""last year""&&(f=""Last 365 days"");var l=a.replace(/\ /g,""""),c="""";s===!0&&(c=' class=""active""',s=!1),i+=""<li""+c+'><a href=""#'+r+t+l+'""data-toogle=""tab"">',i+=f+""</a></li>""}}return i+=""</ul>"",i}function l(e,t,n,r,i){var s=t+""_""+e,o="""",u="""";return s in r&&(o=r[s].desc,o=o.toLowerCase()),i===""""?data_period_formatted=""Complete history"":i===""last month""?data_period_formatted=""Last 30 days"":i===""last year""&&(data_period_formatted=""Last 365 days""),Utils.isReleasePage()&&(data_period_formatted=""Release history""),n===!0?u+=""<h6>Top ""+o+""</h6>"":u+='<div class=""toptable-title"">'+data_period_formatted+""</div>"",u}function c(e,t,n,r,c,h,p,d){var v="""",m="""",g="""",y=!0,b=$(""#""+e),w=b.attr(""data-data-source"");periods=s(),d!==undefined&&(y=!1),g+=l(n,w,y,c,d),y===!0&&(v+=f(periods,n,t,w)),m+='<div class=""tab-content"">';var E=i(n,w);if(y===!0){var S=!0,x="""";for(var T=0;T<periods.length;T++){x="""";var N=n+"".""+periods[T];if(t[N]){var C=periods[T];C===""""&&(C=""all"");var k=C.replace(/\ /g,"""");S===!0&&(x="" active in"",S=!1),m+='<div class=""tab-pane fade'+x+'"" id=""'+w+n+k+'"">',m+='<table class=""table table-striped"">',n===""threads""?m+=u(t[N],r,p):n===""packages""||n===""ips""?m+=o(t[N],r,E):(unit=c[w+""_""+n].action,m+=""<thead><th>#</th><th>""+n.capitalize()+""</th><th>""+unit.capitalize()+""</th></thead><tbody>"",m+=a(t[N],r,h,E),m+=""</tbody>""),m+=""</table>"",m+=""</div>""}}}else m+='<table class=""table table-striped""><tbody>',n===""threads""?m+=u(t,r,p):n===""packages""||n===""ips""?m+=o(t,r,E):(unit=c[w+""_""+n].action,m+=""<thead><th>#</th><th>""+n.capitalize()+""</th><th>""+unit.capitalize()+""</th></thead><tbody>"",m+=a(t,r,h,E),m+=""</tbody>""),m+=""</tbody></table>"";m+=""</div>"",y===!1&&b.append(g),b.append(v),b.append(m),y===!0&&(script=""<script>$('#myTab a').click(function (e) {e.preventDefault();$(this).tab('show');});</script>"",b.append(script))}function h(e,n,r,i,s,o,u,a){var f=n.name;if(!i||$.isEmptyObject(i))return;var l=n.action;u&&i[l].length<u&&(u=i[l].length,s=!1);var c=n.column;c===undefined&&(c=t(i,l));var h=""Top ""+f+"" ""+r,p=displayTopMetricTable(i,l,c,u,a,h),d=null;if(p===undefined)return;if(o===!1){d=$(""#""+e),d.append(p);return}var v="""",m="""";s&&(v=""top-""+s+""-""+c+""-"",v+=l+""-""+r,m+=""<div id='""+v+""' class='graph' style='float:right'></div>""),m+=p,d=$(""#""+e),d.append(m);if(s){var g=i[c],y=i[l];if(u){g=[],y=[];for(var b=0;b<u;b++)g.push(i[c][b]),y.push(i[l][b])}O(v,g,y,s)}}function p(e){dsources=Report.getDataSources(),html='<table class=""table table-striped"">',html+=""<thead><th>Data Source</th><th>From</th>"",html+=""<th>To <small>(Updated on)</small></th></thead><tbody>"",$.each(dsources,function(e,t){if(t.getName()===""people"")return;var n=t.getGlobalData(),r=t.getTitle();r===undefined&&(r=""-"");var i=n.last_date;if(i===undefined)return;var s=n.first_date;s===undefined&&(s=""-"");var o=n.type;html+=""<tr><td>""+r,o!==undefined&&(o=o.toLowerCase(),o=o.charAt(0).toUpperCase()+o.slice(1),html+="" (""+o+"")""),html+=""</td>"",html+=""<td>""+s+""</td>"",html+=""<td>""+i+""</td></tr>""}),html+=""</tbody></table>"",$(e).append(html)}function d(e,t,n){var r=Report.getAllMetrics(),i='<a href=""#"" class=""help""',s="""";if(n===""""){var o=function(e,n){if(t[u]===e)return s+=""<strong>""+n.name+""</strong>: ""+n.desc+""<br>"",!1};for(var u=0;u<t.length;u++)$.each(r,o)}else s=""<strong>Description</strong>: ""+n;i+='data-content=""'+s+'"" data-html=""true"">',i+='<img src=""qm_15.png""></a>';var a=$(""#""+e).prev()[0];a&&a.className===""help""&&$(""#""+e).prev().empty(),$(""#""+e).before(i)}function v(e,t,n,r,i){(!i||i.help!==!1)&&d(e,t,i.custom_help);var s=[];i.remove_last_point&&(n=DataProcess.revomeLastPoint(n)),i.frame_time&&(n=DataProcess.frameTime(n,t)),i.start_time&&(n=DataProcess.filterDates(i.start_time,i.end_time,n)),$.each(t,function(e,t){if(!n[t])return;var r=[[],[]];$.each(n[t],function(e,i){r[e]=[n.id[e],n[t][e]]});var i=t;Report.getAllMetrics()[t]&&(i=Report.getAllMetrics()[t].name),s.push({label:i,data:r})}),N(e,n,s,r,i)}function m(e,t,n,r,i,s){(!i||i.help!==!1)&&d(e,t,i.custom_help);var o=[],u=t[0],a={};$.each(n,function(e,t){if(t===undefined)return!1;if(t[u]===undefined)return!1;i.remove_last_point&&(t=DataProcess.revomeLastPoint(t)),i.frame_time&&(t=DataProcess.frameTime(t,[u])),i.start_time&&(t=DataProcess.filterDates(i.start_time,i.end_time,t));var n=[[],[]];$.each(t[u],function(e,r){n[e]=[t.id[e],t[u][e]]}),o.push({label:e,data:n}),a=t}),N(e,a,o,r,i)}function g(e,t,n,r,i,s,o,u,a){var f=[],l={};$.each(n,function(e,n){if(n===undefined)return!1;if(n[t]===undefined)return!1;u&&(n=DataProcess.convert(n,u,t)),s&&(n=DataProcess.filterDates(s,o,n)),i.frame_time&&(n=DataProcess.frameTime(n,[t]));var r=[[],[]];for(var a=0;a<n.id.length;a++)r[a]=[n.id[a],n[t][a]];e=Report.cleanLabel(e),f.push({label:e,data:r}),l=n});if(f.length===0)return;if(a){var c=[];$.each(a,function(e,t){$.each(f,function(e,n){if(t===n.label)return c.push(n),!1})}),f=c}N(e,l,f,r,i)}function y(e,t,n,r,i){var s={subtitle:e,legend:{show:!0,container:t},xaxis:{minorTickFreq:4,mode:""time"",timeUnit:""second"",timeFormat:""%b %y"",margin:!0},yaxis:{min:null,noTicks:2,autoscale:!0},grid:{verticalLines:!1,color:""#000000"",outlineWidth:1,outline:""s""},mouse:{container:t,track:!0,trackY:!1,relative:!0,position:""ne"",trackFormatter:function(e){var t=n.date[parseInt(e.index,10)];t===undefined?t="""":t+=""<br>"";for(var i=0;i<r.length;i++){var s=r[i].data[e.index][1];if(s===undefined)continue;r.length>1&&r[i].label!==undefined&&(value_name=r[i].label,t+=value_name+"":""),t+=""<strong>""+Report.formatValue(s)+""</strong><br>""}return t}},selection:{mode:""x"",fps:10},shadowSize:4};return i&&(Viz._history=n,Viz._lines_data=r,s.mouse.trackFormatter=Viz[i]),s}function b(e,t){if(t.length===0)return t;if(t.length>1)for(var n=0;n<t.length;n++){var r=t[n].data.length-1;t[n].data[r][1]=undefined}}function w(e,t){if(t.length!==1)return t;var n=t[0].data.length,r=[],i=0;for(var s=0;s<n-1;s++)i=parseInt(e.unixtime[s],10),r.push([i,undefined]);i=parseInt(e.unixtime[n-1],10),r.push([i,t[0].data[n-1][1]]);var o={data:r};return o.points={show:!0,radius:3,lineWidth:1,fillColor:null,shadowSize:0},t.push(o),t[0].data[n-1][1]=undefined,t[1].label=t[0].label,t}function E(e,t,n){var r=[""Jan"",""Feb"",""Mar"",""Apr"",""May"",""Jun"",""Jul"",""Aug"",""Sep"",""Oct"",""Nov"",""Dec""],i=new Date(parseInt(t,10)*1e3),s=r[i.getMonth()]+"" ""+i.getFullYear();i=new Date(parseInt(n,10)*1e3);var o=r[i.getMonth()]+"" ""+i.getFullYear();return e+"" ( ""+s+"" - ""+o+"" )""}function S(e){return e.sort(function(e,t){return e[1]>t[1]||t[1]===undefined?1:-1}),e}function x(e,t,n){t=Math.round(t),n=Math.round(n);var r=e.length,i=[];for(var s=0;s<r;s++)for(var o=e[s].data.length-1;o>0;o--){var u=e[s].data[o][0],a=u<t||u>n;a&&e[s].data.splice(o,1)}var f=[];for(s=0;s<r;s++)i=e[s].data,i=S(i),f.push(i[i.length-1][1]);return f.sort(function(e,t){return e-t}),f[f.length-1]}function T(e){var t=e[0].data[1][0]-e[0].data[0][0],n=e.length,r=0;for(var i=0;i<n;i++){var s=e[i].data.length;r=e[i].data[s-1][0],e[i].data.push([r+t,undefined])}return e}function N(e,t,n,r,i){var s=!1;i&&i.lines&&i.lines.stacked&&(s=!0),s?C(e,t,n,r,i):t.unixtime===undefined?C(e,t,n,r,i):A(e,t,n,r,i)}function C(e,t,n,r,i){var s=document.getElementById(e),o=null;i&&i.legend&&i.legend.container&&(o=$(""#""+i.legend.container));var u={subtitle:r,legend:{show:!0,container:o},xaxis:{minorTickFreq:4,tickFormatter:function(e){var n=null;for(var r=0;r<t.id.length;r++)if(parseInt(e,10)===t.id[r]){n=r;break}return t.date[n]}},yaxis:{min:0,noTicks:2,autoscale:!1},grid:{verticalLines:!1,color:""#000000"",outlineWidth:1,outline:""s""},mouse:{container:o,track:!0,trackY:!1,trackFormatter:function(e){var r=t.date[parseInt(e.index,10)];r===undefined?r="""":r+=""<br>"";for(var i=0;i<n.length;i++){var s=n[i].data[e.index][1];if(s===undefined)continue;n.length>1&&n[i].label!==undefined&&(r+=n[i].label+"":""),r+=Report.formatValue(s)+""<br>""}return r}}};i&&(i.show_title||(u.title=""""),""show_legend""in i&&(i.show_legend===!0?u.legend.show=!0:u.legend.show=!1),i.lines&&i.lines.stacked&&(u.lines={stacked:!0,fill:!0,fillOpacity:1,fillBorder:!0,lineWidth:.01}),i.show_labels||(u.xaxis.showLabels=!1,u.yaxis.showLabels=!1),i.show_grid===!1&&(u.grid.verticalLines=!1,u.grid.horizontalLines=!1,u.grid.outlineWidth=0),i.show_mouse===!1&&(u.mouse.track=!1),i.graph===""bars""&&(u.bars={show:!0}),i.light_style===!0&&(u.grid.color=""#ccc"",u.legend.show=!1),i.custom_title&&(u.subtitle=i.custom_title));var a=!1;i.graph!==""bars""&&n.length===1&&n[0].data[0][0]===0&&(a=!0);if(a){n=w(t,n);var f=t.id[t.id.length-1]+1;n[0].data.push([f,undefined]),n[1].data.push([f,undefined]),t.date.push(""""),t.id.push(f)}graph=Flotr.draw(s,n,u),a&&(t.date&&t.date.pop(),t.id&&t.id.pop())}function k(e,t){var n,r=e.length;return n=parseInt(t.unixtime[1])-parseInt(t.unixtime[0]),n/(r+1)}function L(e,t,n,r){var i=e.length,s=e[0].data.length;for(var o=0;o<i;o++)for(var u=0;u<s;u++)n?e[o].data[u][0]=parseInt(t.unixtime[u],10)+o*r:e[o].data[u][0]=parseInt(t.unixtime[u],10);return e}function A(e,t,n,r,i){function c(e){var t=Flotr._.extend(Flotr._.clone(f),e||{});return Flotr.draw(u,n,t)}var s=!1,o;if(n.length===0)return;var u=document.getElementById(e),a=null;i&&i.legend&&i.legend.container&&(a=$(""#""+i.legend.container));var f=y(r,a,t,n,i.mouse_tracker);i&&(i.show_title||(f.title=""""),""show_legend""in i&&(i.show_legend===!0?f.legend.show=!0:f.legend.show=!1),i.lines&&i.lines.stacked&&(f.lines={stacked:!0,fill:!0,fillOpacity:1,fillBorder:!0,lineWidth:.01}),i.show_labels||(f.xaxis.showLabels=!1,f.yaxis.showLabels=!1),i.show_grid===!1&&(f.grid.verticalLines=!1,f.grid.horizontalLines=!1,f.grid.outlineWidth=0),i.show_mouse===!1&&(f.mouse.track=!1),i.graph===""bars""&&(f.bars={show:!0,stacked:!1,horizontal:!1,barWidth:728e3,lineWidth:1},f.bars.barWidth=k(n,t),s=!0,o=f.bars.barWidth),i.light_style===!0&&(f.grid.color=""#ccc"",f.legend.show=!1),i.custom_title&&(f.subtitle=i.custom_title),f.mouse.position=""n"",f.mouse.margin=20),n.length>1&&(f.legend.show=!0),n=L(n,t,s,o);var l=!1;Utils.isReleasePage()===!1&&(i.graph!==""bars""&&n.length===1&&(l=!0),l?(n=w(t,n),T(n)):!l&&n.length>1&&b(t,n)),console.log(f),graph=c(),Flotr.EventAdapter.observe(u,""flotr:select"",function(e){var t={xaxis:{minorTickFreq:4,mode:""time"",timeUnit:""second"",timeFormat:""%b %y"",min:e.x1,max:e.x2},yaxis:{min:e.y1,autoscale:!0},grid:{verticalLines:!0,color:""#000000"",outlineWidth:1,outline:""s""}};t.subtitle=E(f.subtitle,e.xfirst,e.xsecond);var r=JSON.parse(JSON.stringify(n)),i=x(r,e.x1,e.x2);t.yaxis.max=i+i*.2,graph=c(t)}),Flotr.EventAdapter.observe(u,""flotr:click"",function(){c()})}function O(e,t,n,r,i,s,o,u,a){var f=!1;o&&(f=!0);var l=document.getElementById(e),c=null;s&&s.legend&&s.legend.container&&(c=$(""#""+s.legend.container));var h=[],p,d="""";if(!f)for(p=0;p<n.length;p++)t&&(d=DataProcess.hideEmail(t[p])),h.push({data:[[p,n[p]]],label:d});else for(p=0;p<n.length;p++)t&&(d=DataProcess.hideEmail(t[p])),h.push({data:[[n[p],p]],label:d});var v={subtitle:i,grid:{verticalLines:!1,horizontalLines:!1,outlineWidth:0},xaxis:{showLabels:!1,min:0},yaxis:{showLabels:!1,min:0},mouse :{container:c,track:!0,trackFormatter:function(e){var r=""x"";f&&(r=""y"");var i="""";return t&&(i=DataProcess.hideEmail(t[parseInt(e[r],10)])+"": ""),i+n[parseInt(e[r],10)]}},legend:{show:!1,position:""se"",backgroundColor:""#D2E8FF"",container:c}};s&&(s.show_title||(v.title=""""),s.show_legend&&(v.legend.show=!0)),r===""bars""&&(v.bars={show:!0,horizontal:f},u&&(v.bars.color=u,v.bars.fillColor=u),s&&s.show_legend!==!1&&(v.legend={show:!0,position:""ne"",container:c}),v.grid.horizontalLines=!0,v.yaxis={showLabels:!0,min:0},s&&s.xaxis&&(v.xaxis={showLabels:s.xaxis,min:0}),a&&(v.yaxis={showLabels:!0,min:0,tickFormatter:a})),r===""pie""&&(v.pie={show:!0},v.mouse.position=""ne""),r=Flotr.draw(l,h,v)}function M(e,t,n,r,i,s,o,u){var a=.4,f=n[0].length;n[1].length>f&&(f=n[1].length);var l=!1;s&&(l=!0);var c=document.getElementById(e),h=null;i&&i.legend&&i.legend.container&&(h=$(""#""+i.legend.container));var p=[],d,v=[],m=[];for(d=0;d<f;d++){var g,y;n[0].length>d?g=n[0][d]:g=undefined,n[1].length>d?y=n[1][d]:y=undefined,l?(p.push([g,d-a/2]),v.push([y,d+a/2])):(p.push([d-a/2,g]),v.push([d+a/2,y]))}m=[{data:p,label:t[0]},{data:v,label:t[1]}];var b={title:r,bars:{show:!0,horizontal:l,barWidth:a},grid:{verticalLines:!1,horizontalLines:!1,outlineWidth:0},xaxis:{showLabels:!1,min:0},yaxis:{showLabels:!0,min:0},mouse:{container:h,track:!0,trackFormatter:function(e){var n,r=""x"";l&&(r=""y"");var i=parseFloat(e[r],1),s=Math.round((i-.2)*10)/10,o=Math.round((i+.2)*10)/10;s===parseInt(i,10)?n=s:n=o;var a=n;u&&(a=n*u);var f=a+"" years: "",c,h;return p[n]===undefined?c=0:c=parseInt(p[n][0],10),isNaN(c)&&(c=0),v[n]===undefined?h=0:h=parseInt(v[n][0],10),isNaN(h)&&(h=0),f+=c+"" ""+t[0],f+="" , "",f+=h+"" ""+t[1],f+="" (""+parseInt(c/h*100,10)+""% )"",f}},legend:{show:!0,position:""ne"",backgroundColor:""#D2E8FF"",container:h}};i&&(i.show_title||(b.title=""""),i.show_legend&&(b.legend.show=!0)),i&&i.show_legend!==!1&&(b.legend={show:!0,position:""ne"",container:h}),b.grid.horizontalLines=!0,b.yaxis={showLabels:!0,min:0},o&&(b.yaxis={showLabels:!0,min:0,tickFormatter:o}),i&&i.xaxis&&(b.xaxis={showLabels:i.xaxis,min:0}),graph=Flotr.draw(c,m,b)}function _(e,t,n,r){var i=document.getElementById(e),s=Report.getMetricDS(t)[0],o=Report.getMetricDS(n)[0],u=[];if(s!=o){Report.log(""Metrics for bubbles have different data sources"");return}var a=[],f=[];$.each(Report.getDataSources(),function(e,t){t.getName()===s.getName()&&(a.push(t.getData()),f.push(t.getProject()))});var l=[[],[]];l=[a[0].id,a[0].date];for(var c=0;c<a.length;c++){if(a[c]instanceof Array)return;l=DataProcess.fillDates(l,[a[c].id,a[c].date])}for(var h=0;h<a.length;h++){var p=[],d=a[h],v=DataProcess.fillHistory(l[0],[d.id,d[t]]),m=DataProcess.fillHistory(l[0],[d.id,d[n]]);for(c=0;c<l[0].length;c++)p.push([l[0][c],v[1][c],m[1][c]]);u.push({label:f[h],data:p})}var g={bubbles:{show:!0,baseRadius:5},mouse:{track:!0,trackFormatter:function(e){var r=a[0].date[e.index]+"": "";return r+=e.series.label+"" "",r+=e.series.data[e.index][1]+"" ""+t+"","",r+=e.series.data[e.index][2]+"" ""+n,r}},xaxis:{tickFormatter:function(e){return a[0].date[parseInt(e,10)-a[0].id[0]]}}};s.getName()===""its""&&$.extend(g.bubbles,{baseRadius:1}),r&&$.extend(g.bubbles,{baseRadius:r}),Flotr.draw(i,u,g)}function D(e,t,n,r){if(!n)return;r?period=365*r:r=.25;var i=[],s=[],o=[],u,a={show_legend:!1,xaxis:!0},f,l;for(u=0;u<n.aging.persons.age.length;u++)f=n.aging.persons.age[u],l=parseInt(f/period,10),i[l]||(i[l]=0),i[l]+=1;for(u=0;u<n.birth.persons.age.length;u++)f=n.birth.persons.age[u],l=parseInt(f/period,10),s[l]||(s[l]=0),s[l]+=1;o=[""Retained"",""Attracted""],yticks=function(e,t){var n=r,i=""years"";return e*=r,e+"" ""+i};var c=[i,s];n&&M(e,o,c,"""",a,!0,yticks,r)}function P(e,t,n){var r=document.getElementById(e),i=$(""#""+e).data(""max""),s=.2;i||(i=0);for(var o=0;o<n.length;o++)for(var u=0;u<n[o].data.length;u++){var a=n[o].data[u][1];a>i&&(i=a,i=parseInt(i*(1+s),10))}(function(){var e=[n,t]})(),graph=Flotr.draw(r,n,{radar:{show:!0},mouse:{track:!0,trackFormatter:function(e){var r="""";for(var i=0;i<n.length;i++)r+=n[i].label+"" "",r+=n[i].data[e.index][1]+"" "",r+=t[e.index][1]+""<br>"";return r}},grid:{circular:!0,minorHorizontalLines:!0},yaxis:{min:0,max:i,minorTickFreq:1},xaxis:{ticks:t}})}function H(e,t){var n=[],r=[],i=[],s=[],o=0,u=0;for(o=0;o<t.length;o++){var a=Report.getMetricDS(t[o]);for(u=0;u<a.length;u++)n[u]||(n[u]=[],s[u]=a[u].getProject()),n[u].push([o,parseInt(a[u].getGlobalData()[t[o]],10)]);r.push([o,a[0].getMetrics()[t[o]].name])}for(u=0;u<n.length;u++)i.push({label:s[u],data:n[u]});P(e,r,i)}function B(e){var t=[""scm_committers"",""scm_authors"",""its_openers"",""its_closers"",""its_changers"",""mls_senders""];H(e,t)}function j(e){var t=[""scm_commits"",""scm_files"",""its_opened"",""its_closed"",""its_changed"",""mls_sent""];H(e,t)}function F(e,t,n,r,i){q(e,t,n,r,i)}function I(e,t,n,r,i){q(e,t,n,r,i)}function q(e,t,n,r,i){var s=n.split("",""),o=t.data;if(!o[s[0]])return;var u={};u.date=o.date,$.each(o,function(e,t){if($.inArray(e,s)===-1)return;u[e]=[];for(var n=0;n<t.length;n++){var r=parseFloat((parseInt(t[n],null)/24).toFixed(2),10);u[e].push(r)}}),u.id=[];for(var a=0;a<o[s[0]].length;a++)u.id.push(a);var f={show_legend:!0,show_labels:!0};v(e,s,u,n,f)}function R(e,t,n,r,i,s,o,u,a,f,l,h){var p=t.getMetrics();n===undefined&&(n=!0);var d;h===undefined?d=t.getGlobalTopData():d=t.getRepositoriesTopData()[h],Utils.isReleasePage()&&(s=!1,i="""");if(s===!0){var v={};$.each(d,function(e,t){var n=e.split("".""),i=n[0],s=n[1];if(r&&r!==i)return!0;r&&r===i&&(v[e]=d[e])}),c(e,v,r,a,p,f,l)}else $.each(d,function(t,n){var s=t.split("".""),o=s[0],u=s[1];if(r&&r!==o)return!0;if(i!==undefined&&i!==u)return!0;c(e,d[t],r,a,p,f,l,i)})}function U(e,t,n,r,i,s){var o=null;h(n,r,i,t,o,s)}function z(e,t,n,r,i){var s=t.getProject(),o=t.getMetrics()[n],u=null;if(!t.getGlobalTopData()[n])return;data=t.getGlobalTopData()[n][r],h(e,s,o,r,data,u,i)}function W(e,t,n){if(n===undefined){if(t===undefined)return;Loader.get_file_data_div(t,Viz.displayTreeMap,e);return}if(n===null)return;var r=d3.scale.category20c(),i=d3.select(""#""+e),s=$(""#""+e).width(),o=$(""#""+e).height(),u=d3.layout.treemap().size([s,o]).sticky(!0).value(function(e){return e.size}),a=function(){this.style(""left"",function(e){return e.x+""px""}).style(""top"",function(e){return e.y+""px""}).style(""width"",function(e){return Math.max(0,e.dx-1)+""px""}).style(""height"",function(e){return Math.max(0,e.dy-1)+""px""})},f=i.datum(n).selectAll("".node"").data(u.nodes).enter().append(""div"").attr(""class"",""treemap-node"").call(a).style(""background"",function(e){return e.children?r(e.name):null}).text(function(e){return e.children?null:e.name});d3.selectAll(""input"").on(""change"",function(){var t=this.value===""count""?function(){return 1}:function(e){return e.size};f.data(u.value(t).nodes).transition().duration(1500).call(a)})}function X(e,t,n,r,i){var s=null,o="""",u=[[],[]];n?$.each(Report.getDataSources(),function(e,t){if(t.getName()===n)return s=t.getMetrics(),!1}):s=Report.getAllMetrics(),$.each(Report.getDataSources(),function(e,t){o=t.getMainMetric();if(n===null&&t.getName()===""scm""||n&&t.getName()==n)return u=[t.getData().id,t.getData()[o]],i===!1&&(u=[t.getData().id,[]]),!1});var a=[[],[]];$.each(t,function(e,t){$.each(t,function(e,t){if(n&&n!==t.getName())return;a=DataProcess.fillDates(a,[t.getData().id,t.getData().date])})});var f=a[0][0],l=document.getElementById(e),c,h=Report.getMarkers();c={container:l,xTickFormatter:function(e){var t=a[1][e-f];return t===""0""&&(t=""""),t},yTickFormatter:function(e){return e+""""},selection:{data:{x:{min:a[0][0],max:a[0][a[0].length-1]}}}},c.data={summary:DataProcess.fillHistory(a[0],u),markers:h,dates:a[1],envision_hide:r,main_metric:o};var p=null,d=function(e,t){var n=t.getData();if(n[m]===undefined)return;c.data[m]===undefined&&(c.data[m]=[]);var r=DataProcess.fillHistory(a[0],[n.id,n[m]]);if(m===o){c.data[m].push({label:p,data:r});if(n[m+""_relative""]===undefined)return;c.data[m+""_relative""]===undefined&&(c.data[m+""_relative""]=[]),r=DataProcess.fillHistory(a[0],[n.id,n[m+""_relative""]]),c.data[m+""_relative""].push({label:p,data:r})}else c.data[m].push({label:p,data:r})},v=function(e,t){p=e,$.each(t,d)};for(var m in s)$.each(t,v);return c.trackFormatter=function(e){var t=e.series.data,n=t[e.index][0]-f,r={},i=Report.getProjectsList();for(var o=0;o<i.length;o++)r[i[o]]={};var u=a[1][n]+"":<br>"";for(var l in s){if(c.data[l]===undefined)continue;if($.inArray(l,c.data.envision_hide)>-1)continue;for(o=0;o<i.length;o++){if(c.data[l][o]===undefined)continue;var h=c.data[l][o].label,p=c.data[l][o].data;u=p[1][n],r[h][l]=u}}u=""<table><tr><td align='right'>""+a[1][n]+""</td></tr>"",u+=""<tr>"",i.length>1&&(u+=""<td></td>"");for(l in s){if(c.data[l]===undefined)continue;if($.inArray(l,c.data.envision_hide)>-1)continue;u+=""<td>""+s[l].name+""</td>""}return u+=""</tr>"",$.each(r,function(e,t){var n=""<tr>"";for(var o in s){if(c.data[o]===undefined)continue;if($.inArray(o,c.data.envision_hide)>-1)continue;mvalue=r[e][o],mvalue===undefined&&(mvalue=""n/a""),n+=""<td>""+mvalue+""</td>""}i.length>1&&(n=""<td>""+e+""</td>""+n),n+=""</tr>"",u+=n}),u+=""</table>"",u},c}function V(e){return e===undefined&&(e={}),e.show_desc===undefined&&(e.show_desc=!0),e.show_title===undefined&&(e.show_title=!0),e.show_labels===undefined&&(e.show_labels=!0),e}function J(e,t){var n=e.getMetrics(),r="""";for(var i=0;i<t.length;i++)i!==0&&(r+="" vs. ""),t[i]in n?r+=n[t[i]].name:r+=t[i];return r}function K(e,t,n,r,i,s){i=V(i);var o="""";i.show_title&&(i.title===undefined?o=J(e,t):o=i.title),s!==undefined?m(r,t,n,o,i):v(r,t,n,o,i)}function Q(e,t,n,r,i,s){s=V(s);var o=J(e,n);v(i,n,r,o,s)}function G(e,t,n,r,i,s){s=V(s);var o=J(e,n);v(i,n,r,o,s)}function Y(e,t,n,r,i,s){s=V(s);var o=J(e,n);v(i,n,r,o,s)}function Z(e,t,n,r,i,s){s=V(s);var o=J(e,n);v(i,n,r,o,s)}function et(e,t,n,r,i,s){s=V(s);var o=J(e,n);v(i,n,r,o,s)}function tt(e,t,n,r,i,s){r=V(r),r.show_legend!==!1&&(r.show_legend=!0);var o=e;g(n,e,t,o,r,i,s)}function nt(e,t,n,r,i,s){s=V(s);var o=J(e,n);v(i,n,r,o,s)}function rt(e,t,n,r,i,s,o){r=V(r),r.show_legend!==!1&&(r.show_legend=!0);var u=e;g(n,e,t,u,r,i,s,null,o)}function it(e,t,n,r,i,s){r=V(r),r.show_legend!==!1&&(r.show_legend=!0);var o=e;g(n,e,t,o,r,i,s)}function st(e,t,n,r,i,s){r=V(r),r.show_legend!==!1&&(r.show_legend=!0);var o=e;g(n,e,t,o,r,i,s)}function ot(e,t,n,r,i){i=V(i);var s="""";i.title===undefined?s=e:s=i.title;var o=[],u=[],a=""bars"";i.graph&&(a=i.graph),$.each(n,function(n,r){var i=Report.cleanLabel(r);u.push(i),o.push(t[r][e])}),O(r,u,o,a,s,i)}function ut(e,t,n,r){var i=Report.getProjectsDataSources(),s=Report.getVizConfig(),o=Viz.getEnvisionOptions(e,i,null,s.summary_hide,r);o.legend_show=n,t&&($.each(i,function(e,t){$.each(t,function(e,t){main_metric=t.getMainMetric()})}),DataProcess.addRelativeValues(o.data,main_metric)),new envision.templates.Envision_Report(o)}var e=""#ffa500"";Viz.displayTop=R,Viz.displayTopCompany=U,Viz.displayTopGlobal=z,Viz.displayBasicChart=O,Viz.displayMetricCompanies=rt,Viz.displayMetricSubReportStatic=ot,Viz.displayMetricsCompany=Q,Viz.displayMetricsDomain=Y,Viz.displayMetricsProject=Z,Viz.displayMetricsPeople=et,Viz.displayMetricsRepo=G,Viz.displayMetricRepos=tt,Viz.displayMetricsCountry=nt,Viz.displayMetricDomains=it,Viz.displayMetricProjects=st,Viz.displayMetricsEvol=K,Viz.displayBubbles=_,Viz.displayDemographicsChart=D,Viz.displayEnvisionAll=ut,Viz.displayTimeToFix=I,Viz.displayTimeToAttention=F,Viz.displayMetricSubReportLines=g,Viz.displayRadarActivity=j,Viz.displayRadarCommunity=B,Viz.displayTreeMap=W,Viz.displayMarkovTable=n,Viz.displayDataSourcesTable=p,Viz.getEnvisionOptions=X,Viz.checkBasicConfig=V,String.prototype.capitalize=function(){return this.replace(/(?:^|\s)\S/g,function(e){return e.toUpperCase()})},Viz.track_formatter_com_pending=function(e){scr=Report.getDataSourceByName(""scr""),companies=scr.getCompaniesMetricsData(),dhistory=Viz._history,lines_data=Viz._lines_data;var t=dhistory.date[parseInt(e.index,10)];t===undefined?t="""":t+=""<br>"";for(var n=0;n<lines_data.length;n++){var r=lines_data[n].data[e.index][1];if(r===undefined)continue;lines_data.length>1&&(lines_data[n].label!==undefined&&(company_name=lines_data[n].label),t+=lines_data[n].label+"":""),t+=""<strong>""+Report.formatValue(r)+""</strong>"",company_name&&(t+=""(""+companies[company_name].pending[e.index]+"")""),t+=""<br>""}return t},Viz.getEnvisionOptionsMin=function(e,t,n){var r=t.id[0],i=document.getElementById(e),s,o=Report.getMarkers(),u=Report.getAllMetrics();s={container:i,xTickFormatter:function(e){var n=t.date[e-r];return n===""0""&&(n=""""),n},yTickFormatter:function(e){return e+""""},selection:{data:{x:{min:t.id[0],max:t.id[t.id.length-1]}}}},s.data={summary:[t.id,t.sent],markers:o,dates:t.date,envision_hide:n,main_metric:""sent""};var a=Report.getAllMetrics(),f=null;for(var l in t)f=l,a[l]&&(f=a[l].name),s.data[l]=[{label:f,data:[t.id,t[l]]}];return s.trackFormatter=function(e){var n=e.series.data,i=n[e.index][0]-r,s=t.date[i]+"":<br>"";for(var o in u){if(t[o]===undefined)continue;s+=t[o][i]+"" ""+o+"" , ""}return s},s}})(),IRC.prototype=new DataSource(""irc""),ITS.prototype=new DataSource(""its""),MediaWiki.prototype=new DataSource(""mediawiki""),MLS.prototype=new DataSource(""mls""),SCM.prototype=new DataSource(""scm""),SCR.prototype=new DataSource(""scr""),People.prototype=new DataSource(""people""),Downloads.prototype=new DataSource(""downloads""),QAForums.prototype=new DataSource(""qaforums""),Releases.prototype=new DataSource(""releases"");var Identity={};(function(){function t(t,n,r){var i="""";t===e?i="""":i=e,$(""#""+r).sortable({handle:"".handle"",connectWith:""#""+i,start:function(e,t){t.item.siblings("".ui-selected"").appendTo(t.item)},stop:function(t,n){n.item.parent()[0].id===e&&n.item.find("".handle"").remove(),n.item.parent().append(n.item.find(""li"")),n.item.parent().find(""li"").addClass(""mjs-nestedSortable-leaf"")}}).selectable().find(""li"").prepend(""<div class='handle'></div>"")}function n(e,t){$(""#""+e.getName()+""filter"").autocomplete({source:t,select:function(t,n){return $(""#""+e.getName()+""filter"").val(""""),$(""#""+e.getName()+""_people_""+n.item.value).addClass(""ui-selected""),!1}})}var e=""unique-sortable"";Identity.showListNested=function(t,n){list=""<ol id=""+e+' class=""nested_sortable"" ',list+='style=""padding: 5px; background: #eee;""></ol>',$(""#""+t).append(list),$(""#""+e).nestedSortable({forcePlaceholderSize:!0,handle:""div"",helper:""clone"",items:""li"",tolerance:""pointer"",toleranceElement:""> div"",maxLevels:2,isTree:!0,expandOnHover:700,startCollapsed:!0}),$("".disclose"").on(""click"",function(){$(this).closest(""li"").toggleClass(""mjs-nestedSortable-collapsed"").toggleClass(""mjs-nestedSortable-expanded"")})},Identity.showList=function(e,r){var i="""",s=r.getPeopleData(),o=[];i='<ol id=""'+r.getName()+'-sortable"" class=""sortable"">';for(var u=0;u<s.id.length;u++){var a=s.id[u];typeof a==""string""&&(a=a.replace(""@"",""_at_"").replace(""."",""_"")),o.push({value:a,label:s.name[u]}),i+='<li id=""'+r.getName()+""_people_""+a+'"" ',i+='class=""ui-widget-content ui-selectee"">',i+='<div><span class=""disclose""><span></span></span>',i+=s.id[u]+"" ""+s.name[u],i+=""</div></li>""}i+=""</ol>"",$(""#""+e).append(""<input id='""+r.getName()+""filter'>""),n(r,o),$(""#""+e).append(i),t(e,i,r.getName()+""-sortable"")}})();",4,4
openstack%2Foslo-specs~master~I8c481d50ad0a00567361d59c9fb78d9513163ebb,openstack/oslo-specs,master,I8c481d50ad0a00567361d59c9fb78d9513163ebb,Remove specs not completed in juno,MERGED,2014-09-09 13:47:29.000000000,2014-09-26 13:39:27.000000000,2014-09-26 13:39:27.000000000,"[{'_account_id': 3}, {'_account_id': 1669}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 6928}, {'_account_id': 7198}, {'_account_id': 9712}]","[{'number': 1, 'created': '2014-09-09 13:47:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/b7cd7662f8c95b4a90b861305a37a6be7df1d154', 'message': 'Remove specs not completed in juno\n\nThese specs were not completed in juno and can be resubmitted for kilo.\n\nChange-Id: I8c481d50ad0a00567361d59c9fb78d9513163ebb\n'}, {'number': 2, 'created': '2014-09-11 19:56:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/fb0082dd7d706ffa8f408a419dc5812550bcf312', 'message': 'Remove specs not completed in juno\n\nThese specs were not completed in juno and can be resubmitted for kilo.\n\nChange-Id: I8c481d50ad0a00567361d59c9fb78d9513163ebb\n'}, {'number': 3, 'created': '2014-09-15 17:04:13.000000000', 'files': ['specs/juno/oslo-messaging-greenio-executor.rst', 'specs/juno/taskflow-improved-scoping.rst', 'specs/juno/app-agnostic-logging-parameters.rst', 'specs/juno/enable-mysql-connector.rst', 'specs/juno/remove-context-adapter.rst', 'specs/juno/taskflow-conditional-execution.rst'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/79518fba6efe5b44a6fe647f47ad4db60bf58d71', 'message': 'Remove specs not completed in juno\n\nThese specs were not completed in juno and can be resubmitted for kilo.\n\nChange-Id: I8c481d50ad0a00567361d59c9fb78d9513163ebb\n'}]",1,120095,79518fba6efe5b44a6fe647f47ad4db60bf58d71,19,7,3,2472,,,0,"Remove specs not completed in juno

These specs were not completed in juno and can be resubmitted for kilo.

Change-Id: I8c481d50ad0a00567361d59c9fb78d9513163ebb
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/95/120095/1 && git format-patch -1 --stdout FETCH_HEAD,"['specs/juno/oslo-messaging-greenio-executor.rst', 'specs/juno/pbr-semver.rst', 'specs/juno/taskflow-improved-scoping.rst', 'specs/juno/app-agnostic-logging-parameters.rst', 'specs/juno/remove-context-adapter.rst', 'specs/juno/taskflow-conditional-execution.rst']",6,b7cd7662f8c95b4a90b861305a37a6be7df1d154,remove-incomplete-juno-specs,,"======================= Conditional execution ======================= Include the URL of your launchpad blueprint: https://blueprints.launchpad.net/taskflow/+spec/conditional-flow-choices Allow a level of conditional execution of compiled `atoms`_ (typically `tasks`_) and associated `flows`_ (aka workflows) that can be used to alter the execution *path* based on some type of condition and a decided outcome. .. _atoms: http://docs.openstack.org/developer/taskflow/atoms.html#atom .. _flows: http://docs.openstack.org/developer/taskflow/patterns.html .. _tasks: http://docs.openstack.org/developer/taskflow/atoms.html#task Problem description =================== Mistral and cinder (and others?) would like to be able to conditionally have a task or subflow execute based on some type of selection made by either a prior atom or an error or a function (for example). This kind of conditional execution right now is difficult currently in taskflow since the current model is more of a declaratively *fixed* pipeline that is verified before execution and only completes execution when all atoms have ``EXECUTED``, ``FAILED`` or ``REVERTED`` (when retries are involved an atom or group of atoms can flip between ``FAILED``, ``REVERTING`` and ``RUNNING`` as the associated `retry`_ controller tries to resolve the execution state). Conditional execution alters this since now we cannot, at compile time, make a decision about whether an atom will or will not execute (since it is not known until runtime whether the atom will execute) and we will be required skip parts of the workflow based on a conditional outcome. Due to this these *skipped* pieces of the workflow will now not have a ``EXECUTED`` or ``REVERTED`` state but will be placed in a new state ``IGNORED``. Overall, to accommodate this feature we will try to adjust taskflow to provide at least a *basic* level of support to accomplish a *type* of conditional execution. In the future this support can be expanded as the idea and implementation matures and proves itself. Proposed change =============== In order to support conditionals we first need to determine what a conditional means in a workflow, what it looks like and how it would be incorporated into a user's workflow. The following is an example of how this *may* look when a user decides to add a conditional choice into a workflow: :: nfs_flow = linear_flow.Flow(""nfs-work"") nfs_flow.add( ConfigureNFS(), StartNFS(), AttachNFS(), ... ) nfs_matcher = lambda volume_type: volume_type == 'nfs' ceph_flow = linear_flow.Flow(""ceph-work"") ceph_flow.add( ConfigureCeph(), AttachCeph(), ... ) ceph_matcher = lambda volume_type: volume_type == 'ceph' root_flow = linear_flow.Flow(""my-work"") root_flow.add( PopulateDatabase(), # Only one of these two will run (this is equivalent to an if/elif) LambdaGate(requires=""volume_type"", decider=nfs_matcher).add(nfs_flow), LambdaGate(requires=""volume_type"", decider=ceph_matcher).add(ceph_flow), SendNotification(), ... ) **Note:** that after this workflow has been constructed that the user will then hand it off to taskflow for reliable, consistent execution using one of the supported `engines`_ types, parallel, threaded or other... Implementation -------------- To make it simple (to start) let's limit the scope and suggest that a conditional (a gate, gate seems to be a common name for these according to `advances in dataflow programming`_) is restricted to being a runtime scheduling decision that causes the path of execution to allow (aka pass) or ignore (aka discard) the components that are contained with-in the gate. This means that at the `compile time`_ phase when a gate is inserted into a flow that any output symbols produced by the subgraph under the gate can not be depended upon by any successors that follow the gate decision. For example the following will **not** be valid since it introduces a symbol dependency on a conditional gate (in a future change we could set these symbols to some defaults when they will not be produced, for now though we will continue *more* being strict): :: nfs_flow = linear_flow.Flow(""nfs-work"") nfs_flow.add( ConfigureNFS(provides=[""where_configured""]), AttachNFS(), ... ) nfs_matcher = lambda volume_type: volume_type == 'nfs' root_flow = linear_flow.Flow(""my-work"") root_flow.add( PopulateDatabase(), LambdaGate(requires=""volume_type"", decider=nfs_matcher).add(nfs_flow), SendNotification(requires=['where_configured']), ... ) The next thing a conditional then needs a mechanism to influence the *outcome* which should be executed to either allow its subgraph to pass or to be discarded. When a gate is encountered while executing (assume there is some way to know that we have *hit* a gate) we need to first freeze execution of any successors (nothing must execute in any outcomes subgraph ahead of time, this would violate the conditional constraint). **Note:** that this does *not* mean that a subgraph executing that is not connected to this conditional will have to be stopped (its execution is not dependent on this outcome decision). So assuming we can freeze dependent execution (which we currently can do, since taskflow `engines`_ are responsible for scheduling further work) we now need to provide the gate a way to make a decision; usually the decision will be based on some prior execution or other external state. We have a mechanism for doing this already so we will continue using the existing `inputs and outputs`_ mechanism to communicate any state (local or otherwise) to the gate. Now the gate just needs to be able to make a boolean (or `truthy`_ value) decision about whether what is contained after the gate should run or should not run so that the runtime can continue down that execution path. To accommodate this we will require the gate object to provide an ``execute()`` method (this allows gates themselves to be an `atom`_ derived type) that returns a `truthy`_ value. If the value returned is ``true`` then the gate will have been determined to have been passed and the contained subgraph is then eligible for execution and further scheduling. Otherwise if the value that is returned is ``false`` (or falsey) then the contained subgraphs nodes will be put into the ``IGNORE`` state before further scheduling occurs. **Note:** this occurs *before* further scheduling so that if a failure occurs (``kill -9`` for example) during saving those atoms ``IGNORE`` states that a resuming entity can attempt to make forward progress in saving those same ``IGNORE`` states; without having to worry about an outcomes subgraph having started to execute. After this completes scheduling will resume and the nodes marked ``IGNORE`` will not be executed by current & future scheduling decisions (and the engine will continue scheduling and completing atoms and all will be merry...). Retries ####### Conditionals have another an interesting interaction with `retry`_ logic, in that when a subgraph is retried, we must decide what to do about the prior outcome which may have been traversed and decide if we should allow the prior outcome decision to be altered. This means that when a execution graph is retried it can be possible to alter the gates decision and enter a new subgraph (which may contain its own new set of retry strategies and so-on). To start we will *clear* the outcome of a gate when a retry resets/unwinds the graph that a retry object has *control* over (this will cause the gate to be executed again). It will also be required to flip the ``IGNORED`` state back to the ``PENDING`` state so that the gates contained nodes *could* be rescheduled. The gate would then have to use whatever provided symbol inputs to recalculate its decision and decide on a new outcome (this then could cause a new subgraph to be executed and so-on). This way will allow for the most natural integration with the existing codebase (and is likely what users would expect to happen). .. _engines: http://docs.openstack.org/developer/taskflow/engines.html .. _advances in dataflow programming: http://www.cs.ucf.edu/~dcm/Teaching/COT4810-Spring2011/Literature/DataFlowProgrammingLanguages.pdf .. _inputs and outputs: http://docs.openstack.org/developer/taskflow/inputs_and_outputs.html .. _retry: http://docs.openstack.org/developer/taskflow/atoms.html#retry .. _truthy: https://docs.python.org/release/2.5.2/lib/truth.html .. _compile time: http://docs.openstack.org/developer/taskflow/engines.html#compiling .. _atom: http://docs.openstack.org/developer/taskflow/atoms.html#atom Alternatives ------------ Some of the current and prior research was investigated to understand the different strategies others have done to make this kind of conditionals possible in similar languages and libraries: * http://www.cs.ucf.edu/~dcm/Teaching/COT4810-Spring2011/Literature/DataFlowProgrammingLanguages.pdf * http://paginas.fe.up.pt/~prodei/dsie12/papers/paper_17.pdf * http://dl.acm.org/citation.cfm?id=3885 * And a few others... Various pipelining/workflow like pypi libraries were looked at. None that I could find actually provide conditional execution primitives. A not fully inclusive list: * https://pypi.python.org/pypi/DAGPype * https://github.com/SegFaultAX/graffiti * And a few others... Impact on Existing APIs ----------------------- This will require a new type/s that when encountered can be used to decide which outcome should taken (for now a *gate* type and possibly a *lambda gate* derived class). These new types will be publicly useable types that can be depended upon working as expected (observing and operating by the constraints described above). Security impact --------------- N/A Performance Impact ------------------ N/A Configuration Impact -------------------- N/A Developer Impact ---------------- A new way of using taskflow would be introduced that would hopefully receive usage and mature as taskflow continues to progress, mature and get more innovative usage. Implementation ============== Assignee(s) ----------- Primary assignee: harlowja Milestones ---------- Juno/2 Work Items ---------- * Introduce new gate types. * Connect types into compilation routine. * Connect types into scheduling/runtime routine. * Test like crazy. Incubation ========== N/A Adoption -------- N/A Library ------- N/A Anticipated API Stabilization ----------------------------- N/A Documentation Impact ==================== New developer docs explaining the concepts, how to use this and examples will be provided and updated accordingly. Dependencies ============ None References ========== * Brainstorm: https://etherpad.openstack.org/p/BrainstormFlowConditions * Prototype: https://review.openstack.org/#/c/87417/ .. note:: This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ",0,1679
openstack%2Fproject-config~master~I8a2c1621a8326d1a5c2049065058a2bd699b9194,openstack/project-config,master,I8a2c1621a8326d1a5c2049065058a2bd699b9194,Perform merge-check jobs at low precedence,MERGED,2014-09-26 01:15:37.000000000,2014-09-26 13:39:19.000000000,2014-09-26 13:39:19.000000000,"[{'_account_id': 1}, {'_account_id': 3}, {'_account_id': 5263}, {'_account_id': 6316}, {'_account_id': 6482}, {'_account_id': 6547}, {'_account_id': 7069}]","[{'number': 1, 'created': '2014-09-26 01:15:37.000000000', 'files': ['zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/f150f7a30be847084f0216a7a7f99ee1f55c5296', 'message': 'Perform merge-check jobs at low precedence\n\nNow that Zuul supports precedence passthrough to the merge-check\npipeline, merge-check pseudo-jobs should yield to more important\npipelines when necessary.\n\nChange-Id: I8a2c1621a8326d1a5c2049065058a2bd699b9194\n'}]",0,124241,f150f7a30be847084f0216a7a7f99ee1f55c5296,11,7,1,5263,,,0,"Perform merge-check jobs at low precedence

Now that Zuul supports precedence passthrough to the merge-check
pipeline, merge-check pseudo-jobs should yield to more important
pipelines when necessary.

Change-Id: I8a2c1621a8326d1a5c2049065058a2bd699b9194
",git fetch https://review.opendev.org/openstack/project-config refs/changes/41/124241/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul/layout.yaml'],1,f150f7a30be847084f0216a7a7f99ee1f55c5296,merge-low, precedence: low,,1,0
openstack%2Fpuppet-openstacklib~master~I1cd7765cdcbddb7e7ad5d720f1efa382641712f2,openstack/puppet-openstacklib,master,I1cd7765cdcbddb7e7ad5d720f1efa382641712f2,policy.json: Allow puppet modules to manage policy.json,MERGED,2014-09-19 20:54:21.000000000,2014-09-26 13:39:13.000000000,2014-09-26 13:39:12.000000000,"[{'_account_id': 3}, {'_account_id': 1607}, {'_account_id': 3153}, {'_account_id': 6994}, {'_account_id': 7155}, {'_account_id': 7616}, {'_account_id': 8482}]","[{'number': 1, 'created': '2014-09-19 20:54:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-openstacklib/commit/ba433b98ec25d65e99e27857a82848c665a0c763', 'message': 'policy.json: Allow puppet modules to manage policy.json\n\nCurrently puppet modules does not allow one to manage policy.json.\nThis commit aims to create a common resource for people to manage\ntheir policies.\n\nChange-Id: I1cd7765cdcbddb7e7ad5d720f1efa382641712f2\n'}, {'number': 2, 'created': '2014-09-19 21:00:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-openstacklib/commit/7b9238c01720493e9111008bd3149979345e1437', 'message': 'policy.json: Allow puppet modules to manage policy.json\n\nCurrently puppet modules does not allow one to manage policy.json.\nThis commit aims to create a common resource for people to manage\ntheir policies.\n\nChange-Id: I1cd7765cdcbddb7e7ad5d720f1efa382641712f2\n'}, {'number': 3, 'created': '2014-09-22 12:46:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-openstacklib/commit/2485368d340565ff1cda4fd5124fac7c74eff520', 'message': 'policy.json: Allow puppet modules to manage policy.json\n\nCurrently puppet modules does not allow one to manage policy.json.\nThis commit aims to create a common resource for people to manage\ntheir policies.\n\nChange-Id: I1cd7765cdcbddb7e7ad5d720f1efa382641712f2\n'}, {'number': 4, 'created': '2014-09-22 13:11:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-openstacklib/commit/c7aca09fe187f4a85746fe614cf781d8a8a2dc89', 'message': 'policy.json: Allow puppet modules to manage policy.json\n\nCurrently puppet modules does not allow one to manage policy.json.\nThis commit aims to create a common resource for people to manage\ntheir policies.\n\nChange-Id: I1cd7765cdcbddb7e7ad5d720f1efa382641712f2\n'}, {'number': 5, 'created': '2014-09-22 14:22:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-openstacklib/commit/adf8b367e0c123c38ff05a38af3e35059290dcbe', 'message': 'policy.json: Allow puppet modules to manage policy.json\n\nCurrently puppet modules does not allow one to manage policy.json.\nThis commit aims to create a common resource for people to manage\ntheir policies.\n\nChange-Id: I1cd7765cdcbddb7e7ad5d720f1efa382641712f2\n'}, {'number': 6, 'created': '2014-09-25 12:42:22.000000000', 'files': ['spec/classes/openstacklib_policy_spec.rb', 'spec/defines/openstacklib_policy_spec.rb', 'manifests/policy/base.pp', 'manifests/policy.pp'], 'web_link': 'https://opendev.org/openstack/puppet-openstacklib/commit/37935472f1505abe6ee31dcb27b5f364201a53e9', 'message': 'policy.json: Allow puppet modules to manage policy.json\n\nCurrently puppet modules does not allow one to manage policy.json.\nThis commit aims to create a common resource for people to manage\ntheir policies.\n\nChange-Id: I1cd7765cdcbddb7e7ad5d720f1efa382641712f2\n'}]",2,122859,37935472f1505abe6ee31dcb27b5f364201a53e9,21,7,6,9410,,,0,"policy.json: Allow puppet modules to manage policy.json

Currently puppet modules does not allow one to manage policy.json.
This commit aims to create a common resource for people to manage
their policies.

Change-Id: I1cd7765cdcbddb7e7ad5d720f1efa382641712f2
",git fetch https://review.opendev.org/openstack/puppet-openstacklib refs/changes/59/122859/4 && git format-patch -1 --stdout FETCH_HEAD,"['spec/defines/openstacklib_policy_spec.rb', 'manifests/policies.pp', 'manifests/policy.pp']",3,ba433b98ec25d65e99e27857a82848c665a0c763,,"# == Definition: openstacklib::policy # # This resource configures the policy.json file for an OpenStack service # # == Parameters: # # [*file_path*] # Path to the policy.json file # string; required # # [*key*] # The key to replace the value for # string; required; the key to replace the value for # # [*value*] # The value to set # string; optional; the valye to set # define openstacklib::policy ( $file_path, $key, $value = '', ) { augeas { ""${file_path}-${key}-${value}"" : lens => 'Json.lns', incl => $file_path, changes => ""set dict/entry[*][.=\""${key}\""]/string ${value}"" } } ",,65,0
openstack%2Fswift-specs~master~Iee14588d544b8e821c0661fa1c043a0bca5d2021,openstack/swift-specs,master,Iee14588d544b8e821c0661fa1c043a0bca5d2021,Initial draft for high level EC Get flow,MERGED,2014-08-28 12:00:18.000000000,2014-09-26 13:36:19.000000000,2014-09-26 13:36:19.000000000,"[{'_account_id': 3}, {'_account_id': 7479}]","[{'number': 1, 'created': '2014-08-28 12:00:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/63729c9195e7b905402e1b8c8521ba1c85f38d25', 'message': 'Initial draft for high level EC Get flow\n\nChange-Id: Iee14588d544b8e821c0661fa1c043a0bca5d2021\n'}, {'number': 2, 'created': '2014-09-26 06:03:58.000000000', 'files': ['specs/swift/erasure_coding.rst'], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/5078527c9cc03a13dd5f90f9cfbf2a32489ddf0f', 'message': 'Initial draft for high level EC Get flow\n\nChange-Id: Iee14588d544b8e821c0661fa1c043a0bca5d2021\n'}]",2,117492,5078527c9cc03a13dd5f90f9cfbf2a32489ddf0f,10,2,2,5189,,,0,"Initial draft for high level EC Get flow

Change-Id: Iee14588d544b8e821c0661fa1c043a0bca5d2021
",git fetch https://review.opendev.org/openstack/swift-specs refs/changes/92/117492/2 && git format-patch -1 --stdout FETCH_HEAD,['specs/swift/erasure_coding.rst'],1,63729c9195e7b905402e1b8c8521ba1c85f38d25,ec_get," #. Proxy opens (ec_k + ec_m) backend requests to object servers #. Proxy validates the number of successful connections >= ec_k and also the avaiable fragment archives are the same version via response header value, proxy would continue searching from the hand-off nodes (ec_k + ec_m) if not enough data found #. Proxy reads from the first ec_k fragment archives concurrently #. Proxy buffers the content up-to a minimum segment size list, each each element is 1MB/ec_k #. Proxy feeds the assembled list to PyECLib's decode() to get the original content #. Proxy sends the original content to Client #. Proxy then continues with the next list of contents",,12,0
openstack%2Ftripleo-image-elements~master~I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d,openstack/tripleo-image-elements,master,I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d,Add swift.proxy-memcache metadata to proxy-server.conf,MERGED,2014-04-09 12:53:11.000000000,2014-09-26 13:27:48.000000000,2014-09-26 13:27:47.000000000,"[{'_account_id': 3}, {'_account_id': 215}, {'_account_id': 1005}, {'_account_id': 1253}, {'_account_id': 4190}, {'_account_id': 7144}, {'_account_id': 7582}]","[{'number': 1, 'created': '2014-04-09 12:53:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/c5ade8639ee0ad8b9665862e71ff0510996b04d3', 'message': ""Add swift.proxy-memcache metadata to proxy-server.conf\n\nWe're building a list of proxies to add to the memcache stanza\nin /etc/swift/proxy-server.conf. The list is built as metadata\nin the swift-source and swift-storage-source heat yaml templates.\nThis element change will consume the metadata if it exists.\n\nChange-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d\n""}, {'number': 2, 'created': '2014-04-14 12:52:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/d61d1364b9475d8c055cf3aabb87362d380d37d0', 'message': ""Add swift.proxy-memcache metadata to proxy-server.conf\n\nWe're building a list of proxies to add to the memcache stanza\nin /etc/swift/proxy-server.conf. The list is built as metadata\nin the swift-source and swift-storage-source heat yaml templates.\nThis element change will consume the metadata if it exists.\n\nChange-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d\n""}, {'number': 3, 'created': '2014-05-09 14:02:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/0894428ae26f8e5114bf268797b9f8e7c4094f39', 'message': 'Add swift.proxy-memcache metadata to proxy-server.conf\n\nWe\'re building a list of proxies to add to the memcache stanza\nin /etc/swift/proxy-server.conf. The list is built as metadata\nin the swift-source and swift-storage-source heat yaml templates.\nThis element change will consume the metadata if it exists.\n\nIn addition to the swift-proxy element change, we have to change\nthe memcached element to set ""-l <local swift server ip>"" and\nrestart memcached to enable memcache connections.\n\nChange-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d\n'}, {'number': 4, 'created': '2014-07-09 13:46:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/cfe186ac2b271585f80c89ea9463073f0e2edd0b', 'message': 'Add swift.proxy-memcache metadata to proxy-server.conf\n\nWe\'re building a list of proxies to add to the memcache stanza\nin /etc/swift/proxy-server.conf. The list is built as metadata\nin the swift-source and swift-storage-source heat yaml templates.\nThis element change will consume the metadata if it exists.\n\nIn addition to the swift-proxy element change, we have to change\nthe memcached element to set ""-l <local swift server ip>"" in\n/etc/memcached.config on debian-based systems, or set\nOPTIONS=""-l <local swift server ip>"" in /etc/sysconfig/memcached\non redhat-based systems, and then restart memcached to enable\nmemcache connections.\n\nChange-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d\n'}, {'number': 5, 'created': '2014-09-09 16:15:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/195248e20721fa28a350afce2bc306bfd45dc6ef', 'message': 'Add swift.proxy-memcache metadata to proxy-server.conf\n\nWe\'re building a list of proxies to add to the memcache stanza\nin /etc/swift/proxy-server.conf. The list is built as metadata\nin the swift-source and swift-storage-source heat yaml templates.\nThis element change will consume the metadata if it exists.\n\nIn addition to the swift-proxy element change, we have to change\nthe memcached element to set ""-l <local swift server ip>"" in\n/etc/memcached.config on debian-based systems, or set\nOPTIONS=""-l <local swift server ip>"" in /etc/sysconfig/memcached\non redhat-based systems, and then restart memcached to enable\nmemcache connections.\n\nChange-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d\n'}, {'number': 6, 'created': '2014-09-19 14:47:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/4d71f685fa29026bd61660de20e6f089d0c4122f', 'message': 'Add swift.proxy-memcache metadata to proxy-server.conf\n\nWe\'re building a list of proxies to add to the memcache stanza\nin /etc/swift/proxy-server.conf. The list is built as metadata\nin the swift-source and swift-storage-source heat yaml templates.\nThis element change will consume the metadata if it exists.\n\nIn addition to the swift-proxy element change, we have to change\nthe memcached element to set ""-l <local swift server ip>"" in\n/etc/memcached.config on debian-based systems and then restart\nmemcached to enable memcache connections. Otherwise we get\nmemcache connection errors in the proxy logs.  This change is\nrequired only for debian-based systems; RedHat-based systems work\nfine without it.\n\nChange-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d\n'}, {'number': 7, 'created': '2014-09-24 09:11:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/7a8c0c6b11a221b877b31c7caf89c775f69627db', 'message': 'Add swift.proxy-memcache metadata to proxy-server.conf\n\nWe\'re building a list of proxies to add to the memcache stanza\nin /etc/swift/proxy-server.conf. The list is built as metadata\nin the swift-source and swift-storage-source heat yaml templates.\nThis element change will consume the metadata if it exists.\n\nIn addition to the swift-proxy element change, we have to change\nthe memcached element to set ""-l 0.0.0.0"" in /etc/memcached.config\non debian-based systems at install-time to enable memcache\nconnections.  Otherwise we get memcache connection errors in the\nproxy logs.  This change is required only for debian-based systems;\nRedHat-based systems have the equivalent setting by default.\n\nChange-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d\n'}, {'number': 8, 'created': '2014-09-25 10:20:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/4233e1e9ea168250fb585e9a9baab7a63414ba07', 'message': ""Add swift.proxy-memcache metadata to proxy-server.conf\n\nWe're building a list of proxies to add to the memcache stanza\nin /etc/swift/proxy-server.conf. The list is built as metadata\nin the swift-source and swift-storage-source heat yaml templates.\nThis element change will consume the metadata if it exists.\n\nChange-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d\n""}, {'number': 9, 'created': '2014-09-25 13:18:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/205f4231174e2d4bfc5b8c909fd5a6fe7319dcf3', 'message': ""Add swift.proxy-memcache metadata to proxy-server.conf\n\nWe're building a list of proxies to add to the memcache stanza\nin /etc/swift/proxy-server.conf. The list is built as metadata\nin the swift-source and swift-storage-source heat yaml templates.\nThis element change will consume the metadata if it exists.\n\nChange-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d\n""}, {'number': 10, 'created': '2014-09-25 13:24:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/ed3241b49e24704666e6928c62fa2b022dbd8a70', 'message': ""Add swift.proxy-memcache metadata to proxy-server.conf\n\nWe're building a list of proxies to add to the memcache stanza\nin /etc/swift/proxy-server.conf. The list is built as metadata\nin the swift-source and swift-storage-source heat yaml templates.\nThis element change will consume the metadata if it exists.\n\nChange-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d\n""}, {'number': 11, 'created': '2014-09-25 13:27:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/1f4a35c6dbe53d08c3e587d7b283e6851e8d998e', 'message': ""Add swift.proxy-memcache metadata to proxy-server.conf\n\nWe're building a list of proxies to add to the memcache stanza\nin /etc/swift/proxy-server.conf. The list is built as metadata\nin the swift-source and swift-storage-source heat yaml templates.\nThis element change will consume the metadata if it exists.\n\nChange-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d\n""}, {'number': 12, 'created': '2014-09-25 13:48:37.000000000', 'files': ['elements/swift-proxy/os-apply-config/etc/swift/proxy-server.conf', 'elements/swift-proxy/README.md'], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/4e038e1f3ba3631e6eaeee842b9b52b8015a5208', 'message': ""Add swift.proxy-memcache metadata to proxy-server.conf\n\nWe're building a list of proxies to add to the memcache stanza\nin /etc/swift/proxy-server.conf. The list is built as metadata\nin the swift-source and swift-storage-source heat yaml templates.\nThis element change will consume the metadata if it exists.\n\nChange-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d\n""}]",11,86312,4e038e1f3ba3631e6eaeee842b9b52b8015a5208,79,7,12,1005,,,0,"Add swift.proxy-memcache metadata to proxy-server.conf

We're building a list of proxies to add to the memcache stanza
in /etc/swift/proxy-server.conf. The list is built as metadata
in the swift-source and swift-storage-source heat yaml templates.
This element change will consume the metadata if it exists.

Change-Id: I0e6b68086246dd731ac2e9a868e63eedb3ac3d6d
",git fetch https://review.opendev.org/openstack/tripleo-image-elements refs/changes/12/86312/4 && git format-patch -1 --stdout FETCH_HEAD,['elements/swift-proxy/os-config-applier/etc/swift/proxy-server.conf'],1,c5ade8639ee0ad8b9665862e71ff0510996b04d3,swift-memcache-metadata,{{#swift.proxy-memcache}} memcache_servers = {{swift.proxy-memcache}} {{/swift.proxy-memcache}},,3,0
openstack%2Ftraining-guides~master~I2489563385aa08e6fd1ab3db3a5cbcbf97e8d0a0,openstack/training-guides,master,I2489563385aa08e6fd1ab3db3a5cbcbf97e8d0a0,Updated from openstack-manuals,MERGED,2014-09-26 06:16:15.000000000,2014-09-26 13:27:21.000000000,2014-09-26 13:27:21.000000000,"[{'_account_id': 3}, {'_account_id': 11889}]","[{'number': 1, 'created': '2014-09-26 06:16:15.000000000', 'files': ['doc/training-guides/openstack.ent'], 'web_link': 'https://opendev.org/openstack/training-guides/commit/d4cfb9d5eb8ad8c02298b0c6a7ae1f456724fb0d', 'message': 'Updated from openstack-manuals\n\nChange-Id: I2489563385aa08e6fd1ab3db3a5cbcbf97e8d0a0\n'}]",0,124299,d4cfb9d5eb8ad8c02298b0c6a7ae1f456724fb0d,6,2,1,11131,,,0,"Updated from openstack-manuals

Change-Id: I2489563385aa08e6fd1ab3db3a5cbcbf97e8d0a0
",git fetch https://review.opendev.org/openstack/training-guides refs/changes/99/124299/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/training-guides/openstack.ent'],1,d4cfb9d5eb8ad8c02298b0c6a7ae1f456724fb0d,openstack/openstack-manuals,"<!-- The master of this file is in openstack-manuals repository, file doc/common/entities/openstack.ent. Any changes to the master file will override changs in other repositories. --><!ENTITY hellip ""&#133;"">",,5,1
openstack%2Ftripleo-image-elements~master~Ie8837cffdff7d384cabf5deff7811eebd4968e2c,openstack/tripleo-image-elements,master,Ie8837cffdff7d384cabf5deff7811eebd4968e2c,Fix memcache listen port setting on debian-based OSes,MERGED,2014-09-25 10:20:58.000000000,2014-09-26 13:26:49.000000000,2014-09-26 13:26:48.000000000,"[{'_account_id': 3}, {'_account_id': 215}, {'_account_id': 1005}, {'_account_id': 7144}, {'_account_id': 7471}, {'_account_id': 7582}]","[{'number': 1, 'created': '2014-09-25 10:20:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/677c8dc8dfd1f4c20c0f536ce4d9badfdbc2c08a', 'message': 'Fix memcache listen port setting on debian-based OSes\n\nOn debian-based systems, we need to set ""-l 0.0.0.0"" in\n/etc/memcached.conf.  This is to avoid memcached\nconnection errors, for instance when running the Swift\nproxy code. RedHat-based systems already have this\nsetting by default.\n\nChange-Id: Ie8837cffdff7d384cabf5deff7811eebd4968e2c\n'}, {'number': 2, 'created': '2014-09-25 13:18:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/5b776e288c3432a7a3e27269c6577240fe03b35f', 'message': 'Fix memcache listen port setting on debian-based OSes\n\nOn debian-based systems, we need to set ""-l 0.0.0.0"" in\n/etc/memcached.conf.  This is to avoid memcached\nconnection errors, for instance when running the Swift\nproxy code. RedHat-based systems already have this\nsetting by default.\n\nChange-Id: Ie8837cffdff7d384cabf5deff7811eebd4968e2c\n'}, {'number': 3, 'created': '2014-09-25 13:46:04.000000000', 'files': ['elements/memcached/README.md', 'elements/memcached/install.d/74-memcached'], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/834d5aa6137ac518b80183ddae6194d07e389106', 'message': 'Fix memcache listen port setting on debian-based OSes\n\nOn debian-based systems, we need to set ""-l 0.0.0.0"" in\n/etc/memcached.conf.  This is to avoid memcached\nconnection errors, for instance when running the Swift\nproxy code. RedHat-based systems already have this\nsetting by default.\n\nChange-Id: Ie8837cffdff7d384cabf5deff7811eebd4968e2c\n'}]",1,123999,834d5aa6137ac518b80183ddae6194d07e389106,20,6,3,1005,,,0,"Fix memcache listen port setting on debian-based OSes

On debian-based systems, we need to set ""-l 0.0.0.0"" in
/etc/memcached.conf.  This is to avoid memcached
connection errors, for instance when running the Swift
proxy code. RedHat-based systems already have this
setting by default.

Change-Id: Ie8837cffdff7d384cabf5deff7811eebd4968e2c
",git fetch https://review.opendev.org/openstack/tripleo-image-elements refs/changes/99/123999/1 && git format-patch -1 --stdout FETCH_HEAD,"['elements/memcached/README.md', 'elements/memcached/install.d/74-memcached']",2,677c8dc8dfd1f4c20c0f536ce4d9badfdbc2c08a,memcache-listen-port," # On debian-based distros set memcached daemon to listen on port 0.0.0.0 # This is the default on RedHat-based distros. # Without this setting the memcached daemon will suffer connection errors if [ -f /etc/debian_version ]; then sed -i -r ""s/\-l( )*(([0-9]{1,3}\.){3}[0-9]{1,3})/\-l 0\.0\.0\.0/"" /etc/memcached.conf fi ",,13,0
openstack%2Fnova~master~Ia5519b84bc47ea1e0a5a70f9368806f762d12db5,openstack/nova,master,Ia5519b84bc47ea1e0a5a70f9368806f762d12db5,VMware: Make DatastorePath hashable,MERGED,2014-09-24 10:24:34.000000000,2014-09-26 12:35:42.000000000,2014-09-26 12:35:40.000000000,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9555}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-24 10:24:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ecebf597417eff33044611455c63505a03fc6232', 'message': 'VMware: Make DatastorePath hashable\n\nMake DatastorePath work properly when used as a key value in a dict or set.\n\nChange-Id: Ia5519b84bc47ea1e0a5a70f9368806f762d12db5\n'}, {'number': 2, 'created': '2014-09-25 15:09:26.000000000', 'files': ['nova/virt/vmwareapi/ds_util.py', 'nova/tests/virt/vmwareapi/test_ds_util.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/43e2069cc9dea194da03d63d34d68e7a79686375', 'message': ""VMware: Make DatastorePath hashable\n\nMake DatastorePath work properly when used as a key value in a dict or\nset.\n\nThis change specifically enables a subsequent change in the fake\ndriver, which stores DatastorePath objects in a set. However, it's\nprobably a good idea in general as I spent quite some time trying to\nwork out why it didn't work when I hit it.\n\nChange-Id: Ia5519b84bc47ea1e0a5a70f9368806f762d12db5\n""}]",0,123680,43e2069cc9dea194da03d63d34d68e7a79686375,17,7,2,9555,,,0,"VMware: Make DatastorePath hashable

Make DatastorePath work properly when used as a key value in a dict or
set.

This change specifically enables a subsequent change in the fake
driver, which stores DatastorePath objects in a set. However, it's
probably a good idea in general as I spent quite some time trying to
work out why it didn't work when I hit it.

Change-Id: Ia5519b84bc47ea1e0a5a70f9368806f762d12db5
",git fetch https://review.opendev.org/openstack/nova refs/changes/80/123680/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/virt/vmwareapi/ds_util.py'],1,ecebf597417eff33044611455c63505a03fc6232,vmware_better_fake, def __hash__(self): return str(self).__hash__() ,,3,0
openstack%2Fnova~master~Ie6029896edb51c613bd53b5441df4825588b2334,openstack/nova,master,Ie6029896edb51c613bd53b5441df4825588b2334,VMware: trivial formatting fix in fake driver,MERGED,2014-09-24 10:24:34.000000000,2014-09-26 12:30:20.000000000,2014-09-26 12:30:17.000000000,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-24 10:24:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/7c9a0006cf3c9f806d7f09c3673da3aacae981c2', 'message': 'VMware: trivial formatting fix in fake driver\n\nChange-Id: Ie6029896edb51c613bd53b5441df4825588b2334\n'}, {'number': 2, 'created': '2014-09-25 15:09:26.000000000', 'files': ['nova/tests/virt/vmwareapi/fake.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/e6f456a48d77ab97b57eeb1a6c64abaa9a230189', 'message': 'VMware: trivial formatting fix in fake driver\n\nChange-Id: Ie6029896edb51c613bd53b5441df4825588b2334\n'}]",0,123683,e6f456a48d77ab97b57eeb1a6c64abaa9a230189,16,6,2,9555,,,0,"VMware: trivial formatting fix in fake driver

Change-Id: Ie6029896edb51c613bd53b5441df4825588b2334
",git fetch https://review.opendev.org/openstack/nova refs/changes/83/123683/2 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/virt/vmwareapi/fake.py'],1,7c9a0006cf3c9f806d7f09c3673da3aacae981c2,vmware_better_fake," task_mdo = create_task(method, ""error"", error_fault=FileNotFound())"," task_mdo = create_task(method, ""error"", error_fault=FileNotFound())",1,2
openstack%2Fnova~master~I8ad6dc52c460d11f71c750e8cc287b36d8238106,openstack/nova,master,I8ad6dc52c460d11f71c750e8cc287b36d8238106,VMware: Improve logging of DatastorePath in error messages,MERGED,2014-09-24 10:24:34.000000000,2014-09-26 12:29:55.000000000,2014-09-26 12:29:52.000000000,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9555}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-24 10:24:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a81d710e89b421f41094fb0b9146f1c63abc551e', 'message': 'VMware: Improve logging of DatastorePath in error messages\n\nChange-Id: I8ad6dc52c460d11f71c750e8cc287b36d8238106\n'}, {'number': 2, 'created': '2014-09-25 15:09:26.000000000', 'files': ['nova/virt/vmwareapi/ds_util.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/37561488da1e849b66f95dc71b1e5f521039c013', 'message': 'VMware: Improve logging of DatastorePath in error messages\n\nChange-Id: I8ad6dc52c460d11f71c750e8cc287b36d8238106\n'}]",0,123681,37561488da1e849b66f95dc71b1e5f521039c013,24,7,2,9555,,,0,"VMware: Improve logging of DatastorePath in error messages

Change-Id: I8ad6dc52c460d11f71c750e8cc287b36d8238106
",git fetch https://review.opendev.org/openstack/nova refs/changes/81/123681/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/virt/vmwareapi/ds_util.py'],1,a81d710e89b421f41094fb0b9146f1c63abc551e,vmware_better_fake," def __repr__(self): return ""%s(%s, %s)"" % (self.__class__.__name__, self.datastore, self.rel_path) ",,4,0
openstack%2Fnova~master~I71188ec7c12d51c2b4a253f13a5f51cc8bc0ae36,openstack/nova,master,I71188ec7c12d51c2b4a253f13a5f51cc8bc0ae36,VMware: fix broken mock of ds_util.mkdir,MERGED,2014-09-18 14:26:35.000000000,2014-09-26 12:29:29.000000000,2014-09-26 12:29:27.000000000,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 100}, {'_account_id': 1063}, {'_account_id': 1653}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9550}, {'_account_id': 9555}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-18 14:26:35.000000000', 'files': ['nova/tests/virt/vmwareapi/test_vmops.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/4d59e1f4398285c6fb635286e0362b35b704dbb9', 'message': 'VMware: fix broken mock of ds_util.mkdir\n\nChange I788e33dbcb3dedc41831b976137607274b1c02ca introduced an\nincorrect use of mock which had side-effects for other tests. This\nchange fixes it.\n\nChange-Id: I71188ec7c12d51c2b4a253f13a5f51cc8bc0ae36\n'}]",0,122432,4d59e1f4398285c6fb635286e0362b35b704dbb9,24,11,1,9555,,,0,"VMware: fix broken mock of ds_util.mkdir

Change I788e33dbcb3dedc41831b976137607274b1c02ca introduced an
incorrect use of mock which had side-effects for other tests. This
change fixes it.

Change-Id: I71188ec7c12d51c2b4a253f13a5f51cc8bc0ae36
",git fetch https://review.opendev.org/openstack/nova refs/changes/32/122432/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/virt/vmwareapi/test_vmops.py'],1,4d59e1f4398285c6fb635286e0362b35b704dbb9,vmware_fix_mock," @mock.patch.object(ds_util, 'mkdir') def test_create_folder_if_missing(self, mock_mkdir): mock_mkdir.assert_called_with(ops._session, path, dc) @mock.patch.object(ds_util, 'mkdir') def test_create_folder_if_missing_exception(self, mock_mkdir): mock_mkdir.assert_called_with(ops._session, path, dc)"," ds_util.mkdir = mock.Mock() def test_create_folder_if_missing(self): ds_util.mkdir.assert_called_with(ops._session, path, dc) def test_create_folder_if_missing_exception(self): ds_util.mkdir.assert_called_with(ops._session, path, dc)",6,5
openstack%2Fmistral~master~I25135e32aff528d132f48e8cd6200f4eacca405d,openstack/mistral,master,I25135e32aff528d132f48e8cd6200f4eacca405d,Update requirements due to global requirements (master),MERGED,2014-09-25 09:28:23.000000000,2014-09-26 12:26:17.000000000,2014-09-26 12:26:16.000000000,"[{'_account_id': 3}, {'_account_id': 7700}, {'_account_id': 8592}]","[{'number': 1, 'created': '2014-09-25 09:28:23.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/mistral/commit/ad4795e797061af23bf4159b357f0def328982ba', 'message': 'Update requirements due to global requirements (master)\n\nChange-Id: I25135e32aff528d132f48e8cd6200f4eacca405d\n'}]",0,123985,ad4795e797061af23bf4159b357f0def328982ba,8,3,1,8592,,,0,"Update requirements due to global requirements (master)

Change-Id: I25135e32aff528d132f48e8cd6200f4eacca405d
",git fetch https://review.opendev.org/openstack/mistral refs/changes/85/123985/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,ad4795e797061af23bf4159b357f0def328982ba,,"eventlet>=0.15.0pecan>=0.5.0amqplib>=0.6.1 # This is not in global requirements (master branch)requests>=1.2.1,!=2.4.0oslo.config>=1.4.0 # Apache-2.0 oslo.db>=1.0.0 # Apache-2.0 oslo.messaging>=1.4.0python-keystoneclient>=0.10.0python-novaclient>=2.18.0 python-glanceclient>=0.14.0SQLAlchemy>=0.8.4,<=0.8.99,>=0.9.7,<=0.9.99 stevedore>=1.0.0 # Apache-2.0","eventlet>=0.13.0pecan>=0.4.5amqplib>=0.6.1requests>=1.1oslo.config>=1.2.1 oslo.db>=0.2.0 # Apache-2.0 oslo.messaging>=1.3.0python-keystoneclient>=0.9.0python-novaclient>=2.17 python-glanceclient>=0.13SQLAlchemy>=0.7.8,!=0.9.5,<=0.9.99 stevedore>=0.14",12,12
openstack%2Ftripleo-heat-templates~master~I91667a3e7861591285c931006c448cc304649349,openstack/tripleo-heat-templates,master,I91667a3e7861591285c931006c448cc304649349,undercloud: use Nova compute Ironic driver,MERGED,2014-09-19 14:45:56.000000000,2014-09-26 12:23:13.000000000,2014-09-26 12:23:12.000000000,"[{'_account_id': 3}, {'_account_id': 4330}, {'_account_id': 8399}]","[{'number': 1, 'created': '2014-09-19 14:45:56.000000000', 'files': ['undercloud-vm-ironic-deploy.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/46e724baa074dde2cfb93f6c00dca5e6144909be', 'message': 'undercloud: use Nova compute Ironic driver\n\nUpdates the NovaComputeDriver and NovaSchedulerHostManager\ndefaults so that we use the Nova in-tree versions of\nthe Ironic compute and scheduler host managers.\n\nThe old Ironic in-tree drivers are now deprecated.\n\nChange-Id: I91667a3e7861591285c931006c448cc304649349\n'}]",0,122754,46e724baa074dde2cfb93f6c00dca5e6144909be,9,3,1,360,,,0,"undercloud: use Nova compute Ironic driver

Updates the NovaComputeDriver and NovaSchedulerHostManager
defaults so that we use the Nova in-tree versions of
the Ironic compute and scheduler host managers.

The old Ironic in-tree drivers are now deprecated.

Change-Id: I91667a3e7861591285c931006c448cc304649349
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/54/122754/1 && git format-patch -1 --stdout FETCH_HEAD,['undercloud-vm-ironic-deploy.yaml'],1,46e724baa074dde2cfb93f6c00dca5e6144909be,undercloud_ironic_params, default: nova.virt.ironic.driver.IronicDriver default: nova.scheduler.ironic_host_manager.IronicHostManager, default: ironic.nova.virt.ironic.driver.IronicDriver default: ironic.nova.scheduler.ironic_host_manager.IronicHostManager,2,2
openstack%2Fnova~stable%2Ficehouse~Ibaf8482562094cd2b3165dc62a907fa9e0e56e19,openstack/nova,stable/icehouse,Ibaf8482562094cd2b3165dc62a907fa9e0e56e19,Fixes Hyper-V resize down exception,MERGED,2014-09-10 10:23:20.000000000,2014-09-26 12:21:15.000000000,2014-09-26 12:21:12.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 1420}, {'_account_id': 5170}, {'_account_id': 8213}, {'_account_id': 8543}, {'_account_id': 9656}, {'_account_id': 10635}, {'_account_id': 12582}]","[{'number': 1, 'created': '2014-09-10 10:23:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1386bf49c6a6b7cb9c70d4ea204996d3ecd451fe', 'message': 'Fixes Hyper-V resize down exception\n\nThe Hyper-V driver does not support resize down and is currently\nraising an exception if the user attempts to do that, causing the\ninstance to go in ERROR state.\n\nThe driver should use the recently introduced instance faults\n""exception.InstanceFaultRollback"" instead, which will leave the\ninstance in ACTIVE state as expected.\n\nCloses-Bug: #1354448\n\n(cherry picked from commit 3a5919fd4af6c3b772397a5e7d90eebdf9b371af)\n\nConflicts:\n\tnova/virt/hyperv/migrationops.py\n\nChange-Id: Ibaf8482562094cd2b3165dc62a907fa9e0e56e19\n'}, {'number': 2, 'created': '2014-09-17 14:20:56.000000000', 'files': ['nova/tests/virt/hyperv/test_hypervapi.py', 'nova/virt/hyperv/migrationops.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/0d3dad790515a16b4452d91b6c9ce42094b800c3', 'message': 'Fixes Hyper-V resize down exception\n\nThe Hyper-V driver does not support resize down and is currently\nraising an exception if the user attempts to do that, causing the\ninstance to go in ERROR state.\n\nThe driver should use the recently introduced instance faults\n""exception.InstanceFaultRollback"" instead, which will leave the\ninstance in ACTIVE state as expected.\n\nCloses-Bug: #1354448\n\n(cherry picked from commit 3a5919fd4af6c3b772397a5e7d90eebdf9b371af)\n\nConflicts:\n\tnova/virt/hyperv/migrationops.py\n\nChange-Id: Ibaf8482562094cd2b3165dc62a907fa9e0e56e19\n'}]",0,120371,0d3dad790515a16b4452d91b6c9ce42094b800c3,16,9,2,3185,,,0,"Fixes Hyper-V resize down exception

The Hyper-V driver does not support resize down and is currently
raising an exception if the user attempts to do that, causing the
instance to go in ERROR state.

The driver should use the recently introduced instance faults
""exception.InstanceFaultRollback"" instead, which will leave the
instance in ACTIVE state as expected.

Closes-Bug: #1354448

(cherry picked from commit 3a5919fd4af6c3b772397a5e7d90eebdf9b371af)

Conflicts:
	nova/virt/hyperv/migrationops.py

Change-Id: Ibaf8482562094cd2b3165dc62a907fa9e0e56e19
",git fetch https://review.opendev.org/openstack/nova refs/changes/71/120371/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/hyperv/test_hypervapi.py', 'nova/virt/hyperv/migrationops.py']",2,1386bf49c6a6b7cb9c70d4ea204996d3ecd451fe,icehouse/bug/1354448,"from nova import exception raise exception.InstanceFaultRollback( vmutils.VHDResizeException( _(""Cannot resize the root disk to a smaller size. "" ""Current size: %(curr_root_gb)s GB. Requested size: "" ""%(new_root_gb)s GB"") % {'curr_root_gb': curr_root_gb, 'new_root_gb': new_root_gb}))"," raise vmutils.VHDResizeException( _(""Cannot resize the root disk to a smaller size. Current "" ""size: %(curr_root_gb)s GB. Requested size: "" ""%(new_root_gb)s GB"") % {'curr_root_gb': curr_root_gb, 'new_root_gb': new_root_gb})",9,6
openstack%2Fglance~stable%2Ficehouse~If61894a686135bff3188ca43e8b0f3a748adc391,openstack/glance,stable/icehouse,If61894a686135bff3188ca43e8b0f3a748adc391,Updated from global requirements,MERGED,2014-09-13 13:16:39.000000000,2014-09-26 12:21:02.000000000,2014-09-26 12:21:01.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 9656}]","[{'number': 1, 'created': '2014-09-13 13:16:39.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/glance/commit/85f741f0161d6be854d553e6f356939aaa4a4351', 'message': 'Updated from global requirements\n\nChange-Id: If61894a686135bff3188ca43e8b0f3a748adc391\n'}]",0,121347,85f741f0161d6be854d553e6f356939aaa4a4351,8,3,1,11131,,,0,"Updated from global requirements

Change-Id: If61894a686135bff3188ca43e8b0f3a748adc391
",git fetch https://review.opendev.org/openstack/glance refs/changes/47/121347/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,85f741f0161d6be854d553e6f356939aaa4a4351,openstack/requirements,"sqlalchemy-migrate>=0.8.2,!=0.8.4,!=0.9.2","sqlalchemy-migrate>=0.8.2,!=0.8.4",1,1
openstack%2Fhorizon~master~I7df556295e7e1b5c5e73230b294d4e8f8b7f05f3,openstack/horizon,master,I7df556295e7e1b5c5e73230b294d4e8f8b7f05f3,Making credentials optional for data sources,MERGED,2014-09-05 18:25:37.000000000,2014-09-26 12:20:56.000000000,2014-09-26 12:20:55.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 5623}, {'_account_id': 6786}, {'_account_id': 8090}, {'_account_id': 9981}, {'_account_id': 10670}]","[{'number': 1, 'created': '2014-09-05 18:25:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/1f6a886c6a641838ad5d09d11d6589dc01187ccc', 'message': 'Making credentials optional for data sources\n\nUsername/password credentials are now optional\nfields when creating a data source.  This is\npossible because a fully updated sahara system\nwill use trusts to grant/revoke permissions\nrather than passing around a username/password.\n\nChange-Id: I7df556295e7e1b5c5e73230b294d4e8f8b7f05f3\nImplements: blueprint data-processing-swift-trust-auth\n'}, {'number': 2, 'created': '2014-09-09 18:29:54.000000000', 'files': ['openstack_dashboard/dashboards/project/data_processing/data_sources/workflows/create.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/6c9b164c0d5aa444cce9caa158d713807da5bc2d', 'message': 'Making credentials optional for data sources\n\nUsername/password credentials are now optional\nfields when creating a data source.  This is\npossible because a fully updated sahara system\nwill use trusts to grant/revoke permissions\nrather than passing around a username/password.\n\nChange-Id: I7df556295e7e1b5c5e73230b294d4e8f8b7f05f3\nCloses-Bug: #1367394\n'}]",0,119439,6c9b164c0d5aa444cce9caa158d713807da5bc2d,18,7,2,8090,,,0,"Making credentials optional for data sources

Username/password credentials are now optional
fields when creating a data source.  This is
possible because a fully updated sahara system
will use trusts to grant/revoke permissions
rather than passing around a username/password.

Change-Id: I7df556295e7e1b5c5e73230b294d4e8f8b7f05f3
Closes-Bug: #1367394
",git fetch https://review.opendev.org/openstack/horizon refs/changes/39/119439/2 && git format-patch -1 --stdout FETCH_HEAD,['openstack_dashboard/dashboards/project/data_processing/data_sources/workflows/create.py'],1,1f6a886c6a641838ad5d09d11d6589dc01187ccc,bp/data-processing-swift-trust-auth," data_source_credential_user = forms.CharField(label=_(""Source username""), required=False) label=_(""Source password""), required=False)"," data_source_credential_user = forms.CharField(label=_(""Source username"")) label=_(""Source password"")) def clean(self): cleaned_data = super(workflows.Action, self).clean() ds_type = cleaned_data.get(""data_source_type"", """") if ds_type == ""hdfs"": if ""data_source_credential_user"" in self._errors: del self._errors[""data_source_credential_user""] if ""data_source_credential_pass"" in self._errors: del self._errors[""data_source_credential_pass""] return cleaned_data ",4,14
openstack%2Fhorizon~master~Idb615f4e2830ea5d9ddd0f871bfb3d90854f45f5,openstack/horizon,master,Idb615f4e2830ea5d9ddd0f871bfb3d90854f45f5,Fix concatenation in data processing actions,MERGED,2014-09-25 12:47:45.000000000,2014-09-26 12:20:44.000000000,2014-09-26 12:20:43.000000000,"[{'_account_id': 3}, {'_account_id': 2455}, {'_account_id': 5623}, {'_account_id': 9622}, {'_account_id': 11592}, {'_account_id': 11881}]","[{'number': 1, 'created': '2014-09-25 12:47:45.000000000', 'files': ['openstack_dashboard/dashboards/project/data_processing/jobs/tables.py', 'openstack_dashboard/dashboards/project/data_processing/data_image_registry/tables.py', 'openstack_dashboard/dashboards/project/data_processing/data_sources/tables.py', 'openstack_dashboard/dashboards/project/data_processing/cluster_templates/tables.py', 'openstack_dashboard/dashboards/project/data_processing/job_binaries/tables.py', 'openstack_dashboard/dashboards/project/data_processing/nodegroup_templates/tables.py', 'openstack_dashboard/dashboards/project/data_processing/clusters/tables.py', 'openstack_dashboard/dashboards/project/data_processing/job_executions/tables.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/6a7791f6366abb574ed5b673cb926c4811f7270e', 'message': 'Fix concatenation in data processing actions\n\nRemove concatenation and pluralization issues from Delete Job,\nLaunch Job (incl Launch Job, Launch New Cluster, Choose Plugin,\nRelaunch, Relaunch new cluster), Delete Job binary,\nDelete Data source, Delete Template, Delete Job execution,\nDelete Cluster, Unregister Image, and Delete Template actions.\n\nChange-Id: Idb615f4e2830ea5d9ddd0f871bfb3d90854f45f5\npartial-bug: 1307476\n'}]",0,124041,6a7791f6366abb574ed5b673cb926c4811f7270e,11,6,1,9981,,,0,"Fix concatenation in data processing actions

Remove concatenation and pluralization issues from Delete Job,
Launch Job (incl Launch Job, Launch New Cluster, Choose Plugin,
Relaunch, Relaunch new cluster), Delete Job binary,
Delete Data source, Delete Template, Delete Job execution,
Delete Cluster, Unregister Image, and Delete Template actions.

Change-Id: Idb615f4e2830ea5d9ddd0f871bfb3d90854f45f5
partial-bug: 1307476
",git fetch https://review.opendev.org/openstack/horizon refs/changes/41/124041/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/data_processing/jobs/tables.py', 'openstack_dashboard/dashboards/project/data_processing/data_image_registry/tables.py', 'openstack_dashboard/dashboards/project/data_processing/data_sources/tables.py', 'openstack_dashboard/dashboards/project/data_processing/cluster_templates/tables.py', 'openstack_dashboard/dashboards/project/data_processing/job_binaries/tables.py', 'openstack_dashboard/dashboards/project/data_processing/nodegroup_templates/tables.py', 'openstack_dashboard/dashboards/project/data_processing/clusters/tables.py', 'openstack_dashboard/dashboards/project/data_processing/job_executions/tables.py']",8,6a7791f6366abb574ed5b673cb926c4811f7270e,bug/1307476,"from django.utils.translation import ungettext_lazy @staticmethod def action_present(count): return ungettext_lazy( u""Delete Job execution"", u""Delete Job executions"", count ) @staticmethod def action_past(count): return ungettext_lazy( u""Deleted Job execution"", u""Deleted Job executions"", count ) @staticmethod def action_present(count): return ungettext_lazy( u""Launch Job"", u""Launch Jobs"", count ) @staticmethod def action_past(count): return ungettext_lazy( u""Launched Job"", u""Launched Jobs"", count ) @staticmethod def action_present(count): return ungettext_lazy( u""Launch Job"", u""Launch Jobs"", count ) @staticmethod def action_past(count): return ungettext_lazy( u""Launched Job"", u""Launched Jobs"", count ) "," action_present = _(""Delete"") action_past = _(""Deleted"") data_type_singular = _(""Job execution"") data_type_plural = _(""Job executions"") action_present = _(""Launch"") action_past = _(""Launched"") data_type_singular = _(""Job"") data_type_plural = _(""Jobs"") action_present = _(""Launch"") action_past = _(""Launched"") data_type_singular = _(""Job"") data_type_plural = _(""Jobs"")",216,54
openstack%2Fdesignate~stable%2Ficehouse~Idd2a051f5ce298f7cc5327ceb2946cfc30fe68ac,openstack/designate,stable/icehouse,Idd2a051f5ce298f7cc5327ceb2946cfc30fe68ac,Ensure that 404's are returned as JSON,MERGED,2014-09-24 21:45:24.000000000,2014-09-26 11:49:04.000000000,2014-09-26 11:49:03.000000000,"[{'_account_id': 3}, {'_account_id': 741}, {'_account_id': 8005}, {'_account_id': 8094}, {'_account_id': 8099}, {'_account_id': 8130}]","[{'number': 1, 'created': '2014-09-24 21:45:24.000000000', 'files': ['designate/exceptions.py', 'designate/api/v2/controllers/root.py', 'designate/api/v2/__init__.py', 'designate/api/middleware.py'], 'web_link': 'https://opendev.org/openstack/designate/commit/7dfb026d78a999a46bec183db9a130393c929756', 'message': ""Ensure that 404's are returned as JSON\n\nOveride default Pecan 404 behaviour, and return a error from middleware\nlike all other errors\n\nChange-Id: Idd2a051f5ce298f7cc5327ceb2946cfc30fe68ac\nCloses-Bug: #1333347\n(cherry picked from commit 047f94c17859fe424e936c7255812adc372ea9e7)\n""}]",0,123861,7dfb026d78a999a46bec183db9a130393c929756,8,6,1,8005,,,0,"Ensure that 404's are returned as JSON

Overide default Pecan 404 behaviour, and return a error from middleware
like all other errors

Change-Id: Idd2a051f5ce298f7cc5327ceb2946cfc30fe68ac
Closes-Bug: #1333347
(cherry picked from commit 047f94c17859fe424e936c7255812adc372ea9e7)
",git fetch https://review.opendev.org/openstack/designate refs/changes/61/123861/1 && git format-patch -1 --stdout FETCH_HEAD,"['designate/exceptions.py', 'designate/api/v2/controllers/root.py', 'designate/api/v2/__init__.py', 'designate/api/middleware.py']",4,7dfb026d78a999a46bec183db9a130393c929756,bug/1333347," # Log the exception ASAP unless it is a 404 Not Found if not getattr(e, 'expected', False): LOG.exception(e)", # Log the exception ASAP LOG.exception(e),18,3
openstack%2Fdesignate~master~I75e204d5ff44d3dd0100393a98cc2537886eaa05,openstack/designate,master,I75e204d5ff44d3dd0100393a98cc2537886eaa05,Notif handlers:remove double check of event type,MERGED,2014-09-23 16:20:17.000000000,2014-09-26 11:48:55.000000000,2014-09-26 11:48:54.000000000,"[{'_account_id': 3}, {'_account_id': 741}, {'_account_id': 8099}]","[{'number': 1, 'created': '2014-09-23 16:20:17.000000000', 'files': ['designate/tests/test_notification_handler/__init__.py', 'designate/notification_handler/neutron.py', 'designate/notification_handler/nova.py'], 'web_link': 'https://opendev.org/openstack/designate/commit/dcb58846a6359291a29c63fa574ebd5b7062d848', 'message': ""Notif handlers:remove double check of event type\n\nChange Id50a4a42a8dddbffb85b5fbca43f1b7f31447874 made the\ncall to handler's process_notification to happen only on registered\nevent types. There's not need to check the event type inside the\nhandler itself.\n\nAlso remove the unit test because the part of the code that is\ntested is never executed.\n\nChange-Id: I75e204d5ff44d3dd0100393a98cc2537886eaa05\n""}]",0,123500,dcb58846a6359291a29c63fa574ebd5b7062d848,7,3,1,7350,,,0,"Notif handlers:remove double check of event type

Change Id50a4a42a8dddbffb85b5fbca43f1b7f31447874 made the
call to handler's process_notification to happen only on registered
event types. There's not need to check the event type inside the
handler itself.

Also remove the unit test because the part of the code that is
tested is never executed.

Change-Id: I75e204d5ff44d3dd0100393a98cc2537886eaa05
",git fetch https://review.opendev.org/openstack/designate refs/changes/00/123500/1 && git format-patch -1 --stdout FETCH_HEAD,"['designate/tests/test_notification_handler/__init__.py', 'designate/notification_handler/neutron.py', 'designate/notification_handler/nova.py']",3,dcb58846a6359291a29c63fa574ebd5b7062d848,,, else: raise ValueError('NovaFixedHandler received an invalid event type'),0,23
openstack%2Fmurano~master~If949aa28862ab715cde36f11f66926e9985f3ee8,openstack/murano,master,If949aa28862ab715cde36f11f66926e9985f3ee8,Do not check config on pep8,MERGED,2014-09-26 00:17:52.000000000,2014-09-26 11:47:32.000000000,2014-09-26 11:47:32.000000000,"[{'_account_id': 3}, {'_account_id': 7225}, {'_account_id': 7226}, {'_account_id': 7821}, {'_account_id': 11098}]","[{'number': 1, 'created': '2014-09-26 00:17:52.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/murano/commit/3b70e826761a4c1cfab4bbefe81af3527aecbc1f', 'message': 'Do not check config on pep8\n\nThere were a few instances of config check (run by pep8) failers\ncaused by changes in external libraries. Since we do not have any\ncontrol over external libraries and pep8 is a gating job, this\ncommit removes config checks from pep8.\nWe hope that Murano contributors will check config by running\n./tools/config/check_uptodate.sh manually\n\nChange-Id: If949aa28862ab715cde36f11f66926e9985f3ee8\n'}]",0,124232,3b70e826761a4c1cfab4bbefe81af3527aecbc1f,10,5,1,7600,,,0,"Do not check config on pep8

There were a few instances of config check (run by pep8) failers
caused by changes in external libraries. Since we do not have any
control over external libraries and pep8 is a gating job, this
commit removes config checks from pep8.
We hope that Murano contributors will check config by running
./tools/config/check_uptodate.sh manually

Change-Id: If949aa28862ab715cde36f11f66926e9985f3ee8
",git fetch https://review.opendev.org/openstack/murano refs/changes/32/124232/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,3b70e826761a4c1cfab4bbefe81af3527aecbc1f,,, {toxinidir}/tools/config/check_uptodate.sh,0,1
openstack%2Fhorizon~master~I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51,openstack/horizon,master,I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51,Checking session timeout before authentication,MERGED,2014-04-17 09:26:25.000000000,2014-09-26 11:34:52.000000000,2014-09-26 11:34:51.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 5623}, {'_account_id': 6650}, {'_account_id': 7012}, {'_account_id': 7976}, {'_account_id': 8040}, {'_account_id': 8674}, {'_account_id': 8794}, {'_account_id': 8871}, {'_account_id': 8984}, {'_account_id': 8996}, {'_account_id': 9317}, {'_account_id': 9910}, {'_account_id': 9981}, {'_account_id': 10020}, {'_account_id': 10442}]","[{'number': 1, 'created': '2014-04-17 09:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/5441430bcbf483bf662d5bd9c6e322580dc90853', 'message': 'Checking session timeout before authentication.\n\nIf both the keyston token and the session expired, the user was asked to login\ntwice. This is because the session timestamp was written only when a user was\nverified as logged an authenticated.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user loged out\nagain and asked to log in a second time.\n\nWe now check the timestamp validity before authetication validity and force\nback the login page to retreive a new keystone token, avoiding the timeout\nrace condition between session and token validity which was forcing a dual\nlogin.\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n'}, {'number': 2, 'created': '2014-04-17 09:43:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/9a54b883c5ca151adf138a43a24486d4b6ac60fc', 'message': 'Checking session timeout before authentication.\n\nIf both the keyston token and the session expired, the user was asked to login\ntwice. This is because the session timestamp was written only when a user was\nverified as logged an authenticated.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user loged out\nagain and asked to log in a second time.\n\nWe now check the timestamp validity before authetication validity and force\nback the login page to retreive a new keystone token, avoiding the timeout\nrace condition between session and token validity which was forcing a dual\nlogin.\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n'}, {'number': 3, 'created': '2014-06-18 17:10:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/d8503c52293fdf730250923b440f681a81c63ba2', 'message': 'Checking session timeout before authentication.\n\nIf both the keyston token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user loged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthetication validity and force back the login page to retreive a new keystone\ntoken, avoiding the timeout race condition between session and token validity\nwhich was forcing a dual login. A keystone token expiration is now concidered\nas a session timeout too.\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n'}, {'number': 4, 'created': '2014-06-19 09:45:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/5b72c9fa14237c3398dbd29de36494142b8b85c8', 'message': ""Checking session timeout before authentication.\n\nIf both the keyston token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user loged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthetication validity and force back the login page to retreive a new keystone\ntoken, avoiding the timeout race condition between session and token validity\nwhich was forcing a dual login. A keystone token expiration is now concidered\nas a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployement specific. This margin is preset to\none second.\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}, {'number': 5, 'created': '2014-06-19 09:56:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/0e2426c600502179cdbea1bdc8f86467c376a368', 'message': ""Checking session timeout before authentication.\n\nIf both the keystone token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user logged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthentication validity and force back the login page to retrieve a new\nkeystone token, avoiding the timeout race condition between session and token\nvalidity which was forcing a dual login. A keystone token expiration is now\nconsidered as a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployment specific. This margin is preset to\none second.\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}, {'number': 6, 'created': '2014-06-19 13:45:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/a9289412c388a5e4189c90eec70b43956ab99722', 'message': ""Checking session timeout before authentication.\n\nIf both the keystone token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user logged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthentication validity and force back the login page to retrieve a new\nkeystone token, avoiding the timeout race condition between session and token\nvalidity which was forcing a dual login. A keystone token expiration is now\nconsidered as a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployment specific. This margin is preset to\none second.\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}, {'number': 7, 'created': '2014-06-20 14:16:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/7ece44b453068a6b4cb5818c417f34cbe448f014', 'message': ""Checking session timeout before authentication.\n\nIf both the keystone token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user logged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthentication validity and force back the login page to retrieve a new\nkeystone token, avoiding the timeout race condition between session and token\nvalidity which was forcing a dual login. A keystone token expiration is now\nconsidered as a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployment specific. This margin is preset to\none second.\n\nRequires: https://review.openstack.org/101556\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}, {'number': 8, 'created': '2014-06-20 14:52:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/d35650cfc99dd97f9bd95ee54c7cdaf6895e64a0', 'message': ""Checking session timeout before authentication.\n\nIf both the keystone token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user logged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthentication validity and force back the login page to retrieve a new\nkeystone token, avoiding the timeout race condition between session and token\nvalidity which was forcing a dual login. A keystone token expiration is now\nconsidered as a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployment specific. This margin is preset to\none second.\n\nRequires: https://review.openstack.org/101556\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}, {'number': 9, 'created': '2014-06-26 10:20:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/446736189ad226c7deedc7811ff85dc3e104d957', 'message': ""Checking session timeout before authentication.\n\nIf both the keystone token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user logged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthentication validity and force back the login page to retrieve a new\nkeystone token, avoiding the timeout race condition between session and token\nvalidity which was forcing a dual login. A keystone token expiration is now\nconsidered as a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployment specific. This margin is preset to\none second.\n\nRequires: https://review.openstack.org/101556\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}, {'number': 10, 'created': '2014-09-12 16:59:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/192c16ac24bf57463a5093fefea1ddce1220044d', 'message': ""Checking session timeout before authentication\n\nIf both the keystone token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user logged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthentication validity and force back the login page to retrieve a new\nkeystone token, avoiding the timeout race condition between session and token\nvalidity which was forcing a dual login. A keystone token expiration is now\nconsidered as a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployment specific. This margin is preset to\none second.\n\nRequires: https://review.openstack.org/101556\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}, {'number': 11, 'created': '2014-09-18 09:06:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/a999f3dbdd566ff9f8ff86170e269c6fe16de787', 'message': ""Checking session timeout before authentication\n\nIf both the keystone token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user logged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthentication validity and force back the login page to retrieve a new\nkeystone token, avoiding the timeout race condition between session and token\nvalidity which was forcing a dual login. A keystone token expiration is now\nconsidered as a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployment specific. This margin is preset to\none second.\n\nRequires: https://review.openstack.org/101556\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}, {'number': 12, 'created': '2014-09-22 08:57:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/d9d6870adf0685763720fe64bf80b860da95f92f', 'message': ""Checking session timeout before authentication\n\nIf both the keystone token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user logged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthentication validity and force back the login page to retrieve a new\nkeystone token, avoiding the timeout race condition between session and token\nvalidity which was forcing a dual login. A keystone token expiration is now\nconsidered as a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployment specific. This margin is preset to\none second.\n\nRequires: https://review.openstack.org/101556\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}, {'number': 13, 'created': '2014-09-24 08:27:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/e110f7b737b95c4fde5a1775b56f3355ebc18850', 'message': ""Checking session timeout before authentication\n\nIf both the keystone token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user logged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthentication validity and force back the login page to retrieve a new\nkeystone token, avoiding the timeout race condition between session and token\nvalidity which was forcing a dual login. A keystone token expiration is now\nconsidered as a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployment specific. This margin is preset to\nten seconds.\n\nRequires: https://review.openstack.org/101556\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}, {'number': 14, 'created': '2014-09-24 11:49:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/2221f384db994f7a9101b204968ea0e7375fdc02', 'message': ""Checking session timeout before authentication\n\nIf both the keystone token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user logged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthentication validity and force back the login page to retrieve a new\nkeystone token, avoiding the timeout race condition between session and token\nvalidity which was forcing a dual login. A keystone token expiration is now\nconsidered as a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployment specific. This margin is preset to\nten seconds.\n\nRequires: https://review.openstack.org/101556\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}, {'number': 15, 'created': '2014-09-24 11:52:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/0153c1f649029f79c71910820c5b099f9718909d', 'message': ""Checking session timeout before authentication\n\nIf both the keystone token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user logged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthentication validity and force back the login page to retrieve a new\nkeystone token, avoiding the timeout race condition between session and token\nvalidity which was forcing a dual login. A keystone token expiration is now\nconsidered as a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployment specific. This margin is preset to\nten seconds.\n\nRequires: https://review.openstack.org/101556\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}, {'number': 16, 'created': '2014-09-25 13:06:38.000000000', 'files': ['horizon/test/dummy_auth/backend.py', 'horizon/test/tests/middleware.py', 'horizon/test/helpers.py', 'horizon/middleware.py', 'openstack_dashboard/settings.py', 'openstack_dashboard/dashboards/settings/password/tests.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/4824239730cf628df20d28d0e72ec02c4f7139a8', 'message': ""Checking session timeout before authentication\n\nIf both the keystone token and the session expired, the user was asked to login\ntwice. This is because the token expiration was not checked.\nWhen a user had timed out both in session and keystone token validity, the user\nwas asked to log in, then the timestamp was checked, and the user logged out\nagain and asked to log in a second time.\n\nWe now check both the timestamp and keystone token validity before\nauthentication validity and force back the login page to retrieve a new\nkeystone token, avoiding the timeout race condition between session and token\nvalidity which was forcing a dual login. A keystone token expiration is now\nconsidered as a session timeout too.\n\nAlso, a page can start loading while the token is valid, and finish while it's\ninvalid. This was leading to errors during the page loading.\nWe now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining\na margin before which we consider the token as expired.\nThis is a configurable parameter in the django settings because the time a page\ntakes to render is infra and deployment specific. This margin is preset to\nten seconds.\n\nRequires: https://review.openstack.org/101556\n\nCloses-Bug: 1308918\n\nChange-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51\n""}]",37,88220,4824239730cf628df20d28d0e72ec02c4f7139a8,103,17,16,8674,,,0,"Checking session timeout before authentication

If both the keystone token and the session expired, the user was asked to login
twice. This is because the token expiration was not checked.
When a user had timed out both in session and keystone token validity, the user
was asked to log in, then the timestamp was checked, and the user logged out
again and asked to log in a second time.

We now check both the timestamp and keystone token validity before
authentication validity and force back the login page to retrieve a new
keystone token, avoiding the timeout race condition between session and token
validity which was forcing a dual login. A keystone token expiration is now
considered as a session timeout too.

Also, a page can start loading while the token is valid, and finish while it's
invalid. This was leading to errors during the page loading.
We now set a TOKEN_TIMEOUT_MARGIN period in seconds which allows defining
a margin before which we consider the token as expired.
This is a configurable parameter in the django settings because the time a page
takes to render is infra and deployment specific. This margin is preset to
ten seconds.

Requires: https://review.openstack.org/101556

Closes-Bug: 1308918

Change-Id: I0bf0d079a9dc000c1a30f0e20dcaa03b22d63e51
",git fetch https://review.opendev.org/openstack/horizon refs/changes/20/88220/2 && git format-patch -1 --stdout FETCH_HEAD,['horizon/middleware.py'],1,5441430bcbf483bf662d5bd9c6e322580dc90853,bug/1308918," # Check last activity before valid authentication to prevent race # condition between keystone token expiration and session timeout which # would lead a user to log back in twice in a row. if (isinstance(last_activity, int) and (timestamp - last_activity) > timeout): request.session.pop('last_activity') response = HttpResponseRedirect( '%s?next=%s' % (settings.LOGOUT_URL, request.path)) self.logout_reason = _(""Session timed out."") utils.add_logout_reason(request, response, self.logout_reason) return response request.session['last_activity'] = timestamp if not hasattr(request, ""user"") or not request.user.is_authenticated(): # proceed no further if the current request is already known # not to be authenticated return None "," if not hasattr(request, ""user"") or not request.user.is_authenticated(): # proceed no further if the current request is already known # not to be authenticated return None if (isinstance(last_activity, int) and (timestamp - last_activity) > timeout): request.session.pop('last_activity') response = HttpResponseRedirect( '%s?next=%s' % (settings.LOGOUT_URL, request.path)) self.logout_reason = _(""Session timed out."") utils.add_logout_reason(request, response, self.logout_reason) return response request.session['last_activity'] = timestamp ",18,14
openstack%2Foslotest~master~Ibd42d14977b3438aada162199d82661c58fa5f73,openstack/oslotest,master,Ibd42d14977b3438aada162199d82661c58fa5f73,Handle tempfile content encoding,MERGED,2014-09-25 02:49:31.000000000,2014-09-26 11:34:44.000000000,2014-09-26 11:34:44.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 1669}, {'_account_id': 5638}, {'_account_id': 11816}]","[{'number': 1, 'created': '2014-09-25 02:49:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslotest/commit/6d240d0307c097dd25293e669b8cae4192a6d157', 'message': ""Handle tempfile content encoding\n\nWhen a tempfile tuple is given to create allow that tuple\nto optionally provide the encoding to use when the content\nstring is written (default it to utf-8 if not provided).\n\nThis is to resolve the need to worry about encodings when\noslotest is used in py3.x where the tempfiles that are currently\nopened are opened in binary mode (and therefore aren't able\nto write unicode strings without encoding).\n\nChange-Id: Ibd42d14977b3438aada162199d82661c58fa5f73\n""}, {'number': 2, 'created': '2014-09-25 19:52:22.000000000', 'files': ['oslotest/base.py', 'tests/unit/test_base.py'], 'web_link': 'https://opendev.org/openstack/oslotest/commit/356f0603ca4af81527c80a00e04149a92aabdd7f', 'message': ""Handle tempfile content encoding\n\nWhen a tempfile tuple is given to create allow that tuple\nto optionally provide the encoding to use when the content\nstring is written (default it to utf-8 if not provided).\n\nThis is to resolve the need to worry about encodings when\noslotest is used in py3.x where the tempfiles that are currently\nopened are opened in binary mode (and therefore aren't able\nto write unicode strings without encoding).\n\nChange-Id: Ibd42d14977b3438aada162199d82661c58fa5f73\n""}]",5,123919,356f0603ca4af81527c80a00e04149a92aabdd7f,12,5,2,1297,,,0,"Handle tempfile content encoding

When a tempfile tuple is given to create allow that tuple
to optionally provide the encoding to use when the content
string is written (default it to utf-8 if not provided).

This is to resolve the need to worry about encodings when
oslotest is used in py3.x where the tempfiles that are currently
opened are opened in binary mode (and therefore aren't able
to write unicode strings without encoding).

Change-Id: Ibd42d14977b3438aada162199d82661c58fa5f73
",git fetch https://review.opendev.org/openstack/oslotest refs/changes/19/123919/2 && git format-patch -1 --stdout FETCH_HEAD,['oslotest/base.py'],1,6d240d0307c097dd25293e669b8cae4192a6d157,,"import six def create_tempfiles(self, files, ext='.conf', default_encoding='utf-8'): for f in files: if len(f) == 2: basename, contents = f encoding = default_encoding elif len(f) == 3: basename, contents, encoding = f elif len(f) >= 3: raise ValueError('too many values to unpack') else: raise ValueError(""need more than 1 value to unpack"") if isinstance(contents, six.text_type): contents = contents.encode(default_encoding)"," def create_tempfiles(self, files, ext='.conf'): for (basename, contents) in files:",14,2
openstack%2Fos-refresh-config~master~I1e37b16a1a89e26b5fe3a66c245c7b7d50f72cb6,openstack/os-refresh-config,master,I1e37b16a1a89e26b5fe3a66c245c7b7d50f72cb6,Updated from global requirements,MERGED,2014-09-11 00:30:22.000000000,2014-09-26 11:30:43.000000000,2014-09-26 11:30:42.000000000,"[{'_account_id': 3}, {'_account_id': 215}, {'_account_id': 6488}, {'_account_id': 8532}, {'_account_id': 9369}]","[{'number': 1, 'created': '2014-09-11 00:30:22.000000000', 'files': ['requirements.txt', 'test-requirements.txt'], 'web_link': 'https://opendev.org/openstack/os-refresh-config/commit/71b04c8586e1928cd670a0deee699c0c9fc3a948', 'message': 'Updated from global requirements\n\nChange-Id: I1e37b16a1a89e26b5fe3a66c245c7b7d50f72cb6\n'}]",0,120645,71b04c8586e1928cd670a0deee699c0c9fc3a948,13,5,1,11131,,,0,"Updated from global requirements

Change-Id: I1e37b16a1a89e26b5fe3a66c245c7b7d50f72cb6
",git fetch https://review.opendev.org/openstack/os-refresh-config refs/changes/45/120645/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'test-requirements.txt']",2,71b04c8586e1928cd670a0deee699c0c9fc3a948,openstack/requirements,"# The order of packages is significant, because pip processes them in the order # of appearance. Changing the order has an impact on the overall integration # process, which may cause wedges in the gate later.",,6,0
openstack%2Fmagnetodb~master~I4baf9aa9802709c2f68c294489dfdc04d44a778a,openstack/magnetodb,master,I4baf9aa9802709c2f68c294489dfdc04d44a778a,Typo in table name,MERGED,2014-09-25 16:21:05.000000000,2014-09-26 10:51:59.000000000,2014-09-26 10:51:59.000000000,"[{'_account_id': 3}, {'_account_id': 8188}, {'_account_id': 8491}]","[{'number': 1, 'created': '2014-09-25 16:21:05.000000000', 'files': ['doc/source/cli.rst'], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/07c7de452631509ca656c0ea60deeecd995c17b1', 'message': 'Typo in table name\n\nChange-Id: I4baf9aa9802709c2f68c294489dfdc04d44a778a\n'}]",0,124100,07c7de452631509ca656c0ea60deeecd995c17b1,7,3,1,7553,,,0,"Typo in table name

Change-Id: I4baf9aa9802709c2f68c294489dfdc04d44a778a
",git fetch https://review.opendev.org/openstack/magnetodb refs/changes/00/124100/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/cli.rst'],1,07c7de452631509ca656c0ea60deeecd995c17b1,,$ magnetodb item-put Thread --request-file ~/item-put-request.json$ magnetodb item-update Thread --request-file ~/item-update-request.json$ magnetodb item-delete Thread --request-file ~/item-delete-request.json$ magnetodb item-get Thread --request-file ~/item-get-request.json$ magnetodb query Thread --request-file ~/query-request.json$ magnetodb scan Thread --request-file ~/scan-request.json,$ magnetodb item-put Tread --request-file ~/item-put-request.json$ magnetodb item-update Tread --request-file ~/item-update-request.json$ magnetodb item-delete Tread --request-file ~/item-delete-request.json$ magnetodb item-get Tread --request-file ~/item-get-request.json$ magnetodb query Tread --request-file ~/query-request.json$ magnetodb scan Tread --request-file ~/scan-request.json,6,6
openstack%2Fhorizon~master~Ibeb81167553b42f299613359dad49a9bff05fc48,openstack/horizon,master,Ibeb81167553b42f299613359dad49a9bff05fc48,Rename Extra Specs with Metadata in Flavors table,MERGED,2014-09-21 15:41:08.000000000,2014-09-26 10:41:46.000000000,2014-09-26 10:41:46.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 5623}, {'_account_id': 9576}, {'_account_id': 9622}, {'_account_id': 9647}, {'_account_id': 10295}, {'_account_id': 11592}, {'_account_id': 11881}]","[{'number': 1, 'created': '2014-09-21 15:41:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/2050a3996d4f1cb03ffc9bb1438a2a21480d34b1', 'message': ""Rename Extra Specs with Metadata in Flavors table\n\nThe 'Extra Specs' column heading in the Flavors table has been renamed\nto 'Metadata' for consistency (for example, 'Update Metadata' form)\n\nCloses Bug: #1371251\n\nChange-Id: Ibeb81167553b42f299613359dad49a9bff05fc48\n""}, {'number': 2, 'created': '2014-09-21 15:43:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/79d50123b6f96262d3cf7cb08e37e97f2b4ed248', 'message': ""Rename Extra Specs with Metadata in Flavors table\n\nThe 'Extra Specs' column heading in the Flavors table has been renamed\nto 'Metadata' for consistency (for example, 'Update Metadata' form)\n\nCloses-Bug: #1371251\n\nChange-Id: Ibeb81167553b42f299613359dad49a9bff05fc48\n""}, {'number': 3, 'created': '2014-09-24 16:42:58.000000000', 'files': ['openstack_dashboard/dashboards/admin/flavors/tables.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/6e8992a11f1e2b08c1822145c0b1ca5bab565a18', 'message': ""Rename Extra Specs with Metadata in Flavors table\n\nThe 'Extra Specs' column heading in the Flavors table has been renamed\nto 'Metadata' for consistency (for example, 'Update Metadata' form)\n\nCloses-Bug: #1371251\n\nChange-Id: Ibeb81167553b42f299613359dad49a9bff05fc48\n""}]",0,123002,6e8992a11f1e2b08c1822145c0b1ca5bab565a18,28,9,3,10295,,,0,"Rename Extra Specs with Metadata in Flavors table

The 'Extra Specs' column heading in the Flavors table has been renamed
to 'Metadata' for consistency (for example, 'Update Metadata' form)

Closes-Bug: #1371251

Change-Id: Ibeb81167553b42f299613359dad49a9bff05fc48
",git fetch https://review.opendev.org/openstack/horizon refs/changes/02/123002/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack_dashboard/dashboards/admin/flavors/tables.py'],1,2050a3996d4f1cb03ffc9bb1438a2a21480d34b1,bug/1371251," verbose_name=_(""Metadata""),"," verbose_name=_(""Extra Specs""),",1,1
openstack%2Fnova~stable%2Ficehouse~Ieb7c02115c12cd9f6c9fa67691a643aed7632784,openstack/nova,stable/icehouse,Ieb7c02115c12cd9f6c9fa67691a643aed7632784,remove test_multiprocess_api,MERGED,2014-09-25 08:37:20.000000000,2014-09-26 10:41:25.000000000,2014-09-26 10:41:23.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 1955}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 9656}]","[{'number': 1, 'created': '2014-09-25 08:37:20.000000000', 'files': ['nova/tests/integrated/test_multiprocess_api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/4e6371b82a68c265c010fb2a2edb7235319d36b7', 'message': ""remove test_multiprocess_api\n\nThis test suite should be removed for the following reasons.\n\n1) it's not a unit test\n\n2) it's failing randomly, a lot, and depends on timing of the host for\nprocess start and signal handling which are provably not predicable on\nslower machines\n\n3) it's triggering testr bugs in the ways that it fails which means we\ndon't actually get any subunit in the fail scenario, which makes it\nactually impossible to move forward on fixes.\n\nConflicts:\n\tnova/tests/integrated/test_multiprocess_api.py\n\nChange-Id: Ieb7c02115c12cd9f6c9fa67691a643aed7632784\nCloses-Bug: #1357578\n(cherry picked from commit f3c99ba1efb1c039ec47dfdf29781f2e05f32b81)\n""}]",0,123976,4e6371b82a68c265c010fb2a2edb7235319d36b7,17,6,1,9656,,,0,"remove test_multiprocess_api

This test suite should be removed for the following reasons.

1) it's not a unit test

2) it's failing randomly, a lot, and depends on timing of the host for
process start and signal handling which are provably not predicable on
slower machines

3) it's triggering testr bugs in the ways that it fails which means we
don't actually get any subunit in the fail scenario, which makes it
actually impossible to move forward on fixes.

Conflicts:
	nova/tests/integrated/test_multiprocess_api.py

Change-Id: Ieb7c02115c12cd9f6c9fa67691a643aed7632784
Closes-Bug: #1357578
(cherry picked from commit f3c99ba1efb1c039ec47dfdf29781f2e05f32b81)
",git fetch https://review.opendev.org/openstack/nova refs/changes/76/123976/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/integrated/test_multiprocess_api.py'],1,4e6371b82a68c265c010fb2a2edb7235319d36b7,,,"# Copyright (c) 2012 Intel, LLC # Copyright (c) 2012 OpenStack Foundation # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """""" Test multiprocess enabled API service. """""" import errno import fixtures import os import signal import time import traceback from nova.openstack.common import log as logging from nova import service from nova.tests.integrated.api import client from nova.tests.integrated import integrated_helpers LOG = logging.getLogger(__name__) class MultiprocessWSGITest(integrated_helpers._IntegratedTestBase): _api_version = 'v2' def _start_api_service(self): # Process will be started in _spawn() self.osapi = service.WSGIService(""osapi_compute"") self.auth_url = 'http://%(host)s:%(port)s/%(api_version)s' % ({ 'host': self.osapi.host, 'port': self.osapi.port, 'api_version': self._api_version}) LOG.info('auth_url = %s' % self.auth_url) def _get_flags(self): self.workers = 2 f = super(MultiprocessWSGITest, self)._get_flags() f['osapi_compute_workers'] = self.workers return f def _spawn(self): pid = os.fork() if pid == 0: # NOTE(johannes): We can't let the child processes exit back # into the unit test framework since then we'll have multiple # processes running the same tests (and possibly forking more # processes that end up in the same situation). So we need # to catch all exceptions and make sure nothing leaks out, in # particlar SystemExit, which is raised by sys.exit(). We use # os._exit() which doesn't have this problem. status = 0 try: launcher = service.process_launcher() launcher.launch_service(self.osapi, workers=self.osapi.workers) launcher.wait() except SystemExit as exc: status = exc.code except BaseException: # We need to be defensive here too try: traceback.print_exc() except BaseException: LOG.error(""Couldn't print traceback"") status = 2 # Really exit os._exit(status) self.pid = pid # Wait at most 10 seconds to spawn workers cond = lambda: self.workers == len(self._get_workers()) timeout = 10 self._wait(cond, timeout) workers = self._get_workers() self.assertEqual(len(workers), self.workers) return workers def _wait(self, cond, timeout): start = time.time() while True: if cond(): break if time.time() - start > timeout: break time.sleep(.1) def tearDown(self): if self.pid: # Make sure all processes are stopped os.kill(self.pid, signal.SIGTERM) try: # Make sure we reap our test process self._reap_test() except fixtures.TimeoutException: # If the child gets stuck or is too slow in existing # after receiving the SIGTERM, gracefully handle the # timeout exception and try harder to kill it. We need # to do this otherwise the child process can hold up # the test run os.kill(self.pid, signal.SIGKILL) super(MultiprocessWSGITest, self).tearDown() def _reap_test(self): pid, status = os.waitpid(self.pid, 0) self.pid = None return status def _get_workers(self): # NOTE(hartsocks): use of ps checks the process table for child pid # entries these processes may be ended but not reaped so ps may # show processes that are still being cleaned out of the table. f = os.popen('ps ax -o pid,ppid,command') # Skip ps header f.readline() processes = [tuple(int(p) for p in l.strip().split()[:2]) for l in f.readlines()] return [p for p, pp in processes if pp == self.pid] def wait_on_process_until_end(self, worker_pid): # NOTE: the testing framework itself has a # built in test timeout function so a test # stuck in an infinite loop will eventually # be killed by the test framework. LOG.info('waiting on process %r to exit' % worker_pid) while True: # poll the process until it isn't there to poll try: os.kill(worker_pid, 0) time.sleep(0.1) except OSError as err: # by watching specifically for errno.ESRCH # we guarantee this loop continues until # the process table has cleared the pid. # Child process table entries hang around # for several cycles in case a parent process # needs to check their exit state. if err.errno == errno.ESRCH: break LOG.info('process %r has exited' % worker_pid) def test_killed_worker_recover(self): start_workers = self._spawn() worker_pid = start_workers[0] # kill one worker and check if new worker can come up LOG.info('pid of first child is %s' % worker_pid) # signal child os.kill(worker_pid, signal.SIGTERM) self.wait_on_process_until_end(worker_pid) # Make sure worker pids don't match end_workers = self._get_workers() LOG.info('workers: %r' % end_workers) self.assertNotEqual(start_workers, end_workers) # check if api service still works flavors = self.api.get_flavors() self.assertTrue(len(flavors) > 0, 'Num of flavors > 0.') def _terminate_with_signal(self, sig): self._spawn() # check if api service is working flavors = self.api.get_flavors() self.assertTrue(len(flavors) > 0, 'Num of flavors > 0.') worker_pids = self._get_workers() LOG.info(""sent launcher_process pid: %r signal: %r"" % (self.pid, sig)) os.kill(self.pid, sig) # did you know the test framework has a timeout of its own? # if a test takes too long, the test will be killed. for pid in worker_pids: self.wait_on_process_until_end(pid) workers = self._get_workers() self.assertFalse(workers, 'OS processes left %r' % workers) def test_terminate_sigkill(self): self._terminate_with_signal(signal.SIGKILL) status = self._reap_test() self.assertTrue(os.WIFSIGNALED(status)) self.assertEqual(os.WTERMSIG(status), signal.SIGKILL) def test_terminate_sigterm(self): self._terminate_with_signal(signal.SIGTERM) status = self._reap_test() self.assertTrue(os.WIFEXITED(status)) self.assertEqual(os.WEXITSTATUS(status), 0) class MultiprocessWSGITestV3(client.TestOpenStackClientV3Mixin, MultiprocessWSGITest): _api_version = 'v3' ",0,213
openstack%2Fgrenade~master~I312a4860353a6c5b142c9fc82d3cd83b896cd2ee,openstack/grenade,master,I312a4860353a6c5b142c9fc82d3cd83b896cd2ee,javelin resources: add device for volumes,MERGED,2014-09-19 14:24:13.000000000,2014-09-26 10:40:48.000000000,2014-09-26 10:40:48.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 1849}, {'_account_id': 2750}, {'_account_id': 6537}]","[{'number': 1, 'created': '2014-09-19 14:24:13.000000000', 'files': ['resources.yaml'], 'web_link': 'https://opendev.org/openstack/grenade/commit/555f8bd97689c7443ebdb866e5f96a4aa6875bf1', 'message': 'javelin resources: add device for volumes\n\nAdd /dev/vdb device patch for cinder volumes.\nIt will allow in javelin to have a flexibility and attach multiple\nvolumes to a single instance.\n\nChange-Id: I312a4860353a6c5b142c9fc82d3cd83b896cd2ee\n'}]",0,122747,555f8bd97689c7443ebdb866e5f96a4aa6875bf1,9,5,1,3153,,,0,"javelin resources: add device for volumes

Add /dev/vdb device patch for cinder volumes.
It will allow in javelin to have a flexibility and attach multiple
volumes to a single instance.

Change-Id: I312a4860353a6c5b142c9fc82d3cd83b896cd2ee
",git fetch https://review.opendev.org/openstack/grenade refs/changes/47/122747/1 && git format-patch -1 --stdout FETCH_HEAD,['resources.yaml'],1,555f8bd97689c7443ebdb866e5f96a4aa6875bf1,resources-vol, device: /dev/vdb device: /dev/vdb,,2,0
openstack%2Fgrenade~master~I3e74e53e697c4774a24ec86a1698072705509c06,openstack/grenade,master,I3e74e53e697c4774a24ec86a1698072705509c06,Allow BASE_RUN_SMOKE to be overridden by localrc,MERGED,2014-09-23 18:38:58.000000000,2014-09-26 10:40:41.000000000,2014-09-26 10:40:41.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 1420}, {'_account_id': 1849}, {'_account_id': 3153}]","[{'number': 1, 'created': '2014-09-23 18:38:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/grenade/commit/e26f4ad011a81dd384817f873c1377718035469f', 'message': 'Allow BASE_RUN_SMOKE to be overridden by env\n\nCurrently, only TARGET_RUN_SMOKE is allowed to be overridden\nby the user/gate environment while BASE_RUN_SMOKE is ignored\nif set in the environment.  This updates grenaderc and matches\nthe expected defaults and overrides.\n\nChange-Id: I3e74e53e697c4774a24ec86a1698072705509c06\n'}, {'number': 2, 'created': '2014-09-24 04:07:18.000000000', 'files': ['grenaderc'], 'web_link': 'https://opendev.org/openstack/grenade/commit/818eb6a336a8ad7c17a88631c2d4fbd8218f0c88', 'message': 'Allow BASE_RUN_SMOKE to be overridden by localrc\n\nCurrently, only TARGET_RUN_SMOKE is allowed to be overridden\nby the user/gate while BASE_RUN_SMOKE is ignored if set via\nlocalrc.  This updates grenaderc and matches the expected defaults\nand overrides.\n\nChange-Id: I3e74e53e697c4774a24ec86a1698072705509c06\n'}]",0,123538,818eb6a336a8ad7c17a88631c2d4fbd8218f0c88,13,5,2,1420,,,0,"Allow BASE_RUN_SMOKE to be overridden by localrc

Currently, only TARGET_RUN_SMOKE is allowed to be overridden
by the user/gate while BASE_RUN_SMOKE is ignored if set via
localrc.  This updates grenaderc and matches the expected defaults
and overrides.

Change-Id: I3e74e53e697c4774a24ec86a1698072705509c06
",git fetch https://review.opendev.org/openstack/grenade refs/changes/38/123538/2 && git format-patch -1 --stdout FETCH_HEAD,['grenaderc'],1,e26f4ad011a81dd384817f873c1377718035469f,123538,# Allow skipping smoke tests RUN_SMOKE=${RUN_SMOKE:=True} BASE_RUN_SMOKE=${BASE_RUN_SMOKE:-$RUN_SMOKE} TARGET_RUN_SMOKE=${TARGET_RUN_SMOKE:-$RUN_SMOKE},# Allow skipping smoke tests RUN_SMOKE=TrueBASE_RUN_SMOKE=${RUN_SMOKE:=True} TARGET_RUN_SMOKE=${TARGET_RUN_SMOKE:-$BASE_RUN_SMOKE},4,4
openstack%2Ftaskflow~master~I5fd7a062adec052c338790c9ba343dfbc51075e3,openstack/taskflow,master,I5fd7a062adec052c338790c9ba343dfbc51075e3,Expand toctree to three levels,MERGED,2014-09-20 17:41:12.000000000,2014-09-26 10:40:39.000000000,2014-09-26 10:40:38.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 2472}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-09-20 17:41:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/c8277beca62b290dea21762bcaf7987210c87a99', 'message': 'Expand toctree to three levels\n\nThree levels makes it easier to find content in the\nmain toctree so lets make it easier for folks to use\nthe table of contents to find what they are looking for\ninstead of making it harder...\n\nChange-Id: I5fd7a062adec052c338790c9ba343dfbc51075e3\n'}, {'number': 2, 'created': '2014-09-20 18:15:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/0af4d22a8703c971485fa78817a437a4feb728f8', 'message': 'Expand toctree to three levels\n\nThree levels makes it easier to find content in the\nmain toctree so lets make it easier for folks to use\nthe table of contents to find what they are looking for\ninstead of making it harder...\n\nThis change makes three levels look readable as well as\nfixes some discrepancies among the various sections...\n\nChange-Id: I5fd7a062adec052c338790c9ba343dfbc51075e3\n'}, {'number': 3, 'created': '2014-09-25 03:06:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/8c1ff2e9199da36023e235d0aa04f1763e104a07', 'message': 'Expand toctree to three levels\n\nThree levels makes it easier to find content in the\nmain toctree so lets make it easier for folks to use\nthe table of contents to find what they are looking for\ninstead of making it harder...\n\nThis change makes three levels look readable as well as\nfixes some discrepancies among the various sections...\n\nChange-Id: I5fd7a062adec052c338790c9ba343dfbc51075e3\n'}, {'number': 4, 'created': '2014-09-25 04:50:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/a20cd6a42514918829da8a060fcc998e56381a12', 'message': 'Expand toctree to three levels\n\nThree levels makes it easier to find content in the\nmain toctree so lets make it easier for folks to use\nthe table of contents to find what they are looking for\ninstead of making it harder...\n\nThis change makes three levels look readable as well as\nfixes some discrepancies among the various sections...\n\nChange-Id: I5fd7a062adec052c338790c9ba343dfbc51075e3\n'}, {'number': 5, 'created': '2014-09-25 07:11:01.000000000', 'files': ['doc/source/index.rst', 'doc/source/engines.rst', 'doc/source/resumption.rst', 'doc/source/jobs.rst', 'doc/source/atoms.rst', 'doc/source/arguments_and_results.rst', 'doc/source/persistence.rst'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/b5e3fbebc6ea35f6e90936fbfed1c198e1045314', 'message': 'Expand toctree to three levels\n\nThree levels makes it easier to find content in the\nmain toctree so lets make it easier for folks to use\nthe table of contents to find what they are looking for\ninstead of making it harder...\n\nThis change makes three levels look readable as well as\nfixes some discrepancies among the various sections...\n\nChange-Id: I5fd7a062adec052c338790c9ba343dfbc51075e3\n'}]",0,122957,b5e3fbebc6ea35f6e90936fbfed1c198e1045314,17,4,5,1297,,,0,"Expand toctree to three levels

Three levels makes it easier to find content in the
main toctree so lets make it easier for folks to use
the table of contents to find what they are looking for
instead of making it harder...

This change makes three levels look readable as well as
fixes some discrepancies among the various sections...

Change-Id: I5fd7a062adec052c338790c9ba343dfbc51075e3
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/57/122957/3 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/index.rst'],1,c8277beca62b290dea21762bcaf7987210c87a99,three-levels, :maxdepth: 3, :maxdepth: 2,1,1
openstack%2Ftrove-specs~master~I9107884d964757a9154bdd1feb4ea6df4e9df43e,openstack/trove-specs,master,I9107884d964757a9154bdd1feb4ea6df4e9df43e,Remove docutils pin,MERGED,2014-09-18 17:56:32.000000000,2014-09-26 10:40:14.000000000,2014-09-26 10:40:13.000000000,"[{'_account_id': 3}, {'_account_id': 5293}, {'_account_id': 8415}]","[{'number': 1, 'created': '2014-09-18 17:56:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove-specs/commit/023560c242da4300ee5876e1eb5e54c0f39e3055', 'message': 'Remove docutils pin\n\nThis was added to work around bug 1091333.  A new sphinx has been\nreleased and so this is no longer necessary.\n\nChange-Id: I9107884d964757a9154bdd1feb4ea6df4e9df43e\n'}, {'number': 2, 'created': '2014-09-19 06:21:21.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/trove-specs/commit/81917979693da3c0b29528b4a5f0bda6a8e76a60', 'message': 'Remove docutils pin\n\nThis was added to work around bug 1091333.  A new sphinx has been\nreleased and so this is no longer necessary.\n\nChange-Id: I9107884d964757a9154bdd1feb4ea6df4e9df43e\n'}]",0,122479,81917979693da3c0b29528b4a5f0bda6a8e76a60,10,3,2,6547,,,0,"Remove docutils pin

This was added to work around bug 1091333.  A new sphinx has been
released and so this is no longer necessary.

Change-Id: I9107884d964757a9154bdd1feb4ea6df4e9df43e
",git fetch https://review.opendev.org/openstack/trove-specs refs/changes/79/122479/2 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,023560c242da4300ee5876e1eb5e54c0f39e3055,bug/1091333,"sphinx>=1.1.2,<1.2 ","docutils==0.9.1sphinx>=1.1.2,<1.2",1,2
openstack%2Fironic~master~I7ab07168f1f30b4544801a1798cbda191c8d15c7,openstack/ironic,master,I7ab07168f1f30b4544801a1798cbda191c8d15c7,Remove unneeded context initialization in tests,MERGED,2014-09-25 14:54:38.000000000,2014-09-26 10:16:15.000000000,2014-09-26 10:16:14.000000000,"[{'_account_id': 3}, {'_account_id': 6773}, {'_account_id': 8106}, {'_account_id': 12081}]","[{'number': 1, 'created': '2014-09-25 14:54:38.000000000', 'files': ['ironic/tests/drivers/test_ipmitool.py', 'ironic/tests/drivers/test_ipminative.py', 'ironic/tests/api/v1/test_nodes.py', 'ironic/tests/drivers/test_pxe.py', 'ironic/tests/test_hash_ring.py', 'ironic/tests/conductor/test_rpcapi.py', 'ironic/tests/drivers/test_iboot.py', 'ironic/tests/drivers/test_snmp.py', 'ironic/tests/conductor/test_manager.py', 'ironic/tests/api/v1/test_ports.py', 'ironic/tests/drivers/test_agent.py', 'ironic/tests/test_pxe_utils.py', 'ironic/tests/conductor/test_conductor_utils.py', 'ironic/tests/drivers/test_iscsi_deploy.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/760a9798f34675a4a03237de6b74349b3ab2e643', 'message': ""Remove unneeded context initialization in tests\n\nSome test case classes that inherit from\nironic.tests.db.base.DbTestCase set context in their setUp method,\nwhile it is already initialized in DbTestCase's setUp method. This\nchange removes such unneeded initializations.\n\nCloses-bug: #1373979\nChange-Id: I7ab07168f1f30b4544801a1798cbda191c8d15c7\n""}]",0,124073,760a9798f34675a4a03237de6b74349b3ab2e643,13,4,1,12356,,,0,"Remove unneeded context initialization in tests

Some test case classes that inherit from
ironic.tests.db.base.DbTestCase set context in their setUp method,
while it is already initialized in DbTestCase's setUp method. This
change removes such unneeded initializations.

Closes-bug: #1373979
Change-Id: I7ab07168f1f30b4544801a1798cbda191c8d15c7
",git fetch https://review.opendev.org/openstack/ironic refs/changes/73/124073/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/tests/drivers/test_ipmitool.py', 'ironic/tests/drivers/test_ipminative.py', 'ironic/tests/api/v1/test_nodes.py', 'ironic/tests/drivers/test_pxe.py', 'ironic/tests/test_hash_ring.py', 'ironic/tests/conductor/test_rpcapi.py', 'ironic/tests/drivers/test_iboot.py', 'ironic/tests/drivers/test_snmp.py', 'ironic/tests/conductor/test_manager.py', 'ironic/tests/api/v1/test_ports.py', 'ironic/tests/drivers/test_agent.py', 'ironic/tests/test_pxe_utils.py', 'ironic/tests/conductor/test_conductor_utils.py', 'ironic/tests/drivers/test_iscsi_deploy.py']",14,760a9798f34675a4a03237de6b74349b3ab2e643,bug/1373979,, self.context = context.get_admin_context() self.context = context.get_admin_context(),6,30
openstack%2Fcloudkitty~master~I49e768dd5c2c2348bc63f7363de94aecc4eb4d39,openstack/cloudkitty,master,I49e768dd5c2c2348bc63f7363de94aecc4eb4d39,Added pluggable transformer support,MERGED,2014-09-12 15:48:45.000000000,2014-09-26 10:13:23.000000000,2014-09-26 10:13:22.000000000,"[{'_account_id': 3}, {'_account_id': 6484}, {'_account_id': 7923}]","[{'number': 1, 'created': '2014-09-12 15:48:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cloudkitty/commit/7e7c509214074269563698cfc18ae7d930e5f2a7', 'message': 'Added pluggable transformer support\n\nChange-Id: I49e768dd5c2c2348bc63f7363de94aecc4eb4d39\n'}, {'number': 2, 'created': '2014-09-23 14:58:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cloudkitty/commit/bab0e6f8c3155a5bc9f69543cf1b31e280bfacbd', 'message': 'Added pluggable transformer support\n\nChange-Id: I49e768dd5c2c2348bc63f7363de94aecc4eb4d39\n'}, {'number': 3, 'created': '2014-09-23 14:59:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cloudkitty/commit/b221d8e4be841945012ac22c02ed6ff234a9d90c', 'message': 'Added pluggable transformer support\n\nChange-Id: I49e768dd5c2c2348bc63f7363de94aecc4eb4d39\n'}, {'number': 4, 'created': '2014-09-25 12:35:53.000000000', 'files': ['cloudkitty/transformer/format.py', 'etc/cloudkitty/cloudkitty.conf.sample', 'cloudkitty/transformer/__init__.py', 'cloudkitty/transformer/ceilometer.py', 'cloudkitty/collector/__init__.py', 'cloudkitty/collector/ceilometer.py', 'cloudkitty/orchestrator.py', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/cloudkitty/commit/e1d8bdccf64b8ee8ee2622925f75a30220eee5f1', 'message': 'Added pluggable transformer support\n\nChange-Id: I49e768dd5c2c2348bc63f7363de94aecc4eb4d39\n'}]",5,121150,e1d8bdccf64b8ee8ee2622925f75a30220eee5f1,12,3,4,7042,,,0,"Added pluggable transformer support

Change-Id: I49e768dd5c2c2348bc63f7363de94aecc4eb4d39
",git fetch https://review.opendev.org/openstack/cloudkitty refs/changes/50/121150/4 && git format-patch -1 --stdout FETCH_HEAD,"['cloudkitty/transformer/__init__.py', 'cloudkitty/transformer/ceilometer.py', 'cloudkitty/collector/__init__.py', 'cloudkitty/collector/ceilometer.py', 'cloudkitty/orchestrator.py', 'setup.cfg']",6,7e7c509214074269563698cfc18ae7d930e5f2a7,refactoring/new_arch,cloudkitty.transformer.backends = ceilometer = cloudkitty.transformer.ceilometer:CeilometerTransformer ,,173,48
openstack%2Ftaskflow~master~I1183bda96e1ddb062d9cab91990186f0f56f0a0e,openstack/taskflow,master,I1183bda96e1ddb062d9cab91990186f0f56f0a0e,Switch to using oslo.utils and oslo.serialization,MERGED,2014-09-11 00:24:11.000000000,2014-09-26 09:41:36.000000000,2014-09-26 09:41:35.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 2472}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-09-11 00:24:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/d8d360668690f3ff62ed86946d85c72e7d3a6cea', 'message': ""Switch to using oslo.utils and oslo.serialization\n\nInstead of copying modules from the incubator into taskflow\nwe can now directly use these same modules from supported\nlibraries instead so this moves the usage of everything except\nuuidutils which wasn't moved over to using those newly published\nlibraries.\n\nPart of blueprint integrate-and-use-oslo-utils-serialization\n\nChange-Id: I1183bda96e1ddb062d9cab91990186f0f56f0a0e\n""}, {'number': 2, 'created': '2014-09-20 16:03:22.000000000', 'files': ['taskflow/jobs/backends/impl_zookeeper.py', 'taskflow/utils/reflection.py', 'taskflow/persistence/backends/impl_sqlalchemy.py', 'taskflow/openstack/common/network_utils.py', 'taskflow/openstack/common/timeutils.py', 'taskflow/persistence/backends/impl_zookeeper.py', 'taskflow/openstack/common/strutils.py', 'taskflow/persistence/backends/sqlalchemy/models.py', 'openstack-common.conf', 'taskflow/engines/helpers.py', 'taskflow/openstack/common/gettextutils.py', 'taskflow/engines/worker_based/protocol.py', 'taskflow/openstack/common/importutils.py', 'taskflow/utils/persistence_utils.py', 'taskflow/persistence/logbook.py', 'taskflow/tests/unit/worker_based/test_executor.py', 'requirements-py2.txt', 'taskflow/listeners/base.py', 'taskflow/openstack/common/excutils.py', 'taskflow/engines/action_engine/engine.py', 'taskflow/engines/worker_based/executor.py', 'taskflow/openstack/common/jsonutils.py', 'taskflow/persistence/backends/impl_dir.py', 'taskflow/tests/unit/jobs/test_zk_job.py', 'requirements-py3.txt', 'taskflow/utils/misc.py', 'taskflow/tests/unit/test_engine_helpers.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/aaa51fd689db73f40088eba7a07ac8a131aad528', 'message': ""Switch to using oslo.utils and oslo.serialization\n\nInstead of copying modules from the incubator into taskflow\nwe can now directly use these same modules from supported\nlibraries instead so this moves the usage of everything except\nuuidutils which wasn't moved over to using those newly published\nlibraries.\n\nPart of blueprint integrate-and-use-oslo-utils-serialization\n\nChange-Id: I1183bda96e1ddb062d9cab91990186f0f56f0a0e\n""}]",0,120630,aaa51fd689db73f40088eba7a07ac8a131aad528,17,4,2,1297,,,0,"Switch to using oslo.utils and oslo.serialization

Instead of copying modules from the incubator into taskflow
we can now directly use these same modules from supported
libraries instead so this moves the usage of everything except
uuidutils which wasn't moved over to using those newly published
libraries.

Part of blueprint integrate-and-use-oslo-utils-serialization

Change-Id: I1183bda96e1ddb062d9cab91990186f0f56f0a0e
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/30/120630/2 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/jobs/backends/impl_zookeeper.py', 'taskflow/utils/reflection.py', 'taskflow/persistence/backends/impl_sqlalchemy.py', 'taskflow/openstack/common/network_utils.py', 'taskflow/openstack/common/timeutils.py', 'taskflow/persistence/backends/impl_zookeeper.py', 'taskflow/openstack/common/strutils.py', 'taskflow/persistence/backends/sqlalchemy/models.py', 'openstack-common.conf', 'taskflow/engines/helpers.py', 'taskflow/openstack/common/gettextutils.py', 'taskflow/engines/worker_based/protocol.py', 'taskflow/openstack/common/importutils.py', 'taskflow/utils/persistence_utils.py', 'taskflow/persistence/logbook.py', 'taskflow/tests/unit/worker_based/test_executor.py', 'requirements-py2.txt', 'taskflow/listeners/base.py', 'taskflow/openstack/common/excutils.py', 'taskflow/engines/action_engine/engine.py', 'taskflow/engines/worker_based/executor.py', 'taskflow/openstack/common/jsonutils.py', 'taskflow/persistence/backends/impl_dir.py', 'taskflow/tests/unit/jobs/test_zk_job.py', 'requirements-py3.txt', 'taskflow/utils/misc.py', 'taskflow/tests/unit/test_engine_helpers.py']",27,d8d360668690f3ff62ed86946d85c72e7d3a6cea,bp/integrate-and-use-oslo-utils-serialization," with mock.patch('oslo.utils.importutils.import_class', with mock.patch('oslo.utils.importutils.import_class',"," with mock.patch('taskflow.openstack.common.importutils.import_class', with mock.patch('taskflow.openstack.common.importutils.import_class',",33,1588
openstack%2Ftempest~master~I62193af963020311d5ed90a847c9764faff3e4c2,openstack/tempest,master,I62193af963020311d5ed90a847c9764faff3e4c2,Properly preserve trace on error during wait,MERGED,2014-09-19 21:08:10.000000000,2014-09-26 09:41:26.000000000,2014-09-26 09:41:25.000000000,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 6537}]","[{'number': 1, 'created': '2014-09-19 21:08:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/800dfd6bda12b5a6e3f6a12d81d6dda561f9fea5', 'message': 'Properly preserve trace on error during wait\n\nThis commit starts using save_and_rereraise_exception from oslo\nexcutils when handling an server ERROR state during a wait loop.\nPreviously, if the wait loop in the common api create_server method\ncaught a BuildErrorException it attempts to do some final processing\nand cleanup around that server. However, doing this interferes with\nthe trace output from the failure and only would show the subsequent\nre-raise as the stack trace. This commit avoids that but using the\noslo method to get around this.\n\nChange-Id: I62193af963020311d5ed90a847c9764faff3e4c2\n'}, {'number': 2, 'created': '2014-09-23 20:58:19.000000000', 'files': ['tempest/api/compute/base.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/83d97bed1c0bca52f38bd7f3927f76cb0d8256d8', 'message': 'Properly preserve trace on error during wait\n\nThis commit starts using save_and_rereraise_exception from oslo\nexcutils when handling an server ERROR state during a wait loop.\nPreviously, if the wait loop in the common api create_server method\ncaught a BuildErrorException it attempts to do some final processing\nand cleanup around that server. However, doing this interferes with\nthe trace output from the failure and only would show the subsequent\nre-raise as the stack trace. This commit avoids that but using the\noslo method to get around this.\n\nChange-Id: I62193af963020311d5ed90a847c9764faff3e4c2\n'}]",1,122861,83d97bed1c0bca52f38bd7f3927f76cb0d8256d8,11,4,2,5196,,,0,"Properly preserve trace on error during wait

This commit starts using save_and_rereraise_exception from oslo
excutils when handling an server ERROR state during a wait loop.
Previously, if the wait loop in the common api create_server method
caught a BuildErrorException it attempts to do some final processing
and cleanup around that server. However, doing this interferes with
the trace output from the failure and only would show the subsequent
re-raise as the stack trace. This commit avoids that but using the
oslo method to get around this.

Change-Id: I62193af963020311d5ed90a847c9764faff3e4c2
",git fetch https://review.opendev.org/openstack/tempest refs/changes/61/122861/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/compute/base.py'],1,800dfd6bda12b5a6e3f6a12d81d6dda561f9fea5,fix-exception-trace,from tempest.openstack.common import excutils with excutils.save_and_reraise_exception(): if ('preserve_server_on_error' not in kwargs or kwargs['preserve_server_on_error'] is False): for server in servers: try: cls.servers_client.delete_server( server['id']) except Exception: pass, if ('preserve_server_on_error' not in kwargs or kwargs['preserve_server_on_error'] is False): for server in servers: try: cls.servers_client.delete_server(server['id']) except Exception: pass raise ex,10,8
openstack%2Fpython-cinderclient~master~I49f8f7fcc2647dc09e38fee1fa5068df714a8d83,openstack/python-cinderclient,master,I49f8f7fcc2647dc09e38fee1fa5068df714a8d83,Update version to 1.1.1 in index,MERGED,2014-09-24 00:18:51.000000000,2014-09-26 09:38:32.000000000,2014-09-26 09:38:31.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 5538}]","[{'number': 1, 'created': '2014-09-24 00:18:51.000000000', 'files': ['doc/source/index.rst'], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/ac9b0913904c43f4bf12c8164324d6e6a55dc1ab', 'message': 'Update version to 1.1.1 in index\n\nPrep for new tag\n\nChange-Id: I49f8f7fcc2647dc09e38fee1fa5068df714a8d83\n'}]",0,123613,ac9b0913904c43f4bf12c8164324d6e6a55dc1ab,19,3,1,2243,,,0,"Update version to 1.1.1 in index

Prep for new tag

Change-Id: I49f8f7fcc2647dc09e38fee1fa5068df714a8d83
",git fetch https://review.opendev.org/openstack/python-cinderclient refs/changes/13/123613/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/index.rst'],1,ac9b0913904c43f4bf12c8164324d6e6a55dc1ab,update_index_file_for_111,1.1.1 ------ .. _1370152 http://bugs.launchpad.net/python-cinderclient/+bug/1370152 ,,4,0
openstack%2Ffuel-docs~master~Ife19b84b16bbacfac3b146f12c6e062226c26c25,openstack/fuel-docs,master,Ife19b84b16bbacfac3b146f12c6e062226c26c25,Explains partition types,ABANDONED,2014-09-25 13:27:15.000000000,2014-09-26 09:31:29.000000000,,"[{'_account_id': 3}, {'_account_id': 8787}, {'_account_id': 8971}, {'_account_id': 9788}, {'_account_id': 10014}, {'_account_id': 12139}, {'_account_id': 13082}]","[{'number': 1, 'created': '2014-09-25 13:27:15.000000000', 'files': ['pages/user-guide/config-environment/1200-customize-partitions.rst'], 'web_link': 'https://opendev.org/openstack/fuel-docs/commit/7c681623bb1ec99fc93f4ce7e6bc04d4bbba27d9', 'message': 'Explains partition types\n\nChange-Id: Ife19b84b16bbacfac3b146f12c6e062226c26c25\n'}]",1,124051,7c681623bb1ec99fc93f4ce7e6bc04d4bbba27d9,7,7,1,12866,,,0,"Explains partition types

Change-Id: Ife19b84b16bbacfac3b146f12c6e062226c26c25
",git fetch https://review.opendev.org/openstack/fuel-docs refs/changes/51/124051/1 && git format-patch -1 --stdout FETCH_HEAD,['pages/user-guide/config-environment/1200-customize-partitions.rst'],1,7c681623bb1ec99fc93f4ce7e6bc04d4bbba27d9,clarify-virtual-storage-meaning,A partition type could be: - Base System: where the operating system and basic software will be installed - Cinder: dedicated to cinder storage - Ceph: dedicated to ceph storage - Virtual Storage: dedicated to nova running instances ,,7,0
openstack%2Foperations-guide~master~Ic9cdb0475af8fb862d6057458f7e309f087547ef,openstack/operations-guide,master,Ic9cdb0475af8fb862d6057458f7e309f087547ef,Adds entity file pointer to some files,MERGED,2014-09-25 14:00:04.000000000,2014-09-26 09:21:15.000000000,2014-09-26 09:21:14.000000000,"[{'_account_id': 3}, {'_account_id': 964}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-09-25 14:00:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/operations-guide/commit/561713ad4912d6691fcb872e07cdc33a85ff4f49', 'message': 'Adds entity file pointer to some files\n\n- Adds &times; entity and some edits for incorrect use of x\n- Fixes some other entity (mdash) misuses\n\nChange-Id: Ic9cdb0475af8fb862d6057458f7e309f087547ef\n'}, {'number': 2, 'created': '2014-09-25 15:30:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/operations-guide/commit/f19eca2416f4c95a264e9d7787ec551a37a85e38', 'message': 'Adds entity file pointer to some files\n\n- Adds &times; entity and some edits for incorrect use of x\n- Fixes some other entity (mdash) misuses\n- Fixes ellipse issue\n\nChange-Id: Ic9cdb0475af8fb862d6057458f7e309f087547ef\n'}, {'number': 3, 'created': '2014-09-25 18:41:59.000000000', 'files': ['doc/openstack-ops/ch_arch_scaling.xml', 'doc/openstack-ops/section_arch_example-neutron.xml', 'doc/openstack-ops/app_crypt.xml', 'doc/openstack-ops/openstack.ent'], 'web_link': 'https://opendev.org/openstack/operations-guide/commit/80e961aa12a8df7d84736d771539f3e62a44478d', 'message': 'Adds entity file pointer to some files\n\n- Adds &times; entity and some edits for incorrect use of x\n- Fixes some other entity (mdash) misuses\n- Fixes ellipse issue\n\nChange-Id: Ic9cdb0475af8fb862d6057458f7e309f087547ef\n'}]",4,124057,80e961aa12a8df7d84736d771539f3e62a44478d,16,3,3,964,,,0,"Adds entity file pointer to some files

- Adds &times; entity and some edits for incorrect use of x
- Fixes some other entity (mdash) misuses
- Fixes ellipse issue

Change-Id: Ic9cdb0475af8fb862d6057458f7e309f087547ef
",git fetch https://review.opendev.org/openstack/operations-guide refs/changes/57/124057/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/openstack-ops/ch_arch_scaling.xml', 'doc/openstack-ops/section_arch_example-neutron.xml', 'doc/openstack-ops/app_crypt.xml', 'doc/openstack-ops/openstack.ent']",4,561713ad4912d6691fcb872e07cdc33a85ff4f49,entity-times,"<!ENTITY times ""&#215;"">",,45,43
openstack%2Fopenstack-manuals~master~I62804843f58f719ae0f4cac6c059647147e43188,openstack/openstack-manuals,master,I62804843f58f719ae0f4cac6c059647147e43188,Imported Translations from Transifex,MERGED,2014-09-26 06:10:43.000000000,2014-09-26 09:11:47.000000000,2014-09-26 09:11:46.000000000,"[{'_account_id': 3}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-09-26 06:10:43.000000000', 'files': ['doc/glossary/locale/fr.po', 'doc/arch-design/locale/arch-design.pot'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/5d194b9a47e841b2a981e9d02582ffd0efb6c3f8', 'message': 'Imported Translations from Transifex\n\nChange-Id: I62804843f58f719ae0f4cac6c059647147e43188\n'}]",0,124292,5d194b9a47e841b2a981e9d02582ffd0efb6c3f8,6,2,1,11131,,,0,"Imported Translations from Transifex

Change-Id: I62804843f58f719ae0f4cac6c059647147e43188
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/92/124292/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/glossary/locale/fr.po', 'doc/arch-design/locale/arch-design.pot']",2,5d194b9a47e841b2a981e9d02582ffd0efb6c3f8,transifex/translations,"""POT-Creation-Date: 2014-09-26 06:09+0000\n""msgid ""As an example, consider that a m1.small instance uses 1 vCPU, 20GB of ephemeral storage and 2,048MB of RAM. When designing a hardware node as a compute resource pool to service instances, take into consideration the number of processor cores available on the node as well as the required disk and memory to service instances running at capacity. For a server with 2 CPUs of 10 cores each, with hyperthreading turned on, the default CPU overcommit ratio of 16:1 would allow for 640 (2 10 2 16) total m1.small instances. By the same reasoning, using the default memory overcommit ratio of 1.5:1 you can determine that the server will need at least 853GB (640 2,048MB / 1.5) of RAM. When sizing nodes for memory, it is also important to consider the additional memory required to service operating system and service needs.""msgid ""1600 = (16 (number of physical cores)) / 2""msgid ""storage required = 50GB 1600""","""POT-Creation-Date: 2014-09-17 06:14+0000\n""msgid ""As an example, consider that a m1.small instance uses 1 vCPU, 20GB of ephemeral storage and 2,048MB of RAM. When designing a hardware node as a compute resource pool to service instances, take into consideration the number of processor cores available on the node as well as the required disk and memory to service instances running at capacity. For a server with 2 CPUs of 10 cores each, with hyperthreading turned on, the default CPU overcommit ratio of 16:1 would allow for 640 (2 x 10 x 2 x 16) total m1.small instances. By the same reasoning, using the default memory overcommit ratio of 1.5:1 you can determine that the server will need at least 853GB (640 x 2,048 MB % 1.5) of RAM. When sizing nodes for memory, it is also important to consider the additional memory required to service operating system and service needs.""msgid ""1600 = (16 x (number of physical cores)) / 2""msgid ""storage required = 50GB x 1600""",7,7
openstack%2Fhorizon~master~I953dde0e7ee4a876376faf28c6f0bb859a3dc6b6,openstack/horizon,master,I953dde0e7ee4a876376faf28c6f0bb859a3dc6b6,Move datepicker language js inclusion outside compress,MERGED,2014-09-24 16:18:50.000000000,2014-09-26 08:36:08.000000000,2014-09-26 08:36:08.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 1926}, {'_account_id': 5623}, {'_account_id': 6650}, {'_account_id': 6969}, {'_account_id': 9981}, {'_account_id': 11880}, {'_account_id': 11881}, {'_account_id': 11902}, {'_account_id': 12459}]","[{'number': 1, 'created': '2014-09-24 16:18:50.000000000', 'files': ['horizon/templates/horizon/_scripts.html'], 'web_link': 'https://opendev.org/openstack/horizon/commit/a05857872083f4e97d86c2d658574b095dde123f', 'message': ""Move datepicker language js inclusion outside compress\n\nThe datapicker language support works by including a different\njs file based on the language of the user.  This will not\nwork if the js is included at compress time becuase only 1\nlanguage can be included.\n\nAdditionally cleaning up the logic in case the locale\nisn't set (as is the case at compression time) to prevent\nan improper path from being build.\n\nChange-Id: I953dde0e7ee4a876376faf28c6f0bb859a3dc6b6\nCloses-Bug: #1373430\n""}]",0,123779,a05857872083f4e97d86c2d658574b095dde123f,18,11,1,9981,,,0,"Move datepicker language js inclusion outside compress

The datapicker language support works by including a different
js file based on the language of the user.  This will not
work if the js is included at compress time becuase only 1
language can be included.

Additionally cleaning up the logic in case the locale
isn't set (as is the case at compression time) to prevent
an improper path from being build.

Change-Id: I953dde0e7ee4a876376faf28c6f0bb859a3dc6b6
Closes-Bug: #1373430
",git fetch https://review.opendev.org/openstack/horizon refs/changes/79/123779/1 && git format-patch -1 --stdout FETCH_HEAD,['horizon/templates/horizon/_scripts.html'],1,a05857872083f4e97d86c2d658574b095dde123f,bug/1373430,"{% comment %}Locale sensitive js needs to be included outisde the ""compress"" tag{% endcomment %} {% if DATEPICKER_LOCALE and DATEPICKER_LOCALE != 'en' %} <script src='{{ STATIC_URL }}horizon/lib/bootstrap_datepicker/locales/bootstrap-datepicker.{{ DATEPICKER_LOCALE }}.js' type='text/javascript' charset='utf-8'></script> {% endif %} ",{% ifnotequal DATEPICKER_LOCALE 'en' %} <script src='{{ STATIC_URL }}horizon/lib/bootstrap_datepicker/locales/bootstrap-datepicker.{{ DATEPICKER_LOCALE }}.js' type='text/javascript' charset='utf-8'></script> {% endifnotequal %},5,3
openstack%2Fhorizon~stable%2Ficehouse~I0c7a87a6f00c4cc1a54f8b70e5df9bb1d05de866,openstack/horizon,stable/icehouse,I0c7a87a6f00c4cc1a54f8b70e5df9bb1d05de866,Set the correct min_disk size when creating volume from image,MERGED,2014-09-16 06:02:33.000000000,2014-09-26 08:30:50.000000000,2014-09-26 08:30:49.000000000,"[{'_account_id': 3}, {'_account_id': 2455}, {'_account_id': 4978}, {'_account_id': 5623}, {'_account_id': 6610}, {'_account_id': 9576}, {'_account_id': 9622}, {'_account_id': 9656}, {'_account_id': 9981}, {'_account_id': 11592}]","[{'number': 1, 'created': '2014-09-16 06:02:33.000000000', 'files': ['openstack_dashboard/dashboards/project/volumes/volumes/tests.py', 'openstack_dashboard/dashboards/project/volumes/volumes/forms.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/737fe401736017bb94875b676addf637abaa8edf', 'message': 'Set the correct min_disk size when creating volume from image\n\nSometimes the image min_disk size is specified in a property rather\nthan the min_disk attribute (for instance, after creating a snapshot\nfrom an instance). This ensures that the correct tooltip and error\nmessage are displayed in these cases too.\n\nChange-Id: I0c7a87a6f00c4cc1a54f8b70e5df9bb1d05de866\nCloses-Bug: #1368600\n(cherry picked from commit 6b62f616b1d6f7d71a34ea59ba8c8120745915f4)\n'}]",1,121755,737fe401736017bb94875b676addf637abaa8edf,15,10,1,4978,,,0,"Set the correct min_disk size when creating volume from image

Sometimes the image min_disk size is specified in a property rather
than the min_disk attribute (for instance, after creating a snapshot
from an instance). This ensures that the correct tooltip and error
message are displayed in these cases too.

Change-Id: I0c7a87a6f00c4cc1a54f8b70e5df9bb1d05de866
Closes-Bug: #1368600
(cherry picked from commit 6b62f616b1d6f7d71a34ea59ba8c8120745915f4)
",git fetch https://review.opendev.org/openstack/horizon refs/changes/55/121755/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/volumes/volumes/tests.py', 'openstack_dashboard/dashboards/project/volumes/volumes/forms.py']",2,737fe401736017bb94875b676addf637abaa8edf,bug/1368600-i," properties = getattr(image, 'properties', {}) min_disk_size = getattr(image, 'min_disk', 0) or \ properties.get('min_disk', 0) properties = getattr(image, 'properties', {}) min_disk_size = getattr(image, 'min_disk', 0) or \ properties.get('min_disk', 0) if (min_disk_size > 0 and data['size'] < min_disk_size):"," min_disk_size = getattr(image, 'min_disk', 0) min_disk_size = getattr(image, 'min_disk', 0) if (min_disk_size > 0 and data['size'] < image.min_disk):",19,6
openstack%2Fhorizon~stable%2Ficehouse~Ieab0b13afd79158c6bf603891164f01aff3aabf3,openstack/horizon,stable/icehouse,Ieab0b13afd79158c6bf603891164f01aff3aabf3,Add OS_REGION_NAME to openrc,MERGED,2014-08-11 15:41:20.000000000,2014-09-26 08:30:39.000000000,2014-09-26 08:30:38.000000000,"[{'_account_id': 3}, {'_account_id': 2424}, {'_account_id': 5623}, {'_account_id': 6282}, {'_account_id': 6650}, {'_account_id': 9500}, {'_account_id': 9656}, {'_account_id': 9981}, {'_account_id': 10369}, {'_account_id': 12071}]","[{'number': 1, 'created': '2014-08-11 15:41:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/60c73d66c149f179a0ac81140daa90ca28e4c13d', 'message': 'Add OS_REGION_NAME to openrc\n\nThe openrc file that is downloaded from Horizon is missing the\nOS_REGION_NAME setting. This causes issues in environments that have\nmultiple regions. If the Region is none, ensure that it is unset after\nsourcing the rc file to avoid and empty variable. This fix also cleans\nup some language in the generated openrc file.\n\nChange-Id: Ieab0b13afd79158c6bf603891164f01aff3aabf3\nCloses-Bug: #1334791\n(cherry picked from commit a8692b7726fb43af867844950538eb5ce6436a35)\n'}, {'number': 2, 'created': '2014-09-18 21:03:08.000000000', 'files': ['openstack_dashboard/dashboards/project/access_and_security/api_access/views.py', 'openstack_dashboard/dashboards/project/access_and_security/templates/access_and_security/api_access/openrc.sh.template', 'openstack_dashboard/test/tests/templates.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/839c53d983e26a69edd6f92cb8746c8916541101', 'message': 'Add OS_REGION_NAME to openrc\n\nThe openrc file that is downloaded from Horizon is missing the\nOS_REGION_NAME setting. This causes issues in environments that have\nmultiple regions. If the Region is none, ensure that it is unset after\nsourcing the rc file to avoid and empty variable. This fix also cleans\nup some language in the generated openrc file.\n\nChange-Id: Ieab0b13afd79158c6bf603891164f01aff3aabf3\nCloses-Bug: #1334791\n(cherry picked from commit a8692b7726fb43af867844950538eb5ce6436a35)\n'}]",0,113293,839c53d983e26a69edd6f92cb8746c8916541101,18,10,2,9500,,,0,"Add OS_REGION_NAME to openrc

The openrc file that is downloaded from Horizon is missing the
OS_REGION_NAME setting. This causes issues in environments that have
multiple regions. If the Region is none, ensure that it is unset after
sourcing the rc file to avoid and empty variable. This fix also cleans
up some language in the generated openrc file.

Change-Id: Ieab0b13afd79158c6bf603891164f01aff3aabf3
Closes-Bug: #1334791
(cherry picked from commit a8692b7726fb43af867844950538eb5ce6436a35)
",git fetch https://review.opendev.org/openstack/horizon refs/changes/93/113293/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/access_and_security/api_access/views.py', 'openstack_dashboard/dashboards/project/access_and_security/templates/access_and_security/api_access/openrc.sh.template', 'openstack_dashboard/test/tests/templates.py']",3,60c73d66c149f179a0ac81140daa90ca28e4c13d,," def test_openrc_set_region(self): context = { ""user"": FakeUser(), ""tenant_id"": ""some-cool-id"", ""auth_url"": ""http://tests.com"", ""tenant_name"": ""Tenant"", ""region"": ""Colorado""} out = loader.render_to_string( 'project/access_and_security/api_access/openrc.sh.template', context, template.Context(context)) self.assertTrue(""OS_REGION_NAME=\""Colorado\"""" in out) def test_openrc_region_not_set(self): context = { ""user"": FakeUser(), ""tenant_id"": ""some-cool-id"", ""auth_url"": ""http://tests.com"", ""tenant_name"": ""Tenant""} out = loader.render_to_string( 'project/access_and_security/api_access/openrc.sh.template', context, template.Context(context)) self.assertTrue(""OS_REGION_NAME=\""\"""" in out)",,42,5
openstack%2Fhorizon~stable%2Ficehouse~I1f558cf7f3198a68629738a011e9cae0c3749320,openstack/horizon,stable/icehouse,I1f558cf7f3198a68629738a011e9cae0c3749320,Rename add_error methods: Django 1.7 conflict,MERGED,2014-08-07 17:18:04.000000000,2014-09-26 08:30:33.000000000,2014-09-26 08:30:32.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 1669}, {'_account_id': 1941}, {'_account_id': 2455}, {'_account_id': 5623}, {'_account_id': 6476}, {'_account_id': 6914}, {'_account_id': 9531}, {'_account_id': 9656}, {'_account_id': 13308}]","[{'number': 1, 'created': '2014-08-07 17:18:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/b024a18b10f0e15385d6267c1acf90bcd54a4d22', 'message': ""Rename add_error methods: Django 1.7 conflict\n\nIn Django 1.7, Forms have an add_error() method and our definition\nwas conflicting with Django's method. Thanks to hertzog@debian.org\nfor the patch.\n\nNote that it is required to support Django 1.7 in Icehouse, because\nwe're planning to release Debian Jessie with both Django 1.7 and\nIcehouse. This is to *support* Django 1.7, not at all to *switch* to\nit, which is very different. There's not so many patches needed,\nand they are non-disruptive/simple. Also, please don't just -1 on the\nreivew because of the freeze for next point release, just delay it\nuntil tomorrow after 2014.1.2 is released. Cheers! :)\n\nChange-Id: I1f558cf7f3198a68629738a011e9cae0c3749320\n(cherry picked from commit 04dd1568c1c927901085941124e4c6852183f4f0)\n""}, {'number': 2, 'created': '2014-09-25 11:18:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/5ab443279b8d568aee93f8acbba28c1723d7571c', 'message': ""Rename add_error methods: Django 1.7 conflict\n\nIn Django 1.7, Forms have an add_error() method and our definition\nwas conflicting with Django's method. Thanks to hertzog@debian.org\nfor the patch.\n\nNote that it is required to support Django 1.7 in Icehouse, because\nwe're planning to release Debian Jessie with both Django 1.7 and\nIcehouse. This is to *support* Django 1.7, not at all to *switch* to\nit, which is very different. There's not so many patches needed,\nand they are non-disruptive/simple. Also, please don't just -1 on the\nreivew because of the freeze for next point release, just delay it\nuntil tomorrow after 2014.1.2 is released. Cheers! :)\n\nClose-bug: #1352919\nChange-Id: I1f558cf7f3198a68629738a011e9cae0c3749320\n(cherry picked from commit 04dd1568c1c927901085941124e4c6852183f4f0)\n""}, {'number': 3, 'created': '2014-09-25 11:18:53.000000000', 'files': ['horizon/workflows/base.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/ad2d4281a38c168747c2bbe42513af72d4d7d0bd', 'message': ""Rename add_error methods: Django 1.7 conflict\n\nIn Django 1.7, Forms have an add_error() method and our definition\nwas conflicting with Django's method. Thanks to hertzog@debian.org\nfor the patch.\n\nNote that it is required to support Django 1.7 in Icehouse, because\nwe're planning to release Debian Jessie with both Django 1.7 and\nIcehouse. This is to *support* Django 1.7, not at all to *switch* to\nit, which is very different.\n\nClose-bug: #1352919\nChange-Id: I1f558cf7f3198a68629738a011e9cae0c3749320\n(cherry picked from commit 04dd1568c1c927901085941124e4c6852183f4f0)\n""}]",1,112630,ad2d4281a38c168747c2bbe42513af72d4d7d0bd,27,11,3,6476,,,0,"Rename add_error methods: Django 1.7 conflict

In Django 1.7, Forms have an add_error() method and our definition
was conflicting with Django's method. Thanks to hertzog@debian.org
for the patch.

Note that it is required to support Django 1.7 in Icehouse, because
we're planning to release Debian Jessie with both Django 1.7 and
Icehouse. This is to *support* Django 1.7, not at all to *switch* to
it, which is very different.

Close-bug: #1352919
Change-Id: I1f558cf7f3198a68629738a011e9cae0c3749320
(cherry picked from commit 04dd1568c1c927901085941124e4c6852183f4f0)
",git fetch https://review.opendev.org/openstack/horizon refs/changes/30/112630/1 && git format-patch -1 --stdout FETCH_HEAD,['horizon/workflows/base.py'],1,b024a18b10f0e15385d6267c1acf90bcd54a4d22,rename-add-error-icehouse," def add_action_error(self, message): def add_step_error(self, message): self.action.add_action_error(message) step.add_step_error(message)"," def add_error(self, message): def add_error(self, message): self.action.add_error(message) step.add_error(message)",4,4
openstack%2Fironic~master~I87cf8dea55529d9f3ed8c8f4dea70d3f51911e74,openstack/ironic,master,I87cf8dea55529d9f3ed8c8f4dea70d3f51911e74,Imported Translations from Transifex,ABANDONED,2014-09-23 06:07:18.000000000,2014-09-26 08:28:48.000000000,,"[{'_account_id': 3}, {'_account_id': 2889}, {'_account_id': 6547}, {'_account_id': 12081}]","[{'number': 1, 'created': '2014-09-23 06:07:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c3d3339c17c84edcbbf47f6847fe902860525bb5', 'message': 'Imported Translations from Transifex\n\nChange-Id: I87cf8dea55529d9f3ed8c8f4dea70d3f51911e74\n'}, {'number': 2, 'created': '2014-09-24 06:07:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2bb246ed712e83e67371f6e6459c13c733fe95de', 'message': 'Imported Translations from Transifex\n\nChange-Id: I87cf8dea55529d9f3ed8c8f4dea70d3f51911e74\n'}, {'number': 3, 'created': '2014-09-25 06:25:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/8bfa5806dfcdeb8a69698a2264aab0b81fb3492f', 'message': 'Imported Translations from Transifex\n\nChange-Id: I87cf8dea55529d9f3ed8c8f4dea70d3f51911e74\n'}, {'number': 4, 'created': '2014-09-26 06:11:24.000000000', 'files': ['ironic/locale/ironic.pot', 'ironic/locale/en_GB/LC_MESSAGES/ironic-log-warning.po', 'ironic/locale/de/LC_MESSAGES/ironic-log-warning.po', 'ironic/locale/ironic-log-warning.pot', 'ironic/locale/fr/LC_MESSAGES/ironic-log-critical.po', 'ironic/locale/fr/LC_MESSAGES/ironic-log-warning.po', 'ironic/locale/en_AU/LC_MESSAGES/ironic.po', 'ironic/locale/en_US/LC_MESSAGES/ironic.po'], 'web_link': 'https://opendev.org/openstack/ironic/commit/321f7c5ecf944a864f7064dddbc514f5bcba1075', 'message': 'Imported Translations from Transifex\n\nChange-Id: I87cf8dea55529d9f3ed8c8f4dea70d3f51911e74\n'}]",0,123342,321f7c5ecf944a864f7064dddbc514f5bcba1075,23,4,4,11131,,,0,"Imported Translations from Transifex

Change-Id: I87cf8dea55529d9f3ed8c8f4dea70d3f51911e74
",git fetch https://review.opendev.org/openstack/ironic refs/changes/42/123342/4 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/locale/ironic.pot', 'ironic/locale/en_GB/LC_MESSAGES/ironic-log-warning.po', 'ironic/locale/de/LC_MESSAGES/ironic-log-warning.po', 'ironic/locale/ironic-log-warning.pot', 'ironic/locale/en_AU/LC_MESSAGES/ironic.po', 'ironic/locale/fr/LC_MESSAGES/ironic-log-warning.po', 'ironic/locale/en_US/LC_MESSAGES/ironic.po']",7,c3d3339c17c84edcbbf47f6847fe902860525bb5,transifex/translations,"""POT-Creation-Date: 2014-09-23 06:07+0000\n""#: ironic/dhcp/neutron.py:66#: ironic/dhcp/neutron.py:149 #, python-format msgid """" ""No VIFs found for node %(node)s when attempting to update DHCP BOOT "" ""options."" msgstr """" #: ironic/dhcp/neutron.py:163#: ironic/drivers/utils.py:206#: ironic/drivers/modules/snmp.py:647 ironic/drivers/modules/ssh.py:558#: ironic/drivers/modules/snmp.py:73#: ironic/drivers/modules/snmp.py:74#: ironic/drivers/modules/snmp.py:75#: ironic/drivers/modules/snmp.py:78#: ironic/drivers/modules/snmp.py:81#: ironic/drivers/modules/snmp.py:82#: ironic/drivers/modules/snmp.py:85#: ironic/drivers/modules/snmp.py:522#: ironic/drivers/modules/snmp.py:531#: ironic/drivers/modules/snmp.py:537#: ironic/drivers/modules/snmp.py:546#: ironic/drivers/modules/snmp.py:549#: ironic/drivers/modules/snmp.py:556#: ironic/drivers/modules/snmp.py:562#: ironic/objects/base.py:349#: ironic/objects/base.py:358#: ironic/objects/base.py:390#: ironic/objects/base.py:439#: ironic/objects/conductor.py:56","""POT-Creation-Date: 2014-09-22 06:09+0000\n""#: ironic/dhcp/neutron.py:70#: ironic/dhcp/neutron.py:160#: ironic/drivers/utils.py:202#: ironic/drivers/modules/snmp.py:634 ironic/drivers/modules/ssh.py:558#: ironic/drivers/modules/snmp.py:71#: ironic/drivers/modules/snmp.py:72#: ironic/drivers/modules/snmp.py:73#: ironic/drivers/modules/snmp.py:76#: ironic/drivers/modules/snmp.py:79#: ironic/drivers/modules/snmp.py:80#: ironic/drivers/modules/snmp.py:83#: ironic/drivers/modules/snmp.py:509#: ironic/drivers/modules/snmp.py:518#: ironic/drivers/modules/snmp.py:524#: ironic/drivers/modules/snmp.py:533#: ironic/drivers/modules/snmp.py:536#: ironic/drivers/modules/snmp.py:543#: ironic/drivers/modules/snmp.py:549#: ironic/objects/base.py:351#: ironic/objects/base.py:360#: ironic/objects/base.py:392#: ironic/objects/base.py:441#: ironic/objects/conductor.py:59",143,147
openstack%2Ftempest~master~I24b6d55f3537d9e055806daab0135b1c42a8aafd,openstack/tempest,master,I24b6d55f3537d9e055806daab0135b1c42a8aafd,Restoring dashboard tests,MERGED,2014-09-22 23:26:13.000000000,2014-09-26 08:17:42.000000000,2014-09-26 08:17:41.000000000,"[{'_account_id': 3}, {'_account_id': 5196}, {'_account_id': 5623}]","[{'number': 1, 'created': '2014-09-22 23:26:13.000000000', 'files': ['tempest/scenario/test_dashboard_basic_ops.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/c73514c7f2bd8d916b4b1a2f2b92d49f04f8f296', 'message': 'Restoring dashboard tests\n\nWith the fix for bugs 1370137 and 1345955 committed to Horizon,\nthe need for skipping the dashboard tests should be resolved.\n\nChange-Id: I24b6d55f3537d9e055806daab0135b1c42a8aafd\n'}]",0,123281,c73514c7f2bd8d916b4b1a2f2b92d49f04f8f296,11,3,1,5623,,,0,"Restoring dashboard tests

With the fix for bugs 1370137 and 1345955 committed to Horizon,
the need for skipping the dashboard tests should be resolved.

Change-Id: I24b6d55f3537d9e055806daab0135b1c42a8aafd
",git fetch https://review.opendev.org/openstack/tempest refs/changes/81/123281/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/test_dashboard_basic_ops.py'],1,c73514c7f2bd8d916b4b1a2f2b92d49f04f8f296,restore-dashboard-test,," @test.skip_because(bug=""1345955"")",0,1
openstack%2Fheat~master~Iab2cf1f3da0261c30b755d0b87ebb693644a4a29,openstack/heat,master,Iab2cf1f3da0261c30b755d0b87ebb693644a4a29,Resolve AWS::EC2::Instance AZ output to a value if not specified,MERGED,2014-06-27 05:40:56.000000000,2014-09-26 08:13:40.000000000,2014-09-26 08:13:39.000000000,"[{'_account_id': 3}, {'_account_id': 3098}, {'_account_id': 4571}, {'_account_id': 6899}, {'_account_id': 8289}, {'_account_id': 9542}, {'_account_id': 12321}]","[{'number': 1, 'created': '2014-06-27 05:40:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/ea0d7ad3029453cefbcf9de09585ef7cf340f285', 'message': 'Resolve AWS::EC2::Instance AZ output to a value if not specified\n\nThe availability zone output attribute of the instance resource is\nempty if no input is provided by the user for that property.\nInstead of relying on solely user input to determine the value for this,\nthis attribute must be resolved to an appropriate value.\n\nChange-Id: Iab2cf1f3da0261c30b755d0b87ebb693644a4a29\nCloses-Bug: #1334519\n'}, {'number': 2, 'created': '2014-06-27 05:50:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/f1548b61c0c03d6842d668b61fc3c9ad18218ea0', 'message': 'Resolve AWS::EC2::Instance AZ output to a value if not specified\n\nThe availability zone output attribute of the instance resource is\nempty if no input is provided by the user for that property.\nInstead of relying on solely user input to determine the value for this,\nthis attribute must be resolved to an appropriate value.\n\nThe VolumeTest.test_volume_default_az test case had an unmocked function\nwhich is now mocked here to successfuly test this feature.\n\nChange-Id: Iab2cf1f3da0261c30b755d0b87ebb693644a4a29\nCloses-Bug: #1334519\n'}, {'number': 3, 'created': '2014-09-01 09:17:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/a9ab0b7e07f70c850ab55389d0ff1f1cbf7e2ea9', 'message': 'Resolve AWS::EC2::Instance AZ output to a value if not specified\n\nThe availability zone output attribute of the instance resource is\nempty if no input is provided by the user for that property.\nInstead of relying on solely user input to determine the value for this,\nthis attribute must be resolved to an appropriate value.\n\nThe VolumeTest.test_volume_default_az test case had an unmocked function\nwhich is now mocked here to successfuly test this feature.\n\nChange-Id: Iab2cf1f3da0261c30b755d0b87ebb693644a4a29\nCloses-Bug: #1334519\n'}, {'number': 4, 'created': '2014-09-18 06:08:59.000000000', 'files': ['heat/tests/test_instance.py', 'heat/tests/v1_1/fakes.py', 'heat/engine/resources/instance.py', 'heat/tests/test_volume.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/db01dc264db55e6f6c4d6283f6534a148c3ede6c', 'message': 'Resolve AWS::EC2::Instance AZ output to a value if not specified\n\nThe availability zone output attribute of the instance resource is\nempty if no input is provided by the user for that property.\nInstead of relying on solely user input to determine the value for this,\nthis attribute must be resolved to an appropriate value.\n\nThe VolumeTest.test_volume_default_az test case had an unmocked function\nwhich is now mocked here to successfuly test this feature.\n\nChange-Id: Iab2cf1f3da0261c30b755d0b87ebb693644a4a29\nCloses-Bug: #1334519\n'}]",2,103020,db01dc264db55e6f6c4d6283f6534a148c3ede6c,26,7,4,6899,,,0,"Resolve AWS::EC2::Instance AZ output to a value if not specified

The availability zone output attribute of the instance resource is
empty if no input is provided by the user for that property.
Instead of relying on solely user input to determine the value for this,
this attribute must be resolved to an appropriate value.

The VolumeTest.test_volume_default_az test case had an unmocked function
which is now mocked here to successfuly test this feature.

Change-Id: Iab2cf1f3da0261c30b755d0b87ebb693644a4a29
Closes-Bug: #1334519
",git fetch https://review.opendev.org/openstack/heat refs/changes/20/103020/1 && git format-patch -1 --stdout FETCH_HEAD,"['heat/engine/resources/instance.py', 'heat/tests/test_volume.py']",2,ea0d7ad3029453cefbcf9de09585ef7cf340f285,103020," ""UserData"" : ""some data"", self.m.StubOutWithMock(instance.Instance, '_resolve_attribute') instance.Instance._resolve_attribute( 'AvailabilityZone').MultipleTimes().AndReturn(None)"," ""UserData"" : ""some data""",16,4
openstack%2Fkeystone~master~I85de743006e7df4fcf8ddbc3afc1a4bcd69583e6,openstack/keystone,master,I85de743006e7df4fcf8ddbc3afc1a4bcd69583e6,Update architecture documentation,MERGED,2014-09-25 06:13:40.000000000,2014-09-26 08:13:31.000000000,2014-09-26 08:13:30.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 792}, {'_account_id': 1916}, {'_account_id': 2218}, {'_account_id': 2903}, {'_account_id': 5046}, {'_account_id': 5707}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 7191}, {'_account_id': 7725}]","[{'number': 1, 'created': '2014-09-25 06:13:40.000000000', 'files': ['doc/source/architecture.rst'], 'web_link': 'https://opendev.org/openstack/keystone/commit/2e32765413a8a36e3f03e11f74413e3b334530d6', 'message': ""Update architecture documentation\n\nThe architecture docs are very dated, and have several references\nto tenants, PAM backends, and controllers that don't exist.\n\nChange-Id: I85de743006e7df4fcf8ddbc3afc1a4bcd69583e6\n""}]",0,123938,2e32765413a8a36e3f03e11f74413e3b334530d6,8,12,1,6482,,,0,"Update architecture documentation

The architecture docs are very dated, and have several references
to tenants, PAM backends, and controllers that don't exist.

Change-Id: I85de743006e7df4fcf8ddbc3afc1a4bcd69583e6
",git fetch https://review.opendev.org/openstack/keystone refs/changes/38/123938/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/architecture.rst'],1,2e32765413a8a36e3f03e11f74413e3b334530d6,update_architecture,"frontend, for example an authenticate call will validate user/projectGroups, Projects, Domains and Roles, as well as any associated metadata.once a user's credentials have already been verified.of pipelines of WSGI middleware, such as: .. code-block:: ini [pipeline:api_v3] pipeline = sizelimit url_normalize build_auth_context token_auth admin_token_auth xml_body_v3 json_body ec2_extension_v3 s3_extension service_v3* Assignment * :mod:`keystone.assignment.controllers.DomainV3` * :mod:`keystone.assignment.controllers.ProjectV3` * :mod:`keystone.assignment.controllers.RoleV3` * Authentication * :mod:`keystone.auth.controllers.Auth` * :mod:`keystone.catalog.controllers.EndpointV3` * :mod:`keystone.catalog.controllers.RegionV3` * :mod:`keystone.catalog.controllers.ServiceV3` * Identity * :mod:`keystone.identity.controllers.GroupV3` * :mod:`keystone.identity.controllers.UserV3` * Policy * :mod:`keystone.policy.controllers.PolicyV3` * :mod:`keystone.token.controllers.Auth` * :mod:`keystone.assignment.core.Driver` * :mod:`keystone.catalog.core.Driver`* :mod:`keystone.policy.core.Driver`expected to subclass from these classes.``keystone-manage`` command introspects the backends to identify SQL based backendsThe LDAP backend stores Users and Projects in separate Subtrees. Roles are recorded as entries under the Projects. * **User**: has account credentials, is associated with one or more projects or domains * **Group**: a collection of users, is associated with one or more projects or domains * **Project**: unit of ownership in OpenStack, contains one or more users * **Domain**: unit of ownership in OpenStack, contains users, groups and projects * **Role**: a first-class piece of metadata associated with many user-project pairs. * **Token**: identifying credential associated with a user or user and project * **Extras**: bucket of key-value metadata associated with a user-project pair.and Groups to Projects and Domains; the actual backend implementations take varying levels of advantage of that functionality.their users, groups, projects and domains in their existing user systems, a","frontend, for example an authenticate call will validate user/tenantTenants and Roles, as well as any associated metadata.once a user/tenant's credentials have already been verified.of pipelines of WSGI middleware, such as:: [pipeline:public_api] pipeline = token_auth admin_token_auth json_body debug ec2_extension public_service* Identity * :mod:`keystone.identity.core.TenantController` * :mod:`keystone.identity.core.UserController` * :mod:`keystone.identity.core.RoleController` * :mod:`keystone.catalog.core.ServiceController` * :mod:`keystone.service.VersionController` * :mod:`keystone.service.TokenController` * Misc * :mod:`keystone.service.ExtensionsController` At this time, the policy service and associated manager is not exposed as a URL frontend, and has no associated Controller class.expected to subclass from these classes. The default response for the defined APIs in these Drivers is to raise a :mod:`keystone.service.TokenController`.keystone-manage command introspects the backends to identify SQL based backendsPAM Backend ----------- Extra simple backend that uses the current system's PAM service to authenticate, providing a one-to-one relationship between Users and Tenants with the `root` User also having the 'admin' role. The LDAP backend stored Users and Tenants in separate Subtrees. Roles are recorded as entries under the Tenants. * **User**: has account credentials, is associated with one or more tenants * **Tenant**: unit of ownership in OpenStack, contains one or more users * **Role**: a first-class piece of metadata associated with many user-tenant pairs. * **Token**: identifying credential associated with a user or user and tenant * **Extras**: bucket of key-value metadata associated with a user-tenant pair.and Tenants and a many-to-one relationship between Extras and User-Tenant pairs, the actual backend implementations take varying levels of advantage of that functionality.their users, tenants and other metadata in their existing user systems, a",47,41
openstack%2Fkeystone~master~I2785cbe8563e5adcae65aebf88152a5e055c28a9,openstack/keystone,master,I2785cbe8563e5adcae65aebf88152a5e055c28a9,New section for CLI examples in docs,MERGED,2014-09-25 08:16:22.000000000,2014-09-26 08:13:27.000000000,2014-09-26 08:13:26.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 792}, {'_account_id': 1916}, {'_account_id': 2218}, {'_account_id': 2903}, {'_account_id': 5046}, {'_account_id': 5707}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 7186}, {'_account_id': 7191}, {'_account_id': 7725}]","[{'number': 1, 'created': '2014-09-25 08:16:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/05573ad21d6ebb6c7d772d91c944e8c702fcca4e', 'message': ""New section for CLI examples in docs\n\nIn an effort to make 'configuration.rst' less bloated, move\ncommon cli examples to their own section, and port the commands\nover to openstackclient instead of keystoneclient.\n\nChange-Id: I2785cbe8563e5adcae65aebf88152a5e055c28a9\n""}, {'number': 2, 'created': '2014-09-25 08:17:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/c32e790b70aa317a4e71ec536c931e55a7b26382', 'message': ""New section for CLI examples in docs\n\nIn an effort to make 'configuration.rst' less bloated, move\ncommon cli examples to their own section, and port the commands\nover to openstackclient instead of keystoneclient.\n\nChange-Id: I2785cbe8563e5adcae65aebf88152a5e055c28a9\n""}, {'number': 3, 'created': '2014-09-25 14:41:51.000000000', 'files': ['doc/source/index.rst', 'doc/source/configuration.rst', 'doc/source/cli_examples.rst'], 'web_link': 'https://opendev.org/openstack/keystone/commit/06516bf9af5cef93af3373501133303b45dc007b', 'message': ""New section for CLI examples in docs\n\nIn an effort to make 'configuration.rst' less bloated, move\ncommon cli examples to their own section. Changed the titling\nso they are no longer subtitles of another section.\n\nChange-Id: I2785cbe8563e5adcae65aebf88152a5e055c28a9\n""}]",2,123969,06516bf9af5cef93af3373501133303b45dc007b,14,13,3,6482,,,0,"New section for CLI examples in docs

In an effort to make 'configuration.rst' less bloated, move
common cli examples to their own section. Changed the titling
so they are no longer subtitles of another section.

Change-Id: I2785cbe8563e5adcae65aebf88152a5e055c28a9
",git fetch https://review.opendev.org/openstack/keystone refs/changes/69/123969/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/index.rst', 'doc/source/configuration.rst', 'doc/source/cli_examples.rst']",3,05573ad21d6ebb6c7d772d91c944e8c702fcca4e,separate_out_cli_examples,".. Copyright 2011-2012 OpenStack Foundation All Rights Reserved. Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. =============================== Command Line Interface Examples =============================== -------- Projects -------- ``project create`` ------------------ keyword arguments * name * description (optional, defaults to None) * enabled (optional, defaults to True) example: .. code-block:: bash $ keystone tenant-create --name=demo creates a tenant named ""demo"". ``tenant-delete`` ^^^^^^^^^^^^^^^^^ arguments * tenant_id example: .. code-block:: bash $ keystone tenant-delete f2b7b39c860840dfa47d9ee4adffa0b3 Users ----- ``user-create`` ^^^^^^^^^^^^^^^ keyword arguments * name * pass * email * tenant_id (optional, defaults to None) * enabled (optional, defaults to True) example: .. code-block:: bash $ keystone user-create --name=admin \ --pass=secrete \ --tenant_id=2395953419144b67955ac4bab96b8fd2 \ --email=admin@example.com ``user-delete`` ^^^^^^^^^^^^^^^ keyword arguments * user_id example: .. code-block:: bash $ keystone user-delete f2b7b39c860840dfa47d9ee4adffa0b3 ``user-list`` ^^^^^^^^^^^^^ list users in the system, optionally by a specific tenant (identified by tenant_id) arguments * tenant_id (optional, defaults to None) example: .. code-block:: bash $ keystone user-list ``user-update`` ^^^^^^^^^^^^^^^^^^^^^ arguments * user_id keyword arguments * name Desired new user name (Optional) * email Desired new email address (Optional) * enabled <true|false> Enable or disable user (Optional) example: .. code-block:: bash $ keystone user-update 03c84b51574841ba9a0d8db7882ac645 --email=newemail@example.com ``user-password-update`` ^^^^^^^^^^^^^^^^^^^^^^^^ arguments * user_id * password example: .. code-block:: bash $ keystone user-password-update --pass foo 03c84b51574841ba9a0d8db7882ac645 Roles ----- ``role-create`` ^^^^^^^^^^^^^^^ arguments * name example: .. code-block:: bash $ keystone role-create --name=demo ``role-delete`` ^^^^^^^^^^^^^^^ arguments * role_id example: .. code-block:: bash $ keystone role-delete 19d1d3344873464d819c45f521ff9890 ``role-list`` ^^^^^^^^^^^^^ example: .. code-block:: bash $ keystone role-list ``role-get`` ^^^^^^^^^^^^ arguments * role_id example: .. code-block:: bash $ keystone role-get 19d1d3344873464d819c45f521ff9890 ``user-role-add`` ^^^^^^^^^^^^^^^^^ keyword arguments * user <user-id> * role <role-id> * tenant_id <tenant-id> example: .. code-block:: bash $ keystone user-role-add \ --user=96a6ebba0d4c441887aceaeced892585 \ --role=f8dd5a2e4dc64a41b96add562d9a764e \ --tenant_id=2395953419144b67955ac4bab96b8fd2 ``user-role-remove`` ^^^^^^^^^^^^^^^^^^^^ keyword arguments * user <user-id> * role <role-id> * tenant_id <tenant-id> example: .. code-block:: bash $ keystone user-role-remove \ --user=96a6ebba0d4c441887aceaeced892585 \ --role=f8dd5a2e4dc64a41b96add562d9a764e \ --tenant_id=2395953419144b67955ac4bab96b8fd2 Services -------- ``service-create`` ^^^^^^^^^^^^^^^^^^ keyword arguments * name * type * description example: .. code-block:: bash $ keystone service-create \ --name=nova \ --type=compute \ --description=""Nova Compute Service"" ``service-list`` ^^^^^^^^^^^^^^^^ arguments * service_id example: .. code-block:: bash $ keystone service-list ``service-get`` ^^^^^^^^^^^^^^^ arguments * service_id example: .. code-block:: bash $ keystone service-get 08741d8ed88242ca88d1f61484a0fe3b ``service-delete`` ^^^^^^^^^^^^^^^^^^ arguments * service_id example: .. code-block:: bash $ keystone service-delete 08741d8ed88242ca88d1f61484a0fe3b ",,290,271
openstack%2Fkeystone~master~I726e13aa7c333973cf92bbe1550b939c634db493,openstack/keystone,master,I726e13aa7c333973cf92bbe1550b939c634db493,Clean up the Configuration documentation,MERGED,2014-09-25 07:52:36.000000000,2014-09-26 08:13:17.000000000,2014-09-26 08:13:16.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 792}, {'_account_id': 1916}, {'_account_id': 2218}, {'_account_id': 2903}, {'_account_id': 5046}, {'_account_id': 5707}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 7191}, {'_account_id': 7725}]","[{'number': 1, 'created': '2014-09-25 07:52:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/60f61c2fca6ec82760851d0fae36d3fb9833ecf4', 'message': 'Clean up the Configuration documentation\n\nLots of lines that were well over 80 characters; several code\nexamples were missing code-block syntax. General inconsistencies\nbetween what is actually available vs. what is documented.\n\nChange-Id: I726e13aa7c333973cf92bbe1550b939c634db493\n'}, {'number': 2, 'created': '2014-09-25 08:17:10.000000000', 'files': ['doc/source/configuration.rst'], 'web_link': 'https://opendev.org/openstack/keystone/commit/ff9637bafbf7b7df3c1d8b79bd9196201606d805', 'message': 'Clean up the Configuration documentation\n\nLots of lines that were well over 80 characters; several code\nexamples were missing code-block syntax. General inconsistencies\nbetween what is actually available vs. what is documented.\n\nChange-Id: I726e13aa7c333973cf92bbe1550b939c634db493\n'}]",1,123960,ff9637bafbf7b7df3c1d8b79bd9196201606d805,10,12,2,6482,,,0,"Clean up the Configuration documentation

Lots of lines that were well over 80 characters; several code
examples were missing code-block syntax. General inconsistencies
between what is actually available vs. what is documented.

Change-Id: I726e13aa7c333973cf92bbe1550b939c634db493
",git fetch https://review.opendev.org/openstack/keystone refs/changes/60/123960/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/configuration.rst'],1,60f61c2fca6ec82760851d0fae36d3fb9833ecf4,update_configuration,"PasteDeploy configuration file is specified by the ``config_file`` parameter in ``[paste_deploy]`` section of the primary configuration file. If the parameter is not an absolute path, then Keystone looks for it in the same directories as above. If not specified, WSGI pipeline definitions are loaded from the primary configuration file.following options: .. code-block:: ini* ``<plugin name>`` - specify the class which handles to authentication method, in the same manner as one would specify a backend driver.* ``auth_context`` - user authentication context, a dictionary shared by all plugins. It contains ``method_names`` and ``extras`` by default. ``method_names`` is a list and ``extras`` is a dictionary.The ``REMOTE_USER`` environment variable is only set from a containing webserver. However, to ensure that a user must go through other authentication mechanisms, even if this variable is set, remove ``external`` from the list of plugins specified in ``methods``. This effectively disables external authentication. For more details, refer to :doc:`ExternalAuthentication <external-auth>`.For a customized provider, ``token_format`` must not set to ``PKI`` or ``UUID``. Both UUID and PKI-based tokens are bearer tokens, meaning that they mustThe current architectural approaches for both UUID and PKI-based tokens have* ``certfile`` - Location of certificate used to verify tokens. Default is ``/etc/keystone/ssl/certs/signing_cert.pem`` * ``keyfile`` - Location of private key used to sign tokens. Default is ``/etc/keystone/ssl/private/signing_key.pem`` * ``ca_certs`` - Location of certificate for the authority that issued the above certificate. Default is ``/etc/keystone/ssl/certs/ca.pem``First create a certificate request configuration file (e.g. ``cert_req.conf``): .. code-block:: iniand ``signing_key.pem``. Send ``signing_cert_req.pem`` to your CA to request a token signing certificate and make sure to ask the certificate to be in PEM format. Also, make sure your trusted CA certificate chain is also in PEM format.If your certificate directory path is different from the default ``/etc/keystone/ssl/certs``, make sure it is reflected in the ``[signing]`` section of the configuration file.A dynamic database-backed driver fully supporting persistent configuration. ``keystone.conf`` example: .. code-block:: ini``keystone.conf`` example: .. code-block:: iniit at the beginning of any desired WSGI pipelines: .. code-block:: ini``*_body`` middleware and before ``*_extension`` filters is recommended): .. code-block:: ini* ``cacert.pem``: Certificate Authority chain to validate against. * ``ssl_cert.pem``: Public certificate for Keystone server. * ``middleware.pem``: Public and private certificate for Keystone middleware/client. * ``cakey.pem``: Private key for the CA. * ``ssl_key.pem``: Private key for the Keystone server.certificates: .. code-block:: ini* ``cert_subject``: The subject to set in the certificate. Defaults to /C=US/ST=Unset/L=Unset/O=Unset/CN=localhost. When setting the subject it is important to set CN to be the address of the server so client validation will succeed. This generally means having the subject be at least /CN=<keystone ip>``keystone-paste.ini`` e.g.: .. code-block:: iniKeystone provides an optional extension that adds the capability to assign roles to a domain that, rather than affect the domain itself, are instead inherited to all projects owned by that domain. This extension is disabled by default, but can be enabled by including the following in ``keystone.conf``: .. code-block:: initoken binding should be used for in ``keystone.conf`` e.g.: .. code-block:: ini mechanism is used. e.g.: .. code-block:: inimay override this global value with a specific limit, for example: .. code-block:: iniof protection is applied to it, where each line is of the form:: <api name>: <rule statement> or <match statement> where: ``<rule statement>`` can contain ``<rule statement>`` or ``<match statement>`` ``<match statement>`` is a set of identifiers that must match between the token.. code-block:: javascript Indicates that to create a user you must have the admin role in your token andEach component of a match statement is of the form:: <attribute from token>:<constant> or <attribute related to API call> .. code-block:: javascript Ensure that your ``keystone.conf`` is configured to use a SQL driver: .. code-block:: inienvironment: .. code-block:: ini* ``db_sync``: Sync the database. * ``db_version``: Print the current migration version of the database. * ``mapping_purge``: Purge the identity mapping table. * ``pki_setup``: Initialize the certificates used to sign tokens. * ``saml_idp_metadata``: Generate identity provider metadata. * ``ssl_setup``: Generate certificates for SSL. * ``token_flush``: Purge expired tokensUsers, tenants, and roles must be administered using admin credentials.=============================================================The corresponding entries in the Keystone configuration file are: .. code-block:: inicorresponding entries in the Keystone configuration file are: .. code-block:: iniis: .. code-block:: iniwill look like: .. code-block:: inito extract the value from an integer attribute like in Active Directory: .. code-block:: inispecified classes in the LDAP module so you can configure them like: .. code-block:: inisection is used to enable debugging: .. code-block:: inimark any user who is a member of ``enabled_users`` as enabled: .. code-block:: inifunctionality can easily be configured as follows: .. code-block:: ini``keystone.conf`` options set: .. code-block:: inisection: .. code-block:: iniConnection pool configuration is added in ``[ldap]`` configuration section: .. code-block:: ini","PasteDeploy configuration file is specified by the ``config_file`` parameter in ``[paste_deploy]`` section of the primary configuration file. If the parameter is not an absolute path, then Keystone looks for it in the same directories as above. If not specified, WSGI pipeline definitions are loaded from the primary configuration file.following options::* ``<plugin name>`` - specify the class which handles to authentication method, in the same manner as one would specify a backend driver.* ``auth_context`` - user authentication context, a dictionary shared by all plugins. It contains ``method_names`` and ``extras`` by default. ``method_names`` is a list and ``extras`` is a dictionary.The ``REMOTE_USER`` environment variable is only set from a containing webserver. However, to ensure that a user must go through other authentication mechanisms, even if this variable is set, remove ``external`` from the list of plugins specified in ``methods``. This effectively disables external authentication. For more details, refer to :doc:`External Authentication <external-auth>`.For a customized provider, ``token_format`` must not set to ``PKI`` or ``UUID``. Both UUID- and PKI-based tokens are bearer tokens, meaning that they mustThe current architectural approaches for both UUID- and PKI-based tokens have* ``certfile`` - Location of certificate used to verify tokens. Default is ``/etc/keystone/ssl/certs/signing_cert.pem`` * ``keyfile`` - Location of private key used to sign tokens. Default is ``/etc/keystone/ssl/private/signing_key.pem`` * ``ca_certs`` - Location of certificate for the authority that issued the above certificate. Default is ``/etc/keystone/ssl/certs/ca.pem``First create a certificate request configuration file (e.g. ``cert_req.conf``)::and ``signing_key.pem``. Send ``signing_cert_req.pem`` to your CA to request a token signing certificate and make sure to ask the certificate to be in PEM format. Also, make sure your trusted CA certificate chain is also in PEM format.If your certificate directory path is different from the default ``/etc/keystone/ssl/certs``, make sure it is reflected in the ``[signing]`` section of the configuration file.A dynamic database-backed driver fully supporting persistent configuration via keystoneclient administration commands (e.g. ``keystone endpoint-create``). ``keystone.conf`` example::``keystone.conf`` example::it at the beginning of any desired WSGI pipelines::``*_body`` middleware and before ``*_extension`` filters is recommended)::cacert.pem Certificate Authority chain to validate against. ssl_cert.pem Public certificate for Keystone server. middleware.pem Public and private certificate for Keystone middleware/client. cakey.pem Private key for the CA. ssl_key.pem Private key for the Keystone server.certificates::* ``cert_subject``: The subject to set in the certificate. Defaults to /C=US/ST=Unset/L=Unset/O=Unset/CN=localhost. When setting the subject it is important to set CN to be the address of the server so client validation will succeed. This generally means having the subject be at least /CN=<keystone ip>``keystone-paste.ini`` e.g.::Keystone provides an optional extension that adds the capability to assign roles to a domain that, rather than affect the domain itself, are instead inherited to all projects owned by that domain. This extension is disabled by default, but can be enabled by including the following in ``keystone.conf``::token binding should be used for in ``keystone.conf`` e.g.:: mechanism is used. e.g.::may override this global value with a specific limit, for example::of protection is applied to it, where each line is of the form: <api name>: <rule statement> or <match statement> where <rule statement> can be contain <rule statement> or <match statement> <match statement> is a set of identifiers that must match between the tokenindicates that to create a user you must have the admin role in your token andEach component of a match statement is of the form: <attribute from token>:<constant> or <attribute related to API call>Ensure that your ``keystone.conf`` is configured to use a SQL driver::environment::* ``db_sync``: Sync the database schema. * ``pki_setup``: Initialize the certificates for PKI based tokens. * ``purge_mapping``: Purge user and group public ID mappings. * ``ssl_setup``: Generate certificates for HTTPS.User, tenants, and roles must be administered using admin credentials.======================================================================================================================The corresponding entries in the Keystone configuration file are::corresponding entries in the Keystone configuration file are::is::will look like::to extract the value from an integer attribute like in Active Directory::specified classes in the LDAP module so you can configure them like::section is used to enable debugging::mark any user who is a member of ``enabled_users`` as enabled::functionality can easily be configured as follows::``keystone.conf`` options set::section::Connection pool configuration is added in ``[ldap]`` configuration section::",142,80
openstack%2Fnova~master~Iaaadc26f3f51ee091419beaa12e0508623eed99e,openstack/nova,master,Iaaadc26f3f51ee091419beaa12e0508623eed99e,Add plan for kilo blueprints: when is a blueprint needed,MERGED,2014-08-25 18:37:49.000000000,2014-09-26 08:13:00.000000000,2014-09-26 08:12:58.000000000,"[{'_account_id': 3}, {'_account_id': 67}, {'_account_id': 308}, {'_account_id': 360}, {'_account_id': 642}, {'_account_id': 679}, {'_account_id': 782}, {'_account_id': 1030}, {'_account_id': 1247}, {'_account_id': 1561}, {'_account_id': 1779}, {'_account_id': 1812}, {'_account_id': 1849}, {'_account_id': 2271}, {'_account_id': 2750}, {'_account_id': 2835}, {'_account_id': 4393}, {'_account_id': 5170}, {'_account_id': 5292}, {'_account_id': 5441}, {'_account_id': 5511}, {'_account_id': 6167}, {'_account_id': 6873}, {'_account_id': 9578}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-25 18:37:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/10bf45b9aae56464419a67934f87aa0bdc7cd62d', 'message': ""Add plan for kilo blueprints: when is a blueprint needed\n\nAt the nova mid-cycle we agreed that while specs have made our\nprocess significantly better, we think the current approach still leaves\nmuch to be desired. This document is the outcome of the discussion at\nthe mid-cycle on how we want to manage blueprints in kilo.\n\nThis proposal is being placed in devref, as we want to use gerrit's\nworkflow (two core reviews, inline comments, revisions etc) and this is\na document for developers.\n\nIt turns out adding the specs requirement to blueprints adds a\nnon-trivial amount of overhead, revisit when a blueprint and spec is\nneeded to minimize overhead in cases where it isn't helpful.\n\nChange-Id: Iaaadc26f3f51ee091419beaa12e0508623eed99e\n""}, {'number': 2, 'created': '2014-08-29 20:44:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/460c7beea6f613d553b9e86e975efa069b5052ff', 'message': ""Add plan for kilo blueprints: when is a blueprint needed\n\nAt the nova mid-cycle we agreed that while specs have made our\nprocess significantly better, we think the current approach still leaves\nmuch to be desired. This document is the outcome of the discussion at\nthe mid-cycle on how we want to manage blueprints in kilo.\n\nThis proposal is being placed in devref, as we want to use gerrit's\nworkflow (two core reviews, inline comments, revisions etc) and this is\na document for developers.\n\nIt turns out adding the specs requirement to blueprints adds a\nnon-trivial amount of overhead, revisit when a blueprint and spec is\nneeded to minimize overhead in cases where it isn't helpful.\n\nChange-Id: Iaaadc26f3f51ee091419beaa12e0508623eed99e\n""}, {'number': 3, 'created': '2014-09-12 22:34:17.000000000', 'files': ['doc/source/devref/index.rst', 'doc/source/devref/kilo.blueprints.rst'], 'web_link': 'https://opendev.org/openstack/nova/commit/93a7b48102441d4c40f7c6f7502cb6366f6e1edf', 'message': ""Add plan for kilo blueprints: when is a blueprint needed\n\nAt the nova mid-cycle we agreed that while specs have made our\nprocess significantly better, we think the current approach still leaves\nmuch to be desired. This document is the outcome of the discussion at\nthe mid-cycle on how we want to manage blueprints in kilo.\n\nThis proposal is being placed in devref, as we want to use gerrit's\nworkflow (two core reviews, inline comments, revisions etc) and this is\na document for developers.\n\nIt turns out adding the specs requirement to blueprints adds a\nnon-trivial amount of overhead, revisit when a blueprint and spec is\nneeded to minimize overhead in cases where it isn't helpful.\n\nChange-Id: Iaaadc26f3f51ee091419beaa12e0508623eed99e\n""}]",62,116699,93a7b48102441d4c40f7c6f7502cb6366f6e1edf,54,25,3,1849,,,0,"Add plan for kilo blueprints: when is a blueprint needed

At the nova mid-cycle we agreed that while specs have made our
process significantly better, we think the current approach still leaves
much to be desired. This document is the outcome of the discussion at
the mid-cycle on how we want to manage blueprints in kilo.

This proposal is being placed in devref, as we want to use gerrit's
workflow (two core reviews, inline comments, revisions etc) and this is
a document for developers.

It turns out adding the specs requirement to blueprints adds a
non-trivial amount of overhead, revisit when a blueprint and spec is
needed to minimize overhead in cases where it isn't helpful.

Change-Id: Iaaadc26f3f51ee091419beaa12e0508623eed99e
",git fetch https://review.opendev.org/openstack/nova refs/changes/99/116699/3 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/devref/index.rst', 'doc/source/devref/kilo.blueprints.rst']",2,10bf45b9aae56464419a67934f87aa0bdc7cd62d,bp/s,"================== Blueprints in Kilo ================== =========================== When is a blueprint needed? =========================== Juno ----- Every new feature needs a blueprint, and all blueprint need a spec. If the blueprint is small, then the spec should be small. Problem -------- The specs repo is very good for hashing out design, but if a new feature doesn't need any design discussion, requiring a spec is a significant overhead. If the spec is just used for documentation and not design discussion, we are adding a whole extra round of reviewing when the documentation can just be done in the commit message. Kilo ----- A blueprint is needed for any feature that requires a design discussion. In the past we have used blueprints and specs for all new features no matter how big or small they are. We have added the specs workflow to help improve the design discussion part of blueprints, but are using it as a documentation tool as well. If a new feature is straight forward enough that it doesn't need any design discussion we shouldn't require a blueprint purely for documentation purposes. Instead we should require the ``DocImpact`` flag and a thorough commit message. Guidelines for when a feature doesn't need a blueprint (and can use a wishlist bug instead). * Is the feature a single self contained change? * If the feature touches code all over the place, it probably should have a design discussion. * If the feature is big enough that it needs more then one commit, it probably should have a design discussion. * Not an API change. * API changes always require a design discussion. Examples --------- Juno blueprints where no blueprint would be needed in Kilo: * backportable-db-migrations-juno.rst * add-all-in-list-operator-to-extra-spec-ops.rst * enabled-qemu-memballoon-stats.rst * hyper-v-console-log.rst * hyper-v-soft-reboot.rst * instance-network-info-hook.rst * io-ops-weight.rst * per-aggregate-filters.rst * restrict-image-isolation-with-defined-keys.rst Juno blueprints where a blueprint would still be needed in Kilo: * add-ironic-driver.rst * config-drive-image-property.rst * db2-database.rst * cross-service-request-id.rst * clean-logs.rst * i18n-enablement.rst * encryption-with-barbican.rst * juno-slaveification.rst * extensible-resource-tracking.rst * cold-migration-with-target.rst * allow-image-to-be-specified-during-rescue.rst * add-differencing-vhdx-resize-support.rst * add-virtio-scsi-bus-for-bdm.rst * compute-manager-objects-juno.rst * allocation-ratio-to-resource-tracker.rst * better-support-for-multiple-networks.rst * nova-pagination.rst * convert_ec2_api_to_use_nova_objects.rst * make-resource-tracker-use-objects.rst * object-subclassing.rst * virt-objects-juno.rst * scheduler-lib.rst * return-status-for-hypervisor-node.rst * server-count-api.rst * server-group-quotas.rst * ec2-volume-and-snapshot-tags.rst * enforce-unique-instance-uuid-in-db.rst * find-host-and-evacuate-instance.rst * input-output-based-numa-scheduling.rst * libvirt-domain-listing-speedup.rst * libvirt-disk-discard-option.rst * libvirt-driver-class-refactor.rst * libvirt-lxc-user-namespaces.rst * libvirt-driver-domain-metadata.rst * libvirt-sheepdog-backed-instances.rst * libvirt-start-lxc-from-block-devices.rst * libvirt-volume-snap-network-disk.rst * log-request-id-mappings.rst * lvm-ephemeral-storage-encryption.rst * v2-on-v3-api.rst * v3-api-schema.rst * v3-diagnostics.rst * tag-instances.rst * support-cinderclient-v2.rst * use-oslo-vmware.rst * return-all-servers-during-multiple-create.rst * remove-cast-to-schedule-run-instance.rst * persistent-resource-claim.rst * serial-ports.rst * servers-list-support-multi-status.rst * xenapi-vcpu-topology.rst * selecting-subnet-when-creating-vm.rst * refactor-network-api.rst * migrate-libvirt-volumes.rst * move-prep-resize-to-conductor.rst * nfv-multiple-if-1-net.rst * on-demand-compute-update.rst * pci-passthrough-sriov.rst * quiesced-image-snapshots-with-qemu-guest-agent.rst * rbd-clone-image-handler.rst * rescue-attach-all-disks.rst * standardize-nova-image.rst * string-field-max-length.rst * support-console-log-migration.rst * use-libvirt-storage-pools.rst * user-defined-shutdown.rst * vif-vhostuser.rst * virt-driver-cpu-pinning.rst * virt-driver-large-pages.rst * virt-driver-numa-placement.rst * virt-driver-vcpu-topology.rst * vmware-driver-ova-support.rst * vmware-ephemeral-disk-support.rst * vmware-hot-plug.rst * vmware-spbm-support.rst * vmware-vsan-support.rst * websocket-proxy-to-host-security.rst * xenapi-set-ipxe-url-as-img-metadata.rst ",,148,0
openstack%2Fnova~master~Id7bbd60407f82c3f6fe196ccb51498ea7f7b8d8b,openstack/nova,master,Id7bbd60407f82c3f6fe196ccb51498ea7f7b8d8b,Reopen eventlet hub in MultiprocessWSGITest launcher process,ABANDONED,2014-09-24 09:25:49.000000000,2014-09-26 08:07:57.000000000,,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 782}, {'_account_id': 1812}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-24 09:25:49.000000000', 'files': ['nova/tests/integrated/test_multiprocess_api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/5406d2c05cba7dbc541d7af7053bc17ec9ee1900', 'message': ""Reopen eventlet hub in MultiprocessWSGITest launcher process\n\nThe 'launcher' worker we spawn is forked off of a process that ran the\nwhole setUp for the integrated_helpers._IntegratedTestBase that starts a\nbunch of services in the same process (meaning a bunch of\ngreentrheads).\n\nIt's a good idea to re-initialize the eventlet hub in this case (we do\nit in child workers as well). In a real service process we don't need to\ndo this  we are usually the first and only greenthread at the time.\n\nChange-Id: Id7bbd60407f82c3f6fe196ccb51498ea7f7b8d8b\nPartial-bug: #1357578\n""}]",0,123667,5406d2c05cba7dbc541d7af7053bc17ec9ee1900,12,7,1,5511,,,0,"Reopen eventlet hub in MultiprocessWSGITest launcher process

The 'launcher' worker we spawn is forked off of a process that ran the
whole setUp for the integrated_helpers._IntegratedTestBase that starts a
bunch of services in the same process (meaning a bunch of
greentrheads).

It's a good idea to re-initialize the eventlet hub in this case (we do
it in child workers as well). In a real service process we don't need to
do this  we are usually the first and only greenthread at the time.

Change-Id: Id7bbd60407f82c3f6fe196ccb51498ea7f7b8d8b
Partial-bug: #1357578
",git fetch https://review.opendev.org/openstack/nova refs/changes/67/123667/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/integrated/test_multiprocess_api.py'],1,5406d2c05cba7dbc541d7af7053bc17ec9ee1900,1357578/fix_wsgi_integrated_timeout,import eventlet # NOTE(ndipanov): Reopen the eventlet hub since the process we fork # off of has all the other service started in greenthread workers. # (see integrated_helpers._IntegratedTestBase._setup_services) eventlet.hubs.use_hub(),,5,0
openstack%2Fkeystone~master~I47b61886698232a7d3dfb4b502d61723cb0eb786,openstack/keystone,master,I47b61886698232a7d3dfb4b502d61723cb0eb786,Fix failure of delete domain group grant when identity is LDAP.,MERGED,2014-09-23 20:53:52.000000000,2014-09-26 07:57:34.000000000,2014-09-26 07:57:33.000000000,"[{'_account_id': 3}, {'_account_id': 2218}, {'_account_id': 2903}, {'_account_id': 5707}, {'_account_id': 6482}, {'_account_id': 7725}, {'_account_id': 9098}, {'_account_id': 9142}]","[{'number': 1, 'created': '2014-09-23 20:53:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/041a01159e7ecdbb83b0a3ba14cc82ca741c8cef', 'message': 'Fix exception delete domain group grant in LDAP\n\nNow, when deleting a domain group assignment\nusing a not domain-aware backend, such as\nLDAP, we get NotImplemented exception, instead\nof AttributeError.\n\nChange-Id: I47b61886698232a7d3dfb4b502d61723cb0eb786\nCloses-Bug: 1373113\n'}, {'number': 2, 'created': '2014-09-25 11:24:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/2e255a202b01df9504e1756a415e7ce216262a51', 'message': 'Fix failure of delete domain group grant when identity is LDAP.\n\nWhen deleting a domain group assignment while using a non\ndomain-aware backend, such as LDAP, an AttributeError was being\nraised when trying to find all the relevent tokens. This was due\nto a hang over from when you had to pass domain scope to\nlist_user_in_group(). This only affected domain group grants,\nby luck we got away with it for group project grants.\n\nChange-Id: I47b61886698232a7d3dfb4b502d61723cb0eb786\nCloses-Bug: 1373113\n'}, {'number': 3, 'created': '2014-09-25 11:32:17.000000000', 'files': ['keystone/assignment/core.py', 'keystone/tests/test_backend_ldap.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/5b331f469dcf8062f5172209bc75f21a8ab8c290', 'message': 'Fix failure of delete domain group grant when identity is LDAP.\n\nWhen deleting a domain group assignment while using a non\ndomain-aware backend, such as LDAP, an AttributeError was being\nraised when trying to find all the relevent tokens. This was due\nto a hang over from when you had to pass domain scope to\nlist_user_in_group(). This only affected domain group grants,\nby luck we got away with it for group project grants.\n\nChange-Id: I47b61886698232a7d3dfb4b502d61723cb0eb786\nCloses-Bug: 1373113\n'}]",1,123585,5b331f469dcf8062f5172209bc75f21a8ab8c290,20,8,3,9142,,,0,"Fix failure of delete domain group grant when identity is LDAP.

When deleting a domain group assignment while using a non
domain-aware backend, such as LDAP, an AttributeError was being
raised when trying to find all the relevent tokens. This was due
to a hang over from when you had to pass domain scope to
list_user_in_group(). This only affected domain group grants,
by luck we got away with it for group project grants.

Change-Id: I47b61886698232a7d3dfb4b502d61723cb0eb786
Closes-Bug: 1373113
",git fetch https://review.opendev.org/openstack/keystone refs/changes/85/123585/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/assignment/core.py'],1,041a01159e7ecdbb83b0a3ba14cc82ca741c8cef,bug/1373113," hints = None if domain_id: hints = driver_hints.Hints() hints.add_filter('domain_id', domain_id) hints):", domain_id):,5,1
openstack%2Fpython-novaclient~master~I2b3ab478e5ad6e885e88bf0471502c9c971e46bd,openstack/python-novaclient,master,I2b3ab478e5ad6e885e88bf0471502c9c971e46bd,Fix parameter description in create_server,MERGED,2014-09-25 00:18:21.000000000,2014-09-26 07:57:23.000000000,2014-09-26 07:57:22.000000000,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 1849}, {'_account_id': 5367}, {'_account_id': 7118}, {'_account_id': 9545}]","[{'number': 1, 'created': '2014-09-25 00:18:21.000000000', 'files': ['novaclient/v1_1/servers.py'], 'web_link': 'https://opendev.org/openstack/python-novaclient/commit/6bbedd1a13966c1d808521228196700746ce7323', 'message': 'Fix parameter description in create_server\n\nMake clear the parameter name is ""metadata"", not ""meta"" -- this is\nconfusing at the moment as some other functions do use just ""meta"" as\ntheir metadata argument.\n\nChange-Id: I2b3ab478e5ad6e885e88bf0471502c9c971e46bd\n'}]",0,123893,6bbedd1a13966c1d808521228196700746ce7323,12,6,1,7118,,,0,"Fix parameter description in create_server

Make clear the parameter name is ""metadata"", not ""meta"" -- this is
confusing at the moment as some other functions do use just ""meta"" as
their metadata argument.

Change-Id: I2b3ab478e5ad6e885e88bf0471502c9c971e46bd
",git fetch https://review.opendev.org/openstack/python-novaclient refs/changes/93/123893/1 && git format-patch -1 --stdout FETCH_HEAD,['novaclient/v1_1/servers.py'],1,6bbedd1a13966c1d808521228196700746ce7323,meta-v-metadata, :param metadata: Metadata to give newly-created image entity, :param meta: Metadata to give newly-created image entity,1,1
openstack%2Fnova~master~Icea492134239296f1570b2a4208eb0868315aa5f,openstack/nova,master,Icea492134239296f1570b2a4208eb0868315aa5f,Fix unset extra_spec for a flavor,MERGED,2014-07-13 04:19:19.000000000,2014-09-26 07:57:08.000000000,2014-09-26 07:57:06.000000000,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 1501}, {'_account_id': 1653}, {'_account_id': 2750}, {'_account_id': 4393}, {'_account_id': 4601}, {'_account_id': 5170}, {'_account_id': 5367}, {'_account_id': 5511}, {'_account_id': 5754}, {'_account_id': 7166}, {'_account_id': 7665}, {'_account_id': 8247}, {'_account_id': 9008}, {'_account_id': 9420}, {'_account_id': 9550}, {'_account_id': 9569}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 10614}, {'_account_id': 10618}, {'_account_id': 11600}, {'_account_id': 11929}, {'_account_id': 12857}]","[{'number': 1, 'created': '2014-07-13 04:19:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/5adc179e1fa68554cff03634e85e045af8e6a65c', 'message': 'Fix unset extra_spec for a flavor\n\nA flavor extra_specs key cannot be deleted using \'nova flavor-key\'.\nUpdates to a flavor\'s extra_specs are not being registered by\nNovaObject, such that when NovaObject.obj_what_changed is called,\nno changes in the extra_specs field are picked up. In order for\nchanges in extra_specs to be picked up, the flavor has to have a new\ncopy of the updated extra_specs dict. Simply deleting the key from the\ndict via \'del\' does not register the updates.\n\nAlso, if you delete the last remaining extra_specs key, nova would\nthrow an sqlalchemy warning, i.e. \'SAWarning: The IN-predicate on\n""instance_type_extra_specs.key"" was invoked with an empty sequence\'.\nThis was caused by calling db.flavor_extra_specs_update_or_create\nwith an empty dict of keys to add or update. This patch fixes it\ncalling db.flavor_extra_specs_update_or_create only on a non-empty\ndict of keys.\n\nChange-Id: Icea492134239296f1570b2a4208eb0868315aa5f\nCloses-Bug: #1340885\n'}, {'number': 2, 'created': '2014-07-18 13:47:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/fea9cedbb920abb119d0fddec03e0709f31716f4', 'message': 'Fix unset extra_spec for a flavor\n\nA flavor extra_specs key cannot be deleted using \'nova flavor-key\'.\nUpdates to a flavor\'s extra_specs are not being registered by\nNovaObject, such that when NovaObject.obj_what_changed is called,\nno changes in the extra_specs field are picked up. In order for\nchanges in extra_specs to be picked up, the flavor has to have a new\ncopy of the updated extra_specs dict. Simply deleting the key from the\ndict via \'del\' does not register the updates.\n\nAlso, if you delete the last remaining extra_specs key, nova would\nthrow an sqlalchemy warning, i.e. \'SAWarning: The IN-predicate on\n""instance_type_extra_specs.key"" was invoked with an empty sequence\'.\nThis was caused by calling db.flavor_extra_specs_update_or_create\nwith an empty dict of keys to add or update. This patch fixes it\ncalling db.flavor_extra_specs_update_or_create only on a non-empty\ndict of keys.\n\nChange-Id: Icea492134239296f1570b2a4208eb0868315aa5f\nCloses-Bug: #1340885\n'}, {'number': 3, 'created': '2014-08-12 05:12:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/03d827dc418f59931b91deb78ef77b4d2d160cad', 'message': 'Fix unset extra_spec for a flavor\n\nA flavor extra_specs key cannot be deleted using \'nova flavor-key\'.\nUpdates to a flavor\'s extra_specs are not being registered by\nNovaObject, such that when NovaObject.obj_what_changed is called,\nno changes in the extra_specs field are picked up. In order for\nchanges in extra_specs to be picked up, the updated extra_specs\nhas to be compared against the _orig_extra_specs. The same approach\nalso applies to the flavor\'s projects, where the updated projects\nhas to be compared against the _orig_projects.\n\nAlso, if you delete the last remaining extra_specs key, nova would\nthrow an sqlalchemy warning, i.e. \'SAWarning: The IN-predicate on\n""instance_type_extra_specs.key"" was invoked with an empty sequence\'.\nThis was caused by calling db.flavor_extra_specs_update_or_create\nwith an empty dict of keys to add or update. This patch fixes it\ncalling db.flavor_extra_specs_update_or_create only on a non-empty\ndict of keys.\n\nChange-Id: Icea492134239296f1570b2a4208eb0868315aa5f\nCloses-Bug: #1340885\n'}, {'number': 4, 'created': '2014-09-03 17:24:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/79218c9a266a988a2f5753ee64bbf48cd60b1ee8', 'message': 'Fix unset extra_spec for a flavor\n\nA flavor extra_specs key cannot be deleted using \'nova flavor-key\'.\nUpdates to a flavor\'s extra_specs are not being registered by\nNovaObject, such that when NovaObject.obj_what_changed is called,\nno changes in the extra_specs field are picked up. In order for\nchanges in extra_specs to be picked up, the updated extra_specs\nhas to be compared against the _orig_extra_specs. The same approach\nalso applies to the flavor\'s projects, where the updated projects\nhas to be compared against the _orig_projects.\n\nAlso, if you delete the last remaining extra_specs key, nova would\nthrow an sqlalchemy warning, i.e. \'SAWarning: The IN-predicate on\n""instance_type_extra_specs.key"" was invoked with an empty sequence\'.\nThis was caused by calling db.flavor_extra_specs_update_or_create\nwith an empty dict of keys to add or update. This patch fixes it\ncalling db.flavor_extra_specs_update_or_create only on a non-empty\ndict of keys.\n\nChange-Id: Icea492134239296f1570b2a4208eb0868315aa5f\nCloses-Bug: #1340885\n'}, {'number': 5, 'created': '2014-09-19 17:07:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/629014fa6174969461661f9b4cdbb2675ebac912', 'message': 'Fix unset extra_spec for a flavor\n\nA flavor extra_specs key cannot be deleted using \'nova flavor-key\'.\nUpdates to a flavor\'s extra_specs are not being registered by\nNovaObject, such that when NovaObject.obj_what_changed is called,\nno changes in the extra_specs field are picked up. In order for\nchanges in extra_specs to be picked up, the updated extra_specs\nhas to be compared against the _orig_extra_specs. The same approach\nalso applies to the flavor\'s projects, where the updated projects\nhas to be compared against the _orig_projects.\n\nAlso, if you delete the last remaining extra_specs key, nova would\nthrow an sqlalchemy warning, i.e. \'SAWarning: The IN-predicate on\n""instance_type_extra_specs.key"" was invoked with an empty sequence\'.\nThis was caused by calling db.flavor_extra_specs_update_or_create\nwith an empty dict of keys to add or update. This patch fixes it\ncalling db.flavor_extra_specs_update_or_create only on a non-empty\ndict of keys.\n\nChange-Id: Icea492134239296f1570b2a4208eb0868315aa5f\nCloses-Bug: #1340885\n'}, {'number': 6, 'created': '2014-09-22 19:31:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1d7d08176f4a77ba3c801ae2e37dc0b4975ebeb8', 'message': 'Fix unset extra_spec for a flavor\n\nA flavor extra_specs key cannot be deleted using \'nova flavor-key\'.\nUpdates to a flavor\'s extra_specs are not being registered by\nNovaObject, such that when NovaObject.obj_what_changed is called,\nno changes in the extra_specs field are picked up. In order for\nchanges in extra_specs to be picked up, the updated extra_specs\nhas to be compared against the _orig_extra_specs. The same approach\nalso applies to the flavor\'s projects, where the updated projects\nhas to be compared against the _orig_projects.\n\nAlso, if you delete the last remaining extra_specs key, nova would\nthrow an sqlalchemy warning, i.e. \'SAWarning: The IN-predicate on\n""instance_type_extra_specs.key"" was invoked with an empty sequence\'.\nThis was caused by calling db.flavor_extra_specs_update_or_create\nwith an empty dict of keys to add or update. This patch fixes it\ncalling db.flavor_extra_specs_update_or_create only on a non-empty\ndict of keys.\n\nChange-Id: Icea492134239296f1570b2a4208eb0868315aa5f\nCloses-Bug: #1340885\n'}, {'number': 7, 'created': '2014-09-23 16:19:33.000000000', 'files': ['nova/tests/objects/test_flavor.py', 'nova/objects/flavor.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/ae744b4b60169f653d6bafba237593388266cb90', 'message': 'Fix unset extra_spec for a flavor\n\nA flavor extra_specs key cannot be deleted using \'nova flavor-key\'.\nUpdates to a flavor\'s extra_specs are not being registered by\nNovaObject, such that when NovaObject.obj_what_changed is called,\nno changes in the extra_specs field are picked up. In order for\nchanges in extra_specs to be picked up, the updated extra_specs\nhas to be compared against the _orig_extra_specs. The same approach\nalso applies to the flavor\'s projects, where the updated projects\nhas to be compared against the _orig_projects.\n\nAlso, if you delete the last remaining extra_specs key, nova would\nthrow an sqlalchemy warning, i.e. \'SAWarning: The IN-predicate on\n""instance_type_extra_specs.key"" was invoked with an empty sequence\'.\nThis was caused by calling db.flavor_extra_specs_update_or_create\nwith an empty dict of keys to add or update. This patch fixes it\ncalling db.flavor_extra_specs_update_or_create only on a non-empty\ndict of keys.\n\nChange-Id: Icea492134239296f1570b2a4208eb0868315aa5f\nCloses-Bug: #1340885\n'}]",17,106603,ae744b4b60169f653d6bafba237593388266cb90,144,26,7,8247,,,0,"Fix unset extra_spec for a flavor

A flavor extra_specs key cannot be deleted using 'nova flavor-key'.
Updates to a flavor's extra_specs are not being registered by
NovaObject, such that when NovaObject.obj_what_changed is called,
no changes in the extra_specs field are picked up. In order for
changes in extra_specs to be picked up, the updated extra_specs
has to be compared against the _orig_extra_specs. The same approach
also applies to the flavor's projects, where the updated projects
has to be compared against the _orig_projects.

Also, if you delete the last remaining extra_specs key, nova would
throw an sqlalchemy warning, i.e. 'SAWarning: The IN-predicate on
""instance_type_extra_specs.key"" was invoked with an empty sequence'.
This was caused by calling db.flavor_extra_specs_update_or_create
with an empty dict of keys to add or update. This patch fixes it
calling db.flavor_extra_specs_update_or_create only on a non-empty
dict of keys.

Change-Id: Icea492134239296f1570b2a4208eb0868315aa5f
Closes-Bug: #1340885
",git fetch https://review.opendev.org/openstack/nova refs/changes/03/106603/4 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/objects/test_flavor.py', 'nova/api/openstack/compute/contrib/flavorextraspecs.py', 'nova/objects/flavor.py']",3,5adc179e1fa68554cff03634e85e045af8e6a65c,bug/1340885," if to_add: db.flavor_extra_specs_update_or_create(context, self.flavorid, to_add) "," db.flavor_extra_specs_update_or_create(context, self.flavorid, to_add)",32,2
openstack%2Fhorizon~master~I0b31d8565228e37be06bb85c199ee31d3fe7f5cc,openstack/horizon,master,I0b31d8565228e37be06bb85c199ee31d3fe7f5cc,Fix policy check for subnet operation,MERGED,2014-09-22 12:24:18.000000000,2014-09-26 07:52:02.000000000,2014-09-26 07:52:01.000000000,"[{'_account_id': 3}, {'_account_id': 841}, {'_account_id': 1941}, {'_account_id': 5623}]","[{'number': 1, 'created': '2014-09-22 12:24:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/3f7ecfe40fada4846b5d7cf5c157be3e4e6466ca', 'message': 'Fix policy check for subnet operation\n\nNeutron policy has a policy to check a network owning a given\nsubnet, so we need a special handling for neutron subnet.\n\nChange-Id: I0b31d8565228e37be06bb85c199ee31d3fe7f5cc\nCloses-Bug: #1324634\n'}, {'number': 2, 'created': '2014-09-23 10:53:57.000000000', 'files': ['openstack_dashboard/dashboards/project/networks/subnets/tables.py', 'openstack_dashboard/dashboards/admin/networks/subnets/tables.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/a5c83d1604e938cd5d53aee566de5b3ab83e6ae4', 'message': 'Fix policy check for subnet operation\n\nNeutron policy has a policy to check a network owning a given\nsubnet, so we need a special handling for neutron subnet.\n\nChange-Id: I0b31d8565228e37be06bb85c199ee31d3fe7f5cc\nCloses-Bug: #1324634\n'}]",0,123106,a5c83d1604e938cd5d53aee566de5b3ab83e6ae4,20,4,2,841,,,0,"Fix policy check for subnet operation

Neutron policy has a policy to check a network owning a given
subnet, so we need a special handling for neutron subnet.

Change-Id: I0b31d8565228e37be06bb85c199ee31d3fe7f5cc
Closes-Bug: #1324634
",git fetch https://review.opendev.org/openstack/horizon refs/changes/06/123106/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/networks/subnets/tables.py', 'openstack_dashboard/dashboards/admin/networks/subnets/tables.py']",2,3f7ecfe40fada4846b5d7cf5c157be3e4e6466ca,bug/1324634,"from openstack_dashboard.dashboards.project.networks.subnets \ import tables as proj_tablesclass DeleteSubnet(proj_tables.SubnetPolicyTargetMixin, tables.DeleteAction):class CreateSubnet(proj_tables.SubnetPolicyTargetMixin, tables.LinkAction):class UpdateSubnet(proj_tables.SubnetPolicyTargetMixin, tables.LinkAction):","from openstack_dashboard import policyclass DeleteSubnet(policy.PolicyTargetMixin, tables.DeleteAction): policy_target_attrs = ((""network:project_id"", ""tenant_id""),)class CreateSubnet(policy.PolicyTargetMixin, tables.LinkAction): policy_target_attrs = ((""network:project_id"", ""tenant_id""),) def get_policy_target(self, request, datum=None): return super(CreateSubnet, self)\ .get_policy_target(request, self.table._get_network())class UpdateSubnet(policy.PolicyTargetMixin, tables.LinkAction): policy_target_attrs = ((""network:project_id"", ""tenant_id""),)",18,21
openstack%2Fglance~master~Ia567b9d56891940a8170d67d95c1cb318545094e,openstack/glance,master,Ia567b9d56891940a8170d67d95c1cb318545094e,Add documentation for a new storage file permissions option,MERGED,2014-09-23 22:47:08.000000000,2014-09-26 07:10:20.000000000,2014-09-26 07:10:20.000000000,"[{'_account_id': 3}, {'_account_id': 6549}, {'_account_id': 8127}, {'_account_id': 9236}]","[{'number': 1, 'created': '2014-09-23 22:47:08.000000000', 'files': ['doc/source/configuring.rst'], 'web_link': 'https://opendev.org/openstack/glance/commit/5929511f70f44f0323a0c1314ab16d42c056d604', 'message': 'Add documentation for a new storage file permissions option\n\nThis patch adds a detailed description for filesystem_store_file_perm option which\nwas added in https://review.openstack.org/#/c/119529.\n\nChange-Id: Ia567b9d56891940a8170d67d95c1cb318545094e\n'}]",0,123602,5929511f70f44f0323a0c1314ab16d42c056d604,10,4,1,8443,,,0,"Add documentation for a new storage file permissions option

This patch adds a detailed description for filesystem_store_file_perm option which
was added in https://review.openstack.org/#/c/119529.

Change-Id: Ia567b9d56891940a8170d67d95c1cb318545094e
",git fetch https://review.opendev.org/openstack/glance refs/changes/02/123602/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/configuring.rst'],1,5929511f70f44f0323a0c1314ab16d42c056d604,docs,"* ``filesystem_store_file_perm=PERM_MODE`` Optional. Default: ``0`` Can only be specified in configuration files. `This option is specific to the filesystem storage backend.` The required permission value, in octal representation, for the created image file. You can use this value to specify the user of the consuming service (such as Nova) as the only member of the group that owns the created files. To keep the default value, assign a permission value that is less than or equal to 0. Note that the file owner must maintain read permission; if this value removes that permission an error message will be logged and the BadStoreConfiguration exception will be raised. If the Glance service has insufficient privileges to change file access permissions, a file will still be saved, but a warning message will appear in the Glance log. ",,17,0
openstack%2Fcinder~stable%2Ficehouse~Iab48b46559f18a3d6b044abe4bc3e615e7fee075,openstack/cinder,stable/icehouse,Iab48b46559f18a3d6b044abe4bc3e615e7fee075,NetApp NFS: Do not reference dst_img_local before assignment,MERGED,2014-09-25 12:06:42.000000000,2014-09-26 07:10:12.000000000,2014-09-26 07:10:11.000000000,"[{'_account_id': 3}, {'_account_id': 5538}, {'_account_id': 7198}, {'_account_id': 9366}, {'_account_id': 9656}, {'_account_id': 12780}, {'_account_id': 13308}]","[{'number': 1, 'created': '2014-09-25 12:06:42.000000000', 'files': ['cinder/tests/test_netapp_nfs.py', 'cinder/volume/drivers/netapp/nfs.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/db5a25d70d3fe085521c9d548a192177562befc1', 'message': 'NetApp NFS: Do not reference dst_img_local before assignment\n\nThis patch moves the assignment of the dst_img_local variable to before the\ntry/finally block that references it so that the finally block will have this\nvariable no matter where in the try block an error is raised.\n\nChange-ID: Iab48b46559f18a3d6b044abe4bc3e615e7fee075\nCloses-Bug: 1309047\n(cherry-picked from commit 48a53d32ee243d80b034b38e292e9bfb4b66f881)\n'}]",0,124030,db5a25d70d3fe085521c9d548a192177562befc1,15,7,1,9366,,,0,"NetApp NFS: Do not reference dst_img_local before assignment

This patch moves the assignment of the dst_img_local variable to before the
try/finally block that references it so that the finally block will have this
variable no matter where in the try block an error is raised.

Change-ID: Iab48b46559f18a3d6b044abe4bc3e615e7fee075
Closes-Bug: 1309047
(cherry-picked from commit 48a53d32ee243d80b034b38e292e9bfb4b66f881)
",git fetch https://review.opendev.org/openstack/cinder refs/changes/30/124030/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/test_netapp_nfs.py', 'cinder/volume/drivers/netapp/nfs.py']",2,db5a25d70d3fe085521c9d548a192177562befc1,bug/1309047," dst_dir = self._get_mount_point_for_share(dst_share) dst_img_local = os.path.join(dst_dir, tmp_img_file)"," dst_dir = self._get_mount_point_for_share(dst_share) dst_img_local = os.path.join(dst_dir, tmp_img_file)",29,2
openstack%2Fcinder~stable%2Ficehouse~I6053708296f2b5e24513dc87ed63da0f67c220ae,openstack/cinder,stable/icehouse,I6053708296f2b5e24513dc87ed63da0f67c220ae,volume_image_metadata missing from volume list,MERGED,2014-07-11 03:00:31.000000000,2014-09-26 07:01:29.000000000,2014-09-26 07:01:29.000000000,"[{'_account_id': 3}, {'_account_id': 1207}, {'_account_id': 2243}, {'_account_id': 4159}, {'_account_id': 5538}, {'_account_id': 7156}, {'_account_id': 7160}, {'_account_id': 7198}, {'_account_id': 8871}, {'_account_id': 8874}, {'_account_id': 9533}, {'_account_id': 9656}, {'_account_id': 9751}, {'_account_id': 11811}, {'_account_id': 12033}]","[{'number': 1, 'created': '2014-07-11 03:00:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/5e4fcb2f3af6fd358041d21d37b0ca7b4b0bafbd', 'message': 'volume_image_metadata missing from volume list\n\nThe volume_image_metadata field was missing from the volume list\nbecause we tried to filter VolumeGlanceMetadata against a non-existent\nproject_id field. Filtering should be done on the Volume associated\nto the VolumeGlanceMetadata instead.\n\nMove project_id filtering out of model_query as it assumes\nthe project_id field is in the queried model itself.\nBuild the filter ourselves against Volume.\n\nChange-Id: I6053708296f2b5e24513dc87ed63da0f67c220ae\nCloses-bug: #1337526\n(cherry picked from commit 06dd7f28d1d3ec8619df0d25ddfe977d538897b3)\n'}, {'number': 2, 'created': '2014-08-07 14:39:52.000000000', 'files': ['cinder/db/sqlalchemy/api.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/ce35e7a2558d5b62617616c3ec930c79213186d6', 'message': 'volume_image_metadata missing from volume list\n\nThe volume_image_metadata field was missing from the volume list\nbecause we tried to filter VolumeGlanceMetadata against a non-existent\nproject_id field. Filtering should be done on the Volume associated\nto the VolumeGlanceMetadata instead.\n\nMove project_id filtering out of model_query as it assumes\nthe project_id field is in the queried model itself.\nBuild the filter ourselves against Volume.\n\nFix glance metadata SQL query performance\n\nThe query built to retrieve glance metadata associated to a volume\nwas sub-optimal: all rows of volume_glance_metadata were returned.\n\nFix it by properly joining the 2 tables with volume_id field.\n\nCloses-bug: #1337526\nCloses-bug: #1349936\nChange-Id: I6053708296f2b5e24513dc87ed63da0f67c220ae\n(cherry picked from commit 06dd7f28d1d3ec8619df0d25ddfe977d538897b3)\n(cherry picked from commit 8f8a8a6ffe14cf7bb461dc26d178bf8acc8ddf12)\n'}]",2,106238,ce35e7a2558d5b62617616c3ec930c79213186d6,37,15,2,7156,,,0,"volume_image_metadata missing from volume list

The volume_image_metadata field was missing from the volume list
because we tried to filter VolumeGlanceMetadata against a non-existent
project_id field. Filtering should be done on the Volume associated
to the VolumeGlanceMetadata instead.

Move project_id filtering out of model_query as it assumes
the project_id field is in the queried model itself.
Build the filter ourselves against Volume.

Fix glance metadata SQL query performance

The query built to retrieve glance metadata associated to a volume
was sub-optimal: all rows of volume_glance_metadata were returned.

Fix it by properly joining the 2 tables with volume_id field.

Closes-bug: #1337526
Closes-bug: #1349936
Change-Id: I6053708296f2b5e24513dc87ed63da0f67c220ae
(cherry picked from commit 06dd7f28d1d3ec8619df0d25ddfe977d538897b3)
(cherry picked from commit 8f8a8a6ffe14cf7bb461dc26d178bf8acc8ddf12)
",git fetch https://review.opendev.org/openstack/cinder refs/changes/38/106238/2 && git format-patch -1 --stdout FETCH_HEAD,['cinder/db/sqlalchemy/api.py'],1,5e4fcb2f3af6fd358041d21d37b0ca7b4b0bafbd,bug/1337526," query = model_query(context, models.VolumeGlanceMetadata, session=session) if is_user_context(context): query = query.filter(models.Volume.project_id == context.project_id) return query.all()"," rows = model_query(context, models.VolumeGlanceMetadata, project_only=True, session=session).\ filter_by(deleted=False).\ all() return rows",6,8
openstack%2Fkeystone~master~I202b5c87a221d8dba99d16b0a1baa7546fef093b,openstack/keystone,master,I202b5c87a221d8dba99d16b0a1baa7546fef093b,Adding an index on token.user_id and token.trust_id,MERGED,2014-06-23 22:31:07.000000000,2014-09-26 06:53:33.000000000,2014-09-26 06:53:32.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 220}, {'_account_id': 1032}, {'_account_id': 1916}, {'_account_id': 2218}, {'_account_id': 2903}, {'_account_id': 5046}, {'_account_id': 5707}, {'_account_id': 6482}, {'_account_id': 7725}, {'_account_id': 8871}, {'_account_id': 8978}, {'_account_id': 9453}, {'_account_id': 9751}, {'_account_id': 11333}, {'_account_id': 11387}, {'_account_id': 11717}]","[{'number': 1, 'created': '2014-06-23 22:31:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/73b5ef9adf79fcce2ec3209ba4c462cfe1404405', 'message': 'Adding an index on token.user_id\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}, {'number': 2, 'created': '2014-06-24 02:49:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/3b8bc611830377ffb65fd3aa3a6ac151a08ea619', 'message': 'Adding an index on token.user_id\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}, {'number': 3, 'created': '2014-06-24 03:44:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/b630d3ea97f23d73fef14c235db2405a8838532f', 'message': 'Adding an index on token.user_id\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}, {'number': 4, 'created': '2014-06-24 14:52:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/4bf6d20de32b405c46a6ce54e32140f6679349b7', 'message': 'Adding an index on token.user_id\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}, {'number': 5, 'created': '2014-06-24 20:48:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/86ae9dff6f025a2c96fec9ab8aac5fbfcaa9aef1', 'message': 'Adding an index on token.user_id\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}, {'number': 6, 'created': '2014-06-25 02:26:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/1f1d6a08822ac7e9b581e289a7e6b0af5adc9273', 'message': 'Adding an index on token.user_id\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}, {'number': 7, 'created': '2014-07-03 18:32:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/29b12e0d2538b91430fd155db92aa53f6324b9ed', 'message': 'Adding an index on token.user_id\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}, {'number': 8, 'created': '2014-07-17 12:57:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/5df7a73675dd0f4c9d3e0bc54c4bb2facd773261', 'message': 'Adding an index on token.user_id\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}, {'number': 9, 'created': '2014-07-17 15:16:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/cdecc1ecd41d825d853f1030b5ba0dad95366822', 'message': 'Adding an index on token.user_id\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}, {'number': 10, 'created': '2014-09-25 04:05:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/d42e840e830ef4bd13648fd9233442460bb37647', 'message': 'Adding an index on token.user_id and token.trust_id\n\nThis is a performance update to ensure that we are scanning the fewest\nnumber of rows on a user delete (causing token revocations). Without\nthese indexes it is possible to scan all valid tokens, causing\nsignificant overhead, to find the user or trust matching tokens.\n\nDue to selecting the extra column (needed for other matches in\nsome cases) this can also cause issues with buffer pool sizes.\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}, {'number': 11, 'created': '2014-09-25 04:19:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/526ef0f441121531e3fc5aadd30feb9c5d68679b', 'message': 'Adding an index on token.user_id and token.trust_id\n\nThis is a performance update to ensure that we are scanning the fewest\nnumber of rows on a user delete (causing token revocations). Without\nthese indexes it is possible to scan all valid tokens, causing\nsignificant overhead, to find the user or trust matching tokens.\n\nDue to selecting the extra column (needed for other matches in\nsome cases) this can also cause issues with buffer pool sizes.\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}, {'number': 12, 'created': '2014-09-25 04:28:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/583ef9045fa600eb51af104a90fd1a86719488ce', 'message': 'Adding an index on token.user_id and token.trust_id\n\nThis is a performance update to ensure that we are scanning the fewest\nnumber of rows on a user delete (causing token revocations). Without\nthese indexes it is possible to scan all valid tokens, causing\nsignificant overhead, to find the user or trust matching tokens.\n\nDue to selecting the extra column (needed for other matches in\nsome cases) this can also cause issues with buffer pool sizes.\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}, {'number': 13, 'created': '2014-09-25 06:19:42.000000000', 'files': ['keystone/common/sql/migrate_repo/versions/055_add_indexes_to_token_table.py', 'keystone/token/persistence/backends/sql.py', 'keystone/tests/test_sql_upgrade.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/4a462a47fc95635ac542aa4259e7b3c461e72408', 'message': 'Adding an index on token.user_id and token.trust_id\n\nThis is a performance update to ensure that we are scanning the fewest\nnumber of rows on a user delete (causing token revocations). Without\nthese indexes it is possible to scan all valid tokens, causing\nsignificant overhead, to find the user or trust matching tokens.\n\nDue to selecting the extra column (needed for other matches in\nsome cases) this can also cause issues with buffer pool sizes.\n\nChange-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b\nCloses-Bug: 1332666\n'}]",3,102041,4a462a47fc95635ac542aa4259e7b3c461e72408,90,18,13,1032,,,0,"Adding an index on token.user_id and token.trust_id

This is a performance update to ensure that we are scanning the fewest
number of rows on a user delete (causing token revocations). Without
these indexes it is possible to scan all valid tokens, causing
significant overhead, to find the user or trust matching tokens.

Due to selecting the extra column (needed for other matches in
some cases) this can also cause issues with buffer pool sizes.

Change-Id: I202b5c87a221d8dba99d16b0a1baa7546fef093b
Closes-Bug: 1332666
",git fetch https://review.opendev.org/openstack/keystone refs/changes/41/102041/9 && git format-patch -1 --stdout FETCH_HEAD,['keystone/common/sql/migrate_repo/versions/045_add_index_to_user_id.py'],1,73b5ef9adf79fcce2ec3209ba4c462cfe1404405,bug/1332666,"""""""Adds an index on the `user_id` column for the `token` table."""""" import sqlalchemy as sql def upgrade(migrate_engine): meta = sql.MetaData() meta.bind = migrate_engine token = sql.Table('token', meta, autoload=True) sql.Index('ix_token_user_id', token.c.user_id).create() def downgrade(migrate_engine): meta = sql.MetaData() meta.bind = migrate_engine token = sql.Table('token', meta, autoload=True) sql.Index('ix_token_user_id', token.c.user_id).drop()",def upgrade(migrate_engine): pass def downgrade(migration_engine): pass,18,3
openstack%2Fironic~master~I9319c37d8af11a2163528c60e3a2bb0a84db4f21,openstack/ironic,master,I9319c37d8af11a2163528c60e3a2bb0a84db4f21,Remove untranslated PO files,MERGED,2014-09-25 19:19:09.000000000,2014-09-26 06:32:58.000000000,2014-09-26 06:32:57.000000000,"[{'_account_id': 3}, {'_account_id': 2889}, {'_account_id': 5805}, {'_account_id': 7882}, {'_account_id': 8106}, {'_account_id': 12081}]","[{'number': 1, 'created': '2014-09-25 19:19:09.000000000', 'files': ['ironic/locale/en_AU/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/de/LC_MESSAGES/ironic-log-warning.po', 'ironic/locale/en_GB/LC_MESSAGES/ironic-log-info.po', 'ironic/locale/fr/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/fr/LC_MESSAGES/ironic-log-critical.po', 'ironic/locale/fr/LC_MESSAGES/ironic-log-warning.po', 'ironic/locale/de/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/pt_BR/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/en_GB/LC_MESSAGES/ironic-log-warning.po', 'ironic/locale/te_IN/LC_MESSAGES/ironic-log-critical.po', 'ironic/locale/en_GB/LC_MESSAGES/ironic-log-critical.po', 'ironic/locale/it/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/en_GB/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/zh_CN/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/en_AU/LC_MESSAGES/ironic.po', 'ironic/locale/en_US/LC_MESSAGES/ironic.po', 'ironic/locale/es/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/vi_VN/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/zh_TW/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/ja/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/ko_KR/LC_MESSAGES/ironic-log-error.po'], 'web_link': 'https://opendev.org/openstack/ironic/commit/b4df218fcd5a6fed3c36f67ed485efe19c4a74f6', 'message': 'Remove untranslated PO files\n\nThe proposal bot only imports file that are at least 75 % translated.\nRemove any PO file that is untranslated or has less than 75 % of\ntranslations.\nThis actually removed *all* files.\n\nChecked number of translated entries with:\ncd ironic/locale\nfor i in `find . -name *.po `; do echo $i;msgfmt --statistics $i; done\n\nSee also https://www.transifex.com/projects/p/ironic/ for current\ntranslation state.\n\nWhenever a file is again translated sufficiently, the proposal bot will\npropose a new patch - thus we do not lose any translations.\n\nChange-Id: I9319c37d8af11a2163528c60e3a2bb0a84db4f21\n'}]",0,124141,b4df218fcd5a6fed3c36f67ed485efe19c4a74f6,18,6,1,6547,,,0,"Remove untranslated PO files

The proposal bot only imports file that are at least 75 % translated.
Remove any PO file that is untranslated or has less than 75 % of
translations.
This actually removed *all* files.

Checked number of translated entries with:
cd ironic/locale
for i in `find . -name *.po `; do echo $i;msgfmt --statistics $i; done

See also https://www.transifex.com/projects/p/ironic/ for current
translation state.

Whenever a file is again translated sufficiently, the proposal bot will
propose a new patch - thus we do not lose any translations.

Change-Id: I9319c37d8af11a2163528c60e3a2bb0a84db4f21
",git fetch https://review.opendev.org/openstack/ironic refs/changes/41/124141/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/locale/en_AU/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/de/LC_MESSAGES/ironic-log-warning.po', 'ironic/locale/en_GB/LC_MESSAGES/ironic-log-info.po', 'ironic/locale/fr/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/fr/LC_MESSAGES/ironic-log-critical.po', 'ironic/locale/fr/LC_MESSAGES/ironic-log-warning.po', 'ironic/locale/de/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/pt_BR/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/en_GB/LC_MESSAGES/ironic-log-warning.po', 'ironic/locale/te_IN/LC_MESSAGES/ironic-log-critical.po', 'ironic/locale/en_GB/LC_MESSAGES/ironic-log-critical.po', 'ironic/locale/it/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/en_GB/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/zh_CN/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/en_AU/LC_MESSAGES/ironic.po', 'ironic/locale/en_US/LC_MESSAGES/ironic.po', 'ironic/locale/es/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/vi_VN/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/zh_TW/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/ja/LC_MESSAGES/ironic-log-error.po', 'ironic/locale/ko_KR/LC_MESSAGES/ironic-log-error.po']",21,b4df218fcd5a6fed3c36f67ed485efe19c4a74f6,rm-unused-translations,,"# Translations template for ironic. # Copyright (C) 2014 ORGANIZATION # This file is distributed under the same license as the ironic project. # # Translators: msgid """" msgstr """" ""Project-Id-Version: Ironic\n"" ""Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"" ""POT-Creation-Date: 2014-09-22 06:09+0000\n"" ""PO-Revision-Date: 2014-07-21 23:13+0000\n"" ""Last-Translator: openstackjenkins <jenkins@openstack.org>\n"" ""Language-Team: Korean (Korea) (http://www.transifex.com/projects/p/ironic/"" ""language/ko_KR/)\n"" ""Language: ko_KR\n"" ""MIME-Version: 1.0\n"" ""Content-Type: text/plain; charset=UTF-8\n"" ""Content-Transfer-Encoding: 8bit\n"" ""Generated-By: Babel 1.3\n"" ""Plural-Forms: nplurals=1; plural=0;\n"" #: ironic/api/middleware/parsable_error.py:82 #, python-format msgid ""Error parsing HTTP response: %s"" msgstr """" #: ironic/common/exception.py:79 msgid ""Exception in string format operation"" msgstr """" #: ironic/common/images.py:132 #, python-format msgid ""vfat image creation failed. Error: %s"" msgstr """" #: ironic/common/images.py:200 msgid ""Creating the filesystem root failed."" msgstr """" #: ironic/common/images.py:214 msgid ""Creating ISO image failed."" msgstr """" #: ironic/common/service.py:88 #, python-format msgid ""Service error occurred when stopping the RPC server. Error: %s"" msgstr """" #: ironic/common/service.py:93 #, python-format msgid ""Service error occurred when cleaning up the RPC manager. Error: %s"" msgstr """" #: ironic/common/utils.py:415 #, python-format msgid ""Could not remove tmpdir: %s"" msgstr """" #: ironic/common/utils.py:448 #, python-format msgid ""Failed to make file system. File system %s is not supported."" msgstr """" #: ironic/common/utils.py:452 #, python-format msgid ""Failed to create a file system in %(path)s. Error: %(error)s"" msgstr """" #: ironic/common/glance_service/base_image_service.py:128 #, python-format msgid """" ""Error contacting glance server '%(host)s:%(port)s' for '%(method)s', "" ""%(extra)s."" msgstr """" #: ironic/conductor/manager.py:705 #, python-format msgid ""Failed to change power state of node %(node)s to '%(state)s'."" msgstr """" #: ironic/dhcp/neutron.py:114 #, python-format msgid ""Failed to update Neutron port %s."" msgstr """" #: ironic/dhcp/neutron.py:128 #, python-format msgid ""Failed to update MAC address on Neutron port %s."" msgstr """" #: ironic/dhcp/neutron.py:190 #, python-format msgid ""Failed to Get IP address on Neutron port %s."" msgstr """" #: ironic/dhcp/neutron.py:206 #, python-format msgid ""Neutron returned invalid IPv4 address %s."" msgstr """" #: ironic/dhcp/neutron.py:210 #, python-format msgid ""No IP address assigned to Neutron port %s."" msgstr """" #: ironic/drivers/modules/agent.py:380 #, python-format msgid ""vendor_passthru failed with method %s"" msgstr """" #: ironic/drivers/modules/agent.py:417 #, python-format msgid ""Async exception for %(node)s: %(msg)s"" msgstr """" #: ironic/drivers/modules/agent.py:584 #, python-format msgid ""Could not find matching node for the provided MACs %s."" msgstr """" #: ironic/drivers/modules/deploy_utils.py:236 #, python-format msgid """" ""Failed to erase beginning of disk for node %(node)s. Command: %(command)s. "" ""Error: %(error)s."" msgstr """" #: ironic/drivers/modules/deploy_utils.py:248 #, python-format msgid """" ""Failed to get disk block count for node %(node)s. Command: %(command)s. "" ""Error: %(error)s."" msgstr """" #: ironic/drivers/modules/deploy_utils.py:261 #, python-format msgid """" ""Failed to erase the end of the disk on node %(node)s. Command: %(command)s. "" ""Error: %(error)s."" msgstr """" #: ironic/drivers/modules/deploy_utils.py:326 msgid ""Failed to detect root device UUID."" msgstr """" #: ironic/drivers/modules/deploy_utils.py:364 #: ironic/drivers/modules/deploy_utils.py:370 #, python-format msgid ""Deploy to address %s failed."" msgstr """" #: ironic/drivers/modules/deploy_utils.py:365 #, python-format msgid ""Command: %s"" msgstr """" #: ironic/drivers/modules/deploy_utils.py:366 #, python-format msgid ""StdOut: %r"" msgstr """" #: ironic/drivers/modules/deploy_utils.py:367 #, python-format msgid ""StdErr: %r"" msgstr """" #: ironic/drivers/modules/ipminative.py:263 #, python-format msgid """" ""IPMI get sensor data failed for node %(node_id)s with the following error: "" ""%(error)s"" msgstr """" #: ironic/drivers/modules/ipminative.py:414 #, python-format msgid """" ""IPMI set boot device failed for node %(node_id)s with the following error: "" ""%(error)s"" msgstr """" #: ironic/drivers/modules/ipminative.py:449 #, python-format msgid """" ""IPMI get boot device failed for node %(node_id)s with the following error: "" ""%(error)s"" msgstr """" #: ironic/drivers/modules/ipmitool.py:413 #, python-format msgid """" ""IPMI power %(state)s timed out after %(tries)s retries on node %(node_id)s."" msgstr """" #: ironic/drivers/modules/ipmitool.py:818 #, python-format msgid ""IPMI \""raw bytes\"" failed for node %(node_id)s with error: %(error)s."" msgstr """" #: ironic/drivers/modules/ipmitool.py:852 #, python-format msgid ""IPMI \""bmc reset\"" failed for node %(node_id)s with error: %(error)s."" msgstr """" #: ironic/drivers/modules/iscsi_deploy.py:293 #, python-format msgid ""Error returned from deploy ramdisk: %s"" msgstr """" #: ironic/drivers/modules/iscsi_deploy.py:306 #: ironic/drivers/modules/pxe.py:485 ironic/drivers/modules/ilo/deploy.py:533 #, python-format msgid ""Deploy failed for instance %(instance)s. Error: %(error)s"" msgstr """" #: ironic/drivers/modules/pxe.py:289 msgid ""UEFI boot mode is not supported with iPXE boot enabled."" msgstr """" #: ironic/drivers/modules/pxe.py:462 ironic/drivers/modules/ilo/deploy.py:502 #, python-format msgid ""Node %s is not waiting to be deployed."" msgstr """" #: ironic/drivers/modules/seamicro.py:167 #, python-format msgid ""SeaMicro client exception %(msg)s for node %(uuid)s"" msgstr """" #: ironic/drivers/modules/seamicro.py:451 #: ironic/drivers/modules/seamicro.py:487 #, python-format msgid ""SeaMicro client exception: %s"" msgstr """" #: ironic/drivers/modules/seamicro.py:550 #, python-format msgid """" ""Seamicro set boot device failed for node %(node)s with the following error: "" ""%(error)s"" msgstr """" #: ironic/drivers/modules/ssh.py:368 #, python-format msgid ""Node \""%(host)s\"" with MAC address %(mac)s not found."" msgstr """" #: ironic/drivers/modules/ssh.py:652 #, python-format msgid """" ""Failed to set boot device for node %(node)s, virt_type %(vtype)s does not "" ""support this operation"" msgstr """" #: ironic/drivers/modules/drac/management.py:77 #, python-format msgid """" ""DRAC driver failed to get next boot mode for node %(node_uuid)s. Reason: "" ""%(error)s."" msgstr """" #: ironic/drivers/modules/drac/management.py:150 #, python-format msgid """" ""DRAC driver failed to list the configuration jobs for node %(node_uuid)s. "" ""Reason: %(error)s."" msgstr """" #: ironic/drivers/modules/drac/management.py:234 #, python-format msgid """" ""DRAC driver failed to set the boot device for node %(node_uuid)s. Can't find "" ""the ID for the %(device)s type. Reason: %(error)s."" msgstr """" #: ironic/drivers/modules/drac/management.py:301 #, python-format msgid """" ""DRAC driver failed to get the current boot device for node %(node_uuid)s. "" ""Reason: %(error)s."" msgstr """" #: ironic/drivers/modules/drac/power.py:60 #, python-format msgid """" ""DRAC driver failed to get power state for node %(node_uuid)s. Reason: "" ""%(error)s."" msgstr """" #: ironic/drivers/modules/drac/power.py:89 #: ironic/drivers/modules/drac/power.py:101 #, python-format msgid """" ""DRAC driver failed to set power state for node %(node_uuid)s to "" ""%(target_power_state)s. Reason: %(error)s."" msgstr """" #: ironic/drivers/modules/ilo/common.py:401 #, python-format msgid """" ""Error while deleting %(object_name)s from %(container)s. Error: %(error)s"" msgstr """" #: ironic/drivers/modules/ilo/common.py:411 #, python-format msgid """" ""Error while ejecting virtual media %(device)s from node %(uuid)s. Error: "" ""%(error)s"" msgstr """" #: ironic/drivers/modules/ilo/deploy.py:100 #, python-format msgid """" ""Unable to find boot_iso in Glance, required to deploy node %(node)s in UEFI "" ""boot mode."" msgstr """" #: ironic/drivers/modules/ilo/deploy.py:110 #, python-format msgid """" ""Unable to find 'kernel_id' and 'ramdisk_id' in Glance image %(image)s for "" ""generating boot ISO for %(node)s"" msgstr """" #: ironic/drivers/modules/ilo/deploy.py:151 #, python-format msgid ""Failed to clean up boot ISO for %(node)s.Error: %(error)s."" msgstr """" #: ironic/drivers/modules/ilo/deploy.py:515 #, python-format msgid ""Cannot get boot ISO for node %s"" msgstr """" #: ironic/drivers/modules/ilo/power.py:83 #, python-format msgid ""iLO get_power_state failed for node %(node_id)s with error: %(error)s."" msgstr """" #: ironic/drivers/modules/ilo/power.py:155 #, python-format msgid """" ""iLO set_power_state failed to set state to %(tstate)s for node %(node_id)s "" ""with error: %(error)s"" msgstr """" #: ironic/drivers/modules/ilo/power.py:168 #, python-format msgid ""iLO failed to change state to %(tstate)s within %(timeout)s sec"" msgstr """" #: ironic/objects/base.py:71 #, python-format msgid ""Error setting %(attr)s"" msgstr """" #: ironic/objects/base.py:222 #, python-format msgid ""Unable to instantiate unregistered object type %(objtype)s"" msgstr """" #: ironic/openstack/common/excutils.py:76 #, python-format msgid ""Original exception being dropped: %s"" msgstr ""  : %s"" #: ironic/openstack/common/excutils.py:105 #, python-format msgid ""Unexpected exception occurred %d time(s)... retrying."" msgstr ""   %d  ...  ."" #: ironic/openstack/common/lockutils.py:119 #, python-format msgid ""Could not release the acquired lock `%s`"" msgstr """" #: ironic/openstack/common/loopingcall.py:95 msgid ""in fixed duration looping call"" msgstr ""   "" #: ironic/openstack/common/loopingcall.py:138 msgid ""in dynamic looping call"" msgstr ""  "" #: ironic/openstack/common/periodic_task.py:202 #, python-format msgid ""Error during %(full_task_name)s: %(e)s"" msgstr """" #: ironic/openstack/common/service.py:188 msgid ""Exception during rpc cleanup."" msgstr """" #: ironic/openstack/common/service.py:277 msgid ""Unhandled exception"" msgstr """" #: ironic/tests/db/sqlalchemy/test_migrations.py:172 #, python-format msgid ""Failed to migrate to version %(version)s on engine %(engine)s"" msgstr """" ",0,10110
openstack%2Fmistral~master~I1867fb93f540c8f061fd09c40a283ce1ef2c072c,openstack/mistral,master,I1867fb93f540c8f061fd09c40a283ce1ef2c072c,Add script to run functional tests locally,MERGED,2014-09-23 15:06:09.000000000,2014-09-26 06:26:31.000000000,2014-09-26 06:26:30.000000000,"[{'_account_id': 3}, {'_account_id': 7700}, {'_account_id': 8731}, {'_account_id': 9432}]","[{'number': 1, 'created': '2014-09-23 15:06:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/d962fab35e084bd1537fd479ebfdd10575a710ef', 'message': ""Add script to run functional tests locally\n\n- Added script that can run part of functional tests locally\n- Added needed mocks in base.py file to don't interact with keystone\n- Added small description how to run new script\n\nChange-Id: I1867fb93f540c8f061fd09c40a283ce1ef2c072c\n""}, {'number': 2, 'created': '2014-09-23 15:09:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/72961ba5f2d1294ea2bf85027caeabf7cfb9f4bd', 'message': ""Add script to run functional tests locally\n\n- Added script that can run part of functional tests locally\n- Added needed mocks in base.py file to don't interact with keystone\n- Added small description how to run new script\n\nChange-Id: I1867fb93f540c8f061fd09c40a283ce1ef2c072c\n""}, {'number': 3, 'created': '2014-09-24 05:41:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/2af7e13e5584af4916b731db2bf260c499c9ea99', 'message': ""Add script to run functional tests locally\n\n- Added script that can run part of functional tests locally\n- Added needed mocks in base.py file to don't interact with keystone\n- Added small description how to run new script\n\nChange-Id: I1867fb93f540c8f061fd09c40a283ce1ef2c072c\n""}, {'number': 4, 'created': '2014-09-25 10:11:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/6c665e75adcf626ed36771d9d5a109fb4bae369b', 'message': ""Add script to run functional tests locally\n\n- Added script that can run part of functional tests locally\n- Added needed mocks in base.py file to don't interact with keystone\n- Added small description how to run new script\n\nChange-Id: I1867fb93f540c8f061fd09c40a283ce1ef2c072c\n""}, {'number': 5, 'created': '2014-09-25 10:37:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/831242e265d364b9b609c2e65574f06c46a9c9da', 'message': ""Add script to run functional tests locally\n\n- Added script that can run part of functional tests locally\n- Added needed mocks in base.py file to don't interact with keystone\n- Added small description how to run new script\n\nChange-Id: I1867fb93f540c8f061fd09c40a283ce1ef2c072c\n""}, {'number': 6, 'created': '2014-09-25 22:53:43.000000000', 'files': ['requirements.txt', 'test-requirements.txt', 'README.rst', 'mistral/tests/functional/base.py', 'run_functional_tests.sh'], 'web_link': 'https://opendev.org/openstack/mistral/commit/aacdb47af3dd18237c0ba8b73eb8ef0a2fb86b01', 'message': ""Add script to run functional tests locally\n\n- Added script that can run part of functional tests locally\n- Added needed mocks in base.py file so that we don't interact with keystone\n- Added small a description how to run new script\n\nChange-Id: I1867fb93f540c8f061fd09c40a283ce1ef2c072c\n""}]",5,123465,aacdb47af3dd18237c0ba8b73eb8ef0a2fb86b01,22,4,6,8592,,,0,"Add script to run functional tests locally

- Added script that can run part of functional tests locally
- Added needed mocks in base.py file so that we don't interact with keystone
- Added small a description how to run new script

Change-Id: I1867fb93f540c8f061fd09c40a283ce1ef2c072c
",git fetch https://review.opendev.org/openstack/mistral refs/changes/65/123465/6 && git format-patch -1 --stdout FETCH_HEAD,"['README.rst', 'mistral/tests/functional/base.py', 'run_functional_tests.sh']",3,d962fab35e084bd1537fd479ebfdd10575a710ef,(detached,"#! /usr/bin/env bash ARG=$1 WAS_TEMPEST=""False"" function pre_hook() { export WITHOUT_AUTH=""True"" echo ""Cloning tempest repository for successful import of all dependencies in the tests"" if ! [ -d /etc/tempest ] then git clone http://github.com/openstack/tempest.git mkdir /etc/tempest echo ""Coping tempest.conf file to the /etc/tempest/"" cp tempest/etc/tempest.conf.sample /etc/tempest/tempest.conf else WAS_TEMPEST=""True"" fi } function run_tests_by_version() { echo ""Running integration API and workflow execution tests for v$1"" export VERSION=""v$1"" nosetests -v mistral/tests/functional/api/v$1/ unset VERSION } function run_tests() { if [ -z ""$ARG"" ] then run_tests_by_version 1 run_tests_by_version 2 elif [ ""$ARG"" == ""v1"" ] then run_tests_by_version 1 elif [ ""$ARG"" == ""v2"" ] then run_tests_by_version 2 fi } function post_hook () { if [ ""WAS_TEMPEST"" == ""False"" ] then rm -rf /etc/tempest rm -rf tempest fi unset LOCAL_RUN } #----------main-part---------- echo ""Preparation for tests running..."" pre_hook echo ""Running tests..."" run_tests post_hook ",,92,2
openstack%2Fopenstack-manuals~master~I3c4edc17197156d85f0556e0a8910301ee80ab6d,openstack/openstack-manuals,master,I3c4edc17197156d85f0556e0a8910301ee80ab6d,Add hellip to openstack.ent,MERGED,2014-09-25 17:39:09.000000000,2014-09-26 06:14:04.000000000,2014-09-26 06:14:04.000000000,"[{'_account_id': 3}, {'_account_id': 964}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-09-25 17:39:09.000000000', 'files': ['doc/common/entities/openstack.ent'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/e76edb4009279d609c91639112492d8fe3419cba', 'message': 'Add hellip to openstack.ent\n\nAdd a comment about syncing the file.\nAdd hellip as entitity.\n\nChange-Id: I3c4edc17197156d85f0556e0a8910301ee80ab6d\n'}]",0,124119,e76edb4009279d609c91639112492d8fe3419cba,7,3,1,6547,,,0,"Add hellip to openstack.ent

Add a comment about syncing the file.
Add hellip as entitity.

Change-Id: I3c4edc17197156d85f0556e0a8910301ee80ab6d
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/19/124119/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/common/entities/openstack.ent'],1,e76edb4009279d609c91639112492d8fe3419cba,hellip,"<!-- The master of this file is in openstack-manuals repository, file doc/common/entities/openstack.ent. Any changes to the master file will override changs in other repositories. --><!ENTITY hellip ""&#133;"">",,5,1
openstack%2Fneutron~master~I1e9bdc3dffb8be705c0fcaf88bd914d27e10936d,openstack/neutron,master,I1e9bdc3dffb8be705c0fcaf88bd914d27e10936d,Update URL of Ryu official site in ofagent README files,MERGED,2014-09-05 06:04:25.000000000,2014-09-26 05:38:26.000000000,2014-09-18 08:14:57.000000000,"[{'_account_id': 3}, {'_account_id': 2592}, {'_account_id': 4149}, {'_account_id': 5170}, {'_account_id': 6635}, {'_account_id': 6659}, {'_account_id': 6854}, {'_account_id': 7249}, {'_account_id': 8124}, {'_account_id': 8645}, {'_account_id': 9681}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 11080}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-09-05 06:04:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/bf8bbfe1af12aa0251a534ab8c2904c2acf4707e', 'message': 'Update URL of Ryu official site\n\nChange-Id: I1e9bdc3dffb8be705c0fcaf88bd914d27e10936d\n'}, {'number': 2, 'created': '2014-09-17 20:55:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/fc0e2a28e3492fdb3f38e4e84a4388883bd17aac', 'message': 'Update URL of Ryu official site in ofagent README files\nChange-Id: I1e9bdc3dffb8be705c0fcaf88bd914d27e10936d\n'}, {'number': 3, 'created': '2014-09-17 20:55:11.000000000', 'files': ['neutron/plugins/ryu/README', 'neutron/plugins/ofagent/README'], 'web_link': 'https://opendev.org/openstack/neutron/commit/13812210d4bd1447019eadb6057fcfb8cf8481dc', 'message': 'Update URL of Ryu official site in ofagent README files\n\nChange-Id: I1e9bdc3dffb8be705c0fcaf88bd914d27e10936d\n'}]",5,119295,13812210d4bd1447019eadb6057fcfb8cf8481dc,79,26,3,6854,,,0,"Update URL of Ryu official site in ofagent README files

Change-Id: I1e9bdc3dffb8be705c0fcaf88bd914d27e10936d
",git fetch https://review.opendev.org/openstack/neutron refs/changes/95/119295/3 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ryu/README', 'neutron/plugins/ofagent/README']",2,bf8bbfe1af12aa0251a534ab8c2904c2acf4707e,ofagent-ryu-url,http://osrg.github.io/ryu/,http://www.osrg.net/ryu/,2,2
openstack%2Fnova~stable%2Ficehouse~I64c5a3f31c912cca6b5b9987152ba7a9b3f5987d,openstack/nova,stable/icehouse,I64c5a3f31c912cca6b5b9987152ba7a9b3f5987d,Made unassigned networks visible in flat networking,MERGED,2014-09-03 19:10:14.000000000,2014-09-26 04:54:58.000000000,2014-09-26 04:54:55.000000000,"[{'_account_id': 3}, {'_account_id': 67}, {'_account_id': 5170}, {'_account_id': 6873}, {'_account_id': 8328}, {'_account_id': 9656}]","[{'number': 1, 'created': '2014-09-03 19:10:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f8f77a051b5e9d3e2f23c6f762fee4c4352f1b8f', 'message': 'Made unassigned networks visible in flat networking\n\nThis change fixes a bug in Nova\'s\nGET /v2/{tenant_id}/os-networks\n\nThe doc\nhttp://docs.openstack.org/api/openstack-compute/2/content/GET_os-networks-v2_ListNetworks__v2__tenant_id__os-networks_ext-os-networks.html\nsays that ""Lists networks that are available to the tenant"".\n\nWhen invoked by a non-admin user it was returning only networks\nassigned to the user\'s tenant.  But in flat and flat DHCP nova\nnetworking, networks CAN NOT be assigned to tenants --- thus a\nnon-admin user would get zero networks from this operation.\n\nThe fix was to make this operation conditionally use an option already\npresent in the lower-level code to ""allow_none"" when fetching the list\nof networks, meaning to include networks whose project_id field in the\nDB held ""none"" (meaning the network is not assigned to a tenant).  The\ncondition under which this is done is that the Nova configuration\noption named network_manager contains the string\n""\'nova.network.manager.Flat"" --- which is true for flat and flat DHCP\nnova networking and false for VLAN nova networking.\n\nConflicts:\n        nova/network/api.py\n        nova/tests/network/test_api.py\n\nNOTE(mriedem): The conflicts are due to the objects conversion\nin Juno. This cherry pick adds the related network_get_all unit\ntests from Juno and makes them work with the DB API in Icehouse.\n\nChange-Id: I64c5a3f31c912cca6b5b9987152ba7a9b3f5987d\nCloses-Bug: #1327406\n(cherry picked from commit 46e88320e6e6231550f3e2b40312c51f55e059f5)\n'}, {'number': 2, 'created': '2014-09-03 19:18:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/04b2fc859ea4e2036196095bed7aad6cbaffb8d3', 'message': 'Made unassigned networks visible in flat networking\n\nThis change fixes a bug in Nova\'s\nGET /v2/{tenant_id}/os-networks\n\nThe doc\nhttp://docs.openstack.org/api/openstack-compute/2/content/GET_os-networks-v2_ListNetworks__v2__tenant_id__os-networks_ext-os-networks.html\nsays that ""Lists networks that are available to the tenant"".\n\nWhen invoked by a non-admin user it was returning only networks\nassigned to the user\'s tenant.  But in flat and flat DHCP nova\nnetworking, networks CAN NOT be assigned to tenants --- thus a\nnon-admin user would get zero networks from this operation.\n\nThe fix was to make this operation conditionally use an option already\npresent in the lower-level code to ""allow_none"" when fetching the list\nof networks, meaning to include networks whose project_id field in the\nDB held ""none"" (meaning the network is not assigned to a tenant).  The\ncondition under which this is done is that the Nova configuration\noption named network_manager contains the string\n""\'nova.network.manager.Flat"" --- which is true for flat and flat DHCP\nnova networking and false for VLAN nova networking.\n\nConflicts:\n        nova/network/api.py\n        nova/tests/network/test_api.py\n\nNOTE(mriedem): The conflicts are due to the objects conversion\nin Juno. This cherry pick adds the related network_get_all unit\ntests from Juno and makes them work with the DB API in Icehouse.\nAlso note the oslo.config import in test_api is removed since it\nwas erroneously merged into master (and needs to be cleand up).\n\nChange-Id: I64c5a3f31c912cca6b5b9987152ba7a9b3f5987d\nCloses-Bug: #1327406\n(cherry picked from commit 46e88320e6e6231550f3e2b40312c51f55e059f5)\n'}, {'number': 3, 'created': '2014-09-23 14:45:57.000000000', 'files': ['nova/network/api.py', 'nova/tests/network/test_api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/f93b8ee75ef457a2843ea6b551cd4c9dd27fbaa8', 'message': 'Made unassigned networks visible in flat networking\n\nThis change fixes a bug in Nova\'s\nGET /v2/{tenant_id}/os-networks\n\nThe doc\nhttp://docs.openstack.org/api/openstack-compute/2/content/GET_os-networks-v2_ListNetworks__v2__tenant_id__os-networks_ext-os-networks.html\nsays that ""Lists networks that are available to the tenant"".\n\nWhen invoked by a non-admin user it was returning only networks\nassigned to the user\'s tenant.  But in flat and flat DHCP nova\nnetworking, networks CAN NOT be assigned to tenants --- thus a\nnon-admin user would get zero networks from this operation.\n\nThe fix was to make this operation conditionally use an option already\npresent in the lower-level code to ""allow_none"" when fetching the list\nof networks, meaning to include networks whose project_id field in the\nDB held ""none"" (meaning the network is not assigned to a tenant).  The\ncondition under which this is done is that the Nova configuration\noption named network_manager contains the string\n""\'nova.network.manager.Flat"" --- which is true for flat and flat DHCP\nnova networking and false for VLAN nova networking.\n\nConflicts:\n        nova/network/api.py\n        nova/tests/network/test_api.py\n\nNOTE(mriedem): The conflicts are due to the objects conversion\nin Juno. This cherry pick adds the related network_get_all unit\ntests from Juno and makes them work with the DB API in Icehouse.\nAlso note the oslo.config import in test_api is removed since it\nwas erroneously merged into master and fixed with commit\n0e88148907e1db5218b96e2fa54bf9fee1cba74f but rather than backport\nthat and squash it with this, I\'ve just removed it.\n\nChange-Id: I64c5a3f31c912cca6b5b9987152ba7a9b3f5987d\nCloses-Bug: #1327406\n(cherry picked from commit 46e88320e6e6231550f3e2b40312c51f55e059f5)\n'}]",7,118748,f93b8ee75ef457a2843ea6b551cd4c9dd27fbaa8,20,6,3,6873,,,0,"Made unassigned networks visible in flat networking

This change fixes a bug in Nova's
GET /v2/{tenant_id}/os-networks

The doc
http://docs.openstack.org/api/openstack-compute/2/content/GET_os-networks-v2_ListNetworks__v2__tenant_id__os-networks_ext-os-networks.html
says that ""Lists networks that are available to the tenant"".

When invoked by a non-admin user it was returning only networks
assigned to the user's tenant.  But in flat and flat DHCP nova
networking, networks CAN NOT be assigned to tenants --- thus a
non-admin user would get zero networks from this operation.

The fix was to make this operation conditionally use an option already
present in the lower-level code to ""allow_none"" when fetching the list
of networks, meaning to include networks whose project_id field in the
DB held ""none"" (meaning the network is not assigned to a tenant).  The
condition under which this is done is that the Nova configuration
option named network_manager contains the string
""'nova.network.manager.Flat"" --- which is true for flat and flat DHCP
nova networking and false for VLAN nova networking.

Conflicts:
        nova/network/api.py
        nova/tests/network/test_api.py

NOTE(mriedem): The conflicts are due to the objects conversion
in Juno. This cherry pick adds the related network_get_all unit
tests from Juno and makes them work with the DB API in Icehouse.
Also note the oslo.config import in test_api is removed since it
was erroneously merged into master and fixed with commit
0e88148907e1db5218b96e2fa54bf9fee1cba74f but rather than backport
that and squash it with this, I've just removed it.

Change-Id: I64c5a3f31c912cca6b5b9987152ba7a9b3f5987d
Closes-Bug: #1327406
(cherry picked from commit 46e88320e6e6231550f3e2b40312c51f55e059f5)
",git fetch https://review.opendev.org/openstack/nova refs/changes/48/118748/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/network/api.py', 'nova/tests/network/test_api.py']",2,f8f77a051b5e9d3e2f23c6f762fee4c4352f1b8f,bug/1327406-icehouse,"from oslo.config import cfgCONF = cfg.CONF @mock.patch('nova.db.network_get_all') def test_get_all(self, mock_get_all): mock_get_all.return_value = mock.sentinel.get_all self.assertEqual(mock.sentinel.get_all, self.network_api.get_all(self.context)) mock_get_all.assert_called_once_with(self.context, project_only=True) @mock.patch('nova.db.network_get_all') def test_get_all_liberal(self, mock_get_all): self.flags(network_manager='nova.network.manager.FlatDHCPManaager') mock_get_all.return_value = mock.sentinel.get_all self.assertEqual(mock.sentinel.get_all, self.network_api.get_all(self.context)) mock_get_all.assert_called_once_with(self.context, project_only=""allow_none"") @mock.patch('nova.db.network_get_all') def test_get_all_no_networks(self, mock_get_all): mock_get_all.side_effect = exception.NoNetworksFound self.assertEqual([], self.network_api.get_all(self.context)) mock_get_all.assert_called_once_with(self.context, project_only=True) ",,40,3
openstack%2Fneutron~stable%2Ficehouse~I856d67ef5ff6264374cb8f2569668da4c205ad9f,openstack/neutron,stable/icehouse,I856d67ef5ff6264374cb8f2569668da4c205ad9f,NSX: Optionally not enforce nat rule match length check,MERGED,2014-07-21 23:54:51.000000000,2014-09-26 04:52:22.000000000,2014-09-26 04:52:20.000000000,"[{'_account_id': 3}, {'_account_id': 261}, {'_account_id': 1420}, {'_account_id': 4395}, {'_account_id': 5170}, {'_account_id': 7088}, {'_account_id': 8767}, {'_account_id': 9008}, {'_account_id': 9656}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9846}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 13308}]","[{'number': 1, 'created': '2014-07-21 23:54:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/4f4044a2c11f8c235ebaf39546d740af7f308f3c', 'message': ""NSX: Optionally not enforce nat rule match length check\n\nThis patch adds the 'raise_on_len_mismatch' parameter to the\n'delete_nat_rules_by_match' function. The plugin then leverages\nthis parameter for ensuring NAT rules deletion operations\nare completed successfully even when duplicate rules are found\nor no corresponding rules are found at all.\n\nWith this change, the 'remove_router_interface' operation will\ncorrectly complete even in cases when NAT rules in Neutron and\nthe NSX backend are out of sync.\n\nThis patch also changes a check in delete_nat_rules_by_match in\norder to make it less expensive.\n\nCloses-Bug: 1328181\n\nChange-Id: I856d67ef5ff6264374cb8f2569668da4c205ad9f\n(cherry picked from commit 138d9774000335d66c335e036ea96dac7b4a4799)\n""}, {'number': 2, 'created': '2014-09-24 20:36:10.000000000', 'files': ['neutron/plugins/vmware/plugins/base.py', 'neutron/tests/unit/vmware/test_nsx_plugin.py', 'neutron/plugins/vmware/nsxlib/router.py', 'neutron/tests/unit/vmware/nsxlib/test_router.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/3a8594624aa3bd1b6375f8450d0df3cbff782c82', 'message': ""NSX: Optionally not enforce nat rule match length check\n\nThis patch adds the 'raise_on_len_mismatch' parameter to the\n'delete_nat_rules_by_match' function. The plugin then leverages\nthis parameter for ensuring NAT rules deletion operations\nare completed successfully even when duplicate rules are found\nor no corresponding rules are found at all.\n\nWith this change, the 'remove_router_interface' operation will\ncorrectly complete even in cases when NAT rules in Neutron and\nthe NSX backend are out of sync.\n\nThis patch also changes a check in delete_nat_rules_by_match in\norder to make it less expensive.\n\nCloses-Bug: 1328181\n\nChange-Id: I856d67ef5ff6264374cb8f2569668da4c205ad9f\n(cherry picked from commit 138d9774000335d66c335e036ea96dac7b4a4799)\n""}]",0,108532,3a8594624aa3bd1b6375f8450d0df3cbff782c82,57,24,2,4395,,,0,"NSX: Optionally not enforce nat rule match length check

This patch adds the 'raise_on_len_mismatch' parameter to the
'delete_nat_rules_by_match' function. The plugin then leverages
this parameter for ensuring NAT rules deletion operations
are completed successfully even when duplicate rules are found
or no corresponding rules are found at all.

With this change, the 'remove_router_interface' operation will
correctly complete even in cases when NAT rules in Neutron and
the NSX backend are out of sync.

This patch also changes a check in delete_nat_rules_by_match in
order to make it less expensive.

Closes-Bug: 1328181

Change-Id: I856d67ef5ff6264374cb8f2569668da4c205ad9f
(cherry picked from commit 138d9774000335d66c335e036ea96dac7b4a4799)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/32/108532/2 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/vmware/plugins/base.py', 'neutron/tests/unit/vmware/test_nsx_plugin.py', 'neutron/plugins/vmware/nsxlib/router.py', 'neutron/tests/unit/vmware/nsxlib/test_router.py']",4,4f4044a2c11f8c235ebaf39546d740af7f308f3c,bug/1328181," def test_delete_nat_rules_by_match_len_mismatch_does_not_raise(self): lrouter = self._prepare_nat_rules_for_delete_tests() rules = routerlib.query_nat_rules(self.fake_cluster, lrouter['uuid']) self.assertEqual(len(rules), 3) deleted_rules = routerlib.delete_nat_rules_by_match( self.fake_cluster, lrouter['uuid'], 'DestinationNatRule', max_num_expected=1, min_num_expected=1, raise_on_len_mismatch=False, destination_ip_addresses='99.99.99.99') self.assertEqual(0, deleted_rules) # add an extra rule to emulate a duplicate one with mock.patch.object(self.fake_cluster.api_client, 'get_version', new=lambda: version_module.Version('2.0')): routerlib.create_lrouter_snat_rule( self.fake_cluster, lrouter['uuid'], '10.0.0.2', '10.0.0.2', order=220, match_criteria={'source_ip_addresses': '192.168.0.0/24'}) deleted_rules_2 = routerlib.delete_nat_rules_by_match( self.fake_cluster, lrouter['uuid'], 'SourceNatRule', min_num_expected=1, max_num_expected=1, raise_on_len_mismatch=False, source_ip_addresses='192.168.0.0/24') self.assertEqual(2, deleted_rules_2)",,81,8
openstack%2Ftrove~stable%2Ficehouse~I3b49b1d667f6ade9ae3f6765d735440a3e838917,openstack/trove,stable/icehouse,I3b49b1d667f6ade9ae3f6765d735440a3e838917,Sync latest process and str utils from oslo,MERGED,2014-09-14 18:56:31.000000000,2014-09-26 04:45:07.000000000,2014-09-26 04:45:06.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 1955}, {'_account_id': 5293}, {'_account_id': 8415}, {'_account_id': 9311}, {'_account_id': 9656}, {'_account_id': 10215}]","[{'number': 1, 'created': '2014-09-14 18:56:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/trove/commit/faa23cb312f6e3ee1a92270d53c09b2fd4744c18', 'message': 'Sync latest process and str utils from oslo\n\nThis sync required changes to fix these issues:\n* Make execute method clean password in exception\n* Make sure mask_password works properly\n\nThis is not trivial as these fixes relies on many other changes,\nonly the necessary code have been imported/adapted.\n\n------------------------------------------------\nThe sync pulls in the following changes (newest to oldest):\n\n63c99a0f - Mask passwords in exceptions and error messages\n66142c34 - Make strutils.mask_password more secure\nd6b55fb2 - Remove `processutils` dependency on `log`\ncb5a804b - Move `mask_password` to strutils\n\n-----------------------------------------------\n\nCloses-Bug: 1343604\nCloses-Bug: 1345233\nSecurityImpact\n\nChange-Id: I3b49b1d667f6ade9ae3f6765d735440a3e838917\n'}, {'number': 2, 'created': '2014-09-14 19:12:09.000000000', 'files': ['trove/openstack/common/processutils.py', 'trove/common/auth.py', 'trove/openstack/common/log.py', 'trove/openstack/common/strutils.py', 'trove/instance/service.py', 'trove/extensions/mysql/service.py'], 'web_link': 'https://opendev.org/openstack/trove/commit/a36c2a7732cc33623b95989d6c27812014a9a727', 'message': 'Sync latest process and str utils from oslo\n\nThis sync required changes to fix these issues:\n* Make execute method clean password in exception\n* Make sure mask_password works properly\n\nThis is not trivial as these fixes relies on many other changes,\nonly the necessary code have been imported/adapted.\n\n------------------------------------------------\nThe sync pulls in the following changes (newest to oldest):\n\n63c99a0f - Mask passwords in exceptions and error messages\n66142c34 - Make strutils.mask_password more secure\nd6b55fb2 - Remove `processutils` dependency on `log`\ncb5a804b - Move `mask_password` to strutils\n\n-----------------------------------------------\n\nCloses-Bug: 1343604\nCloses-Bug: 1345233\nSecurityImpact\n\nChange-Id: I3b49b1d667f6ade9ae3f6765d735440a3e838917\n'}]",0,121416,a36c2a7732cc33623b95989d6c27812014a9a727,17,8,2,9311,,,0,"Sync latest process and str utils from oslo

This sync required changes to fix these issues:
* Make execute method clean password in exception
* Make sure mask_password works properly

This is not trivial as these fixes relies on many other changes,
only the necessary code have been imported/adapted.

------------------------------------------------
The sync pulls in the following changes (newest to oldest):

63c99a0f - Mask passwords in exceptions and error messages
66142c34 - Make strutils.mask_password more secure
d6b55fb2 - Remove `processutils` dependency on `log`
cb5a804b - Move `mask_password` to strutils

-----------------------------------------------

Closes-Bug: 1343604
Closes-Bug: 1345233
SecurityImpact

Change-Id: I3b49b1d667f6ade9ae3f6765d735440a3e838917
",git fetch https://review.opendev.org/openstack/trove refs/changes/16/121416/2 && git format-patch -1 --stdout FETCH_HEAD,"['trove/openstack/common/processutils.py', 'trove/common/auth.py', 'trove/openstack/common/log.py', 'trove/openstack/common/strutils.py', 'trove/instance/service.py', 'trove/extensions/mysql/service.py']",6,faa23cb312f6e3ee1a92270d53c09b2fd4744c18,bug/1343604,"from trove.openstack.common import strutils LOG.info(strutils.mask_password(_(""req : '%s'\n\n"") % req)) LOG.info(strutils.mask_password(_(""body : '%s'\n\n"") % body)) LOG.info(strutils.mask_password(_(""req : '%s'\n\n"") % req)) LOG.info(strutils.mask_password(_(""req : '%s'\n\n"") % req))"," LOG.info(logging.mask_password(_(""req : '%s'\n\n"") % req)) LOG.info(logging.mask_password(_(""body : '%s'\n\n"") % body)) LOG.info(logging.mask_password(_(""req : '%s'\n\n"") % req)) LOG.info(logging.mask_password(_(""req : '%s'\n\n"") % req))",150,85
openstack%2Fopenstack-manuals~master~If4c5a3451ecf5ffd2001374017865b28337b6c42,openstack/openstack-manuals,master,If4c5a3451ecf5ffd2001374017865b28337b6c42,Test. Ignore it.,ABANDONED,2014-09-26 04:28:49.000000000,2014-09-26 04:31:58.000000000,,[],"[{'number': 1, 'created': '2014-09-26 04:28:49.000000000', 'files': ['projects.txt'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/fece86e6108644f53dab36be879bc6c921657afc', 'message': 'Test. Ignore it.\n\nChange-Id: If4c5a3451ecf5ffd2001374017865b28337b6c42\n'}]",0,124263,fece86e6108644f53dab36be879bc6c921657afc,2,0,1,2861,,,0,"Test. Ignore it.

Change-Id: If4c5a3451ecf5ffd2001374017865b28337b6c42
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/63/124263/1 && git format-patch -1 --stdout FETCH_HEAD,['projects.txt'],1,fece86e6108644f53dab36be879bc6c921657afc,test, ,,2,0
openstack%2Frequirements~master~If249b241416586eec655fd3f11983545e976eb89,openstack/requirements,master,If249b241416586eec655fd3f11983545e976eb89,Correcting case of pyscss to match pypi,MERGED,2014-08-29 23:16:04.000000000,2014-09-26 03:52:59.000000000,2014-09-26 03:52:58.000000000,"[{'_account_id': 3}, {'_account_id': 308}, {'_account_id': 2750}, {'_account_id': 6547}, {'_account_id': 6786}]","[{'number': 1, 'created': '2014-08-29 23:16:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/809fec53caef0b88fa335d82e34f0f130155f23f', 'message': 'Correcting case of pyscss to match pypi\n\nChange-Id: If249b241416586eec655fd3f11983545e976eb89\n'}, {'number': 2, 'created': '2014-09-23 13:15:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/531bd180457d54dbaafde931298411d67dcc49a3', 'message': 'Correcting case of pyscss to match pypi\n\nChange-Id: If249b241416586eec655fd3f11983545e976eb89\n'}, {'number': 3, 'created': '2014-09-25 12:29:48.000000000', 'files': ['global-requirements.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/0e0504ebedff0f61725346f58e835588f0e18265', 'message': 'Correcting case of pyscss to match pypi\n\nChange-Id: If249b241416586eec655fd3f11983545e976eb89\n'}]",0,117910,0e0504ebedff0f61725346f58e835588f0e18265,22,5,3,5623,,,0,"Correcting case of pyscss to match pypi

Change-Id: If249b241416586eec655fd3f11983545e976eb89
",git fetch https://review.opendev.org/openstack/requirements refs/changes/10/117910/3 && git format-patch -1 --stdout FETCH_HEAD,['global-requirements.txt'],1,809fec53caef0b88fa335d82e34f0f130155f23f,117910,pyScss>=1.2.0 # MIT License,pyscss>=1.2.0 # MIT License,1,1
openstack%2Frequirements~master~I6c7b881f31c1d96a707dc0d79fcebe6c3201ed35,openstack/requirements,master,I6c7b881f31c1d96a707dc0d79fcebe6c3201ed35,Require kombu 2.5.0 or newer for switch to py-amqp,MERGED,2014-05-05 11:47:07.000000000,2014-09-26 03:51:05.000000000,2014-09-26 03:51:04.000000000,"[{'_account_id': 3}, {'_account_id': 308}, {'_account_id': 2472}, {'_account_id': 2592}, {'_account_id': 2750}, {'_account_id': 4190}, {'_account_id': 6786}, {'_account_id': 6873}, {'_account_id': 8871}]","[{'number': 1, 'created': '2014-05-05 11:47:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/611449d8272a2bf1222f9354930f6a29b40ccd85', 'message': 'Require kombu 2.5.0 or newer for switch to py-amqp\n\nkombu 2.5.0 and newer has switched away from amqplib\nto amqp, which is a fork of amqplib started with the following\ngoals:\n\n- Uses AMQP 0.9.1 instead of 0.8\n- Support for heartbeats (Issue #79 + Issue #131)\n- Automatically revives channels on channel errors.\n- Support for all RabbitMQ extensions\n  - Consumer Cancel Notifications (Issue #131)\n  - Publisher Confirms (Issue #131).\n  - Exchange-to-exchange bindings: ``exchange_bind`` / ``exchange_unbind``.\n\nChange-Id: I6c7b881f31c1d96a707dc0d79fcebe6c3201ed35\n'}, {'number': 2, 'created': '2014-06-11 17:01:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/033a7e1463802e5200584708b446b6f58245fd42', 'message': 'Require kombu 2.5.0 or newer for switch to py-amqp\n\nkombu 2.5.0 and newer has switched away from amqplib\nto amqp, which is a fork of amqplib started with the following\ngoals:\n\n- Uses AMQP 0.9.1 instead of 0.8\n- Support for heartbeats (Issue #79 + Issue #131)\n- Automatically revives channels on channel errors.\n- Support for all RabbitMQ extensions\n  - Consumer Cancel Notifications (Issue #131)\n  - Publisher Confirms (Issue #131).\n  - Exchange-to-exchange bindings: ``exchange_bind`` / ``exchange_unbind``.\n\nChange-Id: I6c7b881f31c1d96a707dc0d79fcebe6c3201ed35\n'}, {'number': 3, 'created': '2014-09-23 12:47:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/5f956f04d0e3c14bc13b009379083950aaa73d97', 'message': 'Require kombu 2.5.0 or newer for switch to py-amqp\n\nkombu 2.5.0 and newer has switched away from amqplib\nto amqp, which is a fork of amqplib started with the following\ngoals:\n\n- Uses AMQP 0.9.1 instead of 0.8\n- Support for heartbeats (Issue #79 + Issue #131)\n- Automatically revives channels on channel errors.\n- Support for all RabbitMQ extensions\n  - Consumer Cancel Notifications (Issue #131)\n  - Publisher Confirms (Issue #131).\n  - Exchange-to-exchange bindings: ``exchange_bind`` / ``exchange_unbind``.\n\nChange-Id: I6c7b881f31c1d96a707dc0d79fcebe6c3201ed35\n'}, {'number': 4, 'created': '2014-09-25 11:50:37.000000000', 'files': ['global-requirements.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/95615d5b626913a8beac0b383d8da16bbf405acd', 'message': 'Require kombu 2.5.0 or newer for switch to py-amqp\n\nkombu 2.5.0 and newer has switched away from amqplib\nto amqp, which is a fork of amqplib started with the following\ngoals:\n\n- Uses AMQP 0.9.1 instead of 0.8\n- Support for heartbeats (Issue #79 + Issue #131)\n- Automatically revives channels on channel errors.\n- Support for all RabbitMQ extensions\n  - Consumer Cancel Notifications (Issue #131)\n  - Publisher Confirms (Issue #131).\n  - Exchange-to-exchange bindings: ``exchange_bind`` / ``exchange_unbind``.\n\nChange-Id: I6c7b881f31c1d96a707dc0d79fcebe6c3201ed35\n'}]",0,92095,95615d5b626913a8beac0b383d8da16bbf405acd,40,9,4,6593,,,0,"Require kombu 2.5.0 or newer for switch to py-amqp

kombu 2.5.0 and newer has switched away from amqplib
to amqp, which is a fork of amqplib started with the following
goals:

- Uses AMQP 0.9.1 instead of 0.8
- Support for heartbeats (Issue #79 + Issue #131)
- Automatically revives channels on channel errors.
- Support for all RabbitMQ extensions
  - Consumer Cancel Notifications (Issue #131)
  - Publisher Confirms (Issue #131).
  - Exchange-to-exchange bindings: ``exchange_bind`` / ``exchange_unbind``.

Change-Id: I6c7b881f31c1d96a707dc0d79fcebe6c3201ed35
",git fetch https://review.opendev.org/openstack/requirements refs/changes/95/92095/3 && git format-patch -1 --stdout FETCH_HEAD,['global-requirements.txt'],1,611449d8272a2bf1222f9354930f6a29b40ccd85,kombu,kombu>=2.5.0,kombu>=2.4.8,1,1
openstack%2Fneutron~stable%2Fhavana~I6537bb1da5ef0d6899bc71e4e949f2c760c103c2,openstack/neutron,stable/havana,I6537bb1da5ef0d6899bc71e4e949f2c760c103c2,Forbid regular users to reset admin-only attrs to default values,ABANDONED,2014-09-25 06:56:00.000000000,2014-09-26 03:49:46.000000000,,"[{'_account_id': 3}, {'_account_id': 5170}, {'_account_id': 7293}, {'_account_id': 9656}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9846}, {'_account_id': 10153}, {'_account_id': 10192}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12040}, {'_account_id': 12737}]","[{'number': 1, 'created': '2014-09-25 06:56:00.000000000', 'files': ['neutron/common/constants.py', 'neutron/api/v2/base.py', 'neutron/tests/unit/test_policy.py', 'neutron/tests/unit/test_api_v2.py', 'neutron/policy.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/cb14cbaa1b6beb59cb98daf45f0ee9f7676ed53e', 'message': 'Forbid regular users to reset admin-only attrs to default values\n\nA regular user can reset an admin-only attribute to its default\nvalue due to the fact that a corresponding policy rule is\nenforced only in the case when an attribute is present in the\ntarget AND has a non-default value.\n\nAdded a new attribute ""attributes_to_update"" which contains a list\nof all to-be updated attributes to the body of the target that is\npassed to policy.enforce.\n\nChanged a check for whether an attribute is explicitly set.\nNow, in the case of update, the function should not pay attention\nto a default value of an attribute, but check whether it was\nexplicitly marked as being updated.\n\nAdded unit-tests.\n\nConflicts:\n        neutron/common/constants.py\n\nCloses-Bug: #1357379\nRelated-Bug: #1338880\nChange-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2\n(cherry-picked from commit 74d10939903984d5f06c1749a8707fa3257e44ff)\n'}]",0,123952,cb14cbaa1b6beb59cb98daf45f0ee9f7676ed53e,21,14,1,7293,,,0,"Forbid regular users to reset admin-only attrs to default values

A regular user can reset an admin-only attribute to its default
value due to the fact that a corresponding policy rule is
enforced only in the case when an attribute is present in the
target AND has a non-default value.

Added a new attribute ""attributes_to_update"" which contains a list
of all to-be updated attributes to the body of the target that is
passed to policy.enforce.

Changed a check for whether an attribute is explicitly set.
Now, in the case of update, the function should not pay attention
to a default value of an attribute, but check whether it was
explicitly marked as being updated.

Added unit-tests.

Conflicts:
        neutron/common/constants.py

Closes-Bug: #1357379
Related-Bug: #1338880
Change-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2
(cherry-picked from commit 74d10939903984d5f06c1749a8707fa3257e44ff)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/52/123952/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/common/constants.py', 'neutron/api/v2/base.py', 'neutron/tests/unit/test_policy.py', 'neutron/tests/unit/test_api_v2.py', 'neutron/policy.py']",5,cb14cbaa1b6beb59cb98daf45f0ee9f7676ed53e,," import collectionsfrom neutron.common import constants as constdef _is_attribute_explicitly_set(attribute_name, resource, target, action): """"""Verify that an attribute is present and is explicitly set."""""" if 'update' in action: # In the case of update, the function should not pay attention to a # default value of an attribute, but check whether it was explicitly # marked as being updated instead. return (attribute_name in target[const.ATTRIBUTES_TO_UPDATE] and target[attribute_name] is not attributes.ATTR_NOT_SPECIFIED)def _should_validate_sub_attributes(attribute, sub_attr): """"""Verify that sub-attributes are iterable and should be validated."""""" validate = attribute.get('validate') return (validate and isinstance(sub_attr, collections.Iterable) and any([k.startswith('type:dict') and v for (k, v) in validate.iteritems()])) target, action): # Build match entries for sub-attributes if _should_validate_sub_attributes( attribute, target[attribute_name]):","def _is_attribute_explicitly_set(attribute_name, resource, target): """"""Verify that an attribute is present and has a non-default value."""""" target): # Build match entries for sub-attributes, if present validate = attribute.get('validate') if (validate and any([k.startswith('type:dict') and v for (k, v) in validate.iteritems()])):",59,13
openstack%2Fnova~master~If2a8cec0550b9fc05bb0a1c2382ba38fed7abf30,openstack/nova,master,If2a8cec0550b9fc05bb0a1c2382ba38fed7abf30,Clean launcher and children in multiprocess tests,ABANDONED,2014-09-23 17:27:01.000000000,2014-09-26 03:34:50.000000000,,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 1812}, {'_account_id': 1849}, {'_account_id': 2750}, {'_account_id': 4190}, {'_account_id': 5170}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-23 17:27:01.000000000', 'files': ['nova/tests/integrated/test_multiprocess_api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/48b0735999bd2c9b38a211b6b356da69d2f0d618', 'message': 'Clean launcher and children in multiprocess tests\n\nTwo of the test methods in test_multiprocess_api.py were relying on\ntearDown() to terminate child worker processes and terminate the\nlauncher process PID itself. This patch changes those two test methods\nto manually terminate the workers and launcher process PID, and adds\nsome debugging log output to identify exactly what happened right before\nthe blocking call to os.waitpid() if we see a timeout occur.\n\nChange-Id: If2a8cec0550b9fc05bb0a1c2382ba38fed7abf30\nPartial-bug: #1357578\n'}]",3,123521,48b0735999bd2c9b38a211b6b356da69d2f0d618,9,8,1,7,,,0,"Clean launcher and children in multiprocess tests

Two of the test methods in test_multiprocess_api.py were relying on
tearDown() to terminate child worker processes and terminate the
launcher process PID itself. This patch changes those two test methods
to manually terminate the workers and launcher process PID, and adds
some debugging log output to identify exactly what happened right before
the blocking call to os.waitpid() if we see a timeout occur.

Change-Id: If2a8cec0550b9fc05bb0a1c2382ba38fed7abf30
Partial-bug: #1357578
",git fetch https://review.opendev.org/openstack/nova refs/changes/21/123521/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/integrated/test_multiprocess_api.py'],1,48b0735999bd2c9b38a211b6b356da69d2f0d618,bug/1357578," workers = self._get_workers() LOG.info(""in tearDown() with a self.pid, but we should not be. "" ""Child processes still exist %r for launcher pid %d"", workers, self.pid) while workers: worker_pid = workers.pop(0) LOG.info(""in tearDown(), killing child pid %d in "" ""launcher pid %d"", worker_pid, self.pid) os.kill(worker_pid, signal.SIGTERM) LOG.info(""in tearDown(), killing launcher pid %d."", self.pid) # This will deliberately block if the launcher pid cannot be # killed for some reason. We want to see what is happening # right before the os.waitpid() and not mask the exception # behaviour here by trapping fixtures.TimeoutException. self._reap_test() worker_pid = start_workers.pop(0) # Kill the remaining workers while end_workers: worker_pid = end_workers.pop(0) LOG.info(""Cleanup in test_killed_worker_rescue: "" ""killing child %d of launcher parent %d with SIGTERM"", worker_pid, self.pid) os.kill(worker_pid, signal.SIGTERM) # Finally, terminate the whole launcher process group LOG.info(""Cleanup in test_killed_worker_rescue: "" ""killing launcher parent %d with SIGTERM"", self.pid) os.kill(self.pid, signal.SIGTERM) self._reap_test() # Kill the remaining workers while end_workers: worker_pid = end_workers.pop(0) LOG.info(""Cleanup in test_restart_sighup: "" ""killing child %d of launcher parent %d with SIGTERM"", worker_pid, self.pid) os.kill(worker_pid, signal.SIGTERM) # Finally, terminate the whole launcher process group LOG.info(""Cleanup in test_restart_sighup: "" ""killing launcher parent %d with SIGTERM"", self.pid) os.kill(self.pid, signal.SIGTERM) self._reap_test() "," # Make sure all processes are stopped LOG.info(""in tearDown(), stopping launcher pid %d with SIGTERM"", self.pid) try: # Make sure we reap our test process self._wait_for_all_workers_to_end() self._reap_test() except fixtures.TimeoutException: # If the child gets stuck or is too slow in exiting # after receiving the SIGTERM, gracefully handle the # timeout exception and try harder to kill it. We need # to do this otherwise the child process can hold up # the test run LOG.warn(""got fixtures.TimeoutException during tearDown(). "" ""going nuclear with SIGKILL."") for worker_pid in self._get_workers(): LOG.warn(""worker pid %d"" % worker_pid) os.kill(worker_pid, signal.SIGKILL) LOG.warn(""parent pid %d"" % self.pid) os.kill(self.pid, signal.SIGKILL) raise worker_pid = start_workers[0]",45,24
openstack%2Fheat-specs~master~Ica51815dd579c4ddc349e4c9b619c1d7468f2357,openstack/heat-specs,master,Ica51815dd579c4ddc349e4c9b619c1d7468f2357,Add a file folder 'juno' for juno bp-specs,MERGED,2014-09-22 11:01:07.000000000,2014-09-26 02:31:10.000000000,2014-09-26 02:31:10.000000000,"[{'_account_id': 3}, {'_account_id': 4571}, {'_account_id': 4715}, {'_account_id': 6577}, {'_account_id': 8246}, {'_account_id': 8290}, {'_account_id': 9542}]","[{'number': 1, 'created': '2014-09-22 11:01:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-specs/commit/b2b529b193f75bf101aa575bfb55724dee2a4907', 'message': ""Add a file floder 'juno' for juno bp-specs\n\nCreate 'juno' file floder and put the specs into it.\nChange-Id: Ica51815dd579c4ddc349e4c9b619c1d7468f2357\n""}, {'number': 2, 'created': '2014-09-23 01:26:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-specs/commit/bdc01b367d24d38a51109000ce3ffe6db3f39d1a', 'message': ""Add a file floder 'juno' for juno bp-specs\n\nCreate 'juno' file floder and put the specs into it.\nChange-Id: Ica51815dd579c4ddc349e4c9b619c1d7468f2357\n""}, {'number': 3, 'created': '2014-09-23 01:29:40.000000000', 'files': ['specs/juno/encrypt-hidden-parameters.rst', 'specs/juno/implement-ec2instance-bdm.rst', 'specs/juno/implement-launchconfiguration-bdm.rst', 'specs/juno/cfn-liststacks-filter.rst', 'specs/juno/implement-aws-updates-not-supported.rst', 'specs/juno/stack-display-fields.rst', 'specs/juno/template.rst', 'specs/juno/action-aware-sw-config.rst', 'specs/juno/vnc-console.rst', 'specs/juno/stack-lifecycle-plugpoint.rst', 'specs/juno/neutron-custom-constraint.rst', 'specs/juno/reorg-autoscaling-group.rst', 'specs/juno/stack-breakpoint.rst', 'specs/juno/convergence.rst', 'doc/source/index.rst', 'specs/juno/explode-nested-resources.rst', 'specs/juno/decouple-nested.rst', 'specs/juno/convergence-observer.rst', 'specs/juno/events-pagination.rst', 'specs/juno/log-translation-hints.rst'], 'web_link': 'https://opendev.org/openstack/heat-specs/commit/f5f88cf4198c34037402cc166076d787a4096b20', 'message': ""Add a file folder 'juno' for juno bp-specs\n\nCreate 'juno' file folder and put the specs into it.\nChange-Id: Ica51815dd579c4ddc349e4c9b619c1d7468f2357\n""}]",0,123093,f5f88cf4198c34037402cc166076d787a4096b20,15,7,3,8289,,,0,"Add a file folder 'juno' for juno bp-specs

Create 'juno' file folder and put the specs into it.
Change-Id: Ica51815dd579c4ddc349e4c9b619c1d7468f2357
",git fetch https://review.opendev.org/openstack/heat-specs refs/changes/93/123093/1 && git format-patch -1 --stdout FETCH_HEAD,"['specs/juno/encrypt-hidden-parameters.rst', 'specs/juno/implement-ec2instance-bdm.rst', 'specs/juno/implement-launchconfiguration-bdm.rst', 'specs/juno/cfn-liststacks-filter.rst', 'specs/juno/implement-aws-updates-not-supported.rst', 'specs/juno/stack-display-fields.rst', 'specs/juno/template.rst', 'specs/juno/action-aware-sw-config.rst', 'specs/juno/vnc-console.rst', 'specs/juno/stack-lifecycle-plugpoint.rst', 'specs/juno/neutron-custom-constraint.rst', 'specs/juno/reorg-autoscaling-group.rst', 'specs/juno/stack-breakpoint.rst', 'specs/juno/convergence.rst', 'specs/juno/explode-nested-resources.rst', 'specs/juno/decouple-nested.rst', 'specs/juno/convergence-observer.rst', 'specs/juno/events-pagination.rst', 'specs/juno/log-translation-hints.rst']",19,b2b529b193f75bf101aa575bfb55724dee2a4907,add_juno_folder,,,0,0
openstack%2Fheat-templates~master~If2b024d7717e2846bb199e950c314ce9c9d778e7,openstack/heat-templates,master,If2b024d7717e2846bb199e950c314ce9c9d778e7,HOT for MSSQL Server with unit tests,MERGED,2014-09-10 00:43:41.000000000,2014-09-26 02:21:49.000000000,2014-09-26 02:21:49.000000000,"[{'_account_id': 3}, {'_account_id': 3185}, {'_account_id': 4328}, {'_account_id': 4571}, {'_account_id': 4715}, {'_account_id': 9649}, {'_account_id': 13178}]","[{'number': 1, 'created': '2014-09-10 00:43:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-templates/commit/f95d5553cd3f84bb36c4815341b8f27a8d8ed13f', 'message': 'HOT for MSSQL Server 2012 SP2 with unit tests\n\nIt deploys an MSSQL Server 2012 SP2 instance.\nThe MSSQL ISO file is copied from an SMB share.\n\nThe unit tests for the powershell module(user data scripts) are written\nusing Pester 3.0\n\nChange-Id: If2b024d7717e2846bb199e950c314ce9c9d778e7\nPartially-Implements: blueprint windows-instances\n'}, {'number': 2, 'created': '2014-09-10 00:54:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-templates/commit/353f8db3df213359e5ca4663b96439317ac92fa9', 'message': 'HOT for MSSQL Server 2012 SP2 with unit tests\n\nIt deploys an MSSQL Server 2012 SP2 instance.\nThe MSSQL ISO file is copied from an SMB share.\n\nThe unit tests for the powershell module(user data scripts) are written\nusing Pester 3.0\n\nChange-Id: If2b024d7717e2846bb199e950c314ce9c9d778e7\nPartially-Implements: blueprint windows-instances\n'}, {'number': 3, 'created': '2014-09-10 01:01:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-templates/commit/4b2d7157c7098ba399aa526147f1ba5f5c9993e0', 'message': 'HOT for MSSQL Server 2012 SP2 with unit tests\n\nIt deploys an MSSQL Server 2012 SP2 instance.\nThe MSSQL ISO file is copied from an SMB share.\n\nThe unit tests for the powershell module(user data scripts) are written\nusing Pester 3.0\n\nChange-Id: If2b024d7717e2846bb199e950c314ce9c9d778e7\nPartially-Implements: blueprint windows-instances\n'}, {'number': 4, 'created': '2014-09-10 03:29:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-templates/commit/60135e9297b9ef4f6ed4d11934f7e60c4d54b9d0', 'message': 'HOT for MSSQL Server 2012 SP2 with unit tests\n\nIt deploys an MSSQL Server 2012 SP2 instance.\nThe MSSQL ISO file is copied from an SMB share.\n\nThe unit tests for the powershell module(user data scripts) are written\nusing Pester 3.0\n\nChange-Id: If2b024d7717e2846bb199e950c314ce9c9d778e7\nPartially-Implements: blueprint windows-instances\n'}, {'number': 5, 'created': '2014-09-17 02:39:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-templates/commit/641675663af05b65af9c21c552abe16b5ba1f873', 'message': 'HOT for MSSQL Server 2012 SP2 with unit tests\n\nIt deploys an MSSQL Server 2012 SP2 instance.\nThe MSSQL ISO file is copied from an SMB share.\n\nThe unit tests for the powershell module(user data scripts) are written\nusing Pester 3.0\n\nChange-Id: If2b024d7717e2846bb199e950c314ce9c9d778e7\nPartially-Implements: blueprint windows-instances\n'}, {'number': 6, 'created': '2014-09-17 02:46:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat-templates/commit/2f225176682e835477beb593f5cf4fd09855936a', 'message': 'HOT for MSSQL Server 2012 SP2 with unit tests\n\nIt deploys an MSSQL Server 2012 SP2 instance.\nThe MSSQL ISO file is copied from an SMB share.\n\nThe unit tests for the powershell module(user data scripts) are written\nusing Pester 3.0\n\nChange-Id: If2b024d7717e2846bb199e950c314ce9c9d778e7\nPartially-Implements: blueprint windows-instances\n'}, {'number': 7, 'created': '2014-09-18 01:01:53.000000000', 'files': ['hot/Windows/MSSQLServer/Tests/MSSQL.Tests.ps1', 'hot/Windows/MSSQLServer/MSSQL.ps1', 'hot/Windows/MSSQLServer/MSSQL.yaml', 'hot/Windows/MSSQLServer/MSSQL.psm1', 'hot/Windows/MSSQLServer/heat-powershell-utils.psm1'], 'web_link': 'https://opendev.org/openstack/heat-templates/commit/d22d8b25877d0afe61a8db4c5291b49c141cdda9', 'message': 'HOT for MSSQL Server with unit tests\n\nIt deploys an MSSQL Server instance.\nThe MSSQL ISO file is copied from an SMB share.\n\nThe unit tests for the powershell module(user data scripts) are written\nusing Pester 3.0\n\nChange-Id: If2b024d7717e2846bb199e950c314ce9c9d778e7\nPartially-Implements: blueprint windows-instances\n'}]",9,120289,d22d8b25877d0afe61a8db4c5291b49c141cdda9,22,7,7,9649,,,0,"HOT for MSSQL Server with unit tests

It deploys an MSSQL Server instance.
The MSSQL ISO file is copied from an SMB share.

The unit tests for the powershell module(user data scripts) are written
using Pester 3.0

Change-Id: If2b024d7717e2846bb199e950c314ce9c9d778e7
Partially-Implements: blueprint windows-instances
",git fetch https://review.opendev.org/openstack/heat-templates refs/changes/89/120289/6 && git format-patch -1 --stdout FETCH_HEAD,"['hot/Windows/MSSQLServer/Tests/MSSQL.Tests.ps1', 'hot/Windows/MSSQLServer/MSSQL.ps1', 'hot/Windows/MSSQLServer/MSSQL.yaml', 'hot/Windows/MSSQLServer/MSSQL.psm1']",4,f95d5553cd3f84bb36c4815341b8f27a8d8ed13f,bp/windows-instances,"#ps1_sysnative # Copyright 2014 Cloudbase Solutions Srl # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. $ErrorActionPreference = 'Stop' $rebootCode = 1001 $reexecuteCode = 1002 $rebootAndReexecuteCode = 1003 #HEAT PARAMS $mssqlServiceUsername = ""mssql-service-username"" $mssqlServicePassword = ""mssql-service-user-password"" $mssqlSaPassword = ""mssql-sa-password"" $mssqlFeatures = ""mssql-features"" $mssqlInstanceName = ""mssql-instance-name"" $isoName = ""mssql-iso"" $smbShare = ""samba-share"" $smbMsSQLFolder = ""samba-folder"" #LOCAL PARAMS $windowsFeatures = @('NET-Framework-Core') $smbDrive = ""X"" $temp = ($smbDrive + "":\\"" + $smbMsSQLFolder) $copyLocal = $env:temp $powershellPath = Join-Path $PShome ""powershell.exe"" $sqlSetupLogFile = ""C:\Program Files\Microsoft SQL Server\110\Setup Bootstrap\Log\Summary.txt"" $sqlErrorString = ""Failed: see details below"" $localAdminUsername = ""adminlocal"" #UTILS function Log { param ( $message ) Write-Host $message } function Is-FeatureAvailable { param( [Parameter(Mandatory=$true)] [string]$FeatureName ) $featureInstallState = (Get-WindowsFeature -Name $FeatureName).InstallState $isAvailable = ($featureInstallState -eq ""Available"") -or ` ($featureInstallState -eq ""Removed"") return $isAvailable } function Is-FeatureInstalled { param( [Parameter(Mandatory=$true)] [string]$FeatureName ) $installState = (Get-WindowsFeature -Name $FeatureName).InstallState $isInstalled = ($installState -eq ""Installed"") ` -or ($installState -eq ""InstallPending"" ) return $isInstalled } function Install-WindowsFeatures { param( [Parameter(Mandatory=$true)] [array]$Features ) $installedFeatures = 0 $rebootNeeded = $false foreach ($feature in $Features) { $isAvailable = Is-FeatureAvailable $feature if ($isAvailable -eq $true) { $res = Install-WindowsFeature -Name $feature if ($res.RestartNeeded -eq 'Yes') { $rebootNeeded = $true } } $isInstalled = Is-FeatureInstalled $feature if ($isInstalled -eq $true) { $installedFeatures = $installedFeatures + 1 } else { Log ""Install failed for feature $feature"" } } return @{""InstalledFeatures"" = $installedFeatures; ""Reboot"" = $rebootNeeded } } function ExecuteWith-RetryPSCommand { param( [ScriptBlock]$Command, [int]$MaxRetryCount=3, [int]$RetryInterval=0, [array]$ArgumentList=@() ) $currentErrorActionPreference = $ErrorActionPreference $ErrorActionPreference = ""Continue"" $retryCount = 0 while ($true) { try { $res = Invoke-Command -ScriptBlock $Command ` -ArgumentList $ArgumentList return $res } catch [System.Exception] { if ($retryCount -ge $MaxRetryCount) { $ErrorActionPreference = $currentErrorActionPreference throw $_.Exception } else { Start-Sleep $RetryInterval } $retryCount++ } } $ErrorActionPreference = $currentErrorActionPreference } function Copy-FilesLocal { param() New-PSDrive -Name $smbDrive -Root $smbShare -PSProvider FileSystem if (!(Test-Path ""$copyLocal\$isoName"")){ Copy-Item ""$temp\$isoName"" $copyLocal } } #LOCAL METHODS function Add-MSSQLUser { param() Log ""Adding the MSSQL user."" NET USER $mssqlServiceUsername $mssqlServicePassword '/ADD' } function Add-NetRules { param() netsh advfirewall firewall add rule name=""Open Port 80"" dir=in action=allow protocol=TCP localport=80 netsh advfirewall firewall add rule name=""SQL Server"" dir=in action=allow protocol=TCP localport=1433 netsh advfirewall firewall add rule name=""SQL Admin Connection"" dir=in action=allow protocol=TCP localport=1434 netsh advfirewall firewall add rule name=""SQL Service Broker"" dir=in action=allow protocol=TCP localport=4022 netsh advfirewall firewall add rule name=""SQL Debugger/RPC"" dir=in action=allow protocol=TCP localport=135 netsh advfirewall firewall add rule name=""Analysis Services"" dir=in action=allow protocol=TCP localport=2383 netsh advfirewall firewall add rule name=""SQL Browser"" dir=in action=allow protocol=TCP localport=2382 netsh advfirewall firewall add rule name=""HTTP"" dir=in action=allow protocol=TCP localport=80 netsh advfirewall firewall add rule name=""SSL"" dir=in action=allow protocol=TCP localport=443 netsh advfirewall firewall add rule name=""SQL Browser"" dir=in action=allow protocol=UDP localport=1434 netsh firewall set multicastbroadcastresponse ENABLE $sqlServerBinaryPath = ""C:\Program Files\Microsoft SQL Server\MSSQL11.MSSQL\MSSQL\Binn\sqlservr.exe"" New-NetFirewallRule -DisplayName ""Allow TCP Sql Server Ports"" ` -Direction Inbound -Action Allow -EdgeTraversalPolicy Allow ` -Protocol UDP -LocalPort 100-65000 -Program $sqlServerBinaryPath New-NetFirewallRule -DisplayName ""Allow TCP Sql Server Ports"" ` -Direction Inbound -Action Allow -EdgeTraversalPolicy Allow ` -Protocol TCP -LocalPort 100-65000 -Program $sqlServerBinaryPath } function Get-MSSQLError { param() $errorsCount = (Select-String $sqlErrorString -Path $sqlSetupLogFile).Length if ($errorsCount -ne 0) { Log ""MSSQL log file has an error."" return $true } return $false } function Install-RequiredFeatures { param() $installStatus = Install-WindowsFeatures $windowsFeatures if ($installStatus.Reboot -eq $true) { exit $rebootAndReexecuteCode } } #EXPORTED METHODS function Install-MSSQL { param() Log ""Started MSSQL instalation."" Install-RequiredFeatures Add-MSSQLUser Copy-FilesLocal Log ""MSSQL ISO Mount."" $iso = Mount-DiskImage -PassThru ""$copyLocal\$isoName"" $isoSetupPath = (Get-Volume -DiskImage $iso).DriveLetter + "":\setup.exe"" if (Test-Path $sqlSetupLogFile) { Remove-Item $sqlSetupLogFile -Force } $parameters = ""/ACTION=install "" $parameters += ""/Q "" $parameters += ""/IACCEPTSQLSERVERLICENSETERMS=1 "" $parameters += ""/INSTANCENAME=$mssqlInstanceName "" $parameters += ""/FEATURES=$mssqlFeatures "" $parameters += ""/SQLSYSADMINACCOUNTS=Admin "" $parameters += ""/UpdateEnabled=1 "" $parameters += ""/AGTSVCSTARTUPTYPE=Automatic "" $parameters += ""/BROWSERSVCSTARTUPTYPE=Automatic "" $parameters += ""/SECURITYMODE=SQL "" $parameters += ""/SAPWD=$mssqlSaPassword "" $parameters += ""/SQLSVCACCOUNT=.\$mssqlServiceUsername "" $parameters += ""/SQLSVCPASSWORD=$mssqlServicePassword "" $parameters += ""/SQLSVCSTARTUPTYPE=Automatic "" $parameters += ""/NPENABLED=1 "" $parameters += ""/TCPENABLED=1 /ERRORREPORTING=1"" ExecuteWith-RetryPSCommand -Command { param($isoSetupPath, $parameters) Start-Process -Wait -FilePath $isoSetupPath ` -ArgumentList $parameters } -ArgumentList @($isoSetupPath, $parameters) Dismount-DiskImage -ImagePath $iso.ImagePath if ((Get-MSSQLError) -eq $true) { throw ""Failed to install MSSQL Server."" } Add-NetRules Remove-Item ""$copyLocal\$isoName"" Log ""Finished MSSQL instalation."" } try { Export-ModuleMember -Function * -ErrorAction SilentlyContinue } catch { Log ""Outside of the module. This file has been dot sourced or included as text."" }",,476,0
openstack%2Fswift~master~I363651046cf414e14f15affd834043aabd5427c0,openstack/swift,master,I363651046cf414e14f15affd834043aabd5427c0,Fix profile tests to clean up its tempdirs.,MERGED,2014-09-23 16:17:23.000000000,2014-09-26 02:16:09.000000000,2014-09-26 02:16:09.000000000,"[{'_account_id': 3}, {'_account_id': 2622}, {'_account_id': 6968}, {'_account_id': 7233}, {'_account_id': 7847}, {'_account_id': 13052}]","[{'number': 1, 'created': '2014-09-23 16:17:23.000000000', 'files': ['test/unit/common/middleware/test_xprofile.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/d10462e8704e7f5efe03c4024b541a3780348615', 'message': 'Fix profile tests to clean up its tempdirs.\n\nChange-Id: I363651046cf414e14f15affd834043aabd5427c0\n'}]",1,123499,d10462e8704e7f5efe03c4024b541a3780348615,10,6,1,2696,,,0,"Fix profile tests to clean up its tempdirs.

Change-Id: I363651046cf414e14f15affd834043aabd5427c0
",git fetch https://review.opendev.org/openstack/swift refs/changes/99/123499/1 && git format-patch -1 --stdout FETCH_HEAD,['test/unit/common/middleware/test_xprofile.py'],1,d10462e8704e7f5efe03c4024b541a3780348615,fxup-profile-tests," self.tempdirs = [tempfile.mkdtemp(), tempfile.mkdtemp()] self.log_filename_prefix1 = self.tempdirs[0] + '/unittest.profile' self.log_filename_prefix2 = self.tempdirs[1] + '/unittest.profile' for tempdir in self.tempdirs: shutil.rmtree(tempdir, ignore_errors=True)", self.log_filename_prefix1 = tempfile.mkdtemp() + '/unittest.profile' self.log_filename_prefix2 = tempfile.mkdtemp() + '/unittest.profile',5,2
openstack%2Ftaskflow~master~I8dc579083799289510c572cae83ad8cea0847b1d,openstack/taskflow,master,I8dc579083799289510c572cae83ad8cea0847b1d,Ensure that details always exist in the memory backend,ABANDONED,2014-09-05 05:24:08.000000000,2014-09-26 01:32:19.000000000,,[{'_account_id': 3}],"[{'number': 1, 'created': '2014-09-05 05:24:08.000000000', 'files': ['taskflow/persistence/backends/impl_memory.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/34495b7908e7439817d57971fc0be4e66ea5c6f4', 'message': 'Ensure that details always exist in the memory backend\n\nIn the case where an existing flow detail or logbook is updated\nwithout the memory backends dictionaries tracking the addition we need\nto make sure that we add new atom/flow details when they are discovered\nnot existing in the memory backends objects.\n\nFixes bug 1365830\n\nChange-Id: I8dc579083799289510c572cae83ad8cea0847b1d\n'}]",0,119291,34495b7908e7439817d57971fc0be4e66ea5c6f4,3,1,1,1297,,,0,"Ensure that details always exist in the memory backend

In the case where an existing flow detail or logbook is updated
without the memory backends dictionaries tracking the addition we need
to make sure that we add new atom/flow details when they are discovered
not existing in the memory backends objects.

Fixes bug 1365830

Change-Id: I8dc579083799289510c572cae83ad8cea0847b1d
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/91/119291/1 && git format-patch -1 --stdout FETCH_HEAD,['taskflow/persistence/backends/impl_memory.py'],1,34495b7908e7439817d57971fc0be4e66ea5c6f4,bug/1365830, if e_ad.uuid not in self.backend.atom_details: self.backend.atom_details[e_ad.uuid] = e_ad if e_fd.uuid not in self.backend.flow_details: self.backend.flow_details[e_fd.uuid] = e_fd, self.backend.flow_details[e_fd.uuid] = e_fd,4,1
openstack%2Fnova~master~I1e725f7fea4a0ce40bc6c3b50834cd007c72f803,openstack/nova,master,I1e725f7fea4a0ce40bc6c3b50834cd007c72f803,Move class var to start of class,ABANDONED,2014-08-28 08:42:18.000000000,2014-09-26 01:19:54.000000000,,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 6062}, {'_account_id': 7730}, {'_account_id': 8412}, {'_account_id': 8543}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9550}, {'_account_id': 9569}, {'_account_id': 9578}, {'_account_id': 10385}, {'_account_id': 12175}]","[{'number': 1, 'created': '2014-08-28 08:42:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/bba6fbd513c2f555d65dc31ada06fa2a2bd36181', 'message': 'Move class var to start of class\n\nPut class vairable at middle of class bring some confusion, move\nto beginning make it more readable.\n\nChange-Id: I1e725f7fea4a0ce40bc6c3b50834cd007c72f803\n'}, {'number': 2, 'created': '2014-09-11 05:52:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c42a02e232cb87f3aff37cbb5a31d5f8898aa900', 'message': 'Move class var to start of class\n\nPut class vairable at middle of class bring some confusion, move\nto beginning make it more readable.\n\nChange-Id: I1e725f7fea4a0ce40bc6c3b50834cd007c72f803\n'}, {'number': 3, 'created': '2014-09-12 09:40:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/5ba564e4f1f2d946fe9aa64e230cce51ca96f3a9', 'message': 'Move class var to start of class\n\nPut class variable at middle of class bring some confusion, move\nto beginning make it more readable.\n\nChange-Id: I1e725f7fea4a0ce40bc6c3b50834cd007c72f803\n'}, {'number': 4, 'created': '2014-09-16 03:31:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d15d39d71d923143d0ce49221834a95bb75877ec', 'message': 'Move class var to start of class\n\nPut class variable at middle of class bring some confusion, move\nto beginning make it more readable.\n\nChange-Id: I1e725f7fea4a0ce40bc6c3b50834cd007c72f803\n'}, {'number': 5, 'created': '2014-09-17 08:04:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/7f17b7bff6a0368bfbfb96e4dd86f73256823d41', 'message': 'Move class var to start of class\n\nPut class variable at middle of class bring some confusion, move\nto beginning make it more readable.\n\nChange-Id: I1e725f7fea4a0ce40bc6c3b50834cd007c72f803\n'}, {'number': 6, 'created': '2014-09-22 06:46:38.000000000', 'files': ['nova/tests/api/openstack/compute/contrib/test_quotas.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/6ef751f50b413f06136da441720c0c94d39e2152', 'message': 'Move class var to start of class\n\nPut class variable at middle of class bring some confusion, move\nto beginning make it more readable\n\nChange-Id: I1e725f7fea4a0ce40bc6c3b50834cd007c72f803\n'}]",3,117453,6ef751f50b413f06136da441720c0c94d39e2152,66,14,6,6062,,,0,"Move class var to start of class

Put class variable at middle of class bring some confusion, move
to beginning make it more readable

Change-Id: I1e725f7fea4a0ce40bc6c3b50834cd007c72f803
",git fetch https://review.opendev.org/openstack/nova refs/changes/53/117453/4 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/api/openstack/compute/contrib/test_quotas.py'],1,bba6fbd513c2f555d65dc31ada06fa2a2bd36181,adjust_test_quotas_indent," def setUp(self): super(ExtendedQuotasTest, self).setUp() self.ext_mgr = self.mox.CreateMock(extensions.ExtensionManager) self.controller = quotas.QuotaSetsController(self.ext_mgr) self.ext_mgr.is_loaded('os-extended-quotas').AndReturn(True) self.ext_mgr.is_loaded('os-user-quotas').AndReturn(True) self.mox.ReplayAll() "," def setUp(self): super(ExtendedQuotasTest, self).setUp() self.ext_mgr = self.mox.CreateMock(extensions.ExtensionManager) self.controller = quotas.QuotaSetsController(self.ext_mgr) self.ext_mgr.is_loaded('os-extended-quotas').AndReturn(True) self.ext_mgr.is_loaded('os-user-quotas').AndReturn(True) self.mox.ReplayAll() ",8,9
openstack%2Fnova~master~I8ef124e9f5d86a990722019978c725c955ffb1ea,openstack/nova,master,I8ef124e9f5d86a990722019978c725c955ffb1ea,libvirt: convert CacheConcurrencyTestCase to avoid DB usage,MERGED,2014-09-16 14:24:23.000000000,2014-09-26 01:05:38.000000000,2014-09-26 01:05:35.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 2271}, {'_account_id': 5170}, {'_account_id': 5367}, {'_account_id': 5511}, {'_account_id': 7730}, {'_account_id': 8412}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-16 14:24:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/79486b160e714593d49cf13e72d2fa87bdf6bda2', 'message': 'libvirt: convert CacheConcurrencyTestCase to avoid DB usage\n\nThe CacheConcurrencyTestCase test case does not do anything DB\nrelated. Convert it to NoDBTestCase to avoid DB setup penalty.\nThis improves perf from 1.7s to 0.2s\n\nPartial-bug: #1369516\nChange-Id: I8ef124e9f5d86a990722019978c725c955ffb1ea\n'}, {'number': 2, 'created': '2014-09-17 09:59:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/2c1718cc02f36dcb223c9a14cd879246955c17d0', 'message': 'libvirt: convert CacheConcurrencyTestCase to avoid DB usage\n\nThe CacheConcurrencyTestCase test case does not do anything DB\nrelated. Convert it to NoDBTestCase to avoid DB setup penalty.\nThis improves perf from 1.7s to 0.2s\n\nPartial-bug: #1369516\nChange-Id: I8ef124e9f5d86a990722019978c725c955ffb1ea\n'}, {'number': 3, 'created': '2014-09-22 11:00:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/75066a6bb4cd15efe970ce7ae341da332bcd2238', 'message': 'libvirt: convert CacheConcurrencyTestCase to avoid DB usage\n\nThe CacheConcurrencyTestCase test case does not do anything DB\nrelated. Convert it to NoDBTestCase to avoid DB setup penalty.\nThis improves perf from 1.7s to 0.2s\n\nPartial-bug: #1369516\nChange-Id: I8ef124e9f5d86a990722019978c725c955ffb1ea\n'}, {'number': 4, 'created': '2014-09-24 17:17:56.000000000', 'files': ['nova/tests/virt/libvirt/test_driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/144b3c441f61585c888c9a7152d6859ebdd689fe', 'message': 'libvirt: convert CacheConcurrencyTestCase to avoid DB usage\n\nThe CacheConcurrencyTestCase test case does not do anything DB\nrelated. Convert it to NoDBTestCase to avoid DB setup penalty.\nThis improves perf from 1.7s to 0.2s\n\nPartial-bug: #1369516\nChange-Id: I8ef124e9f5d86a990722019978c725c955ffb1ea\n'}]",0,121867,144b3c441f61585c888c9a7152d6859ebdd689fe,41,13,4,1779,,,0,"libvirt: convert CacheConcurrencyTestCase to avoid DB usage

The CacheConcurrencyTestCase test case does not do anything DB
related. Convert it to NoDBTestCase to avoid DB setup penalty.
This improves perf from 1.7s to 0.2s

Partial-bug: #1369516
Change-Id: I8ef124e9f5d86a990722019978c725c955ffb1ea
",git fetch https://review.opendev.org/openstack/nova refs/changes/67/121867/4 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/virt/libvirt/test_driver.py'],1,79486b160e714593d49cf13e72d2fa87bdf6bda2,libvirt-nodb-tests,class CacheConcurrencyTestCase(test.NoDBTestCase):,class CacheConcurrencyTestCase(test.TestCase):,1,1
openstack%2Fproject-config~master~I4d6e7ddf7d1c3b01680222fe7e6a6bd393f7304d,openstack/project-config,master,I4d6e7ddf7d1c3b01680222fe7e6a6bd393f7304d,Add project-config to infra integration test,MERGED,2014-09-25 21:06:29.000000000,2014-09-26 00:53:47.000000000,2014-09-26 00:53:46.000000000,"[{'_account_id': 3}, {'_account_id': 5263}]","[{'number': 1, 'created': '2014-09-25 21:06:29.000000000', 'files': ['zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/92dc8a93ad4f5fb3fda7e315dc0fceb890e60758', 'message': 'Add project-config to infra integration test\n\nChange-Id: I4d6e7ddf7d1c3b01680222fe7e6a6bd393f7304d\nDepends-On: I3a772e84a1bc6028caeabcc8e09a0942f44b8612\n'}]",0,124181,92dc8a93ad4f5fb3fda7e315dc0fceb890e60758,11,2,1,1,,,0,"Add project-config to infra integration test

Change-Id: I4d6e7ddf7d1c3b01680222fe7e6a6bd393f7304d
Depends-On: I3a772e84a1bc6028caeabcc8e09a0942f44b8612
",git fetch https://review.opendev.org/openstack/project-config refs/changes/81/124181/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul/layout.yaml'],1,92dc8a93ad4f5fb3fda7e315dc0fceb890e60758,(detached, - gate-infra-puppet-apply-precise - gate-infra-puppet-apply-centos6 - gate-infra-puppet-apply-precise - gate-infra-puppet-apply-centos6,,4,0
openstack%2Fneutron~stable%2Ficehouse~Ifacd3a384cfc797ba6d6af5f3c8649c333473259,openstack/neutron,stable/icehouse,Ifacd3a384cfc797ba6d6af5f3c8649c333473259,Big Switch: Retry on 503 errors from backend,MERGED,2014-09-13 23:27:41.000000000,2014-09-26 00:32:56.000000000,2014-09-19 02:43:39.000000000,"[{'_account_id': 3}, {'_account_id': 2592}, {'_account_id': 5170}, {'_account_id': 6659}, {'_account_id': 6854}, {'_account_id': 7787}, {'_account_id': 8213}, {'_account_id': 8645}, {'_account_id': 9656}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 12040}]","[{'number': 1, 'created': '2014-09-13 23:27:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3028654fd19e9ef0f4b9d952678a14b00fd71aac', 'message': 'Big Switch: Retry on 503 errors from backend\n\nRetries requests to the backend controller up to 3\nadditional times with 3 seconds in between each request\nif a 503 service unavailable message was returned.\nThe scenarios that return 503 messages from floodlight\nare normally short lived locks for things like OpenStack\nsynchronization or upgrade blocks. Retrying should work\nin the majority of cases.\n\nConflicts:\n\tneutron/tests/unit/bigswitch/test_servermanager.py\n\nCloses-Bug: #1357105\nChange-Id: Ifacd3a384cfc797ba6d6af5f3c8649c333473259\n(cherry picked from commit 48a2221648f490540fdf1ee099d39b8e5230f053)\n'}, {'number': 2, 'created': '2014-09-14 09:29:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3940e48479128665b6f673d5a1aeedc2bf3a23e8', 'message': 'Big Switch: Retry on 503 errors from backend\n\nRetries requests to the backend controller up to 3\nadditional times with 3 seconds in between each request\nif a 503 service unavailable message was returned.\nThe scenarios that return 503 messages from floodlight\nare normally short lived locks for things like OpenStack\nsynchronization or upgrade blocks. Retrying should work\nin the majority of cases.\n\nConflicts:\n\tneutron/tests/unit/bigswitch/test_servermanager.py\n\nCloses-Bug: #1357105\nChange-Id: Ifacd3a384cfc797ba6d6af5f3c8649c333473259\n(cherry picked from commit 48a2221648f490540fdf1ee099d39b8e5230f053)\n'}, {'number': 3, 'created': '2014-09-15 05:50:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9d40ad66ea58c3ad9130d522b2a34bf253108815', 'message': 'Big Switch: Retry on 503 errors from backend\n\nRetries requests to the backend controller up to 3\nadditional times with 3 seconds in between each request\nif a 503 service unavailable message was returned.\nThe scenarios that return 503 messages from floodlight\nare normally short lived locks for things like OpenStack\nsynchronization or upgrade blocks. Retrying should work\nin the majority of cases.\n\nConflicts:\n\tneutron/tests/unit/bigswitch/test_servermanager.py\n\nCloses-Bug: #1357105\nChange-Id: Ifacd3a384cfc797ba6d6af5f3c8649c333473259\n(cherry picked from commit 48a2221648f490540fdf1ee099d39b8e5230f053)\n'}, {'number': 4, 'created': '2014-09-16 02:33:08.000000000', 'files': ['neutron/tests/unit/bigswitch/test_servermanager.py', 'neutron/plugins/bigswitch/servermanager.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/b201432e02b959484f70a252a6cca94845712c57', 'message': 'Big Switch: Retry on 503 errors from backend\n\nRetries requests to the backend controller up to 3\nadditional times with 3 seconds in between each request\nif a 503 service unavailable message was returned.\nThe scenarios that return 503 messages from floodlight\nare normally short lived locks for things like OpenStack\nsynchronization or upgrade blocks. Retrying should work\nin the majority of cases.\n\nConflicts:\n\tneutron/tests/unit/bigswitch/test_servermanager.py\n\nCloses-Bug: #1357105\nChange-Id: Ifacd3a384cfc797ba6d6af5f3c8649c333473259\n(cherry picked from commit 48a2221648f490540fdf1ee099d39b8e5230f053)\n'}]",0,121391,b201432e02b959484f70a252a6cca94845712c57,152,21,4,7787,,,0,"Big Switch: Retry on 503 errors from backend

Retries requests to the backend controller up to 3
additional times with 3 seconds in between each request
if a 503 service unavailable message was returned.
The scenarios that return 503 messages from floodlight
are normally short lived locks for things like OpenStack
synchronization or upgrade blocks. Retrying should work
in the majority of cases.

Conflicts:
	neutron/tests/unit/bigswitch/test_servermanager.py

Closes-Bug: #1357105
Change-Id: Ifacd3a384cfc797ba6d6af5f3c8649c333473259
(cherry picked from commit 48a2221648f490540fdf1ee099d39b8e5230f053)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/91/121391/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/bigswitch/test_servermanager.py', 'neutron/plugins/bigswitch/servermanager.py']",2,3028654fd19e9ef0f4b9d952678a14b00fd71aac,,"import timeHTTP_SERVICE_UNAVAILABLE_RETRY_COUNT = 3 HTTP_SERVICE_UNAVAILABLE_RETRY_INTERVAL = 3 for x in range(HTTP_SERVICE_UNAVAILABLE_RETRY_COUNT + 1): ret = active_server.rest_call(action, resource, data, headers, timeout, reconnect=self.always_reconnect, hash_handler=hash_handler) if ret[0] != httplib.SERVICE_UNAVAILABLE: break time.sleep(HTTP_SERVICE_UNAVAILABLE_RETRY_INTERVAL) "," ret = active_server.rest_call(action, resource, data, headers, timeout, reconnect=self.always_reconnect, hash_handler=hash_handler)",32,4
openstack%2Foslo-incubator~master~I3f465cb7513ca31ea7c7db9c49c73c382ecd6301,openstack/oslo-incubator,master,I3f465cb7513ca31ea7c7db9c49c73c382ecd6301,Skip string templates during config generation,ABANDONED,2014-09-24 21:17:17.000000000,2014-09-25 23:52:56.000000000,,"[{'_account_id': 3}, {'_account_id': 2472}]","[{'number': 1, 'created': '2014-09-24 21:17:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/5237be4f0297f6a920f1f94dcd9cff31986baaaa', 'message': 'Skip string templates during config generation\n\noslo config looks up values at runtime, the config generator does\nnot support the template mechanism, so we should just skip the\ndefault values which are templated strings.\n\nCloses-Bug: #1373360\nChange-Id: I3f465cb7513ca31ea7c7db9c49c73c382ecd6301\n'}, {'number': 2, 'created': '2014-09-24 21:17:50.000000000', 'files': ['openstack/common/config/generator.py', 'tests/testmods/blaa_opt.py'], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/755a7dbd6dacc0dc232fe16164a8b6f148c27868', 'message': 'Skip string templates during config generation\n\noslo config looks up values at runtime, the config generator does\nnot support the template mechanism, so we should just skip the\ndefault values which are templated strings.\n\nCloses-Bug: #1373360\nChange-Id: I3f465cb7513ca31ea7c7db9c49c73c382ecd6301\n'}]",0,123855,755a7dbd6dacc0dc232fe16164a8b6f148c27868,6,2,2,5638,,,0,"Skip string templates during config generation

oslo config looks up values at runtime, the config generator does
not support the template mechanism, so we should just skip the
default values which are templated strings.

Closes-Bug: #1373360
Change-Id: I3f465cb7513ca31ea7c7db9c49c73c382ecd6301
",git fetch https://review.opendev.org/openstack/oslo-incubator refs/changes/55/123855/2 && git format-patch -1 --stdout FETCH_HEAD,"['openstack/common/config/generator.py', 'tests/testmods/blaa_opt.py']",2,5237be4f0297f6a920f1f94dcd9cff31986baaaa,bug/1373360," opts2 = [ cfg.BoolOpt('baz', default=True), cfg.BoolOpt('quux', default='$baz') ] CONF.register_opts(opts2)",,16,1
openstack%2Fnova~stable%2Ficehouse~I349e3b2221fff0ae217a71a91895afd21ff7d18d,openstack/nova,stable/icehouse,I349e3b2221fff0ae217a71a91895afd21ff7d18d,Fix attaching config drive issue on Hyper-V when migrate instances,MERGED,2014-05-30 03:52:12.000000000,2014-09-25 23:52:38.000000000,2014-09-25 23:52:35.000000000,"[{'_account_id': 3}, {'_account_id': 308}, {'_account_id': 1420}, {'_account_id': 3185}, {'_account_id': 5170}, {'_account_id': 6873}, {'_account_id': 8213}, {'_account_id': 8543}, {'_account_id': 8622}, {'_account_id': 9008}, {'_account_id': 9656}, {'_account_id': 10118}, {'_account_id': 12604}]","[{'number': 1, 'created': '2014-05-30 03:52:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f817e38558a11b262b0a23d1595ada6ab37734a4', 'message': ""Fix config drive issue on Hyper-V when migrate instances\n\nAfter instance resized or migrated on Hyper-V hypervisor.\nThe configdrive iso or vhd is copied to resized or migrated\ninstance, but is not attached to instance.\n\nBecause there are configurations for config drive like\nconfig_drive_cdrom, config_drive_format, and the configurations\non different Hyper-V compute node may be different. it will need\nto convert configdrive format after resized or migrated.\nIt is easy to convert from iso9660 or vfat to vhd, but it seems\nimpossible to convert from vhd to iso9660 or vfat.\nSo this commit just ingore the target Hyper-V compute node's\nconfig drive configurations, leave the original config drive format.\n\nChange-Id: I349e3b2221fff0ae217a71a91895afd21ff7d18d\nCloses-Bug: #1321640\n""}, {'number': 2, 'created': '2014-05-30 04:01:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b592fcad75d64996a51c9ed4baa2106b7495e948', 'message': ""Fix config drive issue on Hyper-V when migrate instances\n\nAfter instance resized or migrated on Hyper-V hypervisor.\nThe configdrive iso or vhd is copied to resized or migrated\ninstance, but is not attached to instance.\n\nBecause there are configurations for config drive like\nconfig_drive_cdrom, config_drive_format, and the configurations\non different Hyper-V compute node may be different. it will need\nto convert configdrive format after resized or migrated.\nIt is easy to convert from iso9660 or vfat to vhd, but it seems\nimpossible to convert from vhd to iso9660 or vfat.\nSo this commit just ingore the target Hyper-V compute node's\nconfig drive configurations, leave the original config drive format.\n\nCloses-Bug: #1321640\n\nChange-Id: I349e3b2221fff0ae217a71a91895afd21ff7d18d\n(cherry picked from commit 573a7b757fb63694780a7042cf0e451fdbe6f3a5)\n""}, {'number': 3, 'created': '2014-06-01 14:22:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/05fbc5aedf6c3cd08d25aadd1e19631db0fc2993', 'message': ""Fix config drive issue on Hyper-V when migrate instances\n\nAfter instance resized or migrated on Hyper-V hypervisor.\nThe configdrive iso or vhd is copied to resized or migrated\ninstance, but is not attached to instance.\n\nBecause there are configurations for config drive like\nconfig_drive_cdrom, config_drive_format, and the configurations\non different Hyper-V compute node may be different. it will need\nto convert configdrive format after resized or migrated.\nIt is easy to convert from iso9660 or vfat to vhd, but it seems\nimpossible to convert from vhd to iso9660 or vfat.\nSo this commit just ingore the target Hyper-V compute node's\nconfig drive configurations, leave the original config drive format.\n\nChange-Id: I349e3b2221fff0ae217a71a91895afd21ff7d18d\nCloses-Bug: #1321640\n(cherry picked from commit 573a7b757fb63694780a7042cf0e451fdbe6f3a5)\n""}, {'number': 4, 'created': '2014-09-10 14:40:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/4d3d942dabdb928100675f93332c0a0b46a77d82', 'message': ""Fix attaching config drive issue on Hyper-V when migrate instances\n\nAfter instance resized or migrated on Hyper-V hypervisor.\nThe configdrive iso or vhd is copied to resized or migrated\ninstance, but is not attached to instance.\n\nBecause there are configurations for config drive like\nconfig_drive_cdrom, config_drive_format, and the configurations\non different Hyper-V compute node may be different. it will need\nto convert configdrive format after resized or migrated.\nIt is easy to convert from iso9660 or vfat to vhd, but it seems\nimpossible to convert from vhd to iso9660 or vfat.\nSo this commit just ignore the target Hyper-V compute node's\nconfig drive configurations, leave the original config drive format.\n\nCloses-Bug: #1321640\n(cherry picked from commit 513c6bbd36563e57a85d33f9c94f4a20ab7c00f4)\n\nConflicts:\n\tnova/virt/hyperv/pathutils.py\n\nChange-Id: I349e3b2221fff0ae217a71a91895afd21ff7d18d\n""}, {'number': 5, 'created': '2014-09-24 09:10:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c01cd6ad2836304dc4d483a5c91ef1287a643050', 'message': ""Fix attaching config drive issue on Hyper-V when migrate instances\n\nAfter instance resized or migrated on Hyper-V hypervisor.\nThe configdrive iso or vhd is copied to resized or migrated\ninstance, but is not attached to instance.\n\nBecause there are configurations for config drive like\nconfig_drive_cdrom, config_drive_format, and the configurations\non different Hyper-V compute node may be different. it will need\nto convert configdrive format after resized or migrated.\nIt is easy to convert from iso9660 or vfat to vhd, but it seems\nimpossible to convert from vhd to iso9660 or vfat.\nSo this commit just ignore the target Hyper-V compute node's\nconfig drive configurations, leave the original config drive format.\n\nCloses-Bug: #1321640\n(cherry picked from commit 513c6bbd36563e57a85d33f9c94f4a20ab7c00f4)\n\nConflicts:\n\tnova/virt/hyperv/pathutils.py\n\nChange-Id: I349e3b2221fff0ae217a71a91895afd21ff7d18d\n""}, {'number': 6, 'created': '2014-09-24 11:43:20.000000000', 'files': ['nova/tests/virt/hyperv/test_migrationops.py', 'nova/virt/hyperv/pathutils.py', 'nova/tests/virt/hyperv/test_vmops.py', 'nova/virt/hyperv/vmops.py', 'nova/tests/virt/hyperv/fake.py', 'nova/tests/virt/hyperv/test_hypervapi.py', 'nova/tests/virt/hyperv/test_pathutils.py', 'nova/virt/hyperv/constants.py', 'nova/virt/hyperv/migrationops.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/7523ab4d1956c024647e8106de88e34b2d999d50', 'message': ""Fix attaching config drive issue on Hyper-V when migrate instances\n\nAfter instance resized or migrated on Hyper-V hypervisor.\nThe configdrive iso or vhd is copied to resized or migrated\ninstance, but is not attached to instance.\n\nBecause there are configurations for config drive like\nconfig_drive_cdrom, config_drive_format, and the configurations\non different Hyper-V compute node may be different. it will need\nto convert configdrive format after resized or migrated.\nIt is easy to convert from iso9660 or vfat to vhd, but it seems\nimpossible to convert from vhd to iso9660 or vfat.\nSo this commit just ignore the target Hyper-V compute node's\nconfig drive configurations, leave the original config drive format.\n\nCloses-Bug: #1321640\n(cherry picked from commit 513c6bbd36563e57a85d33f9c94f4a20ab7c00f4)\n\nConflicts:\n\tnova/virt/hyperv/pathutils.py\n\nChange-Id: I349e3b2221fff0ae217a71a91895afd21ff7d18d\n""}]",2,96660,7523ab4d1956c024647e8106de88e34b2d999d50,53,13,6,8622,,,0,"Fix attaching config drive issue on Hyper-V when migrate instances

After instance resized or migrated on Hyper-V hypervisor.
The configdrive iso or vhd is copied to resized or migrated
instance, but is not attached to instance.

Because there are configurations for config drive like
config_drive_cdrom, config_drive_format, and the configurations
on different Hyper-V compute node may be different. it will need
to convert configdrive format after resized or migrated.
It is easy to convert from iso9660 or vfat to vhd, but it seems
impossible to convert from vhd to iso9660 or vfat.
So this commit just ignore the target Hyper-V compute node's
config drive configurations, leave the original config drive format.

Closes-Bug: #1321640
(cherry picked from commit 513c6bbd36563e57a85d33f9c94f4a20ab7c00f4)

Conflicts:
	nova/virt/hyperv/pathutils.py

Change-Id: I349e3b2221fff0ae217a71a91895afd21ff7d18d
",git fetch https://review.opendev.org/openstack/nova refs/changes/60/96660/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/hyperv/test_migrationops.py', 'nova/virt/hyperv/pathutils.py', 'nova/tests/virt/hyperv/test_vmops.py', 'nova/virt/hyperv/vmops.py', 'nova/tests/virt/hyperv/fake.py', 'nova/tests/virt/hyperv/test_hypervapi.py', 'nova/tests/virt/hyperv/test_pathutils.py', 'nova/virt/hyperv/constants.py', 'nova/virt/hyperv/migrationops.py']",9,f817e38558a11b262b0a23d1595ada6ab37734a4,bug/1344036,"from nova.virt import configdrive def _check_and_attach_config_drive(self, instance): if configdrive.required_by(instance): configdrive_path = self._pathutils.lookup_configdrive_path( instance.name) if configdrive_path: self._vmops.attach_config_drive(instance, configdrive_path) else: raise vmutils.HyperVException( _(""Config drive is required by instance: %s, "" ""but it does not exist."") % instance.name) self._check_and_attach_config_drive(instance) self._check_and_attach_config_drive(instance) ",,258,8
openstack%2Foslo-incubator~master~Id07ef1773e2baf2b4ee673a0f0a7454b8969de57,openstack/oslo-incubator,master,Id07ef1773e2baf2b4ee673a0f0a7454b8969de57,Add a script for checking out all OpenStack repos,MERGED,2014-09-12 20:00:17.000000000,2014-09-25 23:51:25.000000000,2014-09-25 23:51:25.000000000,"[{'_account_id': 3}, {'_account_id': 1669}, {'_account_id': 1849}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-09-12 20:00:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/10ecb62eab76806a4765b9c250f33a533a7050c5', 'message': 'Add a script for checking out all OpenStack repos\n\nAdd a tool script for checking out a local copy of all of the OpenStack\nrepositories.\n\nChange-Id: Id07ef1773e2baf2b4ee673a0f0a7454b8969de57\n'}, {'number': 2, 'created': '2014-09-12 20:57:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/a645d30c3121648f9dbcf7325df76a1dc9f339d3', 'message': 'Add a script for checking out all OpenStack repos\n\nAdd a tool script for checking out a local copy of all of the OpenStack\nrepositories.\n\nChange-Id: Id07ef1773e2baf2b4ee673a0f0a7454b8969de57\n'}, {'number': 3, 'created': '2014-09-25 19:37:16.000000000', 'files': ['tools/clone_openstack.sh'], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/b028663599d87ebff800c9f74e5488fcae48e034', 'message': 'Add a script for checking out all OpenStack repos\n\nAdd a tool script for checking out a local copy of all of the OpenStack\nrepositories.\n\nChange-Id: Id07ef1773e2baf2b4ee673a0f0a7454b8969de57\n'}]",7,121214,b028663599d87ebff800c9f74e5488fcae48e034,20,6,3,2472,,,0,"Add a script for checking out all OpenStack repos

Add a tool script for checking out a local copy of all of the OpenStack
repositories.

Change-Id: Id07ef1773e2baf2b4ee673a0f0a7454b8969de57
",git fetch https://review.opendev.org/openstack/oslo-incubator refs/changes/14/121214/2 && git format-patch -1 --stdout FETCH_HEAD,['tools/clone_openstack.sh'],1,10ecb62eab76806a4765b9c250f33a533a7050c5,release-preview,"#!/bin/bash # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # # Check out every active repository from git.openstack.org. For new # copies, set up git-review. For any existing copies, update their # remotes and pull changes up to the local master. # # This script is based on prior art from mordred on the openstack-dev # mailing list. # http://lists.openstack.org/pipermail/openstack-dev/2013-October/017532.html # # Usage: # # Check out everything under the current directory: # $ clone_openstack.sh # # Check out a specific project (you can list multiple names): # $ clone_openstack.sh openstack/oslo-incubator # trouble_with="""" branched="""" # Figure out if git-hooks is installed and should be used. # https://github.com/icefox/git-hooks which git-hooks 2>&1 > /dev/null USE_GIT_HOOKS=$? # If we have any trouble at all working with a repository, report that # and then record the name for the summary at the end. function track_trouble { if [ $1 -ne 0 ] then echo ""Remembering trouble with $2"" trouble_with=""$trouble_with $2"" fi } # Determine the current branch of a local repository. function current_branch { (cd $1 && git branch | grep '^*' | cut -f2 -d' ') } # Print a summary report for any repositories that had trouble # updating. function report_trouble { if [ ! -z ""$trouble_with"" ] then echo echo ""Had trouble updating:"" for r in $trouble_with do echo "" $r - $(current_branch $r)"" done fi } # Print a summary report for any repositories that were not on the # master branch when we updated them. function report_branched { if [ ! -z ""$branched"" ] then echo echo ""Branched repos:"" for r in $branched do echo "" $r - $(current_branch $r)"" done fi } # Check out a new copy of a repository and set it up to be a useful # local copy. function clone_new { typeset repo=""$1"" typeset url=""$2"" echo ""Cloning $repo"" git clone $url $repo (cd $repo && git review -s) if [ $USE_GIT_HOOKS -eq 0 ] then echo ""Configuring git hooks"" (cd $repo && git hooks --install) fi } # Update an existing copy of a repository, including all remotes and # pulling into the local master branch if we're on that branch # already. function update_existing { typeset repo=""$1"" echo ""Updating $repo"" (cd $repo && git remote update) RC=$? if [ $RC -ne 0 ] then return $RC fi # Only run git pull for repos where I'm not working in a branch. typeset b=$(current_branch $repo) if [ $b == ""master"" ] then (cd $repo && git pull) else echo ""Skipping pull for branch $b"" branched=""$branched $repo"" fi } # Process a single repository found in gerrit, determining whether it # exists locally already or not. function get_one_repo { typeset repo=""$1"" typeset url=""$2"" typeset pardir=$(dirname $repo) if [ ! -z ""$pardir"" ] then mkdir -p $pardir fi if [ ! -d $repo ] ; then clone_new $repo $url else update_existing $repo fi RC=$? return $RC } # If we are given a list of projects on the command line, we will only # work on those. Otherwise, ask gerrit for the full list of openstack # projects, ignoring the ones in the attic. projects=""$*"" if [ -z ""$projects"" ] then projects=$(ssh review.openstack.org gerrit ls-projects | grep -v 'attic') fi for repo in $projects; do get_one_repo $repo git://git.openstack.org/$repo track_trouble $? $repo echo done report_branched report_trouble ",,156,0
openstack%2Frally~master~Iab56650edc32609d2887550ea26c4e5137ab9e7e,openstack/rally,master,Iab56650edc32609d2887550ea26c4e5137ab9e7e,Stop using intersphinx,MERGED,2014-09-25 08:48:52.000000000,2014-09-25 23:41:49.000000000,2014-09-25 23:41:48.000000000,"[{'_account_id': 3}, {'_account_id': 6172}, {'_account_id': 7369}, {'_account_id': 8507}]","[{'number': 1, 'created': '2014-09-25 08:48:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/9bcbebe3854a3c203a0de51f5adbb1370cdb63dd', 'message': ""Stop using intersphinx\n\nRemove intersphinx from the docs build as it triggers network calls that\noccasionally fail, and we don't really use intersphinx (links other\nsphinx documents out on the internet)\n\nThis also removes the requirement for internet access during docs build.\n\nThis can cause docs jobs to fail if the project errors out on\nwarnings.\n\nChange-Id: Iab56650edc32609d2887550ea26c4e5137ab9e7e\nRelated-Bug: #1368910\n""}, {'number': 2, 'created': '2014-09-25 18:09:22.000000000', 'files': ['doc/source/conf.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/cb5c679cc6a8cc32382a6c4cdfee318abd02a823', 'message': ""Stop using intersphinx\n\nRemove intersphinx from the docs build as it triggers network calls that\noccasionally fail, and we don't really use intersphinx (links other\nsphinx documents out on the internet)\n\nThis also removes the requirement for internet access during docs build.\n\nThis can cause docs jobs to fail if the project errors out on\nwarnings.\n\nChange-Id: Iab56650edc32609d2887550ea26c4e5137ab9e7e\nRelated-Bug: #1368910\n""}]",0,123979,cb5c679cc6a8cc32382a6c4cdfee318abd02a823,16,4,2,167,,,0,"Stop using intersphinx

Remove intersphinx from the docs build as it triggers network calls that
occasionally fail, and we don't really use intersphinx (links other
sphinx documents out on the internet)

This also removes the requirement for internet access during docs build.

This can cause docs jobs to fail if the project errors out on
warnings.

Change-Id: Iab56650edc32609d2887550ea26c4e5137ab9e7e
Related-Bug: #1368910
",git fetch https://review.opendev.org/openstack/rally refs/changes/79/123979/2 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/conf.py'],1,9bcbebe3854a3c203a0de51f5adbb1370cdb63dd,bug/1368910,," 'sphinx.ext.intersphinx', # Example configuration for intersphinx: refer to the Python standard library. intersphinx_mapping = {'http://docs.python.org/': None}",0,5
openstack%2Fbarbican~master~I7e5f8e10b41f2236800276fa0d0a1793dff876d9,openstack/barbican,master,I7e5f8e10b41f2236800276fa0d0a1793dff876d9,Enabling the version discovery test,ABANDONED,2014-09-25 04:44:03.000000000,2014-09-25 23:40:25.000000000,,"[{'_account_id': 3}, {'_account_id': 7789}]","[{'number': 1, 'created': '2014-09-25 04:44:03.000000000', 'files': ['functionaltests/api/test_versions.py'], 'web_link': 'https://opendev.org/openstack/barbican/commit/3e6a3bb8ab8623af7b543822fa6ffcf3d17ea08d', 'message': 'Enabling the version discovery test\n\nI was able to find a workaround that allows for us to renable\nthis test before we deal with the fix-version-api blueprint.\n\nChange-Id: I7e5f8e10b41f2236800276fa0d0a1793dff876d9\n'}]",0,123931,3e6a3bb8ab8623af7b543822fa6ffcf3d17ea08d,4,2,1,7262,,,0,"Enabling the version discovery test

I was able to find a workaround that allows for us to renable
this test before we deal with the fix-version-api blueprint.

Change-Id: I7e5f8e10b41f2236800276fa0d0a1793dff876d9
",git fetch https://review.opendev.org/openstack/barbican refs/changes/31/123931/1 && git format-patch -1 --stdout FETCH_HEAD,['functionaltests/api/test_versions.py'],1,3e6a3bb8ab8623af7b543822fa6ffcf3d17ea08d,enabling_version_test," def setUp(self): super(VersionDiscoveryTestCase, self).setUp() # This prevents /v1 to be added to the url self.client.api_version = '' resp, body = self.client.get('')","import testtools @testtools.skipIf(True, 'Skip until blueprint fix-version-api is complete') resp, body = self.client.get(' ')",7,4
openstack%2Fkeystone~stable%2Ficehouse~I1b8af6ce5709f829f345cd351ec9242d0217e743,openstack/keystone,stable/icehouse,I1b8af6ce5709f829f345cd351ec9242d0217e743,Remove `with_lockmode` use from Trust SQL backend.,MERGED,2014-08-08 15:14:35.000000000,2014-09-25 23:35:51.000000000,2014-09-25 23:35:50.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 1313}, {'_account_id': 1420}, {'_account_id': 1955}, {'_account_id': 2903}, {'_account_id': 5046}, {'_account_id': 8978}, {'_account_id': 9656}]","[{'number': 1, 'created': '2014-08-08 15:14:35.000000000', 'files': ['keystone/trust/backends/sql.py', 'keystone/exception.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/8485dbc4d6d15b8e30c82363695b2a4e13512de9', 'message': ""Remove `with_lockmode` use from Trust SQL backend.\n\nDue to the lack of support of `SELECT .. FOR UPDATE` call in\nMySQL + Galera, the use of `with_lockmode('update')` needs to be\nremoved from the Trust SQL backend when utilizing the MySQL\ndialect.\n\nInstead of the pessimistic locking method (row lock, table lock),\nuse optimistic locking. The consume_use method now attempts to\nupdate the remaining_uses column for the trust in question only\nif the remaining_uses equal the same number as on the initial\nquery. This is done in a loop with a 10-iteration failsafe to\nprevent endless looping.\n\nConflicts:\n\tkeystone/trust/backends/sql.py\n\nMinor changes to the sql.py driver over the course of Juno\ncaused the conflict. The resulting code for consume has\nnot changed.\n\nChange-Id: I1b8af6ce5709f829f345cd351ec9242d0217e743\nCloses-Bug: #1325143\n(cherry-picked from commit 0724fc4d27c90461de3310b3b2cb7387e6c92bba)\n""}]",0,112920,8485dbc4d6d15b8e30c82363695b2a4e13512de9,18,9,1,2903,,,0,"Remove `with_lockmode` use from Trust SQL backend.

Due to the lack of support of `SELECT .. FOR UPDATE` call in
MySQL + Galera, the use of `with_lockmode('update')` needs to be
removed from the Trust SQL backend when utilizing the MySQL
dialect.

Instead of the pessimistic locking method (row lock, table lock),
use optimistic locking. The consume_use method now attempts to
update the remaining_uses column for the trust in question only
if the remaining_uses equal the same number as on the initial
query. This is done in a loop with a 10-iteration failsafe to
prevent endless looping.

Conflicts:
	keystone/trust/backends/sql.py

Minor changes to the sql.py driver over the course of Juno
caused the conflict. The resulting code for consume has
not changed.

Change-Id: I1b8af6ce5709f829f345cd351ec9242d0217e743
Closes-Bug: #1325143
(cherry-picked from commit 0724fc4d27c90461de3310b3b2cb7387e6c92bba)
",git fetch https://review.opendev.org/openstack/keystone refs/changes/20/112920/1 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/exception.py', 'keystone/trust/backends/sql.py']",2,8485dbc4d6d15b8e30c82363695b2a4e13512de9,bug/1325143,"import time from keystone.openstack.common import logLOG = log.getLogger(__name__) # The maximum number of iterations that will be attempted for optimistic # locking on consuming a limited-use trust. MAXIMUM_CONSUME_ATTEMPTS = 10 for attempt in range(MAXIMUM_CONSUME_ATTEMPTS): with sql.transaction() as session: try: query_result = (session.query(TrustModel.remaining_uses). filter_by(id=trust_id). filter_by(deleted_at=None).one()) except sql.NotFound: raise exception.TrustNotFound(trust_id=trust_id) remaining_uses = query_result.remaining_uses if remaining_uses is None: # unlimited uses, do nothing break elif remaining_uses > 0: # NOTE(morganfainberg): use an optimistic locking method # to ensure we only ever update a trust that has the # expected number of remaining uses. rows_affected = ( session.query(TrustModel). filter_by(id=trust_id). filter_by(deleted_at=None). filter_by(remaining_uses=remaining_uses). update({'remaining_uses': (remaining_uses - 1)}, synchronize_session=False)) if rows_affected == 1: # Successfully consumed a single limited-use trust. # Since trust_id is the PK on the Trust table, there is # no case we should match more than 1 row in the # update. We either update 1 row or 0 rows. break else: raise exception.TrustUseLimitReached(trust_id=trust_id) # NOTE(morganfainberg): Ensure we have a yield point for eventlet # here. This should cost us nothing otherwise. This can be removed # if/when oslo.db cleanly handles yields on db calls. time.sleep(0) else: # NOTE(morganfainberg): In the case the for loop is not prematurely # broken out of, this else block is executed. This means the trust # was not unlimited nor was it consumed (we hit the maximum # iteration limit). This is just an indicator that we were unable # to get the optimistic lock rather than silently failing or # incorrectly indicating a trust was consumed. raise exception.TrustConsumeMaximumAttempt(trust_id=trust_id)"," session = sql.get_session() with session.begin(): ref = (session.query(TrustModel). with_lockmode('update'). filter_by(deleted_at=None). filter_by(id=trust_id).first()) if ref is None: raise exception.TrustNotFound(trust_id=trust_id) if ref.remaining_uses is None: # unlimited uses, do nothing pass elif ref.remaining_uses > 0: ref.remaining_uses -= 1 else: raise exception.TrustUseLimitReached(trust_id=trust_id)",59,15
openstack%2Fcongress-specs~master~I71e941e2a639641a662a163c682eb86d51de42fb,openstack/congress-specs,master,I71e941e2a639641a662a163c682eb86d51de42fb,Stop using intersphinx,MERGED,2014-09-13 07:52:48.000000000,2014-09-25 23:35:21.000000000,2014-09-25 23:35:21.000000000,"[{'_account_id': 3}, {'_account_id': 4395}]","[{'number': 1, 'created': '2014-09-13 07:52:48.000000000', 'files': ['doc/source/conf.py'], 'web_link': 'https://opendev.org/openstack/congress-specs/commit/de4c7ef718b96bff5958f559cb9e866c65582a5f', 'message': ""Stop using intersphinx\n\nRemove intersphinx from the docs build as it triggers network calls that\noccasionally fail, and we don't really use intersphinx (links other\nsphinx documents out on the internet)\n\nThis also removes the requirement for internet access during docs build.\n\nThis can cause docs jobs to fail if the project errors out on\nwarnings.\n\nChange-Id: I71e941e2a639641a662a163c682eb86d51de42fb\nRelated-Bug: #1368910\n""}]",0,121323,de4c7ef718b96bff5958f559cb9e866c65582a5f,6,2,1,6547,,,0,"Stop using intersphinx

Remove intersphinx from the docs build as it triggers network calls that
occasionally fail, and we don't really use intersphinx (links other
sphinx documents out on the internet)

This also removes the requirement for internet access during docs build.

This can cause docs jobs to fail if the project errors out on
warnings.

Change-Id: I71e941e2a639641a662a163c682eb86d51de42fb
Related-Bug: #1368910
",git fetch https://review.opendev.org/openstack/congress-specs refs/changes/23/121323/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/conf.py'],1,de4c7ef718b96bff5958f559cb9e866c65582a5f,bug/1368910,," 'sphinx.ext.intersphinx',",0,1
openstack%2Fswift~feature%2Fec~I2a3145fdf898462120d4ca555f3b5b33c357d5ba,openstack/swift,feature/ec,I2a3145fdf898462120d4ca555f3b5b33c357d5ba,Make ec_object_segment_size configurable per EC policy,ABANDONED,2014-08-12 04:06:43.000000000,2014-09-25 23:31:28.000000000,,"[{'_account_id': 3}, {'_account_id': 330}, {'_account_id': 2622}, {'_account_id': 5189}, {'_account_id': 7233}, {'_account_id': 7479}, {'_account_id': 7485}, {'_account_id': 7847}]","[{'number': 1, 'created': '2014-08-12 04:06:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/eb48379aa2b2cb848381847afe9a3eb7cc3986bd', 'message': 'Make ec_object_segment_size configurable per EC policy\n\nUsing one segment size in different EC ratio will resulting\nextra padding contents, e.g, using 1048576 in 8+2 is perfect while\nin 10+4 will have extra padding bytes in each segment.\nThis patch makes this parameter configurable in each EC policy. If\nit is not configured then will fall back to default 1048576\n\nChange-Id: I2a3145fdf898462120d4ca555f3b5b33c357d5ba\n'}, {'number': 2, 'created': '2014-08-15 08:54:50.000000000', 'files': ['test/unit/common/test_storage_policy.py', 'test/unit/proxy/test_server.py', 'swift/common/storage_policy.py', 'swift/common/constraints.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/dd62a3d209c832ab22c53c74c3a837c1f0cdcb2a', 'message': 'Make ec_object_segment_size configurable per EC policy\n\nUsing one segment size in different EC ratio will result in\nextra padding per segment, e.g, using 1048576 in 8+2 is perfect\nwhile in 10+4 will have extra padding bytes in each segment.\nThis patch makes this parameter configurable in each EC policy. If\nit is not configured then will fall back to default 1048576\n\nChange-Id: I2a3145fdf898462120d4ca555f3b5b33c357d5ba\n'}]",6,113434,dd62a3d209c832ab22c53c74c3a837c1f0cdcb2a,35,8,2,5189,,,0,"Make ec_object_segment_size configurable per EC policy

Using one segment size in different EC ratio will result in
extra padding per segment, e.g, using 1048576 in 8+2 is perfect
while in 10+4 will have extra padding bytes in each segment.
This patch makes this parameter configurable in each EC policy. If
it is not configured then will fall back to default 1048576

Change-Id: I2a3145fdf898462120d4ca555f3b5b33c357d5ba
",git fetch https://review.opendev.org/openstack/swift refs/changes/34/113434/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/common/test_storage_policy.py', 'test/unit/proxy/test_server.py', 'swift/common/storage_policy.py', 'swift/common/constraints.py']",4,eb48379aa2b2cb848381847afe9a3eb7cc3986bd,ec_segment_size,"EC_OBJECT_SEGMENT_SIZE = 1048576 'ec_object_segment_size': EC_OBJECT_SEGMENT_SIZE,",,85,5
openstack%2Fproject-config~master~I78322202235ce6a1dd6487c5debde1ad1dd546ae,openstack/project-config,master,I78322202235ce6a1dd6487c5debde1ad1dd546ae,Add notify_mpact to dev gerrit,MERGED,2014-09-25 22:50:35.000000000,2014-09-25 23:29:28.000000000,2014-09-25 23:29:27.000000000,"[{'_account_id': 3}, {'_account_id': 4146}]","[{'number': 1, 'created': '2014-09-25 22:50:35.000000000', 'files': ['dev/gerrit/notify_impact.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/b4194cae382060509f1b4195d0590c81fcb56552', 'message': 'Add notify_mpact to dev gerrit\n\nThe status quo is that the files are the same.  They probably\nshould not be, but we can change that in a later change.\n\nChange-Id: I78322202235ce6a1dd6487c5debde1ad1dd546ae\n'}]",0,124214,b4194cae382060509f1b4195d0590c81fcb56552,6,2,1,1,,,0,"Add notify_mpact to dev gerrit

The status quo is that the files are the same.  They probably
should not be, but we can change that in a later change.

Change-Id: I78322202235ce6a1dd6487c5debde1ad1dd546ae
",git fetch https://review.opendev.org/openstack/project-config refs/changes/14/124214/1 && git format-patch -1 --stdout FETCH_HEAD,['dev/gerrit/notify_impact.yaml'],1,b4194cae382060509f1b4195d0590c81fcb56552,url,"# This file is used by the notify_impact jeepyb plugin to control the bugs it # create. # # Please keep these structures in alphabetical order. # For automatically created DocImpact bugs, there are two structures here. The # author_map maps email addresses of authors to ""groups"". These groups can be # anything you want, although the most likely example is a dev team at a # particular company. The second structure is ""subscriber_map"", which lists # the people to subscribe to a bug owned by a particular group. author_map: joshua.hesketh@rackspace.com: rcbau lana.brindley@rackspace.com: rcbau matt@oliver.net.au: rcbau matthew.oliver@rackspace.com: rcbau michael.davies@rackspace.com: rcbau michael.still@rackspace.com: rcbau michael@the-davies.net: rcbau mikal@stillhq.com: rcbau openstack@lanabrindley.com: rcbau subscriber_map: rcbau: ['loquacity', 'mikalstill'] ",,24,0
openstack%2Fproject-config~master~I9adce15a0a651b7cda0cef8ec7efec332774826b,openstack/project-config,master,I9adce15a0a651b7cda0cef8ec7efec332774826b,Update URL for projects.yaml,MERGED,2014-09-25 20:13:09.000000000,2014-09-25 23:24:23.000000000,2014-09-25 23:24:22.000000000,"[{'_account_id': 1}, {'_account_id': 3}, {'_account_id': 5263}, {'_account_id': 6316}]","[{'number': 1, 'created': '2014-09-25 20:13:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/9635a89211f786b139e554c44190f142a7be1211', 'message': 'Update URL for projects.yaml\n\nUpdate the URL of projects.yaml used by the git repo cache scripts\nin nodepool image builds.\n\nChange-Id: I9adce15a0a651b7cda0cef8ec7efec332774826b\n'}, {'number': 2, 'created': '2014-09-25 22:45:10.000000000', 'files': ['nodepool/scripts/cache_git_repos.py', 'nodepool/elements/openstack-repos/extra-data.d/50-create-repo-list'], 'web_link': 'https://opendev.org/openstack/project-config/commit/5904df1e86d95f8b4bfe9bf138c307cebcebceef', 'message': 'Update URL for projects.yaml\n\nUpdate the URL of projects.yaml used by the git repo cache scripts\nin nodepool image builds.\n\nChange-Id: I9adce15a0a651b7cda0cef8ec7efec332774826b\n'}]",2,124173,5904df1e86d95f8b4bfe9bf138c307cebcebceef,11,4,2,1,,,0,"Update URL for projects.yaml

Update the URL of projects.yaml used by the git repo cache scripts
in nodepool image builds.

Change-Id: I9adce15a0a651b7cda0cef8ec7efec332774826b
",git fetch https://review.opendev.org/openstack/project-config refs/changes/73/124173/1 && git format-patch -1 --stdout FETCH_HEAD,"['nodepool/scripts/cache_git_repos.py', 'nodepool/elements/openstack-repos/extra-data.d/50-create-repo-list']",2,9635a89211f786b139e554c44190f142a7be1211,url,URL = ('https://git.openstack.org/cgit/openstack-infra/project-config/' 'plain/gerrit/projects.yaml'),URL = ('https://git.openstack.org/cgit/openstack-infra/config/plain/' 'modules/openstack_project/files/review.projects.yaml'),4,4
openstack%2Fproject-config~master~Ic8e6dc852241cac9a0f0cb6509d0d0e51304fb23,openstack/project-config,master,Ic8e6dc852241cac9a0f0cb6509d0d0e51304fb23,"Add license, readme, and contributing files",MERGED,2014-09-25 21:59:12.000000000,2014-09-25 23:09:44.000000000,2014-09-25 23:09:44.000000000,"[{'_account_id': 1}, {'_account_id': 3}, {'_account_id': 6316}]","[{'number': 1, 'created': '2014-09-25 21:59:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/84df1c0e74706c62dce473e7768bd8867446952e', 'message': 'Add license, readme, and contributing files\n\nChange-Id: Ic8e6dc852241cac9a0f0cb6509d0d0e51304fb23\n'}, {'number': 2, 'created': '2014-09-25 22:21:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/2f8faa055fa8a3836ce95a1c9b685d2b69727373', 'message': 'Add license, readme, and contributing files\n\nChange-Id: Ic8e6dc852241cac9a0f0cb6509d0d0e51304fb23\n'}, {'number': 3, 'created': '2014-09-25 22:30:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/00f5e15a09038ac7159ed5e24e531f90f09abe52', 'message': 'Add license, readme, and contributing files\n\nChange-Id: Ic8e6dc852241cac9a0f0cb6509d0d0e51304fb23\n'}, {'number': 4, 'created': '2014-09-25 22:32:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/d86d2053a82eccbb576b8b7483a2ab93dd7af346', 'message': 'Add license, readme, and contributing files\n\nChange-Id: Ic8e6dc852241cac9a0f0cb6509d0d0e51304fb23\n'}, {'number': 5, 'created': '2014-09-25 22:34:51.000000000', 'files': ['CONTRIBUTING.rst', 'README.rst', 'LICENSE'], 'web_link': 'https://opendev.org/openstack/project-config/commit/2befddda033ada397920235e78e7c533716b79e2', 'message': 'Add license, readme, and contributing files\n\nChange-Id: Ic8e6dc852241cac9a0f0cb6509d0d0e51304fb23\n'}]",5,124200,2befddda033ada397920235e78e7c533716b79e2,17,3,5,4146,,,0,"Add license, readme, and contributing files

Change-Id: Ic8e6dc852241cac9a0f0cb6509d0d0e51304fb23
",git fetch https://review.opendev.org/openstack/project-config refs/changes/00/124200/3 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'LICENSE', 'README.md']",3,84df1c0e74706c62dce473e7768bd8867446952e,supporting-files,OpenStack Infra Config Files ============================ This repo contains a set of config files that are consumed by the openstack-infra/config puppet modules in order to deploy and config the OpenStack Infrastructure. You should edit these files to make configuration changes to the OpenStack Infrastructure. See http://ci.openstack.org for more information. ,,228,0
openstack%2Fmagnetodb~master~I31eafeabb4330fb9f1c7f146f51401e3c360b79b,openstack/magnetodb,master,I31eafeabb4330fb9f1c7f146f51401e3c360b79b,Improve json schema validation for requests left,MERGED,2014-09-18 15:28:16.000000000,2014-09-25 23:02:40.000000000,2014-09-25 23:02:39.000000000,"[{'_account_id': 3}, {'_account_id': 8188}, {'_account_id': 8491}, {'_account_id': 8601}, {'_account_id': 8863}, {'_account_id': 11006}]","[{'number': 1, 'created': '2014-09-18 15:28:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/c92c106a5d4f6921988cb26dd49bd435d3783697', 'message': 'Improve json schema validation for left requests\n\nChange-Id: I31eafeabb4330fb9f1c7f146f51401e3c360b79b\nImplements: bp implement-table-schema-based-validation\n'}, {'number': 2, 'created': '2014-09-18 15:36:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/3fd64c0793ed1703e53d4c875b0b1e16246bba43', 'message': 'Improve json schema validation for other requests\n\nChange-Id: I31eafeabb4330fb9f1c7f146f51401e3c360b79b\nImplements: bp implement-table-schema-based-validation\n'}, {'number': 3, 'created': '2014-09-19 08:26:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/c8be66f32af57b2d2f14202daa758719c6dd6d84', 'message': 'Improve json schema validation for left requests\n\nImplements: bp implement-table-schema-based-validation\nChange-Id: I31eafeabb4330fb9f1c7f146f51401e3c360b79b\n'}, {'number': 4, 'created': '2014-09-19 09:36:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/b9e9489daeec47f72a908ab372603cf561acfd90', 'message': 'Improve json schema validation for requests left\n\nImplements: bp implement-table-schema-based-validation\nChange-Id: I31eafeabb4330fb9f1c7f146f51401e3c360b79b\n'}, {'number': 5, 'created': '2014-09-19 15:24:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/499f57acad724972e1c15902c84292e184e6240d', 'message': 'Improve json schema validation for requests left\n\nImplements: bp implement-table-schema-based-validation\nChange-Id: I31eafeabb4330fb9f1c7f146f51401e3c360b79b\n'}, {'number': 6, 'created': '2014-09-22 11:18:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/a0ae634985111e7706dba8cf50544e39bcd10761', 'message': 'Improve json schema validation for requests left\n\nImplements: bp implement-table-schema-based-validation\nChange-Id: I31eafeabb4330fb9f1c7f146f51401e3c360b79b\n'}, {'number': 7, 'created': '2014-09-22 13:59:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/5072ff3bb496925b8469e0d79b26ad31d0e10405', 'message': 'Improve json schema validation for requests left\n\nImplements: bp implement-table-schema-based-validation\nChange-Id: I31eafeabb4330fb9f1c7f146f51401e3c360b79b\n'}, {'number': 8, 'created': '2014-09-22 14:58:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/a436124e5bf62048b7d088604b9c0254b0a739da', 'message': 'Improve json schema validation for requests left\n\nImplements: bp implement-table-schema-based-validation\nChange-Id: I31eafeabb4330fb9f1c7f146f51401e3c360b79b\n'}, {'number': 9, 'created': '2014-09-22 18:00:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/4dd66e0818a8b07f23211a6e42ccc6221cdb35ee', 'message': 'Improve json schema validation for requests left\n\nImplements: bp implement-table-schema-based-validation\nChange-Id: I31eafeabb4330fb9f1c7f146f51401e3c360b79b\n'}, {'number': 10, 'created': '2014-09-23 10:05:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/0a1af1f5ff93b64552b21b5c57ae5ff9708deb89', 'message': 'Improve json schema validation for requests left\n\nImplements: bp implement-table-schema-based-validation\nCloses-bug: #1372864 \nChange-Id: I31eafeabb4330fb9f1c7f146f51401e3c360b79b\n'}, {'number': 11, 'created': '2014-09-24 13:41:30.000000000', 'files': ['magnetodb/api/validation.py', 'magnetodb/api/openstack/v1/delete_table.py', 'magnetodb/tests/storage/test_cassandra_impl.py', 'magnetodb/api/openstack/v1/update_item.py', 'magnetodb/storage/models.py', 'magnetodb/api/openstack/v1/delete_item.py', 'magnetodb/tests/unittests/api/openstack/v1/test_delete_item.py', 'magnetodb/api/openstack/v1/scan.py', 'magnetodb/tests/unittests/storage/manager/test_simple_storage_manager.py', 'magnetodb/tests/unittests/storage/manager/test_notify_storage_manager.py', 'magnetodb/api/openstack/v1/query.py', 'magnetodb/storage/driver/cassandra/cassandra_impl.py', 'magnetodb/api/openstack/v1/parser.py', 'magnetodb/api/openstack/v1/describe_table.py', 'magnetodb/api/openstack/v1/list_tables.py', 'magnetodb/api/openstack/v1/validation.py', 'magnetodb/storage/manager/simple_impl.py', 'magnetodb/api/openstack/v1/put_item.py'], 'web_link': 'https://opendev.org/openstack/magnetodb/commit/f2eb0d5f1e623ae9fd87147bfda03f66d4465e58', 'message': 'Improve json schema validation for requests left\n\nImplements: bp implement-table-schema-based-validation\nChange-Id: I31eafeabb4330fb9f1c7f146f51401e3c360b79b\n'}]",2,122445,f2eb0d5f1e623ae9fd87147bfda03f66d4465e58,36,6,11,8601,,,0,"Improve json schema validation for requests left

Implements: bp implement-table-schema-based-validation
Change-Id: I31eafeabb4330fb9f1c7f146f51401e3c360b79b
",git fetch https://review.opendev.org/openstack/magnetodb refs/changes/45/122445/7 && git format-patch -1 --stdout FETCH_HEAD,"['magnetodb/api/validation.py', 'magnetodb/api/openstack/v1/delete_table.py', 'magnetodb/tests/storage/test_cassandra_impl.py', 'magnetodb/api/openstack/v1/update_item.py', 'magnetodb/storage/models.py', 'magnetodb/api/openstack/v1/delete_item.py', 'magnetodb/tests/unittests/api/openstack/v1/test_delete_item.py', 'magnetodb/api/openstack/v1/scan.py', 'magnetodb/api/openstack/v1/query.py', 'magnetodb/api/openstack/v1/parser.py', 'magnetodb/api/openstack/v1/describe_table.py', 'magnetodb/api/openstack/v1/list_tables.py', 'magnetodb/api/openstack/v1/put_item.py']",13,c92c106a5d4f6921988cb26dd49bd435d3783697,bp/implement-table-schema-based-validation," validation.validate_integer( time_to_live, parser.Props.TIME_TO_LIVE, min_val=0 )"," validation.validate_integer(time_to_live, parser.Props.TIME_TO_LIVE)",240,373
openstack%2Fpuppet-neutron~master~I7dc59ce9580bfb1d4afdfbced668d0cb2979458a,openstack/puppet-neutron,master,I7dc59ce9580bfb1d4afdfbced668d0cb2979458a,Hide secrets from puppet logs,MERGED,2014-07-12 00:33:25.000000000,2014-09-25 23:00:43.000000000,2014-08-11 17:11:41.000000000,"[{'_account_id': 3}, {'_account_id': 3153}, {'_account_id': 7156}, {'_account_id': 7822}, {'_account_id': 8482}]","[{'number': 1, 'created': '2014-07-12 00:33:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-neutron/commit/aaf9726c2f0e6a86ed034ca613147b3daa9620bf', 'message': 'Hide secrets from puppet logs\n\nCurrently secrets like rabbit_password or admin_password are laked\n\npuppet logs when changed. This commit changes glance_*_config and\nglance_*_ini types adding a new parameter that triggers obfuscation\nthe values in puppet logs.\n\nChange-Id: I7dc59ce9580bfb1d4afdfbced668d0cb2979458a\nCloses-Bug: #1328818\n'}, {'number': 2, 'created': '2014-07-12 00:40:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-neutron/commit/c6441757371e39679f6c5df7b9d9aa22f862d77a', 'message': 'Hide secrets from puppet logs\n\nCurrently secrets like rabbit_password or admin_password are laked\n\npuppet logs when changed. This commit changes glance_*_config and\nglance_*_ini types adding a new parameter that triggers obfuscation\nthe values in puppet logs.\n\nChange-Id: I7dc59ce9580bfb1d4afdfbced668d0cb2979458a\nCloses-Bug: #1328448'}, {'number': 3, 'created': '2014-07-22 13:07:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-neutron/commit/2d42c1fb88b21dd7982c91e46f387d30ab1ef478', 'message': 'Hide secrets from puppet logs\n\nCurrently secrets like rabbit_password, admin_password or database connection are laked in puppet logs when changed. This commit changes neutron_*_config and neutron_*_ini types adding a new parameter that triggers obfuscation the values in puppet logs.\n\nChange-Id: I7dc59ce9580bfb1d4afdfbced668d0cb2979458a\nCloses-Bug: #1328448'}, {'number': 4, 'created': '2014-07-22 13:08:00.000000000', 'files': ['spec/classes/neutron_init_spec.rb', 'spec/classes/neutron_server_notifications_spec.rb', 'manifests/server/notifications.pp', 'lib/puppet/type/neutron_plugin_nvp.rb', 'spec/classes/neutron_plugins_nvp_spec.rb', 'manifests/init.pp', 'spec/classes/neutron_agents_metadata_spec.rb', 'manifests/plugins/cisco.pp', 'manifests/plugins/nvp.pp', 'spec/classes/neutron_plugins_cisco_spec.rb', 'lib/puppet/type/neutron_config.rb', 'lib/puppet/type/neutron_metadata_agent_config.rb', 'lib/puppet/type/neutron_plugin_cisco.rb', 'lib/puppet/type/neutron_plugin_cisco_credentials.rb', 'spec/classes/neutron_server_spec.rb', 'lib/puppet/type/neutron_api_config.rb', 'manifests/agents/metadata.pp', 'manifests/server.pp'], 'web_link': 'https://opendev.org/openstack/puppet-neutron/commit/c7f8696a3abfa5771d811f6d6c35f5a1f2ba02ec', 'message': 'Hide secrets from puppet logs\n\nCurrently secrets like rabbit_password, admin_password or database connection\nare laked in puppet logs when changed. This commit changes neutron_*_config and \nneutron_*_ini types adding a new parameter that triggers obfuscation the values\nin puppet logs.\n\nChange-Id: I7dc59ce9580bfb1d4afdfbced668d0cb2979458a\nCloses-Bug: #1328448'}]",0,106524,c7f8696a3abfa5771d811f6d6c35f5a1f2ba02ec,32,5,4,7155,,,0,"Hide secrets from puppet logs

Currently secrets like rabbit_password, admin_password or database connection
are laked in puppet logs when changed. This commit changes neutron_*_config and 
neutron_*_ini types adding a new parameter that triggers obfuscation the values
in puppet logs.

Change-Id: I7dc59ce9580bfb1d4afdfbced668d0cb2979458a
Closes-Bug: #1328448",git fetch https://review.opendev.org/openstack/puppet-neutron refs/changes/24/106524/4 && git format-patch -1 --stdout FETCH_HEAD,"['spec/classes/neutron_init_spec.rb', 'spec/classes/neutron_server_notifications_spec.rb', 'manifests/server/notifications.pp', 'lib/puppet/type/neutron_plugin_nvp.rb', 'spec/classes/neutron_plugins_nvp_spec.rb', 'manifests/init.pp', 'spec/classes/neutron_agents_metadata_spec.rb', 'manifests/plugins/cisco.pp', 'manifests/plugins/nvp.pp', 'spec/classes/neutron_plugins_cisco_spec.rb', 'lib/puppet/type/neutron_config.rb', 'lib/puppet/type/neutron_metadata_agent_config.rb', 'lib/puppet/type/neutron_plugin_cisco.rb', 'lib/puppet/type/neutron_plugin_cisco_credentials.rb', 'spec/classes/neutron_server_spec.rb', 'lib/puppet/type/neutron_api_config.rb', 'manifests/agents/metadata.pp', 'manifests/server.pp']",18,aaf9726c2f0e6a86ed034ca613147b3daa9620bf,bug/1328818," 'database/connection': value => $database_connection_real, secret => true; 'keystone_authtoken/admin_password': value => $auth_password, secret => true; 'filter:authtoken/admin_password': value => $auth_password, secret => true;", 'database/connection': value => $database_connection_real; 'keystone_authtoken/admin_password': value => $auth_password; 'filter:authtoken/admin_password': value => $auth_password;,166,9
openstack%2Fpuppet-heat~master~Ib06a0f967dd5d5f8cc1c4dc7257c0e196786e8ae,openstack/puppet-heat,master,Ib06a0f967dd5d5f8cc1c4dc7257c0e196786e8ae,Hide secrets from puppet logs,MERGED,2014-07-12 00:33:30.000000000,2014-09-25 22:59:38.000000000,2014-07-29 22:38:47.000000000,"[{'_account_id': 3}, {'_account_id': 3153}, {'_account_id': 7156}, {'_account_id': 7822}]","[{'number': 1, 'created': '2014-07-12 00:33:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-heat/commit/4cc4ef51521f54e6929836ad5c50e747652b22c4', 'message': 'Hide secrets from puppet logs\n\nCurrently secrets like rabbit_password or admin_password are laked\n\npuppet logs when changed. This commit changes glance_*_config and\nglance_*_ini types adding a new parameter that triggers obfuscation\nthe values in puppet logs.\n\nChange-Id: Ib06a0f967dd5d5f8cc1c4dc7257c0e196786e8ae\nCloses-Bug: #1328818\n'}, {'number': 2, 'created': '2014-07-12 00:40:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-heat/commit/691982d255c8ecc48667ac63c67e95f742f4683c', 'message': 'Hide secrets from puppet logs\n\nCurrently secrets like rabbit_password or admin_password are laked\n\npuppet logs when changed. This commit changes glance_*_config and\nglance_*_ini types adding a new parameter that triggers obfuscation\nthe values in puppet logs.\n\nChange-Id: Ib06a0f967dd5d5f8cc1c4dc7257c0e196786e8ae\nCloses-Bug: #1328448'}, {'number': 3, 'created': '2014-07-22 13:13:23.000000000', 'files': ['manifests/init.pp', 'spec/classes/heat_init_spec.rb', 'lib/puppet/type/heat_config.rb'], 'web_link': 'https://opendev.org/openstack/puppet-heat/commit/6a89a44f9d2c6114b3bb1a4a74eff62ec13f545e', 'message': 'Hide secrets from puppet logs\n\nCurrently secrets like rabbit_password or admin_password are laked\n\npuppet logs when changed. This commit changes heat_*_config and\nheat_*_ini types adding a new parameter that triggers obfuscation\nthe values in puppet logs.\n\nChange-Id: Ib06a0f967dd5d5f8cc1c4dc7257c0e196786e8ae\nCloses-Bug: #1328448'}]",0,106526,6a89a44f9d2c6114b3bb1a4a74eff62ec13f545e,18,4,3,7155,,,0,"Hide secrets from puppet logs

Currently secrets like rabbit_password or admin_password are laked

puppet logs when changed. This commit changes heat_*_config and
heat_*_ini types adding a new parameter that triggers obfuscation
the values in puppet logs.

Change-Id: Ib06a0f967dd5d5f8cc1c4dc7257c0e196786e8ae
Closes-Bug: #1328448",git fetch https://review.opendev.org/openstack/puppet-heat refs/changes/26/106526/1 && git format-patch -1 --stdout FETCH_HEAD,"['manifests/init.pp', 'spec/classes/heat_init_spec.rb', 'lib/puppet/type/heat_config.rb']",3,4cc4ef51521f54e6929836ad5c50e747652b22c4,bug/1328818," def is_to_s( currentvalue ) if resource.secret? return '[old secret redacted]' else return currentvalue end end def should_to_s( newvalue ) if resource.secret? return '[new secret redacted]' else return newvalue end end end newparam(:secret, :boolean => true) do desc 'Whether to hide the value from Puppet logs. Defaults to `false`.' newvalues(:true, :false) defaultto false",,31,3
openstack%2Fpuppet-keystone~master~Idad892f0c461fce53eaa81ec8a7f3cfe871a9d00,openstack/puppet-keystone,master,Idad892f0c461fce53eaa81ec8a7f3cfe871a9d00,Use secret parameter for rabbit_password,MERGED,2014-06-11 11:38:26.000000000,2014-09-25 22:59:10.000000000,2014-06-11 23:05:33.000000000,"[{'_account_id': 3}, {'_account_id': 7155}, {'_account_id': 7156}]","[{'number': 1, 'created': '2014-06-11 11:38:26.000000000', 'files': ['spec/classes/keystone_spec.rb', 'manifests/init.pp'], 'web_link': 'https://opendev.org/openstack/puppet-keystone/commit/cbac8130da6732a1e8ad1dbcf9aec6ba75b7c20d', 'message': 'Use secret parameter for rabbit_password\n\nRabbit_password configuration was not using secret parameter so it\nwas visible on the logs.\n\nChange-Id: Idad892f0c461fce53eaa81ec8a7f3cfe871a9d00\nCloses-Bug: 1328448\n'}]",0,99344,cbac8130da6732a1e8ad1dbcf9aec6ba75b7c20d,14,3,1,8748,,,0,"Use secret parameter for rabbit_password

Rabbit_password configuration was not using secret parameter so it
was visible on the logs.

Change-Id: Idad892f0c461fce53eaa81ec8a7f3cfe871a9d00
Closes-Bug: 1328448
",git fetch https://review.opendev.org/openstack/puppet-keystone refs/changes/44/99344/1 && git format-patch -1 --stdout FETCH_HEAD,"['spec/classes/keystone_spec.rb', 'manifests/init.pp']",2,cbac8130da6732a1e8ad1dbcf9aec6ba75b7c20d,bug/1328448," 'DEFAULT/rabbit_password': value => $rabbit_password, secret => true;", 'DEFAULT/rabbit_password': value => $rabbit_password;,5,1
openstack%2Fpuppet-glance~master~I31f974a9afadef42939ee092ecba3b8f4333bb8b,openstack/puppet-glance,master,I31f974a9afadef42939ee092ecba3b8f4333bb8b,Hide secrets from puppet logs,MERGED,2014-06-11 08:42:54.000000000,2014-09-25 22:58:47.000000000,2014-08-04 17:00:17.000000000,"[{'_account_id': 3}, {'_account_id': 3153}, {'_account_id': 7155}, {'_account_id': 7156}, {'_account_id': 8748}]","[{'number': 1, 'created': '2014-06-11 08:42:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-glance/commit/f310d5f43d2c9b9b47636b88d6155afbb1eb7e6d', 'message': 'Hide secrets from puppet logs\n\nCurrently secrets like rabbit_password or admin_password are displayed in\npuppet logs when changed. This commit changes glance_*_config and\nglance_*_ini types adding a new parameter that triggers obfuscation of\nthe values in puppet logs.\n\nChange-Id: I31f974a9afadef42939ee092ecba3b8f4333bb8b\nCloses-Bug: #1328818\n'}, {'number': 2, 'created': '2014-06-25 06:38:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/puppet-glance/commit/3878585a2453c159bd98c3907c1e1a4df3054b21', 'message': 'Hide secrets from puppet logs\n\nCurrently secrets like rabbit_password or admin_password are displayed in\npuppet logs when changed. This commit changes glance_*_config and\nglance_*_ini types adding a new parameter that triggers obfuscation of\nthe values in puppet logs.\n\nChange-Id: I31f974a9afadef42939ee092ecba3b8f4333bb8b\nCloses-Bug: #1328818\n'}, {'number': 3, 'created': '2014-07-12 00:42:29.000000000', 'files': ['lib/puppet/type/glance_api_paste_ini.rb', 'manifests/notify/qpid.pp', 'manifests/notify/rabbitmq.pp', 'manifests/api.pp', 'spec/classes/glance_registry_spec.rb', 'lib/puppet/type/glance_cache_config.rb', 'lib/puppet/type/glance_api_config.rb', 'lib/puppet/type/glance_registry_paste_ini.rb', 'manifests/registry.pp', 'spec/classes/glance_notify_qpid_spec.rb', 'spec/classes/glance_api_spec.rb', 'lib/puppet/type/glance_registry_config.rb', 'spec/classes/glance_notify_rabbitmq_spec.rb'], 'web_link': 'https://opendev.org/openstack/puppet-glance/commit/a0c5c271effa0d613a0436646bf240ef9be27d2b', 'message': 'Hide secrets from puppet logs\n\nCurrently secrets like rabbit_password or admin_password are displayed in\npuppet logs when changed. This commit changes glance_*_config and\nglance_*_ini types adding a new parameter that triggers obfuscation of\nthe values in puppet logs.\n\nChange-Id: I31f974a9afadef42939ee092ecba3b8f4333bb8b\nCloses-Bug: #1328448\n'}]",7,99294,a0c5c271effa0d613a0436646bf240ef9be27d2b,29,5,3,8748,,,0,"Hide secrets from puppet logs

Currently secrets like rabbit_password or admin_password are displayed in
puppet logs when changed. This commit changes glance_*_config and
glance_*_ini types adding a new parameter that triggers obfuscation of
the values in puppet logs.

Change-Id: I31f974a9afadef42939ee092ecba3b8f4333bb8b
Closes-Bug: #1328448
",git fetch https://review.opendev.org/openstack/puppet-glance refs/changes/94/99294/1 && git format-patch -1 --stdout FETCH_HEAD,"['lib/puppet/type/glance_api_paste_ini.rb', 'manifests/notify/qpid.pp', 'manifests/notify/rabbitmq.pp', 'manifests/api.pp', 'spec/classes/glance_registry_spec.rb', 'lib/puppet/type/glance_cache_config.rb', 'lib/puppet/type/glance_api_config.rb', 'lib/puppet/type/glance_registry_paste_ini.rb', 'manifests/registry.pp', 'spec/classes/glance_notify_qpid_spec.rb', 'spec/classes/glance_api_spec.rb', 'lib/puppet/type/glance_registry_config.rb', 'spec/classes/glance_notify_rabbitmq_spec.rb']",13,f310d5f43d2c9b9b47636b88d6155afbb1eb7e6d,bug/1328818, it { should contain_glance_api_config('DEFAULT/rabbit_password').with_value(params[:rabbit_password]).with_secret(true) },,206,12
openstack%2Fironic~master~Icd0ceb7b588d435eba9eb30846a9c66565e98a5e,openstack/ironic,master,Icd0ceb7b588d435eba9eb30846a9c66565e98a5e,Add parameter to override locale to utils.execute,MERGED,2014-09-19 14:07:37.000000000,2014-09-25 22:48:01.000000000,2014-09-25 22:48:01.000000000,"[{'_account_id': 3}, {'_account_id': 6773}, {'_account_id': 7711}, {'_account_id': 9315}, {'_account_id': 12081}, {'_account_id': 12356}]","[{'number': 1, 'created': '2014-09-19 14:07:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f2eef2d2811ae01ac6bad60c10dcbd2afa3bc3fe', 'message': 'Add parameter to override locale to utils.execute\n\nIn common.utils.mkfs and in common.disk_partitioner.list_partitions\nfunctions standard locale is required in order to get correct console\noutput. In this change use_standard_locale flag is added to\ncommon.utils.execute function to avoid code duplication when copying\nenvironment variables and setting correct loacale.\n\nChange-Id: Icd0ceb7b588d435eba9eb30846a9c66565e98a5e\n'}, {'number': 2, 'created': '2014-09-22 16:19:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1b8c59fb326e9843be528624cc490801ee715864', 'message': 'Add parameter to override locale to utils.execute\n\nIn common.utils.mkfs and in common.disk_partitioner.list_partitions\nfunctions standard locale is required in order to get correct console\noutput. In this change use_standard_locale flag is added to\ncommon.utils.execute function to avoid code duplication when copying\nenvironment variables and setting correct loacale.\n\nChange-Id: Icd0ceb7b588d435eba9eb30846a9c66565e98a5e\n'}, {'number': 3, 'created': '2014-09-23 08:36:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0075cf86e9c0340a3de0fbffbbb6efc17c85427e', 'message': 'Add parameter to override locale to utils.execute\n\nIn common.utils.mkfs and in common.disk_partitioner.list_partitions\nfunctions standard locale is required in order to get correct console\noutput. In this change use_standard_locale flag is added to\ncommon.utils.execute function to avoid code duplication when copying\nenvironment variables and setting correct loacale.\n\nChange-Id: Icd0ceb7b588d435eba9eb30846a9c66565e98a5e\n'}, {'number': 4, 'created': '2014-09-25 12:07:37.000000000', 'files': ['ironic/tests/test_utils.py', 'ironic/tests/test_disk_partitioner.py', 'ironic/common/disk_partitioner.py', 'ironic/common/utils.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/17490a2fd9fae336172fb80ca06d21ca28af27cf', 'message': 'Add parameter to override locale to utils.execute\n\nIn common.utils.mkfs and in common.disk_partitioner.list_partitions\nfunctions standard locale is required in order to get correct console\noutput. In this change use_standard_locale flag is added to\ncommon.utils.execute function to avoid code duplication when copying\nenvironment variables and setting correct loacale.\n\nChange-Id: Icd0ceb7b588d435eba9eb30846a9c66565e98a5e\n'}]",10,122743,17490a2fd9fae336172fb80ca06d21ca28af27cf,35,6,4,12356,,,0,"Add parameter to override locale to utils.execute

In common.utils.mkfs and in common.disk_partitioner.list_partitions
functions standard locale is required in order to get correct console
output. In this change use_standard_locale flag is added to
common.utils.execute function to avoid code duplication when copying
environment variables and setting correct loacale.

Change-Id: Icd0ceb7b588d435eba9eb30846a9c66565e98a5e
",git fetch https://review.opendev.org/openstack/ironic refs/changes/43/122743/4 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/tests/test_utils.py', 'ironic/tests/test_disk_partitioner.py', 'ironic/common/disk_partitioner.py', 'ironic/common/utils.py']",4,f2eef2d2811ae01ac6bad60c10dcbd2afa3bc3fe,add_locale_param," use_standard_locale = kwargs.pop('use_standard_locale', False) if use_standard_locale: env = kwargs.pop('env_variables', os.environ.copy()) env['LC_ALL'] = 'C' kwargs['env_variables'] = env execute(*args, run_as_root=True, use_standard_locale=True)"," env = os.environ.copy() env['LC_ALL'] = 'C' execute(*args, run_as_root=True, env_variables=env)",19,27
openstack%2Fproject-config~master~I4806364ce7628a5bd2b81ea98898e6b6afb6ff4e,openstack/project-config,master,I4806364ce7628a5bd2b81ea98898e6b6afb6ff4e,Move review-dev acl file,MERGED,2014-09-25 22:13:35.000000000,2014-09-25 22:47:08.000000000,2014-09-25 22:47:08.000000000,"[{'_account_id': 3}, {'_account_id': 4146}, {'_account_id': 6316}]","[{'number': 1, 'created': '2014-09-25 22:13:35.000000000', 'files': ['dev/gerrit/acls/test-manage-project.config'], 'web_link': 'https://opendev.org/openstack/project-config/commit/ccdbf15c7cd4da2dc6c75c7a4ad84d6b505bc44f', 'message': 'Move review-dev acl file\n\nThe only acl file used by review-dev was in the wrong location.\n\nChange-Id: I4806364ce7628a5bd2b81ea98898e6b6afb6ff4e\n'}]",0,124203,ccdbf15c7cd4da2dc6c75c7a4ad84d6b505bc44f,7,3,1,1,,,0,"Move review-dev acl file

The only acl file used by review-dev was in the wrong location.

Change-Id: I4806364ce7628a5bd2b81ea98898e6b6afb6ff4e
",git fetch https://review.opendev.org/openstack/project-config refs/changes/03/124203/1 && git format-patch -1 --stdout FETCH_HEAD,['dev/gerrit/acls/test-manage-project.config'],1,ccdbf15c7cd4da2dc6c75c7a4ad84d6b505bc44f,,,,0,0
openstack%2Ffuel-main~stable%2F5.1~I3b659c67669fe31a98709af8ba92114ff963c897,openstack/fuel-main,stable/5.1,I3b659c67669fe31a98709af8ba92114ff963c897,Add neutron tests with enabled assign public network to all nodes,MERGED,2014-09-25 11:40:47.000000000,2014-09-25 22:40:12.000000000,2014-09-25 22:40:12.000000000,"[{'_account_id': 3}, {'_account_id': 6719}, {'_account_id': 8882}, {'_account_id': 8971}, {'_account_id': 11081}]","[{'number': 1, 'created': '2014-09-25 11:40:47.000000000', 'files': ['fuelweb_test/tests/test_neutron.py', 'fuelweb_test/models/fuel_web_client.py'], 'web_link': 'https://opendev.org/openstack/fuel-main/commit/87c3909dadf7ca00a7cd972d7d858c333de7b0d0', 'message': 'Add neutron tests with enabled assign public\nnetwork to all nodes\n\nChange-Id: I3b659c67669fe31a98709af8ba92114ff963c897\nCloses-Bug: #1362971\n'}]",0,124023,87c3909dadf7ca00a7cd972d7d858c333de7b0d0,8,5,1,10136,,,0,"Add neutron tests with enabled assign public
network to all nodes

Change-Id: I3b659c67669fe31a98709af8ba92114ff963c897
Closes-Bug: #1362971
",git fetch https://review.opendev.org/openstack/fuel-main refs/changes/23/124023/1 && git format-patch -1 --stdout FETCH_HEAD,"['fuelweb_test/tests/test_neutron.py', 'fuelweb_test/models/fuel_web_client.py']",2,87c3909dadf7ca00a7cd972d7d858c333de7b0d0,, if option == 'assign_to_all_nodes': section = 'public_network_assignment',,128,16
openstack%2Fapi-site~master~Ibbee21c5b4c3a178760855b155663e740dd4a380,openstack/api-site,master,Ibbee21c5b4c3a178760855b155663e740dd4a380,Change the following parameters from query strings in URI to parts of JSON body.,MERGED,2014-09-17 07:12:07.000000000,2014-09-25 22:30:36.000000000,2014-09-25 22:30:35.000000000,"[{'_account_id': 3}, {'_account_id': 612}, {'_account_id': 964}, {'_account_id': 8878}, {'_account_id': 10068}]","[{'number': 1, 'created': '2014-09-17 07:12:07.000000000', 'files': ['api-ref/src/wadls/compute-api/src/v2/ext/os-multi-server-create.wadl'], 'web_link': 'https://opendev.org/openstack/api-site/commit/f1e5934357574710299f05adf85976f953a836f7', 'message': 'Change the following parameters from query strings in URI to parts of JSON body.\n\n- return_reservation_id\n- min_count\n- max_count\n\nChange-Id: Ibbee21c5b4c3a178760855b155663e740dd4a380\nCloses-Bug: #1350708\n'}]",0,122077,f1e5934357574710299f05adf85976f953a836f7,9,5,1,12811,,,0,"Change the following parameters from query strings in URI to parts of JSON body.

- return_reservation_id
- min_count
- max_count

Change-Id: Ibbee21c5b4c3a178760855b155663e740dd4a380
Closes-Bug: #1350708
",git fetch https://review.opendev.org/openstack/api-site refs/changes/77/122077/1 && git format-patch -1 --stdout FETCH_HEAD,['api-ref/src/wadls/compute-api/src/v2/ext/os-multi-server-create.wadl'],1,f1e5934357574710299f05adf85976f953a836f7,bug/1350708," name=""return_reservation_id"" style=""plain"" <param name=""min_count"" style=""plain"" required=""false"" <param name=""max_count"" style=""plain"" required=""false"""," name=""return_reservation_id"" style=""query"" <param name=""min_count"" style=""query"" required=""false"" <param name=""max_count"" style=""query"" required=""false""",3,3
